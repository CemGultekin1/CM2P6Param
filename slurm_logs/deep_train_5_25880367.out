model 6174bdadc99f3cc96ca2c64d45ccbdad1a70c953399df274df2ee057 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=330.0, disp=1, domain='global', epoch=8, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  8    val-loss:  2.628682575727764  train-loss:  2.3710515098646283  lr:  0.0025
#epoch  9    val-loss:  2.5787842524679085  train-loss:  2.3629725072532892  lr:  0.0025
#epoch  10    val-loss:  2.564309986014115  train-loss:  2.3638976803049445  lr:  0.0025
#epoch  11    val-loss:  2.6485643637807748  train-loss:  2.352020093239844  lr:  0.00125
#epoch  12    val-loss:  2.6064914276725366  train-loss:  2.3602136047556996  lr:  0.00125
#epoch  13    val-loss:  2.5762255066319515  train-loss:  2.35871420847252  lr:  0.00125
#epoch  14    val-loss:  2.6342098210987293  train-loss:  2.352297941688448  lr:  0.000625
#epoch  15    val-loss:  2.5792399707593416  train-loss:  2.3590600532479584  lr:  0.000625
#epoch  16    val-loss:  2.5587518215179443  train-loss:  2.3528685020282865  lr:  0.000625
#epoch  17    val-loss:  2.581169555061742  train-loss:  2.359903168864548  lr:  0.000625
#epoch  18    val-loss:  2.618828535079956  train-loss:  2.3626652797684073  lr:  0.000625
#epoch  19    val-loss:  2.597344624368768  train-loss:  2.365480797830969  lr:  0.000625
#epoch  20    val-loss:  2.554275293099253  train-loss:  2.3538940348662436  lr:  0.0003125
#epoch  21    val-loss:  2.554449432774594  train-loss:  2.3523281761445105  lr:  0.0003125
#epoch  22    val-loss:  2.5477630150945565  train-loss:  2.356774954125285  lr:  0.0003125
#epoch  23    val-loss:  2.5478614568710327  train-loss:  2.351606440730393  lr:  0.0003125
#epoch  24    val-loss:  2.582217254136738  train-loss:  2.351613618899137  lr:  0.0003125
#epoch  25    val-loss:  2.6217633423052336  train-loss:  2.349548430647701  lr:  0.0003125
#epoch  26    val-loss:  2.60973186241953  train-loss:  2.3524138247594237  lr:  0.00015625
#epoch  27    val-loss:  2.57991529765882  train-loss:  2.352911259047687  lr:  0.00015625
#epoch  28    val-loss:  2.5760727179677865  train-loss:  2.3514068205840886  lr:  0.00015625
#epoch  29    val-loss:  2.6056438998172156  train-loss:  2.3535359906964004  lr:  7.8125e-05
#epoch  30    val-loss:  2.6233758173490824  train-loss:  2.352731051389128  lr:  7.8125e-05
#epoch  31    val-loss:  2.5751433999914872  train-loss:  2.3545721732079983  lr:  7.8125e-05
#epoch  32    val-loss:  2.6001555919647217  train-loss:  2.358051632065326  lr:  3.90625e-05
#epoch  33    val-loss:  2.6436096743533484  train-loss:  2.355930589605123  lr:  3.90625e-05
#epoch  34    val-loss:  2.6412457039481714  train-loss:  2.3511407733894885  lr:  3.90625e-05
#epoch  35    val-loss:  2.560814518677561  train-loss:  2.356191001832485  lr:  1.953125e-05
#epoch  36    val-loss:  2.6066659626207853  train-loss:  2.3508164351806045  lr:  1.953125e-05
#epoch  37    val-loss:  2.6285393990968404  train-loss:  2.357908882200718  lr:  1.953125e-05
#epoch  38    val-loss:  2.619447432066265  train-loss:  2.3527703476138413  lr:  9.765625e-06
#epoch  39    val-loss:  2.61113651175248  train-loss:  2.351567594334483  lr:  9.765625e-06
#epoch  40    val-loss:  2.5774020772231254  train-loss:  2.352311698719859  lr:  9.765625e-06
#epoch  41    val-loss:  2.577481269836426  train-loss:  2.3516071434132755  lr:  4.8828125e-06
#epoch  42    val-loss:  2.5578788330680444  train-loss:  2.3527824180200696  lr:  4.8828125e-06
#epoch  43    val-loss:  2.5825606208098564  train-loss:  2.3563772961497307  lr:  4.8828125e-06
#epoch  44    val-loss:  2.566401720046997  train-loss:  2.356936854775995  lr:  2.44140625e-06
#epoch  45    val-loss:  2.5746993014686987  train-loss:  2.3482794631272554  lr:  2.44140625e-06
#epoch  46    val-loss:  2.599030620173404  train-loss:  2.3549835309386253  lr:  2.44140625e-06
#epoch  47    val-loss:  2.5570973471591345  train-loss:  2.352004101034254  lr:  1.220703125e-06
#epoch  48    val-loss:  2.605010885941355  train-loss:  2.35357269924134  lr:  1.220703125e-06
#epoch  49    val-loss:  2.5772564662130257  train-loss:  2.355809065513313  lr:  1.220703125e-06
#epoch  50    val-loss:  2.611441925952309  train-loss:  2.3590954286046326  lr:  6.103515625e-07
#epoch  51    val-loss:  2.571380188590602  train-loss:  2.36662609083578  lr:  6.103515625e-07
#epoch  52    val-loss:  2.569655280364187  train-loss:  2.3475540126673877  lr:  6.103515625e-07
#epoch  53    val-loss:  2.604179206647371  train-loss:  2.3564191446639597  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.566446900367737  train-loss:  2.3546493807807565  lr:  3.0517578125e-07
#epoch  55    val-loss:  2.612061902096397  train-loss:  2.3660741467028856  lr:  3.0517578125e-07
#epoch  56    val-loss:  2.58663282896343  train-loss:  2.344557491131127  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.5744649987471733  train-loss:  2.3617317536845803  lr:  1.52587890625e-07
#epoch  58    val-loss:  2.5816142119859395  train-loss:  2.3477537161670625  lr:  1.52587890625e-07
#epoch  59    val-loss:  2.636937982157657  train-loss:  2.361720127053559  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.6143844504105416  train-loss:  2.3581856065429747  lr:  7.62939453125e-08
#epoch  61    val-loss:  2.612711417047601  train-loss:  2.35186205804348  lr:  7.62939453125e-08
#epoch  62    val-loss:  2.5780813317549858  train-loss:  2.356509828940034  lr:  3.814697265625e-08
#epoch  63    val-loss:  2.578264261546888  train-loss:  2.357580889482051  lr:  3.814697265625e-08
#epoch  64    val-loss:  2.5969225858387195  train-loss:  2.3505946225486696  lr:  3.814697265625e-08
#epoch  65    val-loss:  2.5773931678972746  train-loss:  2.3547999979928136  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.5876050622839677  train-loss:  2.354963114950806  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.5695447168852152  train-loss:  2.3573596449568868  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.577937778673674  train-loss:  2.354813967831433  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.595649982753553  train-loss:  2.3507296568714082  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.607995447359587  train-loss:  2.3488237555138767  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.614042959715191  train-loss:  2.3660860173404217  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.6274766294579757  train-loss:  2.3550875922665  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.572043908269782  train-loss:  2.352317415177822  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.575587423224198  train-loss:  2.357441820204258  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.589960587652106  train-loss:  2.360121825709939  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.657274522279438  train-loss:  2.356353025883436  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.554447236813997  train-loss:  2.3464535768143833  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.578812410956935  train-loss:  2.3576995208859444  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.5379276212893034  train-loss:  2.3548923833295703  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.6307181935561332  train-loss:  2.3531504017300904  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.5914919125406364  train-loss:  2.3505939403548837  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.605791493466026  train-loss:  2.3570911888964474  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.5462793927443657  train-loss:  2.3582472414709628  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.581358558253238  train-loss:  2.354667382314801  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.6083313038474634  train-loss:  2.359100494068116  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.5811335413079513  train-loss:  2.3560008322820067  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.6028806661304675  train-loss:  2.3514622445218265  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.603034860209415  train-loss:  2.3563508330844343  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.5842433979636743  train-loss:  2.3548186416737735  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.5838645131964433  train-loss:  2.3576434827409685  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.626134282664249  train-loss:  2.3572362237609923  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.5755110665371546  train-loss:  2.358382265083492  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.618051152480276  train-loss:  2.3578530405648053  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.6684226550553976  train-loss:  2.347810996696353  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.597852242620368  train-loss:  2.348618634045124  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.5838655170641447  train-loss:  2.3660579519346356  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.584124251415855  train-loss:  2.352774583734572  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.5603754143965873  train-loss:  2.356265150476247  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.559424826973363  train-loss:  2.348081548232585  lr:  1.9073486328125e-08
