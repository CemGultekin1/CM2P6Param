model ba4c19b045af244f92ef5415cc764437d466e1160cc3db553a461431 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=5.0, disp=1, domain='global', epoch=13, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=16, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
			 train-loss:  2.254342555999756 	 ± 0.0
	data : 4.808758735656738
	model : 1.913236379623413
			 train-loss:  2.0998889803886414 	 ± 0.1544535756111145
	data : 2.425809621810913
	model : 0.9910826683044434
			 train-loss:  1.9911175568898518 	 ± 0.19891300398066367
	data : 1.6554307142893474
	model : 0.6838946342468262
			 train-loss:  2.103538930416107 	 ± 0.25998169751166733
	data : 1.2704407572746277
	model : 0.5291704535484314
			 train-loss:  2.036795806884766 	 ± 0.2681249048255647
	data : 1.0399317264556884
	model : 0.43718829154968264
			 train-loss:  2.0564425388971963 	 ± 0.24867470227241284
	data : 0.10107693672180176
	model : 0.06747303009033204
			 train-loss:  2.1207126208714078 	 ± 0.27890645376297146
	data : 0.11619925498962402
	model : 0.06838140487670899
			 train-loss:  2.1531797647476196 	 ± 0.2746707423937932
	data : 0.11533679962158203
	model : 0.06751513481140137
			 train-loss:  2.1706545088026257 	 ± 0.2636366469977602
	data : 0.1158289909362793
	model : 0.0674363136291504
			 train-loss:  2.197457695007324 	 ± 0.2627157220128118
	data : 0.11608414649963379
	model : 0.06661553382873535
			 train-loss:  2.1622235016389326 	 ± 0.27415238429856403
	data : 0.11708545684814453
	model : 0.06680355072021485
			 train-loss:  2.157746066649755 	 ± 0.2629006605737435
	data : 0.11700620651245117
	model : 0.06503801345825196
			 train-loss:  2.147592773804298 	 ± 0.25502383409516893
	data : 0.11864800453186035
	model : 0.06677899360656739
			 train-loss:  2.1253948296819414 	 ± 0.25845188762942595
	data : 0.11695947647094726
	model : 0.06746292114257812
			 train-loss:  2.101057489713033 	 ± 0.2657752909996282
	data : 0.11684150695800781
	model : 0.06747326850891114
			 train-loss:  2.1051394268870354 	 ± 0.25782097837589457
	data : 0.1169243335723877
	model : 0.06938309669494629
			 train-loss:  2.095769538598902 	 ± 0.25291555177777825
	data : 0.11515789031982422
	model : 0.06937270164489746
			 train-loss:  2.096531238820818 	 ± 0.2458097994294431
	data : 0.298597526550293
	model : 0.06752610206604004
			 train-loss:  2.075735192549856 	 ± 0.25500370170400266
	data : 0.30420942306518556
	model : 0.0675445556640625
			 train-loss:  2.0788577795028687 	 ± 0.24891927190338262
	data : 0.3339656352996826
	model : 0.06896953582763672
			 train-loss:  2.0847522644769576 	 ± 0.24434645181481682
	data : 0.36062121391296387
	model : 0.06679668426513671
			 train-loss:  2.103200218894265 	 ± 0.25325517521884994
	data : 0.362565279006958
	model : 0.06677727699279785
			 train-loss:  2.090587263521941 	 ± 0.2546555973077922
	data : 0.17926945686340331
	model : 0.0667567253112793
			 train-loss:  2.102310578028361 	 ± 0.2555551647385721
	data : 0.1755948543548584
	model : 0.06785097122192382
			 train-loss:  2.1233998584747313 	 ± 0.27086950830514733
	data : 0.18838753700256347
	model : 0.0665407657623291
			 train-loss:  2.122863393563491 	 ± 0.2656229492031205
	data : 0.16158833503723144
	model : 0.06667418479919433
			 train-loss:  2.1097671146746033 	 ± 0.2690756550147589
	data : 0.16165060997009278
	model : 0.06684784889221192
			 train-loss:  2.116875933749335 	 ± 0.26679652577342505
	data : 0.16177148818969728
	model : 0.0668680191040039
			 train-loss:  2.108105285414334 	 ± 0.2662325481833088
	data : 0.16176390647888184
	model : 0.06499056816101074
			 train-loss:  2.1049124280611675 	 ± 0.2623218376842164
	data : 0.11995725631713867
	model : 0.06472883224487305
			 train-loss:  2.1084079550158594 	 ± 0.2587654184194348
	data : 0.12058892250061035
	model : 0.06468496322631836
			 train-loss:  2.110937375575304 	 ± 0.25507918987804973
	data : 0.17987356185913086
	model : 0.06460785865783691
			 train-loss:  2.115310231844584 	 ± 0.2523997116326471
	data : 0.1830522060394287
	model : 0.06477575302124024
			 train-loss:  2.120972040821524 	 ± 0.2507783295253667
	data : 0.25092878341674807
	model : 0.06482667922973633
			 train-loss:  2.11457907812936 	 ± 0.2499649955884905
	data : 0.25025348663330077
	model : 0.06658005714416504
			 train-loss:  2.113833877775404 	 ± 0.24650823591038987
	data : 0.3174447059631348
	model : 0.06651010513305664
			 train-loss:  2.1048385130392537 	 ± 0.2490722280412143
	data : 0.2931621551513672
	model : 0.06646041870117188
			 train-loss:  2.1037586801930477 	 ± 0.24586086767841941
	data : 0.28975629806518555
	model : 0.06765856742858886
			 train-loss:  2.098657339047163 	 ± 0.24471724204841533
	data : 0.25077285766601565
	model : 0.07140069007873535
			 train-loss:  2.1013230860233305 	 ± 0.24221169957825134
	data : 0.25060214996337893
	model : 0.06968913078308106
			 train-loss:  2.0987924482764266 	 ± 0.23977444001421433
	data : 0.18270502090454102
	model : 0.06979589462280274
			 train-loss:  2.1067261468796503 	 ± 0.2422882924211248
	data : 0.1477839469909668
	model : 0.06974968910217286
			 train-loss:  2.1081172810044397 	 ± 0.23962407419059376
	data : 0.14776163101196288
	model : 0.06848235130310058
			 train-loss:  2.100727753205733 	 ± 0.24179066157813728
	data : 0.11887211799621582
	model : 0.0648733139038086
			 train-loss:  2.1018404006958007 	 ± 0.2392028923248988
	data : 0.11907896995544434
	model : 0.06481480598449707
			 train-loss:  2.1025291162988413 	 ± 0.2366336798290145
	data : 0.11906819343566895
	model : 0.06474084854125976
			 train-loss:  2.1127049060578043 	 ± 0.24406403446200753
	data : 0.11880850791931152
	model : 0.06499428749084472
			 train-loss:  2.1039638419946036 	 ± 0.24883199136578282
	data : 0.17443737983703614
	model : 0.06488022804260254
			 train-loss:  2.101781502061961 	 ± 0.24674348117149925
	data : 0.17678956985473632
	model : 0.06476478576660157
			 train-loss:  2.092650122642517 	 ± 0.25248845717301105
	data : 0.1766740322113037
	model : 0.06473736763000489
			 train-loss:  2.0886825556848563 	 ± 0.25157005411767835
	data : 0.17695488929748535
	model : 0.06473145484924317
			 train-loss:  2.0885900419491987 	 ± 0.2491402445828546
	data : 0.21600422859191895
	model : 0.06519851684570313
			 train-loss:  2.07892984489225 	 ± 0.25642215644222155
	data : 0.25976061820983887
	model : 0.06739425659179688
			 train-loss:  2.082457469569312 	 ± 0.2553315999181465
	data : 0.25527515411376955
	model : 0.0674072265625
			 train-loss:  2.084144945578142 	 ± 0.25330346507653184
	data : 0.255267333984375
	model : 0.06954903602600097
			 train-loss:  2.0844313693898067 	 ± 0.25104062622894024
	data : 0.2534346580505371
	model : 0.06971588134765624
			 train-loss:  2.080018677209553 	 ± 0.2510103209398902
	data : 0.21455760002136232
	model : 0.06907320022583008
			 train-loss:  2.075936126297918 	 ± 0.25073870742267157
	data : 0.15827302932739257
	model : 0.06848258972167968
			 train-loss:  2.0740198301056685 	 ± 0.2490327186703203
	data : 0.1601891040802002
	model : 0.0684587001800537
			 train-loss:  2.077931108077367 	 ± 0.24876949373359117
	data : 0.1624000072479248
	model : 0.06874799728393555
			 train-loss:  2.084029449791205 	 ± 0.251203340827574
	data : 0.16393561363220216
	model : 0.06866974830627441
			 train-loss:  2.086352780941994 	 ± 0.2498291338811261
	data : 0.24146618843078613
	model : 0.06894397735595703
			 train-loss:  2.085650949251084 	 ± 0.24790003521461826
	data : 0.19834537506103517
	model : 0.06932101249694825
			 train-loss:  2.0967127699404955 	 ± 0.26115728264616933
	data : 0.1986301898956299
	model : 0.07018871307373047
			 train-loss:  2.0927899030538706 	 ± 0.26103397485451924
	data : 0.19715232849121095
	model : 0.06790175437927246
			 train-loss:  2.0958596287351665 	 ± 0.26022843803340884
	data : 0.2003326416015625
	model : 0.07204146385192871
			 train-loss:  2.0917113361073962 	 ± 0.26046854109575646
	data : 0.12449941635131836
	model : 0.07466788291931152
			 train-loss:  2.1050307505271015 	 ± 0.2805929662272322
	data : 0.12603917121887206
	model : 0.07331357002258301
			 train-loss:  2.1079785029093423 	 ± 0.27961085786519096
	data : 0.21829357147216796
	model : 0.07261781692504883
			 train-loss:  2.11273649420057 	 ± 0.28040577384866466
	data : 0.22108540534973145
	model : 0.07263855934143067
			 train-loss:  2.1105756104831963 	 ± 0.2790104463232805
	data : 0.2179971694946289
	model : 0.06851692199707031
			 train-loss:  2.1087450749344296 	 ± 0.2774951067767887
	data : 0.21640582084655763
	model : 0.06564135551452636
			 train-loss:  2.1057046766150487 	 ± 0.27679281227908564
	data : 0.2154533863067627
	model : 0.06501789093017578
			 train-loss:  2.10414296871907 	 ± 0.27523985009858354
	data : 0.12332968711853028
	model : 0.06486067771911622
			 train-loss:  2.105471642812093 	 ± 0.27363757019049556
	data : 0.26578578948974607
	model : 0.06481938362121582
			 train-loss:  2.1057708451622412 	 ± 0.2718437114396602
	data : 0.26542115211486816
	model : 0.06479148864746094
			 train-loss:  2.107085268218796 	 ± 0.2703157079002399
	data : 0.26533875465393064
	model : 0.06648917198181152
			 train-loss:  2.1112636572275405 	 ± 0.2710684739648288
	data : 0.2649601936340332
	model : 0.06659460067749023
			 train-loss:  2.1104453575762014 	 ± 0.2694443258641491
	data : 0.26831927299499514
	model : 0.06682634353637695
			 train-loss:  2.1069616720080377 	 ± 0.2695394049615374
	data : 0.12248358726501465
	model : 0.0668118953704834
			 train-loss:  2.10708083782667 	 ± 0.26787253476399425
	data : 0.12291445732116699
	model : 0.06972546577453613
			 train-loss:  2.108604269783671 	 ± 0.2665869724316195
	data : 0.12285933494567872
	model : 0.06798934936523438
			 train-loss:  2.109033491238054 	 ± 0.2650046649722235
	data : 0.1228250503540039
	model : 0.06782093048095703
			 train-loss:  2.1094691313448406 	 ± 0.263452430334938
	data : 0.12030620574951172
	model : 0.06758580207824708
			 train-loss:  2.1077362341039323 	 ± 0.26237925663794526
	data : 0.14840483665466309
	model : 0.0675389289855957
			 train-loss:  2.1035324099451995 	 ± 0.2637129262431752
	data : 0.14820051193237305
	model : 0.06467576026916504
			 train-loss:  2.099177159112075 	 ± 0.26528552798335575
	data : 0.14836468696594238
	model : 0.06471214294433594
			 train-loss:  2.0995468239892614 	 ± 0.2637964521079245
	data : 0.14958038330078124
	model : 0.06900100708007813
			 train-loss:  2.101368450046925 	 ± 0.26286628998808287
	data : 0.14688448905944823
	model : 0.06893653869628906
			 train-loss:  2.0999175667762757 	 ± 0.261759954483744
	data : 0.11868176460266114
	model : 0.06900324821472167
			 train-loss:  2.1003475071309685 	 ± 0.2603496919369868
	data : 0.2683894157409668
	model : 0.06885600090026855
			 train-loss:  2.098555739807046 	 ± 0.25949441376874693
	data : 0.2682944774627686
	model : 0.0688908576965332
			 train-loss:  2.0975833721058343 	 ± 0.25826397182923555
	data : 0.26766114234924315
	model : 0.06474390029907226
			 train-loss:  2.0997819482012 	 ± 0.25776004196554736
	data : 0.3205695629119873
	model : 0.0647961139678955
			 train-loss:  2.0967947395224322 	 ± 0.25803036520859746
	data : 0.32056174278259275
	model : 0.06630873680114746
			 train-loss:  2.1001617896060147 	 ± 0.25877238771223265
	data : 0.17088651657104492
	model : 0.06638636589050292
			 train-loss:  2.0974636532596707 	 ± 0.2587888736384977
	data : 0.17089028358459474
	model : 0.06633362770080567
			 train-loss:  2.0962126595633372 	 ± 0.2577597722395972
	data : 0.17313380241394044
	model : 0.06633539199829101
			 train-loss:  2.0988049507141113 	 ± 0.2577354175431572
	data : 0.12424354553222657
	model : 0.06895251274108886
			 train-loss:  2.1036275243759155 	 ± 0.2608941058307247
	data : 0.12436089515686036
	model : 0.06733322143554688
			 train-loss:  2.099678536452869 	 ± 0.262585732392122
	data : 0.12452664375305175
	model : 0.06727466583251954
			 train-loss:  2.09934308832767 	 ± 0.2613171236515299
	data : 0.12457776069641113
	model : 0.06723337173461914
			 train-loss:  2.097114509749181 	 ± 0.2610177224016559
	data : 0.12171497344970703
	model : 0.06726384162902832
			 train-loss:  2.097026038628358 	 ± 0.25976135015020835
	data : 0.126891565322876
	model : 0.06597456932067872
			 train-loss:  2.095448747135344 	 ± 0.2590213641571146
	data : 0.12690434455871583
	model : 0.06599240303039551
			 train-loss:  2.100000994385413 	 ± 0.2619828874914708
	data : 0.12678313255310059
	model : 0.06595988273620605
			 train-loss:  2.1012776688994648 	 ± 0.26108686982886764
	data : 0.2145145893096924
	model : 0.0693885326385498
			 train-loss:  2.099708224888201 	 ± 0.26038191409601275
	data : 0.21519384384155274
	model : 0.06935820579528809
			 train-loss:  2.1002641544429532 	 ± 0.25924913233558594
	data : 0.2076840877532959
	model : 0.06808772087097167
			 train-loss:  2.1032269618727946 	 ± 0.2599152524004593
	data : 0.20816469192504883
	model : 0.06915116310119629
			 train-loss:  2.101682685516976 	 ± 0.2592482463072959
	data : 0.20802726745605468
	model : 0.06923565864562989
			 train-loss:  2.102514517094408 	 ± 0.2582370476574373
	data : 0.16371893882751465
	model : 0.06857790946960449
			 train-loss:  2.100493933247254 	 ± 0.2579796443680868
	data : 0.16281509399414062
	model : 0.07094388008117676
			 train-loss:  2.0998332061265645 	 ± 0.25694167746959923
	data : 0.16483402252197266
	model : 0.07095494270324706
			 train-loss:  2.0978535973507424 	 ± 0.25669377892819506
	data : 0.16425104141235353
	model : 0.06992731094360352
			 train-loss:  2.094871937200941 	 ± 0.2575772669720344
	data : 0.1642509937286377
	model : 0.06989541053771972
			 train-loss:  2.0959202703247723 	 ± 0.256722559368663
	data : 0.17588658332824708
	model : 0.0685495376586914
			 train-loss:  2.0949407432038907 	 ± 0.2558519131186027
	data : 0.17589612007141114
	model : 0.0661628246307373
			 train-loss:  2.0974911060653816 	 ± 0.256276470058393
	data : 0.17576985359191893
	model : 0.06609468460083008
			 train-loss:  2.0961645513772966 	 ± 0.2556163638408693
	data : 0.2266528606414795
	model : 0.06827659606933593
			 train-loss:  2.097554789101782 	 ± 0.2550130566143556
	data : 0.22515368461608887
	model : 0.06832046508789062
			 train-loss:  2.098236032196733 	 ± 0.25407630312344404
	data : 0.17131500244140624
	model : 0.06899213790893555
			 train-loss:  2.09995376869915 	 ± 0.25375166556818246
	data : 0.1715330123901367
	model : 0.07164325714111328
			 train-loss:  2.099285795803993 	 ± 0.25283495689244545
	data : 0.21416401863098145
	model : 0.07277207374572754
			 train-loss:  2.0994623479843137 	 ± 0.2518292605452452
	data : 0.16417460441589354
	model : 0.07066364288330078
			 train-loss:  2.0985374261462497 	 ± 0.25104102103846526
	data : 0.16928043365478515
	model : 0.07555427551269531
			 train-loss:  2.0961893111702024 	 ± 0.2514360363094849
	data : 0.16577725410461425
	model : 0.07441253662109375
			 train-loss:  2.095402020961046 	 ± 0.250609040864804
	data : 0.16623320579528808
	model : 0.07301959991455079
			 train-loss:  2.094109351320784 	 ± 0.2500638300003823
	data : 0.19717631340026856
	model : 0.07211360931396485
			 train-loss:  2.0958849971111 	 ± 0.24991524709553636
	data : 0.19705123901367189
	model : 0.07443175315856934
			 train-loss:  2.0944919904679744 	 ± 0.2494656603617633
	data : 0.19150009155273437
	model : 0.06954636573791503
			 train-loss:  2.092321818525141 	 ± 0.2497571147380126
	data : 0.2046494960784912
	model : 0.07065443992614746
			 train-loss:  2.094736106413648 	 ± 0.25035775195166415
	data : 0.20446648597717285
	model : 0.07425222396850586
			 train-loss:  2.0937025431376783 	 ± 0.2497064834088632
	data : 0.1301414966583252
	model : 0.07887763977050781
			 train-loss:  2.096978133696097 	 ± 0.25165294252926657
	data : 0.14260873794555665
	model : 0.07764158248901368
			 train-loss:  2.096488716847756 	 ± 0.2507905181034856
	data : 0.1468271255493164
	model : 0.08037691116333008
			 train-loss:  2.0943494104120854 	 ± 0.2511159331514615
	data : 0.13305702209472656
	model : 0.08388357162475586
			 train-loss:  2.094433414763299 	 ± 0.2502063705102978
	data : 0.13027644157409668
	model : 0.08464455604553223
			 train-loss:  2.093648443976752 	 ± 0.2494752044403177
	data : 0.23418059349060058
	model : 0.08235650062561035
			 train-loss:  2.0933393120765684 	 ± 0.2486093412437606
	data : 0.21871848106384278
	model : 0.08568105697631836
			 train-loss:  2.093379450182543 	 ± 0.24772663439761028
	data : 0.21996288299560546
	model : 0.08296647071838378
			 train-loss:  2.092517895597807 	 ± 0.24706471604883826
	data : 0.22266545295715331
	model : 0.07735576629638671
			 train-loss:  2.0922164908655874 	 ± 0.24622553466735803
	data : 0.2252200126647949
	model : 0.07181692123413086
			 train-loss:  2.091506325536304 	 ± 0.24551601346528062
	data : 0.12014079093933105
	model : 0.07145500183105469
			 train-loss:  2.0910259246826173 	 ± 0.24473584694892817
	data : 0.1343143939971924
	model : 0.07528257369995117
			 train-loss:  2.089820281283496 	 ± 0.24432797471597184
	data : 0.13174548149108886
	model : 0.0787663459777832
			 train-loss:  2.0916679367727165 	 ± 0.24451683419618284
	data : 0.1603309154510498
	model : 0.07882347106933593
			 train-loss:  2.0938304380790607 	 ± 0.24509577418531292
	data : 0.1752096176147461
	model : 0.08062210083007812
			 train-loss:  2.0949446266929574 	 ± 0.24464770744567155
	data : 0.1751941204071045
	model : 0.07894287109375
			 train-loss:  2.0958985845247904 	 ± 0.24410874499471025
	data : 0.16451735496520997
	model : 0.07471585273742676
			 train-loss:  2.094751731449405 	 ± 0.24370420701043144
	data : 0.17441263198852539
	model : 0.07140226364135742
