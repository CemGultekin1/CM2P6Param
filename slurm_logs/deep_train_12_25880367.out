model 55a6321c53bb4ce1cd478c910639d569ad92ff257218d0f8bb4467bf state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=1497.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  3.984691964952569  train-loss:  2.1999215490359347  lr:  0.005
#epoch  8    val-loss:  4.040832312483537  train-loss:  2.205832541687414  lr:  0.005
#epoch  9    val-loss:  3.9211146078611674  train-loss:  2.2108604304958135  lr:  0.005
#epoch  10    val-loss:  3.8693414299111617  train-loss:  2.2098406481090933  lr:  0.0025
#epoch  11    val-loss:  3.9599476425271285  train-loss:  2.218326108995825  lr:  0.0025
#epoch  12    val-loss:  3.8577221192811666  train-loss:  2.20264130551368  lr:  0.0025
#epoch  13    val-loss:  3.8910889500065853  train-loss:  2.2034399388357997  lr:  0.00125
#epoch  14    val-loss:  4.049864894465396  train-loss:  2.203493176610209  lr:  0.00125
#epoch  15    val-loss:  4.131296007256759  train-loss:  2.2077191860880703  lr:  0.00125
#epoch  16    val-loss:  3.8904777075115002  train-loss:  2.198781391605735  lr:  0.000625
#epoch  17    val-loss:  3.916202921616404  train-loss:  2.2145261457189918  lr:  0.000625
#epoch  18    val-loss:  3.9610188760255514  train-loss:  2.1993820215575397  lr:  0.000625
#epoch  19    val-loss:  4.089415236523277  train-loss:  2.203500682953745  lr:  0.0003125
#epoch  20    val-loss:  4.003400614387111  train-loss:  2.2103157537057996  lr:  0.0003125
#epoch  21    val-loss:  3.7878355603469047  train-loss:  2.198424505419098  lr:  0.0003125
#epoch  22    val-loss:  4.104833753485429  train-loss:  2.2008198492694646  lr:  0.0003125
#epoch  23    val-loss:  4.048869766687092  train-loss:  2.2057410082779825  lr:  0.0003125
#epoch  24    val-loss:  3.910993776823345  train-loss:  2.202745993854478  lr:  0.0003125
#epoch  25    val-loss:  4.014480879432277  train-loss:  2.200993750942871  lr:  0.00015625
#epoch  26    val-loss:  3.8611405460458053  train-loss:  2.2009003448765725  lr:  0.00015625
#epoch  27    val-loss:  3.895183663619192  train-loss:  2.208958141040057  lr:  0.00015625
#epoch  28    val-loss:  3.989480231937609  train-loss:  2.208710905862972  lr:  7.8125e-05
#epoch  29    val-loss:  3.901239137900503  train-loss:  2.2002459226641804  lr:  7.8125e-05
#epoch  30    val-loss:  4.0049037431415755  train-loss:  2.1922971797175705  lr:  7.8125e-05
#epoch  31    val-loss:  4.131180173472354  train-loss:  2.2074560404289514  lr:  3.90625e-05
#epoch  32    val-loss:  3.885064802671734  train-loss:  2.199154003057629  lr:  3.90625e-05
#epoch  33    val-loss:  3.880037633996261  train-loss:  2.211624951567501  lr:  3.90625e-05
#epoch  34    val-loss:  4.008990934020595  train-loss:  2.2007402037270367  lr:  1.953125e-05
#epoch  35    val-loss:  3.898652898637872  train-loss:  2.2030108934268355  lr:  1.953125e-05
#epoch  36    val-loss:  3.8695758769386694  train-loss:  2.2038291620556265  lr:  1.953125e-05
#epoch  37    val-loss:  4.170322518599661  train-loss:  2.208079559262842  lr:  9.765625e-06
#epoch  38    val-loss:  3.9867378285056665  train-loss:  2.194682959932834  lr:  9.765625e-06
#epoch  39    val-loss:  3.9077392345980595  train-loss:  2.1995513017755  lr:  9.765625e-06
#epoch  40    val-loss:  3.9551373218235217  train-loss:  2.19933294178918  lr:  4.8828125e-06
#epoch  41    val-loss:  4.014208335625498  train-loss:  2.2085218406282365  lr:  4.8828125e-06
#epoch  42    val-loss:  4.024158346025567  train-loss:  2.200919605093077  lr:  4.8828125e-06
#epoch  43    val-loss:  3.784333784329264  train-loss:  2.203465934144333  lr:  2.44140625e-06
#epoch  44    val-loss:  3.9962571922101473  train-loss:  2.2057467771228403  lr:  2.44140625e-06
#epoch  45    val-loss:  3.9216318255976628  train-loss:  2.198478464735672  lr:  2.44140625e-06
#epoch  46    val-loss:  4.016897364666588  train-loss:  2.2011199947446585  lr:  2.44140625e-06
#epoch  47    val-loss:  4.010744885394447  train-loss:  2.2000913117080927  lr:  1.220703125e-06
#epoch  48    val-loss:  3.804361509649377  train-loss:  2.2061054327059537  lr:  1.220703125e-06
#epoch  49    val-loss:  3.8461530051733317  train-loss:  2.193412560969591  lr:  1.220703125e-06
#epoch  50    val-loss:  4.07131845072696  train-loss:  2.1854834859259427  lr:  6.103515625e-07
#epoch  51    val-loss:  4.023967228437725  train-loss:  2.206122793373652  lr:  6.103515625e-07
#epoch  52    val-loss:  4.132539322501735  train-loss:  2.203363668639213  lr:  6.103515625e-07
#epoch  53    val-loss:  4.037841821971693  train-loss:  2.2048251987434924  lr:  3.0517578125e-07
#epoch  54    val-loss:  3.8779204393688  train-loss:  2.2064533795928583  lr:  3.0517578125e-07
#epoch  55    val-loss:  3.9852383513199654  train-loss:  2.203282924834639  lr:  3.0517578125e-07
#epoch  56    val-loss:  4.079504668712616  train-loss:  2.1945218481123447  lr:  1.52587890625e-07
#epoch  57    val-loss:  4.166912078857422  train-loss:  2.2032160917297006  lr:  1.52587890625e-07
#epoch  58    val-loss:  4.073616598781786  train-loss:  2.2005205259192735  lr:  1.52587890625e-07
#epoch  59    val-loss:  3.955747604370117  train-loss:  2.1973138214088976  lr:  7.62939453125e-08
#epoch  60    val-loss:  3.9551116667295756  train-loss:  2.208435738226399  lr:  7.62939453125e-08
#epoch  61    val-loss:  4.029856104599802  train-loss:  2.186050981283188  lr:  7.62939453125e-08
#epoch  62    val-loss:  3.9444809901086906  train-loss:  2.198077807435766  lr:  3.814697265625e-08
#epoch  63    val-loss:  4.101334069904528  train-loss:  2.2029485979583114  lr:  3.814697265625e-08
#epoch  64    val-loss:  4.049421122199611  train-loss:  2.2024796814657748  lr:  3.814697265625e-08
#epoch  65    val-loss:  4.1005806483720475  train-loss:  2.201997095486149  lr:  1.9073486328125e-08
#epoch  66    val-loss:  4.054928026701274  train-loss:  2.2040069079957902  lr:  1.9073486328125e-08
#epoch  67    val-loss:  3.9573958045557927  train-loss:  2.1970045620109886  lr:  1.9073486328125e-08
#epoch  68    val-loss:  3.980564129979987  train-loss:  2.205714588519186  lr:  1.9073486328125e-08
#epoch  69    val-loss:  4.0182989145580095  train-loss:  2.2076073116622865  lr:  1.9073486328125e-08
#epoch  70    val-loss:  4.133614251488133  train-loss:  2.194798598298803  lr:  1.9073486328125e-08
#epoch  71    val-loss:  3.9649359427000346  train-loss:  2.18568465084536  lr:  1.9073486328125e-08
#epoch  72    val-loss:  4.014207149806776  train-loss:  2.2052634772844613  lr:  1.9073486328125e-08
#epoch  73    val-loss:  3.9994192876313863  train-loss:  2.2113258733879775  lr:  1.9073486328125e-08
#epoch  74    val-loss:  3.9278652667999268  train-loss:  2.194267898099497  lr:  1.9073486328125e-08
#epoch  75    val-loss:  3.793783840380217  train-loss:  2.196764770662412  lr:  1.9073486328125e-08
#epoch  76    val-loss:  4.102736761695461  train-loss:  2.1973833742085844  lr:  1.9073486328125e-08
#epoch  77    val-loss:  4.026704637627852  train-loss:  2.1880801278166473  lr:  1.9073486328125e-08
#epoch  78    val-loss:  4.059562419590197  train-loss:  2.2043766765855253  lr:  1.9073486328125e-08
#epoch  79    val-loss:  3.8971026822140344  train-loss:  2.2001361020375043  lr:  1.9073486328125e-08
#epoch  80    val-loss:  3.941207823000456  train-loss:  2.199101700389292  lr:  1.9073486328125e-08
#epoch  81    val-loss:  4.026196454700671  train-loss:  2.1996170612983406  lr:  1.9073486328125e-08
#epoch  82    val-loss:  3.9079112708568573  train-loss:  2.1982368738390505  lr:  1.9073486328125e-08
#epoch  83    val-loss:  3.9543932990023962  train-loss:  2.2132341088727117  lr:  1.9073486328125e-08
#epoch  84    val-loss:  3.890795820637753  train-loss:  2.1880440476816148  lr:  1.9073486328125e-08
#epoch  85    val-loss:  4.018373370170593  train-loss:  2.211124027147889  lr:  1.9073486328125e-08
#epoch  86    val-loss:  3.900760945520903  train-loss:  2.206185817718506  lr:  1.9073486328125e-08
#epoch  87    val-loss:  3.9298365116119385  train-loss:  2.190825119847432  lr:  1.9073486328125e-08
#epoch  88    val-loss:  3.937584249596847  train-loss:  2.196127623785287  lr:  1.9073486328125e-08
#epoch  89    val-loss:  4.060312377779107  train-loss:  2.1925472426228225  lr:  1.9073486328125e-08
#epoch  90    val-loss:  4.00225013180783  train-loss:  2.1925722071900964  lr:  1.9073486328125e-08
#epoch  91    val-loss:  4.0515271864439315  train-loss:  2.207339620683342  lr:  1.9073486328125e-08
#epoch  92    val-loss:  3.931653280007212  train-loss:  2.192173998686485  lr:  1.9073486328125e-08
#epoch  93    val-loss:  3.894646324609455  train-loss:  2.2009792821481824  lr:  1.9073486328125e-08
#epoch  94    val-loss:  3.9313135649028577  train-loss:  2.1828975297976285  lr:  1.9073486328125e-08
#epoch  95    val-loss:  3.9737078014173006  train-loss:  2.21388415270485  lr:  1.9073486328125e-08
#epoch  96    val-loss:  4.143922492077476  train-loss:  2.207546146120876  lr:  1.9073486328125e-08
#epoch  97    val-loss:  3.9195803592079566  train-loss:  2.2096011468674988  lr:  1.9073486328125e-08
#epoch  98    val-loss:  3.96476537930338  train-loss:  2.1904627014882863  lr:  1.9073486328125e-08
#epoch  99    val-loss:  3.8831714454450106  train-loss:  2.2029980767983943  lr:  1.9073486328125e-08
