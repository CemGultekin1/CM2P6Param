model 576a6160a1cd1644c4bc5b4a2b311b9fd757208166efe586c2ebc70f state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=1497.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  2.961265250256187  train-loss:  2.5842309136351105  lr:  0.005
#epoch  8    val-loss:  2.963232241178814  train-loss:  2.5851960272993892  lr:  0.0025
#epoch  9    val-loss:  2.884648222672312  train-loss:  2.5929732681252062  lr:  0.0025
#epoch  10    val-loss:  2.855692556029872  train-loss:  2.5958382359240204  lr:  0.0025
#epoch  11    val-loss:  2.9143820185410347  train-loss:  2.5979768957477063  lr:  0.0025
#epoch  12    val-loss:  2.864459520892093  train-loss:  2.5900607074145228  lr:  0.0025
#epoch  13    val-loss:  2.8943741447047184  train-loss:  2.58657972374931  lr:  0.0025
#epoch  14    val-loss:  2.937893553784019  train-loss:  2.5886231776094064  lr:  0.00125
#epoch  15    val-loss:  2.9816063705243563  train-loss:  2.5872652961406857  lr:  0.00125
#epoch  16    val-loss:  2.877049540218554  train-loss:  2.5775981741026044  lr:  0.00125
#epoch  17    val-loss:  2.904651459894682  train-loss:  2.594204539898783  lr:  0.000625
#epoch  18    val-loss:  2.9406202153155676  train-loss:  2.5811825091950595  lr:  0.000625
#epoch  19    val-loss:  2.9742969337262606  train-loss:  2.5860130642540753  lr:  0.000625
#epoch  20    val-loss:  2.947198459976598  train-loss:  2.591838469495997  lr:  0.0003125
#epoch  21    val-loss:  2.7954706094766917  train-loss:  2.57537994714221  lr:  0.0003125
#epoch  22    val-loss:  2.9464454588137174  train-loss:  2.57455242308788  lr:  0.0003125
#epoch  23    val-loss:  2.9610661581942908  train-loss:  2.5847091586329043  lr:  0.0003125
#epoch  24    val-loss:  2.8979615475002087  train-loss:  2.5775121503975242  lr:  0.0003125
#epoch  25    val-loss:  2.9607070747174715  train-loss:  2.5835748384706676  lr:  0.00015625
#epoch  26    val-loss:  2.8678821952719438  train-loss:  2.5773357504513115  lr:  0.00015625
#epoch  27    val-loss:  2.8833988904953003  train-loss:  2.582960330415517  lr:  0.00015625
#epoch  28    val-loss:  2.924071876626266  train-loss:  2.594350075814873  lr:  7.8125e-05
#epoch  29    val-loss:  2.911118482288561  train-loss:  2.5814848244190216  lr:  7.8125e-05
#epoch  30    val-loss:  2.9389229197251168  train-loss:  2.570764665491879  lr:  7.8125e-05
#epoch  31    val-loss:  2.9859863770635506  train-loss:  2.5899276663549244  lr:  3.90625e-05
#epoch  32    val-loss:  2.9047975602902865  train-loss:  2.577472485601902  lr:  3.90625e-05
#epoch  33    val-loss:  2.854116395900124  train-loss:  2.591949841938913  lr:  3.90625e-05
#epoch  34    val-loss:  2.92269776369396  train-loss:  2.578376810066402  lr:  1.953125e-05
#epoch  35    val-loss:  2.9078826904296875  train-loss:  2.585634683724493  lr:  1.953125e-05
#epoch  36    val-loss:  2.9024850945723686  train-loss:  2.5822252274956554  lr:  1.953125e-05
#epoch  37    val-loss:  3.032012701034546  train-loss:  2.5849564969539642  lr:  9.765625e-06
#epoch  38    val-loss:  2.9299938615999723  train-loss:  2.578205401543528  lr:  9.765625e-06
#epoch  39    val-loss:  2.8767249772423193  train-loss:  2.581023928243667  lr:  9.765625e-06
#epoch  40    val-loss:  2.935903975838109  train-loss:  2.5758003511000425  lr:  4.8828125e-06
#epoch  41    val-loss:  2.8949313979399833  train-loss:  2.588919220957905  lr:  4.8828125e-06
#epoch  42    val-loss:  2.9512746585042855  train-loss:  2.578853110782802  lr:  4.8828125e-06
#epoch  43    val-loss:  2.793649715028311  train-loss:  2.5820412104949355  lr:  2.44140625e-06
#epoch  44    val-loss:  2.9469265435871326  train-loss:  2.586436543148011  lr:  2.44140625e-06
#epoch  45    val-loss:  2.8675240215502287  train-loss:  2.576615792699158  lr:  2.44140625e-06
#epoch  46    val-loss:  2.9302093229795756  train-loss:  2.580094705801457  lr:  2.44140625e-06
#epoch  47    val-loss:  2.962691852920934  train-loss:  2.5795542267151177  lr:  1.220703125e-06
#epoch  48    val-loss:  2.796239782320826  train-loss:  2.5848835124634206  lr:  1.220703125e-06
#epoch  49    val-loss:  2.836776404004348  train-loss:  2.577476345235482  lr:  1.220703125e-06
#epoch  50    val-loss:  2.966740614489505  train-loss:  2.563181586796418  lr:  6.103515625e-07
#epoch  51    val-loss:  2.9809483979877673  train-loss:  2.589085476240143  lr:  6.103515625e-07
#epoch  52    val-loss:  2.9876650132631  train-loss:  2.5828518848866224  lr:  6.103515625e-07
#epoch  53    val-loss:  2.953123971035606  train-loss:  2.584447510074824  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.8620958579213998  train-loss:  2.587954287417233  lr:  3.0517578125e-07
#epoch  55    val-loss:  2.9111075809127405  train-loss:  2.5844784630462527  lr:  3.0517578125e-07
#epoch  56    val-loss:  2.9953466967532507  train-loss:  2.5713335608597845  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.9956665290029427  train-loss:  2.577668675221503  lr:  1.52587890625e-07
#epoch  58    val-loss:  2.973099137607374  train-loss:  2.5811244160868227  lr:  1.52587890625e-07
#epoch  59    val-loss:  2.8945355227119043  train-loss:  2.5794738947879523  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.947829949228387  train-loss:  2.588242714293301  lr:  7.62939453125e-08
#epoch  61    val-loss:  2.9531827412153544  train-loss:  2.5630436211358756  lr:  7.62939453125e-08
#epoch  62    val-loss:  2.9186048131240043  train-loss:  2.572697268333286  lr:  3.814697265625e-08
#epoch  63    val-loss:  3.0044606798573543  train-loss:  2.5843398575671017  lr:  3.814697265625e-08
#epoch  64    val-loss:  3.0286673370160555  train-loss:  2.582249973434955  lr:  3.814697265625e-08
#epoch  65    val-loss:  3.008176333025882  train-loss:  2.583036389667541  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.9873278893922506  train-loss:  2.5824148603715003  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.917699224070499  train-loss:  2.577036015689373  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.9456718595404374  train-loss:  2.582545707235113  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.954015518489637  train-loss:  2.583764902781695  lr:  1.9073486328125e-08
#epoch  70    val-loss:  3.0017613486239783  train-loss:  2.574612698284909  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.9447021484375  train-loss:  2.5622479201410897  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.954957033458509  train-loss:  2.5863198316656053  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.9509931865491366  train-loss:  2.5895275617949665  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.8891047553012243  train-loss:  2.570549488067627  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.863459066340798  train-loss:  2.5786694353446364  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.959513450923719  train-loss:  2.5748948126565665  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.9620134391282735  train-loss:  2.5659114201553166  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.937896916740819  train-loss:  2.5812744703143835  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.873454031191374  train-loss:  2.579908057115972  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.9511775092074743  train-loss:  2.57828767655883  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.9533362765061226  train-loss:  2.5786309603136033  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.8450327327376916  train-loss:  2.5839761418756098  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.96432007613935  train-loss:  2.589338839519769  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.8747418554205644  train-loss:  2.566849468741566  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.939720812596773  train-loss:  2.5905357166193426  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.9014009488256356  train-loss:  2.5835441411472857  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.886573647197924  train-loss:  2.5704131945967674  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.9252058769527234  train-loss:  2.5756651083938777  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.9888505496476827  train-loss:  2.566730999853462  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.960008972569516  train-loss:  2.573064432712272  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.962763742396706  train-loss:  2.589492589700967  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.89952510281613  train-loss:  2.5698198087047786  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.92799342933454  train-loss:  2.580541511066258  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.932167567704853  train-loss:  2.5602408926934004  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.9200640163923564  train-loss:  2.590304776094854  lr:  1.9073486328125e-08
#epoch  96    val-loss:  3.027228744406449  train-loss:  2.587883203290403  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.894390244232981  train-loss:  2.5885079079307616  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.9019672400073  train-loss:  2.5719736458268017  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.8759281635284424  train-loss:  2.585004035383463  lr:  1.9073486328125e-08
