model e894ac54c5c2e5600ad338d37d112ceb01f9c618309d579252dd3f0e state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=5.0, disp=1, domain='global', epoch=8, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  8    val-loss:  2.3841918016734875  train-loss:  2.2834388138726354  lr:  0.0025
#epoch  9    val-loss:  2.3773242297925448  train-loss:  2.2773398705758154  lr:  0.0025
#epoch  10    val-loss:  2.399501869553014  train-loss:  2.2728026318363845  lr:  0.00125
#epoch  11    val-loss:  2.4504084022421586  train-loss:  2.272387396544218  lr:  0.00125
#epoch  12    val-loss:  2.3978989563490214  train-loss:  2.271604629699141  lr:  0.00125
#epoch  13    val-loss:  2.395560471635116  train-loss:  2.2720875632949173  lr:  0.000625
#epoch  14    val-loss:  2.429767363949826  train-loss:  2.272592168767005  lr:  0.000625
#epoch  15    val-loss:  2.3499864088861564  train-loss:  2.2729956787079573  lr:  0.000625
#epoch  16    val-loss:  2.3435626720127307  train-loss:  2.2694730972871184  lr:  0.000625
#epoch  17    val-loss:  2.4423062048460307  train-loss:  2.279461276717484  lr:  0.000625
#epoch  18    val-loss:  2.4282876817803634  train-loss:  2.277599297929555  lr:  0.000625
#epoch  19    val-loss:  2.3916820664154854  train-loss:  2.279953595250845  lr:  0.000625
#epoch  20    val-loss:  2.3458932700910067  train-loss:  2.273103856947273  lr:  0.0003125
#epoch  21    val-loss:  2.3631186171581873  train-loss:  2.2661662194877863  lr:  0.0003125
#epoch  22    val-loss:  2.4076321313255713  train-loss:  2.2731175827793777  lr:  0.0003125
#epoch  23    val-loss:  2.337566005556207  train-loss:  2.271876408252865  lr:  0.00015625
#epoch  24    val-loss:  2.3796298315650537  train-loss:  2.269594434183091  lr:  0.00015625
#epoch  25    val-loss:  2.438883160289965  train-loss:  2.2680918211117387  lr:  0.00015625
#epoch  26    val-loss:  2.389902397205955  train-loss:  2.27020859811455  lr:  0.00015625
#epoch  27    val-loss:  2.453925553121065  train-loss:  2.270207041874528  lr:  7.8125e-05
#epoch  28    val-loss:  2.42556105161968  train-loss:  2.263679479714483  lr:  7.8125e-05
#epoch  29    val-loss:  2.3904373959491125  train-loss:  2.2663560076616704  lr:  7.8125e-05
#epoch  30    val-loss:  2.4014422266106856  train-loss:  2.2704913737252355  lr:  3.90625e-05
#epoch  31    val-loss:  2.3639699095173885  train-loss:  2.27338190888986  lr:  3.90625e-05
#epoch  32    val-loss:  2.3794688300082556  train-loss:  2.2670208369381726  lr:  3.90625e-05
#epoch  33    val-loss:  2.439936399459839  train-loss:  2.273186247330159  lr:  1.953125e-05
#epoch  34    val-loss:  2.437705008607162  train-loss:  2.264109038282186  lr:  1.953125e-05
#epoch  35    val-loss:  2.3836068165929696  train-loss:  2.2735591139644384  lr:  1.953125e-05
#epoch  36    val-loss:  2.3912782418100456  train-loss:  2.268794715870172  lr:  9.765625e-06
#epoch  37    val-loss:  2.439956213298597  train-loss:  2.269162531942129  lr:  9.765625e-06
#epoch  38    val-loss:  2.357274839752599  train-loss:  2.2722746916115284  lr:  9.765625e-06
#epoch  39    val-loss:  2.399456676683928  train-loss:  2.2712403549812734  lr:  4.8828125e-06
#epoch  40    val-loss:  2.417400749106156  train-loss:  2.268472001887858  lr:  4.8828125e-06
#epoch  41    val-loss:  2.3903322847265946  train-loss:  2.273494146298617  lr:  4.8828125e-06
#epoch  42    val-loss:  2.3670133854213513  train-loss:  2.2672348623164  lr:  2.44140625e-06
#epoch  43    val-loss:  2.3692597840961658  train-loss:  2.2682771445252  lr:  2.44140625e-06
#epoch  44    val-loss:  2.400981677205939  train-loss:  2.266849983949214  lr:  2.44140625e-06
#epoch  45    val-loss:  2.386441864465412  train-loss:  2.2640298870392144  lr:  1.220703125e-06
#epoch  46    val-loss:  2.395873935599076  train-loss:  2.266999759245664  lr:  1.220703125e-06
#epoch  47    val-loss:  2.351968062551398  train-loss:  2.2672048239037395  lr:  1.220703125e-06
#epoch  48    val-loss:  2.383051721673263  train-loss:  2.268104725982994  lr:  6.103515625e-07
#epoch  49    val-loss:  2.453863739967346  train-loss:  2.270699928048998  lr:  6.103515625e-07
#epoch  50    val-loss:  2.429580619460658  train-loss:  2.275697168894112  lr:  6.103515625e-07
#epoch  51    val-loss:  2.346203038566991  train-loss:  2.281876797787845  lr:  3.0517578125e-07
#epoch  52    val-loss:  2.3621157156793693  train-loss:  2.264860862866044  lr:  3.0517578125e-07
#epoch  53    val-loss:  2.358441101877313  train-loss:  2.2687094346620142  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.3519512163965324  train-loss:  2.2683535646647215  lr:  1.52587890625e-07
#epoch  55    val-loss:  2.4407947879088554  train-loss:  2.2676803078502417  lr:  1.52587890625e-07
#epoch  56    val-loss:  2.4018688076420833  train-loss:  2.2646257118321955  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.3696277141571045  train-loss:  2.2718948773108423  lr:  7.62939453125e-08
#epoch  58    val-loss:  2.359296754786843  train-loss:  2.2619532984681427  lr:  7.62939453125e-08
#epoch  59    val-loss:  2.3989028115021553  train-loss:  2.2745052855461836  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.3770333528518677  train-loss:  2.2738464176654816  lr:  3.814697265625e-08
#epoch  61    val-loss:  2.3963190442637394  train-loss:  2.264184882864356  lr:  3.814697265625e-08
#epoch  62    val-loss:  2.3775885858033834  train-loss:  2.2792506949044764  lr:  3.814697265625e-08
#epoch  63    val-loss:  2.3859476541218005  train-loss:  2.2713498384691775  lr:  1.9073486328125e-08
#epoch  64    val-loss:  2.3995825554195203  train-loss:  2.2668221811763942  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.3599656883038973  train-loss:  2.2692287690006196  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.3540217813692594  train-loss:  2.271000103559345  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.3654118588096216  train-loss:  2.273096247576177  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.37583589553833  train-loss:  2.2746922383084893  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.3754076393027055  train-loss:  2.270941426977515  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.3990657768751444  train-loss:  2.2652102918364108  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.364773524434943  train-loss:  2.274711879901588  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.418508667694895  train-loss:  2.279147784691304  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.380729499616121  train-loss:  2.2676494726911187  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.3815537000957288  train-loss:  2.2673293659463525  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.3954900816867224  train-loss:  2.27532153762877  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.4567653819134363  train-loss:  2.278708089143038  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.3435302596343193  train-loss:  2.2713992465287447  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.3819722376371684  train-loss:  2.278021091595292  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.338187474953501  train-loss:  2.268894170410931  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.453091966478448  train-loss:  2.2676569344475865  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.397186122442547  train-loss:  2.2717220624908805  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.3745021192651046  train-loss:  2.274627080652863  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.4158491335417094  train-loss:  2.271200304850936  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.3706167936325073  train-loss:  2.263226307928562  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.4261017096670052  train-loss:  2.2781056398525834  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.3710031634882878  train-loss:  2.2680692956782877  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.4136941119244226  train-loss:  2.266453509684652  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.394501911966424  train-loss:  2.27562621422112  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.3987267644781816  train-loss:  2.2728593489155173  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.3780300617218018  train-loss:  2.277231752872467  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.377560483781915  train-loss:  2.2698443778790534  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.358607718819066  train-loss:  2.2689989004284143  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.4238623694369665  train-loss:  2.272558195516467  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.4744324119467485  train-loss:  2.2634717035107315  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.395731668723257  train-loss:  2.278052375651896  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.3821448213175724  train-loss:  2.2721585775725543  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.357100166772541  train-loss:  2.2693102657794952  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.387835515172858  train-loss:  2.2665706798434258  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.364530877063149  train-loss:  2.2684136033058167  lr:  1.9073486328125e-08
