model ecc550747c685ad71460a2157c636f5bfc062b152ea9dbe064ff7f13 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=181.0, disp=1, domain='global', epoch=6, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  6    val-loss:  2.495660769312005  train-loss:  2.252332840580493  lr:  0.005
#epoch  7    val-loss:  2.454109731473421  train-loss:  2.245799895375967  lr:  0.0025
#epoch  8    val-loss:  2.4430968886927555  train-loss:  2.2484528734348714  lr:  0.0025
#epoch  9    val-loss:  2.5252700228440133  train-loss:  2.2415093490853906  lr:  0.0025
#epoch  10    val-loss:  2.482389450073242  train-loss:  2.245071603450924  lr:  0.00125
#epoch  11    val-loss:  2.449263008017289  train-loss:  2.242871878668666  lr:  0.00125
#epoch  12    val-loss:  2.4885284461473165  train-loss:  2.2398615404963493  lr:  0.00125
#epoch  13    val-loss:  2.439065356003611  train-loss:  2.24172853725031  lr:  0.000625
#epoch  14    val-loss:  2.436665315377085  train-loss:  2.2389717884361744  lr:  0.000625
#epoch  15    val-loss:  2.4438325856861316  train-loss:  2.2441948517225683  lr:  0.000625
#epoch  16    val-loss:  2.486093546214857  train-loss:  2.246218075044453  lr:  0.000625
#epoch  17    val-loss:  2.459212579225239  train-loss:  2.250431417953223  lr:  0.000625
#epoch  18    val-loss:  2.43794584274292  train-loss:  2.240478432737291  lr:  0.0003125
#epoch  19    val-loss:  2.42557682489094  train-loss:  2.239032353274524  lr:  0.0003125
#epoch  20    val-loss:  2.413084808148836  train-loss:  2.238708554301411  lr:  0.0003125
#epoch  21    val-loss:  2.421500852233485  train-loss:  2.23831989755854  lr:  0.0003125
#epoch  22    val-loss:  2.4542174088327506  train-loss:  2.2381529407575727  lr:  0.0003125
#epoch  23    val-loss:  2.4974811830018697  train-loss:  2.235106297302991  lr:  0.0003125
#epoch  24    val-loss:  2.4777097952993294  train-loss:  2.240193925332278  lr:  0.00015625
#epoch  25    val-loss:  2.445489532069156  train-loss:  2.2389301755465567  lr:  0.00015625
#epoch  26    val-loss:  2.4615764241469535  train-loss:  2.239321966189891  lr:  0.00015625
#epoch  27    val-loss:  2.482142109619944  train-loss:  2.2405361742712557  lr:  7.8125e-05
#epoch  28    val-loss:  2.517491478669016  train-loss:  2.2374878195114434  lr:  7.8125e-05
#epoch  29    val-loss:  2.4486350134799353  train-loss:  2.238174278754741  lr:  7.8125e-05
#epoch  30    val-loss:  2.4804574502141854  train-loss:  2.2443071538582444  lr:  3.90625e-05
#epoch  31    val-loss:  2.474430197163632  train-loss:  2.239976247306913  lr:  3.90625e-05
#epoch  32    val-loss:  2.511704526449505  train-loss:  2.2402048469521105  lr:  3.90625e-05
#epoch  33    val-loss:  2.4311147928237915  train-loss:  2.242482202127576  lr:  1.953125e-05
#epoch  34    val-loss:  2.4837650876296196  train-loss:  2.239542295690626  lr:  1.953125e-05
#epoch  35    val-loss:  2.517515546397159  train-loss:  2.2435891088098288  lr:  1.953125e-05
#epoch  36    val-loss:  2.476765507145932  train-loss:  2.23957843426615  lr:  9.765625e-06
#epoch  37    val-loss:  2.495809404473556  train-loss:  2.240296117030084  lr:  9.765625e-06
#epoch  38    val-loss:  2.4550155338488127  train-loss:  2.238444298505783  lr:  9.765625e-06
#epoch  39    val-loss:  2.430801849616201  train-loss:  2.237573731224984  lr:  4.8828125e-06
#epoch  40    val-loss:  2.416032226462113  train-loss:  2.2386814155615866  lr:  4.8828125e-06
#epoch  41    val-loss:  2.462623878529197  train-loss:  2.2405726625584066  lr:  4.8828125e-06
#epoch  42    val-loss:  2.4096425269779407  train-loss:  2.2425247225910425  lr:  2.44140625e-06
#epoch  43    val-loss:  2.444575127802397  train-loss:  2.2365515865385532  lr:  2.44140625e-06
#epoch  44    val-loss:  2.468500011845639  train-loss:  2.240213794168085  lr:  2.44140625e-06
#epoch  45    val-loss:  2.4307099643506502  train-loss:  2.237878073938191  lr:  2.44140625e-06
#epoch  46    val-loss:  2.469493602451525  train-loss:  2.240657400339842  lr:  1.220703125e-06
#epoch  47    val-loss:  2.4297623320629724  train-loss:  2.24258684925735  lr:  1.220703125e-06
#epoch  48    val-loss:  2.481244400927895  train-loss:  2.2438934044912457  lr:  1.220703125e-06
#epoch  49    val-loss:  2.438431695887917  train-loss:  2.248835936654359  lr:  6.103515625e-07
#epoch  50    val-loss:  2.4442568327251233  train-loss:  2.234378003515303  lr:  6.103515625e-07
#epoch  51    val-loss:  2.45587914868405  train-loss:  2.243814442306757  lr:  6.103515625e-07
#epoch  52    val-loss:  2.431759188049718  train-loss:  2.2399426554329693  lr:  3.0517578125e-07
#epoch  53    val-loss:  2.48438444890474  train-loss:  2.248703661840409  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.446701212933189  train-loss:  2.2334039099514484  lr:  3.0517578125e-07
#epoch  55    val-loss:  2.444855363745438  train-loss:  2.2444709204137325  lr:  1.52587890625e-07
#epoch  56    val-loss:  2.455892198964169  train-loss:  2.2332853875122964  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.482487527947677  train-loss:  2.247232446912676  lr:  1.52587890625e-07
#epoch  58    val-loss:  2.458190836404499  train-loss:  2.2437396654859185  lr:  7.62939453125e-08
#epoch  59    val-loss:  2.4758258744290003  train-loss:  2.238333687186241  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.4645842251024748  train-loss:  2.2423848966136575  lr:  7.62939453125e-08
#epoch  61    val-loss:  2.4414542787953426  train-loss:  2.241601855494082  lr:  3.814697265625e-08
#epoch  62    val-loss:  2.466017058021144  train-loss:  2.237842690665275  lr:  3.814697265625e-08
#epoch  63    val-loss:  2.4592308370690596  train-loss:  2.2411203267984092  lr:  3.814697265625e-08
#epoch  64    val-loss:  2.459235530150564  train-loss:  2.241915979422629  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.4492429181149133  train-loss:  2.243774052709341  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.4466206023567603  train-loss:  2.2400000523775816  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.4784587935397497  train-loss:  2.237383211031556  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.48725562346609  train-loss:  2.238240052945912  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.4772691099267257  train-loss:  2.2496854085475206  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.4953907414486536  train-loss:  2.238159674219787  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.4545407169743587  train-loss:  2.2400217982940376  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.4545880430623104  train-loss:  2.2439183527603745  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.439761475512856  train-loss:  2.2447489160113037  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.5098317547848352  train-loss:  2.249436615034938  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.4305297136306763  train-loss:  2.235155343078077  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.451482157958181  train-loss:  2.2426917371340096  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.4080975432144966  train-loss:  2.2414740407839417  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.5083995241867867  train-loss:  2.239137904252857  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.451087562661422  train-loss:  2.2382561550475657  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.481419475455033  train-loss:  2.2438060170970857  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.409557191949142  train-loss:  2.24447241704911  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.4508297443389893  train-loss:  2.241724415216595  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.479904814770347  train-loss:  2.24332318501547  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.456843275772898  train-loss:  2.2420811862684786  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.451145523472836  train-loss:  2.2379954596981406  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.462180401149549  train-loss:  2.242346156388521  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.4679941880075553  train-loss:  2.241220877505839  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.47821139034472  train-loss:  2.2422033534385264  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.499249106959293  train-loss:  2.2414566949009895  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.4540320321133264  train-loss:  2.2454515509307384  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.481664218400654  train-loss:  2.240507778711617  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.5516452412856254  train-loss:  2.2338225902058184  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.4693777686671208  train-loss:  2.235171547625214  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.4510471193413985  train-loss:  2.248872261028737  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.45976816353045  train-loss:  2.2421594480983913  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.424827901940597  train-loss:  2.241317114327103  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.4251860756623116  train-loss:  2.235264734365046  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.449740165158322  train-loss:  2.2397479745559394  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.4369270613318994  train-loss:  2.2431891849264503  lr:  1.9073486328125e-08
