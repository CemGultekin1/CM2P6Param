model 603a384860eb57b6267f4a39906a491a4b516378e86b91e3349e9709 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=330.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  2.3732964804298  train-loss:  2.1651012394577265  lr:  0.0025
#epoch  8    val-loss:  2.3396726282019364  train-loss:  2.1620465186424553  lr:  0.0025
#epoch  9    val-loss:  2.3487100224745903  train-loss:  2.159591132774949  lr:  0.00125
#epoch  10    val-loss:  2.3846783010583175  train-loss:  2.158828896936029  lr:  0.00125
#epoch  11    val-loss:  2.360949340619539  train-loss:  2.1616954528726637  lr:  0.00125
#epoch  12    val-loss:  2.363975066887705  train-loss:  2.158018567133695  lr:  0.000625
#epoch  13    val-loss:  2.3669699116757044  train-loss:  2.159843015950173  lr:  0.000625
#epoch  14    val-loss:  2.3437794133236536  train-loss:  2.156875287182629  lr:  0.000625
#epoch  15    val-loss:  2.3218082126818205  train-loss:  2.1547269257716835  lr:  0.0003125
#epoch  16    val-loss:  2.3578561105226217  train-loss:  2.1615718086250126  lr:  0.0003125
#epoch  17    val-loss:  2.3646303603523657  train-loss:  2.163021055981517  lr:  0.0003125
#epoch  18    val-loss:  2.344804004618996  train-loss:  2.1651598098687828  lr:  0.0003125
#epoch  19    val-loss:  2.332330904508892  train-loss:  2.156979377847165  lr:  0.00015625
#epoch  20    val-loss:  2.32965087890625  train-loss:  2.156214801594615  lr:  0.00015625
#epoch  21    val-loss:  2.3480604824266935  train-loss:  2.1555654192343354  lr:  0.00015625
#epoch  22    val-loss:  2.315540859573766  train-loss:  2.1555414483882487  lr:  7.8125e-05
#epoch  23    val-loss:  2.335628816955968  train-loss:  2.157198732253164  lr:  7.8125e-05
#epoch  24    val-loss:  2.374834575151142  train-loss:  2.154765679035336  lr:  7.8125e-05
#epoch  25    val-loss:  2.3478993240155672  train-loss:  2.1544710425660014  lr:  7.8125e-05
#epoch  26    val-loss:  2.3642355517337195  train-loss:  2.1557044931687415  lr:  3.90625e-05
#epoch  27    val-loss:  2.3443612173983923  train-loss:  2.1561555904336274  lr:  3.90625e-05
#epoch  28    val-loss:  2.358920511446501  train-loss:  2.156748336274177  lr:  3.90625e-05
#epoch  29    val-loss:  2.3875330184635364  train-loss:  2.153911589179188  lr:  1.953125e-05
#epoch  30    val-loss:  2.3364183212581433  train-loss:  2.1528744883835316  lr:  1.953125e-05
#epoch  31    val-loss:  2.3424620251906547  train-loss:  2.158426033332944  lr:  1.953125e-05
#epoch  32    val-loss:  2.351694621537861  train-loss:  2.158466863911599  lr:  9.765625e-06
#epoch  33    val-loss:  2.38063130880657  train-loss:  2.1589561286382377  lr:  9.765625e-06
#epoch  34    val-loss:  2.328574958600496  train-loss:  2.1597951957955956  lr:  9.765625e-06
#epoch  35    val-loss:  2.385017520502994  train-loss:  2.1562769804149866  lr:  4.8828125e-06
#epoch  36    val-loss:  2.3797441156286943  train-loss:  2.1619670526124537  lr:  4.8828125e-06
#epoch  37    val-loss:  2.3380648399654187  train-loss:  2.1551204351708293  lr:  4.8828125e-06
#epoch  38    val-loss:  2.383519825182463  train-loss:  2.157982429023832  lr:  2.44140625e-06
#epoch  39    val-loss:  2.341688394546509  train-loss:  2.1539196502417326  lr:  2.44140625e-06
#epoch  40    val-loss:  2.3337308984053764  train-loss:  2.155041351914406  lr:  2.44140625e-06
#epoch  41    val-loss:  2.3209188549142135  train-loss:  2.1562477657571435  lr:  1.220703125e-06
#epoch  42    val-loss:  2.340933448389957  train-loss:  2.1591346990317106  lr:  1.220703125e-06
#epoch  43    val-loss:  2.323561580557572  train-loss:  2.1586398109793663  lr:  1.220703125e-06
#epoch  44    val-loss:  2.3280072588669625  train-loss:  2.1537763006053865  lr:  6.103515625e-07
#epoch  45    val-loss:  2.374483999453093  train-loss:  2.1572432187385857  lr:  6.103515625e-07
#epoch  46    val-loss:  2.3354204077469674  train-loss:  2.1553594721481204  lr:  6.103515625e-07
#epoch  47    val-loss:  2.3525666186684058  train-loss:  2.1595314713194966  lr:  3.0517578125e-07
#epoch  48    val-loss:  2.3651449052911055  train-loss:  2.1592876673676074  lr:  3.0517578125e-07
#epoch  49    val-loss:  2.377155303955078  train-loss:  2.158381890039891  lr:  3.0517578125e-07
#epoch  50    val-loss:  2.331859036495811  train-loss:  2.163213662803173  lr:  1.52587890625e-07
#epoch  51    val-loss:  2.3309014094503304  train-loss:  2.152991531882435  lr:  1.52587890625e-07
#epoch  52    val-loss:  2.3431440654553866  train-loss:  2.1595657002180815  lr:  1.52587890625e-07
#epoch  53    val-loss:  2.333096667339927  train-loss:  2.1578635815531015  lr:  7.62939453125e-08
#epoch  54    val-loss:  2.379459908134059  train-loss:  2.1617453922517598  lr:  7.62939453125e-08
#epoch  55    val-loss:  2.3402929807964123  train-loss:  2.153647660743445  lr:  7.62939453125e-08
#epoch  56    val-loss:  2.3334195739344548  train-loss:  2.1610569334588945  lr:  3.814697265625e-08
#epoch  57    val-loss:  2.348941922187805  train-loss:  2.152897006366402  lr:  3.814697265625e-08
#epoch  58    val-loss:  2.3701792641689905  train-loss:  2.165595223661512  lr:  3.814697265625e-08
#epoch  59    val-loss:  2.3656606674194336  train-loss:  2.1617482039146125  lr:  1.9073486328125e-08
#epoch  60    val-loss:  2.3515693074778508  train-loss:  2.1546742781065404  lr:  1.9073486328125e-08
#epoch  61    val-loss:  2.330363436749107  train-loss:  2.158812343608588  lr:  1.9073486328125e-08
#epoch  62    val-loss:  2.3437163829803467  train-loss:  2.157275780569762  lr:  1.9073486328125e-08
#epoch  63    val-loss:  2.3756948207554065  train-loss:  2.155726883094758  lr:  1.9073486328125e-08
#epoch  64    val-loss:  2.346483356074283  train-loss:  2.1565057798288763  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.333159095362613  train-loss:  2.159442467149347  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.3409338624853837  train-loss:  2.1613923157565296  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.3505444652155827  train-loss:  2.1571477721445262  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.3602566781796908  train-loss:  2.1557998629286885  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.3615993951496326  train-loss:  2.1569558437913656  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.355652169177407  train-loss:  2.164345775730908  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.365951098893818  train-loss:  2.1573613127693534  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.334749372381913  train-loss:  2.1561498451046646  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.3406103535702356  train-loss:  2.160210970789194  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.3391133044895374  train-loss:  2.1602685539983213  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.370673606270238  train-loss:  2.1614354699850082  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.325709474714179  train-loss:  2.1534651475958526  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.346408247947693  train-loss:  2.15960643440485  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.3239349001332332  train-loss:  2.1586261419579387  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.4153965523368432  train-loss:  2.155537675600499  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.355137297981664  train-loss:  2.1570827765390277  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.3645593367124857  train-loss:  2.157966608647257  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.3231714963912964  train-loss:  2.1600736659020185  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.3532139753040515  train-loss:  2.161129569634795  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.382431940028542  train-loss:  2.1630483344197273  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.3508194559498836  train-loss:  2.159725518897176  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.3729054676859  train-loss:  2.1561068883165717  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.3648099648325065  train-loss:  2.1605331036262214  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.359860533162167  train-loss:  2.159732018597424  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.349831405438875  train-loss:  2.159287757240236  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.381249553278873  train-loss:  2.1552968248724937  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.349074646046287  train-loss:  2.1567476359196007  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.360726155732807  train-loss:  2.157482883427292  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.4097548660479093  train-loss:  2.1537303971126676  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.357200685300325  train-loss:  2.1560613475739956  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.3488653960980868  train-loss:  2.1648357114754617  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.3431930792959115  train-loss:  2.155809831805527  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.3483987732937464  train-loss:  2.157326963264495  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.327962699689363  train-loss:  2.1534113115631044  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.3617188114868966  train-loss:  2.1571758915670216  lr:  1.9073486328125e-08
