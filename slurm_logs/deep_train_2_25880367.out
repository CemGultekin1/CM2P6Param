model c73826e8dcf27c78505237c0fdd2b4e023b929299dd1eb1d5404548a state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=55.0, disp=1, domain='global', epoch=10, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
			 train-loss:  2.2178056240081787 	 ± 0.0
	data : 5.567822456359863
	model : 1.8842370510101318
			 train-loss:  2.254881739616394 	 ± 0.03707611560821533
	data : 2.7876358032226562
	model : 0.9745579957962036
			 train-loss:  2.1690405209859214 	 ± 0.12511536755194558
	data : 1.8963294823964436
	model : 0.6710798740386963
			 train-loss:  2.2255301475524902 	 ± 0.1459918666472043
	data : 1.4506250023841858
	model : 0.5193871855735779
			 train-loss:  2.1524041414260866 	 ± 0.19606262058256654
	data : 1.1831813335418702
	model : 0.4283620357513428
			 train-loss:  2.183207889397939 	 ± 0.19177629442570132
	data : 0.09227771759033203
	model : 0.06444416046142579
			 train-loss:  2.2615247624261037 	 ± 0.2613911278033434
	data : 0.1134312629699707
	model : 0.06432647705078125
			 train-loss:  2.267543390393257 	 ± 0.24502698557661937
	data : 0.1133641242980957
	model : 0.06441864967346192
			 train-loss:  2.2770848406685724 	 ± 0.23258466008645184
	data : 0.11356282234191895
	model : 0.06448860168457031
			 train-loss:  2.303774321079254 	 ± 0.2347275377498491
	data : 0.11378073692321777
	model : 0.06449670791625976
			 train-loss:  2.270870414647189 	 ± 0.24680937550612891
	data : 0.11375913619995118
	model : 0.06450543403625489
			 train-loss:  2.249381273984909 	 ± 0.24681621438164075
	data : 0.11374835968017578
	model : 0.0645287036895752
			 train-loss:  2.2428111754930935 	 ± 0.2382230452060526
	data : 0.11366949081420899
	model : 0.06448073387145996
			 train-loss:  2.2261369483811513 	 ± 0.23729943157090716
	data : 0.11333470344543457
	model : 0.06455326080322266
			 train-loss:  2.2138897657394407 	 ± 0.23378806848550335
	data : 0.11330180168151856
	model : 0.06471338272094726
			 train-loss:  2.2128443643450737 	 ± 0.22640053029294388
	data : 0.11350879669189454
	model : 0.06475358009338379
			 train-loss:  2.2102518011541927 	 ± 0.21988545390876324
	data : 0.11370677947998047
	model : 0.06478672027587891
			 train-loss:  2.208604488107893 	 ± 0.2137981644519813
	data : 0.1137317180633545
	model : 0.06479043960571289
			 train-loss:  2.188067398573223 	 ± 0.22560093226483469
	data : 0.11370887756347656
	model : 0.06470065116882324
			 train-loss:  2.178581899404526 	 ± 0.22374206310377165
	data : 0.11351099014282226
	model : 0.06457772254943847
			 train-loss:  2.176898303486052 	 ± 0.21847967149955874
	data : 0.11323080062866211
	model : 0.06448993682861329
			 train-loss:  2.19210284948349 	 ± 0.22454044393769415
	data : 0.11297245025634765
	model : 0.06458959579467774
			 train-loss:  2.1767258384953374 	 ± 0.23114553945876024
	data : 0.11318860054016114
	model : 0.06473450660705567
			 train-loss:  2.1861500591039658 	 ± 0.2307484547691925
	data : 0.1134345531463623
	model : 0.06472043991088867
			 train-loss:  2.20453417301178 	 ± 0.24336489242174356
	data : 0.11335477828979493
	model : 0.06469001770019531
			 train-loss:  2.2040379093243527 	 ± 0.23865181042174735
	data : 0.11347126960754395
	model : 0.06493053436279297
			 train-loss:  2.1975758737987943 	 ± 0.23649727114992808
	data : 0.1132927417755127
	model : 0.06484837532043457
			 train-loss:  2.2023734407765523 	 ± 0.23356984934381472
	data : 0.11316022872924805
	model : 0.06470460891723633
			 train-loss:  2.1973004875511957 	 ± 0.23107195389466362
	data : 0.11318445205688477
	model : 0.06475963592529296
			 train-loss:  2.1978732784589132 	 ± 0.22720905397328672
	data : 0.1127009391784668
	model : 0.06486377716064454
			 train-loss:  2.197931324282 	 ± 0.22351457769363076
	data : 0.11289801597595214
	model : 0.06463131904602051
			 train-loss:  2.1985703222453594 	 ± 0.22002320982245435
	data : 0.11316347122192383
	model : 0.06459589004516601
			 train-loss:  2.201287771716262 	 ± 0.21720852130003265
	data : 0.1130988597869873
	model : 0.06464223861694336
			 train-loss:  2.1999751013867996 	 ± 0.21412326019651998
	data : 0.11287550926208496
	model : 0.0645859718322754
			 train-loss:  2.19708993434906 	 ± 0.21171166265592575
	data : 0.11342415809631348
	model : 0.0645742416381836
			 train-loss:  2.196401811308331 	 ± 0.2087902064035244
	data : 0.11312098503112793
	model : 0.06467385292053222
			 train-loss:  2.1907443162557243 	 ± 0.2087280830249463
	data : 0.1132657527923584
	model : 0.06474900245666504
			 train-loss:  2.195393179592333 	 ± 0.2078955103078632
	data : 0.11339902877807617
	model : 0.06472811698913575
			 train-loss:  2.1910964219998093 	 ± 0.20691516360657877
	data : 0.11338653564453124
	model : 0.06474080085754394
			 train-loss:  2.196065402030945 	 ± 0.2066554561257022
	data : 0.11325211524963379
	model : 0.06469202041625977
			 train-loss:  2.1899162734427104 	 ± 0.2077915480445103
	data : 0.11320157051086426
	model : 0.06459136009216308
			 train-loss:  2.1965290024167015 	 ± 0.20962383308410298
	data : 0.11297955513000488
	model : 0.06458868980407714
			 train-loss:  2.200009684230006 	 ± 0.2083964419716898
	data : 0.11299033164978027
	model : 0.06468887329101562
			 train-loss:  2.1936646645719353 	 ± 0.21017421666422775
	data : 0.11317300796508789
	model : 0.06477046012878418
			 train-loss:  2.195760742823283 	 ± 0.2082903996619135
	data : 0.11348457336425781
	model : 0.06475949287414551
			 train-loss:  2.1955474148625913 	 ± 0.20601890372658227
	data : 0.11357316970825196
	model : 0.06474480628967286
			 train-loss:  2.1965392751896635 	 ± 0.20392641701506647
	data : 0.11357159614562988
	model : 0.06466026306152343
			 train-loss:  2.191113643348217 	 ± 0.20519057117831394
	data : 0.11350088119506836
	model : 0.06456513404846191
			 train-loss:  2.1878387222484665 	 ± 0.20434952242905333
	data : 0.11331057548522949
	model : 0.0645721435546875
			 train-loss:  2.1781689381599425 	 ± 0.21331967621918396
	data : 0.11325407028198242
	model : 0.06479253768920898
			 train-loss:  2.1798545496136534 	 ± 0.2115539838555894
	data : 0.11333260536193848
	model : 0.06489958763122558
			 train-loss:  2.1793084671864142 	 ± 0.209546228363067
	data : 0.11358952522277832
	model : 0.06489715576171876
			 train-loss:  2.1713288370168433 	 ± 0.21538851470781986
	data : 0.11341032981872559
	model : 0.06492748260498046
			 train-loss:  2.171715497970581 	 ± 0.21340342331081918
	data : 0.1135632038116455
	model : 0.06482586860656739
			 train-loss:  2.172359072078358 	 ± 0.21150737256136135
	data : 0.11338090896606445
	model : 0.06502227783203125
			 train-loss:  2.1692481700863158 	 ± 0.2108762596769127
	data : 0.11328907012939453
	model : 0.0650221824645996
			 train-loss:  2.1659907248982213 	 ± 0.21043492038257441
	data : 0.11305499076843262
	model : 0.06512303352355957
			 train-loss:  2.160727591350161 	 ± 0.21236358201094022
	data : 0.11330575942993164
	model : 0.06573615074157715
			 train-loss:  2.1607761908385714 	 ± 0.21055652481544068
	data : 0.11292839050292969
	model : 0.06588330268859863
			 train-loss:  2.1674360553423564 	 ± 0.21496981933277784
	data : 0.11315903663635254
	model : 0.06551694869995117
			 train-loss:  2.1815628810007066 	 ± 0.23964240289155755
	data : 0.11338968276977539
	model : 0.06538953781127929
			 train-loss:  2.1821977053919146 	 ± 0.23775365099042067
	data : 0.1135141372680664
	model : 0.06528258323669434
			 train-loss:  2.181320549949767 	 ± 0.23596027354623347
	data : 0.11379389762878418
	model : 0.06462621688842773
			 train-loss:  2.18697677180171 	 ± 0.2383754243313888
	data : 0.11415009498596192
	model : 0.06446323394775391
			 train-loss:  2.1825956931480994 	 ± 0.23911723432770257
	data : 0.1139756679534912
	model : 0.0646695613861084
			 train-loss:  2.184080113064159 	 ± 0.2376004218183327
	data : 0.11381359100341797
	model : 0.0647648811340332
			 train-loss:  2.1807181746212403 	 ± 0.23739700421414586
	data : 0.11390810012817383
	model : 0.06490321159362793
			 train-loss:  2.194373651462443 	 ± 0.26081062177985925
	data : 0.11367640495300294
	model : 0.06495285034179688
			 train-loss:  2.1987059928368833 	 ± 0.2613668993821237
	data : 0.11380925178527831
	model : 0.06497273445129395
			 train-loss:  2.20314622095653 	 ± 0.2621013910353173
	data : 0.11378231048583984
	model : 0.06469945907592774
			 train-loss:  2.199763516305198 	 ± 0.2617834261276247
	data : 0.11381516456604004
	model : 0.06462855339050293
			 train-loss:  2.1965392761760287 	 ± 0.2613749132270448
	data : 0.11359214782714844
	model : 0.06450066566467286
			 train-loss:  2.193531532810159 	 ± 0.2608301123723967
	data : 0.11345152854919434
	model : 0.06453289985656738
			 train-loss:  2.194085974951048 	 ± 0.2591050599988762
	data : 0.11339826583862304
	model : 0.06465320587158203
			 train-loss:  2.1942309220631917 	 ± 0.2573749166825727
	data : 0.11361346244812012
	model : 0.06473407745361329
			 train-loss:  2.194644843277178 	 ± 0.25570118162713523
	data : 0.11367588043212891
	model : 0.0647660255432129
			 train-loss:  2.1954482220984124 	 ± 0.2541318849007482
	data : 0.11367635726928711
	model : 0.06475987434387206
			 train-loss:  2.2033758805348325 	 ± 0.26190514639904705
	data : 0.11359677314758301
	model : 0.06467952728271484
			 train-loss:  2.2023403131509127 	 ± 0.2604029004148391
	data : 0.11342430114746094
	model : 0.06452951431274415
			 train-loss:  2.1992429539561273 	 ± 0.26023056459070143
	data : 0.11320219039916993
	model : 0.06456198692321777
			 train-loss:  2.1995074145587874 	 ± 0.25863003146338204
	data : 0.11323065757751465
	model : 0.06464405059814453
			 train-loss:  2.200986037893993 	 ± 0.2573924243884991
	data : 0.11338024139404297
	model : 0.0648829460144043
			 train-loss:  2.2034527609147223 	 ± 0.25681044418065085
	data : 0.1134307861328125
	model : 0.06486892700195312
			 train-loss:  2.2030322821367356 	 ± 0.2553059746538905
	data : 0.11345634460449219
	model : 0.0648949146270752
			 train-loss:  2.202248350311728 	 ± 0.25390140974314795
	data : 0.11336874961853027
	model : 0.06480174064636231
			 train-loss:  2.199878003708152 	 ± 0.25336514803178684
	data : 0.1131831169128418
	model : 0.06475963592529296
			 train-loss:  2.198327951047612 	 ± 0.2523146178433381
	data : 0.11324238777160645
	model : 0.06466679573059082
			 train-loss:  2.197853379628875 	 ± 0.25091596393118576
	data : 0.11336994171142578
	model : 0.06478800773620605
			 train-loss:  2.201731404561675 	 ± 0.25214055065034396
	data : 0.1135591983795166
	model : 0.06477222442626954
			 train-loss:  2.199521995915307 	 ± 0.2516007199583378
	data : 0.11370530128479003
	model : 0.06484751701354981
			 train-loss:  2.2001049479285437 	 ± 0.2502755895995848
	data : 0.11361513137817383
	model : 0.06475849151611328
			 train-loss:  2.1982772829739945 	 ± 0.2495215361730617
	data : 0.11342291831970215
	model : 0.06463789939880371
			 train-loss:  2.195883757324629 	 ± 0.24923601006101498
	data : 0.11326055526733399
	model : 0.06460008621215821
			 train-loss:  2.196913758490948 	 ± 0.2481056560912295
	data : 0.11315617561340333
	model : 0.0646784782409668
			 train-loss:  2.1954606972242656 	 ± 0.2471981486314731
	data : 0.1135396957397461
	model : 0.06470508575439453
			 train-loss:  2.197127648939689 	 ± 0.2464434486517903
	data : 0.1139188289642334
	model : 0.06480875015258789
			 train-loss:  2.1949944250362434 	 ± 0.2460591548145858
	data : 0.1141322135925293
	model : 0.06480088233947753
			 train-loss:  2.1924353704160575 	 ± 0.2460945554461291
	data : 0.11419658660888672
	model : 0.06471443176269531
			 train-loss:  2.1948485362409342 	 ± 0.24601113324800825
	data : 0.11405057907104492
	model : 0.06464433670043945
			 train-loss:  2.1976777470111846 	 ± 0.246391359081071
	data : 0.11405038833618164
	model : 0.06454014778137207
			 train-loss:  2.194596585660878 	 ± 0.2470971086450406
	data : 0.1138526439666748
	model : 0.06446590423583984
			 train-loss:  2.1947000166949104 	 ± 0.24588506205678518
	data : 0.1136056900024414
	model : 0.06454730033874512
			 train-loss:  2.1934120909681596 	 ± 0.24503402096628676
	data : 0.11368789672851562
	model : 0.06465544700622558
			 train-loss:  2.195185587956355 	 ± 0.24451648758021036
	data : 0.11399164199829101
	model : 0.0648911476135254
			 train-loss:  2.1953445116678876 	 ± 0.24335473469457122
	data : 0.11354198455810546
	model : 0.06556873321533203
			 train-loss:  2.197910877893556 	 ± 0.24362755999009625
	data : 0.11290206909179687
	model : 0.06553301811218262
			 train-loss:  2.198766349632049 	 ± 0.2426463440095857
	data : 0.1129873275756836
	model : 0.06543998718261719
			 train-loss:  2.19706940430182 	 ± 0.24215740390219545
	data : 0.11279644966125488
	model : 0.06536979675292968
			 train-loss:  2.1973931286313118 	 ± 0.2410675067823794
	data : 0.11249351501464844
	model : 0.06521234512329102
			 train-loss:  2.2010219248858367 	 ± 0.2429414900289474
	data : 0.11265277862548828
	model : 0.0645859718322754
			 train-loss:  2.201326527037062 	 ± 0.241865782665141
	data : 0.11340007781982422
	model : 0.06472539901733398
			 train-loss:  2.201120987534523 	 ± 0.24079334122215806
	data : 0.11353163719177246
	model : 0.06474614143371582
			 train-loss:  2.200520888894005 	 ± 0.2398096253430866
	data : 0.11353445053100586
	model : 0.06474943161010742
			 train-loss:  2.1977496408579644 	 ± 0.2405660276938814
	data : 0.11355051994323731
	model : 0.06466560363769532
			 train-loss:  2.196685539121213 	 ± 0.23978711892957547
	data : 0.11344771385192871
	model : 0.06468443870544434
			 train-loss:  2.194690568693753 	 ± 0.2397079065918804
	data : 0.11349730491638184
	model : 0.06458916664123535
			 train-loss:  2.1957440518925333 	 ± 0.2389508533311386
	data : 0.11336674690246581
	model : 0.06466155052185059
			 train-loss:  2.1959296000205866 	 ± 0.23794465993229902
	data : 0.1136709213256836
	model : 0.06482920646667481
			 train-loss:  2.199465597377104 	 ± 0.24003597490138429
	data : 0.11386241912841796
	model : 0.0647972583770752
			 train-loss:  2.1996105213960013 	 ± 0.23903896061112434
	data : 0.11386666297912598
	model : 0.06478309631347656
			 train-loss:  2.2007148640214904 	 ± 0.2383563405973498
	data : 0.11361918449401856
	model : 0.0647773265838623
			 train-loss:  2.2014548680821404 	 ± 0.237516986750154
	data : 0.11357741355895996
	model : 0.06469249725341797
			 train-loss:  2.2043757923250276 	 ± 0.23873949276083814
	data : 0.11330842971801758
	model : 0.06453251838684082
			 train-loss:  2.204127396306684 	 ± 0.2377908430019379
	data : 0.11328530311584473
	model : 0.06467709541320801
			 train-loss:  2.204444719314575 	 ± 0.23686412809451227
	data : 0.1134676456451416
	model : 0.06470232009887696
			 train-loss:  2.203744466342623 	 ± 0.23605218645975246
	data : 0.11371035575866699
	model : 0.06481919288635254
			 train-loss:  2.201623676330086 	 ± 0.2363230968652734
	data : 0.11386399269104004
	model : 0.0648421287536621
			 train-loss:  2.1988269174471498 	 ± 0.23749876868818123
	data : 0.11390604972839355
	model : 0.06486010551452637
			 train-loss:  2.19907671813817 	 ± 0.23659332019239795
	data : 0.11372060775756836
	model : 0.06476416587829589
			 train-loss:  2.2001615771880516 	 ± 0.23600346193480987
	data : 0.11345248222351074
	model : 0.06468343734741211
			 train-loss:  2.198999937254054 	 ± 0.2354737442263101
	data : 0.11333022117614747
	model : 0.06464152336120606
			 train-loss:  2.1968668012908013 	 ± 0.23584721693282773
	data : 0.11332755088806153
	model : 0.0647658348083496
			 train-loss:  2.2001019187439654 	 ± 0.23788063153871186
	data : 0.11350049972534179
	model : 0.06480355262756347
			 train-loss:  2.1990068652736605 	 ± 0.23732759692452055
	data : 0.1138178825378418
	model : 0.06490354537963867
			 train-loss:  2.202802808196456 	 ± 0.24049532438144552
	data : 0.11404461860656738
	model : 0.06488151550292968
			 train-loss:  2.2012580457855675 	 ± 0.2402808167585985
	data : 0.11395564079284667
	model : 0.06481423377990722
			 train-loss:  2.200063618430256 	 ± 0.23980715960747004
	data : 0.11375241279602051
	model : 0.06475849151611328
			 train-loss:  2.2015598424966782 	 ± 0.23957765550854943
	data : 0.113651704788208
	model : 0.06474194526672364
			 train-loss:  2.200722987703282 	 ± 0.238916652167093
	data : 0.11364331245422363
	model : 0.06471543312072754
			 train-loss:  2.2002514140946525 	 ± 0.23812676270312588
	data : 0.11362500190734863
	model : 0.06484446525573731
			 train-loss:  2.2002446042730455 	 ± 0.2372808527059578
	data : 0.11381902694702148
	model : 0.0649406909942627
			 train-loss:  2.199772337792625 	 ± 0.23651037333854177
	data : 0.11410965919494628
	model : 0.06491060256958008
			 train-loss:  2.199195976857539 	 ± 0.23578201590132195
	data : 0.1142310619354248
	model : 0.06484427452087402
			 train-loss:  2.199125862783856 	 ± 0.23496339800867785
	data : 0.11395974159240722
	model : 0.0647862434387207
			 train-loss:  2.198591470718384 	 ± 0.23423957340548793
	data : 0.11382536888122559
	model : 0.06470685005187989
			 train-loss:  2.197490203870486 	 ± 0.23381236637470046
	data : 0.11365718841552734
	model : 0.06460614204406738
			 train-loss:  2.199119992807609 	 ± 0.2338463960541865
	data : 0.11343326568603515
	model : 0.06464591026306152
			 train-loss:  2.1985928931751766 	 ± 0.23314264118138664
	data : 0.11343340873718262
	model : 0.0648622989654541
			 train-loss:  2.2004716908371686 	 ± 0.23348043070028301
	data : 0.11358556747436524
	model : 0.06491141319274903
			 train-loss:  2.2001132599512734 	 ± 0.23274198849545316
	data : 0.11379542350769042
	model : 0.06487565040588379
			 train-loss:  2.200782194832303 	 ± 0.23211467057322316
	data : 0.11382908821105957
	model : 0.06495027542114258
			 train-loss:  2.198800094817814 	 ± 0.23262846276674554
	data : 0.1140127182006836
	model : 0.06485199928283691
			 train-loss:  2.2014400787602844 	 ± 0.23414027557146438
	data : 0.1137451171875
	model : 0.06462554931640625
			 train-loss:  2.199443116590574 	 ± 0.23468239397782248
	data : 0.11361885070800781
	model : 0.06464042663574218
			 train-loss:  2.197989825279482 	 ± 0.2346183170619464
	data : 0.11353077888488769
	model : 0.06474628448486328
			 train-loss:  2.2004808080502047 	 ± 0.23591242347421293
	data : 0.11367011070251465
	model : 0.06472821235656738
			 train-loss:  2.201323038453509 	 ± 0.2353950766600889
	data : 0.11362409591674805
	model : 0.06519870758056641
			 train-loss:  2.2003769557687303 	 ± 0.2349482218649919
	data : 0.11388421058654785
	model : 0.0652998924255371
			 train-loss:  2.1996247318555726 	 ± 0.23439900981094705
	data : 0.11410250663757324
	model : 0.0652355670928955
			 train-loss:  2.1988067775964737 	 ± 0.23389288471355157
	data : 0.11409177780151367
	model : 0.06520061492919922
			 train-loss:  2.2011771868474734 	 ± 0.23508532033797852
	data : 0.11396298408508301
	model : 0.06512012481689453
			 train-loss:  2.201832631487905 	 ± 0.23450614497999003
	data : 0.11383404731750488
	model : 0.06462740898132324
			 train-loss:  2.2044403318978527 	 ± 0.2361299796438387
	data : 0.11414957046508789
	model : 0.06460990905761718
			 train-loss:  2.2036567708341086 	 ± 0.2356214339020338
	data : 0.11411371231079101
	model : 0.06462664604187011
			 train-loss:  2.204744389562896 	 ± 0.2353189083705005
	data : 0.11396932601928711
	model : 0.06461219787597657
			 train-loss:  2.2095800006245994 	 ± 0.2426924422677798
	data : 0.11403822898864746
	model : 0.06468958854675293
			 train-loss:  2.2079260220784627 	 ± 0.24290130972175616
	data : 0.11425809860229492
	model : 0.06474370956420898
			 train-loss:  2.210222894237155 	 ± 0.24398950466510372
	data : 0.11408510208129882
	model : 0.06470074653625488
			 train-loss:  2.2104050153811303 	 ± 0.24327802389677525
	data : 0.11381087303161622
	model : 0.06467480659484863
			 train-loss:  2.2097489034428315 	 ± 0.2427113639332146
	data : 0.11382474899291992
	model : 0.06463637351989746
			 train-loss:  2.209271050336068 	 ± 0.24208083081961293
	data : 0.11379847526550294
	model : 0.0645988941192627
			 train-loss:  2.2069917071697325 	 ± 0.24320943050635319
	data : 0.11355400085449219
	model : 0.06469225883483887
			 train-loss:  2.207574018853248 	 ± 0.24262571532746752
	data : 0.11337099075317383
	model : 0.0647498607635498
			 train-loss:  2.206401759180529 	 ± 0.242418348364906
	data : 0.11367449760437012
	model : 0.06483864784240723
			 train-loss:  2.2052757481166294 	 ± 0.24218063696887263
	data : 0.11390433311462403
	model : 0.06494703292846679
			 train-loss:  2.206169845028357 	 ± 0.2417811212872327
	data : 0.11398463249206543
	model : 0.06491270065307617
			 train-loss:  2.2060399742449746 	 ± 0.2411033124814686
	data : 0.11390700340270996
	model : 0.06478691101074219
			 train-loss:  2.204000673936994 	 ± 0.2419510853820649
	data : 0.11377320289611817
	model : 0.06471714973449708
			 train-loss:  2.206366150072833 	 ± 0.2433295743405466
	data : 0.11357631683349609
	model : 0.0646897315979004
			 train-loss:  2.2079349319140116 	 ± 0.24355876905146484
	data : 0.1140059471130371
	model : 0.06462287902832031
			 train-loss:  2.206992848802008 	 ± 0.24321366748764742
	data : 0.11439995765686035
	model : 0.06475596427917481
			 train-loss:  2.205738515644283 	 ± 0.24313092941199974
	data : 0.11464858055114746
	model : 0.06487431526184081
			 train-loss:  2.2057299509725934 	 ± 0.24246575479456808
	data : 0.11472744941711426
	model : 0.06495051383972168
			 train-loss:  2.205106946437255 	 ± 0.2419528093740198
	data : 0.11489701271057129
	model : 0.0649989128112793
			 train-loss:  2.2040666554425212 	 ± 0.2417102584412735
	data : 0.11436986923217773
	model : 0.06495108604431152
			 train-loss:  2.2044732814194052 	 ± 0.2411230621223123
	data : 0.11397972106933593
	model : 0.06488957405090331
			 train-loss:  2.2042872765484978 	 ± 0.24049086348309293
	data : 0.11389718055725098
	model : 0.06478409767150879
			 train-loss:  2.204561119383954 	 ± 0.23987963857119105
	data : 0.11378579139709473
	model : 0.06470017433166504
			 train-loss:  2.2040668353832586 	 ± 0.23934016845076575
	data : 0.11360430717468262
	model : 0.06466145515441894
			 train-loss:  2.2044705089769865 	 ± 0.23877399562910004
	data : 0.11367316246032715
	model : 0.06471514701843262
			 train-loss:  2.2021897382137037 	 ± 0.2402142531133002
	data : 0.11367235183715821
	model : 0.06476063728332519
			 train-loss:  2.2026006349672875 	 ± 0.23965516733434758
	data : 0.11374564170837402
	model : 0.0648111343383789
			 train-loss:  2.2028146290408515 	 ± 0.23905188344374778
	data : 0.1139829158782959
	model : 0.06482367515563965
			 train-loss:  2.2014206494252706 	 ± 0.23922012992810954
	data : 0.11405892372131347
	model : 0.06477298736572265
			 train-loss:  2.2029572969827895 	 ± 0.23956396210107012
	data : 0.11405014991760254
	model : 0.06469554901123047
			 train-loss:  2.204936232493848 	 ± 0.24054466838201427
	data : 0.11384077072143554
	model : 0.06458864212036133
			 train-loss:  2.204543772687767 	 ± 0.23999627481401908
	data : 0.11372251510620117
	model : 0.06459732055664062
			 train-loss:  2.2036955832230922 	 ± 0.23968529087508444
	data : 0.11361875534057617
	model : 0.06469178199768066
			 train-loss:  2.204115962862369 	 ± 0.2391554732710993
	data : 0.11374187469482422
	model : 0.06473088264465332
			 train-loss:  2.2029888623952867 	 ± 0.23908610254605486
	data : 0.11376371383666992
	model : 0.0648341178894043
			 train-loss:  2.20259535549885 	 ± 0.238555538792137
	data : 0.1140531063079834
	model : 0.06491355895996094
			 train-loss:  2.2035229058548955 	 ± 0.2383273985714528
	data : 0.11446647644042969
	model : 0.06484832763671874
			 train-loss:  2.2020764016165524 	 ± 0.23862691771294148
	data : 0.11431560516357422
	model : 0.06472234725952149
			 train-loss:  2.2016310557430865 	 ± 0.23812588276825739
	data : 0.11445450782775879
	model : 0.06463780403137206
			 train-loss:  2.201489086267425 	 ± 0.23755303232294217
	data : 0.11425929069519043
	model : 0.06452994346618653
			 train-loss:  2.2043681509286452 	 ± 0.24053430493932854
	data : 0.1142110824584961
	model : 0.06450343132019043
			 train-loss:  2.20484237221704 	 ± 0.24004911396841686
	data : 0.11390929222106934
	model : 0.06449303627014161
			 train-loss:  2.207273206458642 	 ± 0.2420117649957839
	data : 0.11398367881774903
	model : 0.06457180976867676
			 train-loss:  2.2077995276337035 	 ± 0.2415513931482103
	data : 0.1138617992401123
	model : 0.06464447975158691
			 train-loss:  2.206528385480245 	 ± 0.24167526768631395
	data : 0.1140636920928955
	model : 0.06476168632507324
			 train-loss:  2.206270890213302 	 ± 0.24113077093838386
	data : 0.11395597457885742
	model : 0.06468615531921387
			 train-loss:  2.2069335681087567 	 ± 0.24075390603946512
	data : 0.11387829780578614
	model : 0.06467876434326172
			 train-loss:  2.2069420243652775 	 ± 0.24018812270008466
	data : 0.11382322311401367
	model : 0.06460723876953126
			 train-loss:  2.212442482743308 	 ± 0.25271540774027307
	data : 0.11361093521118164
	model : 0.0645895004272461
			 train-loss:  2.2116841604543285 	 ± 0.2523709407095719
	data : 0.11356258392333984
	model : 0.06459269523620606
			 train-loss:  2.211546277558362 	 ± 0.2517941879060828
	data : 0.11360745429992676
	model : 0.0647082805633545
			 train-loss:  2.2096157348650394 	 ± 0.25281055488562704
	data : 0.1137779712677002
	model : 0.06481037139892579
			 train-loss:  2.207992583786676 	 ± 0.25336082839885615
	data : 0.11404385566711425
	model : 0.06487231254577637
			 train-loss:  2.208392245040092 	 ± 0.2528505833222581
	data : 0.11421341896057129
	model : 0.06480460166931153
			 train-loss:  2.2087037742137907 	 ± 0.2523173895842189
	data : 0.11410255432128906
	model : 0.0646754264831543
			 train-loss:  2.209050864116099 	 ± 0.251798522835138
	data : 0.11398234367370605
	model : 0.06456832885742188
			 train-loss:  2.2088346057110004 	 ± 0.2512513381733078
	data : 0.1137934684753418
	model : 0.06448659896850586
			 train-loss:  2.208830388137578 	 ± 0.2506873692705586
	data : 0.11374359130859375
	model : 0.0644033432006836
			 train-loss:  2.207086202289377 	 ± 0.25147964118764426
	data : 0.11370954513549805
	model : 0.0643646240234375
			 train-loss:  2.206740197075738 	 ± 0.25097360722659856
	data : 0.11386017799377442
	model : 0.06426782608032226
			 train-loss:  2.2073405981063843 	 ± 0.25057963399613975
	data : 0.11413993835449218
	model : 0.06412439346313477
			 train-loss:  2.205658128608166 	 ± 0.2513031702677896
	data : 0.11420149803161621
	model : 0.06391878128051758
			 train-loss:  2.205261261316768 	 ± 0.25082274344540073
	data : 0.1140784740447998
	model : 0.06373357772827148
			 train-loss:  2.206098055214861 	 ± 0.25059324532925825
	data : 0.11418685913085938
	model : 0.06366147994995117
			 train-loss:  2.205033826309702 	 ± 0.25056597100267197
	data : 0.11442923545837402
	model : 0.06367354393005371
			 train-loss:  2.2039347056186562 	 ± 0.250578075396292
	data : 0.11435413360595703
	model : 0.06371545791625977
			 train-loss:  2.2036477604816698 	 ± 0.2500754845846237
	data : 0.11468782424926757
	model : 0.0636894702911377
			 train-loss:  2.2043566161470864 	 ± 0.24977173609630873
	data : 0.1147369384765625
	model : 0.06365857124328614
			 train-loss:  2.2039704017150097 	 ± 0.24930717660364984
	data : 0.11469483375549316
	model : 0.0635953426361084
			 train-loss:  2.2037128702123114 	 ± 0.24880735995576714
	data : 0.11447372436523437
	model : 0.06356759071350097
			 train-loss:  2.2038449311660506 	 ± 0.24828791963123073
	data : 0.11454815864562988
	model : 0.06356072425842285
			 train-loss:  2.2030617868849998 	 ± 0.248055477468749
	data : 0.11448001861572266
	model : 0.06365575790405273
			 train-loss:  2.2022461901191903 	 ± 0.24785204467654295
	data : 0.11468896865844727
	model : 0.06377387046813965
			 train-loss:  2.201699861422742 	 ± 0.24747654668866612
	data : 0.1147700309753418
	model : 0.06379685401916504
			 train-loss:  2.2012254764636356 	 ± 0.24706930197109309
	data : 0.1148465633392334
	model : 0.06372041702270508
			 train-loss:  2.199845754753999 	 ± 0.24748094917255506
	data : 0.11465435028076172
	model : 0.06362924575805665
			 train-loss:  2.1983778471789086 	 ± 0.24801820408563113
	data : 0.1144984245300293
	model : 0.06361818313598633
			 train-loss:  2.197062981962667 	 ± 0.24835111473165228
	data : 0.11440801620483398
	model : 0.06362237930297851
			 train-loss:  2.1972034045907316 	 ± 0.24785134247574023
	data : 0.11438922882080078
	model : 0.0637026309967041
			 train-loss:  2.1969211208577057 	 ± 0.24738430633251712
	data : 0.11443123817443848
	model : 0.06368904113769532
			 train-loss:  2.1957845014285264 	 ± 0.24752118097279052
	data : 0.11437420845031739
	model : 0.06368694305419922
			 train-loss:  2.194550105917309 	 ± 0.24777717753844972
	data : 0.114296293258667
	model : 0.0636199951171875
			 train-loss:  2.1955202019983724 	 ± 0.2477466922000102
	data : 0.11426010131835937
	model : 0.06357660293579101
			 train-loss:  2.1961422541055335 	 ± 0.24744269460149612
	data : 0.11430296897888184
	model : 0.06354231834411621
			 train-loss:  2.196493010520935 	 ± 0.24700933190646074
	data : 0.1142643928527832
	model : 0.06364293098449707
			 train-loss:  2.1949310896406136 	 ± 0.24775073591459548
	data : 0.11445574760437012
	model : 0.06372489929199218
			 train-loss:  2.1963748719011034 	 ± 0.2483144484998195
	data : 0.11453590393066407
	model : 0.06369595527648926
			 train-loss:  2.197840312252874 	 ± 0.2489126825766659
	data : 0.11441903114318848
	model : 0.06360969543457032
			 train-loss:  2.197962776413114 	 ± 0.24842985056047195
	data : 0.11415729522705079
	model : 0.06361331939697265
			 train-loss:  2.198275017738342 	 ± 0.24799218799477335
	data : 0.11410894393920898
	model : 0.06360621452331543
			 train-loss:  2.2081779348663986 	 ± 0.2937127358346419
	data : 0.11391515731811523
	model : 0.2539708137512207
#epoch  10    val-loss:  2.437308951428062  train-loss:  2.2081779348663986  lr:  0.00125
			 train-loss:  2.1368916034698486 	 ± 0.0
	data : 5.4486682415008545
	model : 0.07732152938842773
			 train-loss:  2.1217881441116333 	 ± 0.015103459358215332
	data : 2.7851210832595825
	model : 0.07112610340118408
			 train-loss:  2.1065202554066977 	 ± 0.02486550168545667
	data : 1.8947610060373943
	model : 0.06889128684997559
			 train-loss:  2.1475059390068054 	 ± 0.07418354710484347
	data : 1.4494223594665527
	model : 0.06771296262741089
			 train-loss:  2.1095643758773805 	 ± 0.10080083223695036
	data : 1.1822498798370362
	model : 0.06711945533752442
			 train-loss:  2.0895407795906067 	 ± 0.10233309268102536
	data : 0.1153656005859375
	model : 0.06459436416625977
			 train-loss:  2.1006378616605486 	 ± 0.09856431607664327
	data : 0.11394004821777344
	model : 0.06457133293151855
			 train-loss:  2.064726397395134 	 ± 0.1323933220106001
	data : 0.11396832466125488
	model : 0.06465029716491699
			 train-loss:  2.079791453149584 	 ± 0.1318942163906432
	data : 0.11416106224060059
	model : 0.06468715667724609
			 train-loss:  2.09472051858902 	 ± 0.13289984514523084
	data : 0.11425752639770508
	model : 0.0646982192993164
			 train-loss:  2.1511869972402398 	 ± 0.2189550891342831
	data : 0.11415743827819824
	model : 0.0646634578704834
			 train-loss:  2.167209098736445 	 ± 0.2162637399940639
	data : 0.11383981704711914
	model : 0.06463308334350586
			 train-loss:  2.1790113357397227 	 ± 0.21176360901986915
	data : 0.11378045082092285
	model : 0.06464571952819824
			 train-loss:  2.1852366498538425 	 ± 0.20529126779941959
	data : 0.11378250122070313
	model : 0.06479077339172364
			 train-loss:  2.1697938124338787 	 ± 0.20657591383917998
	data : 0.11378464698791504
	model : 0.06481413841247559
			 train-loss:  2.1768410205841064 	 ± 0.201869895744824
	data : 0.11400127410888672
	model : 0.06487545967102051
			 train-loss:  2.1724550443537094 	 ± 0.19662680134572627
	data : 0.11424918174743652
	model : 0.06486639976501465
			 train-loss:  2.1596165696779885 	 ± 0.19828326697463391
	data : 0.11419792175292968
	model : 0.06486024856567382
			 train-loss:  2.177688529616908 	 ± 0.2076672836207509
	data : 0.11399006843566895
	model : 0.06483316421508789
			 train-loss:  2.1748544752597807 	 ± 0.20278565301811716
	data : 0.11400246620178223
	model : 0.06475653648376464
			 train-loss:  2.171405707086836 	 ± 0.19849863885849586
	data : 0.11368093490600586
	model : 0.06473717689514161
			 train-loss:  2.1793163527141917 	 ± 0.19729386058984685
	data : 0.1136397361755371
	model : 0.06480388641357422
			 train-loss:  2.1704389530679453 	 ± 0.19739874068673166
	data : 0.11370649337768554
	model : 0.06487727165222168
			 train-loss:  2.1833438873291016 	 ± 0.20291137743087312
	data : 0.1138580322265625
	model : 0.0648951530456543
			 train-loss:  2.181665620803833 	 ± 0.1989816672258391
	data : 0.11391477584838867
	model : 0.06496982574462891
			 train-loss:  2.1903141828683705 	 ± 0.19985196964629245
	data : 0.11419668197631835
	model : 0.06494340896606446
			 train-loss:  2.1854501565297446 	 ± 0.1976781426422767
	data : 0.1140211582183838
	model : 0.0648836612701416
			 train-loss:  2.202326774597168 	 ± 0.21300516405453096
	data : 0.11386251449584961
	model : 0.06473884582519532
			 train-loss:  2.1908035031680404 	 ± 0.2180015544507824
	data : 0.11370997428894043
	model : 0.0647620677947998
			 train-loss:  2.188596161206563 	 ± 0.21466676551274588
	data : 0.11354608535766601
	model : 0.06482114791870117
			 train-loss:  2.1794927581664054 	 ± 0.21698264514359675
	data : 0.11348838806152343
	model : 0.0649186611175537
			 train-loss:  2.18336970359087 	 ± 0.21465349820091992
	data : 0.11357431411743164
	model : 0.06498608589172364
			 train-loss:  2.1907160065390845 	 ± 0.21542251378050511
	data : 0.11368498802185059
	model : 0.0650219440460205
			 train-loss:  2.1912531642352833 	 ± 0.2122533240722267
	data : 0.11364946365356446
	model : 0.06490473747253418
			 train-loss:  2.1865788051060266 	 ± 0.21096723952179022
	data : 0.11356563568115234
	model : 0.06486673355102539
			 train-loss:  2.1905403070979648 	 ± 0.209332601698631
	data : 0.11348867416381836
	model : 0.06478023529052734
			 train-loss:  2.187135986379675 	 ± 0.20749223710816614
	data : 0.11355814933776856
	model : 0.06476864814758301
			 train-loss:  2.1861840423784757 	 ± 0.20482573893459302
	data : 0.11366376876831055
	model : 0.06482257843017578
			 train-loss:  2.184434303870568 	 ± 0.20247022163358275
	data : 0.1137925624847412
	model : 0.06492152214050292
			 train-loss:  2.183734357357025 	 ± 0.1999711053174791
	data : 0.11398744583129883
	model : 0.06491575241088868
			 train-loss:  2.1898222609264093 	 ± 0.20123523021474107
	data : 0.11392807960510254
	model : 0.06502885818481445
			 train-loss:  2.1981375330970403 	 ± 0.20583082933232533
	data : 0.11364078521728516
	model : 0.06493153572082519
			 train-loss:  2.195206741954005 	 ± 0.20430816760622028
	data : 0.11344127655029297
	model : 0.06485180854797364
			 train-loss:  2.2026386098428206 	 ± 0.2077694728536944
	data : 0.11353425979614258
	model : 0.06477046012878418
			 train-loss:  2.198834631178114 	 ± 0.20699167303085964
	data : 0.11346521377563476
	model : 0.06477689743041992
			 train-loss:  2.197826064151266 	 ± 0.2048411628078527
	data : 0.11354098320007325
	model : 0.06474018096923828
			 train-loss:  2.194864475980718 	 ± 0.2036433262230243
	data : 0.1137852668762207
	model : 0.06484761238098144
			 train-loss:  2.1937421957651773 	 ± 0.2016577062683382
	data : 0.11389031410217285
	model : 0.06484780311584473
			 train-loss:  2.1889918458705044 	 ± 0.20228463893025658
	data : 0.11364307403564453
	model : 0.0648576259613037
			 train-loss:  2.187214024066925 	 ± 0.20063789650038763
	data : 0.11364259719848632
	model : 0.06488542556762696
			 train-loss:  2.1847613301931643 	 ± 0.19941671459548388
	data : 0.11352148056030273
	model : 0.0648695945739746
			 train-loss:  2.184097159367341 	 ± 0.19754688766488157
	data : 0.11362662315368652
	model : 0.06484971046447754
			 train-loss:  2.1790326361386283 	 ± 0.19905332181463795
	data : 0.11375856399536133
	model : 0.0649179458618164
			 train-loss:  2.1839519253483526 	 ± 0.20042716521182297
	data : 0.11400508880615234
	model : 0.0649235725402832
			 train-loss:  2.1795406254855068 	 ± 0.20122495482361774
	data : 0.11399564743041993
	model : 0.06480269432067871
			 train-loss:  2.1760197366986955 	 ± 0.20112244141062754
	data : 0.11397910118103027
	model : 0.06468586921691895
			 train-loss:  2.178125870855231 	 ± 0.1999724681772228
	data : 0.11381568908691406
	model : 0.06468491554260254
			 train-loss:  2.177411506915915 	 ± 0.19831442388490508
	data : 0.11377439498901368
	model : 0.06468877792358399
			 train-loss:  2.1834291886475126 	 ± 0.20189687644175738
	data : 0.11367392539978027
	model : 0.0647404670715332
			 train-loss:  2.184865887959798 	 ± 0.20051124264799713
	data : 0.11377091407775879
	model : 0.06482472419738769
			 train-loss:  2.1845754834472157 	 ± 0.1988736385613656
	data : 0.11392264366149903
	model : 0.06495518684387207
			 train-loss:  2.185172742412936 	 ± 0.19731844572710897
	data : 0.1141730785369873
	model : 0.06487159729003907
			 train-loss:  2.1836409379565525 	 ± 0.19611740956726276
	data : 0.11402878761291504
	model : 0.06489944458007812
			 train-loss:  2.18566532805562 	 ± 0.1952415241914359
	data : 0.11395511627197266
	model : 0.06479182243347167
			 train-loss:  2.1872449434720553 	 ± 0.19414554992913696
	data : 0.11378827095031738
	model : 0.06476683616638183
			 train-loss:  2.1874912507606274 	 ± 0.19267936975226516
	data : 0.11379237174987793
	model : 0.06472420692443848
			 train-loss:  2.183155045580508 	 ± 0.1944536102955262
	data : 0.11361327171325683
	model : 0.06482906341552734
			 train-loss:  2.177796142942765 	 ± 0.19793999041893148
	data : 0.11375870704650878
	model : 0.0648153305053711
			 train-loss:  2.1761145280755083 	 ± 0.19698909255800612
	data : 0.11378626823425293
	model : 0.06494054794311524
			 train-loss:  2.17685912336622 	 ± 0.19567474241932367
	data : 0.11384940147399902
	model : 0.06486940383911133
			 train-loss:  2.1786361412263253 	 ± 0.19485988342881266
	data : 0.11369409561157226
	model : 0.0647963523864746
			 train-loss:  2.1748600701491037 	 ± 0.19610042444974854
	data : 0.11364536285400391
	model : 0.06471285820007325
			 train-loss:  2.1807887325548148 	 ± 0.20114502534401146
	data : 0.11358165740966797
	model : 0.06463346481323243
			 train-loss:  2.1810585163735055 	 ± 0.19979461158430842
	data : 0.11361231803894042
	model : 0.06459984779357911
			 train-loss:  2.180602684020996 	 ± 0.19849691251780796
	data : 0.11371498107910157
	model : 0.06473255157470703
			 train-loss:  2.179619999308335 	 ± 0.19737024837206707
	data : 0.11382522583007812
	model : 0.06483702659606934
			 train-loss:  2.175876592660879 	 ± 0.19878154026576283
	data : 0.11390080451965331
	model : 0.06485948562622071
			 train-loss:  2.1753538999802027 	 ± 0.1975564392859076
	data : 0.11381406784057617
	model : 0.06485109329223633
			 train-loss:  2.173667467093166 	 ± 0.196866327302078
	data : 0.11369123458862304
	model : 0.06478180885314941
			 train-loss:  2.1749107986688614 	 ± 0.1959439216695784
	data : 0.11359610557556152
	model : 0.06466960906982422
			 train-loss:  2.1792914985138694 	 ± 0.1986334883081944
	data : 0.11354117393493653
	model : 0.06465640068054199
			 train-loss:  2.1776390482739703 	 ± 0.19797797572693532
	data : 0.11360201835632325
	model : 0.06471562385559082
			 train-loss:  2.1761436835829033 	 ± 0.19724707367466637
	data : 0.11363520622253417
	model : 0.06480989456176758
			 train-loss:  2.1750810146331787 	 ± 0.19630834309362635
	data : 0.11376066207885742
	model : 0.06481823921203614
			 train-loss:  2.1781561318565816 	 ± 0.19717485276125193
	data : 0.11359605789184571
	model : 0.06477789878845215
			 train-loss:  2.178203904351523 	 ± 0.19602563016704294
	data : 0.11362357139587402
	model : 0.06479010581970215
			 train-loss:  2.1792373273564483 	 ± 0.19513127325112373
	data : 0.11363401412963867
	model : 0.06475815773010254
			 train-loss:  2.1777790757742794 	 ± 0.19449559082601497
	data : 0.11364622116088867
	model : 0.06477761268615723
			 train-loss:  2.1742952874537265 	 ± 0.19614161176769576
	data : 0.1136235237121582
	model : 0.06487998962402344
			 train-loss:  2.177694258424971 	 ± 0.19766711341971313
	data : 0.11386423110961914
	model : 0.06500000953674316
			 train-loss:  2.1751379508238573 	 ± 0.19806828544245147
	data : 0.11390948295593262
	model : 0.06495656967163085
			 train-loss:  2.1765760766423266 	 ± 0.19746601738653025
	data : 0.11390070915222168
	model : 0.0649266242980957
			 train-loss:  2.1770275831222534 	 ± 0.19644924343747344
	data : 0.11389961242675781
	model : 0.0647961139678955
			 train-loss:  2.175640990125372 	 ± 0.19585850592017498
	data : 0.11388859748840333
	model : 0.06470808982849122
			 train-loss:  2.173767055963215 	 ± 0.19567026391706252
	data : 0.11393094062805176
	model : 0.06466183662414551
			 train-loss:  2.1737635048727193 	 ± 0.19464848317533007
	data : 0.11392478942871094
	model : 0.06470870971679688
			 train-loss:  2.174715918363984 	 ± 0.1938672600373542
	data : 0.11388721466064453
	model : 0.06480121612548828
			 train-loss:  2.176861365230716 	 ± 0.19402959686394675
	data : 0.11402645111083984
	model : 0.06490650177001953
			 train-loss:  2.1726124948925443 	 ± 0.19757629148039055
	data : 0.11415963172912598
	model : 0.06496481895446778
			 train-loss:  2.1713627111911773 	 ± 0.19697883392972154
	data : 0.11407928466796875
	model : 0.06497073173522949
			 train-loss:  2.174773894914306 	 ± 0.19894751442289768
	data : 0.11399703025817871
	model : 0.06487560272216797
			 train-loss:  2.1714648803075156 	 ± 0.20074356904052873
	data : 0.11382508277893066
	model : 0.06477775573730468
			 train-loss:  2.169803045328381 	 ± 0.20047052382502148
	data : 0.11386671066284179
	model : 0.0647737979888916
			 train-loss:  2.1670564711093903 	 ± 0.20144230366566965
	data : 0.11377205848693847
	model : 0.06481266021728516
			 train-loss:  2.1654947553362165 	 ± 0.20111237272594393
	data : 0.1138772964477539
	model : 0.06485896110534668
			 train-loss:  2.166253096652481 	 ± 0.2003122618053848
	data : 0.11399693489074707
	model : 0.0649756908416748
			 train-loss:  2.1662569714483815 	 ± 0.19937402990419872
	data : 0.11421747207641601
	model : 0.06512861251831055
			 train-loss:  2.1661267081896463 	 ± 0.19845342990427509
	data : 0.11410226821899414
	model : 0.0651275634765625
			 train-loss:  2.168380461701559 	 ± 0.19892466070057202
	data : 0.11406612396240234
	model : 0.0650179386138916
			 train-loss:  2.168006511168046 	 ± 0.1980568770196506
	data : 0.11379485130310059
	model : 0.06489481925964355
			 train-loss:  2.1695966699101903 	 ± 0.1978668262610155
	data : 0.11376113891601562
	model : 0.06484651565551758
			 train-loss:  2.1703115978411267 	 ± 0.197125468832204
	data : 0.11374092102050781
	model : 0.0647770881652832
			 train-loss:  2.1701713536692933 	 ± 0.19625690620621605
	data : 0.11368784904479981
	model : 0.06480526924133301
			 train-loss:  2.1699541869916414 	 ± 0.1954078709676409
	data : 0.11377172470092774
	model : 0.06489777565002441
			 train-loss:  2.1687886984451956 	 ± 0.19495397571202738
	data : 0.11393966674804687
	model : 0.06503314971923828
			 train-loss:  2.1707490847028534 	 ± 0.1952469302766806
	data : 0.11395845413208008
	model : 0.06504259109497071
			 train-loss:  2.1740847836192856 	 ± 0.19770245087676747
	data : 0.11400032043457031
	model : 0.06490745544433593
			 train-loss:  2.1755308074466253 	 ± 0.19748332798823678
	data : 0.11437869071960449
	model : 0.06482477188110351
			 train-loss:  2.176925575031954 	 ± 0.1972346090274603
	data : 0.11427421569824218
	model : 0.06478219032287598
			 train-loss:  2.176749497652054 	 ± 0.1964204706555941
	data : 0.11425962448120117
	model : 0.06478486061096192
			 train-loss:  2.174797955623343 	 ± 0.19677187806440305
	data : 0.11431217193603516
	model : 0.0648571491241455
			 train-loss:  2.1770061733292754 	 ± 0.19746347766757721
	data : 0.11437010765075684
	model : 0.06496467590332031
			 train-loss:  2.17647466911533 	 ± 0.19674674814630685
	data : 0.11407227516174316
	model : 0.06503481864929199
			 train-loss:  2.176355324445232 	 ± 0.19595627873913854
	data : 0.11425237655639649
	model : 0.06498928070068359
			 train-loss:  2.174801181793213 	 ± 0.19593666536751875
	data : 0.114223051071167
	model : 0.0648813247680664
			 train-loss:  2.1751638688738386 	 ± 0.1951997123345832
	data : 0.1142383098602295
	model : 0.06476864814758301
			 train-loss:  2.180801881579902 	 ± 0.2044702673722378
	data : 0.114093017578125
	model : 0.06472530364990234
			 train-loss:  2.182566162198782 	 ± 0.20463815790133988
	data : 0.11406259536743164
	model : 0.06475439071655273
			 train-loss:  2.1809744566910028 	 ± 0.2046373399052147
	data : 0.11395268440246582
	model : 0.06486005783081054
			 train-loss:  2.1790748632871186 	 ± 0.20498732917684961
	data : 0.11413693428039551
	model : 0.06496267318725586
			 train-loss:  2.1780434473780277 	 ± 0.20454177982252958
	data : 0.11410317420959473
	model : 0.06508774757385254
			 train-loss:  2.178446751652342 	 ± 0.20381780557173051
	data : 0.1140859603881836
	model : 0.06510224342346191
			 train-loss:  2.178244752095158 	 ± 0.20306339005214166
	data : 0.11402177810668945
	model : 0.06497163772583008
			 train-loss:  2.1802458389481503 	 ± 0.20361629797056005
	data : 0.11418137550354004
	model : 0.06488895416259766
			 train-loss:  2.178053539770621 	 ± 0.20444196295385583
	data : 0.11393442153930664
	model : 0.06484022140502929
			 train-loss:  2.178062000695397 	 ± 0.20368897504672037
	data : 0.11388235092163086
	model : 0.06477904319763184
			 train-loss:  2.1785495368233563 	 ± 0.203023850153028
	data : 0.11397886276245117
	model : 0.06481475830078125
			 train-loss:  2.181555910386901 	 ± 0.20532472391667123
	data : 0.11414680480957032
	model : 0.06488428115844727
			 train-loss:  2.1808503394504246 	 ± 0.20475264621590483
	data : 0.11412062644958496
	model : 0.06495294570922852
			 train-loss:  2.1794632477419715 	 ± 0.20467445070366866
	data : 0.11419858932495117
	model : 0.06493902206420898
			 train-loss:  2.1803530353180904 	 ± 0.20421892193624566
	data : 0.11407618522644043
	model : 0.0648564338684082
			 train-loss:  2.180455032368781 	 ± 0.20350217487973177
	data : 0.11408162117004395
	model : 0.06480941772460938
			 train-loss:  2.179144616727229 	 ± 0.20338970892519131
	data : 0.11390218734741211
	model : 0.06487073898315429
			 train-loss:  2.176440211633841 	 ± 0.20524613189103474
	data : 0.11385922431945801
	model : 0.06486701965332031
			 train-loss:  2.174540636457246 	 ± 0.20580344568472833
	data : 0.1139371395111084
	model : 0.06493477821350098
			 train-loss:  2.1752053172621006 	 ± 0.20525354126045886
	data : 0.11406655311584472
	model : 0.06505231857299805
			 train-loss:  2.17556686141864 	 ± 0.20460085171592268
	data : 0.11407146453857422
	model : 0.06508359909057618
			 train-loss:  2.1748649800145947 	 ± 0.20408595788443298
	data : 0.11403284072875977
	model : 0.0650367259979248
			 train-loss:  2.1755952851084253 	 ± 0.20359389948236695
	data : 0.113834810256958
	model : 0.06493339538574219
			 train-loss:  2.175985747973124 	 ± 0.2029700867774629
	data : 0.113773775100708
	model : 0.06484808921813964
			 train-loss:  2.177032685437739 	 ± 0.20270283908447964
	data : 0.11391315460205079
	model : 0.064811372756958
			 train-loss:  2.1775090161122774 	 ± 0.20211972395076788
	data : 0.11379170417785645
	model : 0.06484518051147461
			 train-loss:  2.179927769829245 	 ± 0.20365320999814782
	data : 0.11405658721923828
	model : 0.06484751701354981
			 train-loss:  2.179003320731126 	 ± 0.2033127366948737
	data : 0.11421294212341308
	model : 0.06494102478027344
			 train-loss:  2.18162490014107 	 ± 0.20525052275613564
	data : 0.11427764892578125
	model : 0.06497540473937988
			 train-loss:  2.180881989307893 	 ± 0.20480057201324742
	data : 0.11405272483825683
	model : 0.06489839553833007
			 train-loss:  2.182101143393547 	 ± 0.20471440644563707
	data : 0.11421899795532227
	model : 0.06484050750732422
			 train-loss:  2.1814450644239596 	 ± 0.20423106226564572
	data : 0.11401410102844238
	model : 0.06481881141662597
			 train-loss:  2.184421383359897 	 ± 0.2069967068611243
	data : 0.1140749454498291
	model : 0.06482176780700684
			 train-loss:  2.1848237738013268 	 ± 0.20641120099206964
	data : 0.11409401893615723
	model : 0.06488099098205566
			 train-loss:  2.1902691074039624 	 ± 0.21699128606042717
	data : 0.11412091255187988
	model : 0.06493206024169922
			 train-loss:  2.191422834808444 	 ± 0.21681529864305005
	data : 0.114052152633667
	model : 0.06492629051208496
			 train-loss:  2.1906251600183593 	 ± 0.2163875086709426
	data : 0.11409416198730468
	model : 0.06491851806640625
			 train-loss:  2.1946559504764838 	 ± 0.2217799578944961
	data : 0.11397957801818848
	model : 0.06477766036987305
			 train-loss:  2.193054868958213 	 ± 0.22205553000784772
	data : 0.11384172439575195
	model : 0.06475791931152344
			 train-loss:  2.196491332657366 	 ± 0.22574354907915617
	data : 0.11390914916992187
	model : 0.06477570533752441
			 train-loss:  2.195685513005285 	 ± 0.22530599342690397
	data : 0.113917875289917
	model : 0.06482925415039062
			 train-loss:  2.196250325867108 	 ± 0.2247529899062279
	data : 0.11397628784179688
	model : 0.06485867500305176
			 train-loss:  2.196074686812226 	 ± 0.22409861726919048
	data : 0.11403369903564453
	model : 0.06499080657958985
			 train-loss:  2.194963166994207 	 ± 0.22390527656524226
	data : 0.11415958404541016
	model : 0.06492877006530762
			 train-loss:  2.1963634483995493 	 ± 0.22399492861988474
	data : 0.11422648429870605
	model : 0.06486358642578124
			 train-loss:  2.1954196019228114 	 ± 0.22368360472232543
	data : 0.11422080993652343
	model : 0.06487545967102051
			 train-loss:  2.19580137867459 	 ± 0.2230923770471852
	data : 0.11405138969421387
	model : 0.06488022804260254
			 train-loss:  2.1970178498618904 	 ± 0.2230250631347706
	data : 0.11411280632019043
	model : 0.06489801406860352
			 train-loss:  2.194811934743609 	 ± 0.22428250911691955
	data : 0.11416077613830566
	model : 0.06498146057128906
			 train-loss:  2.193931805139238 	 ± 0.22394730036369315
	data : 0.1142350673675537
	model : 0.06509909629821778
			 train-loss:  2.1939600126891485 	 ± 0.22331409835106775
	data : 0.11422338485717773
	model : 0.06507997512817383
			 train-loss:  2.1935627895794556 	 ± 0.2227486267305017
	data : 0.11433796882629395
	model : 0.06501379013061523
			 train-loss:  2.1944574051063155 	 ± 0.22244599573161913
	data : 0.11417880058288574
	model : 0.06491813659667969
			 train-loss:  2.1928515950838725 	 ± 0.22286519201616706
	data : 0.11460299491882324
	model : 0.06485137939453126
			 train-loss:  2.1913370625090205 	 ± 0.2231756385452628
	data : 0.11432061195373536
	model : 0.0648681640625
			 train-loss:  2.1914236480063134 	 ± 0.22256472263514562
	data : 0.11430873870849609
	model : 0.0648773193359375
			 train-loss:  2.190084205299127 	 ± 0.2226901430872577
	data : 0.11437401771545411
	model : 0.06498265266418457
			 train-loss:  2.1897453685169634 	 ± 0.22213147990564652
	data : 0.11451621055603027
	model : 0.06507401466369629
			 train-loss:  2.1899105039802755 	 ± 0.22154163569984944
	data : 0.11404128074645996
	model : 0.06509394645690918
			 train-loss:  2.191047174315299 	 ± 0.22148554182013738
	data : 0.11420340538024902
	model : 0.06502337455749511
			 train-loss:  2.1914208362446748 	 ± 0.22095131688755465
	data : 0.11426348686218261
	model : 0.06499347686767579
			 train-loss:  2.192250333567883 	 ± 0.22065465014808427
	data : 0.11408443450927734
	model : 0.06485047340393066
			 train-loss:  2.1918107422571333 	 ± 0.22015265811008553
	data : 0.11394739151000977
	model : 0.06479129791259766
			 train-loss:  2.1902644690714386 	 ± 0.2205991703716909
	data : 0.1139634132385254
	model : 0.06483669281005859
			 train-loss:  2.189586774841029 	 ± 0.22021914053784816
	data : 0.11401662826538086
	model : 0.0649193286895752
			 train-loss:  2.189937603349487 	 ± 0.21969841256003642
	data : 0.11399521827697753
	model : 0.06497068405151367
			 train-loss:  2.1892860584308447 	 ± 0.21931440544312106
	data : 0.11410703659057617
	model : 0.06509761810302735
			 train-loss:  2.1895681232521214 	 ± 0.21878352685100555
	data : 0.1141974925994873
	model : 0.06510605812072753
			 train-loss:  2.1908190134244085 	 ± 0.21891624118411676
	data : 0.11425704956054687
	model : 0.0649789810180664
			 train-loss:  2.1907665334185777 	 ± 0.21835829701103035
	data : 0.11416730880737305
	model : 0.06484980583190918
			 train-loss:  2.191501205342675 	 ± 0.21804610351223538
	data : 0.11409730911254883
	model : 0.06482319831848145
			 train-loss:  2.1911818072049303 	 ± 0.21754098122380458
	data : 0.11409025192260742
	model : 0.06483950614929199
			 train-loss:  2.1910190516380808 	 ± 0.21700579247618454
	data : 0.11469168663024902
	model : 0.06487879753112794
			 train-loss:  2.1899559223651885 	 ± 0.21698150825519896
	data : 0.11463932991027832
	model : 0.06501584053039551
			 train-loss:  2.188526962526995 	 ± 0.21738244285691957
	data : 0.11465821266174317
	model : 0.0650825023651123
			 train-loss:  2.19157070098537 	 ± 0.22109573333112342
	data : 0.11468605995178223
	model : 0.06507792472839355
			 train-loss:  2.192519993617617 	 ± 0.22096278501660824
	data : 0.11467099189758301
	model : 0.06498179435729981
			 train-loss:  2.193796603118672 	 ± 0.22116973545310317
	data : 0.11391491889953613
	model : 0.06488900184631348
			 train-loss:  2.1936172915668024 	 ± 0.22064450169952463
	data : 0.11371674537658691
	model : 0.06482939720153809
			 train-loss:  2.1953898251635358 	 ± 0.2215665825412238
	data : 0.11379642486572265
	model : 0.06479148864746094
			 train-loss:  2.1970719738283018 	 ± 0.22234544037349435
	data : 0.11384806632995606
	model : 0.06478452682495117
			 train-loss:  2.1973065539048267 	 ± 0.22183598749595243
	data : 0.11396408081054688
	model : 0.06482739448547363
			 train-loss:  2.1966324981890226 	 ± 0.2215180581038457
	data : 0.11409034729003906
	model : 0.0649001121520996
			 train-loss:  2.1954121549924213 	 ± 0.22169310510206836
	data : 0.11441988945007324
	model : 0.06488142013549805
			 train-loss:  2.19653929296828 	 ± 0.2217694684475658
	data : 0.11426610946655273
	model : 0.0648129940032959
			 train-loss:  2.1952250127522452 	 ± 0.2220679516559857
	data : 0.114265775680542
	model : 0.06480412483215332
			 train-loss:  2.1947148305149704 	 ± 0.22167055243680975
	data : 0.11416282653808593
	model : 0.06485052108764648
			 train-loss:  2.194139918434286 	 ± 0.221311137005289
	data : 0.11417217254638672
	model : 0.06485953330993652
			 train-loss:  2.1934340421543563 	 ± 0.2210371908835346
	data : 0.11407909393310547
	model : 0.06493754386901855
			 train-loss:  2.1936478394049184 	 ± 0.22054721806181868
	data : 0.11422152519226074
	model : 0.06504659652709961
			 train-loss:  2.193543914276334 	 ± 0.22004375898221032
	data : 0.11409425735473633
	model : 0.06506919860839844
			 train-loss:  2.193075481904756 	 ± 0.21964691042023565
	data : 0.11414813995361328
	model : 0.06499519348144531
			 train-loss:  2.1909230446706625 	 ± 0.22143726082927095
	data : 0.11413331031799316
	model : 0.0649289608001709
			 train-loss:  2.1931614740328356 	 ± 0.2234029778860361
	data : 0.11411194801330567
	model : 0.06529912948608399
			 train-loss:  2.193493253505068 	 ± 0.22295128508378897
	data : 0.1135183334350586
	model : 0.06557273864746094
			 train-loss:  2.194488227367401 	 ± 0.22293979659112131
	data : 0.11339597702026367
	model : 0.06549243927001953
			 train-loss:  2.1960939254461382 	 ± 0.22372225731408246
	data : 0.11340961456298829
	model : 0.06543655395507812
			 train-loss:  2.197587985545397 	 ± 0.22433454400311312
	data : 0.11365633010864258
	model : 0.06533393859863282
			 train-loss:  2.197569743792216 	 ± 0.22383563415043392
	data : 0.11373963356018066
	model : 0.06472086906433105
			 train-loss:  2.196544948932344 	 ± 0.22386825396430163
	data : 0.11441650390625
	model : 0.06419596672058106
			 train-loss:  2.1960643388101184 	 ± 0.22349142774394956
	data : 0.1146662712097168
	model : 0.06396803855895997
			 train-loss:  2.197896372853664 	 ± 0.22470255323045182
	data : 0.11482906341552734
	model : 0.06377053260803223
			 train-loss:  2.1975773442780606 	 ± 0.22426314307132555
	data : 0.1147383213043213
	model : 0.06374926567077636
			 train-loss:  2.195415593230206 	 ± 0.22615358352072154
	data : 0.11474981307983398
	model : 0.06373271942138672
			 train-loss:  2.1947965002679206 	 ± 0.22585877887677575
	data : 0.1147383689880371
	model : 0.06374363899230957
			 train-loss:  2.1952294871724884 	 ± 0.2254675480293959
	data : 0.11477847099304199
	model : 0.06381902694702149
			 train-loss:  2.1937757817460746 	 ± 0.22607015018487503
	data : 0.11474547386169434
	model : 0.06381721496582031
			 train-loss:  2.1921938189074526 	 ± 0.22687532054421503
	data : 0.11479291915893555
	model : 0.0637740135192871
			 train-loss:  2.1924855262675185 	 ± 0.22643606457585339
	data : 0.11475944519042969
	model : 0.06375684738159179
			 train-loss:  2.1937578916549683 	 ± 0.22679611184087153
	data : 0.114801025390625
	model : 0.06374015808105468
			 train-loss:  2.1941650809114996 	 ± 0.2264035651878357
	data : 0.11486797332763672
	model : 0.06372671127319336
			 train-loss:  2.1934138065626643 	 ± 0.22622327043123527
	data : 0.1148949146270752
	model : 0.06384730339050293
			 train-loss:  2.194549564537144 	 ± 0.22642845603204922
	data : 0.11489357948303222
	model : 0.06380691528320312
			 train-loss:  2.1945955316225687 	 ± 0.22595735514583976
	data : 0.11483564376831054
	model : 0.06373276710510253
			 train-loss:  2.194321711528351 	 ± 0.22552797452816878
	data : 0.11483025550842285
	model : 0.06374168395996094
			 train-loss:  2.193780321720218 	 ± 0.22521840058477677
	data : 0.11492390632629394
	model : 0.06376943588256836
			 train-loss:  2.1933191666387235 	 ± 0.22486897224169533
	data : 0.11484775543212891
	model : 0.06372485160827637
			 train-loss:  2.194548739761603 	 ± 0.22522476842857936
	data : 0.11490311622619628
	model : 0.06377067565917968
			 train-loss:  2.1950091595552403 	 ± 0.2248796907075931
	data : 0.11499056816101075
	model : 0.06385416984558105
			 train-loss:  2.19515211213895 	 ± 0.2244333070469765
	data : 0.11517586708068847
	model : 0.06381440162658691
			 train-loss:  2.1960264873890742 	 ± 0.22439798534179464
	data : 0.1150094985961914
	model : 0.06379899978637696
			 train-loss:  2.1945448609129077 	 ± 0.22515246449360626
	data : 0.11496849060058593
	model : 0.06376495361328124
			 train-loss:  2.1946423508556014 	 ± 0.22470514112184836
	data : 0.11494460105895996
	model : 0.06379141807556152
			 train-loss:  2.1963107562065125 	 ± 0.2257953528576832
	data : 0.11493844985961914
	model : 0.06376380920410156
			 train-loss:  2.1972493978135614 	 ± 0.22583330513893016
	data : 0.11470794677734375
	model : 0.06381182670593262
			 train-loss:  2.1970471649888963 	 ± 0.22540754973902774
	data : 0.11478428840637207
	model : 0.06375961303710938
			 train-loss:  2.1979665261483476 	 ± 0.2254345488230092
	data : 0.11483120918273926
	model : 0.06374731063842773
			 train-loss:  2.198738467505598 	 ± 0.22532513222263803
	data : 0.11483187675476074
	model : 0.06366190910339356
			 train-loss:  2.198674843825546 	 ± 0.22488517026950103
	data : 0.11473393440246582
	model : 0.06370453834533692
			 train-loss:  2.1989599536173046 	 ± 0.22449168366517894
	data : 0.1146583080291748
	model : 0.055284643173217775
#epoch  11    val-loss:  2.4071284971739115  train-loss:  2.1989599536173046  lr:  0.00125
			 train-loss:  2.0546462535858154 	 ± 0.0
	data : 5.572035551071167
	model : 0.07127833366394043
			 train-loss:  2.038825035095215 	 ± 0.015821218490600586
	data : 2.850332260131836
	model : 0.06807208061218262
			 train-loss:  2.139538606007894 	 ± 0.14301510655050165
	data : 1.9383502006530762
	model : 0.06689453125
			 train-loss:  2.1273319125175476 	 ± 0.12564633151883223
	data : 1.4824716448783875
	model : 0.06635421514511108
			 train-loss:  2.104454231262207 	 ± 0.1213390032009332
	data : 1.2087741374969483
	model : 0.06606144905090332
			 train-loss:  2.1267480850219727 	 ± 0.12146758549288973
	data : 0.1172091007232666
	model : 0.06482472419738769
			 train-loss:  2.1312120301382884 	 ± 0.11298746567960342
	data : 0.11425313949584961
	model : 0.06473045349121094
			 train-loss:  2.1184533536434174 	 ± 0.11094991328035146
	data : 0.11401753425598145
	model : 0.06476612091064453
			 train-loss:  2.1177531348334417 	 ± 0.10462332866836195
	data : 0.11387505531311035
	model : 0.06477408409118653
			 train-loss:  2.1339845657348633 	 ± 0.11055573686359225
	data : 0.11385998725891114
	model : 0.06477537155151367
			 train-loss:  2.175835674459284 	 ± 0.1691939158307252
	data : 0.11390876770019531
	model : 0.06474652290344238
			 train-loss:  2.185712476571401 	 ± 0.16526976778650235
	data : 0.11400032043457031
	model : 0.06485190391540527
			 train-loss:  2.1960643438192515 	 ± 0.16278496764635514
	data : 0.11412091255187988
	model : 0.06486520767211915
			 train-loss:  2.197632380894252 	 ± 0.15696537032097344
	data : 0.1139753818511963
	model : 0.0648460865020752
			 train-loss:  2.1808998743693033 	 ± 0.16405871050482107
	data : 0.1141042709350586
	model : 0.06485872268676758
			 train-loss:  2.187533050775528 	 ± 0.1609131509549936
	data : 0.11398677825927735
	model : 0.06487073898315429
			 train-loss:  2.183016706915463 	 ± 0.15715050051964832
	data : 0.1138908863067627
	model : 0.06491351127624512
			 train-loss:  2.163841817114088 	 ± 0.17197314630368846
	data : 0.11396350860595703
	model : 0.06500349044799805
			 train-loss:  2.1791289480108964 	 ± 0.17951248216295781
	data : 0.11420168876647949
	model : 0.06507081985473633
			 train-loss:  2.167816960811615 	 ± 0.18178216283020826
	data : 0.11411614418029785
	model : 0.06511869430541992
			 train-loss:  2.1911280495779857 	 ± 0.20576523555735862
	data : 0.11411509513854981
	model : 0.06505560874938965
			 train-loss:  2.1830117485739966 	 ± 0.2044460277123209
	data : 0.11411142349243164
	model : 0.0649454116821289
			 train-loss:  2.1818857296653418 	 ± 0.2000218996971447
	data : 0.11405487060546875
	model : 0.06485280990600586
			 train-loss:  2.1901510258515677 	 ± 0.19978232269375412
	data : 0.11401762962341308
	model : 0.06484866142272949
			 train-loss:  2.2056557083129884 	 ± 0.20996652514146757
	data : 0.11392073631286621
	model : 0.06481313705444336
			 train-loss:  2.219337069071256 	 ± 0.21695580043730467
	data : 0.11400704383850098
	model : 0.06485772132873535
			 train-loss:  2.2174160922015154 	 ± 0.213125401246613
	data : 0.11407279968261719
	model : 0.06496243476867676
			 train-loss:  2.201590380498341 	 ± 0.22486093891684064
	data : 0.1141787052154541
	model : 0.06500821113586426
			 train-loss:  2.1963670705926828 	 ± 0.22267202884810844
	data : 0.11412386894226074
	model : 0.06499395370483399
			 train-loss:  2.192558292547862 	 ± 0.21988808122755327
	data : 0.11424593925476074
	model : 0.06498799324035645
			 train-loss:  2.1894362165081884 	 ± 0.21698729646625203
	data : 0.11416969299316407
	model : 0.06503806114196778
			 train-loss:  2.1877803318202496 	 ± 0.21376886686666693
	data : 0.11412019729614258
	model : 0.06499810218811035
			 train-loss:  2.1890736962809707 	 ± 0.2106321350260686
	data : 0.11406569480895996
	model : 0.06499509811401367
			 train-loss:  2.182935416698456 	 ± 0.21048612001323477
	data : 0.11413946151733398
	model : 0.06502127647399902
			 train-loss:  2.1909245593207225 	 ± 0.21262329157643967
	data : 0.11423983573913574
	model : 0.0650136947631836
			 train-loss:  2.18062401149008 	 ± 0.21832639073556612
	data : 0.1142949104309082
	model : 0.06499409675598145
			 train-loss:  2.1771246968088924 	 ± 0.21637689058372897
	data : 0.11431012153625489
	model : 0.06495318412780762
			 train-loss:  2.18332890146657 	 ± 0.21682041109442904
	data : 0.1143181324005127
	model : 0.0648488998413086
			 train-loss:  2.1757710713606615 	 ± 0.21903485642819465
	data : 0.11401176452636719
	model : 0.0648343563079834
			 train-loss:  2.1805586934089662 	 ± 0.21833642227644
	data : 0.11401190757751464
	model : 0.06483674049377441
			 train-loss:  2.1921142078027493 	 ± 0.2277043862846013
	data : 0.11396050453186035
	model : 0.06479821205139161
			 train-loss:  2.1827054563022794 	 ± 0.23290403492408304
	data : 0.11408309936523438
	model : 0.06486649513244629
			 train-loss:  2.1785648395848827 	 ± 0.23173880138047118
	data : 0.11414718627929688
	model : 0.06496891975402833
			 train-loss:  2.1797036176378075 	 ± 0.22921194408256068
	data : 0.11440753936767578
	model : 0.06493325233459472
			 train-loss:  2.1904456271065604 	 ± 0.23758744790776976
	data : 0.11420841217041015
	model : 0.06482677459716797
			 train-loss:  2.190036791822185 	 ± 0.23500678926221924
	data : 0.11419854164123536
	model : 0.06477541923522949
			 train-loss:  2.194377511105639 	 ± 0.2343498408897065
	data : 0.11409330368041992
	model : 0.06475439071655273
			 train-loss:  2.1931527331471443 	 ± 0.2320478145465509
	data : 0.11410837173461914
	model : 0.06473698616027831
			 train-loss:  2.192351825383245 	 ± 0.22973479534219549
	data : 0.11407942771911621
	model : 0.06479997634887695
			 train-loss:  2.1875620317459106 	 ± 0.2298840549025227
	data : 0.1143221378326416
	model : 0.06492419242858886
			 train-loss:  2.1894036695068957 	 ± 0.22799133929673018
	data : 0.11445322036743164
	model : 0.06498823165893555
			 train-loss:  2.195325943139883 	 ± 0.22971541961102754
	data : 0.11454153060913086
	model : 0.06494855880737305
			 train-loss:  2.197662110598582 	 ± 0.2281607514254557
	data : 0.11432361602783203
	model : 0.06494340896606446
			 train-loss:  2.195216827922397 	 ± 0.22673820251506968
	data : 0.11439471244812012
	model : 0.06486406326293945
			 train-loss:  2.197069670937278 	 ± 0.22507968547741006
	data : 0.11411461830139161
	model : 0.06484408378601074
			 train-loss:  2.200371299471174 	 ± 0.22440086412048393
	data : 0.11410994529724121
	model : 0.06482090950012206
			 train-loss:  2.2051105750234505 	 ± 0.22523346705220473
	data : 0.11407690048217774
	model : 0.06485538482666016
			 train-loss:  2.2074618092898666 	 ± 0.22398788087194077
	data : 0.11422882080078126
	model : 0.06485915184020996
			 train-loss:  2.209966437291291 	 ± 0.22289922719232788
	data : 0.11414737701416015
	model : 0.06488394737243652
			 train-loss:  2.2028721511363982 	 ± 0.2276519329847443
	data : 0.1142913818359375
	model : 0.06488180160522461
			 train-loss:  2.1979465699586713 	 ± 0.2289792279316949
	data : 0.11417417526245117
	model : 0.06480250358581544
			 train-loss:  2.1978978745398985 	 ± 0.2271254330270869
	data : 0.11398782730102539
	model : 0.06476125717163086
			 train-loss:  2.1961252859660556 	 ± 0.22574752686246632
	data : 0.11399030685424805
	model : 0.06478238105773926
			 train-loss:  2.1947751585394144 	 ± 0.22423314798746635
	data : 0.11410655975341796
	model : 0.06479229927062988
			 train-loss:  2.192056958491986 	 ± 0.22356169074049398
	data : 0.11412639617919922
	model : 0.06478257179260254
			 train-loss:  2.185052938533552 	 ± 0.2289349659496701
	data : 0.11425738334655762
	model : 0.06493129730224609
			 train-loss:  2.185433499848665 	 ± 0.22724110591218893
	data : 0.11444144248962403
	model : 0.06498398780822753
			 train-loss:  2.1867141635978924 	 ± 0.2258074774864654
	data : 0.11437435150146484
	model : 0.06493144035339356
			 train-loss:  2.1827318962069526 	 ± 0.22655776409671483
	data : 0.11425485610961914
	model : 0.06493611335754394
			 train-loss:  2.182230269908905 	 ± 0.22497226433486217
	data : 0.11426525115966797
	model : 0.06491689682006836
			 train-loss:  2.179258559791135 	 ± 0.2247617440785178
	data : 0.11429057121276856
	model : 0.0648777961730957
			 train-loss:  2.176335064901246 	 ± 0.22455072767788375
	data : 0.1142768383026123
	model : 0.06491551399230958
			 train-loss:  2.1746624446895026 	 ± 0.22345857395662314
	data : 0.11432065963745117
	model : 0.0650026798248291
			 train-loss:  2.182265315506909 	 ± 0.23125444891386812
	data : 0.11444053649902344
	model : 0.06496996879577636
			 train-loss:  2.1783341948191324 	 ± 0.2321834326640661
	data : 0.11441693305969239
	model : 0.06502032279968262
			 train-loss:  2.1765085521497225 	 ± 0.2311921027342072
	data : 0.11435370445251465
	model : 0.06497583389282227
			 train-loss:  2.178078335601014 	 ± 0.23009327673222735
	data : 0.11425256729125977
	model : 0.06481347084045411
			 train-loss:  2.1787621944378586 	 ± 0.22869230574736424
	data : 0.11426753997802734
	model : 0.06470346450805664
			 train-loss:  2.1829640352273287 	 ± 0.23025044968532296
	data : 0.11418828964233399
	model : 0.06471610069274902
			 train-loss:  2.1843748688697815 	 ± 0.22915022230909396
	data : 0.11417956352233886
	model : 0.06468052864074707
			 train-loss:  2.1824257138334673 	 ± 0.22839766061829736
	data : 0.11423439979553222
	model : 0.0647036075592041
			 train-loss:  2.1797165085629717 	 ± 0.228306482816298
	data : 0.11437726020812988
	model : 0.06482076644897461
			 train-loss:  2.187579330191555 	 ± 0.23783485225462683
	data : 0.11434636116027833
	model : 0.0648829460144043
			 train-loss:  2.1831715745585307 	 ± 0.23980110265172
	data : 0.1143402099609375
	model : 0.06487808227539063
			 train-loss:  2.1866435457678404 	 ± 0.24050078909713443
	data : 0.11423182487487793
	model : 0.06479263305664062
			 train-loss:  2.1824060151743336 	 ± 0.2422692317784366
	data : 0.11412081718444825
	model : 0.06476445198059082
			 train-loss:  2.181391759850513 	 ± 0.24105642908870734
	data : 0.11411442756652831
	model : 0.06478548049926758
			 train-loss:  2.18220856514844 	 ± 0.2398039311510766
	data : 0.11412248611450196
	model : 0.06480140686035156
			 train-loss:  2.183818080452051 	 ± 0.23893044778639522
	data : 0.11409435272216797
	model : 0.06486334800720214
			 train-loss:  2.1862940311431887 	 ± 0.2387447381196786
	data : 0.11423649787902831
	model : 0.06495404243469238
			 train-loss:  2.1893941963111963 	 ± 0.23924397371604966
	data : 0.11425180435180664
	model : 0.06499371528625489
			 train-loss:  2.1938035514043723 	 ± 0.2416294472903429
	data : 0.1141280174255371
	model : 0.06492576599121094
			 train-loss:  2.193071085919616 	 ± 0.24042952165301015
	data : 0.11416134834289551
	model : 0.06482019424438476
			 train-loss:  2.191397575621909 	 ± 0.23969116144484873
	data : 0.11415305137634277
	model : 0.06478853225708008
			 train-loss:  2.1888886715236464 	 ± 0.23966390822882072
	data : 0.11423239707946778
	model : 0.06482405662536621
			 train-loss:  2.1883239708840847 	 ± 0.23847591584245262
	data : 0.11428756713867187
	model : 0.06483182907104493
			 train-loss:  2.1894063249076763 	 ± 0.23748037598765404
	data : 0.11433086395263672
	model : 0.06489486694335937
			 train-loss:  2.1886388902761498 	 ± 0.23638650307436776
	data : 0.11440882682800294
	model : 0.06498680114746094
			 train-loss:  2.189196137466816 	 ± 0.23525428825159772
	data : 0.11445803642272949
	model : 0.06499924659729003
			 train-loss:  2.1934179818630217 	 ± 0.23781443896414253
	data : 0.11439990997314453
	model : 0.0649592399597168
			 train-loss:  2.189346442128172 	 ± 0.24011141885022
	data : 0.11434626579284668
	model : 0.06487083435058594
			 train-loss:  2.1878141620579887 	 ± 0.23942723161279278
	data : 0.1143491268157959
	model : 0.06480975151062011
			 train-loss:  2.185245143557058 	 ± 0.23967066420724434
	data : 0.11426959037780762
	model : 0.06484131813049317
			 train-loss:  2.1872046773250284 	 ± 0.23934326118085608
	data : 0.11436071395874023
	model : 0.06482062339782715
			 train-loss:  2.1876368340991794 	 ± 0.2382415714284496
	data : 0.11436963081359863
	model : 0.06483545303344726
			 train-loss:  2.1891005803953925 	 ± 0.23758903961101618
	data : 0.11431403160095215
	model : 0.06491751670837402
			 train-loss:  2.1876602573929547 	 ± 0.23694070018082558
	data : 0.1143406867980957
	model : 0.06493792533874512
			 train-loss:  2.1867864463064404 	 ± 0.236014346383435
	data : 0.11443634033203125
	model : 0.06491103172302246
			 train-loss:  2.190482286138272 	 ± 0.2380481761299893
	data : 0.11415796279907227
	model : 0.06483712196350097
			 train-loss:  2.19169481234117 	 ± 0.23730156798749116
	data : 0.11402225494384766
	model : 0.06487550735473632
			 train-loss:  2.194453866632135 	 ± 0.23799596721379362
	data : 0.11413011550903321
	model : 0.06487698554992676
			 train-loss:  2.1937318295240402 	 ± 0.23705319238772551
	data : 0.11424250602722168
	model : 0.06490607261657715
			 train-loss:  2.191202662687386 	 ± 0.23751494854332086
	data : 0.11422562599182129
	model : 0.06492781639099121
			 train-loss:  2.1906768595963193 	 ± 0.23653696922880227
	data : 0.1143460750579834
	model : 0.06498808860778808
			 train-loss:  2.188994506131048 	 ± 0.23619033438198464
	data : 0.11440587043762207
	model : 0.06498856544494629
			 train-loss:  2.187652274452407 	 ± 0.2356101519973214
	data : 0.11436285972595214
	model : 0.06500439643859864
			 train-loss:  2.18584819838532 	 ± 0.23540438621700366
	data : 0.11424951553344727
	model : 0.06494579315185547
			 train-loss:  2.183093614497427 	 ± 0.23629085718494847
	data : 0.11425762176513672
	model : 0.06492633819580078
			 train-loss:  2.1841619535654533 	 ± 0.23558196093886696
	data : 0.11440253257751465
	model : 0.06491303443908691
			 train-loss:  2.1844668726126355 	 ± 0.23462189564606203
	data : 0.11441779136657715
	model : 0.06486506462097168
			 train-loss:  2.1812204240767423 	 ± 0.23634133751358663
	data : 0.11450772285461426
	model : 0.06492824554443359
			 train-loss:  2.1801588877302702 	 ± 0.23566020384710348
	data : 0.1146468162536621
	model : 0.06499080657958985
			 train-loss:  2.1813332927905447 	 ± 0.23505847652783157
	data : 0.11464056968688965
	model : 0.06496691703796387
			 train-loss:  2.1821931264092846 	 ± 0.23430287752322712
	data : 0.11458783149719239
	model : 0.06497273445129395
			 train-loss:  2.1835328664779663 	 ± 0.23384016707217212
	data : 0.11460943222045898
	model : 0.06505160331726074
			 train-loss:  2.183232304595766 	 ± 0.23293462162334078
	data : 0.11449155807495118
	model : 0.06500101089477539
			 train-loss:  2.1824449019169245 	 ± 0.2321840342138721
	data : 0.11449832916259765
	model : 0.06501169204711914
			 train-loss:  2.181293790228665 	 ± 0.23163881526268118
	data : 0.1144871711730957
	model : 0.06502490043640137
			 train-loss:  2.184766123461169 	 ± 0.23405962298947414
	data : 0.11447463035583497
	model : 0.06507925987243653
			 train-loss:  2.185100475641397 	 ± 0.2331885793342845
	data : 0.11454715728759765
	model : 0.0650134563446045
			 train-loss:  2.184293005302662 	 ± 0.23247921127314117
	data : 0.11459870338439941
	model : 0.06495652198791504
			 train-loss:  2.184554512753631 	 ± 0.23161627413399474
	data : 0.11450791358947754
	model : 0.06492266654968262
			 train-loss:  2.1845197937542333 	 ± 0.23074423813836434
	data : 0.1144437313079834
	model : 0.06488924026489258
			 train-loss:  2.1835633293906254 	 ± 0.23014612735452966
	data : 0.11446037292480468
	model : 0.06484036445617676
			 train-loss:  2.188543702054907 	 ± 0.23642893435893073
	data : 0.11437506675720215
	model : 0.06482982635498047
			 train-loss:  2.1862056676079247 	 ± 0.23711935026085218
	data : 0.11440706253051758
	model : 0.0648573398590088
			 train-loss:  2.185660433595198 	 ± 0.23633791618451636
	data : 0.11439967155456543
	model : 0.0649118423461914
			 train-loss:  2.1845185048338296 	 ± 0.23585908505367378
	data : 0.11445751190185546
	model : 0.06500191688537597
			 train-loss:  2.187264764909264 	 ± 0.23721316161215644
	data : 0.11445708274841308
	model : 0.06499624252319336
			 train-loss:  2.1873724102973937 	 ± 0.23636786062601367
	data : 0.11450681686401368
	model : 0.06498699188232422
			 train-loss:  2.19132566113844 	 ± 0.24012803454561066
	data : 0.11445260047912598
	model : 0.06497893333435059
			 train-loss:  2.190987039619768 	 ± 0.23931480096890836
	data : 0.11439108848571777
	model : 0.0649306297302246
			 train-loss:  2.1930735144581828 	 ± 0.2397691661061861
	data : 0.11426148414611817
	model : 0.06487097740173339
			 train-loss:  2.1917386237117977 	 ± 0.23946782363007124
	data : 0.11435680389404297
	model : 0.06493816375732422
			 train-loss:  2.1943929195404053 	 ± 0.24075688597153735
	data : 0.11439714431762696
	model : 0.06493816375732422
			 train-loss:  2.1974288012883436 	 ± 0.2426999569157419
	data : 0.11442375183105469
	model : 0.06494183540344238
			 train-loss:  2.1959400160783002 	 ± 0.24254107458479618
	data : 0.11447200775146485
	model : 0.06496229171752929
			 train-loss:  2.199383682495839 	 ± 0.24529971048475244
	data : 0.11460490226745605
	model : 0.06495394706726074
			 train-loss:  2.1994964612410373 	 ± 0.24447902119748222
	data : 0.11442503929138184
	model : 0.06489319801330566
			 train-loss:  2.1998670371373494 	 ± 0.2437047123906005
	data : 0.1142357349395752
	model : 0.06485919952392578
			 train-loss:  2.1998942308868004 	 ± 0.2428966309980823
	data : 0.1141690731048584
	model : 0.06486907005310058
			 train-loss:  2.199273521962919 	 ± 0.24221643367109139
	data : 0.11422419548034668
	model : 0.06487784385681153
			 train-loss:  2.198103831484427 	 ± 0.24185389649601508
	data : 0.11422953605651856
	model : 0.06491961479187011
			 train-loss:  2.1990258523396085 	 ± 0.24133700361906635
	data : 0.11431770324707032
	model : 0.0649716854095459
			 train-loss:  2.1971567284676334 	 ± 0.24167292480757244
	data : 0.11431927680969238
	model : 0.0650167465209961
			 train-loss:  2.1978394870574656 	 ± 0.24104701015546323
	data : 0.11444230079650879
	model : 0.06497673988342285
			 train-loss:  2.1978364955088137 	 ± 0.24027812111713737
	data : 0.11436071395874023
	model : 0.06492304801940918
			 train-loss:  2.19745109730129 	 ± 0.23956521575612963
	data : 0.11422967910766602
	model : 0.06487951278686524
			 train-loss:  2.195943797159495 	 ± 0.23956107468449328
	data : 0.11426544189453125
	model : 0.06487250328063965
			 train-loss:  2.1959335021674633 	 ± 0.23881130820744298
	data : 0.1144040584564209
	model : 0.06487908363342285
			 train-loss:  2.197549010655895 	 ± 0.23894390749777067
	data : 0.11438074111938476
	model : 0.06494803428649902
			 train-loss:  2.197475535634123 	 ± 0.23820710909657244
	data : 0.11452574729919433
	model : 0.06500601768493652
			 train-loss:  2.1960898930309742 	 ± 0.23812927881254936
	data : 0.11459550857543946
	model : 0.06504745483398437
			 train-loss:  2.1957975851326452 	 ± 0.23743149583619602
	data : 0.11455135345458985
	model : 0.06501302719116211
			 train-loss:  2.195437044808359 	 ± 0.2367559388574956
	data : 0.11449308395385742
	model : 0.06496982574462891
			 train-loss:  2.193888992430216 	 ± 0.23687786002572456
	data : 0.11449875831604003
	model : 0.0649451732635498
			 train-loss:  2.2002216977273634 	 ± 0.24986446285051941
	data : 0.11438899040222168
	model : 0.0648953914642334
			 train-loss:  2.1999423752228418 	 ± 0.24914585844110868
	data : 0.11439871788024902
	model : 0.06487011909484863
			 train-loss:  2.2005648422523363 	 ± 0.24853863517840374
	data : 0.11443729400634765
	model : 0.0648876667022705
			 train-loss:  2.2052253197221194 	 ± 0.25510542953591825
	data : 0.1144864559173584
	model : 0.06491618156433106
			 train-loss:  2.205641688659177 	 ± 0.25441634036051713
	data : 0.11442904472351074
	model : 0.06491365432739257
			 train-loss:  2.203025711137195 	 ± 0.2559717980962273
	data : 0.11442828178405762
	model : 0.06494579315185547
			 train-loss:  2.20337274584467 	 ± 0.2552714997067489
	data : 0.11430578231811524
	model : 0.06492972373962402
			 train-loss:  2.20381533414468 	 ± 0.2546034631915238
	data : 0.11429195404052735
	model : 0.06491355895996094
			 train-loss:  2.20721657208034 	 ± 0.25780886671273495
	data : 0.11423301696777344
	model : 0.06490187644958496
			 train-loss:  2.208603158593178 	 ± 0.2577289782859242
	data : 0.1143270492553711
	model : 0.06488189697265626
			 train-loss:  2.2102240721384683 	 ± 0.25789797133916964
	data : 0.11434059143066407
	model : 0.06491007804870605
			 train-loss:  2.207493428433879 	 ± 0.2597257972112219
	data : 0.11452527046203613
	model : 0.06497225761413575
			 train-loss:  2.2091950384598205 	 ± 0.2599923596984129
	data : 0.11464834213256836
	model : 0.06503133773803711
			 train-loss:  2.208847392929925 	 ± 0.2593108696041088
	data : 0.1146726131439209
	model : 0.06504263877868652
			 train-loss:  2.2083713758057653 	 ± 0.25867239946441045
	data : 0.11456007957458496
	model : 0.06504702568054199
			 train-loss:  2.211062567574637 	 ± 0.26048927043394693
	data : 0.11456127166748047
	model : 0.06496729850769042
			 train-loss:  2.210717313276614 	 ± 0.2598183288681933
	data : 0.11490569114685059
	model : 0.06488351821899414
			 train-loss:  2.211832029663998 	 ± 0.2595497647866483
	data : 0.11474418640136719
	model : 0.06479558944702149
			 train-loss:  2.213914915033289 	 ± 0.26038472907334714
	data : 0.11471996307373047
	model : 0.06478095054626465
			 train-loss:  2.2141330613884875 	 ± 0.25970077712805223
	data : 0.11480655670166015
	model : 0.06476950645446777
			 train-loss:  2.215107411624276 	 ± 0.25934611729996554
	data : 0.11483659744262695
	model : 0.06483454704284668
			 train-loss:  2.2147494541837816 	 ± 0.2587017614897476
	data : 0.11446475982666016
	model : 0.06492500305175782
			 train-loss:  2.2156423677212347 	 ± 0.25830676293477495
	data : 0.11452660560607911
	model : 0.06494002342224121
			 train-loss:  2.2138661422227557 	 ± 0.258780802985769
	data : 0.11465487480163575
	model : 0.0649897575378418
			 train-loss:  2.214976539162441 	 ± 0.2585559035274545
	data : 0.1144984245300293
	model : 0.0649914264678955
			 train-loss:  2.2153117967148623 	 ± 0.25792332212364333
	data : 0.11448225975036622
	model : 0.06494989395141601
			 train-loss:  2.2148676425064164 	 ± 0.2573278653501701
	data : 0.11453557014465332
	model : 0.06490073204040528
			 train-loss:  2.213546263802912 	 ± 0.25731942860650975
	data : 0.11462650299072266
	model : 0.06491055488586425
			 train-loss:  2.212726699388944 	 ± 0.2569125144175056
	data : 0.11453924179077149
	model : 0.06489901542663574
			 train-loss:  2.21425722204909 	 ± 0.2571460133512718
	data : 0.11464138031005859
	model : 0.06492524147033692
			 train-loss:  2.213173724673121 	 ± 0.25694068272696824
	data : 0.11468725204467774
	model : 0.06495356559753418
			 train-loss:  2.2122042684844048 	 ± 0.2566519766611123
	data : 0.11461281776428223
	model : 0.06494860649108887
			 train-loss:  2.210925216650843 	 ± 0.2566381765808701
	data : 0.1144106388092041
	model : 0.06496806144714355
			 train-loss:  2.2102988082170487 	 ± 0.25614824424153276
	data : 0.11438474655151368
	model : 0.06492595672607422
			 train-loss:  2.2097823566465236 	 ± 0.2556146318096993
	data : 0.11434078216552734
	model : 0.06492586135864258
			 train-loss:  2.2099067049451393 	 ± 0.2549872317463064
	data : 0.11434421539306641
	model : 0.0648998737335205
			 train-loss:  2.2102129700148634 	 ± 0.25439565147393606
	data : 0.11434421539306641
	model : 0.06494784355163574
			 train-loss:  2.2098520357234803 	 ± 0.2538234664151695
	data : 0.11444201469421386
	model : 0.06498465538024903
			 train-loss:  2.2092802542011913 	 ± 0.2533352951583899
	data : 0.11443572044372559
	model : 0.06500611305236817
			 train-loss:  2.209632374129249 	 ± 0.2527699388142567
	data : 0.11454839706420898
	model : 0.06497139930725097
			 train-loss:  2.2080302918014896 	 ± 0.25320488823427184
	data : 0.11439323425292969
	model : 0.06503748893737793
			 train-loss:  2.2076961524211445 	 ± 0.2526412329488806
	data : 0.11435608863830567
	model : 0.0649744987487793
			 train-loss:  2.207628202210203 	 ± 0.2520380086220653
	data : 0.11443805694580078
	model : 0.064979887008667
			 train-loss:  2.208269258907863 	 ± 0.2516079408201052
	data : 0.1145970344543457
	model : 0.06496825218200683
			 train-loss:  2.2074810419037445 	 ± 0.25127076010305444
	data : 0.11461081504821777
	model : 0.06503067016601563
			 train-loss:  2.2069139188190676 	 ± 0.25081276383455514
	data : 0.11470341682434082
	model : 0.06502628326416016
			 train-loss:  2.2076657915339224 	 ± 0.25046267295035923
	data : 0.11473846435546875
	model : 0.06502480506896972
			 train-loss:  2.2077480632568074 	 ± 0.2498796793945972
	data : 0.11464514732360839
	model : 0.06497154235839844
			 train-loss:  2.208683283384456 	 ± 0.24967300314412547
	data : 0.11437497138977051
	model : 0.06498141288757324
			 train-loss:  2.2102364610742637 	 ± 0.2501333058402138
	data : 0.11429948806762695
	model : 0.06492061614990234
			 train-loss:  2.208729541796144 	 ± 0.25053710115009975
	data : 0.11438279151916504
	model : 0.06489419937133789
			 train-loss:  2.2087595156573374 	 ± 0.24996220425925628
	data : 0.11446852684020996
	model : 0.0648930549621582
			 train-loss:  2.2091757412914816 	 ± 0.24946656862019004
	data : 0.11452951431274414
	model : 0.06491584777832031
			 train-loss:  2.208336439999667 	 ± 0.249208664453867
	data : 0.11483330726623535
	model : 0.06521987915039062
			 train-loss:  2.2082958728479585 	 ± 0.2486449327455266
	data : 0.11491613388061524
	model : 0.06517376899719238
			 train-loss:  2.207975067533888 	 ± 0.24813012549369176
	data : 0.11492667198181153
	model : 0.06508674621582031
			 train-loss:  2.2083673541321347 	 ± 0.247642141591411
	data : 0.11478638648986816
	model : 0.06498050689697266
			 train-loss:  2.209948353469372 	 ± 0.2482141259641186
	data : 0.11466836929321289
	model : 0.06477651596069336
			 train-loss:  2.210034721162584 	 ± 0.24766529810455737
	data : 0.11458239555358887
	model : 0.06425566673278808
			 train-loss:  2.209792513762955 	 ± 0.247143464123778
	data : 0.11471171379089355
	model : 0.06412672996520996
			 train-loss:  2.210270815483799 	 ± 0.24670330366413334
	data : 0.1148592472076416
	model : 0.06398973464965821
			 train-loss:  2.2092993034605395 	 ± 0.2465964919564108
	data : 0.11500430107116699
	model : 0.06392030715942383
			 train-loss:  2.208775314701697 	 ± 0.24618465707418805
	data : 0.11521453857421875
	model : 0.0638974666595459
			 train-loss:  2.2078841836556142 	 ± 0.24601875825105793
	data : 0.11521539688110352
	model : 0.06386280059814453
			 train-loss:  2.208593469161492 	 ± 0.24572123472523005
	data : 0.11512227058410644
	model : 0.06383476257324219
			 train-loss:  2.208139077856623 	 ± 0.24528833260831395
	data : 0.11499576568603516
	model : 0.06383485794067383
			 train-loss:  2.2072729834159555 	 ± 0.24511664388428084
	data : 0.11509366035461426
	model : 0.06383943557739258
			 train-loss:  2.2073677925982027 	 ± 0.24459661095081875
	data : 0.11514105796813964
	model : 0.0638397216796875
			 train-loss:  2.206201811546975 	 ± 0.24472646513469234
	data : 0.11534008979797364
	model : 0.06391124725341797
			 train-loss:  2.2061657082226316 	 ± 0.24420805360032533
	data : 0.1153061866760254
	model : 0.06388897895812988
			 train-loss:  2.2064578578441956 	 ± 0.24373362723063233
	data : 0.11527905464172364
	model : 0.0638570785522461
			 train-loss:  2.2050463802674236 	 ± 0.24418976924014096
	data : 0.1153341293334961
	model : 0.06383671760559081
			 train-loss:  2.2042906813042933 	 ± 0.2439571036665919
	data : 0.11524772644042969
	model : 0.06383442878723145
			 train-loss:  2.2029937033851943 	 ± 0.24427264120099865
	data : 0.1151456356048584
	model : 0.06379446983337403
			 train-loss:  2.201898698984835 	 ± 0.2443548686422284
	data : 0.11519651412963867
	model : 0.06380419731140137
			 train-loss:  2.2010955973104998 	 ± 0.24416799032860576
	data : 0.11520543098449706
	model : 0.06380376815795899
			 train-loss:  2.2007810711370084 	 ± 0.2437141895155012
	data : 0.11519069671630859
	model : 0.06377601623535156
			 train-loss:  2.200663069721128 	 ± 0.2432212183806743
	data : 0.11516733169555664
	model : 0.06373796463012696
			 train-loss:  2.2002419593382854 	 ± 0.2428134576257634
	data : 0.11512188911437989
	model : 0.06374707221984863
			 train-loss:  2.199276025702314 	 ± 0.24279064813254528
	data : 0.11526145935058593
	model : 0.06381750106811523
			 train-loss:  2.199963966361907 	 ± 0.2425387975713058
	data : 0.11540226936340332
	model : 0.06381373405456543
			 train-loss:  2.1997467606298384 	 ± 0.24207338457314334
	data : 0.11531529426574708
	model : 0.06382288932800292
			 train-loss:  2.1995406840220992 	 ± 0.2416086008894188
	data : 0.11520285606384277
	model : 0.0638132095336914
			 train-loss:  2.2006228818893434 	 ± 0.24172884408648992
	data : 0.11531414985656738
	model : 0.0638117790222168
			 train-loss:  2.1999142872859756 	 ± 0.24150685395172586
	data : 0.11524596214294433
	model : 0.0637486457824707
			 train-loss:  2.1996241855242897 	 ± 0.24107101398923617
	data : 0.11524262428283691
	model : 0.06381325721740723
			 train-loss:  2.1988897229371807 	 ± 0.24087645588139397
	data : 0.11527299880981445
	model : 0.06384634971618652
			 train-loss:  2.1994211983492993 	 ± 0.2405504104180887
	data : 0.11551337242126465
	model : 0.06389164924621582
			 train-loss:  2.1991650001675476 	 ± 0.2401129989645945
	data : 0.11550884246826172
	model : 0.06389322280883789
			 train-loss:  2.2009334648028016 	 ± 0.24130177056167992
	data : 0.11518535614013672
	model : 0.05544958114624023
#epoch  12    val-loss:  2.405916176344219  train-loss:  2.2009334648028016  lr:  0.00125
			 train-loss:  2.1201131343841553 	 ± 0.0
	data : 5.4646360874176025
	model : 0.0719611644744873
			 train-loss:  2.0611220598220825 	 ± 0.058991074562072754
	data : 2.8303534984588623
	model : 0.06845951080322266
			 train-loss:  2.0672593911488852 	 ± 0.048941784366278024
	data : 1.9251220226287842
	model : 0.06735690434773763
			 train-loss:  2.0645018219947815 	 ± 0.04265309198212285
	data : 1.472397267818451
	model : 0.06672745943069458
			 train-loss:  2.0127814292907713 	 ± 0.11025164438835547
	data : 1.2007805824279785
	model : 0.06632423400878906
			 train-loss:  1.9894844492276509 	 ± 0.1133281399693864
	data : 0.13071670532226562
	model : 0.06490020751953125
			 train-loss:  2.016687273979187 	 ± 0.12429190192041699
	data : 0.11434063911437989
	model : 0.06483206748962403
			 train-loss:  2.0616230219602585 	 ± 0.16628880719684297
	data : 0.11416850090026856
	model : 0.06469154357910156
			 train-loss:  2.0593027671178183 	 ± 0.1569158858660089
	data : 0.11422624588012695
	model : 0.0647085189819336
			 train-loss:  2.0972275614738463 	 ± 0.18736313926684242
	data : 0.11419444084167481
	model : 0.06480393409729004
			 train-loss:  2.1069628217003564 	 ± 0.18127697081934815
	data : 0.11419186592102051
	model : 0.06484222412109375
			 train-loss:  2.1228169103463492 	 ± 0.1813498512040486
	data : 0.11426973342895508
	model : 0.0649144172668457
			 train-loss:  2.104362047635592 	 ± 0.1855934365089742
	data : 0.11445465087890624
	model : 0.06502985954284668
			 train-loss:  2.137656433241708 	 ± 0.21539563935158443
	data : 0.11446990966796874
	model : 0.06500129699707032
			 train-loss:  2.13859707514445 	 ± 0.20812171916664862
	data : 0.1144554615020752
	model : 0.06489768028259277
			 train-loss:  2.1490389853715897 	 ± 0.2055309872074062
	data : 0.11448521614074707
	model : 0.06487197875976562
			 train-loss:  2.1310707611196182 	 ± 0.21195239667440602
	data : 0.11428160667419433
	model : 0.0648576259613037
			 train-loss:  2.1489417221811085 	 ± 0.21876325560172447
	data : 0.11424698829650878
	model : 0.06485471725463868
			 train-loss:  2.160272629637467 	 ± 0.21828780044518015
	data : 0.11423783302307129
	model : 0.06484622955322265
			 train-loss:  2.1556677758693694 	 ± 0.21370534508598693
	data : 0.11426987648010253
	model : 0.06491580009460449
			 train-loss:  2.146758959406898 	 ± 0.2123265269395517
	data : 0.11417560577392578
	model : 0.06493568420410156
			 train-loss:  2.1559095653620632 	 ± 0.21164062090386673
	data : 0.11440787315368653
	model : 0.06492900848388672
			 train-loss:  2.1608579728914346 	 ± 0.20828584478041146
	data : 0.11433897018432618
	model : 0.06492562294006347
			 train-loss:  2.147948826352755 	 ± 0.21309206438088527
	data : 0.11437840461730957
	model : 0.06497416496276856
			 train-loss:  2.1641299390792845 	 ± 0.22332886200325905
	data : 0.11446561813354492
	model : 0.06495866775512696
			 train-loss:  2.179951855769524 	 ± 0.23284288166810127
	data : 0.11460952758789063
	model : 0.06496672630310059
			 train-loss:  2.176303991564998 	 ± 0.22924614373233193
	data : 0.11453385353088379
	model : 0.06500067710876464
			 train-loss:  2.1722364893981387 	 ± 0.22610523364505927
	data : 0.11452975273132324
	model : 0.06500811576843261
			 train-loss:  2.1705264346352937 	 ± 0.22235686355793005
	data : 0.11449179649353028
	model : 0.06498618125915527
			 train-loss:  2.170498518149058 	 ± 0.21861955892871332
	data : 0.11437029838562011
	model : 0.06494345664978027
			 train-loss:  2.164178413729514 	 ± 0.21783265298258137
	data : 0.11437468528747559
	model : 0.06490306854248047
			 train-loss:  2.1660291142761707 	 ± 0.214649474262809
	data : 0.11434249877929688
	model : 0.06489272117614746
			 train-loss:  2.165180802345276 	 ± 0.2114266568317813
	data : 0.11439108848571777
	model : 0.06487894058227539
			 train-loss:  2.1619943625786724 	 ± 0.20909699010538094
	data : 0.11435260772705078
	model : 0.0649271011352539
			 train-loss:  2.160314794949123 	 ± 0.20632080918877402
	data : 0.11448850631713867
	model : 0.06498494148254394
			 train-loss:  2.17382252547476 	 ± 0.2185677963999034
	data : 0.11446108818054199
	model : 0.064979887008667
			 train-loss:  2.166176251462988 	 ± 0.22042120567863172
	data : 0.11455440521240234
	model : 0.06496376991271972
			 train-loss:  2.1633646205851904 	 ± 0.21817295166508663
	data : 0.11445488929748535
	model : 0.06495394706726074
			 train-loss:  2.159590008931282 	 ± 0.21661105880592826
	data : 0.1144211769104004
	model : 0.06493077278137208
			 train-loss:  2.162401369214058 	 ± 0.21460565730204517
	data : 0.11427946090698242
	model : 0.06492013931274414
			 train-loss:  2.1570210282395528 	 ± 0.21468629177981588
	data : 0.11433987617492676
	model : 0.06495137214660644
			 train-loss:  2.15648345152537 	 ± 0.2121430333742168
	data : 0.1143826961517334
	model : 0.06499266624450684
			 train-loss:  2.153489645137343 	 ± 0.21055756285740257
	data : 0.11447839736938477
	model : 0.06506843566894531
			 train-loss:  2.148941132155332 	 ± 0.21027722420137918
	data : 0.1145322322845459
	model : 0.0650524616241455
			 train-loss:  2.1492061773935953 	 ± 0.20793511689744404
	data : 0.11464347839355468
	model : 0.06504416465759277
			 train-loss:  2.141363781431447 	 ± 0.21228452752533217
	data : 0.11460962295532226
	model : 0.06507453918457032
			 train-loss:  2.1396007385659725 	 ± 0.21035417635556047
	data : 0.1145024299621582
	model : 0.06566948890686035
			 train-loss:  2.144247755408287 	 ± 0.21057535903757274
	data : 0.11392312049865723
	model : 0.06559462547302246
			 train-loss:  2.1517461708613803 	 ± 0.21479269697961406
	data : 0.11390314102172852
	model : 0.06559901237487793
			 train-loss:  2.152649927139282 	 ± 0.21272801089029922
	data : 0.11393132209777831
	model : 0.06559262275695801
			 train-loss:  2.1597387229695038 	 ± 0.2165142955018469
	data : 0.11389813423156739
	model : 0.0655595302581787
			 train-loss:  2.168070994890653 	 ± 0.2225257203468319
	data : 0.11403417587280273
	model : 0.06495661735534668
			 train-loss:  2.167131149543906 	 ± 0.22052059346737124
	data : 0.11458883285522461
	model : 0.0649637222290039
			 train-loss:  2.1762556720663 	 ± 0.22834492604880272
	data : 0.11468400955200195
	model : 0.06494154930114746
			 train-loss:  2.187430178035389 	 ± 0.2406996826326658
	data : 0.11464414596557618
	model : 0.06495676040649415
			 train-loss:  2.1886711844376157 	 ± 0.23871838031054204
	data : 0.11446666717529297
	model : 0.06488704681396484
			 train-loss:  2.1842055759931864 	 ± 0.23896325124196313
	data : 0.1142995834350586
	model : 0.0648430347442627
			 train-loss:  2.1940745341366736 	 ± 0.24833542747556925
	data : 0.1144174575805664
	model : 0.06483278274536133
			 train-loss:  2.192873071816008 	 ± 0.24639185410381348
	data : 0.11436796188354492
	model : 0.06485443115234375
			 train-loss:  2.1896355827649434 	 ± 0.24559220018537434
	data : 0.11450190544128418
	model : 0.06489925384521485
			 train-loss:  2.18964644729114 	 ± 0.24357084536874674
	data : 0.11466455459594727
	model : 0.06500263214111328
			 train-loss:  2.190184866228411 	 ± 0.2416351736252774
	data : 0.11481180191040039
	model : 0.06500077247619629
			 train-loss:  2.185844688188462 	 ± 0.24213358612077115
	data : 0.11477341651916503
	model : 0.0650090217590332
			 train-loss:  2.1951623912900686 	 ± 0.2513607661161243
	data : 0.11471953392028808
	model : 0.06498708724975585
			 train-loss:  2.2055464102671696 	 ± 0.2628900583310563
	data : 0.11429343223571778
	model : 0.06489372253417969
			 train-loss:  2.2036656231591194 	 ± 0.26133115399032575
	data : 0.11441617012023926
	model : 0.0648721694946289
			 train-loss:  2.2054282675928145 	 ± 0.2597685805610075
	data : 0.11447439193725586
	model : 0.06491646766662598
			 train-loss:  2.205128762651892 	 ± 0.25786309702079796
	data : 0.11450047492980957
	model : 0.06490402221679688
			 train-loss:  2.203795210174892 	 ± 0.256223795358184
	data : 0.1146078109741211
	model : 0.06497879028320312
			 train-loss:  2.2068988851138522 	 ± 0.25569010576943746
	data : 0.11490650177001953
	model : 0.06502580642700195
			 train-loss:  2.20706286060978 	 ± 0.25388679262600855
	data : 0.114794921875
	model : 0.0650136947631836
			 train-loss:  2.2056638879908457 	 ± 0.2523929521774916
	data : 0.11467304229736328
	model : 0.06495862007141114
			 train-loss:  2.205850037809921 	 ± 0.25066324885570895
	data : 0.11457815170288085
	model : 0.0649306297302246
			 train-loss:  2.2046310949969934 	 ± 0.24918155499948666
	data : 0.11452422142028809
	model : 0.06483650207519531
			 train-loss:  2.202168469429016 	 ± 0.24841967759053538
	data : 0.11455960273742676
	model : 0.06482448577880859
			 train-loss:  2.199639917988526 	 ± 0.2477495705735026
	data : 0.1146432876586914
	model : 0.064813232421875
			 train-loss:  2.2013124134633446 	 ± 0.2465670276791257
	data : 0.1146841049194336
	model : 0.06486825942993164
			 train-loss:  2.1996833040164065 	 ± 0.24539810521054226
	data : 0.11473050117492675
	model : 0.0649261474609375
			 train-loss:  2.1995773300339905 	 ± 0.243841802457715
	data : 0.11478190422058106
	model : 0.0649806022644043
			 train-loss:  2.1974054381251333 	 ± 0.24308073013022105
	data : 0.11473970413208008
	model : 0.06497130393981934
			 train-loss:  2.1938082038620372 	 ± 0.2437087697373371
	data : 0.1146500587463379
	model : 0.06498370170593262
			 train-loss:  2.190298897464101 	 ± 0.2442686667385986
	data : 0.11458463668823242
	model : 0.06493687629699707
			 train-loss:  2.1896094327949616 	 ± 0.24287296986343931
	data : 0.11456260681152344
	model : 0.06487092971801758
			 train-loss:  2.1980478791963485 	 ± 0.25336783994635037
	data : 0.11456336975097656
	model : 0.06482496261596679
			 train-loss:  2.195480498145608 	 ± 0.25296977162789586
	data : 0.11459865570068359
	model : 0.06484422683715821
			 train-loss:  2.1984330720679703 	 ± 0.25296362661393046
	data : 0.11465625762939453
	model : 0.06484270095825195
			 train-loss:  2.1947971650923805 	 ± 0.2537557423453927
	data : 0.114707612991333
	model : 0.06487803459167481
			 train-loss:  2.196242920377038 	 ± 0.25266993863601034
	data : 0.1147146224975586
	model : 0.06491322517395019
			 train-loss:  2.1975953766469205 	 ± 0.25156656194412735
	data : 0.11465120315551758
	model : 0.06493291854858399
			 train-loss:  2.1981581343544856 	 ± 0.2502213944074402
	data : 0.11456084251403809
	model : 0.06491661071777344
			 train-loss:  2.1986569939078864 	 ± 0.24888775284821169
	data : 0.11455092430114747
	model : 0.0648775577545166
			 train-loss:  2.1979062894116277 	 ± 0.2476349748631152
	data : 0.11456995010375977
	model : 0.06484713554382324
			 train-loss:  2.1984046787344 	 ± 0.2463463920306062
	data : 0.1145510196685791
	model : 0.06485166549682617
			 train-loss:  2.196446418762207 	 ± 0.24575918713733294
	data : 0.11465597152709961
	model : 0.06491589546203613
			 train-loss:  2.195040978883442 	 ± 0.24484176245192268
	data : 0.11470837593078613
	model : 0.06494512557983398
			 train-loss:  2.1946450298031173 	 ± 0.24359377931905643
	data : 0.11459321975708008
	model : 0.06498150825500489
			 train-loss:  2.1927111591260458 	 ± 0.2430745240645116
	data : 0.11451487541198731
	model : 0.06499366760253907
			 train-loss:  2.192298283382338 	 ± 0.24186535311305235
	data : 0.11455721855163574
	model : 0.06503033638000488
			 train-loss:  2.1947820355193786 	 ± 0.2418936046900685
	data : 0.11447420120239257
	model : 0.06495127677917481
			 train-loss:  2.1976203083992005 	 ± 0.24233223956202896
	data : 0.11441545486450196
	model : 0.06490950584411621
			 train-loss:  2.1970958237600797 	 ± 0.24118662473167543
	data : 0.11458353996276856
	model : 0.06490368843078613
			 train-loss:  2.1942202483906463 	 ± 0.24173507492762905
	data : 0.11468839645385742
	model : 0.06494364738464356
			 train-loss:  2.1972214481205614 	 ± 0.2424608061455237
	data : 0.11469388008117676
	model : 0.06495723724365235
			 train-loss:  2.197617354301306 	 ± 0.24132576523232638
	data : 0.11469092369079589
	model : 0.06501779556274415
			 train-loss:  2.195090389251709 	 ± 0.2415524241265521
	data : 0.11475906372070313
	model : 0.06507010459899902
			 train-loss:  2.195000369593782 	 ± 0.24041209550567252
	data : 0.11475396156311035
	model : 0.06504936218261718
			 train-loss:  2.1975118980229458 	 ± 0.24067910549922708
	data : 0.11466393470764161
	model : 0.06497707366943359
			 train-loss:  2.199822865150593 	 ± 0.24075198205549367
	data : 0.11458101272583007
	model : 0.06488490104675293
			 train-loss:  2.198874016420557 	 ± 0.23984785538076314
	data : 0.11454572677612304
	model : 0.0648386001586914
			 train-loss:  2.195973802696575 	 ± 0.24066750129951542
	data : 0.1146275520324707
	model : 0.06482467651367188
			 train-loss:  2.196310072331815 	 ± 0.2396069182142405
	data : 0.11461930274963379
	model : 0.06487460136413574
			 train-loss:  2.195958010852337 	 ± 0.23856368321090626
	data : 0.1146151065826416
	model : 0.06498279571533203
			 train-loss:  2.1977939004391693 	 ± 0.23829912775806789
	data : 0.11470346450805664
	model : 0.06504974365234376
			 train-loss:  2.1987743116261664 	 ± 0.23748044890390266
	data : 0.1147695541381836
	model : 0.06508526802062989
			 train-loss:  2.1975572181784586 	 ± 0.2368025027504133
	data : 0.11470541954040528
	model : 0.0650866985321045
			 train-loss:  2.1962399143597175 	 ± 0.23620240207360324
	data : 0.11463899612426758
	model : 0.06505808830261231
			 train-loss:  2.194435588315002 	 ± 0.23599231429500997
	data : 0.11458139419555664
	model : 0.06498761177062988
			 train-loss:  2.1940452951495932 	 ± 0.23502813791749494
	data : 0.11469087600708008
	model : 0.06498627662658692
			 train-loss:  2.193117786856259 	 ± 0.23425531135055244
	data : 0.11469488143920899
	model : 0.0649726390838623
			 train-loss:  2.1925924797852834 	 ± 0.23334757850300747
	data : 0.1147688388824463
	model : 0.06496734619140625
			 train-loss:  2.191519021987915 	 ± 0.23267866417508468
	data : 0.11470365524291992
	model : 0.06501660346984864
			 train-loss:  2.1902743460702117 	 ± 0.23212722999490015
	data : 0.11481676101684571
	model : 0.06502823829650879
			 train-loss:  2.194057937560043 	 ± 0.23492866193179923
	data : 0.11475768089294433
	model : 0.06501073837280273
			 train-loss:  2.1942528986161753 	 ± 0.23398944175782482
	data : 0.11467328071594238
	model : 0.06501941680908203
			 train-loss:  2.191801905632019 	 ± 0.23464433477658275
	data : 0.11442723274230956
	model : 0.06502857208251953
			 train-loss:  2.1910207810856046 	 ± 0.23387446487456795
	data : 0.11438288688659667
	model : 0.0649876594543457
			 train-loss:  2.1919268407220915 	 ± 0.2331737917177494
	data : 0.11441020965576172
	model : 0.06499571800231933
			 train-loss:  2.190128459595144 	 ± 0.23314371441059786
	data : 0.11431169509887695
	model : 0.06502776145935059
			 train-loss:  2.188438886819884 	 ± 0.2330236539115537
	data : 0.11430497169494629
	model : 0.06508889198303222
			 train-loss:  2.1878743428450362 	 ± 0.2322142208663046
	data : 0.11457304954528809
	model : 0.06511645317077637
			 train-loss:  2.1869056042824084 	 ± 0.231589753872108
	data : 0.11467437744140625
	model : 0.06513304710388183
			 train-loss:  2.18588377309568 	 ± 0.2310070984422039
	data : 0.11461820602416992
	model : 0.0651012897491455
			 train-loss:  2.1850100836359467 	 ± 0.23035582115100822
	data : 0.11448092460632324
	model : 0.06506366729736328
			 train-loss:  2.1867719390499056 	 ± 0.23039239502808664
	data : 0.1145824909210205
	model : 0.06493291854858399
			 train-loss:  2.186060084236993 	 ± 0.22968536836172018
	data : 0.11457786560058594
	model : 0.0648770809173584
			 train-loss:  2.190757346503875 	 ± 0.235257599552286
	data : 0.1146622657775879
	model : 0.0648353099822998
			 train-loss:  2.1892011435362546 	 ± 0.23509894117427546
	data : 0.11470789909362793
	model : 0.06490697860717773
			 train-loss:  2.1909407299497854 	 ± 0.23512885399136488
	data : 0.11486454010009765
	model : 0.06496315002441407
			 train-loss:  2.1882004797887458 	 ± 0.2364827224104101
	data : 0.11480474472045898
	model : 0.06505260467529297
			 train-loss:  2.1868431866168976 	 ± 0.2361793647199156
	data : 0.11472368240356445
	model : 0.0650822639465332
			 train-loss:  2.189333331500385 	 ± 0.2371775697410321
	data : 0.11473293304443359
	model : 0.06509642601013184
			 train-loss:  2.1863788458662974 	 ± 0.23893061175042687
	data : 0.11453084945678711
	model : 0.06501994132995606
			 train-loss:  2.1860554826843157 	 ± 0.23812490344060547
	data : 0.11460566520690918
	model : 0.06497058868408204
			 train-loss:  2.18667165024413 	 ± 0.2374110091716852
	data : 0.11470317840576172
	model : 0.06492948532104492
			 train-loss:  2.1865233215792426 	 ± 0.2365976295799645
	data : 0.11465921401977539
	model : 0.06494383811950684
			 train-loss:  2.1850699063849777 	 ± 0.23643460965380278
	data : 0.11462645530700684
	model : 0.06496706008911132
			 train-loss:  2.184565403834492 	 ± 0.2357078782186854
	data : 0.11485261917114258
	model : 0.0649996280670166
			 train-loss:  2.183723526226508 	 ± 0.23513187363131322
	data : 0.11479015350341797
	model : 0.06501579284667969
			 train-loss:  2.1828712753001476 	 ± 0.23457075990514334
	data : 0.11471152305603027
	model : 0.06504206657409668
			 train-loss:  2.181193325519562 	 ± 0.23468304213183416
	data : 0.11461877822875977
	model : 0.06499261856079101
			 train-loss:  2.181722687569675 	 ± 0.2339944899067835
	data : 0.1145705223083496
	model : 0.06499857902526855
			 train-loss:  2.1821391464848268 	 ± 0.23327964028596068
	data : 0.11450390815734864
	model : 0.06499252319335938
			 train-loss:  2.1810440557454926 	 ± 0.2329076861917364
	data : 0.11448736190795898
	model : 0.06502480506896972
			 train-loss:  2.182031378343508 	 ± 0.232471264679384
	data : 0.11450252532958985
	model : 0.06504769325256347
			 train-loss:  2.189049103183131 	 ± 0.24754492165799974
	data : 0.11454734802246094
	model : 0.06507501602172852
			 train-loss:  2.1913621035905986 	 ± 0.2484248838256524
	data : 0.11448459625244141
	model : 0.06506342887878418
			 train-loss:  2.190893236239245 	 ± 0.24770169303298434
	data : 0.11435899734497071
	model : 0.0650510311126709
			 train-loss:  2.1901717238788363 	 ± 0.2470820306624177
	data : 0.11438555717468261
	model : 0.06501431465148926
			 train-loss:  2.189943456799729 	 ± 0.24632052958422265
	data : 0.11429290771484375
	model : 0.0650261402130127
			 train-loss:  2.1912120170891285 	 ± 0.24607003477080885
	data : 0.11438503265380859
	model : 0.06502361297607422
			 train-loss:  2.1893377837187016 	 ± 0.2464475853041301
	data : 0.11450886726379395
	model : 0.06505336761474609
			 train-loss:  2.189369875707744 	 ± 0.24568610459309964
	data : 0.11459689140319824
	model : 0.06509284973144532
			 train-loss:  2.1881951371584933 	 ± 0.24538725878125844
	data : 0.11464939117431641
	model : 0.06514859199523926
			 train-loss:  2.1882226910532974 	 ± 0.2446382358890095
	data : 0.11474652290344238
	model : 0.06510004997253419
			 train-loss:  2.187249223391215 	 ± 0.24421417845843288
	data : 0.1147240161895752
	model : 0.06511077880859376
			 train-loss:  2.187822608344526 	 ± 0.24358885760352852
	data : 0.1146383285522461
	model : 0.06511859893798828
			 train-loss:  2.186359313433756 	 ± 0.24358914892865408
	data : 0.11455907821655273
	model : 0.06508150100708007
			 train-loss:  2.1847081226961955 	 ± 0.24379868517989078
	data : 0.11447887420654297
	model : 0.06506056785583496
			 train-loss:  2.184551830122457 	 ± 0.24308475834845183
	data : 0.1145256519317627
	model : 0.0650665283203125
			 train-loss:  2.18406117663664 	 ± 0.24245266658375517
	data : 0.11457695960998535
	model : 0.0650416374206543
			 train-loss:  2.185157997566357 	 ± 0.24216532847871947
	data : 0.1146484375
	model : 0.06499934196472168
			 train-loss:  2.1864437308422353 	 ± 0.2420449842059195
	data : 0.11481566429138183
	model : 0.06500005722045898
			 train-loss:  2.186318419572246 	 ± 0.24135001385092267
	data : 0.11486659049987794
	model : 0.06494994163513183
			 train-loss:  2.187675924136721 	 ± 0.2413169444480708
	data : 0.11482024192810059
	model : 0.06492714881896973
			 train-loss:  2.1889079652513774 	 ± 0.2411746705672516
	data : 0.11462640762329102
	model : 0.06494340896606446
			 train-loss:  2.1878067553043365 	 ± 0.24092935394984924
	data : 0.11443467140197754
	model : 0.06495027542114258
			 train-loss:  2.189018784269775 	 ± 0.2407852802930964
	data : 0.11450724601745606
	model : 0.06495103836059571
			 train-loss:  2.1901397330037664 	 ± 0.24057065436039166
	data : 0.11455879211425782
	model : 0.06496906280517578
			 train-loss:  2.189490997591498 	 ± 0.24005381193257466
	data : 0.11459174156188964
	model : 0.06503663063049317
			 train-loss:  2.1910569296942817 	 ± 0.24030110854619505
	data : 0.11478328704833984
	model : 0.0650482177734375
			 train-loss:  2.189690496381475 	 ± 0.2403365908537717
	data : 0.11483345031738282
	model : 0.065067720413208
			 train-loss:  2.191833448934031 	 ± 0.24140319284009126
	data : 0.11474943161010742
	model : 0.06506853103637696
			 train-loss:  2.1912777580198695 	 ± 0.24085941203384187
	data : 0.11456141471862794
	model : 0.06506156921386719
			 train-loss:  2.1912639840789465 	 ± 0.24020408332820914
	data : 0.11452460289001465
	model : 0.06497101783752442
			 train-loss:  2.192508752925976 	 ± 0.24014832762762478
	data : 0.11452045440673828
	model : 0.06499176025390625
			 train-loss:  2.1926118801998835 	 ± 0.23950600506487438
	data : 0.11467943191528321
	model : 0.06494593620300293
			 train-loss:  2.194172288007277 	 ± 0.23981087978841178
	data : 0.11471881866455078
	model : 0.06494688987731934
			 train-loss:  2.1950206274681903 	 ± 0.23945341485619384
	data : 0.11480941772460937
	model : 0.06495633125305175
			 train-loss:  2.1952168878424105 	 ± 0.23883426044611925
	data : 0.11490855216979981
	model : 0.06501975059509277
			 train-loss:  2.194514983578732 	 ± 0.23840029046859934
	data : 0.11486201286315918
	model : 0.06500792503356934
			 train-loss:  2.194130842598321 	 ± 0.23783433697572312
	data : 0.11521244049072266
	model : 0.06502952575683593
			 train-loss:  2.1929813902825117 	 ± 0.23774549109138624
	data : 0.11524057388305664
	model : 0.06504125595092773
			 train-loss:  2.1951090838625023 	 ± 0.23895449990681586
	data : 0.11529664993286133
	model : 0.06503415107727051
			 train-loss:  2.195155508124951 	 ± 0.2383387147215826
	data : 0.11514048576354981
	model : 0.06500477790832519
			 train-loss:  2.19541145777091 	 ± 0.23775353302761287
	data : 0.11509532928466797
	model : 0.06496644020080566
			 train-loss:  2.1939639990427056 	 ± 0.23800607637191273
	data : 0.11472291946411133
	model : 0.06499781608581542
			 train-loss:  2.194511566670413 	 ± 0.2375249701673518
	data : 0.11475605964660644
	model : 0.06499462127685547
			 train-loss:  2.197348516998869 	 ± 0.24024713123759672
	data : 0.1148402214050293
	model : 0.06498227119445801
			 train-loss:  2.1982324488797977 	 ± 0.2399652993085607
	data : 0.11487026214599609
	model : 0.06501908302307129
			 train-loss:  2.197761544585228 	 ± 0.23945679478344337
	data : 0.11494860649108887
	model : 0.06502923965454102
			 train-loss:  2.198562226485257 	 ± 0.23912863367496526
	data : 0.11492218971252441
	model : 0.06499233245849609
			 train-loss:  2.197520106145651 	 ± 0.23899311855843877
	data : 0.1148371696472168
	model : 0.06499547958374023
			 train-loss:  2.195652186576956 	 ± 0.2398773554032267
	data : 0.11467957496643066
	model : 0.06504769325256347
			 train-loss:  2.194599977310966 	 ± 0.2397578601355184
	data : 0.11463255882263183
	model : 0.06511063575744629
			 train-loss:  2.1953252379487203 	 ± 0.23939658972746045
	data : 0.1146538257598877
	model : 0.06507892608642578
			 train-loss:  2.194779236918514 	 ± 0.23894274161459328
	data : 0.11460423469543457
	model : 0.06512398719787597
			 train-loss:  2.193813436849106 	 ± 0.23876760602699315
	data : 0.11468396186828614
	model : 0.06521162986755372
			 train-loss:  2.1947861520143657 	 ± 0.23860373315861108
	data : 0.11483664512634277
	model : 0.06515722274780274
			 train-loss:  2.1942188637108324 	 ± 0.23817279133503239
	data : 0.11483354568481445
	model : 0.06512689590454102
			 train-loss:  2.1938231150309244 	 ± 0.23767390759850207
	data : 0.11492204666137695
	model : 0.06520953178405761
			 train-loss:  2.1935067267214516 	 ± 0.23715435446287506
	data : 0.1149782657623291
	model : 0.06527118682861328
			 train-loss:  2.1926616913867445 	 ± 0.2369125711672945
	data : 0.11472620964050292
	model : 0.06518445014953614
			 train-loss:  2.193612854245683 	 ± 0.23676117790307774
	data : 0.11458444595336914
	model : 0.06519060134887696
			 train-loss:  2.1970128578560373 	 ± 0.24136320627207836
	data : 0.11456084251403809
	model : 0.06505436897277832
			 train-loss:  2.1967891948167666 	 ± 0.24082346992710774
	data : 0.11446776390075683
	model : 0.06502513885498047
			 train-loss:  2.198236560380017 	 ± 0.24120082991020583
	data : 0.11449918746948243
	model : 0.0649223804473877
			 train-loss:  2.1978567558499527 	 ± 0.2407091564991021
	data : 0.11475143432617188
	model : 0.06492290496826172
			 train-loss:  2.1977396831599942 	 ± 0.2401626288410073
	data : 0.11486458778381348
	model : 0.06494073867797852
			 train-loss:  2.1969325673090268 	 ± 0.23990983946676978
	data : 0.11498093605041504
	model : 0.06551575660705566
			 train-loss:  2.1960995533249594 	 ± 0.2396811973461411
	data : 0.11450848579406739
	model : 0.0660168170928955
			 train-loss:  2.194709378669704 	 ± 0.24002563126705487
	data : 0.11401724815368652
	model : 0.0659677505493164
			 train-loss:  2.1952996479498372 	 ± 0.23964513164817652
	data : 0.11397442817687989
	model : 0.06587657928466797
			 train-loss:  2.1944690744973085 	 ± 0.23942724080136177
	data : 0.11385807991027833
	model : 0.06577763557434083
			 train-loss:  2.1948562764695714 	 ± 0.23896217276828752
	data : 0.1138718605041504
	model : 0.06510357856750489
			 train-loss:  2.1940541892581518 	 ± 0.2387325664649472
	data : 0.1143613338470459
	model : 0.06438932418823243
			 train-loss:  2.1941387526756895 	 ± 0.23820718878579908
	data : 0.11497769355773926
	model : 0.06422719955444336
			 train-loss:  2.192312780455871 	 ± 0.2392618242651412
	data : 0.11512179374694824
	model : 0.06412014961242676
			 train-loss:  2.1926633203238772 	 ± 0.23879496223156466
	data : 0.1153346061706543
	model : 0.06404342651367187
			 train-loss:  2.1920060268135573 	 ± 0.23847961973745255
	data : 0.11546635627746582
	model : 0.06397442817687989
			 train-loss:  2.1931592557741246 	 ± 0.23859969142037743
	data : 0.11554665565490722
	model : 0.06392168998718262
			 train-loss:  2.1933250096969275 	 ± 0.23809595217799387
	data : 0.11538925170898437
	model : 0.06383399963378907
			 train-loss:  2.192952551718416 	 ± 0.23764969140842018
	data : 0.11535859107971191
	model : 0.06379485130310059
			 train-loss:  2.193223556223857 	 ± 0.23717508835062123
	data : 0.11541585922241211
	model : 0.06374402046203613
			 train-loss:  2.19557319339524 	 ± 0.2393699531530356
	data : 0.11539320945739746
	model : 0.06375727653503419
			 train-loss:  2.195823397534959 	 ± 0.2388907746201537
	data : 0.11538443565368653
	model : 0.06380634307861328
			 train-loss:  2.195619149733398 	 ± 0.23840467439585228
	data : 0.1156430721282959
	model : 0.06388111114501953
			 train-loss:  2.1957852005455565 	 ± 0.23791485514251706
	data : 0.11554450988769531
	model : 0.06389198303222657
			 train-loss:  2.194146222927991 	 ± 0.23875152197474628
	data : 0.11534342765808106
	model : 0.0638437271118164
			 train-loss:  2.1948642516235926 	 ± 0.23850888969262898
	data : 0.11533379554748535
	model : 0.06383109092712402
			 train-loss:  2.1952994709213574 	 ± 0.2381065597666209
	data : 0.11543154716491699
	model : 0.06380672454833984
			 train-loss:  2.1950752265225804 	 ± 0.23763744334148998
	data : 0.1152726173400879
	model : 0.06383142471313477
			 train-loss:  2.194786089018357 	 ± 0.23718842439419247
	data : 0.11530566215515137
	model : 0.06385021209716797
			 train-loss:  2.1947093466181813 	 ± 0.23670288984778723
	data : 0.11551218032836914
	model : 0.06389203071594238
			 train-loss:  2.1943038951178067 	 ± 0.23630188540762134
	data : 0.1156123161315918
	model : 0.0639404296875
			 train-loss:  2.1941865283615734 	 ± 0.2358262698696397
	data : 0.11549758911132812
	model : 0.06390047073364258
			 train-loss:  2.193351681154918 	 ± 0.23570896012891435
	data : 0.11559104919433594
	model : 0.06388154029846191
			 train-loss:  2.193799924753938 	 ± 0.2353363692939567
	data : 0.11574311256408691
	model : 0.0638425350189209
			 train-loss:  2.1930877907622244 	 ± 0.23512794311104013
	data : 0.11566863059997559
	model : 0.06386542320251465
			 train-loss:  2.1933001279830933 	 ± 0.23467914802533973
	data : 0.11556320190429688
	model : 0.06382966041564941
			 train-loss:  2.1934375824928285 	 ± 0.23421936267022653
	data : 0.11568412780761719
	model : 0.06386528015136719
			 train-loss:  2.19328526339208 	 ± 0.23376473115025237
	data : 0.1156768798828125
	model : 0.06388072967529297
			 train-loss:  2.193586399157842 	 ± 0.233349227326306
	data : 0.11549916267395019
	model : 0.06389060020446777
			 train-loss:  2.1939172513871323 	 ± 0.23294682197664254
	data : 0.11544041633605957
	model : 0.0638495922088623
			 train-loss:  2.1939917084738965 	 ± 0.23249082948851463
	data : 0.11553258895874023
	model : 0.06384391784667968
			 train-loss:  2.1947093126820585 	 ± 0.232316198310834
	data : 0.1155886173248291
	model : 0.06382837295532226
			 train-loss:  2.1928247241303325 	 ± 0.23380690616399383
	data : 0.11526579856872558
	model : 0.05544624328613281
#epoch  13    val-loss:  2.4827004984805456  train-loss:  2.1928247241303325  lr:  0.000625
			 train-loss:  2.1228065490722656 	 ± 0.0
	data : 5.266443490982056
	model : 0.10590171813964844
			 train-loss:  2.106836199760437 	 ± 0.015970349311828613
	data : 2.6902966499328613
	model : 0.08713054656982422
			 train-loss:  2.1670122146606445 	 ± 0.08609494899993385
	data : 1.830672264099121
	model : 0.08088421821594238
			 train-loss:  2.2151529788970947 	 ± 0.11185640225880839
	data : 1.400927722454071
	model : 0.07688361406326294
			 train-loss:  2.2252222537994384 	 ± 0.10205412764441898
	data : 1.1435960292816163
	model : 0.07440013885498047
			 train-loss:  2.2025949954986572 	 ± 0.10601494343772566
	data : 0.1131868839263916
	model : 0.06614184379577637
			 train-loss:  2.2075375488826205 	 ± 0.0988946195406756
	data : 0.11337814331054688
	model : 0.06543436050415039
			 train-loss:  2.1911430060863495 	 ± 0.10217188843720408
	data : 0.11398677825927735
	model : 0.06476235389709473
			 train-loss:  2.1728316942850747 	 ± 0.10936923006556366
	data : 0.11457967758178711
	model : 0.0647860050201416
			 train-loss:  2.1501485109329224 	 ± 0.12408145268132177
	data : 0.1146554946899414
	model : 0.06489620208740235
			 train-loss:  2.131193984638561 	 ± 0.13262462967812966
	data : 0.11470060348510742
	model : 0.06499390602111817
			 train-loss:  2.139302651087443 	 ± 0.12979512016325617
	data : 0.11452927589416503
	model : 0.06500000953674316
			 train-loss:  2.139201659422654 	 ± 0.12470360851964794
	data : 0.1144216537475586
	model : 0.06496891975402833
			 train-loss:  2.14914003440312 	 ± 0.12539629916263959
	data : 0.1144946575164795
	model : 0.06494321823120117
			 train-loss:  2.1611581643422446 	 ± 0.1292209192895848
	data : 0.11449642181396484
	model : 0.06492810249328614
			 train-loss:  2.1690222769975662 	 ± 0.12877143365669821
	data : 0.11452488899230957
	model : 0.06490659713745117
			 train-loss:  2.1730425077326156 	 ± 0.12595738014488422
	data : 0.11456527709960937
	model : 0.06493611335754394
			 train-loss:  2.161904123094347 	 ± 0.13073996940481108
	data : 0.11468396186828614
	model : 0.06495013236999511
			 train-loss:  2.1434953589188424 	 ± 0.14930906862083226
	data : 0.11446924209594726
	model : 0.06494860649108887
			 train-loss:  2.1438525795936583 	 ± 0.14553680840048397
	data : 0.11433730125427247
	model : 0.06491637229919434
			 train-loss:  2.1462384973253523 	 ± 0.14242962262538417
	data : 0.11425690650939942
	model : 0.06490030288696289
			 train-loss:  2.1459484967318447 	 ± 0.1391612863786661
	data : 0.11433200836181641
	model : 0.06491694450378419
			 train-loss:  2.131283480188121 	 ± 0.1524967185904476
	data : 0.11437792778015136
	model : 0.0650301456451416
			 train-loss:  2.1257471988598504 	 ± 0.15162862499125165
	data : 0.11451263427734375
	model : 0.06509337425231934
			 train-loss:  2.1308504056930544 	 ± 0.1506539597874781
	data : 0.1146575927734375
	model : 0.06513452529907227
			 train-loss:  2.1580012440681458 	 ± 0.20063117768077096
	data : 0.11468749046325684
	model : 0.06512360572814942
			 train-loss:  2.150716053114997 	 ± 0.20035455230277593
	data : 0.11451826095581055
	model : 0.06507258415222168
			 train-loss:  2.1628993451595306 	 ± 0.2066784604490138
	data : 0.11445255279541015
	model : 0.06495842933654786
			 train-loss:  2.171874798577407 	 ± 0.20856333899370952
	data : 0.11446633338928222
	model : 0.06490740776062012
			 train-loss:  2.1805246472358704 	 ± 0.21028192361465584
	data : 0.1144486904144287
	model : 0.06494426727294922
			 train-loss:  2.1819321301675614 	 ± 0.20700607436943688
	data : 0.11444272994995117
	model : 0.06500945091247559
			 train-loss:  2.185887176543474 	 ± 0.2049324745725808
	data : 0.11456027030944824
	model : 0.06502876281738282
			 train-loss:  2.1978689432144165 	 ± 0.2128818464357292
	data : 0.11450047492980957
	model : 0.06502552032470703
			 train-loss:  2.1901530868866863 	 ± 0.2143604782970081
	data : 0.11443500518798828
	model : 0.06502938270568848
			 train-loss:  2.189831965310233 	 ± 0.21128429121407008
	data : 0.11449041366577148
	model : 0.06501455307006836
			 train-loss:  2.1997983853022256 	 ± 0.21651225360368548
	data : 0.11449742317199707
	model : 0.06496429443359375
			 train-loss:  2.1951938062100798 	 ± 0.21534593643440264
	data : 0.11458396911621094
	model : 0.06500864028930664
			 train-loss:  2.1923029610985205 	 ± 0.21321987651071872
	data : 0.11470303535461426
	model : 0.06507201194763183
			 train-loss:  2.190991114347409 	 ± 0.21062383737698048
	data : 0.11484174728393555
	model : 0.0650716781616211
			 train-loss:  2.1899583101272584 	 ± 0.20807436542579888
	data : 0.11477956771850586
	model : 0.06503329277038575
			 train-loss:  2.1923208818203066 	 ± 0.2060636725952901
	data : 0.11471247673034668
	model : 0.06505427360534669
			 train-loss:  2.1964225485211326 	 ± 0.20528273667169677
	data : 0.1144892692565918
	model : 0.06498947143554687
			 train-loss:  2.1925048218217005 	 ± 0.20446422459273358
	data : 0.11438221931457519
	model : 0.06495900154113769
			 train-loss:  2.201269133524461 	 ± 0.2101391379405898
	data : 0.11434421539306641
	model : 0.0649564266204834
			 train-loss:  2.201880709330241 	 ± 0.20783073724052636
	data : 0.11436395645141602
	model : 0.06500248908996582
			 train-loss:  2.2037847353064497 	 ± 0.20595573037763704
	data : 0.11448984146118164
	model : 0.06500921249389649
			 train-loss:  2.204770915051724 	 ± 0.20386268566079505
	data : 0.11453166007995605
	model : 0.06504225730895996
			 train-loss:  2.2032301475604377 	 ± 0.2020043012255476
	data : 0.11464872360229492
	model : 0.06502375602722169
			 train-loss:  2.206767213587858 	 ± 0.20142861685508576
	data : 0.1145556926727295
	model : 0.06502771377563477
			 train-loss:  2.1981771636009215 	 ± 0.20827308254912658
	data : 0.11449675559997559
	model : 0.06503071784973144
			 train-loss:  2.1962972178178677 	 ± 0.20664908428850612
	data : 0.11439986228942871
	model : 0.06501250267028809
			 train-loss:  2.189886519542107 	 ± 0.20971067197177898
	data : 0.11457138061523438
	model : 0.06502995491027833
			 train-loss:  2.1860683211740457 	 ± 0.20953966424595344
	data : 0.11449222564697266
	model : 0.06508665084838867
			 train-loss:  2.185932148385931 	 ± 0.20759278280107268
	data : 0.11453642845153808
	model : 0.06506152153015136
			 train-loss:  2.1901567914269187 	 ± 0.20802642426758394
	data : 0.11455340385437011
	model : 0.0650052547454834
			 train-loss:  2.1909635812044144 	 ± 0.20624748603058546
	data : 0.11459054946899414
	model : 0.06497778892517089
			 train-loss:  2.192924187894453 	 ± 0.20495611029225133
	data : 0.11444654464721679
	model : 0.06496281623840332
			 train-loss:  2.19348426728413 	 ± 0.20322556106658174
	data : 0.1146130084991455
	model : 0.06486411094665527
			 train-loss:  2.193685998350887 	 ± 0.20150180731184597
	data : 0.11457605361938476
	model : 0.06489319801330566
			 train-loss:  2.195808047056198 	 ± 0.20047928670030485
	data : 0.11463623046875
	model : 0.06492424011230469
			 train-loss:  2.1956918298221026 	 ± 0.1988312612448924
	data : 0.11470980644226074
	model : 0.06497468948364257
			 train-loss:  2.1970915390599157 	 ± 0.19752401808252218
	data : 0.11476311683654786
	model : 0.0649557113647461
			 train-loss:  2.1970357421844726 	 ± 0.1959505889743821
	data : 0.11470513343811035
	model : 0.06497464179992676
			 train-loss:  2.1941494550555944 	 ± 0.19575882377911372
	data : 0.1146927833557129
	model : 0.0650069236755371
			 train-loss:  2.194959554305443 	 ± 0.19435523141117797
	data : 0.11468305587768554
	model : 0.06500630378723145
			 train-loss:  2.1960111874522585 	 ± 0.19306348414053393
	data : 0.11470203399658203
	model : 0.06499114036560058
			 train-loss:  2.1946625478232087 	 ± 0.1919302753190186
	data : 0.1148421287536621
	model : 0.06502156257629395
			 train-loss:  2.1912597119808197 	 ± 0.19253913667995862
	data : 0.11477193832397461
	model : 0.06506462097167968
			 train-loss:  2.198489481124325 	 ± 0.20022083774057098
	data : 0.11484332084655761
	model : 0.065032958984375
			 train-loss:  2.204728356429509 	 ± 0.2054298387711422
	data : 0.11486449241638183
	model : 0.06498155593872071
			 train-loss:  2.2055573983931205 	 ± 0.20409591916469658
	data : 0.11473603248596191
	model : 0.06494560241699218
			 train-loss:  2.2115702579418817 	 ± 0.20891043206751725
	data : 0.1146024227142334
	model : 0.06487627029418945
			 train-loss:  2.2099622912602883 	 ± 0.20792275396237553
	data : 0.11446609497070312
	model : 0.06479101181030274
			 train-loss:  2.212674184425457 	 ± 0.20780886830301276
	data : 0.11448025703430176
	model : 0.06478815078735352
			 train-loss:  2.211917939186096 	 ± 0.20652131409751878
	data : 0.1144641399383545
	model : 0.0648374080657959
			 train-loss:  2.2094090784850873 	 ± 0.20630543802753548
	data : 0.11461844444274902
	model : 0.06487150192260742
			 train-loss:  2.2088004071991163 	 ± 0.20503009025276026
	data : 0.11462616920471191
	model : 0.06493711471557617
			 train-loss:  2.2081126998632383 	 ± 0.203800917970695
	data : 0.11488432884216308
	model : 0.06498970985412597
			 train-loss:  2.2027631017226206 	 ± 0.20794536937646785
	data : 0.11479167938232422
	model : 0.06501936912536621
			 train-loss:  2.2026113003492354 	 ± 0.20664602859951525
	data : 0.1147810935974121
	model : 0.06500391960144043
			 train-loss:  2.2087372644447987 	 ± 0.21255019092971617
	data : 0.11467742919921875
	model : 0.06498713493347168
			 train-loss:  2.208108669374047 	 ± 0.2113259169974486
	data : 0.11478495597839355
	model : 0.06495127677917481
			 train-loss:  2.213014752031809 	 ± 0.2146958274172535
	data : 0.1147852897644043
	model : 0.06494951248168945
			 train-loss:  2.2104077637195587 	 ± 0.2147315926286216
	data : 0.1148266315460205
	model : 0.06493029594421387
			 train-loss:  2.2074621410930857 	 ± 0.21516512845862548
	data : 0.11475172042846679
	model : 0.06495146751403809
			 train-loss:  2.2073008224021557 	 ± 0.2139156809620662
	data : 0.11474981307983398
	model : 0.06494216918945313
			 train-loss:  2.202979442717015 	 ± 0.21642535482469683
	data : 0.11459941864013672
	model : 0.0649681568145752
			 train-loss:  2.2060483382506804 	 ± 0.21708762897416667
	data : 0.11447348594665527
	model : 0.06492738723754883
			 train-loss:  2.2047142486893727 	 ± 0.21622706426728908
	data : 0.11452436447143555
	model : 0.06491475105285645
			 train-loss:  2.2012894484731884 	 ± 0.21743632785403577
	data : 0.11469621658325195
	model : 0.06488971710205078
			 train-loss:  2.1974496893830353 	 ± 0.21928508745509087
	data : 0.11467518806457519
	model : 0.06495833396911621
			 train-loss:  2.197001620479252 	 ± 0.21813194607337028
	data : 0.11474051475524902
	model : 0.06497135162353515
			 train-loss:  2.1988399797870266 	 ± 0.2176713953344943
	data : 0.11474490165710449
	model : 0.06501588821411133
			 train-loss:  2.195920504154043 	 ± 0.21833335825687425
	data : 0.11472096443176269
	model : 0.06506242752075195
			 train-loss:  2.1972363559823287 	 ± 0.21755557771436024
	data : 0.1146697998046875
	model : 0.06505265235900878
			 train-loss:  2.204950165003538 	 ± 0.22910734340462702
	data : 0.11467199325561524
	model : 0.0649789810180664
			 train-loss:  2.2010654567443217 	 ± 0.23107957714196825
	data : 0.11457180976867676
	model : 0.06495370864868164
			 train-loss:  2.2037688858655033 	 ± 0.23143427376340617
	data : 0.11473646163940429
	model : 0.06495413780212403
			 train-loss:  2.201304494732558 	 ± 0.23155122645931747
	data : 0.11484327316284179
	model : 0.06492962837219238
			 train-loss:  2.2024472296237945 	 ± 0.23067095445720784
	data : 0.11480274200439453
	model : 0.06497478485107422
			 train-loss:  2.2074470059706433 	 ± 0.2349085841692562
	data : 0.11491889953613281
	model : 0.06501264572143554
			 train-loss:  2.204410291185566 	 ± 0.23573804911581853
	data : 0.11501502990722656
	model : 0.06504168510437011
			 train-loss:  2.2034111833109438 	 ± 0.23480781025031064
	data : 0.11490163803100586
	model : 0.06503858566284179
			 train-loss:  2.2021435659665327 	 ± 0.23403006701748783
	data : 0.1148292064666748
	model : 0.06503515243530274
			 train-loss:  2.203766713823591 	 ± 0.23350043225821418
	data : 0.11471071243286132
	model : 0.06502356529235839
			 train-loss:  2.2023715747977204 	 ± 0.2328356984355919
	data : 0.11471223831176758
	model : 0.06498942375183106
			 train-loss:  2.2017138984715827 	 ± 0.23184402727300593
	data : 0.11469316482543945
	model : 0.06498970985412597
			 train-loss:  2.2000744055818626 	 ± 0.23139049622593189
	data : 0.11477560997009277
	model : 0.06498847007751465
			 train-loss:  2.2053009588784036 	 ± 0.23664440742654239
	data : 0.1148221492767334
	model : 0.06498188972473144
			 train-loss:  2.2057475957003505 	 ± 0.23561244285876826
	data : 0.11493182182312012
	model : 0.06499800682067872
			 train-loss:  2.2047575250402227 	 ± 0.23477847109989522
	data : 0.11476778984069824
	model : 0.06502385139465332
			 train-loss:  2.207734165447099 	 ± 0.23582256551762018
	data : 0.11469755172729493
	model : 0.06498842239379883
			 train-loss:  2.2095056635088626 	 ± 0.235524133196769
	data : 0.11463699340820313
	model : 0.06501183509826661
			 train-loss:  2.211222353734468 	 ± 0.23519786998629078
	data : 0.11455211639404297
	model : 0.06495442390441894
			 train-loss:  2.208087792603866 	 ± 0.2365525641484046
	data : 0.11460328102111816
	model : 0.06493263244628907
			 train-loss:  2.2062092176799117 	 ± 0.23639070774507778
	data : 0.11465849876403808
	model : 0.06494226455688476
			 train-loss:  2.2040132813983493 	 ± 0.23656357035829298
	data : 0.1149294376373291
	model : 0.06498565673828124
			 train-loss:  2.2051187379885526 	 ± 0.23586234090572197
	data : 0.11509122848510742
	model : 0.06495561599731445
			 train-loss:  2.20585465731741 	 ± 0.2350052386874205
	data : 0.11509208679199219
	model : 0.06503090858459473
			 train-loss:  2.2071878562370935 	 ± 0.23447547049222667
	data : 0.11502561569213868
	model : 0.06501388549804688
			 train-loss:  2.2040557142131583 	 ± 0.2360118939750715
	data : 0.11507735252380372
	model : 0.06501002311706543
			 train-loss:  2.2069860315713727 	 ± 0.23724257929078452
	data : 0.11497235298156738
	model : 0.06496362686157227
			 train-loss:  2.206248160300216 	 ± 0.23641673161041954
	data : 0.11468110084533692
	model : 0.06499085426330567
			 train-loss:  2.206725083051189 	 ± 0.2355209099612443
	data : 0.11462860107421875
	model : 0.06490187644958496
			 train-loss:  2.2071598558425904 	 ± 0.23462689025744352
	data : 0.11468658447265626
	model : 0.06489577293395996
			 train-loss:  2.208281241712116 	 ± 0.2340300470417133
	data : 0.11477346420288086
	model : 0.06497092247009277
			 train-loss:  2.207214791943708 	 ± 0.23341401903930836
	data : 0.11470508575439453
	model : 0.0649937629699707
			 train-loss:  2.208692324347794 	 ± 0.23309593904445028
	data : 0.11475119590759278
	model : 0.0650026798248291
			 train-loss:  2.20767342120178 	 ± 0.23247668730305737
	data : 0.11474218368530273
	model : 0.06509151458740234
			 train-loss:  2.211701414218316 	 ± 0.2360564892009474
	data : 0.11479883193969727
	model : 0.06508278846740723
			 train-loss:  2.2110164684193734 	 ± 0.23528342871468547
	data : 0.11475820541381836
	model : 0.06500930786132812
			 train-loss:  2.20903607841694 	 ± 0.2354839390842426
	data : 0.1147089958190918
	model : 0.06503496170043946
			 train-loss:  2.2097992959775423 	 ± 0.23476081065434035
	data : 0.11472725868225098
	model : 0.06501288414001465
			 train-loss:  2.2097428654556843 	 ± 0.23388410249495267
	data : 0.11478095054626465
	model : 0.06496253013610839
			 train-loss:  2.2077048963970607 	 ± 0.23420742873536812
	data : 0.11477546691894532
	model : 0.06502370834350586
			 train-loss:  2.2060627656824447 	 ± 0.23412353150813803
	data : 0.1148313045501709
	model : 0.06502456665039062
			 train-loss:  2.2077341340754155 	 ± 0.23408041204888486
	data : 0.11488895416259766
	model : 0.06501178741455078
			 train-loss:  2.2063148108081543 	 ± 0.23382165748121114
	data : 0.11499338150024414
	model : 0.06501870155334473
			 train-loss:  2.2036969215749838 	 ± 0.23500000246934946
	data : 0.1149505615234375
	model : 0.06511778831481933
			 train-loss:  2.203819876057761 	 ± 0.23416369966641304
	data : 0.11488351821899415
	model : 0.06509795188903808
			 train-loss:  2.2034731925802027 	 ± 0.23336790868285304
	data : 0.1146963119506836
	model : 0.06509475708007813
			 train-loss:  2.203208951882913 	 ± 0.23256590603458308
	data : 0.11465692520141602
	model : 0.0650266170501709
			 train-loss:  2.2041774596367683 	 ± 0.23203850457941372
	data : 0.11463675498962403
	model : 0.06500387191772461
			 train-loss:  2.203648552298546 	 ± 0.231317895965968
	data : 0.11484241485595703
	model : 0.06492595672607422
			 train-loss:  2.2052530124269683 	 ± 0.2313215221505195
	data : 0.1149254322052002
	model : 0.06492862701416016
			 train-loss:  2.2029967855100763 	 ± 0.23212340116147517
	data : 0.11506390571594238
	model : 0.06496214866638184
			 train-loss:  2.2020430005326563 	 ± 0.23161941010309872
	data : 0.11506862640380859
	model : 0.06502175331115723
			 train-loss:  2.2010249922404417 	 ± 0.23116532999713849
	data : 0.11491098403930664
	model : 0.06505308151245118
			 train-loss:  2.1995064332181173 	 ± 0.23112780153227053
	data : 0.11472153663635254
	model : 0.06505131721496582
			 train-loss:  2.2000807253519694 	 ± 0.23046272759456932
	data : 0.11466026306152344
	model : 0.06503462791442871
			 train-loss:  2.199045190748 	 ± 0.23004820455760902
	data : 0.11454296112060547
	model : 0.06495413780212403
			 train-loss:  2.1990849218870463 	 ± 0.22929073808151032
	data : 0.11460862159729004
	model : 0.06496777534484863
			 train-loss:  2.197975272446676 	 ± 0.22894929787039023
	data : 0.11471943855285645
	model : 0.06501731872558594
			 train-loss:  2.1995207891835795 	 ± 0.22900407192370106
	data : 0.11491775512695312
	model : 0.06501340866088867
			 train-loss:  2.201603914076282 	 ± 0.22972329644260472
	data : 0.11494708061218262
	model : 0.06501889228820801
			 train-loss:  2.2010761545254636 	 ± 0.2290800688984957
	data : 0.11496481895446778
	model : 0.06510453224182129
			 train-loss:  2.20125712558722 	 ± 0.22836053582562857
	data : 0.11499924659729004
	model : 0.06512637138366699
			 train-loss:  2.200779395767405 	 ± 0.22771541835247938
	data : 0.11497740745544434
	model : 0.0650742530822754
			 train-loss:  2.2012129339781947 	 ± 0.22706360553516503
	data : 0.11480364799499512
	model : 0.06508221626281738
			 train-loss:  2.200712876021862 	 ± 0.22644072826730743
	data : 0.11461997032165527
	model : 0.06511569023132324
			 train-loss:  2.201763167884779 	 ± 0.22612700133160865
	data : 0.1144648551940918
	model : 0.06506457328796386
			 train-loss:  2.20184122485879 	 ± 0.22543017386129066
	data : 0.11447629928588868
	model : 0.06505556106567383
			 train-loss:  2.2010044832171105 	 ± 0.22498980834331167
	data : 0.11454906463623046
	model : 0.06505708694458008
			 train-loss:  2.199481984464134 	 ± 0.22514348135812695
	data : 0.11469182968139649
	model : 0.06509957313537598
			 train-loss:  2.200218762773456 	 ± 0.2246584156271861
	data : 0.1148111343383789
	model : 0.06508722305297851
			 train-loss:  2.200545998940985 	 ± 0.2240201506348994
	data : 0.11506633758544922
	model : 0.06509370803833008
			 train-loss:  2.201065066331875 	 ± 0.22344852713808963
	data : 0.11497087478637695
	model : 0.06508955955505372
			 train-loss:  2.201084534327189 	 ± 0.22278265123441723
	data : 0.1149214744567871
	model : 0.06510648727416993
			 train-loss:  2.200905421781822 	 ± 0.22213468488207858
	data : 0.11469593048095703
	model : 0.06509027481079102
			 train-loss:  2.2009551034254184 	 ± 0.22148132564684783
	data : 0.11459064483642578
	model : 0.0650334358215332
			 train-loss:  2.2017310892629345 	 ± 0.22106442144213825
	data : 0.11443061828613281
	model : 0.06502633094787598
			 train-loss:  2.200090443672136 	 ± 0.2214624964259329
	data : 0.11452417373657227
	model : 0.06504282951354981
			 train-loss:  2.198725251793172 	 ± 0.2215461607305559
	data : 0.11462788581848145
	model : 0.06504335403442382
			 train-loss:  2.2013624762666635 	 ± 0.22361534763920177
	data : 0.11473221778869629
	model : 0.06503252983093262
			 train-loss:  2.2013702630996703 	 ± 0.2229755549714437
	data : 0.11483182907104492
	model : 0.06511330604553223
			 train-loss:  2.2013342170552774 	 ± 0.22234171066518185
	data : 0.11499767303466797
	model : 0.06526780128479004
			 train-loss:  2.201905958396567 	 ± 0.2218424443332184
	data : 0.1147529125213623
	model : 0.0652388095855713
			 train-loss:  2.202882897318079 	 ± 0.22159990315201575
	data : 0.11465277671813964
	model : 0.06523103713989258
			 train-loss:  2.203867811730454 	 ± 0.22137038878743714
	data : 0.11454205513000489
	model : 0.06520886421203613
			 train-loss:  2.2034068683783214 	 ± 0.2208407384997986
	data : 0.11443500518798828
	model : 0.06513304710388183
			 train-loss:  2.201943218378731 	 ± 0.22110357214358828
	data : 0.11441540718078613
	model : 0.06500577926635742
			 train-loss:  2.2018899629404256 	 ± 0.22049647213086618
	data : 0.11460862159729004
	model : 0.06500544548034667
			 train-loss:  2.2015753912795435 	 ± 0.2199341450695541
	data : 0.11461777687072754
	model : 0.06500306129455566
			 train-loss:  2.201192012299662 	 ± 0.21939699036849092
	data : 0.11473522186279297
	model : 0.06505050659179687
			 train-loss:  2.2001085049397235 	 ± 0.21929629228353645
	data : 0.11476030349731445
	model : 0.06511774063110351
			 train-loss:  2.2001073065624444 	 ± 0.21870599224473344
	data : 0.11473307609558106
	model : 0.0650865077972412
			 train-loss:  2.198891827129425 	 ± 0.21874944084740033
	data : 0.1147265911102295
	model : 0.06508269309997558
			 train-loss:  2.1969034544965056 	 ± 0.2198547647778314
	data : 0.1146019458770752
	model : 0.06505465507507324
			 train-loss:  2.197628274796501 	 ± 0.21949747028478656
	data : 0.11462101936340333
	model : 0.06502118110656738
			 train-loss:  2.196907147608305 	 ± 0.2191434453933694
	data : 0.11477160453796387
	model : 0.06495347023010253
			 train-loss:  2.1982474002538552 	 ± 0.2193483725726516
	data : 0.1148726463317871
	model : 0.06494488716125488
			 train-loss:  2.1985155257085958 	 ± 0.2188077867645455
	data : 0.11481642723083496
	model : 0.06492738723754883
			 train-loss:  2.1997615327489193 	 ± 0.21892205786932287
	data : 0.11494331359863282
	model : 0.06492080688476562
			 train-loss:  2.1994616690370226 	 ± 0.2183968313295826
	data : 0.11491909027099609
	model : 0.0649637222290039
			 train-loss:  2.1985584320166174 	 ± 0.2181991004401447
	data : 0.11489720344543457
	model : 0.06504864692687988
			 train-loss:  2.198322820420168 	 ± 0.2176666257071585
	data : 0.11476726531982422
	model : 0.06506209373474121
			 train-loss:  2.203187861418361 	 ± 0.2275462786212448
	data : 0.11474380493164063
	model : 0.06504426002502442
			 train-loss:  2.203810938681015 	 ± 0.22713935751741726
	data : 0.11485424041748046
	model : 0.06504745483398437
			 train-loss:  2.203872879545892 	 ± 0.22656961329120973
	data : 0.11494565010070801
	model : 0.06499700546264649
			 train-loss:  2.203891875743866 	 ± 0.2260026383227542
	data : 0.11497812271118164
	model : 0.0649418830871582
			 train-loss:  2.2027758620864715 	 ± 0.2259915362128151
	data : 0.11491475105285645
	model : 0.06495256423950195
			 train-loss:  2.201692794809247 	 ± 0.2259538047327866
	data : 0.11493921279907227
	model : 0.064984130859375
			 train-loss:  2.2003151959386367 	 ± 0.22624537585719942
	data : 0.11485357284545898
	model : 0.06499276161193848
			 train-loss:  2.199720431776608 	 ± 0.2258492056474747
	data : 0.11481375694274902
	model : 0.0650184154510498
			 train-loss:  2.1987657756340213 	 ± 0.22570991114223843
	data : 0.11476397514343262
	model : 0.06501636505126954
			 train-loss:  2.197730086960839 	 ± 0.22564917858946715
	data : 0.11485252380371094
	model : 0.06501173973083496
			 train-loss:  2.1992415995989445 	 ± 0.22614644680109577
	data : 0.11495428085327149
	model : 0.06500730514526368
			 train-loss:  2.2008574794118223 	 ± 0.22679689228933672
	data : 0.1149709701538086
	model : 0.06499886512756348
			 train-loss:  2.2004975308641863 	 ± 0.22631321263021992
	data : 0.11506166458129882
	model : 0.0650050163269043
			 train-loss:  2.2008602318309602 	 ± 0.22583460963616955
	data : 0.1150334358215332
	model : 0.06501846313476563
			 train-loss:  2.199758085029385 	 ± 0.22586423248332718
	data : 0.11502761840820312
	model : 0.06502332687377929
			 train-loss:  2.1999739661531628 	 ± 0.22535272341332335
	data : 0.11501331329345703
	model : 0.06503434181213379
			 train-loss:  2.2002178609651017 	 ± 0.22485114825381972
	data : 0.11500358581542969
	model : 0.06508879661560059
			 train-loss:  2.199637717732759 	 ± 0.22448491028839593
	data : 0.11472721099853515
	model : 0.06507744789123535
			 train-loss:  2.198716018920721 	 ± 0.22436774686537497
	data : 0.11470074653625488
	model : 0.06506257057189942
			 train-loss:  2.197894601358308 	 ± 0.22417156920839554
	data : 0.11468420028686524
	model : 0.06509528160095215
			 train-loss:  2.196793761670864 	 ± 0.22423887143763574
	data : 0.11480321884155273
	model : 0.06507172584533691
			 train-loss:  2.1970434626308055 	 ± 0.22375420721422395
	data : 0.11487050056457519
	model : 0.06500806808471679
			 train-loss:  2.1965706511719585 	 ± 0.22335189202516784
	data : 0.11509528160095214
	model : 0.0654381275177002
			 train-loss:  2.19871859442104 	 ± 0.22509932119455675
	data : 0.11520228385925294
	model : 0.06585068702697754
			 train-loss:  2.197611896700449 	 ± 0.22518854608426347
	data : 0.11520824432373047
	model : 0.06577129364013672
			 train-loss:  2.1988444774000495 	 ± 0.22542673708739353
	data : 0.11509947776794434
	model : 0.06571726799011231
			 train-loss:  2.1983513260101524 	 ± 0.2250407161660095
	data : 0.11511049270629883
	model : 0.06562557220458984
			 train-loss:  2.199586072670562 	 ± 0.2252936379756143
	data : 0.11514229774475097
	model : 0.06500964164733887
			 train-loss:  2.199059585465325 	 ± 0.2249304913506208
	data : 0.11500134468078613
	model : 0.06437687873840332
			 train-loss:  2.1995178687888965 	 ± 0.224537558564936
	data : 0.11509933471679687
	model : 0.06418304443359375
			 train-loss:  2.2000449045114054 	 ± 0.22418248959437567
	data : 0.11517062187194824
	model : 0.06402115821838379
			 train-loss:  2.199579857420503 	 ± 0.22380002850770395
	data : 0.11521768569946289
	model : 0.06391973495483398
			 train-loss:  2.199379338447704 	 ± 0.22333137266728306
	data : 0.11516313552856446
	model : 0.06392083168029786
			 train-loss:  2.1988376083581342 	 ± 0.22299607820076628
	data : 0.11543483734130859
	model : 0.06395893096923828
			 train-loss:  2.1983284779957364 	 ± 0.22264680677139279
	data : 0.11557221412658691
	model : 0.06397805213928223
			 train-loss:  2.1978445366538804 	 ± 0.2222881685870163
	data : 0.11559152603149414
	model : 0.0639838695526123
			 train-loss:  2.1970397131637442 	 ± 0.22214913228101124
	data : 0.11548199653625488
	model : 0.06395044326782226
			 train-loss:  2.1965171277013598 	 ± 0.22181742445390357
	data : 0.11562237739562989
	model : 0.06393184661865234
			 train-loss:  2.195462081787434 	 ± 0.22193257077738854
	data : 0.1157010555267334
	model : 0.06394529342651367
			 train-loss:  2.1956782320798456 	 ± 0.22148666263565853
	data : 0.11569252014160156
	model : 0.06394844055175782
			 train-loss:  2.1945106037558384 	 ± 0.22174558605515415
	data : 0.11566853523254395
	model : 0.06395325660705567
			 train-loss:  2.1947865295810858 	 ± 0.22132001189040873
	data : 0.11570801734924316
	model : 0.06398158073425293
			 train-loss:  2.1943880773488447 	 ± 0.22094204135964338
	data : 0.11573481559753418
	model : 0.06397643089294433
			 train-loss:  2.195464941859245 	 ± 0.221108890559669
	data : 0.11548714637756348
	model : 0.06391453742980957
			 train-loss:  2.1959996470771883 	 ± 0.22080511836711594
	data : 0.11531376838684082
	model : 0.06390295028686524
			 train-loss:  2.196149415221096 	 ± 0.22036070320436613
	data : 0.11543726921081543
	model : 0.0638967514038086
			 train-loss:  2.196713628101741 	 ± 0.22008190806084355
	data : 0.11555814743041992
	model : 0.06388692855834961
			 train-loss:  2.1974119749225554 	 ± 0.2199000821326996
	data : 0.1153831958770752
	model : 0.06387009620666503
			 train-loss:  2.198765906509088 	 ± 0.22046759037543426
	data : 0.11549234390258789
	model : 0.06386513710021972
			 train-loss:  2.1972211274674267 	 ± 0.2213436854071723
	data : 0.11562118530273438
	model : 0.06390948295593261
			 train-loss:  2.1979319050244475 	 ± 0.22117629868108327
	data : 0.11547045707702637
	model : 0.0638817310333252
			 train-loss:  2.198447034724297 	 ± 0.22087834847072466
	data : 0.11534657478332519
	model : 0.06387214660644532
			 train-loss:  2.1993445588881713 	 ± 0.22088704900911843
	data : 0.1154944896697998
	model : 0.06385374069213867
			 train-loss:  2.1982357802391053 	 ± 0.22113806139907333
	data : 0.11556496620178222
	model : 0.06400938034057617
			 train-loss:  2.1982600266240033 	 ± 0.2206974406687845
	data : 0.11553220748901367
	model : 0.06398582458496094
			 train-loss:  2.1977664700576236 	 ± 0.2203978680954647
	data : 0.11553711891174316
	model : 0.06403055191040039
			 train-loss:  2.196980141839491 	 ± 0.2203157688042081
	data : 0.11557731628417969
	model : 0.06407136917114258
			 train-loss:  2.198522218569057 	 ± 0.22124550519939817
	data : 0.11549797058105468
	model : 0.06407756805419922
			 train-loss:  2.197197811276305 	 ± 0.22181781792019378
	data : 0.11533551216125489
	model : 0.06396722793579102
			 train-loss:  2.1959485337138176 	 ± 0.2222811769886184
	data : 0.11519784927368164
	model : 0.0555361270904541
#epoch  14    val-loss:  2.419027654748214  train-loss:  2.1959485337138176  lr:  0.000625
			 train-loss:  2.33899188041687 	 ± 0.0
	data : 5.5121238231658936
	model : 0.07256150245666504
			 train-loss:  2.2299249172210693 	 ± 0.10906696319580078
	data : 2.820939064025879
	model : 0.06871569156646729
			 train-loss:  2.219316005706787 	 ± 0.09030780502004528
	data : 1.9189043839772542
	model : 0.06734275817871094
			 train-loss:  2.1063764095306396 	 ± 0.21067197699017226
	data : 1.4677700400352478
	model : 0.06666445732116699
			 train-loss:  2.087494134902954 	 ± 0.19217779968461365
	data : 1.197106695175171
	model : 0.06627893447875977
			 train-loss:  2.1519278287887573 	 ± 0.2270141554658282
	data : 0.11752567291259766
	model : 0.06470913887023926
			 train-loss:  2.1362245764051164 	 ± 0.21366510189646948
	data : 0.11445889472961426
	model : 0.06469807624816895
			 train-loss:  2.151826500892639 	 ± 0.20408360818883917
	data : 0.11442184448242188
	model : 0.0647742748260498
			 train-loss:  2.1437025600009494 	 ± 0.19377903767351018
	data : 0.11440935134887695
	model : 0.0648040771484375
			 train-loss:  2.13317813873291 	 ± 0.18652655276058935
	data : 0.1145094871520996
	model : 0.06483664512634277
			 train-loss:  2.1190324371511284 	 ± 0.18338549682517505
	data : 0.11453099250793457
	model : 0.06489105224609375
			 train-loss:  2.1349936028321586 	 ± 0.18338502479083468
	data : 0.11444010734558105
	model : 0.06489109992980957
			 train-loss:  2.1285813863460836 	 ± 0.17758529585951519
	data : 0.11437005996704101
	model : 0.06492013931274414
			 train-loss:  2.116991468838283 	 ± 0.17615381897609844
	data : 0.11446232795715332
	model : 0.06495695114135742
			 train-loss:  2.1070576508839927 	 ± 0.1741924809665797
	data : 0.11438994407653809
	model : 0.0649993896484375
			 train-loss:  2.0887764021754265 	 ± 0.1829197705658909
	data : 0.11443381309509278
	model : 0.06505756378173828
			 train-loss:  2.102618883637821 	 ± 0.18589582268002922
	data : 0.11450686454772949
	model : 0.06509475708007813
			 train-loss:  2.098626262611813 	 ± 0.1814067437281914
	data : 0.11462483406066895
	model : 0.06503987312316895
			 train-loss:  2.0845858109624764 	 ± 0.18634592121023363
	data : 0.11456189155578614
	model : 0.06508035659790039
			 train-loss:  2.117179149389267 	 ± 0.2305921740223831
	data : 0.11449255943298339
	model : 0.06501884460449218
			 train-loss:  2.119516650835673 	 ± 0.22527759149995155
	data : 0.11441516876220703
	model : 0.06497149467468262
			 train-loss:  2.1187310164624993 	 ± 0.22012754713249974
	data : 0.11446533203125
	model : 0.0649960994720459
			 train-loss:  2.1215525658234307 	 ± 0.21569537208429762
	data : 0.11444859504699707
	model : 0.06506814956665039
			 train-loss:  2.1178690642118454 	 ± 0.21189157890132068
	data : 0.11451067924499511
	model : 0.06507840156555175
			 train-loss:  2.1200372552871705 	 ± 0.20788204545333314
	data : 0.11453752517700196
	model : 0.06513257026672363
			 train-loss:  2.1174009075531592 	 ± 0.20427087358174198
	data : 0.11461935043334961
	model : 0.06511435508728028
			 train-loss:  2.118803443732085 	 ± 0.20057992258271293
	data : 0.11452131271362305
	model : 0.06507062911987305
			 train-loss:  2.1112227269581387 	 ± 0.2008657598972408
	data : 0.11447763442993164
	model : 0.06499443054199219
			 train-loss:  2.1071765135074485 	 ± 0.1985300667513863
	data : 0.11438956260681152
	model : 0.06493968963623047
			 train-loss:  2.106842839717865 	 ± 0.19520145994648033
	data : 0.11457791328430175
	model : 0.06495685577392578
			 train-loss:  2.1062061594378565 	 ± 0.19205890270968926
	data : 0.11472620964050292
	model : 0.06499567031860351
			 train-loss:  2.1076641343533993 	 ± 0.18920838138305537
	data : 0.11483125686645508
	model : 0.06505761146545411
			 train-loss:  2.115022301673889 	 ± 0.19091236956362223
	data : 0.11488404273986816
	model : 0.0650937557220459
			 train-loss:  2.120623620117412 	 ± 0.190816440450298
	data : 0.11485395431518555
	model : 0.06509513854980468
			 train-loss:  2.1144246407917566 	 ± 0.19151274588968395
	data : 0.11462712287902832
	model : 0.06504387855529785
			 train-loss:  2.1140321128898196 	 ± 0.18884839251595886
	data : 0.11444835662841797
	model : 0.0650092601776123
			 train-loss:  2.1238376288800627 	 ± 0.19534882731951925
	data : 0.11445937156677247
	model : 0.06493210792541504
			 train-loss:  2.129229404424366 	 ± 0.19553147874100338
	data : 0.11457524299621583
	model : 0.06490530967712402
			 train-loss:  2.122039253895099 	 ± 0.19803224789032584
	data : 0.1147791862487793
	model : 0.06495776176452636
			 train-loss:  2.120335301756859 	 ± 0.19583050474088615
	data : 0.11483302116394042
	model : 0.06533761024475097
			 train-loss:  2.132798756041178 	 ± 0.20887254133692712
	data : 0.11447863578796387
	model : 0.06531820297241211
			 train-loss:  2.135508982908158 	 ± 0.20709935142795838
	data : 0.11448321342468262
	model : 0.0653313159942627
			 train-loss:  2.1309485213701116 	 ± 0.20679991346809207
	data : 0.11422371864318848
	model : 0.06539387702941894
			 train-loss:  2.1352850903164255 	 ± 0.20640468919948016
	data : 0.11410508155822754
	model : 0.06532745361328125
			 train-loss:  2.1303945594363745 	 ± 0.20666041122614626
	data : 0.11421041488647461
	model : 0.06498908996582031
			 train-loss:  2.1264382808104805 	 ± 0.20611750455896038
	data : 0.11469311714172363
	model : 0.06502671241760254
			 train-loss:  2.132794720061282 	 ± 0.20842048952182038
	data : 0.11463985443115235
	model : 0.06507878303527832
			 train-loss:  2.126378362377485 	 ± 0.2108769606942156
	data : 0.11472058296203613
	model : 0.06508607864379883
			 train-loss:  2.1230686659715614 	 ± 0.20996989399660707
	data : 0.11476082801818847
	model : 0.06515026092529297
			 train-loss:  2.1206023287773133 	 ± 0.20857532786952135
	data : 0.1147125244140625
	model : 0.06516909599304199
			 train-loss:  2.1184744951771757 	 ± 0.2070677138068086
	data : 0.11459169387817383
	model : 0.06511831283569336
			 train-loss:  2.1208002177568583 	 ± 0.2057385183614281
	data : 0.1146402359008789
	model : 0.06502699851989746
			 train-loss:  2.125929245408976 	 ± 0.20711747906457137
	data : 0.11466407775878906
	model : 0.06493549346923828
			 train-loss:  2.123673450063776 	 ± 0.20584689862988972
	data : 0.11478142738342285
	model : 0.06487250328063965
			 train-loss:  2.1215839754451404 	 ± 0.20454409624153694
	data : 0.11481871604919433
	model : 0.06488533020019531
			 train-loss:  2.125452486532075 	 ± 0.20472975070946217
	data : 0.11486258506774902
	model : 0.06492290496826172
			 train-loss:  2.1251365741093955 	 ± 0.20293969936361045
	data : 0.11473712921142579
	model : 0.06497421264648437
			 train-loss:  2.1312961270069253 	 ± 0.20648735762198764
	data : 0.11484799385070801
	model : 0.06497707366943359
			 train-loss:  2.132286259683512 	 ± 0.20486880775083335
	data : 0.1146620750427246
	model : 0.06494650840759278
			 train-loss:  2.1361795445283254 	 ± 0.20534363944905168
	data : 0.11452088356018067
	model : 0.06486835479736328
			 train-loss:  2.134980887663169 	 ± 0.2038650801121504
	data : 0.11462917327880859
	model : 0.06484627723693848
			 train-loss:  2.1336369149146543 	 ± 0.20248657900379463
	data : 0.11477584838867187
	model : 0.06483998298645019
			 train-loss:  2.1319929361343384 	 ± 0.20128977470492315
	data : 0.11455736160278321
	model : 0.06488838195800781
			 train-loss:  2.1351533476263285 	 ± 0.20128025975413028
	data : 0.11459722518920898
	model : 0.06493439674377441
			 train-loss:  2.139783762051509 	 ± 0.2031321289931441
	data : 0.1147313117980957
	model : 0.06494617462158203
			 train-loss:  2.1376614064881294 	 ± 0.20231227233492416
	data : 0.11463794708251954
	model : 0.06497650146484375
			 train-loss:  2.1423292266788767 	 ± 0.2043462752117843
	data : 0.11456832885742188
	model : 0.06494994163513183
			 train-loss:  2.1383441879468807 	 ± 0.20544419323973973
	data : 0.11468400955200195
	model : 0.06495943069458007
			 train-loss:  2.138372512831204 	 ± 0.20395016757191764
	data : 0.11477022171020508
	model : 0.0649724006652832
			 train-loss:  2.1427109020096915 	 ± 0.2056699754682422
	data : 0.11473565101623535
	model : 0.06498150825500489
			 train-loss:  2.1489262228280728 	 ± 0.21073318938278382
	data : 0.11473207473754883
	model : 0.06497602462768555
			 train-loss:  2.150278699066904 	 ± 0.20957472493960014
	data : 0.11475744247436523
	model : 0.06500425338745117
			 train-loss:  2.1506367333947796 	 ± 0.2081565027394612
	data : 0.11471824645996094
	model : 0.06497435569763184
			 train-loss:  2.150598434177605 	 ± 0.20674551490297288
	data : 0.11458053588867187
	model : 0.06491637229919434
			 train-loss:  2.1507456890741983 	 ± 0.20536649299013462
	data : 0.11458334922790528
	model : 0.06491265296936036
			 train-loss:  2.1601264555203286 	 ± 0.21959136014412747
	data : 0.11468358039855957
	model : 0.06486115455627442
			 train-loss:  2.1650774432467177 	 ± 0.2223894336003663
	data : 0.11467814445495605
	model : 0.06483359336853027
			 train-loss:  2.167403091222812 	 ± 0.22189966306463219
	data : 0.1147535800933838
	model : 0.06484718322753906
			 train-loss:  2.1692971353289447 	 ± 0.22112438490225228
	data : 0.1148214340209961
	model : 0.06489119529724122
			 train-loss:  2.1665599018335344 	 ± 0.22108074770970626
	data : 0.11482057571411133
	model : 0.06493477821350098
			 train-loss:  2.1669476356035395 	 ± 0.21973918174406676
	data : 0.11477365493774414
	model : 0.06500883102416992
			 train-loss:  2.1657125659105256 	 ± 0.21867789084174294
	data : 0.11482315063476563
	model : 0.06498289108276367
			 train-loss:  2.1678572505353446 	 ± 0.21822247418604174
	data : 0.11485867500305176
	model : 0.06497998237609863
			 train-loss:  2.166633299418858 	 ± 0.21720605349478
	data : 0.11486897468566895
	model : 0.06495952606201172
			 train-loss:  2.1701328978819006 	 ± 0.21829382209509826
	data : 0.11485409736633301
	model : 0.06492838859558106
			 train-loss:  2.1722219544787738 	 ± 0.2178739334677243
	data : 0.11469092369079589
	model : 0.06490120887756348
			 train-loss:  2.1707490504473106 	 ± 0.21704838628920764
	data : 0.11463737487792969
	model : 0.06504688262939454
			 train-loss:  2.1729725003242493 	 ± 0.21680582444423296
	data : 0.11457901000976563
	model : 0.06507482528686523
			 train-loss:  2.1701634532949896 	 ± 0.2171888755413636
	data : 0.11458892822265625
	model : 0.0651134967803955
			 train-loss:  2.1713825398021274 	 ± 0.21628489233360323
	data : 0.11451091766357421
	model : 0.06514205932617187
			 train-loss:  2.171418243712121 	 ± 0.21509349766197144
	data : 0.11465587615966796
	model : 0.06515250205993653
			 train-loss:  2.1762832960356837 	 ± 0.21889765826391575
	data : 0.11469054222106934
	model : 0.06505904197692872
			 train-loss:  2.1764469877366097 	 ± 0.21772326962789237
	data : 0.11469545364379882
	model : 0.06505346298217773
			 train-loss:  2.1743648990671685 	 ± 0.2174909057472127
	data : 0.1146571159362793
	model : 0.06499567031860351
			 train-loss:  2.175725317001343 	 ± 0.21674488338018497
	data : 0.11471147537231445
	model : 0.06495451927185059
			 train-loss:  2.173819816360871 	 ± 0.21641147225183854
	data : 0.11457414627075195
	model : 0.06488089561462403
			 train-loss:  2.173366845268564 	 ± 0.21533880026511495
	data : 0.11455297470092773
	model : 0.06487817764282226
			 train-loss:  2.1776860940213107 	 ± 0.2184198954576298
	data : 0.11462702751159667
	model : 0.06485438346862793
			 train-loss:  2.1770680781566734 	 ± 0.21740006850014054
	data : 0.11463851928710937
	model : 0.06491522789001465
			 train-loss:  2.1763333642482756 	 ± 0.21643382945884238
	data : 0.11471219062805176
	model : 0.06493153572082519
			 train-loss:  2.1757996802282804 	 ± 0.21542582563993073
	data : 0.11481142044067383
	model : 0.06500687599182128
			 train-loss:  2.1759696859939424 	 ± 0.21437402419861018
	data : 0.11491808891296387
	model : 0.06502199172973633
			 train-loss:  2.1758876923218513 	 ± 0.21333244262933343
	data : 0.11481056213378907
	model : 0.0650393009185791
			 train-loss:  2.1781850033081493 	 ± 0.21358072147278062
	data : 0.11481714248657227
	model : 0.06500124931335449
			 train-loss:  2.1790977761858987 	 ± 0.21276495867207154
	data : 0.11475830078125
	model : 0.06498675346374512
			 train-loss:  2.1791650360485293 	 ± 0.21176009371929283
	data : 0.11483492851257324
	model : 0.06507806777954102
			 train-loss:  2.177860359165156 	 ± 0.21119583635861136
	data : 0.11473956108093261
	model : 0.06510934829711915
			 train-loss:  2.1782210414056427 	 ± 0.21024890976124952
	data : 0.11481242179870606
	model : 0.06510581970214843
			 train-loss:  2.1789456124699442 	 ± 0.20941766300418466
	data : 0.11483268737792969
	model : 0.06513724327087403
			 train-loss:  2.182628845084797 	 ± 0.21198062398447204
	data : 0.11490139961242676
	model : 0.06511526107788086
			 train-loss:  2.182094272192534 	 ± 0.21109806383038382
	data : 0.11479020118713379
	model : 0.06505041122436524
			 train-loss:  2.1828866952231953 	 ± 0.21031931616189425
	data : 0.11478915214538574
	model : 0.06501193046569824
			 train-loss:  2.1840551922806597 	 ± 0.20975148315264308
	data : 0.11470003128051758
	model : 0.06500887870788574
			 train-loss:  2.184223941543646 	 ± 0.2088371984334932
	data : 0.11470060348510742
	model : 0.0650331974029541
			 train-loss:  2.1838750372762266 	 ± 0.20796059694611238
	data : 0.11463236808776855
	model : 0.06511650085449219
			 train-loss:  2.182800090518491 	 ± 0.20738290520251965
	data : 0.11469206809997559
	model : 0.06508927345275879
			 train-loss:  2.1836298887546244 	 ± 0.20668806449723937
	data : 0.11475944519042969
	model : 0.06510353088378906
			 train-loss:  2.188107299602638 	 ± 0.21143189401002577
	data : 0.11485629081726074
	model : 0.06505751609802246
			 train-loss:  2.1865471781802777 	 ± 0.21122262213318907
	data : 0.11488566398620606
	model : 0.06503810882568359
			 train-loss:  2.1874837309122084 	 ± 0.21058865842677574
	data : 0.11485023498535156
	model : 0.06492948532104492
			 train-loss:  2.185622348273096 	 ± 0.21070558499147451
	data : 0.11485128402709961
	model : 0.06492815017700196
			 train-loss:  2.1850728783451143 	 ± 0.20992728973230554
	data : 0.11494255065917969
	model : 0.06494221687316895
			 train-loss:  2.18660835425059 	 ± 0.20975894819586255
	data : 0.1148909091949463
	model : 0.06500043869018554
			 train-loss:  2.187899521281642 	 ± 0.2094016284494733
	data : 0.11491885185241699
	model : 0.06499814987182617
			 train-loss:  2.1866390295028686 	 ± 0.20903412583196335
	data : 0.1149327278137207
	model : 0.06507358551025391
			 train-loss:  2.1849096256589133 	 ± 0.20909885854723947
	data : 0.11498265266418457
	model : 0.0650749683380127
			 train-loss:  2.185679980150358 	 ± 0.20845343967255303
	data : 0.11489334106445312
	model : 0.06504349708557129
			 train-loss:  2.185788230970502 	 ± 0.20764115546432957
	data : 0.11466836929321289
	model : 0.06501083374023438
			 train-loss:  2.18881041504616 	 ± 0.20964189983547807
	data : 0.11465468406677246
	model : 0.06495485305786133
			 train-loss:  2.189162725668687 	 ± 0.2088723609956675
	data : 0.11480064392089843
	model : 0.06492977142333985
			 train-loss:  2.1871613882880174 	 ± 0.20932110085109606
	data : 0.11481895446777343
	model : 0.06496062278747558
			 train-loss:  2.187321864294283 	 ± 0.20853479944183645
	data : 0.11482005119323731
	model : 0.06499309539794922
			 train-loss:  2.189164748765472 	 ± 0.20882551543704747
	data : 0.11503400802612304
	model : 0.06501884460449218
			 train-loss:  2.1910449488839108 	 ± 0.20917178974859238
	data : 0.11544814109802246
	model : 0.06508722305297851
			 train-loss:  2.1897340094601665 	 ± 0.2089474332559366
	data : 0.1154172420501709
	model : 0.06509690284729004
			 train-loss:  2.1902672832503036 	 ± 0.20827001467347359
	data : 0.1154740333557129
	model : 0.06506271362304687
			 train-loss:  2.192985347587697 	 ± 0.2099155396578491
	data : 0.1154242992401123
	model : 0.06503467559814453
			 train-loss:  2.1911242371020108 	 ± 0.21028494214005897
	data : 0.11539554595947266
	model : 0.06504569053649903
			 train-loss:  2.1906177448711808 	 ± 0.20961161916233748
	data : 0.11488814353942871
	model : 0.06497092247009277
			 train-loss:  2.190079661778041 	 ± 0.20895798644140298
	data : 0.11476821899414062
	model : 0.06497273445129395
			 train-loss:  2.1915535148999368 	 ± 0.20894469204363153
	data : 0.1146660327911377
	model : 0.06495342254638672
			 train-loss:  2.190367105980994 	 ± 0.20868373503192128
	data : 0.11477670669555665
	model : 0.06500477790832519
			 train-loss:  2.1904502081704305 	 ± 0.20795514935694276
	data : 0.11491074562072753
	model : 0.06502733230590821
			 train-loss:  2.1907553888029523 	 ± 0.20726395632241437
	data : 0.11494231224060059
	model : 0.06510286331176758
			 train-loss:  2.190717159468552 	 ± 0.20654852596348422
	data : 0.1149571418762207
	model : 0.06513566970825195
			 train-loss:  2.1913860611719627 	 ± 0.20599748387616218
	data : 0.11502299308776856
	model : 0.06512370109558105
			 train-loss:  2.1921119657503505 	 ± 0.20548290199396652
	data : 0.11501045227050781
	model : 0.06508264541625977
			 train-loss:  2.1951822348543115 	 ± 0.208143301989238
	data : 0.11475105285644531
	model : 0.06499948501586914
			 train-loss:  2.1957144161198765 	 ± 0.20754466415378606
	data : 0.11473112106323242
	model : 0.06496357917785645
			 train-loss:  2.1950050497055056 	 ± 0.2070328458812327
	data : 0.11481657028198242
	model : 0.06492104530334472
			 train-loss:  2.192890254866998 	 ± 0.20796536870761265
	data : 0.11484947204589843
	model : 0.0649569034576416
			 train-loss:  2.1935684657410572 	 ± 0.20744761566158884
	data : 0.1147921085357666
	model : 0.0649564266204834
			 train-loss:  2.193186364921869 	 ± 0.20682222818827747
	data : 0.1148604393005371
	model : 0.06501588821411133
			 train-loss:  2.1943750683363383 	 ± 0.2066733241916048
	data : 0.11493043899536133
	model : 0.06501889228820801
			 train-loss:  2.1936952490960397 	 ± 0.20617822719648826
	data : 0.11486973762512206
	model : 0.06501326560974122
			 train-loss:  2.1932846911442585 	 ± 0.20557989027764448
	data : 0.11477470397949219
	model : 0.06498508453369141
			 train-loss:  2.1943395464283646 	 ± 0.20534722761923543
	data : 0.11470317840576172
	model : 0.06494436264038086
			 train-loss:  2.1930370579791973 	 ± 0.20534592238129182
	data : 0.11477270126342773
	model : 0.06492199897766113
			 train-loss:  2.1947198686359815 	 ± 0.2057891626846792
	data : 0.11474804878234864
	model : 0.06492137908935547
			 train-loss:  2.1961485035717487 	 ± 0.2059344938413487
	data : 0.11473851203918457
	model : 0.06493988037109374
			 train-loss:  2.1957723924091885 	 ± 0.20534906684718993
	data : 0.11462020874023438
	model : 0.06501379013061523
			 train-loss:  2.195038226651557 	 ± 0.20492613395743575
	data : 0.1146927833557129
	model : 0.06505832672119141
			 train-loss:  2.194956076657114 	 ± 0.20429923478587975
	data : 0.11475887298583984
	model : 0.06514768600463867
			 train-loss:  2.1946193524977056 	 ± 0.2037207836912911
	data : 0.1147651195526123
	model : 0.06517605781555176
			 train-loss:  2.1928545222137914 	 ± 0.20435613064439528
	data : 0.11466493606567382
	model : 0.0651698112487793
			 train-loss:  2.1913625741579446 	 ± 0.2046390193704668
	data : 0.11475391387939453
	model : 0.06515049934387207
			 train-loss:  2.1929779745147613 	 ± 0.20508424376785322
	data : 0.11469740867614746
	model : 0.06515922546386718
			 train-loss:  2.192397199216343 	 ± 0.20461065877990298
	data : 0.1147183895111084
	model : 0.0651219367980957
			 train-loss:  2.1904888484604967 	 ± 0.20549846603803754
	data : 0.11467204093933106
	model : 0.06513242721557617
			 train-loss:  2.18969064950943 	 ± 0.20515575439607805
	data : 0.11478204727172851
	model : 0.06515755653381347
			 train-loss:  2.1914580237795733 	 ± 0.20584888570904403
	data : 0.11482782363891601
	model : 0.06513919830322265
			 train-loss:  2.1899953713250713 	 ± 0.20613887038293852
	data : 0.11490912437438965
	model : 0.06513776779174804
			 train-loss:  2.189779593765391 	 ± 0.20556170987243427
	data : 0.11477894783020019
	model : 0.06509189605712891
			 train-loss:  2.1904820155823366 	 ± 0.20517827802512406
	data : 0.1147188663482666
	model : 0.06502838134765625
			 train-loss:  2.1901552397864203 	 ± 0.20463661743654002
	data : 0.11464018821716308
	model : 0.06499700546264649
			 train-loss:  2.193247661671855 	 ± 0.2081147528366879
	data : 0.11465144157409668
	model : 0.06495375633239746
			 train-loss:  2.192652281394786 	 ± 0.20767628479064842
	data : 0.11466403007507324
	model : 0.06494283676147461
			 train-loss:  2.1948582034432484 	 ± 0.20916127113476055
	data : 0.11478338241577149
	model : 0.06494803428649902
			 train-loss:  2.194470013320113 	 ± 0.20864049406713736
	data : 0.11486349105834961
	model : 0.06502089500427247
			 train-loss:  2.194542141093148 	 ± 0.2080623678357004
	data : 0.11501431465148926
	model : 0.06501960754394531
			 train-loss:  2.1967489488875667 	 ± 0.20958859233369517
	data : 0.11496639251708984
	model : 0.06501317024230957
			 train-loss:  2.197844636964274 	 ± 0.20953117947899746
	data : 0.1149590015411377
	model : 0.06502842903137207
			 train-loss:  2.1967601118191995 	 ± 0.20946950565561462
	data : 0.11477360725402833
	model : 0.06501941680908203
			 train-loss:  2.1964088892159253 	 ± 0.20895354425591403
	data : 0.11473274230957031
	model : 0.0649808406829834
			 train-loss:  2.194967755755863 	 ± 0.20930293446664008
	data : 0.11468048095703125
	model : 0.06500048637390136
			 train-loss:  2.194581021544754 	 ± 0.2088058002750154
	data : 0.11482176780700684
	model : 0.06505413055419922
			 train-loss:  2.194814616983587 	 ± 0.20827111493249345
	data : 0.11482882499694824
	model : 0.06504402160644532
			 train-loss:  2.194512892276683 	 ± 0.20775743890207318
	data : 0.11491107940673828
	model : 0.06505284309387208
			 train-loss:  2.1952887666288508 	 ± 0.20747999753223845
	data : 0.11499128341674805
	model : 0.06506633758544922
			 train-loss:  2.1942677786475735 	 ± 0.2074087701422309
	data : 0.11495137214660645
	model : 0.0650486946105957
			 train-loss:  2.1933110671517735 	 ± 0.207285014233993
	data : 0.11472249031066895
	model : 0.06506237983703614
			 train-loss:  2.193793577452501 	 ± 0.2068520199521974
	data : 0.11463813781738282
	model : 0.06501951217651367
			 train-loss:  2.194635958251558 	 ± 0.20664535844858414
	data : 0.11467175483703614
	model : 0.06502318382263184
			 train-loss:  2.195259631294565 	 ± 0.20629411051910013
	data : 0.11475210189819336
	model : 0.06498970985412597
			 train-loss:  2.194769807962271 	 ± 0.20587754489915683
	data : 0.11487131118774414
	model : 0.06498851776123046
			 train-loss:  2.1964273294624017 	 ± 0.20665200031889036
	data : 0.11495203971862793
	model : 0.06496682167053222
			 train-loss:  2.195725207401411 	 ± 0.20636108042040383
	data : 0.11492772102355957
	model : 0.06503596305847167
			 train-loss:  2.194378953991514 	 ± 0.20670477192463949
	data : 0.1150658130645752
	model : 0.06502084732055664
			 train-loss:  2.1928130861502795 	 ± 0.20735872158778504
	data : 0.11503286361694336
	model : 0.06506805419921875
			 train-loss:  2.191351541876793 	 ± 0.2078647094685313
	data : 0.11491823196411133
	model : 0.06511244773864747
			 train-loss:  2.1922616181682 	 ± 0.20774605013284927
	data : 0.11473588943481446
	model : 0.06511044502258301
			 train-loss:  2.1926530723524564 	 ± 0.20730549029544845
	data : 0.11487360000610351
	model : 0.06506938934326172
			 train-loss:  2.1932469524186233 	 ± 0.20696644197362835
	data : 0.11478333473205567
	model : 0.0650557041168213
			 train-loss:  2.192374625042373 	 ± 0.20683231362273793
	data : 0.11484303474426269
	model : 0.06519551277160644
			 train-loss:  2.1923879431515205 	 ± 0.20632731553954073
	data : 0.11478009223937988
	model : 0.0651625633239746
			 train-loss:  2.19307530215643 	 ± 0.20606106080221276
	data : 0.11495928764343262
	model : 0.06516876220703124
			 train-loss:  2.1923412395560224 	 ± 0.205832546177785
	data : 0.11477751731872558
	model : 0.06518473625183105
			 train-loss:  2.191378611211593 	 ± 0.20580370897297748
	data : 0.11472077369689941
	model : 0.06518330574035644
			 train-loss:  2.191139266251377 	 ± 0.20533978138066475
	data : 0.11444482803344727
	model : 0.06503348350524903
			 train-loss:  2.1899388029461817 	 ± 0.20558413186849872
	data : 0.1145505428314209
	model : 0.06495785713195801
			 train-loss:  2.189058508353211 	 ± 0.20549272712123162
	data : 0.11455264091491699
	model : 0.06492953300476074
			 train-loss:  2.1876605428614706 	 ± 0.20601076329331444
	data : 0.11476054191589355
	model : 0.06496238708496094
			 train-loss:  2.188027414917386 	 ± 0.20559600649917534
	data : 0.11488456726074218
	model : 0.0649829387664795
			 train-loss:  2.190166253352834 	 ± 0.207476726407757
	data : 0.11504559516906739
	model : 0.06495208740234375
			 train-loss:  2.1910387754440306 	 ± 0.20738681824315297
	data : 0.11500716209411621
	model : 0.06505217552185058
			 train-loss:  2.1928407897551856 	 ± 0.2085865173875036
	data : 0.11503701210021973
	model : 0.06503257751464844
			 train-loss:  2.1941984760047104 	 ± 0.20905977943352322
	data : 0.11503558158874512
	model : 0.06496081352233887
			 train-loss:  2.194769953915832 	 ± 0.20874954983771007
	data : 0.11490869522094727
	model : 0.0649134635925293
			 train-loss:  2.1971657216276754 	 ± 0.21125494294035987
	data : 0.11474032402038574
	model : 0.06488628387451172
			 train-loss:  2.1955713201652873 	 ± 0.21209082210615027
	data : 0.11473217010498046
	model : 0.06487088203430176
			 train-loss:  2.1950357397217557 	 ± 0.21175949148053141
	data : 0.11477484703063964
	model : 0.06485476493835449
			 train-loss:  2.194357799517142 	 ± 0.2115222520592939
	data : 0.11480040550231933
	model : 0.06483426094055175
			 train-loss:  2.1927325244441693 	 ± 0.21243221292349168
	data : 0.11495513916015625
	model : 0.06478261947631836
			 train-loss:  2.192347083772932 	 ± 0.21203564126134056
	data : 0.11517148017883301
	model : 0.06470398902893067
			 train-loss:  2.1913322517606946 	 ± 0.21210843666224072
	data : 0.11531915664672851
	model : 0.06447982788085938
			 train-loss:  2.1913749132536156 	 ± 0.21163961740801307
	data : 0.11531524658203125
	model : 0.06431565284729004
			 train-loss:  2.192553058594859 	 ± 0.21191437659857704
	data : 0.11526975631713868
	model : 0.0641061782836914
			 train-loss:  2.192167969649298 	 ± 0.21152872623609467
	data : 0.11534004211425782
	model : 0.06397695541381836
			 train-loss:  2.1926032752449336 	 ± 0.21116868982039005
	data : 0.11552042961120605
	model : 0.06386394500732422
			 train-loss:  2.193252466554227 	 ± 0.2109380199606259
	data : 0.11561999320983887
	model : 0.06383142471313477
			 train-loss:  2.1920049933644083 	 ± 0.21132949019669495
	data : 0.11569552421569824
	model : 0.06382145881652831
			 train-loss:  2.191866181020079 	 ± 0.21088410057122683
	data : 0.1157524585723877
	model : 0.06385412216186523
			 train-loss:  2.191547227008148 	 ± 0.21048714502471688
	data : 0.11586475372314453
	model : 0.0638467788696289
			 train-loss:  2.191788701929598 	 ± 0.21006924469627827
	data : 0.11572122573852539
	model : 0.06383404731750489
			 train-loss:  2.1921489319902787 	 ± 0.20969422831571544
	data : 0.11555027961730957
	model : 0.06378293037414551
			 train-loss:  2.190750218549017 	 ± 0.2103451986360737
	data : 0.11555614471435546
	model : 0.06381688117980958
			 train-loss:  2.1923564320375144 	 ± 0.211346341180866
	data : 0.11572432518005371
	model : 0.0638012409210205
			 train-loss:  2.1914137997547116 	 ± 0.21140053448999763
	data : 0.11579556465148926
	model : 0.0638089656829834
			 train-loss:  2.1908092014959166 	 ± 0.2111639080235253
	data : 0.11573410034179688
	model : 0.06386113166809082
			 train-loss:  2.189699306090673 	 ± 0.21142095443675357
	data : 0.11584939956665039
	model : 0.06393470764160156
			 train-loss:  2.1912364118821395 	 ± 0.21232144099052383
	data : 0.11586380004882812
	model : 0.06387252807617187
			 train-loss:  2.191295469102781 	 ± 0.21188428972491108
	data : 0.1156682014465332
	model : 0.06389126777648926
			 train-loss:  2.191172558584331 	 ± 0.21145650908711114
	data : 0.11546196937561035
	model : 0.06384663581848145
			 train-loss:  2.1910223032607408 	 ± 0.21103575019217885
	data : 0.11551904678344727
	model : 0.06378750801086426
			 train-loss:  2.1924038653471034 	 ± 0.21170742565049022
	data : 0.11567850112915039
	model : 0.06374478340148926
			 train-loss:  2.1933043070924962 	 ± 0.21174627192140927
	data : 0.11572427749633789
	model : 0.06376180648803711
			 train-loss:  2.1960002053604435 	 ± 0.21550604363942139
	data : 0.11575198173522949
	model : 0.06377291679382324
			 train-loss:  2.1953336269624772 	 ± 0.21532611076923475
	data : 0.11571283340454101
	model : 0.06384272575378418
			 train-loss:  2.194486557719219 	 ± 0.21530693054697328
	data : 0.1158370018005371
	model : 0.0638509750366211
			 train-loss:  2.194290403366089 	 ± 0.21489817751722717
	data : 0.11565465927124023
	model : 0.06385979652404786
			 train-loss:  2.193700228079382 	 ± 0.21467257502179626
	data : 0.1155919075012207
	model : 0.0638819694519043
			 train-loss:  2.192740815499472 	 ± 0.21478472664473477
	data : 0.1156184196472168
	model : 0.06384100914001464
			 train-loss:  2.19116282981375 	 ± 0.2158185042348087
	data : 0.11568107604980468
	model : 0.06381807327270508
			 train-loss:  2.1910967380981745 	 ± 0.21539581105458994
	data : 0.11562132835388184
	model : 0.06384329795837403
			 train-loss:  2.1929302164152555 	 ± 0.21694992534073182
	data : 0.11567821502685546
	model : 0.06383891105651855
			 train-loss:  2.196476766373962 	 ± 0.22380975809143874
	data : 0.11547617912292481
	model : 0.05540952682495117
#epoch  15    val-loss:  2.411744807895861  train-loss:  2.196476766373962  lr:  0.000625
			 train-loss:  2.0424482822418213 	 ± 0.0
	data : 5.681981563568115
	model : 0.0707709789276123
			 train-loss:  2.4397305250167847 	 ± 0.3972822427749634
	data : 2.905942678451538
	model : 0.06937587261199951
			 train-loss:  2.302840312321981 	 ± 0.3777565101307021
	data : 1.9755975405375164
	model : 0.06784272193908691
			 train-loss:  2.2869306802749634 	 ± 0.32830524650880905
	data : 1.5104977488517761
	model : 0.06699323654174805
			 train-loss:  2.32869873046875 	 ± 0.30529616436524487
	data : 1.2311549186706543
	model : 0.06648173332214355
			 train-loss:  2.2789127031962075 	 ± 0.30010781263520475
	data : 0.1176386833190918
	model : 0.06528420448303222
			 train-loss:  2.2180024896349226 	 ± 0.31537063639550483
	data : 0.11465015411376953
	model : 0.06460933685302735
			 train-loss:  2.1901649087667465 	 ± 0.30405727235685087
	data : 0.11464815139770508
	model : 0.06461563110351562
			 train-loss:  2.2049727042516074 	 ± 0.28971136921990537
	data : 0.11459555625915527
	model : 0.06471586227416992
			 train-loss:  2.207368624210358 	 ± 0.2749383087199449
	data : 0.11471962928771973
	model : 0.06479830741882324
			 train-loss:  2.2445416992360894 	 ± 0.28729346126998834
	data : 0.11473174095153808
	model : 0.06482076644897461
			 train-loss:  2.267017036676407 	 ± 0.2849841291570466
	data : 0.11455440521240234
	model : 0.06485176086425781
			 train-loss:  2.2396697264451246 	 ± 0.28972931734120605
	data : 0.11439704895019531
	model : 0.06490788459777833
			 train-loss:  2.232292039053781 	 ± 0.2804545181636278
	data : 0.1143643856048584
	model : 0.064892578125
			 train-loss:  2.231658395131429 	 ± 0.27095517840752026
	data : 0.11443204879760742
	model : 0.06489195823669433
			 train-loss:  2.2311417311429977 	 ± 0.26235885450445456
	data : 0.11442251205444336
	model : 0.0649181842803955
			 train-loss:  2.2343413829803467 	 ± 0.25484705569530974
	data : 0.11450672149658203
	model : 0.06500449180603027
			 train-loss:  2.2294125821855335 	 ± 0.24849916746873696
	data : 0.11452288627624511
	model : 0.06496872901916503
			 train-loss:  2.2440006481973747 	 ± 0.24966447414122947
	data : 0.11452031135559082
	model : 0.0649785041809082
			 train-loss:  2.2505538702011108 	 ± 0.2450136359874294
	data : 0.1145632266998291
	model : 0.06496639251708984
			 train-loss:  2.2502178237551735 	 ± 0.23911354802442594
	data : 0.11437439918518066
	model : 0.06492195129394532
			 train-loss:  2.241275733167475 	 ± 0.23718261476892505
	data : 0.11437311172485351
	model : 0.06483869552612305
			 train-loss:  2.2435544055441152 	 ± 0.23221526449251562
	data : 0.11446247100830079
	model : 0.06533751487731934
			 train-loss:  2.2280835807323456 	 ± 0.23912771975220473
	data : 0.11391386985778809
	model : 0.0654137134552002
			 train-loss:  2.220242338180542 	 ± 0.23742456851362828
	data : 0.11385879516601563
	model : 0.06549568176269531
			 train-loss:  2.2170696350244374 	 ± 0.23335377341998229
	data : 0.11415557861328125
	model : 0.0655472755432129
			 train-loss:  2.218531431975188 	 ± 0.2291129142399621
	data : 0.11419820785522461
	model : 0.06557264328002929
			 train-loss:  2.2239028896604265 	 ± 0.22670907847200175
	data : 0.11422543525695801
	model : 0.06505160331726074
			 train-loss:  2.232125553591498 	 ± 0.22697541135623522
	data : 0.1146115779876709
	model : 0.06492900848388672
			 train-loss:  2.2321035861968994 	 ± 0.22316045821205452
	data : 0.11456103324890136
	model : 0.06486268043518066
			 train-loss:  2.2319090981637277 	 ± 0.21953417553669125
	data : 0.11445636749267578
	model : 0.0648345947265625
			 train-loss:  2.2333851158618927 	 ± 0.2162329531073744
	data : 0.11444759368896484
	model : 0.06482744216918945
			 train-loss:  2.225548661116398 	 ± 0.2174969915897912
	data : 0.11440620422363282
	model : 0.06493058204650878
			 train-loss:  2.2294725635472465 	 ± 0.21545700472633772
	data : 0.11452054977416992
	model : 0.06500520706176757
			 train-loss:  2.228739871297564 	 ± 0.21239971396617144
	data : 0.11455073356628417
	model : 0.06501860618591308
			 train-loss:  2.2235664096143513 	 ± 0.21165360041982945
	data : 0.1145890235900879
	model : 0.06505918502807617
			 train-loss:  2.2304561492559074 	 ± 0.21282710499534183
	data : 0.11457457542419433
	model : 0.06507248878479004
			 train-loss:  2.2372718390665556 	 ± 0.214061153053119
	data : 0.11456437110900879
	model : 0.06495676040649415
			 train-loss:  2.241702993710836 	 ± 0.2130572332051307
	data : 0.11452054977416992
	model : 0.06494135856628418
			 train-loss:  2.2362019032239915 	 ± 0.21316371143028395
	data : 0.11456646919250488
	model : 0.06496777534484863
			 train-loss:  2.2308962083444364 	 ± 0.21320534975918726
	data : 0.11464476585388184
	model : 0.06497440338134766
			 train-loss:  2.2393561743554615 	 ± 0.2175054828369957
	data : 0.11473550796508789
	model : 0.06502213478088378
			 train-loss:  2.236296479092088 	 ± 0.21587410016745184
	data : 0.11468696594238281
	model : 0.06509518623352051
			 train-loss:  2.2333463728427887 	 ± 0.21428190238533476
	data : 0.11473097801208496
	model : 0.06509485244750976
			 train-loss:  2.226264564196269 	 ± 0.21703237185536117
	data : 0.11474223136901855
	model : 0.06507296562194824
			 train-loss:  2.220175696455914 	 ± 0.2185118189986528
	data : 0.1145700454711914
	model : 0.06508035659790039
			 train-loss:  2.221054660513046 	 ± 0.21625691011809847
	data : 0.11440954208374024
	model : 0.06501531600952148
			 train-loss:  2.2210170328617096 	 ± 0.21399253293559742
	data : 0.11455831527709961
	model : 0.06496624946594239
			 train-loss:  2.2199526368355262 	 ± 0.21192602057299434
	data : 0.11465301513671874
	model : 0.06498003005981445
			 train-loss:  2.222066297531128 	 ± 0.2103171318444066
	data : 0.11467385292053223
	model : 0.06501665115356445
			 train-loss:  2.2184531267951515 	 ± 0.2098064025649496
	data : 0.11474919319152832
	model : 0.06503229141235352
			 train-loss:  2.215432359622075 	 ± 0.20889612091899112
	data : 0.11491641998291016
	model : 0.0650787353515625
			 train-loss:  2.2182921643527047 	 ± 0.20794114483164036
	data : 0.11485404968261718
	model : 0.06507949829101563
			 train-loss:  2.215873780073943 	 ± 0.20675773925269905
	data : 0.11484174728393555
	model : 0.06503801345825196
			 train-loss:  2.215308800610629 	 ± 0.2049115648422724
	data : 0.11467118263244629
	model : 0.06498322486877442
			 train-loss:  2.2195656257016316 	 ± 0.20551297064082757
	data : 0.11467242240905762
	model : 0.0648913860321045
			 train-loss:  2.218819785536381 	 ± 0.20377869745753452
	data : 0.1146303653717041
	model : 0.06486477851867675
			 train-loss:  2.213079834806508 	 ± 0.20661020584756773
	data : 0.1146780014038086
	model : 0.06492280960083008
			 train-loss:  2.2100687713946328 	 ± 0.20613129961281407
	data : 0.11473793983459472
	model : 0.06500897407531739
			 train-loss:  2.20876339673996 	 ± 0.2046520957912589
	data : 0.11479177474975585
	model : 0.06504406929016113
			 train-loss:  2.2152500895203135 	 ± 0.20909450798014467
	data : 0.11474823951721191
	model : 0.06506280899047852
			 train-loss:  2.2098513995447466 	 ± 0.21164413707735538
	data : 0.1146934986114502
	model : 0.06506004333496093
			 train-loss:  2.2071180003029958 	 ± 0.21105797347023234
	data : 0.11459474563598633
	model : 0.0649874210357666
			 train-loss:  2.205546911805868 	 ± 0.20977356679625622
	data : 0.11440052986145019
	model : 0.06495785713195801
			 train-loss:  2.2095535498399 	 ± 0.21060710244316302
	data : 0.11450605392456055
	model : 0.06489853858947754
			 train-loss:  2.2062916845986336 	 ± 0.21065347263476594
	data : 0.1146571159362793
	model : 0.06491603851318359
			 train-loss:  2.2052636520186466 	 ± 0.2092422662220892
	data : 0.11478133201599121
	model : 0.06495122909545899
			 train-loss:  2.2113744178239036 	 ± 0.21363601281978714
	data : 0.11483530998229981
	model : 0.06498894691467286
			 train-loss:  2.214105853136035 	 ± 0.21327499133006908
	data : 0.11485805511474609
	model : 0.06496853828430176
			 train-loss:  2.217717352935246 	 ± 0.21386065786309788
	data : 0.11477479934692383
	model : 0.06500515937805176
			 train-loss:  2.2205083924280085 	 ± 0.21362935122104632
	data : 0.11447629928588868
	model : 0.06492428779602051
			 train-loss:  2.217020114262899 	 ± 0.21416717662644222
	data : 0.11440277099609375
	model : 0.06487951278686524
			 train-loss:  2.2212946774208384 	 ± 0.2157656891872641
	data : 0.1144597053527832
	model : 0.06488666534423829
			 train-loss:  2.2222178175642684 	 ± 0.21444794933611783
	data : 0.11453714370727539
	model : 0.06487207412719727
			 train-loss:  2.222309424082438 	 ± 0.21301495641843696
	data : 0.1145134449005127
	model : 0.06493520736694336
			 train-loss:  2.220369147626977 	 ± 0.21227500500129406
	data : 0.11470556259155273
	model : 0.0650489330291748
			 train-loss:  2.2268788752617774 	 ± 0.21839435028407678
	data : 0.11461338996887208
	model : 0.06506314277648925
			 train-loss:  2.22624217852568 	 ± 0.21706178462480047
	data : 0.11450262069702148
	model : 0.06505155563354492
			 train-loss:  2.2220753280422354 	 ± 0.21880059810443073
	data : 0.11454381942749023
	model : 0.06501383781433105
			 train-loss:  2.2232267931103706 	 ± 0.21766953009474843
	data : 0.11459050178527833
	model : 0.06498184204101562
			 train-loss:  2.225193708031266 	 ± 0.21703590971117176
	data : 0.11467118263244629
	model : 0.06492848396301269
			 train-loss:  2.2241109973046838 	 ± 0.21592844462012853
	data : 0.11480522155761719
	model : 0.06493587493896484
			 train-loss:  2.224677845656154 	 ± 0.21468510214652572
	data : 0.11485190391540527
	model : 0.06493153572082519
			 train-loss:  2.222015231847763 	 ± 0.21477764264822374
	data : 0.11469707489013672
	model : 0.06497817039489746
			 train-loss:  2.2213521859225103 	 ± 0.21359696966145147
	data : 0.11469783782958984
	model : 0.06496548652648926
			 train-loss:  2.2249750190002975 	 ± 0.2149622661517347
	data : 0.11463365554809571
	model : 0.06494135856628418
			 train-loss:  2.226018416470495 	 ± 0.21394220445349751
	data : 0.11453194618225097
	model : 0.06494226455688476
			 train-loss:  2.2228973602706734 	 ± 0.2147058571648405
	data : 0.1146728515625
	model : 0.06496806144714355
			 train-loss:  2.2195469341921004 	 ± 0.21579729951444093
	data : 0.11482963562011719
	model : 0.06498503684997559
			 train-loss:  2.2198661194907294 	 ± 0.21461620217777364
	data : 0.1148447036743164
	model : 0.06501207351684571
			 train-loss:  2.2213488463517073 	 ± 0.2138967560312383
	data : 0.11485257148742675
	model : 0.06504969596862793
			 train-loss:  2.225885031015977 	 ± 0.21708759571676922
	data : 0.1149399757385254
	model : 0.06503276824951172
			 train-loss:  2.2245031761866745 	 ± 0.21632373451151743
	data : 0.1147547721862793
	model : 0.06503462791442871
			 train-loss:  2.2232241351553736 	 ± 0.2155232510557471
	data : 0.11463556289672852
	model : 0.06496810913085938
			 train-loss:  2.2299874682175487 	 ± 0.2241899482204071
	data : 0.1146780014038086
	model : 0.06493120193481446
			 train-loss:  2.2309601257244744 	 ± 0.22322064278418768
	data : 0.11474418640136719
	model : 0.06492156982421875
			 train-loss:  2.227384977733966 	 ± 0.22481283735791868
	data : 0.11473665237426758
	model : 0.06493587493896484
			 train-loss:  2.2255240265203984 	 ± 0.22441259722837043
	data : 0.11488537788391114
	model : 0.06493749618530273
			 train-loss:  2.2225816418426207 	 ± 0.22516830192050263
	data : 0.11496458053588868
	model : 0.06503558158874512
			 train-loss:  2.2231642031669616 	 ± 0.2241146022458827
	data : 0.11497058868408203
	model : 0.06502180099487305
			 train-loss:  2.2247106084729187 	 ± 0.22353789686069034
	data : 0.1149968147277832
	model : 0.06502008438110352
			 train-loss:  2.2229472375383565 	 ± 0.22314424590959955
	data : 0.11485190391540527
	model : 0.06503777503967285
			 train-loss:  2.2221996830505075 	 ± 0.22218669020257367
	data : 0.11471219062805176
	model : 0.06499567031860351
			 train-loss:  2.221969824570876 	 ± 0.2211282101540491
	data : 0.1147275447845459
	model : 0.06495676040649415
			 train-loss:  2.2211074874514627 	 ± 0.22024833737559932
	data : 0.11476049423217774
	model : 0.06500730514526368
			 train-loss:  2.2191111749073245 	 ± 0.22015936869275674
	data : 0.11469573974609375
	model : 0.06502165794372558
			 train-loss:  2.219508826175583 	 ± 0.21916641394527855
	data : 0.11479887962341309
	model : 0.0650214672088623
			 train-loss:  2.219969771526478 	 ± 0.21820149602338687
	data : 0.11483964920043946
	model : 0.06502594947814941
			 train-loss:  2.2174615794365558 	 ± 0.21875675415113904
	data : 0.11484971046447753
	model : 0.06499285697937011
			 train-loss:  2.2167677077380095 	 ± 0.21788059880010285
	data : 0.11462054252624512
	model : 0.06493721008300782
			 train-loss:  2.2144894567695825 	 ± 0.21820913611139642
	data : 0.11456451416015626
	model : 0.06494159698486328
			 train-loss:  2.2133640998176167 	 ± 0.2175561184253038
	data : 0.11466307640075683
	model : 0.06488142013549805
			 train-loss:  2.215444405521967 	 ± 0.21770739185662863
	data : 0.11471743583679199
	model : 0.06491775512695312
			 train-loss:  2.2161081475124025 	 ± 0.21686523963136936
	data : 0.11464695930480957
	model : 0.06497106552124024
			 train-loss:  2.220737165990083 	 ± 0.22150472136447896
	data : 0.11481976509094238
	model : 0.06498532295227051
			 train-loss:  2.2202909640197097 	 ± 0.22059979416947034
	data : 0.11488842964172363
	model : 0.06497888565063477
			 train-loss:  2.2201833001568785 	 ± 0.21965809768316294
	data : 0.11471920013427735
	model : 0.06501522064208984
			 train-loss:  2.2204774001897394 	 ± 0.21874849526484236
	data : 0.11463403701782227
	model : 0.06492171287536622
			 train-loss:  2.2208026286934603 	 ± 0.21785609246595958
	data : 0.1146543025970459
	model : 0.06489019393920899
			 train-loss:  2.218959395090739 	 ± 0.2178762724260161
	data : 0.11487088203430176
	model : 0.06492199897766113
			 train-loss:  2.215190231307479 	 ± 0.22086771364879965
	data : 0.11494393348693847
	model : 0.0649531364440918
			 train-loss:  2.2167371159694236 	 ± 0.2206178258922414
	data : 0.11498937606811524
	model : 0.06495585441589355
			 train-loss:  2.21632393007356 	 ± 0.2197665673153387
	data : 0.11504511833190918
	model : 0.06503362655639648
			 train-loss:  2.2160041889836712 	 ± 0.2189073416444673
	data : 0.11510467529296875
	model : 0.06503825187683106
			 train-loss:  2.2152087345123292 	 ± 0.21820981087298072
	data : 0.11479053497314454
	model : 0.06498832702636718
			 train-loss:  2.213044027487437 	 ± 0.2186855394382596
	data : 0.11474747657775879
	model : 0.06496853828430176
			 train-loss:  2.2161211000652763 	 ± 0.220544368689895
	data : 0.11486482620239258
	model : 0.06498861312866211
			 train-loss:  2.217688358388841 	 ± 0.22039004039933693
	data : 0.11493563652038574
	model : 0.0649909496307373
			 train-loss:  2.219093499257583 	 ± 0.220108996702344
	data : 0.11494126319885253
	model : 0.06499481201171875
			 train-loss:  2.216512241730323 	 ± 0.22121213031419626
	data : 0.11498703956604003
	model : 0.06509509086608886
			 train-loss:  2.217103976329774 	 ± 0.22046944898302523
	data : 0.11497998237609863
	model : 0.06508941650390625
			 train-loss:  2.218172429185925 	 ± 0.21997293755361688
	data : 0.11488661766052247
	model : 0.06505756378173828
			 train-loss:  2.2176032890950825 	 ± 0.21924194513003006
	data : 0.11468362808227539
	model : 0.06502394676208496
			 train-loss:  2.2153827574715685 	 ± 0.21991842236028558
	data : 0.11472649574279785
	model : 0.06500601768493652
			 train-loss:  2.2170309490627713 	 ± 0.2199315247256657
	data : 0.114764404296875
	model : 0.06490893363952636
			 train-loss:  2.217421878786648 	 ± 0.21916853386151638
	data : 0.11477360725402833
	model : 0.06490321159362793
			 train-loss:  2.2187531934167346 	 ± 0.21891841661917144
	data : 0.11506633758544922
	model : 0.0649186134338379
			 train-loss:  2.2163883888203166 	 ± 0.21987299459885012
	data : 0.11517457962036133
	model : 0.06501812934875488
			 train-loss:  2.2148583801530246 	 ± 0.21981670111820706
	data : 0.11512508392333984
	model : 0.06502785682678222
			 train-loss:  2.2163490389074596 	 ± 0.21973418140237036
	data : 0.11525816917419433
	model : 0.06505999565124512
			 train-loss:  2.2155196252444114 	 ± 0.21917341730318896
	data : 0.11522068977355956
	model : 0.06505470275878907
			 train-loss:  2.2140630777453034 	 ± 0.21908407981770756
	data : 0.11483502388000488
	model : 0.06501045227050781
			 train-loss:  2.2135673844730936 	 ± 0.21839660235225694
	data : 0.1147878646850586
	model : 0.06494259834289551
			 train-loss:  2.212126841975583 	 ± 0.21831764584977567
	data : 0.11480307579040527
	model : 0.06500773429870606
			 train-loss:  2.209580600672755 	 ± 0.21969863128473577
	data : 0.11473708152770996
	model : 0.06508774757385254
			 train-loss:  2.2110049446968185 	 ± 0.2196157074455694
	data : 0.11475582122802734
	model : 0.06511220932006836
			 train-loss:  2.211892543195867 	 ± 0.21913005237730365
	data : 0.11485867500305176
	model : 0.06519389152526855
			 train-loss:  2.2106813965617 	 ± 0.21888162289685129
	data : 0.11492924690246582
	model : 0.06516914367675782
			 train-loss:  2.208631241081545 	 ± 0.2195670539307335
	data : 0.11476798057556152
	model : 0.06513500213623047
			 train-loss:  2.2074292238553364 	 ± 0.21932527259091542
	data : 0.11468172073364258
	model : 0.06504650115966797
			 train-loss:  2.207412505781414 	 ± 0.21859791947321744
	data : 0.11469650268554688
	model : 0.06503777503967285
			 train-loss:  2.205998622273144 	 ± 0.2185692890578909
	data : 0.11503872871398926
	model : 0.06509151458740234
			 train-loss:  2.204268844299067 	 ± 0.21889517896426594
	data : 0.11507453918457031
	model : 0.06514601707458496
			 train-loss:  2.205308486114849 	 ± 0.21856196648996568
	data : 0.11519265174865723
	model : 0.06516618728637695
			 train-loss:  2.2037882720270465 	 ± 0.21867109041649566
	data : 0.11510491371154785
	model : 0.06519870758056641
			 train-loss:  2.2034856577714286 	 ± 0.21800165239001562
	data : 0.1150886058807373
	model : 0.06521773338317871
			 train-loss:  2.201989391047484 	 ± 0.21810839098954216
	data : 0.11481084823608398
	model : 0.06514892578125
			 train-loss:  2.2026574943639057 	 ± 0.21757818151601155
	data : 0.11507163047790528
	model : 0.0651015281677246
			 train-loss:  2.2033059911907844 	 ± 0.21704601853267455
	data : 0.11507616043090821
	model : 0.06501708030700684
			 train-loss:  2.2021473973989485 	 ± 0.21685934396452214
	data : 0.11512570381164551
	model : 0.06498255729675292
			 train-loss:  2.2053949788490437 	 ± 0.2200530878781725
	data : 0.11504435539245605
	model : 0.0649451732635498
			 train-loss:  2.2046635533556524 	 ± 0.21956908741796452
	data : 0.11506690979003906
	model : 0.06494216918945313
			 train-loss:  2.2039959533082927 	 ± 0.219059387943125
	data : 0.11487503051757812
	model : 0.0649909496307373
			 train-loss:  2.2059753406338576 	 ± 0.2198477688888105
	data : 0.11483573913574219
	model : 0.06505160331726074
			 train-loss:  2.2058815941666112 	 ± 0.21918383895454938
	data : 0.11485524177551269
	model : 0.06506338119506835
			 train-loss:  2.2047613342124297 	 ± 0.21899593692504601
	data : 0.11500811576843262
	model : 0.06510019302368164
			 train-loss:  2.203988896158641 	 ± 0.21856597420470933
	data : 0.11485958099365234
	model : 0.06504931449890136
			 train-loss:  2.2050606253601255 	 ± 0.21835418513169874
	data : 0.11467795372009278
	model : 0.06500482559204102
			 train-loss:  2.20326208433456 	 ± 0.2189517447653469
	data : 0.11486349105834961
	model : 0.06501288414001465
			 train-loss:  2.203461754322052 	 ± 0.21832225042453457
	data : 0.11549506187438965
	model : 0.0650148868560791
			 train-loss:  2.2031129347650626 	 ± 0.21773045127934154
	data : 0.11548290252685547
	model : 0.06500959396362305
			 train-loss:  2.2035720507765926 	 ± 0.21717959067446022
	data : 0.11562275886535645
	model : 0.06504502296447753
			 train-loss:  2.205029427660683 	 ± 0.21739285196061578
	data : 0.11564574241638184
	model : 0.06508774757385254
			 train-loss:  2.2056202374655625 	 ± 0.21690650431627528
	data : 0.11547999382019043
	model : 0.06508479118347169
			 train-loss:  2.205176412718637 	 ± 0.21636510352802288
	data : 0.11493620872497559
	model : 0.06508169174194336
			 train-loss:  2.2090359675613316 	 ± 0.22170858987005027
	data : 0.11479692459106446
	model : 0.06503477096557617
			 train-loss:  2.2105146242400346 	 ± 0.22194999337747306
	data : 0.1147273063659668
	model : 0.06500477790832519
			 train-loss:  2.2107503112782254 	 ± 0.2213478708743149
	data : 0.1148529052734375
	model : 0.0649658203125
			 train-loss:  2.2116653659490235 	 ± 0.22106607470734682
	data : 0.11491303443908692
	model : 0.06493687629699707
			 train-loss:  2.210686782333586 	 ± 0.2208395871622565
	data : 0.11487507820129395
	model : 0.06494321823120117
			 train-loss:  2.211245987955378 	 ± 0.22035644540185384
	data : 0.11489996910095215
	model : 0.06497383117675781
			 train-loss:  2.212087246742877 	 ± 0.22004150333262185
	data : 0.11497135162353515
	model : 0.0649787425994873
			 train-loss:  2.210843380683107 	 ± 0.22008015226435823
	data : 0.11482005119323731
	model : 0.06498227119445801
			 train-loss:  2.2113528471925985 	 ± 0.21958947381378408
	data : 0.11479158401489258
	model : 0.06492533683776855
			 train-loss:  2.2091904749741427 	 ± 0.2209507816873865
	data : 0.11479401588439941
	model : 0.06494379043579102
			 train-loss:  2.20722561113296 	 ± 0.22197073340317947
	data : 0.11493425369262696
	model : 0.06493496894836426
			 train-loss:  2.2062868982712853 	 ± 0.22174630772811416
	data : 0.11494097709655762
	model : 0.06496539115905761
			 train-loss:  2.207223814852694 	 ± 0.22152658041277448
	data : 0.11509866714477539
	model : 0.06500716209411621
			 train-loss:  2.2076656730086714 	 ± 0.22102280371378868
	data : 0.11511478424072266
	model : 0.06506061553955078
			 train-loss:  2.208223893767909 	 ± 0.22057394040099804
	data : 0.11514077186584473
	model : 0.06511216163635254
			 train-loss:  2.210209156206141 	 ± 0.2216911745466886
	data : 0.115012788772583
	model : 0.06513710021972656
			 train-loss:  2.2103200120230517 	 ± 0.2211184077261213
	data : 0.11492643356323243
	model : 0.0651285171508789
			 train-loss:  2.2098699110159603 	 ± 0.2206329854541496
	data : 0.1147085189819336
	model : 0.06509566307067871
			 train-loss:  2.209540689114443 	 ± 0.2201111326911213
	data : 0.11453161239624024
	model : 0.06508350372314453
			 train-loss:  2.2087782725309713 	 ± 0.219802690877872
	data : 0.1144956111907959
	model : 0.06500582695007324
			 train-loss:  2.207600910444649 	 ± 0.21985684494896252
	data : 0.11468319892883301
	model : 0.06501469612121583
			 train-loss:  2.206790495039848 	 ± 0.2195914255111554
	data : 0.11481695175170899
	model : 0.06500539779663086
			 train-loss:  2.2058871484766103 	 ± 0.21940286167552964
	data : 0.11505088806152344
	model : 0.0650404930114746
			 train-loss:  2.2048807078270456 	 ± 0.21930863379192253
	data : 0.1152963638305664
	model : 0.06506805419921875
			 train-loss:  2.2036932545900343 	 ± 0.2194000785419841
	data : 0.11531267166137696
	model : 0.06507692337036133
			 train-loss:  2.203673438053226 	 ± 0.21885380612107644
	data : 0.11524205207824707
	model : 0.06508774757385254
			 train-loss:  2.204681863289068 	 ± 0.218779057183654
	data : 0.11518669128417969
	model : 0.06509652137756347
			 train-loss:  2.2041339504307715 	 ± 0.2183784178188588
	data : 0.11504588127136231
	model : 0.06507444381713867
			 train-loss:  2.2061007133885924 	 ± 0.21963742596266073
	data : 0.11491031646728515
	model : 0.06503763198852539
			 train-loss:  2.2058819985971216 	 ± 0.2191233384776511
	data : 0.11492834091186524
	model : 0.06501908302307129
			 train-loss:  2.205434960647694 	 ± 0.2186845274813343
	data : 0.11492013931274414
	model : 0.06496710777282715
			 train-loss:  2.2067822701689126 	 ± 0.21901103624065102
	data : 0.1148460865020752
	model : 0.06497836112976074
			 train-loss:  2.2055437243901768 	 ± 0.21920941157441226
	data : 0.1148726463317871
	model : 0.0649714469909668
			 train-loss:  2.2059692423879813 	 ± 0.21877045097399575
	data : 0.11492562294006348
	model : 0.06500129699707032
			 train-loss:  2.2042668370973497 	 ± 0.219632245894179
	data : 0.11491680145263672
	model : 0.0649949550628662
			 train-loss:  2.2043960009705965 	 ± 0.2191191668572023
	data : 0.11483378410339355
	model : 0.06503915786743164
			 train-loss:  2.2029960852748944 	 ± 0.21954553531427362
	data : 0.11475701332092285
	model : 0.06503520011901856
			 train-loss:  2.2049133139596857 	 ± 0.2208012957752392
	data : 0.11473803520202637
	model : 0.06498823165893555
			 train-loss:  2.2058463798505126 	 ± 0.2207053098910684
	data : 0.1146960735321045
	model : 0.06497039794921874
			 train-loss:  2.2046281115953312 	 ± 0.22091148858823761
	data : 0.11484780311584472
	model : 0.06498026847839355
			 train-loss:  2.2056969139311047 	 ± 0.22095599991156598
	data : 0.11497530937194825
	model : 0.06495318412780762
			 train-loss:  2.205258940771428 	 ± 0.2205402525688141
	data : 0.11499943733215331
	model : 0.06496982574462891
			 train-loss:  2.2050104152171985 	 ± 0.22006429944901845
	data : 0.11503286361694336
	model : 0.06501517295837403
			 train-loss:  2.206442509611992 	 ± 0.22057710010404893
	data : 0.11504883766174316
	model : 0.06504321098327637
			 train-loss:  2.20508721687577 	 ± 0.22098724987214152
	data : 0.11492562294006348
	model : 0.0650291919708252
			 train-loss:  2.2043645171558155 	 ± 0.22074712904208127
	data : 0.1147282600402832
	model : 0.06524381637573243
			 train-loss:  2.2045988655305124 	 ± 0.22027694111300183
	data : 0.1147768497467041
	model : 0.06511960029602051
			 train-loss:  2.2041706926619526 	 ± 0.2198750630647396
	data : 0.11486787796020508
	model : 0.06502947807312012
			 train-loss:  2.2028289836432253 	 ± 0.22029675009083347
	data : 0.11495981216430665
	model : 0.06486358642578124
			 train-loss:  2.202394315401713 	 ± 0.2199029059798049
	data : 0.11513047218322754
	model : 0.06468253135681153
			 train-loss:  2.2019417850317153 	 ± 0.21952082839167683
	data : 0.1153444766998291
	model : 0.06430063247680665
			 train-loss:  2.2033788911046437 	 ± 0.22009965450108654
	data : 0.11544404029846192
	model : 0.06420297622680664
			 train-loss:  2.2021241407645378 	 ± 0.2204286126865417
	data : 0.11553339958190918
	model : 0.06411633491516114
			 train-loss:  2.2033316787153354 	 ± 0.22070127540049395
	data : 0.11560063362121582
	model : 0.06404175758361816
			 train-loss:  2.2031181843384453 	 ± 0.22024466453849026
	data : 0.11549344062805175
	model : 0.06401944160461426
			 train-loss:  2.201753707159133 	 ± 0.22073952001188446
	data : 0.11550025939941407
	model : 0.06397995948791504
			 train-loss:  2.200133591376502 	 ± 0.22163536082862273
	data : 0.11570510864257813
	model : 0.06399617195129395
			 train-loss:  2.2000115301476018 	 ± 0.2211670515458538
	data : 0.1157804012298584
	model : 0.0639460563659668
			 train-loss:  2.1993017120239062 	 ± 0.22095977448013293
	data : 0.11584668159484864
	model : 0.06395473480224609
			 train-loss:  2.198598606535729 	 ± 0.22075131519816707
	data : 0.11586918830871581
	model : 0.06395416259765625
			 train-loss:  2.198166998261112 	 ± 0.22038246846586068
	data : 0.11593461036682129
	model : 0.0639556884765625
			 train-loss:  2.1973456275111007 	 ± 0.22027873181739366
	data : 0.11580758094787598
	model : 0.06390409469604492
			 train-loss:  2.1969834691336176 	 ± 0.21988616902208225
	data : 0.11561470031738282
	model : 0.06390500068664551
			 train-loss:  2.19697154964862 	 ± 0.21942575097214329
	data : 0.11552591323852539
	model : 0.06389551162719727
			 train-loss:  2.195615451534589 	 ± 0.21996946706207793
	data : 0.11571907997131348
	model : 0.06389570236206055
			 train-loss:  2.196429680986523 	 ± 0.21987474845708355
	data : 0.11579575538635253
	model : 0.06386170387268067
			 train-loss:  2.1954938079699997 	 ± 0.21990046504124136
	data : 0.11579279899597168
	model : 0.06388649940490723
			 train-loss:  2.1948720157882313 	 ± 0.21966060441724355
	data : 0.11589951515197754
	model : 0.06387615203857422
			 train-loss:  2.1943690527657993 	 ± 0.21935018618139993
	data : 0.11593966484069824
	model : 0.06388015747070312
			 train-loss:  2.193519430744405 	 ± 0.21930401659753307
	data : 0.11576600074768066
	model : 0.0638807773590088
			 train-loss:  2.1935754791507875 	 ± 0.2188595811392819
	data : 0.11568279266357422
	model : 0.06387925148010254
			 train-loss:  2.195406927270928 	 ± 0.2202969015550514
	data : 0.11565451622009278
	model : 0.06393017768859863
			 train-loss:  2.1948438805918538 	 ± 0.22003031780684187
	data : 0.11563458442687988
	model : 0.06395397186279297
			 train-loss:  2.194608748677265 	 ± 0.2196192633789069
	data : 0.11576004028320312
	model : 0.06395525932312011
			 train-loss:  2.19419024848938 	 ± 0.2192790478434113
	data : 0.11576275825500489
	model : 0.06398777961730957
			 train-loss:  2.193253804488011 	 ± 0.21934212028128047
	data : 0.11575798988342285
	model : 0.06405553817749024
			 train-loss:  2.192970362447557 	 ± 0.21895253911699591
	data : 0.11579985618591308
	model : 0.06399688720703126
			 train-loss:  2.192981328417661 	 ± 0.21851946749895573
	data : 0.11578311920166015
	model : 0.06397008895874023
			 train-loss:  2.192994936714022 	 ± 0.21808899426393827
	data : 0.11555190086364746
	model : 0.06390810012817383
			 train-loss:  2.191908609165865 	 ± 0.21834842777873129
	data : 0.11562919616699219
	model : 0.063865327835083
			 train-loss:  2.1916767451912165 	 ± 0.21795300050321764
	data : 0.11535110473632812
	model : 0.055423307418823245
#epoch  16    val-loss:  2.4611361905148157  train-loss:  2.1916767451912165  lr:  0.0003125
			 train-loss:  1.8201981782913208 	 ± 0.0
	data : 5.353327751159668
	model : 0.08841133117675781
			 train-loss:  1.951755702495575 	 ± 0.13155752420425415
	data : 2.7330307960510254
	model : 0.07855355739593506
			 train-loss:  2.071762204170227 	 ± 0.2008516260117034
	data : 1.8589617411295574
	model : 0.0739270846048991
			 train-loss:  2.065149337053299 	 ± 0.17431930985458072
	data : 1.4230077862739563
	model : 0.07161957025527954
			 train-loss:  2.0875306844711305 	 ± 0.1622142912307437
	data : 1.1614436626434326
	model : 0.07028384208679199
			 train-loss:  2.1259254018465676 	 ± 0.17116853832195825
	data : 0.11372289657592774
	model : 0.06556453704833984
			 train-loss:  2.1352337258202687 	 ± 0.16010313491444375
	data : 0.11397457122802734
	model : 0.0647646427154541
			 train-loss:  2.137539818882942 	 ± 0.1498870025711018
	data : 0.11472883224487304
	model : 0.06477494239807129
			 train-loss:  2.1631555689705744 	 ± 0.15880557900280368
	data : 0.11443428993225098
	model : 0.06474270820617675
			 train-loss:  2.1742684483528136 	 ± 0.1543008604112497
	data : 0.11419343948364258
	model : 0.06475338935852051
			 train-loss:  2.150960933078419 	 ± 0.16455007106039599
	data : 0.11419758796691895
	model : 0.06479086875915527
			 train-loss:  2.1445241073767343 	 ± 0.15898456540935707
	data : 0.11436877250671387
	model : 0.06481490135192872
			 train-loss:  2.1463663486333995 	 ± 0.15288068386944917
	data : 0.11448254585266113
	model : 0.06492176055908203
			 train-loss:  2.2055346369743347 	 ± 0.2592577124256811
	data : 0.1147191047668457
	model : 0.06501584053039551
			 train-loss:  2.1854225397109985 	 ± 0.26152732376815974
	data : 0.11490697860717773
	model : 0.06500096321105957
			 train-loss:  2.1934325471520424 	 ± 0.25511597470187164
	data : 0.11495485305786132
	model : 0.0649756908416748
			 train-loss:  2.1872389526928173 	 ± 0.2487357037360161
	data : 0.11492977142333985
	model : 0.06503772735595703
			 train-loss:  2.1929482552740307 	 ± 0.2428711447257131
	data : 0.11469297409057617
	model : 0.06494293212890626
			 train-loss:  2.188756911378158 	 ± 0.23706129690288708
	data : 0.11450834274291992
	model : 0.06492986679077148
			 train-loss:  2.200254648923874 	 ± 0.23643163275997583
	data : 0.11451020240783691
	model : 0.06490345001220703
			 train-loss:  2.2097090823309764 	 ± 0.23457566451458903
	data : 0.11451230049133301
	model : 0.06494321823120117
			 train-loss:  2.222556704824621 	 ± 0.2366238824647476
	data : 0.114422607421875
	model : 0.06492071151733399
			 train-loss:  2.246052270350249 	 ± 0.25632282700769177
	data : 0.11459722518920898
	model : 0.06495814323425293
			 train-loss:  2.2416940281788507 	 ± 0.2517949601977023
	data : 0.11475701332092285
	model : 0.0649505615234375
			 train-loss:  2.2297006320953368 	 ± 0.2536077110273051
	data : 0.11470050811767578
	model : 0.06495485305786133
			 train-loss:  2.2175483978711643 	 ± 0.255998174206844
	data : 0.11442885398864747
	model : 0.06493902206420898
			 train-loss:  2.218298391059593 	 ± 0.2512418462252115
	data : 0.11451869010925293
	model : 0.06491508483886718
			 train-loss:  2.2326178891318187 	 ± 0.25769048917300436
	data : 0.11446604728698731
	model : 0.06489481925964355
			 train-loss:  2.2250551355296166 	 ± 0.2563514160240739
	data : 0.11444258689880371
	model : 0.06493573188781739
			 train-loss:  2.245675873756409 	 ± 0.2754210305208155
	data : 0.11444768905639649
	model : 0.0649604320526123
			 train-loss:  2.240290918657857 	 ± 0.27254299578580393
	data : 0.11463818550109864
	model : 0.06500582695007324
			 train-loss:  2.2274092733860016 	 ± 0.2776733417918632
	data : 0.11467056274414063
	model : 0.06501493453979493
			 train-loss:  2.216883901393775 	 ± 0.2798412342537843
	data : 0.11462650299072266
	model : 0.06501479148864746
			 train-loss:  2.2124648269485023 	 ± 0.2768614806813091
	data : 0.11459388732910156
	model : 0.06497435569763184
			 train-loss:  2.207084018843515 	 ± 0.27467548055156765
	data : 0.11473727226257324
	model : 0.06557941436767578
			 train-loss:  2.208184841606352 	 ± 0.27091196643722043
	data : 0.11429004669189453
	model : 0.06551284790039062
			 train-loss:  2.2195469012131563 	 ± 0.27578462860821623
	data : 0.11428041458129883
	model : 0.06550602912902832
			 train-loss:  2.2119055139391044 	 ± 0.27607266563404154
	data : 0.11438789367675781
	model : 0.06561732292175293
			 train-loss:  2.2107868041747656 	 ± 0.2725975325380934
	data : 0.11445350646972656
	model : 0.0656508445739746
			 train-loss:  2.218749925494194 	 ± 0.27372380262224183
	data : 0.1143864631652832
	model : 0.06505837440490722
			 train-loss:  2.21341656184778 	 ± 0.2724611506899261
	data : 0.1147801399230957
	model : 0.06509485244750976
			 train-loss:  2.2037708645775202 	 ± 0.27619231573673286
	data : 0.11459560394287109
	model : 0.06509475708007813
			 train-loss:  2.210416926894077 	 ± 0.2763391698476034
	data : 0.11454267501831054
	model : 0.06496696472167969
			 train-loss:  2.205150319771333 	 ± 0.27535523217399427
	data : 0.11422543525695801
	model : 0.06496224403381348
			 train-loss:  2.2079075786802504 	 ± 0.27289212662450174
	data : 0.11416378021240234
	model : 0.06497726440429688
			 train-loss:  2.2151745531869973 	 ± 0.2742765002880136
	data : 0.11438827514648438
	model : 0.06494603157043458
			 train-loss:  2.2240990349586975 	 ± 0.27801213040237
	data : 0.1145127296447754
	model : 0.06496248245239258
			 train-loss:  2.2329245333870253 	 ± 0.28167591956336446
	data : 0.11452283859252929
	model : 0.06497292518615723
			 train-loss:  2.234422486655566 	 ± 0.27897996065423275
	data : 0.11460366249084472
	model : 0.06494727134704589
			 train-loss:  2.232558786869049 	 ± 0.2764840276499231
	data : 0.1146878719329834
	model : 0.06491856575012207
			 train-loss:  2.2378755807876587 	 ± 0.2763294082866739
	data : 0.11455693244934081
	model : 0.0649369239807129
			 train-loss:  2.241332533267828 	 ± 0.27477080716064445
	data : 0.11467556953430176
	model : 0.06491484642028808
			 train-loss:  2.2401864551148325 	 ± 0.2722917348126839
	data : 0.11470971107482911
	model : 0.06495156288146972
			 train-loss:  2.2427074357315346 	 ± 0.2703823356514186
	data : 0.11493496894836426
	model : 0.06496424674987793
			 train-loss:  2.2412468975240536 	 ± 0.26812793153323305
	data : 0.11483941078186036
	model : 0.06498422622680664
			 train-loss:  2.2483456411531995 	 ± 0.2708880955691972
	data : 0.11479654312133789
	model : 0.06493887901306153
			 train-loss:  2.257748743944001 	 ± 0.2775687531692213
	data : 0.11467313766479492
	model : 0.06493983268737794
			 train-loss:  2.257299310174482 	 ± 0.27518643546302374
	data : 0.11450819969177246
	model : 0.06490888595581054
			 train-loss:  2.248672608601845 	 ± 0.28064286004876976
	data : 0.11441011428833008
	model : 0.06488580703735351
			 train-loss:  2.250414329767227 	 ± 0.27861572690572584
	data : 0.11453890800476074
	model : 0.06481695175170898
			 train-loss:  2.2524041601868925 	 ± 0.27675209041859505
	data : 0.11467909812927246
	model : 0.06486406326293945
			 train-loss:  2.248511016368866 	 ± 0.27619000693685625
	data : 0.11470046043395996
	model : 0.06493825912475586
			 train-loss:  2.2516453058000594 	 ± 0.2750985013293998
	data : 0.11481213569641113
	model : 0.06495332717895508
			 train-loss:  2.2487543430179358 	 ± 0.2739036894734367
	data : 0.11478261947631836
	model : 0.06496362686157227
			 train-loss:  2.244824878986065 	 ± 0.27360049626534483
	data : 0.1147376537322998
	model : 0.06501507759094238
			 train-loss:  2.240454684604298 	 ± 0.2737963476915825
	data : 0.11455559730529785
	model : 0.06502580642700195
			 train-loss:  2.2423549445707405 	 ± 0.272183564849809
	data : 0.11455936431884765
	model : 0.06489009857177734
			 train-loss:  2.239568492945503 	 ± 0.271135820039012
	data : 0.11466279029846191
	model : 0.06490278244018555
			 train-loss:  2.232235842856808 	 ± 0.27587209163626797
	data : 0.11486825942993165
	model : 0.06493420600891113
			 train-loss:  2.236451928956168 	 ± 0.27612441425060164
	data : 0.11499614715576172
	model : 0.06492466926574707
			 train-loss:  2.2363042965741227 	 ± 0.27417576281319944
	data : 0.11503591537475585
	model : 0.06490559577941894
			 train-loss:  2.2405972944365606 	 ± 0.274657616840574
	data : 0.11511440277099609
	model : 0.06496467590332031
			 train-loss:  2.2386297134503925 	 ± 0.27328037721946846
	data : 0.11496973037719727
	model : 0.06497488021850586
			 train-loss:  2.2337921020146965 	 ± 0.2745566016223965
	data : 0.11492996215820313
	model : 0.06490883827209473
			 train-loss:  2.2307122389475507 	 ± 0.2740039676898167
	data : 0.11475214958190919
	model : 0.06493616104125977
			 train-loss:  2.2272577975925647 	 ± 0.2738344225545571
	data : 0.11490888595581054
	model : 0.06492786407470703
			 train-loss:  2.2260097435542514 	 ± 0.27226794997414583
	data : 0.11492538452148438
	model : 0.06496157646179199
			 train-loss:  2.2223422053532724 	 ± 0.27242461061716133
	data : 0.11503095626831054
	model : 0.06495022773742676
			 train-loss:  2.224639746207225 	 ± 0.2714543683342194
	data : 0.11495757102966309
	model : 0.06498546600341797
			 train-loss:  2.2262108817696573 	 ± 0.2701136604304213
	data : 0.11496086120605468
	model : 0.06494903564453125
			 train-loss:  2.2223002160037004 	 ± 0.2707103547965428
	data : 0.11487874984741211
	model : 0.06495938301086426
			 train-loss:  2.2198088677918038 	 ± 0.26998729561378454
	data : 0.11482796669006348
	model : 0.06496963500976563
			 train-loss:  2.223763047930706 	 ± 0.2707342304174781
	data : 0.11469869613647461
	model : 0.06492857933044434
			 train-loss:  2.226182084707987 	 ± 0.2700187664544638
	data : 0.11475777626037598
	model : 0.06491518020629883
			 train-loss:  2.228279117976918 	 ± 0.2691129168751409
	data : 0.1148228645324707
	model : 0.06494250297546386
			 train-loss:  2.2259350003198137 	 ± 0.2684151895147897
	data : 0.11491222381591797
	model : 0.06495699882507325
			 train-loss:  2.2302013388995467 	 ± 0.2697849741417857
	data : 0.11488041877746583
	model : 0.06494770050048829
			 train-loss:  2.228905427184972 	 ± 0.26851992257034346
	data : 0.11496500968933106
	model : 0.06501946449279786
			 train-loss:  2.226404796825366 	 ± 0.268035595990624
	data : 0.11490139961242676
	model : 0.06507768630981445
			 train-loss:  2.2286911023987663 	 ± 0.2674136205547462
	data : 0.11492853164672852
	model : 0.06506218910217285
			 train-loss:  2.22964077598446 	 ± 0.26609282024617803
	data : 0.11475787162780762
	model : 0.06503176689147949
			 train-loss:  2.2301600510659427 	 ± 0.2646890685970336
	data : 0.11471543312072754
	model : 0.0650115966796875
			 train-loss:  2.234432221740805 	 ± 0.2664321660991922
	data : 0.11471972465515137
	model : 0.06495699882507325
			 train-loss:  2.235707030651417 	 ± 0.2652961848132954
	data : 0.11481175422668458
	model : 0.06492338180541993
			 train-loss:  2.2341420888900756 	 ± 0.2643320106279312
	data : 0.11477084159851074
	model : 0.06496310234069824
			 train-loss:  2.2330219633877277 	 ± 0.2631782273371164
	data : 0.11487269401550293
	model : 0.06498899459838867
			 train-loss:  2.2331763702569547 	 ± 0.2618224949367749
	data : 0.11486740112304687
	model : 0.06501626968383789
			 train-loss:  2.23062877752343 	 ± 0.26168888178503785
	data : 0.11485018730163574
	model : 0.06504707336425782
			 train-loss:  2.229964651242651 	 ± 0.2604468604260169
	data : 0.11488633155822754
	model : 0.0650857925415039
			 train-loss:  2.229399170875549 	 ± 0.25920242759515894
	data : 0.11487007141113281
	model : 0.06505742073059081
			 train-loss:  2.2283299370567398 	 ± 0.2581375944173873
	data : 0.11469526290893554
	model : 0.06504344940185547
			 train-loss:  2.2302155728433646 	 ± 0.25756717768208565
	data : 0.11479043960571289
	model : 0.06500487327575684
			 train-loss:  2.2280592663774215 	 ± 0.2572373032037688
	data : 0.11493630409240722
	model : 0.06501412391662598
			 train-loss:  2.2238649301804028 	 ± 0.25951260708476215
	data : 0.11497254371643066
	model : 0.06506142616271973
			 train-loss:  2.221335858390445 	 ± 0.2595584720895525
	data : 0.11493391990661621
	model : 0.0651024341583252
			 train-loss:  2.2201300657020426 	 ± 0.25862654931781076
	data : 0.11502552032470703
	model : 0.06512012481689453
			 train-loss:  2.222874957824422 	 ± 0.2589618181663738
	data : 0.11493797302246093
	model : 0.06513142585754395
			 train-loss:  2.2209347398192794 	 ± 0.25854029033721704
	data : 0.11481156349182128
	model : 0.06512436866760254
			 train-loss:  2.222260707015291 	 ± 0.2577202492204999
	data : 0.11466445922851562
	model : 0.06501803398132325
			 train-loss:  2.2254288933493873 	 ± 0.2586696508408907
	data : 0.11449847221374512
	model : 0.06493887901306153
			 train-loss:  2.2228480620427176 	 ± 0.258920589186626
	data : 0.11447720527648926
	model : 0.06495780944824218
			 train-loss:  2.2245239817670415 	 ± 0.25836615043739974
	data : 0.11467552185058594
	model : 0.06494855880737305
			 train-loss:  2.2230452446810967 	 ± 0.25769602029695937
	data : 0.11467766761779785
	model : 0.06495327949523926
			 train-loss:  2.2225046335605154 	 ± 0.25662763814713646
	data : 0.11474628448486328
	model : 0.06498489379882813
			 train-loss:  2.2257045030593874 	 ± 0.2577835010406325
	data : 0.11484646797180176
	model : 0.06503024101257324
			 train-loss:  2.2242814107187865 	 ± 0.2571232496755644
	data : 0.11492347717285156
	model : 0.0649991512298584
			 train-loss:  2.2240561715558043 	 ± 0.25603356725529397
	data : 0.11489381790161132
	model : 0.06503772735595703
			 train-loss:  2.2256515379679405 	 ± 0.2555297238490501
	data : 0.11490521430969239
	model : 0.06500353813171386
			 train-loss:  2.2264588690605485 	 ± 0.254604888107074
	data : 0.11488628387451172
	model : 0.06497030258178711
			 train-loss:  2.225941823919614 	 ± 0.2536045443565028
	data : 0.1149078369140625
	model : 0.06495189666748047
			 train-loss:  2.2271510007952857 	 ± 0.25290153587758496
	data : 0.11496996879577637
	model : 0.06495013236999511
			 train-loss:  2.2254719314028004 	 ± 0.25253923136084816
	data : 0.11485018730163574
	model : 0.06499161720275878
			 train-loss:  2.2241325368726157 	 ± 0.25194527992472154
	data : 0.11479411125183106
	model : 0.06505465507507324
			 train-loss:  2.222446686798526 	 ± 0.25162292136978115
	data : 0.11482672691345215
	model : 0.06509199142456054
			 train-loss:  2.2197685403823852 	 ± 0.2523825823274135
	data : 0.11487722396850586
	model : 0.06511511802673339
			 train-loss:  2.2183936049067783 	 ± 0.2518486496222357
	data : 0.11479530334472657
	model : 0.06511564254760742
			 train-loss:  2.2185035042875394 	 ± 0.2508581931825059
	data : 0.11470141410827636
	model : 0.06503334045410156
			 train-loss:  2.2177478251978755 	 ± 0.2500214335379938
	data : 0.11478986740112304
	model : 0.06499037742614747
			 train-loss:  2.2159908224445903 	 ± 0.249842513466942
	data : 0.11476287841796876
	model : 0.06499528884887695
			 train-loss:  2.218383424098675 	 ± 0.25035891004006944
	data : 0.11488947868347169
	model : 0.06496820449829102
			 train-loss:  2.2177220537462308 	 ± 0.2495154847447986
	data : 0.11491031646728515
	model : 0.06495871543884277
			 train-loss:  2.217222175814889 	 ± 0.24863438981065175
	data : 0.11500034332275391
	model : 0.06499314308166504
			 train-loss:  2.2164048055060825 	 ± 0.24787586262665
	data : 0.11496534347534179
	model : 0.06499643325805664
			 train-loss:  2.213749165855237 	 ± 0.24884109326410178
	data : 0.11507368087768555
	model : 0.06499791145324707
			 train-loss:  2.212954537956803 	 ± 0.248088333302971
	data : 0.11508064270019532
	model : 0.06501593589782714
			 train-loss:  2.2143137831898296 	 ± 0.24767858716343716
	data : 0.11499032974243165
	model : 0.06503715515136718
			 train-loss:  2.2139989311677697 	 ± 0.2468003100851138
	data : 0.11494317054748535
	model : 0.06499643325805664
			 train-loss:  2.2140903585198997 	 ± 0.2459068087220955
	data : 0.11502485275268555
	model : 0.06498713493347168
			 train-loss:  2.215221314979114 	 ± 0.24538058621368705
	data : 0.11495494842529297
	model : 0.06495199203491211
			 train-loss:  2.21456635424069 	 ± 0.24462456199303736
	data : 0.11487016677856446
	model : 0.06501617431640624
			 train-loss:  2.2137959908086358 	 ± 0.24392592153938727
	data : 0.11501965522766114
	model : 0.06501836776733398
			 train-loss:  2.2131683650151106 	 ± 0.24317973605408247
	data : 0.11510863304138183
	model : 0.06507654190063476
			 train-loss:  2.2114193039340573 	 ± 0.24322263743894224
	data : 0.11503691673278808
	model : 0.06511406898498535
			 train-loss:  2.210964293943511 	 ± 0.24243770950960683
	data : 0.11505918502807617
	model : 0.06510653495788574
			 train-loss:  2.2098791401961755 	 ± 0.24195094423870214
	data : 0.11509017944335938
	model : 0.06506128311157226
			 train-loss:  2.213384318025145 	 ± 0.2447872711578273
	data : 0.11493778228759766
	model : 0.06502227783203125
			 train-loss:  2.2135487608358164 	 ± 0.24396133223484592
	data : 0.11485838890075684
	model : 0.06501431465148926
			 train-loss:  2.2125827976175256 	 ± 0.24341764999808807
	data : 0.11488146781921386
	model : 0.0649679183959961
			 train-loss:  2.2134857545763054 	 ± 0.24284800988803892
	data : 0.1149266242980957
	model : 0.06500706672668458
			 train-loss:  2.215403021176656 	 ± 0.24316599057401797
	data : 0.114801025390625
	model : 0.0650099277496338
			 train-loss:  2.2153367301486186 	 ± 0.24236082756190938
	data : 0.11485147476196289
	model : 0.06506099700927734
			 train-loss:  2.213525307021643 	 ± 0.24258565656447761
	data : 0.11484193801879883
	model : 0.06509571075439453
			 train-loss:  2.2137607569788016 	 ± 0.241809017610393
	data : 0.11484041213989257
	model : 0.0651388168334961
			 train-loss:  2.2118672217641557 	 ± 0.24215799190844797
	data : 0.1146672248840332
	model : 0.06510028839111329
			 train-loss:  2.2130962517953687 	 ± 0.24185695475934627
	data : 0.1147883415222168
	model : 0.06509261131286621
			 train-loss:  2.210224164602084 	 ± 0.24371786380161042
	data : 0.11479272842407226
	model : 0.06509227752685547
			 train-loss:  2.211203009459623 	 ± 0.24324788346068058
	data : 0.11496810913085938
	model : 0.065028715133667
			 train-loss:  2.2132260806952853 	 ± 0.24379830679966152
	data : 0.11499743461608887
	model : 0.06500239372253418
			 train-loss:  2.2133768497023194 	 ± 0.24303782518066686
	data : 0.11500010490417481
	model : 0.06502861976623535
			 train-loss:  2.211165849119425 	 ± 0.2438759696330328
	data : 0.11492643356323243
	model : 0.0650296688079834
			 train-loss:  2.2109434686092118 	 ± 0.2431336835706101
	data : 0.11502485275268555
	model : 0.06503028869628906
			 train-loss:  2.210786861402017 	 ± 0.2423902547178365
	data : 0.11494007110595703
	model : 0.06503834724426269
			 train-loss:  2.2104800676275618 	 ± 0.24167713036853236
	data : 0.1147801399230957
	model : 0.0650393009185791
			 train-loss:  2.2108603970306677 	 ± 0.24098810765233533
	data : 0.11483445167541503
	model : 0.0650402545928955
			 train-loss:  2.21030008503885 	 ± 0.24036385841278463
	data : 0.11494245529174804
	model : 0.06503806114196778
			 train-loss:  2.209407956485289 	 ± 0.23991262152035156
	data : 0.11491074562072753
	model : 0.0650477409362793
			 train-loss:  2.208797512654059 	 ± 0.23932251336749427
	data : 0.11488184928894044
	model : 0.06506690979003907
			 train-loss:  2.2070167639425824 	 ± 0.23971630988161835
	data : 0.11501617431640625
	model : 0.06506834030151368
			 train-loss:  2.2044201934126 	 ± 0.24136398257123265
	data : 0.11501150131225586
	model : 0.06506500244140626
			 train-loss:  2.202979607441846 	 ± 0.24138063308025906
	data : 0.11484217643737793
	model : 0.06507835388183594
			 train-loss:  2.202675181522704 	 ± 0.24070653519683408
	data : 0.11472859382629394
	model : 0.06508288383483887
			 train-loss:  2.201146429361299 	 ± 0.2408369136145161
	data : 0.1148139476776123
	model : 0.06507282257080078
			 train-loss:  2.198742114739611 	 ± 0.24220121844232267
	data : 0.11490015983581543
	model : 0.06511297225952148
			 train-loss:  2.1984437436893067 	 ± 0.24153611931264252
	data : 0.11497869491577148
	model : 0.06512360572814942
			 train-loss:  2.198001449448722 	 ± 0.2409156820255328
	data : 0.11509904861450196
	model : 0.0651235580444336
			 train-loss:  2.201041533865712 	 ± 0.24357331721847128
	data : 0.11517601013183594
	model : 0.0650862216949463
			 train-loss:  2.2001079105387973 	 ± 0.24319988791973535
	data : 0.11510715484619141
	model : 0.06508955955505372
			 train-loss:  2.1999824629740767 	 ± 0.24252152277363756
	data : 0.1151761531829834
	model : 0.0650524616241455
			 train-loss:  2.199580241181997 	 ± 0.2419026693021648
	data : 0.11495866775512695
	model : 0.06504411697387695
			 train-loss:  2.1985757052898407 	 ± 0.24160388045508385
	data : 0.11483974456787109
	model : 0.06504497528076172
			 train-loss:  2.1977757939976224 	 ± 0.24117443866082547
	data : 0.11484556198120117
	model : 0.06503195762634277
			 train-loss:  2.1975728476440515 	 ± 0.2405264564136156
	data : 0.1149660587310791
	model : 0.06503114700317383
			 train-loss:  2.1961427275600327 	 ± 0.24064304282794216
	data : 0.11482157707214355
	model : 0.06505508422851562
			 train-loss:  2.196275264672611 	 ± 0.23999492791863805
	data : 0.11497297286987304
	model : 0.06507420539855957
			 train-loss:  2.19893023838868 	 ± 0.2420397119376322
	data : 0.11493382453918458
	model : 0.06511368751525878
			 train-loss:  2.2009517255649773 	 ± 0.24294905745774634
	data : 0.11501955986022949
	model : 0.06509456634521485
			 train-loss:  2.201896382525643 	 ± 0.24264086409386953
	data : 0.1149284839630127
	model : 0.06507492065429688
			 train-loss:  2.2020929753780365 	 ± 0.24200961455556297
	data : 0.11478228569030761
	model : 0.06497797966003419
			 train-loss:  2.2018221894269265 	 ± 0.24139708280645591
	data : 0.11480727195739746
	model : 0.0649634838104248
			 train-loss:  2.2010065223041333 	 ± 0.2410219867090279
	data : 0.11497077941894532
	model : 0.06489076614379882
			 train-loss:  2.2010256975733173 	 ± 0.2403903563912171
	data : 0.1149754524230957
	model : 0.064933443069458
			 train-loss:  2.198628192767501 	 ± 0.24204218714186929
	data : 0.1149167537689209
	model : 0.06495623588562012
			 train-loss:  2.198048574936822 	 ± 0.24154787899761687
	data : 0.11505627632141113
	model : 0.06508512496948242
			 train-loss:  2.1964920968124546 	 ± 0.24189294106931086
	data : 0.11510705947875977
	model : 0.06511473655700684
			 train-loss:  2.1958956425006573 	 ± 0.24141489060069912
	data : 0.1151505470275879
	model : 0.06512188911437988
			 train-loss:  2.1949624686825033 	 ± 0.2411505861671715
	data : 0.11492910385131835
	model : 0.06508340835571289
			 train-loss:  2.1923534367895368 	 ± 0.2432952743833565
	data : 0.11509242057800292
	model : 0.06509137153625488
			 train-loss:  2.1915053171340864 	 ± 0.24297189452476614
	data : 0.1151151180267334
	model : 0.06501421928405762
			 train-loss:  2.194312513174124 	 ± 0.2455585281424716
	data : 0.1151270866394043
	model : 0.06496248245239258
			 train-loss:  2.1934606593847277 	 ± 0.24523845765010613
	data : 0.11502885818481445
	model : 0.06500310897827148
			 train-loss:  2.193886494162071 	 ± 0.24470176689954795
	data : 0.1150472640991211
	model : 0.06505274772644043
			 train-loss:  2.193473802344634 	 ± 0.24416543055191753
	data : 0.1148993968963623
	model : 0.0650726318359375
			 train-loss:  2.1930886248649633 	 ± 0.24362480975856063
	data : 0.11492557525634765
	model : 0.0650902271270752
			 train-loss:  2.192568642835991 	 ± 0.24313985486245407
	data : 0.11479783058166504
	model : 0.06524319648742676
			 train-loss:  2.1927708131511037 	 ± 0.2425632938532447
	data : 0.11455316543579101
	model : 0.0651845932006836
			 train-loss:  2.191917264924466 	 ± 0.2422822463436235
	data : 0.11469783782958984
	model : 0.06527585983276367
			 train-loss:  2.191474561530035 	 ± 0.2417798210616859
	data : 0.1147963523864746
	model : 0.065238618850708
			 train-loss:  2.1935285759659915 	 ± 0.24300157237030412
	data : 0.1148439884185791
	model : 0.06522026062011718
			 train-loss:  2.1939878355373037 	 ± 0.24251000112000276
	data : 0.11485295295715332
	model : 0.06510872840881347
			 train-loss:  2.1934243684723262 	 ± 0.2420690072394245
	data : 0.1152151107788086
	model : 0.06515030860900879
			 train-loss:  2.191099390034427 	 ± 0.24383365568387178
	data : 0.1152761459350586
	model : 0.06509184837341309
			 train-loss:  2.190298657934621 	 ± 0.24353581153647486
	data : 0.11528010368347168
	model : 0.06507539749145508
			 train-loss:  2.1915734569791336 	 ± 0.2436714306538261
	data : 0.11503148078918457
	model : 0.06508364677429199
			 train-loss:  2.193501920900612 	 ± 0.24472525520552954
	data : 0.115108060836792
	model : 0.0651677131652832
			 train-loss:  2.1946603203928747 	 ± 0.24474283366110924
	data : 0.11483721733093262
	model : 0.06513490676879882
			 train-loss:  2.193091286001382 	 ± 0.24525710185137822
	data : 0.11477417945861816
	model : 0.06512598991394043
			 train-loss:  2.1918183977153443 	 ± 0.24540543001183857
	data : 0.1147146224975586
	model : 0.06518640518188476
			 train-loss:  2.1923690771837845 	 ± 0.24497627189230123
	data : 0.11478199958801269
	model : 0.06522016525268555
			 train-loss:  2.191714849646233 	 ± 0.24460712806504595
	data : 0.11471567153930665
	model : 0.06512389183044434
			 train-loss:  2.190643759207292 	 ± 0.24456476632585294
	data : 0.11494011878967285
	model : 0.06511788368225098
			 train-loss:  2.1906838578875787 	 ± 0.244011549875595
	data : 0.11484255790710449
	model : 0.06500082015991211
			 train-loss:  2.190416377943915 	 ± 0.2434938242809684
	data : 0.11473875045776367
	model : 0.06485366821289062
			 train-loss:  2.1928776005458404 	 ± 0.2456993331840742
	data : 0.11484246253967285
	model : 0.06474766731262208
			 train-loss:  2.1928605884313583 	 ± 0.24515041533986615
	data : 0.11507830619812012
	model : 0.06461200714111329
			 train-loss:  2.1920266660054524 	 ± 0.24492324563621146
	data : 0.11514430046081543
	model : 0.06449589729309083
			 train-loss:  2.19260458925129 	 ± 0.24453448475352585
	data : 0.11534209251403808
	model : 0.06434798240661621
			 train-loss:  2.1923143548587345 	 ± 0.244034276607906
	data : 0.11545701026916504
	model : 0.06426863670349121
			 train-loss:  2.1914689504263696 	 ± 0.2438314394165019
	data : 0.11561164855957032
	model : 0.06411619186401367
			 train-loss:  2.191849436301852 	 ± 0.24336629766828347
	data : 0.11543426513671876
	model : 0.06402001380920411
			 train-loss:  2.1907505818035293 	 ± 0.2434053382089997
	data : 0.11536569595336914
	model : 0.06389431953430176
			 train-loss:  2.191948777669436 	 ± 0.24355674217293113
	data : 0.11537246704101563
	model : 0.0638948917388916
			 train-loss:  2.1919998891394714 	 ± 0.243032510071369
	data : 0.11550898551940918
	model : 0.0638585090637207
			 train-loss:  2.192030663654016 	 ± 0.24251087330717938
	data : 0.11542220115661621
	model : 0.06390275955200195
			 train-loss:  2.19202908134868 	 ± 0.24199213408254847
	data : 0.1155003547668457
	model : 0.06393232345581054
			 train-loss:  2.19280014900451 	 ± 0.24176460504911843
	data : 0.11560702323913574
	model : 0.06395921707153321
			 train-loss:  2.192819469561011 	 ± 0.24125202995673584
	data : 0.11544289588928222
	model : 0.06393494606018066
			 train-loss:  2.1923004429048625 	 ± 0.2408745261303101
	data : 0.11537661552429199
	model : 0.06397891044616699
			 train-loss:  2.1921987698859526 	 ± 0.24037305076199603
	data : 0.11557984352111816
	model : 0.06398596763610839
			 train-loss:  2.1911865482769253 	 ± 0.2403774169475349
	data : 0.11575727462768555
	model : 0.06402978897094727
			 train-loss:  2.191369451582432 	 ± 0.23989277305900575
	data : 0.11569666862487793
	model : 0.06405229568481445
			 train-loss:  2.1912494967092617 	 ± 0.23940176551995038
	data : 0.11583495140075684
	model : 0.06409249305725098
			 train-loss:  2.1918073119210804 	 ± 0.23906351235478876
	data : 0.11593003273010254
	model : 0.06403927803039551
			 train-loss:  2.1924450824290145 	 ± 0.2387773145440125
	data : 0.11570849418640136
	model : 0.06398572921752929
			 train-loss:  2.19347099349147 	 ± 0.23882356551546674
	data : 0.11546268463134765
	model : 0.06388750076293945
			 train-loss:  2.1947410004479546 	 ± 0.23915987229953525
	data : 0.1155087947845459
	model : 0.06384816169738769
			 train-loss:  2.196116195946205 	 ± 0.23964196091789455
	data : 0.11552562713623046
	model : 0.06383414268493652
			 train-loss:  2.1986615585412093 	 ± 0.2424656027553464
	data : 0.11540584564208985
	model : 0.06382884979248046
			 train-loss:  2.198953214672304 	 ± 0.24201967780026917
	data : 0.11545348167419434
	model : 0.06385412216186523
			 train-loss:  2.1985768663835334 	 ± 0.24160590977285812
	data : 0.11559438705444336
	model : 0.06387853622436523
			 train-loss:  2.1975070071220397 	 ± 0.24171248847677126
	data : 0.11546101570129394
	model : 0.06384792327880859
			 train-loss:  2.19603839006082 	 ± 0.24234555384812093
	data : 0.11539068222045898
	model : 0.06388273239135742
			 train-loss:  2.1953825832359373 	 ± 0.24208729255871947
	data : 0.11558723449707031
	model : 0.06389446258544922
			 train-loss:  2.1952935315874726 	 ± 0.24161252107947248
	data : 0.11572051048278809
	model : 0.0639007568359375
			 train-loss:  2.1963177867761745 	 ± 0.24168616662269443
	data : 0.11566672325134278
	model : 0.06391797065734864
			 train-loss:  2.1967631335351983 	 ± 0.24131620841207002
	data : 0.11574530601501465
	model : 0.06400361061096191
			 train-loss:  2.1961444127373397 	 ± 0.24104699898143045
	data : 0.11564788818359376
	model : 0.05553793907165527
#epoch  17    val-loss:  2.393888097060354  train-loss:  2.1961444127373397  lr:  0.0003125
			 train-loss:  1.9861003160476685 	 ± 0.0
	data : 5.841768264770508
	model : 0.07045722007751465
			 train-loss:  1.9570340514183044 	 ± 0.029066264629364014
	data : 2.985901117324829
	model : 0.0676584243774414
			 train-loss:  2.0002677838007608 	 ± 0.06558614989288421
	data : 2.028811772664388
	model : 0.06663282712300618
			 train-loss:  1.9980640709400177 	 ± 0.056927377811091216
	data : 1.5503188371658325
	model : 0.0661705732345581
			 train-loss:  2.016113781929016 	 ± 0.062415938210547285
	data : 1.2632337093353272
	model : 0.06591825485229492
			 train-loss:  2.0446872115135193 	 ± 0.08560760583434465
	data : 0.11778297424316406
	model : 0.06482219696044922
			 train-loss:  2.0773383719580516 	 ± 0.11259795782965631
	data : 0.11477675437927246
	model : 0.06478080749511719
			 train-loss:  2.0773935765028 	 ± 0.1053258464327501
	data : 0.1148366928100586
	model : 0.06483345031738282
			 train-loss:  2.1265841987397938 	 ± 0.17093465878669342
	data : 0.11466660499572753
	model : 0.06481904983520508
			 train-loss:  2.12950781583786 	 ± 0.162399875768105
	data : 0.11452879905700683
	model : 0.06480846405029297
			 train-loss:  2.1516381068663164 	 ± 0.16992235456911123
	data : 0.11486291885375977
	model : 0.06480898857116699
			 train-loss:  2.15376149614652 	 ± 0.1628406254680927
	data : 0.11481060981750488
	model : 0.06489124298095703
			 train-loss:  2.170279548718379 	 ± 0.1665876570166084
	data : 0.11475443840026855
	model : 0.06489219665527343
			 train-loss:  2.16290750673839 	 ± 0.16271358966237923
	data : 0.11489400863647461
	model : 0.06494803428649902
			 train-loss:  2.155074365933736 	 ± 0.15990521600273133
	data : 0.11490654945373535
	model : 0.06500024795532226
			 train-loss:  2.1459095403552055 	 ± 0.15884421186054748
	data : 0.1150045394897461
	model : 0.06497769355773926
			 train-loss:  2.1514950289445767 	 ± 0.155712689418738
	data : 0.11510014533996582
	model : 0.06500363349914551
			 train-loss:  2.1327062050501504 	 ± 0.1700022219467567
	data : 0.11503753662109376
	model : 0.06505537033081055
			 train-loss:  2.1275148893657483 	 ± 0.1669274055614452
	data : 0.11491107940673828
	model : 0.06502113342285157
			 train-loss:  2.127410924434662 	 ± 0.16270134036489212
	data : 0.11491122245788574
	model : 0.06496963500976563
			 train-loss:  2.1275282133193243 	 ± 0.15878111632614084
	data : 0.11444807052612305
	model : 0.0649838924407959
			 train-loss:  2.132617874578996 	 ± 0.15687404473310532
	data : 0.11447076797485352
	model : 0.06493902206420898
			 train-loss:  2.1480033812315567 	 ± 0.16954997216520035
	data : 0.11467118263244629
	model : 0.06492085456848144
			 train-loss:  2.1430180271466575 	 ± 0.16769326081696895
	data : 0.11481428146362305
	model : 0.06498675346374512
			 train-loss:  2.140140199661255 	 ± 0.16490892613606023
	data : 0.11490063667297364
	model : 0.0650439739227295
			 train-loss:  2.142583681986882 	 ± 0.16216738050579435
	data : 0.11491427421569825
	model : 0.06503467559814453
			 train-loss:  2.1607319955472595 	 ± 0.18408596662601223
	data : 0.11477723121643066
	model : 0.0650179386138916
			 train-loss:  2.1785162346703664 	 ± 0.20301947564837222
	data : 0.11461973190307617
	model : 0.06496262550354004
			 train-loss:  2.1772195964023986 	 ± 0.1996063886821419
	data : 0.11446919441223144
	model : 0.06492786407470703
			 train-loss:  2.167890501022339 	 ± 0.20257973406455543
	data : 0.11448893547058106
	model : 0.06488070487976075
			 train-loss:  2.1681885719299316 	 ± 0.19929222235583188
	data : 0.11462678909301757
	model : 0.06487579345703125
			 train-loss:  2.1831831112504005 	 ± 0.21318101217003793
	data : 0.114666748046875
	model : 0.06546382904052735
			 train-loss:  2.1877169320077607 	 ± 0.21148703282634085
	data : 0.114192533493042
	model : 0.06553640365600585
			 train-loss:  2.188606353367076 	 ± 0.2084163556095605
	data : 0.11431922912597656
	model : 0.06551094055175781
			 train-loss:  2.194909177507673 	 ± 0.20867913047579473
	data : 0.11428160667419433
	model : 0.06557245254516601
			 train-loss:  2.2056575814882913 	 ± 0.21536208524567826
	data : 0.11425457000732422
	model : 0.06557812690734863
			 train-loss:  2.213298887819857 	 ± 0.21732306855054812
	data : 0.11404790878295898
	model : 0.06498208045959472
			 train-loss:  2.2133514379200183 	 ± 0.21444472845023962
	data : 0.11449751853942872
	model : 0.06492996215820312
			 train-loss:  2.2210450050158377 	 ± 0.21692546301854895
	data : 0.11450800895690919
	model : 0.06494326591491699
			 train-loss:  2.218906283378601 	 ± 0.21461274699464697
	data : 0.11464509963989258
	model : 0.06488494873046875
			 train-loss:  2.210114426729156 	 ± 0.21915090613148802
	data : 0.11461901664733887
	model : 0.06496567726135254
			 train-loss:  2.2105099076316472 	 ± 0.21654105712349314
	data : 0.11483702659606934
	model : 0.06498713493347168
			 train-loss:  2.2063905416533003 	 ± 0.21566703274807808
	data : 0.1148374080657959
	model : 0.0650113582611084
			 train-loss:  2.2023168693889272 	 ± 0.21486914305493116
	data : 0.11476316452026367
	model : 0.0650202751159668
			 train-loss:  2.20523886680603 	 ± 0.21335053662240486
	data : 0.11460771560668945
	model : 0.0649573802947998
			 train-loss:  2.2013786409212197 	 ± 0.21260169424982986
	data : 0.11460132598876953
	model : 0.06489605903625488
			 train-loss:  2.19406968735634 	 ± 0.2160905972675034
	data : 0.11470913887023926
	model : 0.06493053436279297
			 train-loss:  2.2014925057689347 	 ± 0.21979978990158375
	data : 0.11482400894165039
	model : 0.06491332054138184
			 train-loss:  2.1949746657390983 	 ± 0.22218266579794263
	data : 0.11495318412780761
	model : 0.06496496200561523
			 train-loss:  2.196293258666992 	 ± 0.22014320321014627
	data : 0.11500449180603027
	model : 0.0650336742401123
			 train-loss:  2.190229144750857 	 ± 0.22215185866968784
	data : 0.11495409011840821
	model : 0.06504278182983399
			 train-loss:  2.19270691046348 	 ± 0.2207158523214738
	data : 0.11477932929992676
	model : 0.0650026798248291
			 train-loss:  2.1892879639031753 	 ± 0.22000946683132114
	data : 0.11463956832885742
	model : 0.06503233909606934
			 train-loss:  2.1862062878078885 	 ± 0.21911439619701858
	data : 0.11457638740539551
	model : 0.06493396759033203
			 train-loss:  2.1837109218944204 	 ± 0.21788629866879786
	data : 0.11470293998718262
	model : 0.06492218971252442
			 train-loss:  2.1816553260598863 	 ± 0.21646958767593605
	data : 0.11484479904174805
	model : 0.06490840911865234
			 train-loss:  2.185624599456787 	 ± 0.2166085884184672
	data : 0.11497044563293457
	model : 0.06490921974182129
			 train-loss:  2.1914184956715026 	 ± 0.21914327223080216
	data : 0.11503119468688965
	model : 0.06487951278686524
			 train-loss:  2.1887720843492926 	 ± 0.2182109394238592
	data : 0.11503167152404785
	model : 0.06493210792541504
			 train-loss:  2.1892168323198953 	 ± 0.21641183908046646
	data : 0.11499128341674805
	model : 0.06493539810180664
			 train-loss:  2.1838148324216 	 ± 0.21867146223927872
	data : 0.11487884521484375
	model : 0.06496229171752929
			 train-loss:  2.181409841583621 	 ± 0.21771262236696567
	data : 0.11467056274414063
	model : 0.06492924690246582
			 train-loss:  2.1871091203084068 	 ± 0.22059078514589014
	data : 0.1146815299987793
	model : 0.06493501663208008
			 train-loss:  2.182907061651349 	 ± 0.2213874140464342
	data : 0.11464424133300781
	model : 0.06494603157043458
			 train-loss:  2.1806401564524722 	 ± 0.22042512869712594
	data : 0.11463937759399415
	model : 0.06495671272277832
			 train-loss:  2.184065145073515 	 ± 0.22048481275730653
	data : 0.11475105285644531
	model : 0.06495165824890137
			 train-loss:  2.185724527088564 	 ± 0.2192480591845281
	data : 0.114825439453125
	model : 0.06502537727355957
			 train-loss:  2.185235330287148 	 ± 0.2176668048646197
	data : 0.11479930877685547
	model : 0.06506991386413574
			 train-loss:  2.1819665881170742 	 ± 0.2177584580222221
	data : 0.11486506462097168
	model : 0.06505136489868164
			 train-loss:  2.1780310000692094 	 ± 0.21865513087734922
	data : 0.11477818489074706
	model : 0.06504545211791993
			 train-loss:  2.174381524744168 	 ± 0.21924641833891237
	data : 0.11458721160888671
	model : 0.06508049964904786
			 train-loss:  2.174402736955219 	 ± 0.21771862348727994
	data : 0.11457691192626954
	model : 0.06505155563354492
			 train-loss:  2.1731945227270257 	 ± 0.2164651682300538
	data : 0.1146268367767334
	model : 0.06505031585693359
			 train-loss:  2.1719330194834114 	 ± 0.21526759061983047
	data : 0.1146738052368164
	model : 0.06506567001342774
			 train-loss:  2.1707239151000977 	 ± 0.21408047591806403
	data : 0.11471877098083497
	model : 0.06509599685668946
			 train-loss:  2.169679980528982 	 ± 0.21285946745304227
	data : 0.1147603988647461
	model : 0.06505813598632812
			 train-loss:  2.1690848214285716 	 ± 0.2115363860389449
	data : 0.11487436294555664
	model : 0.06505122184753417
			 train-loss:  2.1682544702138657 	 ± 0.21030227077780725
	data : 0.11487288475036621
	model : 0.06503148078918457
			 train-loss:  2.1705695798125446 	 ± 0.20996492138415443
	data : 0.11474933624267578
	model : 0.06499671936035156
			 train-loss:  2.1702591121196746 	 ± 0.20866676109069923
	data : 0.11464505195617676
	model : 0.06496901512145996
			 train-loss:  2.169112929591426 	 ± 0.20762794281251984
	data : 0.11476073265075684
	model : 0.06492919921875
			 train-loss:  2.1699370523778403 	 ± 0.20649128831430366
	data : 0.11477346420288086
	model : 0.06493268013000489
			 train-loss:  2.170059520077993 	 ± 0.205246591476652
	data : 0.11478209495544434
	model : 0.06497149467468262
			 train-loss:  2.172166185719626 	 ± 0.20492198198097725
	data : 0.11489872932434082
	model : 0.06498723030090332
			 train-loss:  2.1712561747607064 	 ± 0.20388365618733575
	data : 0.11494631767272949
	model : 0.06498484611511231
			 train-loss:  2.169725961463396 	 ± 0.20318519155038772
	data : 0.11483817100524903
	model : 0.06501212120056152
			 train-loss:  2.1733978233118165 	 ± 0.20486383709326164
	data : 0.11466217041015625
	model : 0.06498908996582031
			 train-loss:  2.1697514300996605 	 ± 0.20651643294392844
	data : 0.1145047664642334
	model : 0.06497111320495605
			 train-loss:  2.171877183271258 	 ± 0.20631890642451325
	data : 0.1145045280456543
	model : 0.0649827003479004
			 train-loss:  2.173897621366713 	 ± 0.2060529838731324
	data : 0.11460390090942382
	model : 0.06498556137084961
			 train-loss:  2.1721180349915894 	 ± 0.20561197939030598
	data : 0.11464200019836426
	model : 0.06505932807922363
			 train-loss:  2.173472865768101 	 ± 0.20489948210190082
	data : 0.11484103202819824
	model : 0.06508197784423828
			 train-loss:  2.1741502772095385 	 ± 0.20389844682092695
	data : 0.11497325897216797
	model : 0.06508293151855468
			 train-loss:  2.176262842847946 	 ± 0.20383166267449973
	data : 0.11508717536926269
	model : 0.06506586074829102
			 train-loss:  2.1767478993064477 	 ± 0.20281055819450547
	data : 0.11495018005371094
	model : 0.06507740020751954
			 train-loss:  2.173828033109506 	 ± 0.20374885686402777
	data : 0.114860200881958
	model : 0.06500000953674316
			 train-loss:  2.1717465980765747 	 ± 0.2037192409724892
	data : 0.11483249664306641
	model : 0.06501331329345703
			 train-loss:  2.1721207827937845 	 ± 0.2027106941648853
	data : 0.11483759880065918
	model : 0.06496963500976563
			 train-loss:  2.1728066001275574 	 ± 0.20179854423403534
	data : 0.11474418640136719
	model : 0.06499361991882324
			 train-loss:  2.172582006454468 	 ± 0.2007994514930441
	data : 0.11469821929931641
	model : 0.06500830650329589
			 train-loss:  2.1715645790100098 	 ± 0.20006179921200917
	data : 0.11476902961730957
	model : 0.06506085395812988
			 train-loss:  2.172178724232842 	 ± 0.19917434288515945
	data : 0.11470222473144531
	model : 0.06506929397583008
			 train-loss:  2.1718644054190626 	 ± 0.19823053849074784
	data : 0.11470952033996581
	model : 0.06505751609802246
			 train-loss:  2.168635380955843 	 ± 0.19997861439802375
	data : 0.11463241577148438
	model : 0.06506829261779785
			 train-loss:  2.167275713738941 	 ± 0.1995064900549352
	data : 0.11466145515441895
	model : 0.06501998901367187
			 train-loss:  2.17280339407471 	 ± 0.20648400017049565
	data : 0.11468915939331055
	model : 0.06494274139404296
			 train-loss:  2.1728906130122247 	 ± 0.20551881844881736
	data : 0.11474003791809081
	model : 0.06494841575622559
			 train-loss:  2.1723955637878842 	 ± 0.20462921388434946
	data : 0.11473932266235351
	model : 0.064998197555542
			 train-loss:  2.1720004552001253 	 ± 0.20372976739041943
	data : 0.1147646427154541
	model : 0.06498427391052246
			 train-loss:  2.1714733524756 	 ± 0.20287625973837614
	data : 0.11481380462646484
	model : 0.06502456665039062
			 train-loss:  2.171572499447041 	 ± 0.20196301217838755
	data : 0.11490216255187988
	model : 0.0650296688079834
			 train-loss:  2.1722178320799554 	 ± 0.20117429429409237
	data : 0.11479616165161133
	model : 0.06499662399291992
			 train-loss:  2.1708767846622297 	 ± 0.20078437962752257
	data : 0.11465697288513184
	model : 0.06497106552124024
			 train-loss:  2.17167070560288 	 ± 0.20007987751207143
	data : 0.11469016075134278
	model : 0.06495094299316406
			 train-loss:  2.1697437991266666 	 ± 0.20026765046992218
	data : 0.11466565132141113
	model : 0.06493916511535644
			 train-loss:  2.1696979567922394 	 ± 0.1994031653787865
	data : 0.11468362808227539
	model : 0.06492276191711426
			 train-loss:  2.170733610788981 	 ± 0.19886226043221894
	data : 0.11478772163391113
	model : 0.0649482250213623
			 train-loss:  2.1700537083512645 	 ± 0.19815435050671584
	data : 0.11494441032409668
	model : 0.06496958732604981
			 train-loss:  2.1683301034093905 	 ± 0.1982063153910377
	data : 0.11493387222290039
	model : 0.06500177383422852
			 train-loss:  2.1677186538775763 	 ± 0.19749139945437835
	data : 0.11507768630981445
	model : 0.06502280235290528
			 train-loss:  2.1711072005516243 	 ± 0.20014590942355945
	data : 0.11502656936645508
	model : 0.0650547981262207
			 train-loss:  2.1704139289308766 	 ± 0.19946978049525096
	data : 0.11489653587341309
	model : 0.0650251865386963
			 train-loss:  2.1702403295330885 	 ± 0.19866652654094208
	data : 0.11487874984741211
	model : 0.06501398086547852
			 train-loss:  2.1694544717188804 	 ± 0.1980556909761705
	data : 0.11483278274536132
	model : 0.06502275466918946
			 train-loss:  2.1684577322006224 	 ± 0.19757388742686577
	data : 0.11498265266418457
	model : 0.06499357223510742
			 train-loss:  2.1667004028956094 	 ± 0.1977666849889037
	data : 0.11508898735046387
	model : 0.06503286361694335
			 train-loss:  2.1657200854594314 	 ± 0.19729365083079783
	data : 0.11543459892272949
	model : 0.06507420539855957
			 train-loss:  2.164224565960467 	 ± 0.19724282054518347
	data : 0.11548571586608887
	model : 0.06512203216552734
			 train-loss:  2.163843318473461 	 ± 0.19652416633864436
	data : 0.11550464630126953
	model : 0.06510319709777831
			 train-loss:  2.1645576926378105 	 ± 0.1959349132080262
	data : 0.11528654098510742
	model : 0.06512680053710937
			 train-loss:  2.1669432161418536 	 ± 0.19707162897565056
	data : 0.11510357856750489
	model : 0.06508822441101074
			 train-loss:  2.165116894425768 	 ± 0.1974334055149866
	data : 0.11485366821289063
	model : 0.06505389213562011
			 train-loss:  2.168410544108627 	 ± 0.20029683025982947
	data : 0.11473236083984376
	model : 0.06499409675598145
			 train-loss:  2.168789789747836 	 ± 0.19959597955956823
	data : 0.11474008560180664
	model : 0.06501102447509766
			 train-loss:  2.1676865568867436 	 ± 0.19926502226044393
	data : 0.11478114128112793
	model : 0.06498966217041016
			 train-loss:  2.1675971299409866 	 ± 0.19853379761577886
	data : 0.11489911079406738
	model : 0.0649789810180664
			 train-loss:  2.170253733648871 	 ± 0.20021935238022467
	data : 0.1148688793182373
	model : 0.06498074531555176
			 train-loss:  2.1696266773818196 	 ± 0.19962756869337958
	data : 0.1149360179901123
	model : 0.06503181457519532
			 train-loss:  2.169741523351601 	 ± 0.1989127630633622
	data : 0.1149899959564209
	model : 0.06502423286437989
			 train-loss:  2.170785392182214 	 ± 0.19858281398050123
	data : 0.11494154930114746
	model : 0.06501436233520508
			 train-loss:  2.1698792783926564 	 ± 0.19816760107263726
	data : 0.11501121520996094
	model : 0.06505389213562011
			 train-loss:  2.1716252254768156 	 ± 0.1985539239541736
	data : 0.11505098342895508
	model : 0.06505808830261231
			 train-loss:  2.1701338899719134 	 ± 0.1986549530417845
	data : 0.1149759292602539
	model : 0.06496186256408691
			 train-loss:  2.1723571245869002 	 ± 0.19974121429032632
	data : 0.11491403579711915
	model : 0.0649341106414795
			 train-loss:  2.170877662198297 	 ± 0.19984141847249043
	data : 0.11493735313415528
	model : 0.06496939659118653
			 train-loss:  2.169551130843489 	 ± 0.19979541691481612
	data : 0.11542963981628418
	model : 0.06503958702087402
			 train-loss:  2.170786069363964 	 ± 0.1996730248691036
	data : 0.11546416282653808
	model : 0.06504178047180176
			 train-loss:  2.173403403243503 	 ± 0.20151164673519653
	data : 0.11565756797790527
	model : 0.0650865077972412
			 train-loss:  2.17286958630453 	 ± 0.20093926471342743
	data : 0.115618896484375
	model : 0.06513128280639649
			 train-loss:  2.17183204015096 	 ± 0.20066840720613302
	data : 0.11554584503173829
	model : 0.06517071723937988
			 train-loss:  2.170816819399398 	 ± 0.2003889627467288
	data : 0.11491603851318359
	model : 0.06506910324096679
			 train-loss:  2.1727041602134705 	 ± 0.201070694712266
	data : 0.11487326622009278
	model : 0.06506514549255371
			 train-loss:  2.1736727259517497 	 ± 0.20076796020922202
	data : 0.11472468376159668
	model : 0.06508331298828125
			 train-loss:  2.1723351385686303 	 ± 0.2007978423103809
	data : 0.11468133926391602
	model : 0.06503605842590332
			 train-loss:  2.1718969575820433 	 ± 0.20022291153249036
	data : 0.11470232009887696
	model : 0.06497159004211425
			 train-loss:  2.1716188268783765 	 ± 0.19961017620921118
	data : 0.1147219181060791
	model : 0.06498298645019532
			 train-loss:  2.171197748487922 	 ± 0.19904295386581322
	data : 0.11516265869140625
	model : 0.0650031566619873
			 train-loss:  2.1690123488631428 	 ± 0.20029272967294096
	data : 0.11532020568847656
	model : 0.06497883796691895
			 train-loss:  2.168734336049302 	 ± 0.1996924642161163
	data : 0.11539549827575683
	model : 0.06504802703857422
			 train-loss:  2.1684894233942034 	 ± 0.19909140032714182
	data : 0.11532816886901856
	model : 0.06508102416992187
			 train-loss:  2.169244649247353 	 ± 0.1987019107013488
	data : 0.11533632278442382
	model : 0.06509346961975097
			 train-loss:  2.1704176973413536 	 ± 0.19864610046057102
	data : 0.114910888671875
	model : 0.06503257751464844
			 train-loss:  2.171474879504713 	 ± 0.1984924240137923
	data : 0.11491055488586426
	model : 0.06505637168884278
			 train-loss:  2.170035448743076 	 ± 0.19873784928281796
	data : 0.114841890335083
	model : 0.06503629684448242
			 train-loss:  2.168107347054915 	 ± 0.199667326028618
	data : 0.11494193077087403
	model : 0.0650029182434082
			 train-loss:  2.169873584465808 	 ± 0.20035371544582328
	data : 0.11488614082336426
	model : 0.06498231887817382
			 train-loss:  2.174071638170117 	 ± 0.2069462951520392
	data : 0.11500067710876465
	model : 0.06503257751464844
			 train-loss:  2.175467304530598 	 ± 0.2071162585581854
	data : 0.1149064540863037
	model : 0.0650108814239502
			 train-loss:  2.1749763735652676 	 ± 0.20660059419355914
	data : 0.11494832038879395
	model : 0.0650446891784668
			 train-loss:  2.1750008674228893 	 ± 0.20599229525294194
	data : 0.11495409011840821
	model : 0.06508293151855468
			 train-loss:  2.1760695977517734 	 ± 0.2058612442683694
	data : 0.11498608589172363
	model : 0.06506853103637696
			 train-loss:  2.174999632807665 	 ± 0.20573825188765796
	data : 0.11485161781311035
	model : 0.06504459381103515
			 train-loss:  2.174783256701651 	 ± 0.20516239726732494
	data : 0.11479334831237793
	model : 0.06504330635070801
			 train-loss:  2.1751661033465943 	 ± 0.20463396670869968
	data : 0.11475481986999511
	model : 0.06495680809020996
			 train-loss:  2.176569196837289 	 ± 0.2048861222379271
	data : 0.11480703353881835
	model : 0.06492419242858886
			 train-loss:  2.175450469959866 	 ± 0.204838548876771
	data : 0.11494812965393067
	model : 0.06491413116455078
			 train-loss:  2.174403814272692 	 ± 0.20473050938589674
	data : 0.11506485939025879
	model : 0.06493382453918457
			 train-loss:  2.173516309663151 	 ± 0.20449577722057763
	data : 0.11516242027282715
	model : 0.06497740745544434
			 train-loss:  2.1740812016599005 	 ± 0.2040629809553508
	data : 0.11518950462341308
	model : 0.06500930786132812
			 train-loss:  2.1793413784768845 	 ± 0.21532113408497044
	data : 0.11514945030212402
	model : 0.0650033950805664
			 train-loss:  2.180456565888547 	 ± 0.21524612956259476
	data : 0.11488151550292969
	model : 0.06502790451049804
			 train-loss:  2.179262956420144 	 ± 0.21525380896130858
	data : 0.1147435188293457
	model : 0.06507072448730469
			 train-loss:  2.179580456572152 	 ± 0.21470760737634598
	data : 0.11466107368469239
	model : 0.06501436233520508
			 train-loss:  2.1773580902296565 	 ± 0.21622358245280723
	data : 0.11467080116271973
	model : 0.06503214836120605
			 train-loss:  2.1784218588390867 	 ± 0.21612065084366652
	data : 0.11469769477844238
	model : 0.06502680778503418
			 train-loss:  2.1813271808367904 	 ± 0.2191314273563927
	data : 0.1149113655090332
	model : 0.06498122215270996
			 train-loss:  2.1818636617558527 	 ± 0.21866717079224593
	data : 0.1149627685546875
	model : 0.06495370864868164
			 train-loss:  2.181313820975892 	 ± 0.21821441168113812
	data : 0.1150674819946289
	model : 0.06502137184143067
			 train-loss:  2.1798088922702448 	 ± 0.218612372069622
	data : 0.1149947166442871
	model : 0.0650484561920166
			 train-loss:  2.1820191038282295 	 ± 0.22014337904977896
	data : 0.11494617462158203
	model : 0.06505284309387208
			 train-loss:  2.1819239055923143 	 ± 0.21957025232703506
	data : 0.11483073234558105
	model : 0.06509723663330078
			 train-loss:  2.181768979256352 	 ± 0.21900817488061844
	data : 0.11475949287414551
	model : 0.06507859230041504
			 train-loss:  2.181931781645266 	 ± 0.21845170726690258
	data : 0.1146270751953125
	model : 0.06500253677368165
			 train-loss:  2.1820843631459264 	 ± 0.21789827067193904
	data : 0.1147280216217041
	model : 0.06493558883666992
			 train-loss:  2.1803155795121802 	 ± 0.21873069853115062
	data : 0.11479945182800293
	model : 0.06492929458618164
			 train-loss:  2.17929576367748 	 ± 0.21863628628916487
	data : 0.11493153572082519
	model : 0.06499648094177246
			 train-loss:  2.17883836678442 	 ± 0.2181746602079884
	data : 0.11500835418701172
	model : 0.06501951217651367
			 train-loss:  2.180024843023281 	 ± 0.21825924812239453
	data : 0.11504817008972168
	model : 0.065045166015625
			 train-loss:  2.1820322137382164 	 ± 0.21953488411870864
	data : 0.1150813102722168
	model : 0.06505818367004394
			 train-loss:  2.180668909549713 	 ± 0.21982822517151293
	data : 0.11505193710327148
	model : 0.06508302688598633
			 train-loss:  2.1813964487901374 	 ± 0.21952196042567027
	data : 0.11485962867736817
	model : 0.06503763198852539
			 train-loss:  2.181281600848283 	 ± 0.21898396857248936
	data : 0.1148186206817627
	model : 0.0649728775024414
			 train-loss:  2.1804483688523617 	 ± 0.21876470387072763
	data : 0.11483516693115234
	model : 0.06499414443969727
			 train-loss:  2.179762731580173 	 ± 0.2184463953259873
	data : 0.11489887237548828
	model : 0.06499252319335938
			 train-loss:  2.1803501210561613 	 ± 0.2180743869574612
	data : 0.11498470306396484
	model : 0.0649714469909668
			 train-loss:  2.1814590680946426 	 ± 0.21812309202610916
	data : 0.11502652168273926
	model : 0.06494555473327637
			 train-loss:  2.180858092607507 	 ± 0.217766482110813
	data : 0.11502432823181152
	model : 0.0650442123413086
			 train-loss:  2.182866857602046 	 ± 0.2191563882418746
	data : 0.11499338150024414
	model : 0.065006685256958
			 train-loss:  2.1832749638260838 	 ± 0.21871067354607537
	data : 0.11487960815429688
	model : 0.06501526832580566
			 train-loss:  2.1836136840638662 	 ± 0.218244255193314
	data : 0.11477746963500976
	model : 0.06499919891357422
			 train-loss:  2.183101597555441 	 ± 0.21785290115602216
	data : 0.1147810935974121
	model : 0.06501154899597168
			 train-loss:  2.181492402868451 	 ± 0.21859186911644352
	data : 0.11494760513305664
	model : 0.0651632308959961
			 train-loss:  2.180925775581682 	 ± 0.21823414214319684
	data : 0.11496944427490234
	model : 0.0652130126953125
			 train-loss:  2.181428858052904 	 ± 0.21784741753666795
	data : 0.11506738662719726
	model : 0.06522679328918457
			 train-loss:  2.182180847123612 	 ± 0.21761842550678862
	data : 0.11497206687927246
	model : 0.06526226997375488
			 train-loss:  2.181687392570354 	 ± 0.2172346245026656
	data : 0.11492881774902344
	model : 0.0652644157409668
			 train-loss:  2.182421898512247 	 ± 0.21700217593269028
	data : 0.11486277580261231
	model : 0.06506376266479492
			 train-loss:  2.182616456932978 	 ± 0.21652286146479233
	data : 0.11489691734313964
	model : 0.06498141288757324
			 train-loss:  2.1834366974765307 	 ± 0.21636715325221806
	data : 0.11471099853515625
	model : 0.06490473747253418
			 train-loss:  2.1830545392903415 	 ± 0.21594891650468018
	data : 0.1147679328918457
	model : 0.0653611183166504
			 train-loss:  2.182919452632714 	 ± 0.21546910662598706
	data : 0.11444950103759766
	model : 0.06530256271362304
			 train-loss:  2.181897220847843 	 ± 0.2155197003684939
	data : 0.11453866958618164
	model : 0.06524014472961426
			 train-loss:  2.1834923809419298 	 ± 0.21634541330602042
	data : 0.11458144187927247
	model : 0.06519980430603027
			 train-loss:  2.184127528220415 	 ± 0.21607023475707027
	data : 0.11471438407897949
	model : 0.06510272026062011
			 train-loss:  2.1835023217731053 	 ± 0.21579251444411027
	data : 0.11478404998779297
	model : 0.0646364688873291
			 train-loss:  2.1840717396904936 	 ± 0.21548391240032383
	data : 0.11519298553466797
	model : 0.06445765495300293
			 train-loss:  2.1831008752537193 	 ± 0.2155035661440609
	data : 0.1151947021484375
	model : 0.06429038047790528
			 train-loss:  2.1830259598138038 	 ± 0.21503341361245007
	data : 0.11518673896789551
	model : 0.06416282653808594
			 train-loss:  2.1825951806322457 	 ± 0.21466196786663821
	data : 0.11544189453125
	model : 0.06411900520324706
			 train-loss:  2.1829348631527115 	 ± 0.21425647399550854
	data : 0.11555218696594238
	model : 0.06391067504882812
			 train-loss:  2.182716033675454 	 ± 0.2138179691738626
	data : 0.11567254066467285
	model : 0.06395974159240722
			 train-loss:  2.18348342334402 	 ± 0.21367521133081074
	data : 0.11580362319946289
	model : 0.06394362449645996
			 train-loss:  2.1834802141517016 	 ± 0.21321619339338999
	data : 0.1158529281616211
	model : 0.06392173767089844
			 train-loss:  2.1850704713764353 	 ± 0.21414038533333032
	data : 0.11576833724975585
	model : 0.06387615203857422
			 train-loss:  2.1849425006420056 	 ± 0.21369324831087255
	data : 0.11575136184692383
	model : 0.06385936737060546
			 train-loss:  2.1854038031424508 	 ± 0.21335725307366762
	data : 0.11589250564575196
	model : 0.06380114555358887
			 train-loss:  2.1859661079157253 	 ± 0.21308182564704295
	data : 0.11577672958374023
	model : 0.06382861137390136
			 train-loss:  2.186612343086916 	 ± 0.21286631437820094
	data : 0.11579065322875977
	model : 0.0638498306274414
			 train-loss:  2.1858458474091407 	 ± 0.212749397822072
	data : 0.11583247184753417
	model : 0.06386113166809082
			 train-loss:  2.1844037642081577 	 ± 0.213473039695748
	data : 0.1156855583190918
	model : 0.06386590003967285
			 train-loss:  2.184281860644392 	 ± 0.21303805995617195
	data : 0.11557140350341796
	model : 0.06387877464294434
			 train-loss:  2.1852172751072025 	 ± 0.2130928150631577
	data : 0.11556463241577149
	model : 0.06386847496032715
			 train-loss:  2.187220495914726 	 ± 0.2149251070295275
	data : 0.11561527252197265
	model : 0.06386284828186035
			 train-loss:  2.1895879761117403 	 ± 0.21763614762786826
	data : 0.11553082466125489
	model : 0.0638537883758545
			 train-loss:  2.1924630593280403 	 ± 0.22178614354058163
	data : 0.11565675735473632
	model : 0.06385126113891601
			 train-loss:  2.1960376152178136 	 ± 0.22829720323590877
	data : 0.11565179824829101
	model : 0.06380743980407715
			 train-loss:  2.195929497359735 	 ± 0.22784090509891491
	data : 0.11538004875183105
	model : 0.06379733085632325
			 train-loss:  2.1951186580042683 	 ± 0.22773789823841856
	data : 0.11534881591796875
	model : 0.06382765769958496
			 train-loss:  2.195715864020658 	 ± 0.2274746345724296
	data : 0.11550450325012207
	model : 0.06383447647094727
			 train-loss:  2.1955317821502685 	 ± 0.22703781223395683
	data : 0.11557226181030274
	model : 0.06384916305541992
			 train-loss:  2.1950307633297377 	 ± 0.22672353192546307
	data : 0.11547489166259765
	model : 0.0639493465423584
			 train-loss:  2.193872848673472 	 ± 0.22701566003744442
	data : 0.11566500663757324
	model : 0.06395049095153808
			 train-loss:  2.1936825598652656 	 ± 0.2265867046858454
	data : 0.11561045646667481
	model : 0.06392927169799804
			 train-loss:  2.194607706051173 	 ± 0.22661849939176262
	data : 0.11554698944091797
	model : 0.063916015625
			 train-loss:  2.196294145490609 	 ± 0.2277651075436228
	data : 0.11554670333862305
	model : 0.06387791633605958
			 train-loss:  2.1958021498285234 	 ± 0.22745554539855922
	data : 0.11543335914611816
	model : 0.05544323921203613
#epoch  18    val-loss:  2.3753910127438997  train-loss:  2.1958021498285234  lr:  0.0003125
			 train-loss:  2.171416997909546 	 ± 0.0
	data : 5.654125452041626
	model : 0.07074689865112305
			 train-loss:  2.170049548149109 	 ± 0.0013674497604370117
	data : 2.8919824361801147
	model : 0.07023584842681885
			 train-loss:  2.1599298318227134 	 ± 0.014354927015429847
	data : 1.9669830799102783
	model : 0.06836501757303874
			 train-loss:  2.0859333872795105 	 ± 0.1287671128911399
	data : 1.5040200352668762
	model : 0.0674583911895752
			 train-loss:  2.1062255859375 	 ± 0.12211416295973573
	data : 1.226145362854004
	model : 0.06690435409545899
			 train-loss:  2.1037340561548867 	 ± 0.11161360102045055
	data : 0.11813292503356934
	model : 0.06575345993041992
			 train-loss:  2.081149731363569 	 ± 0.11721027996131132
	data : 0.11494150161743164
	model : 0.06476755142211914
			 train-loss:  2.1728381663560867 	 ± 0.2662111047499201
	data : 0.11451034545898438
	model : 0.06480240821838379
			 train-loss:  2.169013910823398 	 ± 0.2512192078034845
	data : 0.11440038681030273
	model : 0.06486015319824219
			 train-loss:  2.1905060410499573 	 ± 0.2468950916635397
	data : 0.11440649032592773
	model : 0.06495637893676758
			 train-loss:  2.1538323163986206 	 ± 0.2624218830352972
	data : 0.11502885818481445
	model : 0.06509246826171874
			 train-loss:  2.164730876684189 	 ± 0.25383663857637867
	data : 0.114985990524292
	model : 0.06513032913208008
			 train-loss:  2.1679702630409827 	 ± 0.2441363859598785
	data : 0.11494836807250977
	model : 0.06514959335327149
			 train-loss:  2.1581770948001315 	 ± 0.23789079015791467
	data : 0.11490235328674317
	model : 0.06511449813842773
			 train-loss:  2.1536481300989787 	 ± 0.23044823164909128
	data : 0.11481108665466308
	model : 0.06503520011901856
			 train-loss:  2.1566822454333305 	 ± 0.22343975942351532
	data : 0.11423740386962891
	model : 0.06487407684326171
			 train-loss:  2.1767110614215626 	 ± 0.231099540721668
	data : 0.11443696022033692
	model : 0.06485977172851562
			 train-loss:  2.1719954212506614 	 ± 0.22542842711278235
	data : 0.1145820140838623
	model : 0.0648951530456543
			 train-loss:  2.1753402948379517 	 ± 0.21987435656654467
	data : 0.11462268829345704
	model : 0.06496295928955079
			 train-loss:  2.1954513609409334 	 ± 0.23154295639890626
	data : 0.11472821235656738
	model : 0.06497769355773926
			 train-loss:  2.2175667002087547 	 ± 0.2466595755238645
	data : 0.1147526741027832
	model : 0.06499319076538086
			 train-loss:  2.236397022550756 	 ± 0.2559719720452873
	data : 0.11469879150390624
	model : 0.06497249603271485
			 train-loss:  2.2487990493359775 	 ± 0.25701499754933366
	data : 0.11440067291259766
	model : 0.06491436958312988
			 train-loss:  2.2457907646894455 	 ± 0.2520168465630132
	data : 0.1144195556640625
	model : 0.06485414505004883
			 train-loss:  2.2491308450698853 	 ± 0.24746664138879385
	data : 0.11440668106079102
	model : 0.06490812301635743
			 train-loss:  2.258094608783722 	 ± 0.2467652539087284
	data : 0.11453814506530761
	model : 0.06491069793701172
			 train-loss:  2.257810146720321 	 ± 0.24215675661116173
	data : 0.11457691192626954
	model : 0.06496777534484863
			 train-loss:  2.257308930158615 	 ± 0.23780747613017889
	data : 0.11472749710083008
	model : 0.06500482559204102
			 train-loss:  2.250610824288993 	 ± 0.23634407654028897
	data : 0.11470942497253418
	model : 0.06505475044250489
			 train-loss:  2.245588990052541 	 ± 0.23393998700310484
	data : 0.11470298767089844
	model : 0.06501398086547852
			 train-loss:  2.239096999168396 	 ± 0.23286665539326548
	data : 0.11448440551757813
	model : 0.06501035690307617
			 train-loss:  2.230673234909773 	 ± 0.2339488045536084
	data : 0.11444134712219238
	model : 0.06496653556823731
			 train-loss:  2.2280305768504287 	 ± 0.23086137165336798
	data : 0.11448745727539063
	model : 0.06498346328735352
			 train-loss:  2.2209580365349266 	 ± 0.23104134298766046
	data : 0.11458730697631836
	model : 0.06499948501586914
			 train-loss:  2.215689468383789 	 ± 0.22977972483580278
	data : 0.11465921401977539
	model : 0.06501979827880859
			 train-loss:  2.221631487210592 	 ± 0.2292768116301644
	data : 0.1147608757019043
	model : 0.06502866744995117
			 train-loss:  2.2186631576434985 	 ± 0.22685744151782683
	data : 0.1148219108581543
	model : 0.06507248878479004
			 train-loss:  2.2292671517321936 	 ± 0.23296014395931156
	data : 0.11474690437316895
	model : 0.06505951881408692
			 train-loss:  2.22439055565076 	 ± 0.23191068197767586
	data : 0.11458058357238769
	model : 0.06500873565673829
			 train-loss:  2.2166219621896746 	 ± 0.23407625047225147
	data : 0.11446290016174317
	model : 0.06498565673828124
			 train-loss:  2.212755502724066 	 ± 0.23249363148039465
	data : 0.11468830108642578
	model : 0.06497530937194824
			 train-loss:  2.198311229546865 	 ± 0.24762960999743022
	data : 0.11471915245056152
	model : 0.06498756408691406
			 train-loss:  2.2004263927770213 	 ± 0.24511685342270456
	data : 0.11472644805908203
	model : 0.06500029563903809
			 train-loss:  2.2011037225073036 	 ± 0.2423561284619859
	data : 0.11476674079895019
	model : 0.06503124237060547
			 train-loss:  2.198874746428596 	 ± 0.24010381986721901
	data : 0.11487226486206055
	model : 0.065081787109375
			 train-loss:  2.1944930605266406 	 ± 0.2392917665532553
	data : 0.11467118263244629
	model : 0.06509881019592285
			 train-loss:  2.1932085118395217 	 ± 0.23689268241263514
	data : 0.11454834938049316
	model : 0.06510238647460938
			 train-loss:  2.194351777434349 	 ± 0.23454305925404156
	data : 0.11446924209594726
	model : 0.06503667831420898
			 train-loss:  2.1964045981971587 	 ± 0.23257269825986526
	data : 0.1145327091217041
	model : 0.0650097370147705
			 train-loss:  2.1903200364112854 	 ± 0.23414169105594182
	data : 0.11458086967468262
	model : 0.06499156951904297
			 train-loss:  2.197439397082609 	 ± 0.23723753630627412
	data : 0.11455006599426269
	model : 0.06497797966003419
			 train-loss:  2.1985146747185635 	 ± 0.23507079052405075
	data : 0.11454463005065918
	model : 0.06497411727905274
			 train-loss:  2.193703586200498 	 ± 0.23541301864561257
	data : 0.11463398933410644
	model : 0.06506357192993165
			 train-loss:  2.2030836233386286 	 ± 0.2430148479775298
	data : 0.11469917297363282
	model : 0.06504902839660645
			 train-loss:  2.2071454026482322 	 ± 0.24263833520740796
	data : 0.114453125
	model : 0.06498403549194336
			 train-loss:  2.2034802585840225 	 ± 0.2419935580491239
	data : 0.11442270278930664
	model : 0.06500797271728516
			 train-loss:  2.1999402610879195 	 ± 0.24131984345963922
	data : 0.11451182365417481
	model : 0.0650102138519287
			 train-loss:  2.191784860758946 	 ± 0.24702694616256307
	data : 0.11454281806945801
	model : 0.06500029563903809
			 train-loss:  2.1858007140078786 	 ± 0.2491285137619482
	data : 0.11444969177246093
	model : 0.06505565643310547
			 train-loss:  2.1861992557843526 	 ± 0.24706268582499064
	data : 0.11472640037536622
	model : 0.0651155948638916
			 train-loss:  2.1821368405076322 	 ± 0.24704151177903116
	data : 0.11479687690734863
	model : 0.06515421867370605
			 train-loss:  2.1845894398227816 	 ± 0.24578871308003788
	data : 0.11484899520874023
	model : 0.0651120662689209
			 train-loss:  2.1811007942472185 	 ± 0.24537267485116096
	data : 0.11478543281555176
	model : 0.06515970230102539
			 train-loss:  2.178666552528739 	 ± 0.24421366063255603
	data : 0.1147836685180664
	model : 0.06510281562805176
			 train-loss:  2.182854041686425 	 ± 0.24463240375292358
	data : 0.11471576690673828
	model : 0.06507725715637207
			 train-loss:  2.1862510749787996 	 ± 0.24431201286992643
	data : 0.11475772857666015
	model : 0.06501774787902832
			 train-loss:  2.1857336652812673 	 ± 0.2425183655679294
	data : 0.11478133201599121
	model : 0.06503767967224121
			 train-loss:  2.1881018754313972 	 ± 0.2415077492401984
	data : 0.1147979736328125
	model : 0.0649946689605713
			 train-loss:  2.189158192579297 	 ± 0.23990949022468297
	data : 0.11480855941772461
	model : 0.06503505706787109
			 train-loss:  2.1880378127098083 	 ± 0.2383714311762748
	data : 0.11493091583251953
	model : 0.06506710052490235
			 train-loss:  2.1856770800872587 	 ± 0.23750949113406625
	data : 0.11483368873596192
	model : 0.06510591506958008
			 train-loss:  2.1889079163471856 	 ± 0.23742029216588267
	data : 0.11486148834228516
	model : 0.06507129669189453
			 train-loss:  2.1896505666105717 	 ± 0.2358727102773642
	data : 0.11480412483215333
	model : 0.06503267288208008
			 train-loss:  2.1865691268766247 	 ± 0.23574828368172338
	data : 0.11477742195129395
	model : 0.06498494148254394
			 train-loss:  2.1848277950286867 	 ± 0.2346499706460035
	data : 0.11472902297973633
	model : 0.06499876976013183
			 train-loss:  2.189007570869044 	 ± 0.23589492679179025
	data : 0.11493258476257324
	model : 0.06494626998901368
			 train-loss:  2.185329734504997 	 ± 0.23654121893845934
	data : 0.11488509178161621
	model : 0.06499037742614747
			 train-loss:  2.1866661799259677 	 ± 0.23531244506188684
	data : 0.1150588035583496
	model : 0.06499605178833008
			 train-loss:  2.1828533516654485 	 ± 0.2362307632609521
	data : 0.11512484550476074
	model : 0.06506085395812988
			 train-loss:  2.185597315430641 	 ± 0.23601319661492845
	data : 0.11500287055969238
	model : 0.06504559516906738
			 train-loss:  2.185404983567603 	 ± 0.2345581089214894
	data : 0.11481695175170899
	model : 0.0650782585144043
			 train-loss:  2.1823322860206047 	 ± 0.23475800379864617
	data : 0.1147538185119629
	model : 0.06514363288879395
			 train-loss:  2.1806274666843644 	 ± 0.23384963987910073
	data : 0.11462960243225098
	model : 0.06509294509887695
			 train-loss:  2.177851458390554 	 ± 0.23382525699800494
	data : 0.11455044746398926
	model : 0.06509079933166503
			 train-loss:  2.1788909126730527 	 ± 0.232640889370973
	data : 0.11469497680664062
	model : 0.06510143280029297
			 train-loss:  2.1770302112712416 	 ± 0.23191970198650896
	data : 0.11472225189208984
	model : 0.06510944366455078
			 train-loss:  2.177775487132456 	 ± 0.23068653447959347
	data : 0.11468467712402344
	model : 0.06503710746765137
			 train-loss:  2.1754591979763727 	 ± 0.23038732337552362
	data : 0.11472392082214355
	model : 0.0651099681854248
			 train-loss:  2.175030011809274 	 ± 0.22912473204647132
	data : 0.1147010326385498
	model : 0.06507987976074218
			 train-loss:  2.1743204991022744 	 ± 0.22794655837331826
	data : 0.11448192596435547
	model : 0.06506557464599609
			 train-loss:  2.176635068851513 	 ± 0.22775161793536217
	data : 0.11448683738708496
	model : 0.06502299308776856
			 train-loss:  2.1762589604958245 	 ± 0.2265388687764178
	data : 0.11456732749938965
	model : 0.0649862289428711
			 train-loss:  2.174867258276991 	 ± 0.2257126966986731
	data : 0.11460371017456054
	model : 0.06495723724365235
			 train-loss:  2.174173398220793 	 ± 0.22460858055078167
	data : 0.11464085578918456
	model : 0.06495451927185059
			 train-loss:  2.1737347828714473 	 ± 0.22346376941579266
	data : 0.11471085548400879
	model : 0.06497883796691895
			 train-loss:  2.170041127751271 	 ± 0.22519321118307525
	data : 0.11472930908203124
	model : 0.06504998207092286
			 train-loss:  2.1694214503789686 	 ± 0.22411167400528353
	data : 0.11473116874694825
	model : 0.06506857872009278
			 train-loss:  2.167263259693068 	 ± 0.22397619768854188
	data : 0.11483702659606934
	model : 0.0650627613067627
			 train-loss:  2.16758081407258 	 ± 0.222864306138876
	data : 0.11481652259826661
	model : 0.06505403518676758
			 train-loss:  2.17111775636673 	 ± 0.22452238213522802
	data : 0.1149061679840088
	model : 0.06497511863708497
			 train-loss:  2.172708638823859 	 ± 0.22397383529542106
	data : 0.11498017311096191
	model : 0.06492986679077148
			 train-loss:  2.1745473169812968 	 ± 0.22363793666352488
	data : 0.11502151489257813
	model : 0.06495051383972168
			 train-loss:  2.172561673284734 	 ± 0.2234513778472573
	data : 0.11493711471557617
	model : 0.06500239372253418
			 train-loss:  2.1722138799153843 	 ± 0.22240250904197187
	data : 0.11500377655029297
	model : 0.0650364875793457
			 train-loss:  2.1756704852694555 	 ± 0.22413033163328683
	data : 0.11497030258178711
	model : 0.06508088111877441
			 train-loss:  2.176152510463067 	 ± 0.22312528463133532
	data : 0.11485018730163574
	model : 0.0650754451751709
			 train-loss:  2.177414695793223 	 ± 0.22246007085079858
	data : 0.11470217704772949
	model : 0.0650524616241455
			 train-loss:  2.177942830103415 	 ± 0.2214951500445708
	data : 0.1146860122680664
	model : 0.06502251625061035
			 train-loss:  2.176819429485076 	 ± 0.22078566064294003
	data : 0.11486287117004394
	model : 0.0649641990661621
			 train-loss:  2.177275724844499 	 ± 0.21983142197167244
	data : 0.11483926773071289
	model : 0.06494460105895997
			 train-loss:  2.178636522980424 	 ± 0.21930385519547718
	data : 0.1150141716003418
	model : 0.06493549346923828
			 train-loss:  2.180419019290379 	 ± 0.21912883905969097
	data : 0.11519064903259277
	model : 0.0648993968963623
			 train-loss:  2.1864381490555487 	 ± 0.22726694948695872
	data : 0.11518049240112305
	model : 0.06488561630249023
			 train-loss:  2.187952472452532 	 ± 0.22683986120625402
	data : 0.11501312255859375
	model : 0.06492671966552735
			 train-loss:  2.1871419035870097 	 ± 0.22601720475063702
	data : 0.11503062248229981
	model : 0.06498479843139648
			 train-loss:  2.189770762262673 	 ± 0.2267998075824176
	data : 0.11487288475036621
	model : 0.06497688293457031
			 train-loss:  2.195428997023493 	 ± 0.233906653131282
	data : 0.11473751068115234
	model : 0.06500840187072754
			 train-loss:  2.194144560118853 	 ± 0.23332741518102132
	data : 0.11465930938720703
	model : 0.06497340202331543
			 train-loss:  2.192739528768203 	 ± 0.23284573195293512
	data : 0.11452007293701172
	model : 0.06497268676757813
			 train-loss:  2.1919109245141346 	 ± 0.23204962626012912
	data : 0.11461210250854492
	model : 0.06499066352844238
			 train-loss:  2.1916223419599294 	 ± 0.23111037593615147
	data : 0.11486949920654296
	model : 0.06501278877258301
			 train-loss:  2.1917094441710927 	 ± 0.23016324758918838
	data : 0.11494946479797363
	model : 0.06501650810241699
			 train-loss:  2.1913286011393476 	 ± 0.22926430938219328
	data : 0.11506147384643554
	model : 0.06506338119506835
			 train-loss:  2.189258668691881 	 ± 0.22948909291022238
	data : 0.11513442993164062
	model : 0.06509189605712891
			 train-loss:  2.190662392616272 	 ± 0.22910315685466032
	data : 0.115047025680542
	model : 0.06504111289978028
			 train-loss:  2.1887082391315036 	 ± 0.2292357356454094
	data : 0.1148949146270752
	model : 0.06502933502197265
			 train-loss:  2.188742645143524 	 ± 0.22833177575994407
	data : 0.11483030319213867
	model : 0.06502737998962402
			 train-loss:  2.1873165369033813 	 ± 0.22800522509162582
	data : 0.11470708847045899
	model : 0.06495432853698731
			 train-loss:  2.192997132160867 	 ± 0.23603779411770687
	data : 0.11475739479064942
	model : 0.06488676071166992
			 train-loss:  2.1928801701619074 	 ± 0.23513195653526497
	data : 0.11481561660766601
	model : 0.06486954689025878
			 train-loss:  2.192626339788655 	 ± 0.23425066577372097
	data : 0.11486902236938476
	model : 0.06488256454467774
			 train-loss:  2.191279057300452 	 ± 0.23387059310009872
	data : 0.11492552757263183
	model : 0.0649324893951416
			 train-loss:  2.1919728634052706 	 ± 0.23312604044129004
	data : 0.11501693725585938
	model : 0.06500763893127441
			 train-loss:  2.1944050771086965 	 ± 0.23394220172486288
	data : 0.11508731842041016
	model : 0.06502575874328613
			 train-loss:  2.191681739136025 	 ± 0.2351964592252181
	data : 0.11501636505126953
	model : 0.06509041786193848
			 train-loss:  2.1914480915840935 	 ± 0.23434589572028877
	data : 0.11485776901245118
	model : 0.06508212089538574
			 train-loss:  2.1916175262771382 	 ± 0.23349741291758308
	data : 0.11467857360839843
	model : 0.0649991512298584
			 train-loss:  2.1933073712431868 	 ± 0.23348913409224828
	data : 0.11466617584228515
	model : 0.06497116088867187
			 train-loss:  2.191649908642117 	 ± 0.23346108240036123
	data : 0.11478600502014161
	model : 0.06500959396362305
			 train-loss:  2.1930039848600114 	 ± 0.23317294391503343
	data : 0.11486554145812988
	model : 0.06494240760803223
			 train-loss:  2.1911589397606273 	 ± 0.23336796784949798
	data : 0.11492276191711426
	model : 0.06500577926635742
			 train-loss:  2.1957298592782357 	 ± 0.23879497282218512
	data : 0.11550264358520508
	model : 0.06505417823791504
			 train-loss:  2.194860419193348 	 ± 0.2381840003758327
	data : 0.11547951698303223
	model : 0.06508989334106445
			 train-loss:  2.194377732773622 	 ± 0.23742570519860862
	data : 0.11522378921508789
	model : 0.06507086753845215
			 train-loss:  2.193197183773435 	 ± 0.23702930710240638
	data : 0.11516394615173339
	model : 0.06507573127746583
			 train-loss:  2.1911613067535507 	 ± 0.2374848908953569
	data : 0.11508693695068359
	model : 0.06505384445190429
			 train-loss:  2.1918324832202627 	 ± 0.2368146448250006
	data : 0.11501646041870117
	model : 0.06504511833190918
			 train-loss:  2.191192104204281 	 ± 0.23614091486963068
	data : 0.11533141136169434
	model : 0.06507763862609864
			 train-loss:  2.189835409990093 	 ± 0.23592519606154025
	data : 0.11537904739379883
	model : 0.06505470275878907
			 train-loss:  2.1889177346229554 	 ± 0.23540412889242893
	data : 0.11538491249084473
	model : 0.06505069732666016
			 train-loss:  2.1878637501735563 	 ± 0.234978188220728
	data : 0.11558899879455567
	model : 0.06499772071838379
			 train-loss:  2.188422649314529 	 ± 0.2343046342221578
	data : 0.1153038501739502
	model : 0.06497740745544434
			 train-loss:  2.1903027093488405 	 ± 0.2346851293702264
	data : 0.1150404930114746
	model : 0.06495752334594726
			 train-loss:  2.189036785008071 	 ± 0.2344454280535763
	data : 0.11508364677429199
	model : 0.0650357723236084
			 train-loss:  2.189292191690014 	 ± 0.23370942182543034
	data : 0.11504611968994141
	model : 0.06507015228271484
			 train-loss:  2.1886436488383856 	 ± 0.23309903315320318
	data : 0.11493377685546875
	model : 0.06511635780334472
			 train-loss:  2.188621857363707 	 ± 0.23235565308272646
	data : 0.11490564346313477
	model : 0.06517190933227539
			 train-loss:  2.187436281125757 	 ± 0.23209507498230023
	data : 0.11493825912475586
	model : 0.06514992713928222
			 train-loss:  2.186518575410423 	 ± 0.2316514529128425
	data : 0.11509265899658203
	model : 0.06505484580993652
			 train-loss:  2.1861234791576862 	 ± 0.23098014142060194
	data : 0.11548037528991699
	model : 0.06506710052490235
			 train-loss:  2.1842842938737097 	 ± 0.23143393081507957
	data : 0.11547508239746093
	model : 0.06510787010192871
			 train-loss:  2.1838409849155096 	 ± 0.23078708142123253
	data : 0.1155181884765625
	model : 0.06506462097167968
			 train-loss:  2.1847716452885257 	 ± 0.23038277912184613
	data : 0.11554083824157715
	model : 0.06506242752075195
			 train-loss:  2.1832519893239186 	 ± 0.23049731916228547
	data : 0.11545324325561523
	model : 0.06508917808532715
			 train-loss:  2.1838636940175835 	 ± 0.22993126378896986
	data : 0.11508846282958984
	model : 0.06506028175354003
			 train-loss:  2.1838074656854194 	 ± 0.22923879139961106
	data : 0.11511545181274414
	model : 0.0649949073791504
			 train-loss:  2.185169253520623 	 ± 0.22922388981472963
	data : 0.1151209831237793
	model : 0.06498327255249023
			 train-loss:  2.1867143391143706 	 ± 0.22941122364170233
	data : 0.1150968074798584
	model : 0.06495027542114258
			 train-loss:  2.186628772662236 	 ± 0.228734173984544
	data : 0.11489429473876953
	model : 0.06494202613830566
			 train-loss:  2.1868038969881396 	 ± 0.22807179705124575
	data : 0.11485276222229004
	model : 0.0649193286895752
			 train-loss:  2.1868343220816717 	 ± 0.22740428863130524
	data : 0.1147928237915039
	model : 0.06491012573242187
			 train-loss:  2.18674229951792 	 ± 0.22674545915719801
	data : 0.11483941078186036
	model : 0.06490354537963867
			 train-loss:  2.186537052854637 	 ± 0.22610519912978477
	data : 0.11473493576049805
	model : 0.0649643898010254
			 train-loss:  2.1869232716231513 	 ± 0.22551175809314006
	data : 0.11484103202819824
	model : 0.06500029563903809
			 train-loss:  2.1877102163859776 	 ± 0.22510598605198193
	data : 0.11488609313964844
	model : 0.06500935554504395
			 train-loss:  2.1871021938594906 	 ± 0.2246096343744341
	data : 0.11494450569152832
	model : 0.06504578590393066
			 train-loss:  2.185982343167235 	 ± 0.22446642964939995
	data : 0.11490850448608399
	model : 0.0651090145111084
			 train-loss:  2.185869464713536 	 ± 0.22384005560911968
	data : 0.11496467590332031
	model : 0.06507105827331543
			 train-loss:  2.1848496951204437 	 ± 0.223628185510809
	data : 0.11496481895446778
	model : 0.06502747535705566
			 train-loss:  2.183386915259891 	 ± 0.2238632287736806
	data : 0.11492400169372559
	model : 0.06506094932556153
			 train-loss:  2.1841271621746254 	 ± 0.22346476666523815
	data : 0.11491785049438477
	model : 0.06500334739685058
			 train-loss:  2.1840067355187385 	 ± 0.22285589631719324
	data : 0.11483592987060547
	model : 0.06496186256408691
			 train-loss:  2.1876197147890517 	 ± 0.227528277670669
	data : 0.11486091613769531
	model : 0.06497154235839844
			 train-loss:  2.188833105823268 	 ± 0.22750208218834597
	data : 0.11481733322143554
	model : 0.06497926712036133
			 train-loss:  2.190129851006173 	 ± 0.22756720679879447
	data : 0.1148188591003418
	model : 0.06494302749633789
			 train-loss:  2.190165364614097 	 ± 0.22695515658080043
	data : 0.11480941772460937
	model : 0.06497278213500976
			 train-loss:  2.1884082900011603 	 ± 0.22761246822541947
	data : 0.1148493766784668
	model : 0.0650146484375
			 train-loss:  2.1895412124217826 	 ± 0.22753435168278935
	data : 0.11469182968139649
	model : 0.06499381065368652
			 train-loss:  2.1903221184614474 	 ± 0.22718406849077052
	data : 0.11468420028686524
	model : 0.0649789810180664
			 train-loss:  2.190179811653338 	 ± 0.22659387279833307
	data : 0.11481218338012696
	model : 0.06501483917236328
			 train-loss:  2.1882334373384245 	 ± 0.22758680366041423
	data : 0.11489238739013671
	model : 0.06505398750305176
			 train-loss:  2.188666150594751 	 ± 0.22707211770278113
	data : 0.11491222381591797
	model : 0.06502838134765625
			 train-loss:  2.1881044102456286 	 ± 0.22661679848419924
	data : 0.11500363349914551
	model : 0.06506514549255371
			 train-loss:  2.18910817571522 	 ± 0.22646172342045465
	data : 0.115089750289917
	model : 0.0650832176208496
			 train-loss:  2.187720466882755 	 ± 0.22670576827877106
	data : 0.11484293937683106
	model : 0.0650932788848877
			 train-loss:  2.188372795070921 	 ± 0.22631010175725644
	data : 0.11469745635986328
	model : 0.06509060859680176
			 train-loss:  2.188109944314521 	 ± 0.22576497264444634
	data : 0.11453390121459961
	model : 0.06505870819091797
			 train-loss:  2.1887037133929703 	 ± 0.22534829527611788
	data : 0.11454129219055176
	model : 0.06504302024841309
			 train-loss:  2.188537380204129 	 ± 0.22479356529810024
	data : 0.11454391479492188
	model : 0.06506214141845704
			 train-loss:  2.188586115241051 	 ± 0.224231931067132
	data : 0.1147873878479004
	model : 0.06503639221191407
			 train-loss:  2.18649478872024 	 ± 0.22562034301924172
	data : 0.11494679450988769
	model : 0.06503667831420898
			 train-loss:  2.1871240144909017 	 ± 0.2252379130032407
	data : 0.11507201194763184
	model : 0.06504883766174316
			 train-loss:  2.1873599302592535 	 ± 0.2247074723009132
	data : 0.11506719589233398
	model : 0.06506881713867188
			 train-loss:  2.188127182862338 	 ± 0.2244224421630664
	data : 0.1151728630065918
	model : 0.06505422592163086
			 train-loss:  2.190952455125204 	 ± 0.22748210662582746
	data : 0.11506376266479493
	model : 0.0650099277496338
			 train-loss:  2.189186679506765 	 ± 0.22833328042077547
	data : 0.11493244171142578
	model : 0.06490592956542969
			 train-loss:  2.188533960333193 	 ± 0.22797365310755247
	data : 0.1148791790008545
	model : 0.06491165161132813
			 train-loss:  2.186884407240611 	 ± 0.22865995156634547
	data : 0.11504364013671875
	model : 0.06489062309265137
			 train-loss:  2.1854392866198524 	 ± 0.22906240614719264
	data : 0.11502466201782227
	model : 0.06493649482727051
			 train-loss:  2.1841071049372354 	 ± 0.22932650243345476
	data : 0.11514873504638672
	model : 0.06497611999511718
			 train-loss:  2.183433459268362 	 ± 0.22899060600595225
	data : 0.11515302658081054
	model : 0.0650289535522461
			 train-loss:  2.183867536625772 	 ± 0.22853689420025702
	data : 0.11527018547058106
	model : 0.06503524780273437
			 train-loss:  2.1835842266888688 	 ± 0.2280371044158637
	data : 0.11513848304748535
	model : 0.06503582000732422
			 train-loss:  2.1838326320469936 	 ± 0.22753256743760206
	data : 0.11501002311706543
	model : 0.0649521827697754
			 train-loss:  2.183278660441554 	 ± 0.22714741219156054
	data : 0.11481609344482421
	model : 0.06498951911926269
			 train-loss:  2.186406862956506 	 ± 0.23121633095539637
	data : 0.11473550796508789
	model : 0.06497888565063477
			 train-loss:  2.187918590510496 	 ± 0.2317504193007655
	data : 0.11458592414855957
	model : 0.06504597663879394
			 train-loss:  2.186670589884487 	 ± 0.23194798426043098
	data : 0.11462759971618652
	model : 0.06516704559326172
			 train-loss:  2.1865363404086735 	 ± 0.23142630555844662
	data : 0.11460943222045898
	model : 0.0651883602142334
			 train-loss:  2.1859969626773488 	 ± 0.2310376639046063
	data : 0.1147000789642334
	model : 0.06510257720947266
			 train-loss:  2.1842308351896467 	 ± 0.23199805235756502
	data : 0.11484279632568359
	model : 0.06509242057800294
			 train-loss:  2.1851773439226925 	 ± 0.23190221889569493
	data : 0.11500797271728516
	model : 0.06495633125305175
			 train-loss:  2.184812811458057 	 ± 0.23144541374106237
	data : 0.11517095565795898
	model : 0.06472644805908204
			 train-loss:  2.183789980198656 	 ± 0.23143279850606172
	data : 0.11527299880981445
	model : 0.06458983421325684
			 train-loss:  2.183702857759264 	 ± 0.23092161211934015
	data : 0.1151921272277832
	model : 0.06441388130187989
			 train-loss:  2.1834652698145502 	 ± 0.2304377168531017
	data : 0.11533002853393555
	model : 0.06420807838439942
			 train-loss:  2.1847245756224916 	 ± 0.23070764227955615
	data : 0.11560373306274414
	model : 0.06403589248657227
			 train-loss:  2.183645854393641 	 ± 0.2307741643486629
	data : 0.11566100120544434
	model : 0.06393766403198242
			 train-loss:  2.182820921381488 	 ± 0.2306063966092117
	data : 0.11570205688476562
	model : 0.06387476921081543
			 train-loss:  2.182840665008711 	 ± 0.23010472622650444
	data : 0.1158750057220459
	model : 0.06387944221496582
			 train-loss:  2.1839627337146115 	 ± 0.2302358592642212
	data : 0.11588630676269532
	model : 0.06385717391967774
			 train-loss:  2.18555171171139 	 ± 0.23100499205890315
	data : 0.11561827659606934
	model : 0.06384196281433105
			 train-loss:  2.1847043134623845 	 ± 0.2308698220427885
	data : 0.11540989875793457
	model : 0.06384091377258301
			 train-loss:  2.184226255641024 	 ± 0.23049152450035912
	data : 0.11547322273254394
	model : 0.06384162902832032
			 train-loss:  2.1847946496720008 	 ± 0.2301648802871718
	data : 0.11559762954711914
	model : 0.06383967399597168
			 train-loss:  2.187348859289945 	 ± 0.23299041820367894
	data : 0.11561388969421386
	model : 0.06385765075683594
			 train-loss:  2.1897373677306033 	 ± 0.2353759967174316
	data : 0.11564803123474121
	model : 0.06387605667114257
			 train-loss:  2.19072314420668 	 ± 0.2353707392686529
	data : 0.1157651424407959
	model : 0.06385178565979004
			 train-loss:  2.1902858348072325 	 ± 0.2349746865470778
	data : 0.11578302383422852
	model : 0.06384072303771973
			 train-loss:  2.190402379135291 	 ± 0.23449156692183148
	data : 0.11563277244567871
	model : 0.06381349563598633
			 train-loss:  2.1906171631516265 	 ± 0.23402822004813637
	data : 0.11555628776550293
	model : 0.06380605697631836
			 train-loss:  2.1900117806166657 	 ± 0.2337332076316422
	data : 0.11563529968261718
	model : 0.06379575729370117
			 train-loss:  2.1898803303761736 	 ± 0.23326074274076505
	data : 0.11569981575012207
	model : 0.06382012367248535
			 train-loss:  2.19141478763252 	 ± 0.23400798662053646
	data : 0.11566858291625977
	model : 0.06381049156188964
			 train-loss:  2.191388267886882 	 ± 0.23353029840420358
	data : 0.115635347366333
	model : 0.06383142471313477
			 train-loss:  2.1925894366047247 	 ± 0.2338123074807187
	data : 0.11563196182250976
	model : 0.06384563446044922
			 train-loss:  2.1928772361654985 	 ± 0.23338218063229324
	data : 0.11541318893432617
	model : 0.0638613224029541
			 train-loss:  2.195610013219618 	 ± 0.2368379818684528
	data : 0.11526813507080078
	model : 0.06380653381347656
			 train-loss:  2.1951310294699 	 ± 0.23648225549534063
	data : 0.11529541015625
	model : 0.06380705833435059
			 train-loss:  2.193817922115326 	 ± 0.23691665246609683
	data : 0.11548514366149902
	model : 0.0638197898864746
			 train-loss:  2.1942192999965164 	 ± 0.2365293909005453
	data : 0.11547718048095704
	model : 0.063820219039917
			 train-loss:  2.1950725473108745 	 ± 0.23644635753787718
	data : 0.11567807197570801
	model : 0.06381769180297851
			 train-loss:  2.19435272433541 	 ± 0.236255109472609
	data : 0.11580595970153809
	model : 0.06383495330810547
			 train-loss:  2.1969353612952345 	 ± 0.23934126326440974
	data : 0.11572017669677734
	model : 0.06382322311401367
			 train-loss:  2.196446063004288 	 ± 0.238998759572184
	data : 0.11560044288635254
	model : 0.06377649307250977
			 train-loss:  2.1979342489503324 	 ± 0.23971238667713635
	data : 0.11542668342590331
	model : 0.055348539352416994
#epoch  19    val-loss:  2.412065317756251  train-loss:  2.1979342489503324  lr:  0.0003125
			 train-loss:  2.1417958736419678 	 ± 0.0
	data : 5.448021411895752
	model : 0.07452750205993652
			 train-loss:  2.216456651687622 	 ± 0.0746607780456543
	data : 2.7830270528793335
	model : 0.07026755809783936
			 train-loss:  2.3333964347839355 	 ± 0.17625544037035373
	data : 1.894009272257487
	model : 0.06837399800618489
			 train-loss:  2.304569959640503 	 ± 0.1606000691241922
	data : 1.4489935636520386
	model : 0.06738138198852539
			 train-loss:  2.276242256164551 	 ± 0.1544141861975192
	data : 1.1820521831512452
	model : 0.06682305335998535
			 train-loss:  2.2433656454086304 	 ± 0.15897843173373116
	data : 0.11538228988647461
	model : 0.06487159729003907
			 train-loss:  2.205469472067697 	 ± 0.17401226910862003
	data : 0.11476445198059082
	model : 0.06463155746459961
			 train-loss:  2.19858780503273 	 ± 0.1637886976062465
	data : 0.1145510196685791
	model : 0.064689302444458
			 train-loss:  2.1874627802107067 	 ± 0.15759479139493907
	data : 0.11463990211486816
	model : 0.06479277610778808
			 train-loss:  2.160881316661835 	 ± 0.1694451955262619
	data : 0.11476263999938965
	model : 0.06486682891845703
			 train-loss:  2.188363476233049 	 ± 0.18345084634857067
	data : 0.11458659172058105
	model : 0.06490797996520996
			 train-loss:  2.1846157610416412 	 ± 0.1760800793234562
	data : 0.11433019638061523
	model : 0.06489810943603516
			 train-loss:  2.206905667598431 	 ± 0.1859605751780324
	data : 0.11421408653259277
	model : 0.06488966941833496
			 train-loss:  2.200888523033687 	 ± 0.1805046218230696
	data : 0.11434421539306641
	model : 0.0649867057800293
			 train-loss:  2.187243866920471 	 ± 0.18170377972346718
	data : 0.11421117782592774
	model : 0.06502823829650879
			 train-loss:  2.2166314646601677 	 ± 0.2095404748648465
	data : 0.11450662612915039
	model : 0.06501274108886719
			 train-loss:  2.232113508617177 	 ± 0.21250772332663231
	data : 0.1146735668182373
	model : 0.06506452560424805
			 train-loss:  2.2451068229145474 	 ± 0.21335584181735023
	data : 0.11473770141601562
	model : 0.06508374214172363
			 train-loss:  2.231552945940118 	 ± 0.21547998649179845
	data : 0.1144683837890625
	model : 0.06500625610351562
			 train-loss:  2.2207183063030245 	 ± 0.2152683029496641
	data : 0.11452455520629883
	model : 0.06498265266418457
			 train-loss:  2.217582651547023 	 ± 0.21054785950796223
	data : 0.11440982818603515
	model : 0.06496667861938477
			 train-loss:  2.2397308620539578 	 ± 0.22938350139328603
	data : 0.11444754600524902
	model : 0.06498703956604004
			 train-loss:  2.2399076430693916 	 ± 0.22434302238397205
	data : 0.11449074745178223
	model : 0.06557550430297851
			 train-loss:  2.233160341779391 	 ± 0.22199057808065065
	data : 0.11461091041564941
	model : 0.06560988426208496
			 train-loss:  2.242856335639954 	 ± 0.2226318018904214
	data : 0.11462011337280273
	model : 0.0655940055847168
			 train-loss:  2.239530522089738 	 ± 0.21894086430036958
	data : 0.11451091766357421
	model : 0.06562986373901367
			 train-loss:  2.2300449035785816 	 ± 0.2202251775620117
	data : 0.1143906593322754
	model : 0.06558742523193359
			 train-loss:  2.218180992773601 	 ± 0.22487181089923702
	data : 0.11430521011352539
	model : 0.06500482559204102
			 train-loss:  2.215807220031475 	 ± 0.22131742943877636
	data : 0.11435880661010742
	model : 0.06497130393981934
			 train-loss:  2.230693511168162 	 ± 0.2318946729400828
	data : 0.11453161239624024
	model : 0.06496176719665528
			 train-loss:  2.221133251344004 	 ± 0.23405643435082413
	data : 0.11464443206787109
	model : 0.06504478454589843
			 train-loss:  2.2248697616159916 	 ± 0.23130774044226762
	data : 0.11475410461425781
	model : 0.06507091522216797
			 train-loss:  2.227513338580276 	 ± 0.2282664923413139
	data : 0.11479668617248535
	model : 0.06515026092529297
			 train-loss:  2.2411042907658745 	 ± 0.23805174964965012
	data : 0.11481690406799316
	model : 0.06511363983154297
			 train-loss:  2.238457689966474 	 ± 0.23513333344384957
	data : 0.1145263671875
	model : 0.06509232521057129
			 train-loss:  2.236266165971756 	 ± 0.2322068317089467
	data : 0.11444544792175293
	model : 0.06499886512756348
			 train-loss:  2.235434090768969 	 ± 0.22910181060202434
	data : 0.11453304290771485
	model : 0.06498551368713379
			 train-loss:  2.2342699923013387 	 ± 0.22617808377659457
	data : 0.11453452110290527
	model : 0.06490087509155273
			 train-loss:  2.2320756637133083 	 ± 0.22366893561447723
	data : 0.11447334289550781
	model : 0.06497011184692383
			 train-loss:  2.23318432867527 	 ± 0.22096387560381384
	data : 0.11458101272583007
	model : 0.06502008438110352
			 train-loss:  2.2302478610015495 	 ± 0.21904130551600268
	data : 0.11469988822937012
	model : 0.0650069236755371
			 train-loss:  2.2341431634766713 	 ± 0.21785050559698044
	data : 0.11455240249633789
	model : 0.06496410369873047
			 train-loss:  2.2428777855496076 	 ± 0.2226195879214343
	data : 0.11447081565856934
	model : 0.06495394706726074
			 train-loss:  2.245246797800064 	 ± 0.22062287847346185
	data : 0.1145021915435791
	model : 0.06488924026489258
			 train-loss:  2.2393147812949286 	 ± 0.22167793763317262
	data : 0.11454010009765625
	model : 0.06485571861267089
			 train-loss:  2.236753865428593 	 ± 0.21992713822855392
	data : 0.11453309059143066
	model : 0.06490955352783204
			 train-loss:  2.2372031998127064 	 ± 0.21759625078683692
	data : 0.11458172798156738
	model : 0.06498541831970214
			 train-loss:  2.237398646771908 	 ± 0.21532186231982886
	data : 0.1147165298461914
	model : 0.06499457359313965
			 train-loss:  2.24075135649467 	 ± 0.21437551685060197
	data : 0.11472868919372559
	model : 0.06500482559204102
			 train-loss:  2.2348882436752318 	 ± 0.21615307882425122
	data : 0.11475491523742676
	model : 0.0649940013885498
			 train-loss:  2.239220558428297 	 ± 0.21620471869182584
	data : 0.11470532417297363
	model : 0.06493644714355469
			 train-loss:  2.2310168124162235 	 ± 0.22198631027539528
	data : 0.1147639274597168
	model : 0.06494064331054687
			 train-loss:  2.2350692816500395 	 ± 0.22181550936703948
	data : 0.11484789848327637
	model : 0.06491913795471191
			 train-loss:  2.233811071625462 	 ± 0.21994288741047935
	data : 0.1148331642150879
	model : 0.06496024131774902
			 train-loss:  2.232923059030013 	 ± 0.2180319085543013
	data : 0.11473875045776367
	model : 0.06501669883728027
			 train-loss:  2.2305460465805873 	 ± 0.21679433154900465
	data : 0.11468186378479003
	model : 0.06510124206542969
			 train-loss:  2.2313398582893504 	 ± 0.21496630499473665
	data : 0.11471166610717773
	model : 0.06505746841430664
			 train-loss:  2.227531110418254 	 ± 0.21503639983126174
	data : 0.11443047523498535
	model : 0.06509017944335938
			 train-loss:  2.224807224031222 	 ± 0.21421308941244713
	data : 0.11447138786315918
	model : 0.06507678031921386
			 train-loss:  2.2267832418282825 	 ± 0.21296204907938027
	data : 0.11455259323120118
	model : 0.06503171920776367
			 train-loss:  2.2255108766868466 	 ± 0.2114390693405627
	data : 0.11475391387939453
	model : 0.06499905586242676
			 train-loss:  2.2233858704566956 	 ± 0.2103826573769913
	data : 0.11469516754150391
	model : 0.06502289772033691
			 train-loss:  2.2298368631847323 	 ± 0.21479864570470864
	data : 0.11474041938781739
	model : 0.06500964164733887
			 train-loss:  2.2290042880922556 	 ± 0.21321635774064332
	data : 0.11481056213378907
	model : 0.06497073173522949
			 train-loss:  2.2328596426890446 	 ± 0.21380620308489112
	data : 0.11473379135131836
	model : 0.06499261856079101
			 train-loss:  2.235319005720543 	 ± 0.21310471648494003
	data : 0.11452765464782715
	model : 0.0649496078491211
			 train-loss:  2.234489001444916 	 ± 0.21161586147989261
	data : 0.11451606750488282
	model : 0.06491827964782715
			 train-loss:  2.2292368306833157 	 ± 0.21440834789375593
	data : 0.11456551551818847
	model : 0.06494307518005371
			 train-loss:  2.2322886473890664 	 ± 0.21433156076131268
	data : 0.11452083587646485
	model : 0.064933443069458
			 train-loss:  2.227501206738608 	 ± 0.21647912626855054
	data : 0.11472487449645996
	model : 0.06496520042419433
			 train-loss:  2.2239066849292164 	 ± 0.2170428712528535
	data : 0.11479048728942871
	model : 0.06505475044250489
			 train-loss:  2.2271603412098355 	 ± 0.21726702798985129
	data : 0.11466245651245117
	model : 0.06508193016052247
			 train-loss:  2.2257958470958554 	 ± 0.21608417555540174
	data : 0.11468973159790039
	model : 0.06506166458129883
			 train-loss:  2.223187804222107 	 ± 0.2157728683988423
	data : 0.11467742919921875
	model : 0.06509027481079102
			 train-loss:  2.224869829813639 	 ± 0.21481741054658204
	data : 0.11441173553466796
	model : 0.06497335433959961
			 train-loss:  2.222561726444646 	 ± 0.2143335704226307
	data : 0.11444954872131348
	model : 0.0648890495300293
			 train-loss:  2.2230129303870263 	 ± 0.21297357358701552
	data : 0.11462950706481934
	model : 0.06487822532653809
			 train-loss:  2.2180706675236044 	 ± 0.21600239028873536
	data : 0.1146895408630371
	model : 0.06492819786071777
			 train-loss:  2.215939146053942 	 ± 0.2154549168514078
	data : 0.1146322250366211
	model : 0.06494622230529785
			 train-loss:  2.2126522779464723 	 ± 0.2160880320263485
	data : 0.11485753059387208
	model : 0.06505885124206542
			 train-loss:  2.2157042497470054 	 ± 0.21647801397579663
	data : 0.11485810279846191
	model : 0.06505465507507324
			 train-loss:  2.2199428459493125 	 ± 0.21850962299205048
	data : 0.11479120254516602
	model : 0.06502423286437989
			 train-loss:  2.2151194566703705 	 ± 0.22153765761428396
	data : 0.1147456169128418
	model : 0.06498684883117675
			 train-loss:  2.2155034371784756 	 ± 0.2202428167410575
	data : 0.1147834300994873
	model : 0.06494636535644531
			 train-loss:  2.2134200460770552 	 ± 0.2197745037223567
	data : 0.11466875076293945
	model : 0.0649263858795166
			 train-loss:  2.210400146107341 	 ± 0.22025979557027345
	data : 0.11470880508422851
	model : 0.06493768692016602
			 train-loss:  2.208018017911363 	 ± 0.22010168333971272
	data : 0.11466441154479981
	model : 0.06501908302307129
			 train-loss:  2.2107632891698317 	 ± 0.2203404620521757
	data : 0.11465611457824706
	model : 0.06502003669738769
			 train-loss:  2.2117153017708424 	 ± 0.21928103285600722
	data : 0.11459026336669922
	model : 0.06505856513977051
			 train-loss:  2.2146690686543784 	 ± 0.2198326739886894
	data : 0.11468377113342285
	model : 0.06505293846130371
			 train-loss:  2.213527139726576 	 ± 0.2188897103867118
	data : 0.11450119018554687
	model : 0.06508007049560546
			 train-loss:  2.2138875904290574 	 ± 0.21772399543125
	data : 0.114483642578125
	model : 0.06500887870788574
			 train-loss:  2.213553438904465 	 ± 0.21657398975372247
	data : 0.11442198753356933
	model : 0.06501083374023438
			 train-loss:  2.211578437622557 	 ± 0.21625926521868669
	data : 0.11445159912109375
	model : 0.06501636505126954
			 train-loss:  2.2111269197965924 	 ± 0.21516258491209667
	data : 0.1142796516418457
	model : 0.06504592895507813
			 train-loss:  2.211417051653067 	 ± 0.21405769263462734
	data : 0.11436676979064941
	model : 0.06505203247070312
			 train-loss:  2.2116603384312894 	 ± 0.21296478479060998
	data : 0.11447749137878419
	model : 0.06506662368774414
			 train-loss:  2.2104697154492747 	 ± 0.21219969181480508
	data : 0.11463942527770996
	model : 0.06508836746215821
			 train-loss:  2.2142645204910125 	 ± 0.2144414306976152
	data : 0.11467061042785645
	model : 0.06505880355834961
			 train-loss:  2.221021637916565 	 ± 0.2237084729776938
	data : 0.11459860801696778
	model : 0.0649759292602539
			 train-loss:  2.220805272017375 	 ± 0.22260876553166498
	data : 0.11469788551330566
	model : 0.06493849754333496
			 train-loss:  2.223708571172228 	 ± 0.22342823503609513
	data : 0.11478672027587891
	model : 0.06491479873657227
			 train-loss:  2.220140746496256 	 ± 0.22524189295968994
	data : 0.11476383209228516
	model : 0.06490511894226074
			 train-loss:  2.223076192232279 	 ± 0.22612743967724155
	data : 0.11473207473754883
	model : 0.06491103172302246
			 train-loss:  2.2238266649700345 	 ± 0.22517816478942995
	data : 0.11489901542663575
	model : 0.06496620178222656
			 train-loss:  2.22119169977476 	 ± 0.2257340786519193
	data : 0.11494269371032714
	model : 0.06496105194091797
			 train-loss:  2.222032568165075 	 ± 0.22484349993660607
	data : 0.11475377082824707
	model : 0.06499242782592773
			 train-loss:  2.2213384498048714 	 ± 0.22391528298095859
	data : 0.11453170776367187
	model : 0.06493029594421387
			 train-loss:  2.2233340248055415 	 ± 0.2238485253940407
	data : 0.11458258628845215
	model : 0.06495318412780762
			 train-loss:  2.2208858154036784 	 ± 0.22428987953924379
	data : 0.11472644805908203
	model : 0.06496338844299317
			 train-loss:  2.2210430465303026 	 ± 0.223283368582139
	data : 0.11474332809448243
	model : 0.06498346328735352
			 train-loss:  2.2178021179778233 	 ± 0.22489158768310744
	data : 0.1148214340209961
	model : 0.06502127647399902
			 train-loss:  2.215629400405209 	 ± 0.225071916445311
	data : 0.11499404907226562
	model : 0.06508402824401856
			 train-loss:  2.2229810367550766 	 ± 0.2373189112613287
	data : 0.1150576114654541
	model : 0.06503753662109375
			 train-loss:  2.2230545147605563 	 ± 0.23628613943662857
	data : 0.1148451805114746
	model : 0.0650108814239502
			 train-loss:  2.223724774245558 	 ± 0.2353752328854652
	data : 0.11463356018066406
	model : 0.06501626968383789
			 train-loss:  2.2218192520304623 	 ± 0.2352640662946655
	data : 0.11463680267333984
	model : 0.06496939659118653
			 train-loss:  2.219042443622977 	 ± 0.236182700110603
	data : 0.1146932601928711
	model : 0.0649611473083496
			 train-loss:  2.221288587866711 	 ± 0.2364504993144245
	data : 0.11470680236816407
	model : 0.06498732566833496
			 train-loss:  2.2231604009866714 	 ± 0.23634692736708252
	data : 0.11486725807189942
	model : 0.06499838829040527
			 train-loss:  2.2221710140054878 	 ± 0.23561766624953345
	data : 0.11493244171142578
	model : 0.06502714157104492
			 train-loss:  2.220427691936493 	 ± 0.2354323211574389
	data : 0.11496057510375976
	model : 0.06503767967224121
			 train-loss:  2.2220878843369523 	 ± 0.23518928921556131
	data : 0.11484026908874512
	model : 0.06503071784973144
			 train-loss:  2.223661073753911 	 ± 0.23488792406203585
	data : 0.11483502388000488
	model : 0.06501269340515137
			 train-loss:  2.2209163265228273 	 ± 0.23593458671455184
	data : 0.11465973854064941
	model : 0.06503119468688964
			 train-loss:  2.2209418217341104 	 ± 0.2349966461870123
	data : 0.1147038459777832
	model : 0.0649566650390625
			 train-loss:  2.218289592134671 	 ± 0.2359553308657915
	data : 0.11468386650085449
	model : 0.06499404907226562
			 train-loss:  2.2187805799767375 	 ± 0.23509694518768903
	data : 0.11483960151672364
	model : 0.06502623558044433
			 train-loss:  2.2176488804262737 	 ± 0.2345336964981098
	data : 0.11480951309204102
	model : 0.06508054733276367
			 train-loss:  2.216991574030656 	 ± 0.2337491517923972
	data : 0.11486921310424805
	model : 0.06508712768554688
			 train-loss:  2.217249186894366 	 ± 0.23287379474919184
	data : 0.1148463249206543
	model : 0.06515512466430665
			 train-loss:  2.2174432647950724 	 ± 0.23200065447387205
	data : 0.11485056877136231
	model : 0.06511621475219727
			 train-loss:  2.2206533950074276 	 ± 0.2340509799447498
	data : 0.11474523544311524
	model : 0.06506056785583496
			 train-loss:  2.2202523119414033 	 ± 0.23322189363974774
	data : 0.11463909149169922
	model : 0.06502408981323242
			 train-loss:  2.2189093669255575 	 ± 0.23287596226268012
	data : 0.11467995643615722
	model : 0.06495537757873535
			 train-loss:  2.21914783383117 	 ± 0.23203476430818654
	data : 0.11466388702392578
	model : 0.06494612693786621
			 train-loss:  2.2181858207187513 	 ± 0.23145842391130667
	data : 0.11467089653015136
	model : 0.06492066383361816
			 train-loss:  2.2186133455539094 	 ± 0.23067256524235857
	data : 0.11488394737243653
	model : 0.06496386528015137
			 train-loss:  2.216815702349162 	 ± 0.23080939694216843
	data : 0.11493844985961914
	model : 0.0650139331817627
			 train-loss:  2.2169956675597597 	 ± 0.22999338752822496
	data : 0.11494030952453613
	model : 0.06507091522216797
			 train-loss:  2.21791887536962 	 ± 0.22943654100091088
	data : 0.11493887901306152
	model : 0.06503853797912598
			 train-loss:  2.216799085408869 	 ± 0.22901357657684224
	data : 0.11485438346862793
	model : 0.06505284309387208
			 train-loss:  2.2152875178343767 	 ± 0.22892116826555386
	data : 0.11461706161499023
	model : 0.0650439739227295
			 train-loss:  2.2143202763464718 	 ± 0.22841795655382977
	data : 0.11477503776550294
	model : 0.0650254726409912
			 train-loss:  2.213105636629565 	 ± 0.22809512768533313
	data : 0.11471309661865234
	model : 0.06502795219421387
			 train-loss:  2.2124332491665673 	 ± 0.22745678824057391
	data : 0.11481771469116211
	model : 0.06505813598632812
			 train-loss:  2.2117395765927372 	 ± 0.22683671096354588
	data : 0.1148146629333496
	model : 0.0651209831237793
			 train-loss:  2.2100327369329094 	 ± 0.22701427395786475
	data : 0.11478581428527831
	model : 0.06515870094299317
			 train-loss:  2.2105533292629573 	 ± 0.22633982260174473
	data : 0.11458544731140137
	model : 0.06515450477600097
			 train-loss:  2.2109305540720623 	 ± 0.22563108461606235
	data : 0.11466617584228515
	model : 0.0651090145111084
			 train-loss:  2.210900347753866 	 ± 0.22488302504708754
	data : 0.11459946632385254
	model : 0.06507306098937989
			 train-loss:  2.2125665341552936 	 ± 0.2250752423234908
	data : 0.11508469581604004
	model : 0.06498427391052246
			 train-loss:  2.2127691234638487 	 ± 0.22435240017688654
	data : 0.11503353118896484
	model : 0.06495108604431152
			 train-loss:  2.214073278687217 	 ± 0.22420388244250936
	data : 0.1150855541229248
	model : 0.06491594314575196
			 train-loss:  2.2142992634927072 	 ± 0.22349706903096087
	data : 0.11515679359436035
	model : 0.06498942375183106
			 train-loss:  2.21630769662368 	 ± 0.22417845814753384
	data : 0.11537714004516601
	model : 0.06500000953674316
			 train-loss:  2.2145354383310694 	 ± 0.2245570297765572
	data : 0.1150125503540039
	model : 0.06509923934936523
			 train-loss:  2.215019028398055 	 ± 0.223927275092175
	data : 0.11517519950866699
	model : 0.06508851051330566
			 train-loss:  2.2150739663801855 	 ± 0.2232230587396119
	data : 0.11510958671569824
	model : 0.06513619422912598
			 train-loss:  2.212853766232729 	 ± 0.22427853780138932
	data : 0.11505570411682128
	model : 0.06514277458190917
			 train-loss:  2.2114602097813387 	 ± 0.22427473048504123
	data : 0.11493687629699707
	model : 0.06514735221862793
			 train-loss:  2.2146362625522378 	 ± 0.22718433755089099
	data : 0.11499567031860351
	model : 0.06507091522216797
			 train-loss:  2.2143173012996744 	 ± 0.22652276272398375
	data : 0.11467595100402832
	model : 0.06507072448730469
			 train-loss:  2.2135202230476754 	 ± 0.22606025735606372
	data : 0.11476950645446778
	model : 0.06503276824951172
			 train-loss:  2.2157660990050343 	 ± 0.22720196433106607
	data : 0.11475372314453125
	model : 0.06494264602661133
			 train-loss:  2.215428254690515 	 ± 0.22655815418140615
	data : 0.11475067138671875
	model : 0.06497054100036621
			 train-loss:  2.2168005369380563 	 ± 0.2265697346923828
	data : 0.11475181579589844
	model : 0.06500048637390136
			 train-loss:  2.2174063310736702 	 ± 0.22603002639163183
	data : 0.11498665809631348
	model : 0.06501426696777343
			 train-loss:  2.2167177073348907 	 ± 0.2255369896751533
	data : 0.11490802764892578
	model : 0.06505389213562011
			 train-loss:  2.2148548553971685 	 ± 0.22617290550678154
	data : 0.11503024101257324
	model : 0.06509785652160645
			 train-loss:  2.217682781972383 	 ± 0.2285050428257694
	data : 0.11486177444458008
	model : 0.06506829261779785
			 train-loss:  2.2166495427142743 	 ± 0.22824008929259973
	data : 0.11458287239074708
	model : 0.06500787734985351
			 train-loss:  2.2163268313931592 	 ± 0.22761883087322476
	data : 0.11478686332702637
	model : 0.06500697135925293
			 train-loss:  2.2163561849758544 	 ± 0.22696413968041154
	data : 0.11499605178833008
	model : 0.0649801254272461
			 train-loss:  2.2165458018439157 	 ± 0.22632856291180325
	data : 0.11492791175842285
	model : 0.06496353149414062
			 train-loss:  2.2160804251378234 	 ± 0.22576862078320142
	data : 0.1149928092956543
	model : 0.06498098373413086
			 train-loss:  2.2194208364702215 	 ± 0.22945013608164397
	data : 0.11510848999023438
	model : 0.06502723693847656
			 train-loss:  2.219559603192833 	 ± 0.22881215348551626
	data : 0.11501126289367676
	model : 0.06501588821411133
			 train-loss:  2.2191360749345916 	 ± 0.22824207434507393
	data : 0.11494932174682618
	model : 0.06498327255249023
			 train-loss:  2.219648889038298 	 ± 0.22771057076334214
	data : 0.11485400199890136
	model : 0.06498637199401855
			 train-loss:  2.217947722798553 	 ± 0.22822476269677228
	data : 0.11465234756469726
	model : 0.06494789123535157
			 train-loss:  2.2171359114594513 	 ± 0.2278588122715472
	data : 0.11477780342102051
	model : 0.06497492790222167
			 train-loss:  2.2176627187780995 	 ± 0.22734650686171576
	data : 0.11488981246948242
	model : 0.06494956016540528
			 train-loss:  2.21674236136934 	 ± 0.22706946342183296
	data : 0.11495943069458008
	model : 0.06495671272277832
			 train-loss:  2.2183277490976696 	 ± 0.2274737593049167
	data : 0.1149836540222168
	model : 0.0649904727935791
			 train-loss:  2.2177551202876593 	 ± 0.22699510599450065
	data : 0.1151301383972168
	model : 0.06504621505737304
			 train-loss:  2.2163412022718134 	 ± 0.22720712667129603
	data : 0.11501541137695312
	model : 0.06509385108947754
			 train-loss:  2.2158406161247415 	 ± 0.2267054195235934
	data : 0.11483731269836425
	model : 0.06511192321777344
			 train-loss:  2.213746361000828 	 ± 0.22792096050539218
	data : 0.1147496223449707
	model : 0.06511621475219727
			 train-loss:  2.2124716432471025 	 ± 0.22799487058061574
	data : 0.11463594436645508
	model : 0.06506328582763672
			 train-loss:  2.2132228631623754 	 ± 0.22763288136038146
	data : 0.11464366912841797
	model : 0.06503338813781738
			 train-loss:  2.2135685669879117 	 ± 0.2270895781580982
	data : 0.114784574508667
	model : 0.06494755744934082
			 train-loss:  2.2139153282877078 	 ± 0.2265514573871095
	data : 0.11498351097106933
	model : 0.06493611335754394
			 train-loss:  2.214629474374437 	 ± 0.22618450208071303
	data : 0.11497635841369629
	model : 0.06494660377502441
			 train-loss:  2.2150477678347857 	 ± 0.22567901320840936
	data : 0.11500144004821777
	model : 0.06499238014221191
			 train-loss:  2.2124724053606695 	 ± 0.22795723456474942
	data : 0.11505489349365235
	model : 0.06503100395202636
			 train-loss:  2.21119202998689 	 ± 0.22808339784768172
	data : 0.11504063606262208
	model : 0.0650947093963623
			 train-loss:  2.210864746811414 	 ± 0.22755307145556128
	data : 0.11492700576782226
	model : 0.06510653495788574
			 train-loss:  2.2086103375832637 	 ± 0.2291866156296863
	data : 0.11487331390380859
	model : 0.06516499519348144
			 train-loss:  2.2092897623777388 	 ± 0.2288137545080906
	data : 0.11501274108886719
	model : 0.06512603759765626
			 train-loss:  2.207726278115268 	 ± 0.22931235154277316
	data : 0.11492180824279785
	model : 0.06511669158935547
			 train-loss:  2.2080918420659432 	 ± 0.2288027491325302
	data : 0.11502251625061036
	model : 0.06505560874938965
			 train-loss:  2.2056783496452668 	 ± 0.2308017587086165
	data : 0.11511340141296386
	model : 0.06506729125976562
			 train-loss:  2.205087928795347 	 ± 0.23038900191957157
	data : 0.1152735710144043
	model : 0.06505651473999023
			 train-loss:  2.2047696933513734 	 ± 0.22987133283479552
	data : 0.11517744064331055
	model : 0.06509261131286621
			 train-loss:  2.2055569754063504 	 ± 0.22958959569064222
	data : 0.11525840759277343
	model : 0.0650442123413086
			 train-loss:  2.2048828077777 	 ± 0.2292386649323951
	data : 0.11508889198303222
	model : 0.06504173278808593
			 train-loss:  2.2049975504095736 	 ± 0.22869290510949628
	data : 0.11501870155334473
	model : 0.0650186538696289
			 train-loss:  2.20356383335077 	 ± 0.22908024088142837
	data : 0.11471214294433593
	model : 0.06495251655578613
			 train-loss:  2.2015921916280474 	 ± 0.2303048484916766
	data : 0.11477866172790527
	model : 0.06499338150024414
			 train-loss:  2.201897172001301 	 ± 0.22980095742870288
	data : 0.11491985321044922
	model : 0.0649996280670166
			 train-loss:  2.200727294076164 	 ± 0.22988727903281353
	data : 0.11503915786743164
	model : 0.06500978469848633
			 train-loss:  2.2006190640265952 	 ± 0.22935241647418578
	data : 0.11506838798522949
	model : 0.06502389907836914
			 train-loss:  2.2008253870723404 	 ± 0.22883573139398802
	data : 0.11520152091979981
	model : 0.06504664421081544
			 train-loss:  2.200411563695863 	 ± 0.2283831815041119
	data : 0.11522746086120605
	model : 0.06502337455749511
			 train-loss:  2.200280833023566 	 ± 0.227861966480036
	data : 0.11511397361755371
	model : 0.06500554084777832
			 train-loss:  2.200805071861513 	 ± 0.22746685611638232
	data : 0.11506729125976563
	model : 0.06497573852539062
			 train-loss:  2.1982197455309946 	 ± 0.23011787146398402
	data : 0.11491003036499023
	model : 0.06489567756652832
			 train-loss:  2.198890128636469 	 ± 0.22980514953570497
	data : 0.1150545597076416
	model : 0.06495866775512696
			 train-loss:  2.1979776832190425 	 ± 0.22967953621038326
	data : 0.11498627662658692
	model : 0.06490530967712402
			 train-loss:  2.19693625404824 	 ± 0.22967933263553653
	data : 0.11520400047302246
	model : 0.06490440368652343
			 train-loss:  2.1960054700439042 	 ± 0.22957882490112827
	data : 0.1153061866760254
	model : 0.06489753723144531
			 train-loss:  2.1993202286450853 	 ± 0.2343274157368962
	data : 0.11546268463134765
	model : 0.06488089561462403
			 train-loss:  2.1998035428779468 	 ± 0.23391515118563166
	data : 0.11538033485412598
	model : 0.06467170715332031
			 train-loss:  2.19875435034434 	 ± 0.23392241175567335
	data : 0.11558232307434083
	model : 0.0644984245300293
			 train-loss:  2.19796305261882 	 ± 0.23370591862192502
	data : 0.11555972099304199
	model : 0.06430377960205078
			 train-loss:  2.196997493899341 	 ± 0.23364192064277897
	data : 0.11548075675964356
	model : 0.06413798332214356
			 train-loss:  2.1959760628248515 	 ± 0.23363637900891834
	data : 0.11542787551879882
	model : 0.06398992538452149
			 train-loss:  2.196530629453701 	 ± 0.23327604042119365
	data : 0.11548161506652832
	model : 0.06390652656555176
			 train-loss:  2.1964141182277515 	 ± 0.23277504362629503
	data : 0.11561851501464844
	model : 0.06386017799377441
			 train-loss:  2.19643785014297 	 ± 0.23227093392223064
	data : 0.11560621261596679
	model : 0.06388740539550782
			 train-loss:  2.1969387592940497 	 ± 0.2318948140120705
	data : 0.11561951637268067
	model : 0.06386637687683105
			 train-loss:  2.195865281150065 	 ± 0.23197361128153332
	data : 0.11564593315124512
	model : 0.0639266014099121
			 train-loss:  2.1957693986403637 	 ± 0.23148203739279852
	data : 0.11574020385742187
	model : 0.06389551162719727
			 train-loss:  2.196770974423023 	 ± 0.2314965549289541
	data : 0.11551270484924317
	model : 0.06388859748840332
			 train-loss:  2.1978720931683555 	 ± 0.23162146831211997
	data : 0.11548380851745606
	model : 0.06384787559509278
			 train-loss:  2.199393655680403 	 ± 0.2323112471871424
	data : 0.11546311378479004
	model : 0.0638272762298584
			 train-loss:  2.1988825527559808 	 ± 0.23195617627067436
	data : 0.11571121215820312
	model : 0.06376729011535645
			 train-loss:  2.200826970104393 	 ± 0.23340601612177472
	data : 0.11572871208190919
	model : 0.06380343437194824
			 train-loss:  2.200012731552124 	 ± 0.23325914437961653
	data : 0.11574716567993164
	model : 0.06381592750549317
			 train-loss:  2.199513745011136 	 ± 0.23290302387491213
	data : 0.11562471389770508
	model : 0.0638582706451416
			 train-loss:  2.198969526724382 	 ± 0.23257482310783256
	data : 0.11574249267578125
	model : 0.06388607025146484
			 train-loss:  2.1979933501271063 	 ± 0.23259204255122362
	data : 0.11549687385559082
	model : 0.06390905380249023
			 train-loss:  2.198995517902687 	 ± 0.2326400556377227
	data : 0.11546273231506347
	model : 0.0638883113861084
			 train-loss:  2.1973215584852257 	 ± 0.23363264759898636
	data : 0.11552705764770507
	model : 0.06393718719482422
			 train-loss:  2.1962346575124476 	 ± 0.23377715593651785
	data : 0.11566238403320313
	model : 0.06390590667724609
			 train-loss:  2.1957248280405515 	 ± 0.23344043838908696
	data : 0.11562895774841309
	model : 0.06397695541381836
			 train-loss:  2.195263082942655 	 ± 0.23308231409811825
	data : 0.11572465896606446
	model : 0.06398272514343262
			 train-loss:  2.194754221353186 	 ± 0.23275179935946766
	data : 0.11571407318115234
	model : 0.06403021812438965
			 train-loss:  2.1949622468948364 	 ± 0.23230902239585607
	data : 0.11570396423339843
	model : 0.06395506858825684
			 train-loss:  2.1956937617039776 	 ± 0.2321341217432768
	data : 0.11551914215087891
	model : 0.06389479637145996
			 train-loss:  2.196562696070898 	 ± 0.23208173760696604
	data : 0.11540141105651855
	model : 0.06379666328430175
			 train-loss:  2.1973581587373032 	 ± 0.23196658304646114
	data : 0.11541256904602051
	model : 0.06376652717590332
			 train-loss:  2.197774429959575 	 ± 0.23160416977464193
	data : 0.11543507575988769
	model : 0.0637671947479248
			 train-loss:  2.1969879912395105 	 ± 0.23148916172704126
	data : 0.11544227600097656
	model : 0.06382737159729004
			 train-loss:  2.2002964015118778 	 ± 0.23700006201967072
	data : 0.11549172401428223
	model : 0.05550389289855957
#epoch  20    val-loss:  2.436836888915614  train-loss:  2.2002964015118778  lr:  0.0003125
			 train-loss:  2.3994479179382324 	 ± 0.0
	data : 5.0831639766693115
	model : 0.07874512672424316
			 train-loss:  2.262666940689087 	 ± 0.1367809772491455
	data : 2.5985968112945557
	model : 0.07351338863372803
			 train-loss:  2.1899840037027993 	 ± 0.15178375785766002
	data : 1.7706402937571208
	model : 0.07083408037821452
			 train-loss:  2.1331732273101807 	 ± 0.1641984311893417
	data : 1.3565096855163574
	model : 0.0749005675315857
			 train-loss:  2.1507730007171633 	 ± 0.1510228722410658
	data : 1.1081326007843018
	model : 0.07296338081359863
			 train-loss:  2.1584150791168213 	 ± 0.13891938943604557
	data : 0.11434903144836425
	model : 0.07018547058105469
			 train-loss:  2.1836787973131453 	 ± 0.142727667638178
	data : 0.11450095176696777
	model : 0.06947479248046876
			 train-loss:  2.146689862012863 	 ± 0.16553567006326492
	data : 0.11430649757385254
	model : 0.06929802894592285
			 train-loss:  2.1322564019097223 	 ± 0.1613195062896673
	data : 0.11438951492309571
	model : 0.06484932899475097
			 train-loss:  2.131224584579468 	 ± 0.15307242279919306
	data : 0.11436953544616699
	model : 0.06476240158081055
			 train-loss:  2.1938387697393242 	 ± 0.24598053255782776
	data : 0.11444377899169922
	model : 0.0648080825805664
			 train-loss:  2.1727425257364907 	 ± 0.24568228904884012
	data : 0.11439871788024902
	model : 0.06490917205810547
			 train-loss:  2.190761236044077 	 ± 0.24415735429408436
	data : 0.11467566490173339
	model : 0.06500439643859864
			 train-loss:  2.176341244152614 	 ± 0.240952123247664
	data : 0.11465086936950683
	model : 0.06502828598022461
			 train-loss:  2.1678225358327228 	 ± 0.2349539418649358
	data : 0.11472086906433106
	model : 0.06505889892578125
			 train-loss:  2.1577334702014923 	 ± 0.2308245736542664
	data : 0.11461782455444336
	model : 0.06507949829101563
			 train-loss:  2.1593768736895393 	 ± 0.22402919342976557
	data : 0.11455488204956055
	model : 0.06502709388732911
			 train-loss:  2.1506844494077892 	 ± 0.22064742779388047
	data : 0.1144676685333252
	model : 0.06503400802612305
			 train-loss:  2.1434480014600252 	 ± 0.21694583440416187
	data : 0.1144442081451416
	model : 0.06507563591003418
			 train-loss:  2.126624971628189 	 ± 0.22380681990007859
	data : 0.11443266868591309
	model : 0.06514458656311035
			 train-loss:  2.1267109655198597 	 ± 0.2184134304428922
	data : 0.11453466415405274
	model : 0.0651482105255127
			 train-loss:  2.1185380003669043 	 ± 0.216653613212145
	data : 0.11473016738891602
	model : 0.06514406204223633
			 train-loss:  2.128498139588729 	 ± 0.2169803388701065
	data : 0.11472883224487304
	model : 0.06512513160705566
			 train-loss:  2.1327659090360007 	 ± 0.21339564194576455
	data : 0.11467461585998535
	model : 0.06504936218261718
			 train-loss:  2.1301602363586425 	 ± 0.20947348453640585
	data : 0.11462526321411133
	model : 0.06497330665588379
			 train-loss:  2.129674187073341 	 ± 0.20542002717093777
	data : 0.11469087600708008
	model : 0.06492142677307129
			 train-loss:  2.1305086701004594 	 ± 0.20162496563387539
	data : 0.1146024227142334
	model : 0.06496238708496094
			 train-loss:  2.1453697425978526 	 ± 0.21251761594121646
	data : 0.11463694572448731
	model : 0.06495528221130371
			 train-loss:  2.159661786309604 	 ± 0.2220939378232951
	data : 0.11474480628967285
	model : 0.06496386528015137
			 train-loss:  2.1580771764119464 	 ± 0.2185276763049338
	data : 0.11470599174499511
	model : 0.06498456001281738
			 train-loss:  2.152446177697951 	 ± 0.21717533689341734
	data : 0.11458559036254883
	model : 0.06494860649108887
			 train-loss:  2.150405488908291 	 ± 0.2140567996356489
	data : 0.11464223861694336
	model : 0.06492147445678711
			 train-loss:  2.158360885851311 	 ± 0.2155389663595547
	data : 0.11459355354309082
	model : 0.06490325927734375
			 train-loss:  2.1535857179585625 	 ± 0.2141101021676086
	data : 0.11471624374389648
	model : 0.06495137214660644
			 train-loss:  2.157420584133693 	 ± 0.21221060934964656
	data : 0.11485481262207031
	model : 0.06498680114746094
			 train-loss:  2.1559231181939444 	 ± 0.20942994218184557
	data : 0.1149223804473877
	model : 0.06508655548095703
			 train-loss:  2.151922003642933 	 ± 0.2079706517024278
	data : 0.11483941078186036
	model : 0.06514067649841308
			 train-loss:  2.1524180393469963 	 ± 0.20523813198387825
	data : 0.11490435600280761
	model : 0.06515374183654785
			 train-loss:  2.1584281218357577 	 ± 0.20594956494896147
	data : 0.11463966369628906
	model : 0.06512665748596191
			 train-loss:  2.160823878645897 	 ± 0.20390853120244792
	data : 0.11458816528320312
	model : 0.06507816314697265
			 train-loss:  2.1636899418947175 	 ± 0.2022205416637151
	data : 0.11466474533081054
	model : 0.06502723693847656
			 train-loss:  2.1608306765556335 	 ± 0.20063572072755506
	data : 0.11466255187988281
	model : 0.06501607894897461
			 train-loss:  2.163674224254697 	 ± 0.19914351323008928
	data : 0.11461224555969238
	model : 0.06505336761474609
			 train-loss:  2.1663256097923624 	 ± 0.19763375438026853
	data : 0.11475911140441894
	model : 0.06507430076599122
			 train-loss:  2.1675816615422567 	 ± 0.1956030116205457
	data : 0.11482911109924317
	model : 0.06511178016662597
			 train-loss:  2.156539963639301 	 ± 0.20715971252074214
	data : 0.11474838256835937
	model : 0.06514978408813477
			 train-loss:  2.154417469146404 	 ± 0.20544898997215527
	data : 0.11472101211547851
	model : 0.06506896018981934
			 train-loss:  2.149045710762342 	 ± 0.2066062665244212
	data : 0.11467647552490234
	model : 0.06503992080688477
			 train-loss:  2.14825261855612 	 ± 0.20456098169883544
	data : 0.11484837532043457
	model : 0.06503782272338868
			 train-loss:  2.1481902742385866 	 ± 0.20250551049984283
	data : 0.114764404296875
	model : 0.06503114700317383
			 train-loss:  2.151580198138368 	 ± 0.2019380427433704
	data : 0.11478199958801269
	model : 0.06501402854919433
			 train-loss:  2.1541329393020043 	 ± 0.2008160923472247
	data : 0.11475000381469727
	model : 0.06504287719726562
			 train-loss:  2.161734810415304 	 ± 0.2063279266623733
	data : 0.11482076644897461
	model : 0.0650416374206543
			 train-loss:  2.1628935116308705 	 ± 0.20458253764538512
	data : 0.1145869255065918
	model : 0.06502017974853516
			 train-loss:  2.1742615743116898 	 ± 0.21925238077615344
	data : 0.11463022232055664
	model : 0.06493968963623047
			 train-loss:  2.1689137603555406 	 ± 0.22087583889557502
	data : 0.11471672058105468
	model : 0.06491203308105468
			 train-loss:  2.166779643610904 	 ± 0.219511476968828
	data : 0.11492586135864258
	model : 0.06493544578552246
			 train-loss:  2.1693753168500702 	 ± 0.21849152454876725
	data : 0.11499562263488769
	model : 0.06495184898376465
			 train-loss:  2.1651822672051897 	 ± 0.2189729549169893
	data : 0.11512966156005859
	model : 0.06501460075378418
			 train-loss:  2.172733990351359 	 ± 0.22475472237107735
	data : 0.1151669979095459
	model : 0.06508021354675293
			 train-loss:  2.168910794570798 	 ± 0.22486348123425215
	data : 0.1150850772857666
	model : 0.06510362625122071
			 train-loss:  2.168919115297256 	 ± 0.22304270380228278
	data : 0.11510977745056153
	model : 0.06508097648620606
			 train-loss:  2.1779967887060985 	 ± 0.23252409283431705
	data : 0.11505398750305176
	model : 0.06502213478088378
			 train-loss:  2.177603544667363 	 ± 0.2307214601173134
	data : 0.11500105857849122
	model : 0.06495347023010253
			 train-loss:  2.176549759277931 	 ± 0.2290949627397071
	data : 0.11495256423950195
	model : 0.06491646766662598
			 train-loss:  2.179027837334257 	 ± 0.22822891760601388
	data : 0.11500544548034668
	model : 0.06488618850708008
			 train-loss:  2.182590084289437 	 ± 0.22836048972724152
	data : 0.11483349800109863
	model : 0.06490535736083984
			 train-loss:  2.183198748265996 	 ± 0.22672989432471155
	data : 0.11477122306823731
	model : 0.06494174003601075
			 train-loss:  2.178113880364791 	 ± 0.2289533227225413
	data : 0.11482119560241699
	model : 0.06498384475708008
			 train-loss:  2.1769706095967973 	 ± 0.2275103512147205
	data : 0.11480317115783692
	model : 0.06501779556274415
			 train-loss:  2.1796541130039055 	 ± 0.22701545243220486
	data : 0.1146773338317871
	model : 0.0650413990020752
			 train-loss:  2.1803315463993282 	 ± 0.22550569989321478
	data : 0.1147430419921875
	model : 0.06500983238220215
			 train-loss:  2.180018557261114 	 ± 0.22397156065178817
	data : 0.11494307518005371
	model : 0.06503472328186036
			 train-loss:  2.178766519636721 	 ± 0.22271015414369102
	data : 0.11500959396362305
	model : 0.06501450538635253
			 train-loss:  2.1792628622055052 	 ± 0.22126163757393036
	data : 0.11497397422790527
	model : 0.06502394676208496
			 train-loss:  2.1839270293712616 	 ± 0.2234818318041811
	data : 0.1150242805480957
	model : 0.0649794101715088
			 train-loss:  2.180225353736382 	 ± 0.22435883464474138
	data : 0.11488490104675293
	model : 0.06500930786132812
			 train-loss:  2.1816269067617564 	 ± 0.2232550044233421
	data : 0.11470041275024415
	model : 0.06500725746154785
			 train-loss:  2.1783197016655644 	 ± 0.22375211451921218
	data : 0.11457514762878418
	model : 0.06496782302856445
			 train-loss:  2.1809212893247603 	 ± 0.22354840315231708
	data : 0.11463546752929688
	model : 0.06494021415710449
			 train-loss:  2.188434630264471 	 ± 0.2321054724279369
	data : 0.11470565795898438
	model : 0.06494946479797363
			 train-loss:  2.1861689468709433 	 ± 0.23158532369788326
	data : 0.1148404598236084
	model : 0.06495990753173828
			 train-loss:  2.1894818644925773 	 ± 0.23213267982544838
	data : 0.11484909057617188
	model : 0.06494483947753907
			 train-loss:  2.1939031992639815 	 ± 0.23423617723758386
	data : 0.1147953987121582
	model : 0.06498079299926758
			 train-loss:  2.1944858859567082 	 ± 0.2329154727984907
	data : 0.11480727195739746
	model : 0.06497721672058106
			 train-loss:  2.197438034900399 	 ± 0.23315144981594943
	data : 0.11479349136352539
	model : 0.06498422622680664
			 train-loss:  2.1974929749280556 	 ± 0.231808186141897
	data : 0.11476187705993653
	model : 0.06492075920104981
			 train-loss:  2.195117385549979 	 ± 0.23154996810307454
	data : 0.11487045288085937
	model : 0.06489930152893067
			 train-loss:  2.195502269134093 	 ± 0.2302737578504107
	data : 0.11499671936035157
	model : 0.06490135192871094
			 train-loss:  2.1983680420451694 	 ± 0.23058133484067256
	data : 0.11502904891967773
	model : 0.06495685577392578
			 train-loss:  2.199637595113817 	 ± 0.22962697972067936
	data : 0.11496119499206543
	model : 0.0650174617767334
			 train-loss:  2.20281049090883 	 ± 0.23037259986306674
	data : 0.11496958732604981
	model : 0.06507625579833984
			 train-loss:  2.200559422533999 	 ± 0.2301457484197595
	data : 0.11491470336914063
	model : 0.0650907039642334
			 train-loss:  2.1984562303157564 	 ± 0.2298150646138133
	data : 0.11501412391662598
	model : 0.06513190269470215
			 train-loss:  2.199188946422778 	 ± 0.22871266468381496
	data : 0.11483936309814453
	model : 0.06508450508117676
			 train-loss:  2.195592318971952 	 ± 0.23020314245443213
	data : 0.11485028266906738
	model : 0.06502909660339355
			 train-loss:  2.1959575549843384 	 ± 0.22904141183385976
	data : 0.1149317741394043
	model : 0.06500697135925293
			 train-loss:  2.192484147694646 	 ± 0.23042335966850483
	data : 0.1149712085723877
	model : 0.0650825023651123
			 train-loss:  2.1931035253736706 	 ± 0.22933863155586248
	data : 0.11487808227539062
	model : 0.0650674819946289
			 train-loss:  2.192249915599823 	 ± 0.22834706514345945
	data : 0.11497631072998046
	model : 0.06509466171264648
			 train-loss:  2.189439328590242 	 ± 0.2289455413061643
	data : 0.11500582695007325
	model : 0.0651134967803955
			 train-loss:  2.1855172070802427 	 ± 0.23120524363687955
	data : 0.11503806114196777
	model : 0.06514110565185546
			 train-loss:  2.188859987027437 	 ± 0.2325438492859997
	data : 0.11496615409851074
	model : 0.06506237983703614
			 train-loss:  2.190224607403462 	 ± 0.23183718332942363
	data : 0.11485872268676758
	model : 0.0650327205657959
			 train-loss:  2.190497408594404 	 ± 0.23074732722915675
	data : 0.11483888626098633
	model : 0.06502537727355957
			 train-loss:  2.1943001600931273 	 ± 0.23293866257777487
	data : 0.11496658325195312
	model : 0.06498723030090332
			 train-loss:  2.1948794638999156 	 ± 0.2319243123027791
	data : 0.11485471725463867
	model : 0.06501350402832032
			 train-loss:  2.2006661284852913 	 ± 0.23848226035075767
	data : 0.11488480567932129
	model : 0.06503162384033204
			 train-loss:  2.2035057074433073 	 ± 0.23921295358109826
	data : 0.1149289608001709
	model : 0.06504983901977539
			 train-loss:  2.20172864632173 	 ± 0.23884481507767483
	data : 0.11500411033630371
	model : 0.06504106521606445
			 train-loss:  2.201086803599521 	 ± 0.2378617791313736
	data : 0.1148796558380127
	model : 0.06506381034851075
			 train-loss:  2.198628956718104 	 ± 0.23820918431731367
	data : 0.11489496231079102
	model : 0.0650151252746582
			 train-loss:  2.1971777487645108 	 ± 0.23764959943276248
	data : 0.11486635208129883
	model : 0.06504325866699219
			 train-loss:  2.1999082701248036 	 ± 0.23837872454602355
	data : 0.11507959365844726
	model : 0.06502656936645508
			 train-loss:  2.1960055610407956 	 ± 0.24097020710261952
	data : 0.11460685729980469
	model : 0.06502852439880372
			 train-loss:  2.1943721904836853 	 ± 0.24056781715459505
	data : 0.11473045349121094
	model : 0.06509580612182617
			 train-loss:  2.1902101202907724 	 ± 0.24369588174192355
	data : 0.11471433639526367
	model : 0.06517086029052735
			 train-loss:  2.1934519763720237 	 ± 0.24518161460503815
	data : 0.11475529670715331
	model : 0.06516175270080567
			 train-loss:  2.1907765003813413 	 ± 0.2458729927081472
	data : 0.11465706825256347
	model : 0.06518344879150391
			 train-loss:  2.18845378557841 	 ± 0.24615392465156558
	data : 0.1152033805847168
	model : 0.06520576477050781
			 train-loss:  2.1897052083133666 	 ± 0.24551766409385026
	data : 0.1150240421295166
	model : 0.06513347625732421
			 train-loss:  2.1889220964713174 	 ± 0.24466106925540462
	data : 0.11504955291748047
	model : 0.06506896018981934
			 train-loss:  2.1855043114685433 	 ± 0.24657147642167318
	data : 0.1149827480316162
	model : 0.06508326530456543
			 train-loss:  2.187030599001915 	 ± 0.24615792840985282
	data : 0.1149892807006836
	model : 0.06506829261779785
			 train-loss:  2.186468596458435 	 ± 0.2452511792505367
	data : 0.11488652229309082
	model : 0.06510720252990723
			 train-loss:  2.1861347745335293 	 ± 0.24430453201008084
	data : 0.11480765342712403
	model : 0.06513810157775879
			 train-loss:  2.187490828393951 	 ± 0.2438164170020759
	data : 0.11474366188049316
	model : 0.06513285636901855
			 train-loss:  2.188150458969176 	 ± 0.24297588179399662
	data : 0.1148909091949463
	model : 0.06508140563964844
			 train-loss:  2.185892331507779 	 ± 0.24337690046476987
	data : 0.11478090286254883
	model : 0.06507601737976074
			 train-loss:  2.1851997329638553 	 ± 0.2425666152753797
	data : 0.11462149620056153
	model : 0.06501498222351074
			 train-loss:  2.186854168658948 	 ± 0.2423741817571463
	data : 0.1148411750793457
	model : 0.0650017261505127
			 train-loss:  2.185170279307799 	 ± 0.24222232055232262
	data : 0.11497054100036622
	model : 0.06501560211181641
			 train-loss:  2.1866983103572872 	 ± 0.24194775539789418
	data : 0.11496305465698242
	model : 0.0650217056274414
			 train-loss:  2.187468871251861 	 ± 0.24120702907304095
	data : 0.11499357223510742
	model : 0.06501784324645996
			 train-loss:  2.1852342517287644 	 ± 0.24170021520056267
	data : 0.1149972915649414
	model : 0.0650594711303711
			 train-loss:  2.1859240689698387 	 ± 0.24094331710558634
	data : 0.11493124961853027
	model : 0.06503300666809082
			 train-loss:  2.186599113645345 	 ± 0.240191394009997
	data : 0.11500921249389648
	model : 0.06502885818481445
			 train-loss:  2.184261008449223 	 ± 0.2408792047641089
	data : 0.11493005752563476
	model : 0.06503124237060547
			 train-loss:  2.1814314761607765 	 ± 0.2423019274473223
	data : 0.11468071937561035
	model : 0.06502151489257812
			 train-loss:  2.1826285064220428 	 ± 0.24184713317002304
	data : 0.11475048065185547
	model : 0.06499319076538086
			 train-loss:  2.1844972541146244 	 ± 0.24200025492725574
	data : 0.1147951602935791
	model : 0.06501479148864746
			 train-loss:  2.1839818543111775 	 ± 0.24122428317176217
	data : 0.11480021476745605
	model : 0.06499361991882324
			 train-loss:  2.18156901606313 	 ± 0.24209281604325011
	data : 0.11475276947021484
	model : 0.06506619453430176
			 train-loss:  2.1832429005040064 	 ± 0.24207972912825318
	data : 0.11501083374023438
	model : 0.06508517265319824
			 train-loss:  2.179276301943023 	 ± 0.24589453353460414
	data : 0.1150505542755127
	model : 0.0650949478149414
			 train-loss:  2.1819554110095924 	 ± 0.24716540915853116
	data : 0.11524767875671386
	model : 0.06510496139526367
			 train-loss:  2.1823675454068345 	 ± 0.24637360860666005
	data : 0.1150580883026123
	model : 0.06512870788574218
			 train-loss:  2.1834532441319645 	 ± 0.2458924464471469
	data : 0.11496934890747071
	model : 0.06504273414611816
			 train-loss:  2.188870881227839 	 ± 0.25377395415286275
	data : 0.11489305496215821
	model : 0.06502537727355957
			 train-loss:  2.1882438770929973 	 ± 0.2530423982944056
	data : 0.11497187614440918
	model : 0.06499524116516113
			 train-loss:  2.187577988138262 	 ± 0.2523349436741376
	data : 0.11486935615539551
	model : 0.06499624252319336
			 train-loss:  2.188291698694229 	 ± 0.25165639229110676
	data : 0.115008544921875
	model : 0.06498675346374512
			 train-loss:  2.1889283563576494 	 ± 0.2509554197123628
	data : 0.11514749526977539
	model : 0.06502037048339844
			 train-loss:  2.189121676729871 	 ± 0.2501507318016685
	data : 0.11518626213073731
	model : 0.06504006385803222
			 train-loss:  2.1894214907000142 	 ± 0.24937024532295346
	data : 0.11513719558715821
	model : 0.06504158973693848
			 train-loss:  2.1904166019879856 	 ± 0.24887824801037034
	data : 0.1151045322418213
	model : 0.06504077911376953
			 train-loss:  2.190345666970417 	 ± 0.24808595800981056
	data : 0.11491312980651855
	model : 0.06503534317016602
			 train-loss:  2.190474151056024 	 ± 0.24730486980726835
	data : 0.11482377052307129
	model : 0.06496129035949708
			 train-loss:  2.191106986699614 	 ± 0.24665425686196873
	data : 0.11490082740783691
	model : 0.06493711471557617
			 train-loss:  2.1916820034384727 	 ± 0.2459891366477371
	data : 0.11501970291137695
	model : 0.06493983268737794
			 train-loss:  2.19127142799567 	 ± 0.24527899282239957
	data : 0.11495928764343262
	model : 0.06492576599121094
			 train-loss:  2.192919700234025 	 ± 0.24541356992189173
	data : 0.11513633728027343
	model : 0.06492853164672852
			 train-loss:  2.195697231526755 	 ± 0.2472005312393858
	data : 0.11512069702148438
	model : 0.06499357223510742
			 train-loss:  2.1944323174837157 	 ± 0.24697427698338528
	data : 0.11516084671020507
	model : 0.06502900123596192
			 train-loss:  2.195131201455087 	 ± 0.2463873433225603
	data : 0.11511030197143554
	model : 0.06504931449890136
			 train-loss:  2.1974194272454963 	 ± 0.24739635415351807
	data : 0.11504569053649902
	model : 0.06502623558044433
			 train-loss:  2.1971734362448045 	 ± 0.24667489581769314
	data : 0.11489706039428711
	model : 0.06503763198852539
			 train-loss:  2.197585640209062 	 ± 0.24599732939849364
	data : 0.11495809555053711
	model : 0.06501789093017578
			 train-loss:  2.1982602586407634 	 ± 0.24542426426588457
	data : 0.11483163833618164
	model : 0.06498651504516602
			 train-loss:  2.199270615156959 	 ± 0.2450536185122696
	data : 0.11486172676086426
	model : 0.06495590209960937
			 train-loss:  2.1978297177811115 	 ± 0.2450572389988808
	data : 0.11490035057067871
	model : 0.06509332656860352
			 train-loss:  2.1980444747348162 	 ± 0.24435996267368368
	data : 0.11502437591552735
	model : 0.06507725715637207
			 train-loss:  2.1972335101552094 	 ± 0.24388471584959284
	data : 0.11502547264099121
	model : 0.06510605812072753
			 train-loss:  2.1967452339742377 	 ± 0.24326767667032598
	data : 0.11514430046081543
	model : 0.06511797904968261
			 train-loss:  2.196980605806623 	 ± 0.2425914991341594
	data : 0.11510992050170898
	model : 0.0652132511138916
			 train-loss:  2.1966503181240777 	 ± 0.2419407935496874
	data : 0.11504478454589843
	model : 0.06512823104858398
			 train-loss:  2.197008192202466 	 ± 0.2413030882596983
	data : 0.11499981880187989
	model : 0.06508212089538574
			 train-loss:  2.1975514861974825 	 ± 0.24073285237633155
	data : 0.11500515937805175
	model : 0.06505341529846191
			 train-loss:  2.1961772595037963 	 ± 0.2407585992973374
	data : 0.11503815650939941
	model : 0.06506242752075195
			 train-loss:  2.1958679974079134 	 ± 0.24012454500103006
	data : 0.11502799987792969
	model : 0.0649782657623291
			 train-loss:  2.1972888220739626 	 ± 0.24021783528428198
	data : 0.11517343521118165
	model : 0.06496834754943848
			 train-loss:  2.1981279869655985 	 ± 0.2398228719019429
	data : 0.11512761116027832
	model : 0.06500988006591797
			 train-loss:  2.1993606761505045 	 ± 0.23974418310736684
	data : 0.11510715484619141
	model : 0.06500687599182128
			 train-loss:  2.200983283960301 	 ± 0.24009729177026615
	data : 0.11490306854248047
	model : 0.06497335433959961
			 train-loss:  2.199789779895061 	 ± 0.23999417667869966
	data : 0.11481962203979493
	model : 0.0650033950805664
			 train-loss:  2.1986871624505646 	 ± 0.2398175536015315
	data : 0.1145820140838623
	model : 0.06492419242858886
			 train-loss:  2.198892418713493 	 ± 0.23919185179157365
	data : 0.11460685729980469
	model : 0.06488757133483887
			 train-loss:  2.197806102164248 	 ± 0.23901693352057143
	data : 0.1147730827331543
	model : 0.06493220329284669
			 train-loss:  2.1998681214751388 	 ± 0.2400545475767793
	data : 0.11489644050598144
	model : 0.06494321823120117
			 train-loss:  2.2005061124500473 	 ± 0.23958259376878552
	data : 0.11498517990112304
	model : 0.06491146087646485
			 train-loss:  2.2007781835126625 	 ± 0.23898401814590894
	data : 0.11504669189453125
	model : 0.06497454643249512
			 train-loss:  2.2006491497159004 	 ± 0.23836752212563075
	data : 0.11499614715576172
	model : 0.06501927375793456
			 train-loss:  2.2011655105828005 	 ± 0.23785682443323863
	data : 0.11489601135253906
	model : 0.06497917175292969
			 train-loss:  2.2023798841791056 	 ± 0.23784208724018388
	data : 0.11487841606140137
	model : 0.0650825023651123
			 train-loss:  2.201572744662945 	 ± 0.2374976794564949
	data : 0.1146357536315918
	model : 0.06510167121887207
			 train-loss:  2.2004003105114918 	 ± 0.23745613013747216
	data : 0.11463875770568847
	model : 0.06509585380554199
			 train-loss:  2.1993166982825034 	 ± 0.23733802812920965
	data : 0.11475825309753418
	model : 0.06509718894958497
			 train-loss:  2.199450316453221 	 ± 0.2367453593712061
	data : 0.1148503303527832
	model : 0.06509528160095215
			 train-loss:  2.200556308779884 	 ± 0.23666202161473943
	data : 0.11481480598449707
	model : 0.06499085426330567
			 train-loss:  2.1992245656251908 	 ± 0.2368159672374015
	data : 0.11493291854858398
	model : 0.06500468254089356
			 train-loss:  2.198096040469497 	 ± 0.236764655728024
	data : 0.1149681568145752
	model : 0.06501455307006836
			 train-loss:  2.200053414495865 	 ± 0.23780261509026757
	data : 0.11501164436340332
	model : 0.06499080657958985
			 train-loss:  2.1990307181926783 	 ± 0.2376610722517151
	data : 0.11493549346923829
	model : 0.06505064964294434
			 train-loss:  2.199955784222659 	 ± 0.23744394188718965
	data : 0.11482806205749511
	model : 0.06503925323486329
			 train-loss:  2.1983462857037055 	 ± 0.23797701992788134
	data : 0.11483349800109863
	model : 0.06504831314086915
			 train-loss:  2.1984366849788186 	 ± 0.23740223146154785
	data : 0.11492042541503907
	model : 0.06509065628051758
			 train-loss:  2.1984909076045676 	 ± 0.23682938057317848
	data : 0.11480937004089356
	model : 0.06507787704467774
			 train-loss:  2.197574197099759 	 ± 0.23662724916180986
	data : 0.11480827331542968
	model : 0.06503481864929199
			 train-loss:  2.19756250290209 	 ± 0.23606053674279823
	data : 0.11486215591430664
	model : 0.06507630348205566
			 train-loss:  2.197480857939947 	 ± 0.23550077505553793
	data : 0.11489725112915039
	model : 0.06501784324645996
			 train-loss:  2.196935362160488 	 ± 0.2350750036062143
	data : 0.11486282348632812
	model : 0.06497879028320312
			 train-loss:  2.196360756766121 	 ± 0.23466840836833178
	data : 0.11487102508544922
	model : 0.06501288414001465
			 train-loss:  2.1953764650183665 	 ± 0.23455513800039976
	data : 0.11483616828918457
	model : 0.06505064964294434
			 train-loss:  2.19644275948266 	 ± 0.23452335851834935
	data : 0.11481924057006836
	model : 0.06498398780822753
			 train-loss:  2.1946032141530236 	 ± 0.23551973698951098
	data : 0.11484489440917969
	model : 0.06504306793212891
			 train-loss:  2.193191181730341 	 ± 0.23588433119119667
	data : 0.11504311561584472
	model : 0.06501765251159668
			 train-loss:  2.193209612973824 	 ± 0.23534034719082186
	data : 0.11513547897338867
	model : 0.06498417854309083
			 train-loss:  2.1950395850960267 	 ± 0.23634235375716153
	data : 0.11528115272521973
	model : 0.06491250991821289
			 train-loss:  2.196369444398575 	 ± 0.2366182342253778
	data : 0.11539235115051269
	model : 0.06492314338684083
			 train-loss:  2.19658692966808 	 ± 0.23610179090025055
	data : 0.11537356376647949
	model : 0.06481504440307617
			 train-loss:  2.1971835183881527 	 ± 0.2357331586826645
	data : 0.11514692306518555
	model : 0.06476263999938965
			 train-loss:  2.197672856820596 	 ± 0.2353140989222782
	data : 0.11517434120178223
	model : 0.06468944549560547
			 train-loss:  2.196679148973371 	 ± 0.23525227318875974
	data : 0.11499490737915039
	model : 0.06461505889892578
			 train-loss:  2.1947292243795737 	 ± 0.23652580023134345
	data : 0.11494884490966797
	model : 0.06453619003295899
			 train-loss:  2.195371052953932 	 ± 0.23619502025730096
	data : 0.11517839431762696
	model : 0.06438617706298828
			 train-loss:  2.1952752608113584 	 ± 0.23567626583837273
	data : 0.11546921730041504
	model : 0.0642664909362793
			 train-loss:  2.1961355897298467 	 ± 0.23551198612842061
	data : 0.11558489799499512
	model : 0.06415729522705078
			 train-loss:  2.1975644309269753 	 ± 0.23597894911411046
	data : 0.11574549674987793
	model : 0.0640324592590332
			 train-loss:  2.198300766111982 	 ± 0.235725503594876
	data : 0.1158745288848877
	model : 0.06391096115112305
			 train-loss:  2.198835159902987 	 ± 0.23535147456483993
	data : 0.11590542793273925
	model : 0.06387853622436523
			 train-loss:  2.199357276871091 	 ± 0.23497495838729968
	data : 0.11567931175231934
	model : 0.06383304595947266
			 train-loss:  2.1998590492996675 	 ± 0.23459199295979516
	data : 0.11557445526123047
	model : 0.06382498741149903
			 train-loss:  2.1994161969090738 	 ± 0.23418519963726064
	data : 0.11566381454467774
	model : 0.0638354778289795
			 train-loss:  2.200171117599194 	 ± 0.2339682138799268
	data : 0.11588211059570312
	model : 0.06383137702941895
			 train-loss:  2.2010572550144603 	 ± 0.2338630586382735
	data : 0.11588668823242188
	model : 0.06386423110961914
			 train-loss:  2.2023978228286163 	 ± 0.23427016116834068
	data : 0.11593394279479981
	model : 0.0638878345489502
			 train-loss:  2.200607622726054 	 ± 0.2353874961005434
	data : 0.11588473320007324
	model : 0.06393566131591796
			 train-loss:  2.1994726472542068 	 ± 0.23554143113327516
	data : 0.11586165428161621
	model : 0.06405649185180665
			 train-loss:  2.2002310029632377 	 ± 0.23533913289947062
	data : 0.1154442310333252
	model : 0.06407513618469238
			 train-loss:  2.200541018942992 	 ± 0.2348972306364521
	data : 0.11537904739379883
	model : 0.06404047012329102
			 train-loss:  2.1988675802080464 	 ± 0.23583862128316824
	data : 0.11550202369689941
	model : 0.06405797004699706
			 train-loss:  2.1985589610643625 	 ± 0.23539960791802875
	data : 0.11568093299865723
	model : 0.06404151916503906
			 train-loss:  2.1979591552122137 	 ± 0.23509998320015277
	data : 0.11556882858276367
	model : 0.06399359703063964
			 train-loss:  2.1966346922467967 	 ± 0.23552441194831986
	data : 0.11588587760925292
	model : 0.06402969360351562
			 train-loss:  2.1979316419484665 	 ± 0.2359147313842243
	data : 0.1159125804901123
	model : 0.06404590606689453
			 train-loss:  2.197558058955805 	 ± 0.23550734764998948
	data : 0.11595969200134278
	model : 0.064019775390625
			 train-loss:  2.1973899964861543 	 ± 0.23504490981333512
	data : 0.11568250656127929
	model : 0.06395573616027832
			 train-loss:  2.1961757800271435 	 ± 0.23534549176812092
	data : 0.11567234992980957
	model : 0.06388769149780274
			 train-loss:  2.1951008206869225 	 ± 0.23548170641297816
	data : 0.11567916870117187
	model : 0.0638211727142334
			 train-loss:  2.198342730522156 	 ± 0.24051364282408835
	data : 0.11571183204650878
	model : 0.06382193565368652
			 train-loss:  2.1978721694642327 	 ± 0.24014933671329472
	data : 0.11553258895874023
	model : 0.06382946968078614
			 train-loss:  2.1968707163182515 	 ± 0.2401969566109992
	data : 0.11564526557922364
	model : 0.0639009952545166
			 train-loss:  2.19807100531612 	 ± 0.2404778388168126
	data : 0.11572074890136719
	model : 0.06386632919311523
			 train-loss:  2.1971392232602036 	 ± 0.24046117048365087
	data : 0.1155064582824707
	model : 0.06388206481933593
			 train-loss:  2.1973462913550583 	 ± 0.24001190396632002
	data : 0.11534152030944825
	model : 0.06389055252075196
			 train-loss:  2.201682746876031 	 ± 0.24935101278883676
	data : 0.11526293754577636
	model : 0.055448484420776364
#epoch  21    val-loss:  2.4236501894499125  train-loss:  2.201682746876031  lr:  0.0003125
			 train-loss:  1.8965357542037964 	 ± 0.0
	data : 5.572922706604004
	model : 0.08338618278503418
			 train-loss:  1.9330683946609497 	 ± 0.03653264045715332
	data : 2.845346450805664
	model : 0.07517039775848389
			 train-loss:  1.921193242073059 	 ± 0.03423148225175856
	data : 1.9350021680196126
	model : 0.07168269157409668
			 train-loss:  1.9274274110794067 	 ± 0.031550600191810343
	data : 1.4798720479011536
	model : 0.06992089748382568
			 train-loss:  2.003292751312256 	 ± 0.15433260087406872
	data : 1.2069632053375243
	model : 0.06886906623840332
			 train-loss:  2.0238609313964844 	 ± 0.14820270995165744
	data : 0.11535954475402832
	model : 0.06517720222473145
			 train-loss:  2.039711202893938 	 ± 0.14259631627024025
	data : 0.1147536277770996
	model : 0.06475749015808105
			 train-loss:  2.0637411177158356 	 ± 0.14776350490410525
	data : 0.11480822563171386
	model : 0.06477713584899902
			 train-loss:  2.1215176582336426 	 ± 0.21473955509851708
	data : 0.11474771499633789
	model : 0.06481528282165527
			 train-loss:  2.1263110876083373 	 ± 0.2042267405558093
	data : 0.11468358039855957
	model : 0.06485214233398437
			 train-loss:  2.1355946497483687 	 ± 0.19692313069764023
	data : 0.11448850631713867
	model : 0.06484951972961425
			 train-loss:  2.1134486893812814 	 ± 0.20234139126692074
	data : 0.11448378562927246
	model : 0.06489748954772949
			 train-loss:  2.114933958420387 	 ± 0.19447139461400076
	data : 0.11452689170837402
	model : 0.06496186256408691
			 train-loss:  2.1147243891443526 	 ± 0.18739884824118402
	data : 0.11459507942199706
	model : 0.06500697135925293
			 train-loss:  2.1041966835657755 	 ± 0.18528022727876184
	data : 0.11454763412475585
	model : 0.06505160331726074
			 train-loss:  2.0945067554712296 	 ± 0.18328021048070728
	data : 0.11471371650695801
	model : 0.0651479721069336
			 train-loss:  2.084468091235441 	 ± 0.18228563702629821
	data : 0.11480088233947754
	model : 0.06514916419982911
			 train-loss:  2.105517155594296 	 ± 0.19726663015386015
	data : 0.11482272148132325
	model : 0.06511178016662597
			 train-loss:  2.1078363531514217 	 ± 0.19225719182227416
	data : 0.11484813690185547
	model : 0.0651008129119873
			 train-loss:  2.119014233350754 	 ± 0.1936198376362276
	data : 0.11476860046386719
	model : 0.06505818367004394
			 train-loss:  2.1394074928192865 	 ± 0.20981222260698695
	data : 0.11470837593078613
	model : 0.06491556167602539
			 train-loss:  2.121658346869729 	 ± 0.2205354446791883
	data : 0.11464238166809082
	model : 0.06485962867736816
			 train-loss:  2.12463813242705 	 ± 0.21614027727906707
	data : 0.11461038589477539
	model : 0.06485390663146973
			 train-loss:  2.1236187318960824 	 ± 0.2116459182987528
	data : 0.11453757286071778
	model : 0.06488599777221679
			 train-loss:  2.1181277418136597 	 ± 0.20910728890516428
	data : 0.11461863517761231
	model : 0.06492080688476562
			 train-loss:  2.110315286196195 	 ± 0.20873417559876983
	data : 0.11473188400268555
	model : 0.06496896743774414
			 train-loss:  2.1105558342403836 	 ± 0.20483593052463986
	data : 0.11473965644836426
	model : 0.06500205993652344
			 train-loss:  2.119100902761732 	 ± 0.2059872771658203
	data : 0.11462059020996093
	model : 0.06499323844909669
			 train-loss:  2.122394397340972 	 ± 0.20315350781856
	data : 0.11462197303771973
	model : 0.06491823196411133
			 train-loss:  2.1319276809692385 	 ± 0.20623107804852886
	data : 0.11465449333190918
	model : 0.06487908363342285
			 train-loss:  2.1354420261998333 	 ± 0.203788616401785
	data : 0.11466464996337891
	model : 0.0649033546447754
			 train-loss:  2.137399934232235 	 ± 0.2008751588406347
	data : 0.11462507247924805
	model : 0.06494731903076172
			 train-loss:  2.1370252190214214 	 ± 0.19781953947892786
	data : 0.11475534439086914
	model : 0.06500258445739746
			 train-loss:  2.139576371978311 	 ± 0.19543896448538128
	data : 0.11483521461486816
	model : 0.06504769325256347
			 train-loss:  2.1327261788504464 	 ± 0.19672447332231496
	data : 0.1148336410522461
	model : 0.06509184837341309
			 train-loss:  2.1340075267685785 	 ± 0.19412101596913267
	data : 0.11462092399597168
	model : 0.0650550365447998
			 train-loss:  2.130396520769274 	 ± 0.19270165370162087
	data : 0.11445016860961914
	model : 0.06503782272338868
			 train-loss:  2.1300282666557715 	 ± 0.1901623946408816
	data : 0.11458148956298828
	model : 0.0649785041809082
			 train-loss:  2.1302549900152745 	 ± 0.18771378654551915
	data : 0.1146878719329834
	model : 0.06497268676757813
			 train-loss:  2.124813202023506 	 ± 0.18844220127965522
	data : 0.11461386680603028
	model : 0.06499404907226562
			 train-loss:  2.128542827396858 	 ± 0.18761865200731923
	data : 0.11479749679565429
	model : 0.06506590843200684
			 train-loss:  2.1233418186505637 	 ± 0.18833936246211305
	data : 0.11499829292297363
	model : 0.06508193016052247
			 train-loss:  2.1204326208247695 	 ± 0.18708889860443628
	data : 0.11490983963012695
	model : 0.06513638496398926
			 train-loss:  2.119340555234389 	 ± 0.18508925488849795
	data : 0.11484899520874023
	model : 0.06512060165405273
			 train-loss:  2.114129919475979 	 ± 0.18625620679420113
	data : 0.11482462882995606
	model : 0.06510992050170898
			 train-loss:  2.1220759049705835 	 ± 0.19177709743152282
	data : 0.11474776268005371
	model : 0.06504101753234863
			 train-loss:  2.1200772346334253 	 ± 0.19020959545029326
	data : 0.11466245651245117
	model : 0.06503510475158691
			 train-loss:  2.128294805685679 	 ± 0.19646827971315833
	data : 0.1147080898284912
	model : 0.06499185562133789
			 train-loss:  2.1326873934998805 	 ± 0.19682018684673294
	data : 0.11468915939331055
	model : 0.06499543190002441
			 train-loss:  2.134629530906677 	 ± 0.19531575773350265
	data : 0.1147085189819336
	model : 0.06498370170593262
			 train-loss:  2.137102342119404 	 ± 0.1941802775141845
	data : 0.11474447250366211
	model : 0.0650637149810791
			 train-loss:  2.140558967223534 	 ± 0.19388199061477238
	data : 0.11477742195129395
	model : 0.06502318382263184
			 train-loss:  2.139796688871564 	 ± 0.19212285726344538
	data : 0.1146463394165039
	model : 0.06504149436950683
			 train-loss:  2.1363213459650674 	 ± 0.19200985945017354
	data : 0.11460132598876953
	model : 0.06504287719726562
			 train-loss:  2.1341596646742387 	 ± 0.1909183006056082
	data : 0.11469950675964355
	model : 0.0650437355041504
			 train-loss:  2.13659998348781 	 ± 0.19006957105256322
	data : 0.11483850479125976
	model : 0.06497716903686523
			 train-loss:  2.1411037821518746 	 ± 0.19138589276537685
	data : 0.11490702629089355
	model : 0.06498160362243652
			 train-loss:  2.140157703695626 	 ± 0.1898632448034745
	data : 0.11493535041809082
	model : 0.06500053405761719
			 train-loss:  2.1386606612447965 	 ± 0.1885922948943001
	data : 0.11496248245239257
	model : 0.06508135795593262
			 train-loss:  2.135448004802068 	 ± 0.1886351451585704
	data : 0.11492881774902344
	model : 0.06512336730957032
			 train-loss:  2.1361921283065297 	 ± 0.18717133790713233
	data : 0.11483454704284668
	model : 0.06512737274169922
			 train-loss:  2.134615480899811 	 ± 0.18606368376633492
	data : 0.11480498313903809
	model : 0.06513438224792481
			 train-loss:  2.133311152458191 	 ± 0.184866585309752
	data : 0.11464695930480957
	model : 0.06507854461669922
			 train-loss:  2.1387888845056295 	 ± 0.18849936157136374
	data : 0.11463823318481445
	model : 0.0649909496307373
			 train-loss:  2.1336723400996283 	 ± 0.1914701562172051
	data : 0.11463189125061035
	model : 0.06492137908935547
			 train-loss:  2.1354063713189326 	 ± 0.190527688063247
	data : 0.11463913917541504
	model : 0.06495246887207032
			 train-loss:  2.1345030478577116 	 ± 0.18924283999943955
	data : 0.11462607383728027
	model : 0.06499695777893066
			 train-loss:  2.1358598330441643 	 ± 0.18817420365295218
	data : 0.11484789848327637
	model : 0.06504945755004883
			 train-loss:  2.13642974867337 	 ± 0.186864753158375
	data : 0.11481270790100098
	model : 0.06509437561035156
			 train-loss:  2.137731296675546 	 ± 0.18583995547315113
	data : 0.11485300064086915
	model : 0.0650984764099121
			 train-loss:  2.140201770084005 	 ± 0.18568060304553338
	data : 0.11476812362670899
	model : 0.06509814262390137
			 train-loss:  2.1444644033908844 	 ± 0.18785235739914816
	data : 0.11475334167480469
	model : 0.06507663726806641
			 train-loss:  2.140894510974623 	 ± 0.18900444823588908
	data : 0.11461548805236817
	model : 0.06507964134216308
			 train-loss:  2.137233713188687 	 ± 0.1903109236637611
	data : 0.11474661827087403
	model : 0.06504349708557129
			 train-loss:  2.1373148361841836 	 ± 0.189039214679036
	data : 0.11472964286804199
	model : 0.06508660316467285
			 train-loss:  2.1388251012877415 	 ± 0.18824633865797946
	data : 0.11472992897033692
	model : 0.06508417129516601
			 train-loss:  2.1396080410325684 	 ± 0.18714447610207666
	data : 0.11474046707153321
	model : 0.06504812240600585
			 train-loss:  2.1407620157950964 	 ± 0.1862164843875345
	data : 0.11480245590209961
	model : 0.065028715133667
			 train-loss:  2.141714049291007 	 ± 0.18522508389951028
	data : 0.1148759365081787
	model : 0.06501874923706055
			 train-loss:  2.1406448051333427 	 ± 0.18430897135432153
	data : 0.1147303581237793
	model : 0.0649643898010254
			 train-loss:  2.1421609027886097 	 ± 0.18366899814830712
	data : 0.1147580623626709
	model : 0.06493659019470215
			 train-loss:  2.142371292521314 	 ± 0.1825554500381518
	data : 0.11484432220458984
	model : 0.06493992805480957
			 train-loss:  2.1410781231271216 	 ± 0.18182985419144587
	data : 0.11482939720153809
	model : 0.06498260498046875
			 train-loss:  2.1424554997966405 	 ± 0.18117937055432878
	data : 0.11467304229736328
	model : 0.06501011848449707
			 train-loss:  2.1417301163953892 	 ± 0.18023311490849453
	data : 0.11473398208618164
	model : 0.06505184173583985
			 train-loss:  2.1425030578014463 	 ± 0.1793238336844662
	data : 0.11480464935302734
	model : 0.06505422592163086
			 train-loss:  2.1475460049749793 	 ± 0.18432175509516846
	data : 0.11471638679504395
	model : 0.06505508422851562
			 train-loss:  2.146605736830018 	 ± 0.18348120474571378
	data : 0.11473040580749512
	model : 0.06498546600341797
			 train-loss:  2.1477806099345176 	 ± 0.1827800833427497
	data : 0.11470484733581543
	model : 0.06498217582702637
			 train-loss:  2.1513068874677024 	 ± 0.18478104614814925
	data : 0.11484208106994628
	model : 0.06494474411010742
			 train-loss:  2.154445083586724 	 ± 0.1861589927425985
	data : 0.11471877098083497
	model : 0.06494317054748536
			 train-loss:  2.1553753394147623 	 ± 0.18535704235237666
	data : 0.11473274230957031
	model : 0.0649641513824463
			 train-loss:  2.1547591109429636 	 ± 0.18445253152998095
	data : 0.1147500991821289
	model : 0.06504120826721191
			 train-loss:  2.154864842587329 	 ± 0.18347161101241397
	data : 0.11477856636047364
	model : 0.0650507926940918
			 train-loss:  2.152949266684683 	 ± 0.1834459688599167
	data : 0.1146763801574707
	model : 0.06509842872619628
			 train-loss:  2.1529657108088336 	 ± 0.18248809030125387
	data : 0.11473541259765625
	model : 0.06507506370544433
			 train-loss:  2.151793304177904 	 ± 0.18190805395779505
	data : 0.11473121643066406
	model : 0.06503615379333497
			 train-loss:  2.15552241826544 	 ± 0.18466670472629793
	data : 0.11497125625610352
	model : 0.06495161056518554
			 train-loss:  2.1570019637695466 	 ± 0.18431455903091853
	data : 0.11495394706726074
	model : 0.06495251655578613
			 train-loss:  2.155524822473526 	 ± 0.18397866941166544
	data : 0.1149451732635498
	model : 0.0649287223815918
			 train-loss:  2.1523611262293145 	 ± 0.1857792183887236
	data : 0.11500988006591797
	model : 0.06500177383422852
			 train-loss:  2.1603242462756587 	 ± 0.20144499400157403
	data : 0.11521782875061035
	model : 0.06500229835510254
			 train-loss:  2.1618979810511023 	 ± 0.20109381291141806
	data : 0.11489262580871581
	model : 0.06502838134765625
			 train-loss:  2.163100552100402 	 ± 0.20049649367020153
	data : 0.11479339599609376
	model : 0.06496515274047851
			 train-loss:  2.1635901700882685 	 ± 0.19960192714866123
	data : 0.11485743522644043
	model : 0.06495885848999024
			 train-loss:  2.1618377521352947 	 ± 0.19946810073109952
	data : 0.11485891342163086
	model : 0.06490464210510254
			 train-loss:  2.159042737194311 	 ± 0.200608476144765
	data : 0.11473159790039063
	model : 0.06496782302856445
			 train-loss:  2.1591693781040333 	 ± 0.19968187034547502
	data : 0.11486945152282715
	model : 0.06499104499816895
			 train-loss:  2.1594105466790157 	 ± 0.19877958877557053
	data : 0.11504573822021484
	model : 0.06507487297058105
			 train-loss:  2.159510588645935 	 ± 0.197876738865853
	data : 0.11506175994873047
	model : 0.06507720947265624
			 train-loss:  2.159614356788429 	 ± 0.19698639207203456
	data : 0.11489667892456054
	model : 0.06505656242370605
			 train-loss:  2.159772513168199 	 ± 0.19611209573601113
	data : 0.11479048728942871
	model : 0.06507530212402343
			 train-loss:  2.1578723424303847 	 ± 0.19627529948313951
	data : 0.11478843688964843
	model : 0.06506962776184082
			 train-loss:  2.1568153918835153 	 ± 0.19573528232337098
	data : 0.11479673385620118
	model : 0.06506838798522949
			 train-loss:  2.158607919319816 	 ± 0.19581994262706218
	data : 0.1147505760192871
	model : 0.06508145332336426
			 train-loss:  2.1571312474793403 	 ± 0.19561607826362273
	data : 0.11489534378051758
	model : 0.06517891883850098
			 train-loss:  2.1559408719723043 	 ± 0.1951998066682675
	data : 0.11496291160583497
	model : 0.06513261795043945
			 train-loss:  2.1558761505757347 	 ± 0.19437218973104428
	data : 0.11503987312316895
	model : 0.06509575843811036
			 train-loss:  2.1555361958111035 	 ± 0.19358900204705426
	data : 0.11497397422790527
	model : 0.06507115364074707
			 train-loss:  2.1535126040379207 	 ± 0.19404043852258193
	data : 0.11499209403991699
	model : 0.06508197784423828
			 train-loss:  2.152848894931068 	 ± 0.19337368473759572
	data : 0.11497249603271484
	model : 0.06501607894897461
			 train-loss:  2.1513727688398516 	 ± 0.19326285611819757
	data : 0.11509976387023926
	model : 0.06498756408691406
			 train-loss:  2.1543651014808716 	 ± 0.19529276321001585
	data : 0.11513266563415528
	model : 0.06515159606933593
			 train-loss:  2.1545873957295574 	 ± 0.1945193220989325
	data : 0.11508150100708008
	model : 0.06516761779785156
			 train-loss:  2.155745080947876 	 ± 0.1941681067644585
	data : 0.11511778831481934
	model : 0.06515984535217285
			 train-loss:  2.155497295515878 	 ± 0.19341590443381246
	data : 0.11510310173034669
	model : 0.06517601013183594
			 train-loss:  2.1550710464087057 	 ± 0.19271232485705686
	data : 0.11492466926574707
	model : 0.06520333290100097
			 train-loss:  2.158995008096099 	 ± 0.19698573191805072
	data : 0.11476426124572754
	model : 0.06506104469299316
			 train-loss:  2.1589538145434948 	 ± 0.1962212893629993
	data : 0.1146883487701416
	model : 0.0649764060974121
			 train-loss:  2.1608395649836614 	 ± 0.19663506838038544
	data : 0.11464338302612305
	model : 0.06495270729064942
			 train-loss:  2.1602759634265465 	 ± 0.19598849161501378
	data : 0.11468696594238281
	model : 0.06500110626220704
			 train-loss:  2.157704745278214 	 ± 0.19745013304705
	data : 0.11477007865905761
	model : 0.06506152153015136
			 train-loss:  2.1614368858193993 	 ± 0.2013256920664686
	data : 0.11486754417419434
	model : 0.06510605812072753
			 train-loss:  2.1608958030814556 	 ± 0.20067011461259807
	data : 0.11495914459228515
	model : 0.06520748138427734
			 train-loss:  2.1638864305284287 	 ± 0.20290068011172122
	data : 0.11489830017089844
	model : 0.0652045726776123
			 train-loss:  2.1646147072315216 	 ± 0.20233036673346783
	data : 0.11482601165771485
	model : 0.0651432991027832
			 train-loss:  2.1654497289309536 	 ± 0.2018256435591713
	data : 0.11467232704162597
	model : 0.06502714157104492
			 train-loss:  2.1662289647088535 	 ± 0.20129979384893987
	data : 0.11459188461303711
	model : 0.06496624946594239
			 train-loss:  2.1657383304705724 	 ± 0.20065718098532728
	data : 0.11470131874084473
	model : 0.06491694450378419
			 train-loss:  2.1652596746172224 	 ± 0.20001888845458193
	data : 0.1148111343383789
	model : 0.06494998931884766
			 train-loss:  2.1647332137358104 	 ± 0.19940565878855454
	data : 0.11484737396240234
	model : 0.06498589515686035
			 train-loss:  2.1641599023845832 	 ± 0.19881886993366554
	data : 0.11495919227600097
	model : 0.06502418518066407
			 train-loss:  2.1659457133366513 	 ± 0.19926206867843566
	data : 0.11498217582702637
	model : 0.06504426002502442
			 train-loss:  2.1661040004756718 	 ± 0.19857800256559274
	data : 0.11490802764892578
	model : 0.06505708694458008
			 train-loss:  2.1631035089492796 	 ± 0.20114098178579692
	data : 0.11482830047607422
	model : 0.06498990058898926
			 train-loss:  2.1650694012641907 	 ± 0.20184393298515438
	data : 0.11496238708496094
	model : 0.06493196487426758
			 train-loss:  2.167408431468367 	 ± 0.20313197409571795
	data : 0.1149357795715332
	model : 0.06494016647338867
			 train-loss:  2.1673506683594472 	 ± 0.20244576565808164
	data : 0.11491513252258301
	model : 0.0649566650390625
			 train-loss:  2.1680674304898155 	 ± 0.20195360950951619
	data : 0.11496810913085938
	model : 0.06498398780822753
			 train-loss:  2.1676007103919983 	 ± 0.20135991402900655
	data : 0.11499338150024414
	model : 0.06503767967224121
			 train-loss:  2.1705258389971904 	 ± 0.20386455368780335
	data : 0.1148773193359375
	model : 0.06506190299987794
			 train-loss:  2.16899446750942 	 ± 0.20406234369925136
	data : 0.11500229835510253
	model : 0.06507768630981445
			 train-loss:  2.167716157981773 	 ± 0.2040040521130188
	data : 0.11493759155273438
	model : 0.06510190963745117
			 train-loss:  2.1658600259136844 	 ± 0.20463266697576207
	data : 0.11479830741882324
	model : 0.06509747505187988
			 train-loss:  2.1663060988149336 	 ± 0.20404659572026596
	data : 0.11491107940673828
	model : 0.06511621475219727
			 train-loss:  2.1669581899276147 	 ± 0.2035535105361601
	data : 0.11489262580871581
	model : 0.06511774063110351
			 train-loss:  2.165817580405314 	 ± 0.2034037243386049
	data : 0.11481027603149414
	model : 0.06513376235961914
			 train-loss:  2.1646212294131897 	 ± 0.20331238849024075
	data : 0.11478686332702637
	model : 0.06510038375854492
			 train-loss:  2.1666075928406148 	 ± 0.2042042212961933
	data : 0.11495728492736816
	model : 0.06509008407592773
			 train-loss:  2.165143844485283 	 ± 0.20440012204156052
	data : 0.11491174697875976
	model : 0.06507515907287598
			 train-loss:  2.164004653877353 	 ± 0.20427322704868056
	data : 0.11482100486755371
	model : 0.06511030197143555
			 train-loss:  2.1642734820460094 	 ± 0.20367034407064843
	data : 0.11482477188110352
	model : 0.0650606632232666
			 train-loss:  2.1669743887485904 	 ± 0.2059341981589075
	data : 0.11486058235168457
	model : 0.06504521369934083
			 train-loss:  2.166426906498467 	 ± 0.20542434192463238
	data : 0.11481027603149414
	model : 0.06503629684448242
			 train-loss:  2.168935668107235 	 ± 0.20730558332459925
	data : 0.11483216285705566
	model : 0.06503252983093262
			 train-loss:  2.167797952531332 	 ± 0.20719626239936298
	data : 0.11495428085327149
	model : 0.06499748229980469
			 train-loss:  2.1675075935032555 	 ± 0.2066088544307728
	data : 0.11493415832519531
	model : 0.06500802040100098
			 train-loss:  2.1669002508833293 	 ± 0.20614249587652925
	data : 0.11505775451660157
	model : 0.06501297950744629
			 train-loss:  2.1658657500024376 	 ± 0.20596862085311524
	data : 0.11496057510375976
	model : 0.06502385139465332
			 train-loss:  2.1666069998460658 	 ± 0.2055878949843608
	data : 0.11492404937744141
	model : 0.06497001647949219
			 train-loss:  2.1676500721981653 	 ± 0.20543653697585468
	data : 0.11484150886535645
	model : 0.06492929458618164
			 train-loss:  2.167943442976752 	 ± 0.20487438833531876
	data : 0.11495661735534668
	model : 0.06491122245788575
			 train-loss:  2.1691869876288266 	 ± 0.2049313906889326
	data : 0.11491513252258301
	model : 0.06491498947143555
			 train-loss:  2.1697836744374244 	 ± 0.2044923167756825
	data : 0.11501216888427734
	model : 0.06489777565002441
			 train-loss:  2.1703214386531284 	 ± 0.2040305659473431
	data : 0.1149810791015625
	model : 0.06496753692626953
			 train-loss:  2.17256198823452 	 ± 0.20559780484546683
	data : 0.11507139205932618
	model : 0.06497454643249512
			 train-loss:  2.1710607884293895 	 ± 0.20598124906039328
	data : 0.11498098373413086
	model : 0.0649958610534668
			 train-loss:  2.172741742616289 	 ± 0.20661569713930322
	data : 0.11493749618530273
	model : 0.0649754524230957
			 train-loss:  2.1735039356700536 	 ± 0.20628853927031365
	data : 0.11486177444458008
	model : 0.06500263214111328
			 train-loss:  2.1740705887476603 	 ± 0.205854368702784
	data : 0.11491408348083496
	model : 0.06497778892517089
			 train-loss:  2.1764890344103396 	 ± 0.20783334127759254
	data : 0.11499781608581543
	model : 0.06500663757324218
			 train-loss:  2.176442878586905 	 ± 0.20726251430896203
	data : 0.11492547988891602
	model : 0.0650320053100586
			 train-loss:  2.176773247171621 	 ± 0.20674349357211366
	data : 0.11487855911254882
	model : 0.0650627613067627
			 train-loss:  2.1752065510853478 	 ± 0.207267349379913
	data : 0.1149531364440918
	model : 0.065069580078125
			 train-loss:  2.1746849369358374 	 ± 0.20682746963327608
	data : 0.1147702693939209
	model : 0.06556935310363769
			 train-loss:  2.1755336061600716 	 ± 0.20659346404225634
	data : 0.11406097412109376
	model : 0.06605148315429688
			 train-loss:  2.176700815160007 	 ± 0.20665435294044093
	data : 0.11370134353637695
	model : 0.06601614952087402
			 train-loss:  2.1782540247795428 	 ± 0.20719554035447535
	data : 0.11375384330749512
	model : 0.06598644256591797
			 train-loss:  2.1786830589254067 	 ± 0.20673039047115094
	data : 0.113836669921875
	model : 0.0659644603729248
			 train-loss:  2.180050475973832 	 ± 0.2070408610099909
	data : 0.11391272544860839
	model : 0.06545486450195312
			 train-loss:  2.1798358612659716 	 ± 0.20651934664063518
	data : 0.11453628540039062
	model : 0.06526904106140137
			 train-loss:  2.181131575256586 	 ± 0.20675775425312973
	data : 0.11478734016418457
	model : 0.06525797843933105
			 train-loss:  2.182964842554201 	 ± 0.20778007502327667
	data : 0.11480679512023925
	model : 0.0652653694152832
			 train-loss:  2.182466350879866 	 ± 0.20735954268376228
	data : 0.11466960906982422
	model : 0.06524600982666015
			 train-loss:  2.181900729888525 	 ± 0.20697715656398405
	data : 0.11461095809936524
	model : 0.06524457931518554
			 train-loss:  2.1825949841616104 	 ± 0.20667598335269408
	data : 0.11471447944641114
	model : 0.06493544578552246
			 train-loss:  2.184466594366858 	 ± 0.20780930712815388
	data : 0.11496539115905761
	model : 0.06493930816650391
			 train-loss:  2.183202433465707 	 ± 0.20804189431703532
	data : 0.11507244110107422
	model : 0.06500825881958008
			 train-loss:  2.1829733674849696 	 ± 0.20754354837289987
	data : 0.11509451866149903
	model : 0.06501317024230957
			 train-loss:  2.1821918469667434 	 ± 0.2073173821429826
	data : 0.1152029037475586
	model : 0.06550860404968262
			 train-loss:  2.1821863064125404 	 ± 0.20680103906911365
	data : 0.11505913734436035
	model : 0.06555819511413574
			 train-loss:  2.182799692791287 	 ± 0.20647173749913122
	data : 0.11498990058898925
	model : 0.0655677318572998
			 train-loss:  2.184761800789481 	 ± 0.20784188420919059
	data : 0.11489601135253906
	model : 0.06571578979492188
			 train-loss:  2.186227509204079 	 ± 0.20838089606462898
	data : 0.11474366188049316
	model : 0.0656895637512207
			 train-loss:  2.185647192815455 	 ± 0.20803721009680104
	data : 0.11471066474914551
	model : 0.06517844200134278
			 train-loss:  2.189113763350885 	 ± 0.21338438094141624
	data : 0.11469826698303223
	model : 0.06512532234191895
			 train-loss:  2.1891134562699692 	 ± 0.21286833577950398
	data : 0.11468830108642578
	model : 0.06564664840698242
			 train-loss:  2.188701294362545 	 ± 0.21243879680978184
	data : 0.11414241790771484
	model : 0.06541857719421387
			 train-loss:  2.1879360350695523 	 ± 0.21221714715653733
	data : 0.11434388160705566
	model : 0.06546759605407715
			 train-loss:  2.1887884213810875 	 ± 0.21206959091602168
	data : 0.11439781188964844
	model : 0.06548409461975098
			 train-loss:  2.1875910696825147 	 ± 0.21227678515667817
	data : 0.11444406509399414
	model : 0.06555180549621582
			 train-loss:  2.1884311746876195 	 ± 0.2121268450934705
	data : 0.11454672813415527
	model : 0.0650519847869873
			 train-loss:  2.1890439533851516 	 ± 0.21181630404432417
	data : 0.11520605087280274
	model : 0.06504740715026855
			 train-loss:  2.189944759150532 	 ± 0.21172938113735273
	data : 0.11512365341186523
	model : 0.06501555442810059
			 train-loss:  2.1899081002834233 	 ± 0.21123709406732855
	data : 0.11504144668579101
	model : 0.06507611274719238
			 train-loss:  2.189984787945394 	 ± 0.2107505518662413
	data : 0.11488490104675293
	model : 0.06498031616210938
			 train-loss:  2.1917561116855815 	 ± 0.21186984918022483
	data : 0.11482267379760742
	model : 0.06494998931884766
			 train-loss:  2.191555302624309 	 ± 0.21140404745952768
	data : 0.11486139297485351
	model : 0.06514487266540528
			 train-loss:  2.18972702494495 	 ± 0.2126412151145285
	data : 0.11478519439697266
	model : 0.06525187492370606
			 train-loss:  2.1892641040411864 	 ± 0.21226796405280526
	data : 0.11473374366760254
	model : 0.06514368057250977
			 train-loss:  2.1884056750465843 	 ± 0.21216956825597935
	data : 0.1148984432220459
	model : 0.06509838104248047
			 train-loss:  2.188172691040211 	 ± 0.21171950187878968
	data : 0.11495909690856934
	model : 0.06504311561584472
			 train-loss:  2.189141249442849 	 ± 0.21173662234029259
	data : 0.1147313117980957
	model : 0.06474323272705078
			 train-loss:  2.1897225693932603 	 ± 0.21144174527003418
	data : 0.1148444652557373
	model : 0.06447257995605468
			 train-loss:  2.1898468075858224 	 ± 0.21097954549062248
	data : 0.11516265869140625
	model : 0.06429977416992187
			 train-loss:  2.18929258964758 	 ± 0.21067634341535865
	data : 0.11530685424804688
	model : 0.06415271759033203
			 train-loss:  2.189269192418338 	 ± 0.21021208068738598
	data : 0.1153027057647705
	model : 0.06403565406799316
			 train-loss:  2.1879983591405967 	 ± 0.21062268598363978
	data : 0.1156301498413086
	model : 0.06397371292114258
			 train-loss:  2.187643467078563 	 ± 0.2102306160483456
	data : 0.11592755317687989
	model : 0.06393351554870605
			 train-loss:  2.1882865174956945 	 ± 0.2099986812019669
	data : 0.11582379341125489
	model : 0.06392974853515625
			 train-loss:  2.187351994700246 	 ± 0.210022393973029
	data : 0.11574206352233887
	model : 0.0639488697052002
			 train-loss:  2.188910598384923 	 ± 0.21090385230784914
	data : 0.11585412025451661
	model : 0.06388773918151855
			 train-loss:  2.190128982322922 	 ± 0.2112674285110275
	data : 0.11580042839050293
	model : 0.0638503074645996
			 train-loss:  2.1884884314659314 	 ± 0.21229762347306766
	data : 0.11569952964782715
	model : 0.0638343334197998
			 train-loss:  2.188960943830774 	 ± 0.21196871750233964
	data : 0.11570701599121094
	model : 0.0638397216796875
			 train-loss:  2.1881292603783686 	 ± 0.21190304764373624
	data : 0.11575455665588379
	model : 0.06386213302612305
			 train-loss:  2.1880583219890353 	 ± 0.21145833039411932
	data : 0.11561455726623535
	model : 0.06385788917541504
			 train-loss:  2.1877391298278037 	 ± 0.211070830142512
	data : 0.1154484748840332
	model : 0.06390461921691895
			 train-loss:  2.1865734305840654 	 ± 0.21139512137752772
	data : 0.11556143760681152
	model : 0.06392474174499511
			 train-loss:  2.1870078653097154 	 ± 0.21106114062951914
	data : 0.11587314605712891
	model : 0.06389446258544922
			 train-loss:  2.1863182877109257 	 ± 0.21089354592424941
	data : 0.11597380638122559
	model : 0.06387710571289062
			 train-loss:  2.1874723050219953 	 ± 0.211218501583223
	data : 0.11612334251403808
	model : 0.06389927864074707
			 train-loss:  2.187374506467654 	 ± 0.21078893801678733
	data : 0.11620826721191406
	model : 0.06387896537780761
			 train-loss:  2.186899803700994 	 ± 0.2104866656263587
	data : 0.11618986129760742
	model : 0.06389179229736328
			 train-loss:  2.187107174737113 	 ± 0.21008163609901567
	data : 0.11594657897949219
	model : 0.06391901969909668
			 train-loss:  2.1865687612595597 	 ± 0.20982351858263668
	data : 0.11583137512207031
	model : 0.06390171051025391
			 train-loss:  2.185064508364751 	 ± 0.21072330077577867
	data : 0.11568865776062012
	model : 0.06392269134521485
			 train-loss:  2.184472339287881 	 ± 0.21050385724744575
	data : 0.11585936546325684
	model : 0.063934326171875
			 train-loss:  2.186272046173433 	 ± 0.21198389776533658
	data : 0.11583948135375977
	model : 0.06388015747070312
			 train-loss:  2.1875964102745056 	 ± 0.21258917034486485
	data : 0.11586165428161621
	model : 0.0638885498046875
			 train-loss:  2.1887381575497025 	 ± 0.21293190381484475
	data : 0.11582889556884765
	model : 0.0638892650604248
			 train-loss:  2.189249133779889 	 ± 0.21266313812734186
	data : 0.11591958999633789
	model : 0.06390619277954102
			 train-loss:  2.188697414907071 	 ± 0.21242306885368595
	data : 0.11576809883117675
	model : 0.0639047622680664
			 train-loss:  2.1887644121027368 	 ± 0.21200717910346853
	data : 0.11562299728393555
	model : 0.06391382217407227
			 train-loss:  2.1884803804696773 	 ± 0.21163948653740328
	data : 0.1155557632446289
	model : 0.06389565467834472
			 train-loss:  2.1916964636184275 	 ± 0.21737943788123065
	data : 0.11547927856445313
	model : 0.055463647842407225
#epoch  22    val-loss:  2.3809015374434623  train-loss:  2.1916964636184275  lr:  0.00015625
			 train-loss:  2.3330459594726562 	 ± 0.0
	data : 5.672550439834595
	model : 0.07785463333129883
			 train-loss:  2.2633315324783325 	 ± 0.06971442699432373
	data : 2.894739866256714
	model : 0.07149374485015869
			 train-loss:  2.286160151163737 	 ± 0.06543973728756194
	data : 1.967905839284261
	model : 0.06914377212524414
			 train-loss:  2.3679861426353455 	 ± 0.15263763642409456
	data : 1.5045966506004333
	model : 0.06800246238708496
			 train-loss:  2.316124200820923 	 ± 0.17145624070752713
	data : 1.2266468048095702
	model : 0.06732640266418458
			 train-loss:  2.3703556458155313 	 ± 0.19799734896116036
	data : 0.11510825157165527
	model : 0.0647036075592041
			 train-loss:  2.3610689640045166 	 ± 0.18471595354348966
	data : 0.1147007942199707
	model : 0.06463804244995117
			 train-loss:  2.285922884941101 	 ± 0.2634075130800776
	data : 0.11469817161560059
	model : 0.06473832130432129
			 train-loss:  2.2743060853746204 	 ± 0.25050716243748644
	data : 0.11475906372070313
	model : 0.06479125022888184
			 train-loss:  2.237441349029541 	 ± 0.2621250344041235
	data : 0.1146669864654541
	model : 0.06481351852416992
			 train-loss:  2.2020453214645386 	 ± 0.2738466725984542
	data : 0.1145474910736084
	model : 0.06486907005310058
			 train-loss:  2.2041808466116586 	 ± 0.26228387654664365
	data : 0.1145810604095459
	model : 0.06489911079406738
			 train-loss:  2.2028788878367496 	 ± 0.25203455468731994
	data : 0.11477055549621581
	model : 0.06490983963012695
			 train-loss:  2.1967717494283403 	 ± 0.2438627390898775
	data : 0.11477203369140625
	model : 0.0649594783782959
			 train-loss:  2.1815592527389525 	 ± 0.2423722631119356
	data : 0.11482791900634766
	model : 0.06507062911987305
			 train-loss:  2.1721046939492226 	 ± 0.23751552482785876
	data : 0.1150202751159668
	model : 0.06507806777954102
			 train-loss:  2.1894551375333 	 ± 0.2406486698763706
	data : 0.11493458747863769
	model : 0.06505661010742188
			 train-loss:  2.200346383783552 	 ± 0.2381406900062698
	data : 0.11486077308654785
	model : 0.06504330635070801
			 train-loss:  2.216230549310383 	 ± 0.24138707555047084
	data : 0.11467971801757812
	model : 0.06499729156494141
			 train-loss:  2.2051569640636446 	 ± 0.2401753437486605
	data : 0.11453542709350586
	model : 0.06493711471557617
			 train-loss:  2.2067048947016397 	 ± 0.23448934104976
	data : 0.11448063850402831
	model : 0.06489596366882325
			 train-loss:  2.21320684931495 	 ± 0.23102749871326614
	data : 0.11465744972229004
	model : 0.0648881435394287
			 train-loss:  2.2116126651349277 	 ± 0.22607304261125102
	data : 0.11471319198608398
	model : 0.06490015983581543
			 train-loss:  2.2062162905931473 	 ± 0.22282113411699672
	data : 0.11478824615478515
	model : 0.06491098403930665
			 train-loss:  2.20779360294342 	 ± 0.21845593935351137
	data : 0.11495952606201172
	model : 0.06489858627319336
			 train-loss:  2.2297874643252444 	 ± 0.24079191431483785
	data : 0.11493725776672363
	model : 0.06488409042358398
			 train-loss:  2.2288483028058654 	 ± 0.23633925529995184
	data : 0.1147031307220459
	model : 0.06493134498596191
			 train-loss:  2.231752561671393 	 ± 0.2325706670693811
	data : 0.1146728515625
	model : 0.064963960647583
			 train-loss:  2.2450921247745383 	 ± 0.2391786021035713
	data : 0.11471495628356934
	model : 0.06495809555053711
			 train-loss:  2.238225034872691 	 ± 0.23804847062862244
	data : 0.11467256546020507
	model : 0.06495952606201172
			 train-loss:  2.233249222078631 	 ± 0.2357580658568198
	data : 0.11464786529541016
	model : 0.06504178047180176
			 train-loss:  2.2308604307472706 	 ± 0.2324259631813961
	data : 0.11480193138122559
	model : 0.06503238677978515
			 train-loss:  2.2230065952647817 	 ± 0.2331494170614831
	data : 0.1147270679473877
	model : 0.06498074531555176
			 train-loss:  2.213077657362994 	 ± 0.23667093715112905
	data : 0.11477770805358886
	model : 0.0649764060974121
			 train-loss:  2.205773557935442 	 ± 0.2371215991957583
	data : 0.11467351913452148
	model : 0.06494231224060058
			 train-loss:  2.211724751525455 	 ± 0.23644108360952704
	data : 0.11466856002807617
	model : 0.06487827301025391
			 train-loss:  2.207346349149137 	 ± 0.23469893764540037
	data : 0.11470608711242676
	model : 0.06488533020019531
			 train-loss:  2.200428445088236 	 ± 0.23538212934074046
	data : 0.11493892669677734
	model : 0.06489076614379882
			 train-loss:  2.2204971405176015 	 ± 0.2632274846141779
	data : 0.11498680114746093
	model : 0.06491279602050781
			 train-loss:  2.213388466835022 	 ± 0.26368027487734114
	data : 0.11507682800292969
	model : 0.06497259140014648
			 train-loss:  2.213052708928178 	 ± 0.2604534683925327
	data : 0.11513090133666992
	model : 0.06498422622680664
			 train-loss:  2.2050440283048722 	 ± 0.2623939094185211
	data : 0.11506996154785157
	model : 0.0650144100189209
			 train-loss:  2.2064590537270834 	 ± 0.25948696358709
	data : 0.1148298740386963
	model : 0.06503057479858398
			 train-loss:  2.2109266492453488 	 ± 0.2581887534898954
	data : 0.11468749046325684
	model : 0.06500053405761719
			 train-loss:  2.210915478070577 	 ± 0.2553038831560753
	data : 0.11467471122741699
	model : 0.06499309539794922
			 train-loss:  2.2107014578321706 	 ± 0.25251767435373873
	data : 0.11468520164489746
	model : 0.06497993469238281
			 train-loss:  2.2128353803715806 	 ± 0.2502357623526876
	data : 0.11457209587097168
	model : 0.06491079330444335
			 train-loss:  2.213809164861838 	 ± 0.24770539851612466
	data : 0.11457943916320801
	model : 0.06490092277526856
			 train-loss:  2.2078495901458117 	 ± 0.2486172853524682
	data : 0.11467318534851074
	model : 0.06493306159973145
			 train-loss:  2.205267643928528 	 ± 0.24678127836545538
	data : 0.11475229263305664
	model : 0.06491508483886718
			 train-loss:  2.19969469425725 	 ± 0.24750707163923252
	data : 0.11462512016296386
	model : 0.06493029594421387
			 train-loss:  2.211084498808934 	 ± 0.2582591497318385
	data : 0.11470370292663574
	model : 0.06492314338684083
			 train-loss:  2.2065174489651085 	 ± 0.25792237724642325
	data : 0.1147761344909668
	model : 0.06491761207580567
			 train-loss:  2.2039963845853454 	 ± 0.25618134829042616
	data : 0.11484065055847167
	model : 0.06491532325744628
			 train-loss:  2.202287435531616 	 ± 0.25415219487691054
	data : 0.11482090950012207
	model : 0.06493058204650878
			 train-loss:  2.2006852669375285 	 ± 0.252152865589127
	data : 0.11488018035888672
	model : 0.06492815017700196
			 train-loss:  2.207217634769908 	 ± 0.2546669054862015
	data : 0.11484484672546387
	model : 0.06496334075927734
			 train-loss:  2.205225504677871 	 ± 0.25290956540455534
	data : 0.11479568481445312
	model : 0.06496119499206543
			 train-loss:  2.2015241849220404 	 ± 0.25233650540747876
	data : 0.11473031044006347
	model : 0.06501727104187012
			 train-loss:  2.1997940222422283 	 ± 0.25057752840107034
	data : 0.11490340232849121
	model : 0.06496543884277343
			 train-loss:  2.1991672046848985 	 ± 0.24856255205372155
	data : 0.11483631134033204
	model : 0.06493244171142579
			 train-loss:  2.1971207011130547 	 ± 0.247067431183881
	data : 0.11486725807189942
	model : 0.06496319770812989
			 train-loss:  2.199084891213311 	 ± 0.24558621410539103
	data : 0.1150174617767334
	model : 0.06496233940124511
			 train-loss:  2.2022836841642857 	 ± 0.24497925988134672
	data : 0.1149993896484375
	model : 0.06489710807800293
			 train-loss:  2.200993435199444 	 ± 0.24330654756615472
	data : 0.11480622291564942
	model : 0.06493949890136719
			 train-loss:  2.2018363656419697 	 ± 0.24155189940109348
	data : 0.11490716934204101
	model : 0.06498241424560547
			 train-loss:  2.1978596270974005 	 ± 0.24190952491407725
	data : 0.11501049995422363
	model : 0.06492319107055664
			 train-loss:  2.1996928295668434 	 ± 0.24059257905447481
	data : 0.11491284370422364
	model : 0.0648869514465332
			 train-loss:  2.1995462110077124 	 ± 0.2388458517649237
	data : 0.11493592262268067
	model : 0.06485428810119628
			 train-loss:  2.195320040839059 	 ± 0.23971807269935586
	data : 0.114902925491333
	model : 0.06487116813659669
			 train-loss:  2.208329210818653 	 ± 0.26172903333414144
	data : 0.11495785713195801
	model : 0.06486611366271973
			 train-loss:  2.217297818925646 	 ± 0.27066883851730117
	data : 0.11491775512695312
	model : 0.06487312316894531
			 train-loss:  2.216769306627038 	 ± 0.26884595538094996
	data : 0.11485981941223145
	model : 0.06492791175842286
			 train-loss:  2.2149753957181364 	 ± 0.26746277963890414
	data : 0.1147674560546875
	model : 0.06496548652648926
			 train-loss:  2.218081448872884 	 ± 0.2670139332402253
	data : 0.11482610702514648
	model : 0.06495952606201172
			 train-loss:  2.215813250918137 	 ± 0.2659777878541071
	data : 0.11472053527832031
	model : 0.06498713493347168
			 train-loss:  2.21634629484895 	 ± 0.26428587234929607
	data : 0.11472282409667969
	model : 0.06499829292297363
			 train-loss:  2.214957451209044 	 ± 0.26286892541868945
	data : 0.11469144821166992
	model : 0.06498970985412597
			 train-loss:  2.216133223304266 	 ± 0.2614062314047922
	data : 0.11478223800659179
	model : 0.06497039794921874
			 train-loss:  2.2123342558741568 	 ± 0.2619526535918413
	data : 0.11475028991699218
	model : 0.06496086120605468
			 train-loss:  2.2108698582943576 	 ± 0.26065992982829295
	data : 0.11481180191040039
	model : 0.06490154266357422
			 train-loss:  2.2106551847806792 	 ± 0.2590728689082971
	data : 0.11471781730651856
	model : 0.06491475105285645
			 train-loss:  2.2089792001678283 	 ± 0.25795430516187806
	data : 0.11478667259216309
	model : 0.06493449211120605
			 train-loss:  2.2071939210096994 	 ± 0.25692959162178536
	data : 0.11495461463928222
	model : 0.06497015953063964
			 train-loss:  2.207870944808511 	 ± 0.2554891309629946
	data : 0.11500186920166015
	model : 0.06498656272888184
			 train-loss:  2.2033779468647268 	 ± 0.2573549796315925
	data : 0.11508126258850097
	model : 0.06506214141845704
			 train-loss:  2.200841359708501 	 ± 0.2569506772655937
	data : 0.11512641906738282
	model : 0.06509261131286621
			 train-loss:  2.2011097601868888 	 ± 0.25549882420099324
	data : 0.11513671875
	model : 0.06510868072509765
			 train-loss:  2.200970643022087 	 ± 0.2540627345351576
	data : 0.11494994163513184
	model : 0.06506400108337403
			 train-loss:  2.199987656540341 	 ± 0.2528174674451575
	data : 0.11500277519226074
	model : 0.0650205135345459
			 train-loss:  2.200579896078005 	 ± 0.25148729217674376
	data : 0.1149360179901123
	model : 0.06497864723205567
			 train-loss:  2.195942107750022 	 ± 0.2539994674779796
	data : 0.1149942398071289
	model : 0.06498360633850098
			 train-loss:  2.1958374092655797 	 ± 0.25263218409557375
	data : 0.1149399757385254
	model : 0.06497187614440918
			 train-loss:  2.192600431594443 	 ± 0.2532163273005454
	data : 0.11499004364013672
	model : 0.0650301456451416
			 train-loss:  2.1945966181002166 	 ± 0.2526225331146748
	data : 0.11489701271057129
	model : 0.06505694389343261
			 train-loss:  2.193535684297482 	 ± 0.25151600754659637
	data : 0.11500492095947265
	model : 0.06505513191223145
			 train-loss:  2.197577954567585 	 ± 0.2533313424898215
	data : 0.11481690406799316
	model : 0.06499838829040527
			 train-loss:  2.196887017512808 	 ± 0.2521273712160777
	data : 0.1147428035736084
	model : 0.06494064331054687
			 train-loss:  2.1937320364846125 	 ± 0.2527876399074357
	data : 0.11479463577270507
	model : 0.06494641304016113
			 train-loss:  2.1933333337306977 	 ± 0.2515518085917563
	data : 0.1148900032043457
	model : 0.06496000289916992
			 train-loss:  2.191548525696934 	 ± 0.2509389336729369
	data : 0.11477551460266114
	model : 0.06500816345214844
			 train-loss:  2.191594136696236 	 ± 0.2497062317853005
	data : 0.11499242782592774
	model : 0.0650291919708252
			 train-loss:  2.194010668587916 	 ± 0.24968675012595293
	data : 0.11507134437561035
	model : 0.0650754451751709
			 train-loss:  2.193529927959809 	 ± 0.2485313284181693
	data : 0.1149979591369629
	model : 0.06503987312316895
			 train-loss:  2.1934141874313355 	 ± 0.24734783082111875
	data : 0.1149759292602539
	model : 0.06503119468688964
			 train-loss:  2.1923126420884764 	 ± 0.24643696530517742
	data : 0.11510705947875977
	model : 0.06498041152954101
			 train-loss:  2.1917013219583814 	 ± 0.24536342488480653
	data : 0.1150907039642334
	model : 0.06498112678527831
			 train-loss:  2.189647490227664 	 ± 0.24514714599668141
	data : 0.11509466171264648
	model : 0.06495819091796876
			 train-loss:  2.1889583725448047 	 ± 0.24412509256487372
	data : 0.11511869430541992
	model : 0.06497430801391602
			 train-loss:  2.1878054803067988 	 ± 0.24331080504915412
	data : 0.11513919830322265
	model : 0.06495070457458496
			 train-loss:  2.195772303117288 	 ± 0.25621971511698666
	data : 0.11513991355895996
	model : 0.06494779586791992
			 train-loss:  2.197345687874726 	 ± 0.2556113839591994
	data : 0.11513762474060059
	model : 0.0649646282196045
			 train-loss:  2.198641533345248 	 ± 0.254847104386921
	data : 0.115087890625
	model : 0.06499834060668945
			 train-loss:  2.1986821001036123 	 ± 0.25372725836806953
	data : 0.11501874923706054
	model : 0.06498661041259765
			 train-loss:  2.1981001615524294 	 ± 0.25269808761722806
	data : 0.1150707721710205
	model : 0.06500406265258789
			 train-loss:  2.199306351357493 	 ± 0.2519387836963334
	data : 0.11483674049377442
	model : 0.06502265930175781
			 train-loss:  2.1988845798704357 	 ± 0.25090093771509864
	data : 0.11466035842895508
	model : 0.06500239372253418
			 train-loss:  2.1971683845681658 	 ± 0.2505242469240745
	data : 0.11475944519042969
	model : 0.06502585411071778
			 train-loss:  2.1967334787384805 	 ± 0.2495141320018931
	data : 0.11487717628479004
	model : 0.06507244110107421
			 train-loss:  2.1960606733957926 	 ± 0.24858068829769284
	data : 0.11484661102294921
	model : 0.06508517265319824
			 train-loss:  2.196874058936253 	 ± 0.24771166599396277
	data : 0.1148447036743164
	model : 0.06510977745056153
			 train-loss:  2.1944564532061093 	 ± 0.2481236240206206
	data : 0.11495094299316407
	model : 0.06512079238891602
			 train-loss:  2.1947444328447667 	 ± 0.24713340400154807
	data : 0.11498746871948243
	model : 0.06509699821472167
			 train-loss:  2.1934554067350205 	 ± 0.24654970100033402
	data : 0.11490154266357422
	model : 0.06506919860839844
			 train-loss:  2.1933179693222047 	 ± 0.24556629097681548
	data : 0.11469054222106934
	model : 0.06506476402282715
			 train-loss:  2.1933258251538352 	 ± 0.24458989615614143
	data : 0.11484169960021973
	model : 0.06503314971923828
			 train-loss:  2.1941086485629944 	 ± 0.24378345892667927
	data : 0.11485853195190429
	model : 0.06501650810241699
			 train-loss:  2.1949911760166287 	 ± 0.24303289816501106
	data : 0.11489238739013671
	model : 0.06500358581542968
			 train-loss:  2.200003059335457 	 ± 0.24864100662908142
	data : 0.11492958068847656
	model : 0.0649984359741211
			 train-loss:  2.196626988741068 	 ± 0.25063343079209666
	data : 0.1149970531463623
	model : 0.06503820419311523
			 train-loss:  2.1955185582619587 	 ± 0.24999463396811045
	data : 0.1149223804473877
	model : 0.06510162353515625
			 train-loss:  2.1967096825440726 	 ± 0.2494187488628064
	data : 0.11494913101196289
	model : 0.06511335372924805
			 train-loss:  2.1977083656124603 	 ± 0.24874409067482312
	data : 0.11476826667785645
	model : 0.06512570381164551
			 train-loss:  2.1970397385198677 	 ± 0.24793414138475428
	data : 0.11467390060424805
	model : 0.06509938240051269
			 train-loss:  2.1953689327946417 	 ± 0.24777019294839833
	data : 0.11478176116943359
	model : 0.06509733200073242
			 train-loss:  2.1956659222350403 	 ± 0.24688170916316696
	data : 0.11488337516784668
	model : 0.06503543853759766
			 train-loss:  2.198651613110173 	 ± 0.24843114778006567
	data : 0.11487178802490235
	model : 0.06502442359924317
			 train-loss:  2.1986901051756265 	 ± 0.24752980837232386
	data : 0.11496176719665527
	model : 0.06503133773803711
			 train-loss:  2.1968512423604514 	 ± 0.24758199227489316
	data : 0.11505093574523925
	model : 0.06507315635681152
			 train-loss:  2.194884988239833 	 ± 0.24778297739909957
	data : 0.11506948471069336
	model : 0.06503815650939941
			 train-loss:  2.198685644366217 	 ± 0.25096467197383066
	data : 0.11507234573364258
	model : 0.0650050163269043
			 train-loss:  2.1982721862658647 	 ± 0.25012761954610324
	data : 0.11496086120605468
	model : 0.06500897407531739
			 train-loss:  2.1958793418390767 	 ± 0.2508771946962694
	data : 0.11500759124755859
	model : 0.06501297950744629
			 train-loss:  2.192508806784948 	 ± 0.25323277891939305
	data : 0.11503133773803711
	model : 0.0650099277496338
			 train-loss:  2.1927505887787917 	 ± 0.25237472986960846
	data : 0.11510224342346191
	model : 0.06499204635620118
			 train-loss:  2.193099239101149 	 ± 0.2515439854074491
	data : 0.11512622833251954
	model : 0.06504631042480469
			 train-loss:  2.194038999323942 	 ± 0.250943974729403
	data : 0.11559853553771973
	model : 0.06507763862609864
			 train-loss:  2.1959280613306404 	 ± 0.2511413220778071
	data : 0.11556029319763184
	model : 0.06505794525146484
			 train-loss:  2.1947101554614585 	 ± 0.2507352976786637
	data : 0.1155815601348877
	model : 0.06505627632141113
			 train-loss:  2.1937570587793984 	 ± 0.2501687807025071
	data : 0.11542057991027832
	model : 0.06510009765625
			 train-loss:  2.195671679957813 	 ± 0.2504392513700325
	data : 0.11529421806335449
	model : 0.06508927345275879
			 train-loss:  2.1958357315314445 	 ± 0.24962221874628918
	data : 0.11485867500305176
	model : 0.06502017974853516
			 train-loss:  2.1958438633314143 	 ± 0.24880514274715212
	data : 0.1148834228515625
	model : 0.06505608558654785
			 train-loss:  2.1959591097645945 	 ± 0.24800011520935325
	data : 0.11488962173461914
	model : 0.0650406837463379
			 train-loss:  2.1965698396005937 	 ± 0.24731497605037941
	data : 0.115057373046875
	model : 0.0650094985961914
			 train-loss:  2.197648300574376 	 ± 0.24688639712730517
	data : 0.11507091522216797
	model : 0.06501312255859375
			 train-loss:  2.196670146504785 	 ± 0.2464019411826359
	data : 0.11506471633911133
	model : 0.06508049964904786
			 train-loss:  2.1966499527798424 	 ± 0.2456210808257338
	data : 0.11520233154296874
	model : 0.0650454044342041
			 train-loss:  2.195626272345489 	 ± 0.2451853478326536
	data : 0.11542758941650391
	model : 0.06504511833190918
			 train-loss:  2.194843605160713 	 ± 0.24461710692220262
	data : 0.11536293029785157
	model : 0.06505327224731446
			 train-loss:  2.1949746075624263 	 ± 0.24386187317591876
	data : 0.11530232429504395
	model : 0.06503376960754395
			 train-loss:  2.194362066410206 	 ± 0.243232257845804
	data : 0.11532530784606934
	model : 0.06499085426330567
			 train-loss:  2.1945897962418073 	 ± 0.242502321814112
	data : 0.11517200469970704
	model : 0.06494693756103516
			 train-loss:  2.193360942166026 	 ± 0.2422703818981773
	data : 0.11496534347534179
	model : 0.06496620178222656
			 train-loss:  2.1959559801853064 	 ± 0.24381063042693507
	data : 0.11500058174133301
	model : 0.06497373580932617
			 train-loss:  2.1949087869690125 	 ± 0.2434470592161177
	data : 0.11503996849060058
	model : 0.06499013900756836
			 train-loss:  2.1939859004791624 	 ± 0.24300816247147763
	data : 0.11510148048400878
	model : 0.06502656936645508
			 train-loss:  2.1949418499356224 	 ± 0.24259858294169515
	data : 0.11510977745056153
	model : 0.06503839492797851
			 train-loss:  2.1943767056662655 	 ± 0.24199066239739367
	data : 0.11494569778442383
	model : 0.06501998901367187
			 train-loss:  2.1940010589711805 	 ± 0.24132728983344087
	data : 0.11482729911804199
	model : 0.0649810791015625
			 train-loss:  2.197215656090898 	 ± 0.24424373190333712
	data : 0.1148867130279541
	model : 0.06493568420410156
			 train-loss:  2.197107755860617 	 ± 0.24353677347782732
	data : 0.11505956649780273
	model : 0.0649528980255127
			 train-loss:  2.1985615677916246 	 ± 0.24357926970797847
	data : 0.11521472930908203
	model : 0.06499214172363281
			 train-loss:  2.197048491445081 	 ± 0.24369231600338004
	data : 0.11532878875732422
	model : 0.06498804092407226
			 train-loss:  2.1967313425881523 	 ± 0.24303106413336506
	data : 0.11535286903381348
	model : 0.065020751953125
			 train-loss:  2.1976021094755693 	 ± 0.24261326795388863
	data : 0.11528739929199219
	model : 0.06505990028381348
			 train-loss:  2.197891048118893 	 ± 0.24195731462841735
	data : 0.11513586044311523
	model : 0.06505308151245118
			 train-loss:  2.198905244302214 	 ± 0.24165369445104287
	data : 0.11492209434509278
	model : 0.06502013206481934
			 train-loss:  2.1985911363995942 	 ± 0.24101417534784716
	data : 0.11486024856567383
	model : 0.06502528190612793
			 train-loss:  2.1975344353251987 	 ± 0.24075920960284217
	data : 0.11485605239868164
	model : 0.06503176689147949
			 train-loss:  2.198910212648508 	 ± 0.24080167314644538
	data : 0.11499972343444824
	model : 0.06503238677978515
			 train-loss:  2.1983824457441057 	 ± 0.24024416704783166
	data : 0.11497645378112793
	model : 0.06501779556274415
			 train-loss:  2.197439037385534 	 ± 0.2399246724629519
	data : 0.11502742767333984
	model : 0.06502881050109863
			 train-loss:  2.1985309940317403 	 ± 0.239727355633479
	data : 0.11508455276489257
	model : 0.06500396728515626
			 train-loss:  2.198435641623832 	 ± 0.23908206463574366
	data : 0.11517081260681153
	model : 0.06498947143554687
			 train-loss:  2.197301508918885 	 ± 0.23893697514209594
	data : 0.11492629051208496
	model : 0.06497039794921874
			 train-loss:  2.1961883230005355 	 ± 0.23878037459987742
	data : 0.11484675407409668
	model : 0.0649794101715088
			 train-loss:  2.1954244026478302 	 ± 0.2383734858063139
	data : 0.11486959457397461
	model : 0.06509156227111816
			 train-loss:  2.1954403156956666 	 ± 0.2377421318763163
	data : 0.11494731903076172
	model : 0.06510252952575683
			 train-loss:  2.195985506082836 	 ± 0.23723409869651715
	data : 0.1149836540222168
	model : 0.06512560844421386
			 train-loss:  2.1965336556209945 	 ± 0.23673285938055819
	data : 0.1151050090789795
	model : 0.06549510955810547
			 train-loss:  2.194747790073355 	 ± 0.23740201826495097
	data : 0.11479401588439941
	model : 0.06556377410888672
			 train-loss:  2.193535587948221 	 ± 0.23738119204581146
	data : 0.11484856605529785
	model : 0.06548738479614258
			 train-loss:  2.193805299468876 	 ± 0.23679824100879618
	data : 0.11480169296264649
	model : 0.06551222801208496
			 train-loss:  2.1934942263823287 	 ± 0.23623002287188885
	data : 0.11465439796447754
	model : 0.06550107002258301
			 train-loss:  2.1931379309722354 	 ± 0.23567914797435846
	data : 0.11461372375488281
	model : 0.06512198448181153
			 train-loss:  2.194740107216811 	 ± 0.23614790718094214
	data : 0.11493029594421386
	model : 0.06501207351684571
			 train-loss:  2.194048477543725 	 ± 0.23575076607609935
	data : 0.11499333381652832
	model : 0.0650186538696289
			 train-loss:  2.194061512324079 	 ± 0.2351577529774906
	data : 0.11501469612121581
	model : 0.06500310897827148
			 train-loss:  2.1956377428770066 	 ± 0.23562064635752045
	data : 0.11498417854309081
	model : 0.06498284339904785
			 train-loss:  2.1948836982546753 	 ± 0.2352755856981202
	data : 0.11497397422790527
	model : 0.06504316329956054
			 train-loss:  2.1942962531996244 	 ± 0.23484022648705724
	data : 0.11526298522949219
	model : 0.06505870819091797
			 train-loss:  2.1932513390855837 	 ± 0.2347313575705045
	data : 0.11514072418212891
	model : 0.06505637168884278
			 train-loss:  2.192689061749215 	 ± 0.23429233373694744
	data : 0.11500391960144044
	model : 0.06507148742675781
			 train-loss:  2.1939576875872726 	 ± 0.23442151642914905
	data : 0.1150132179260254
	model : 0.06506943702697754
			 train-loss:  2.1962117633773284 	 ± 0.2360683351895052
	data : 0.11516518592834472
	model : 0.06499366760253907
			 train-loss:  2.1954453630723814 	 ± 0.23575419050202578
	data : 0.11496634483337402
	model : 0.06498022079467773
			 train-loss:  2.1944451097112436 	 ± 0.23562667828851505
	data : 0.11498589515686035
	model : 0.06496138572692871
			 train-loss:  2.1947339065907676 	 ± 0.23509920006784352
	data : 0.11500706672668456
	model : 0.0649254322052002
			 train-loss:  2.193559837908972 	 ± 0.23515214012495148
	data : 0.11503043174743652
	model : 0.06493186950683594
			 train-loss:  2.1921655086544454 	 ± 0.23546280492299757
	data : 0.11501684188842773
	model : 0.06500329971313476
			 train-loss:  2.1912046316659675 	 ± 0.23532110663290715
	data : 0.11490707397460938
	model : 0.0650634765625
			 train-loss:  2.1916218469960027 	 ± 0.23484664027218877
	data : 0.11469268798828125
	model : 0.06502971649169922
			 train-loss:  2.1914019100019866 	 ± 0.23431927727693896
	data : 0.11485605239868164
	model : 0.06505250930786133
			 train-loss:  2.191839737670366 	 ± 0.2338614365198197
	data : 0.11503810882568359
	model : 0.06504154205322266
			 train-loss:  2.1921370884886495 	 ± 0.2333601966650761
	data : 0.11503891944885254
	model : 0.06500988006591797
			 train-loss:  2.190146270435527 	 ± 0.23465317505787575
	data : 0.11505928039550781
	model : 0.06498003005981445
			 train-loss:  2.19030882787267 	 ± 0.23412660736118632
	data : 0.11528925895690918
	model : 0.06498064994812011
			 train-loss:  2.1905642644455443 	 ± 0.2336219045285152
	data : 0.11520628929138184
	model : 0.0649688720703125
			 train-loss:  2.1913911407644098 	 ± 0.23341131652653677
	data : 0.11510381698608399
	model : 0.06500039100646973
			 train-loss:  2.1922563751358792 	 ± 0.23323597867046414
	data : 0.11489510536193848
	model : 0.06487207412719727
			 train-loss:  2.192994746001991 	 ± 0.23296881430206706
	data : 0.11500954627990723
	model : 0.06476392745971679
			 train-loss:  2.193929775947947 	 ± 0.23286299738846286
	data : 0.11503682136535645
	model : 0.06464200019836426
			 train-loss:  2.1937048009463718 	 ± 0.2323669205001262
	data : 0.11506462097167969
	model : 0.06454086303710938
			 train-loss:  2.193387409845988 	 ± 0.2318986326129191
	data : 0.11514453887939453
	model : 0.06433610916137696
			 train-loss:  2.1946482035966044 	 ± 0.23215659539567693
	data : 0.11555795669555664
	model : 0.06422438621520996
			 train-loss:  2.1947718212783074 	 ± 0.2316521272017968
	data : 0.11560602188110351
	model : 0.06410446166992187
			 train-loss:  2.1942390945919774 	 ± 0.23128287304843217
	data : 0.11555204391479493
	model : 0.06403880119323731
			 train-loss:  2.193122621707 	 ± 0.2313922719755704
	data : 0.11561951637268067
	model : 0.06394610404968262
			 train-loss:  2.1935063388036644 	 ± 0.23096170298493024
	data : 0.11576523780822753
	model : 0.06391654014587403
			 train-loss:  2.1933326664425077 	 ± 0.2304762940387946
	data : 0.1156731128692627
	model : 0.06395859718322754
			 train-loss:  2.1947586654589095 	 ± 0.23099803424819956
	data : 0.11562366485595703
	model : 0.06394948959350585
			 train-loss:  2.1949511735736045 	 ± 0.23052044666734112
	data : 0.11571359634399414
	model : 0.06391024589538574
			 train-loss:  2.1935686846064706 	 ± 0.2309933136396458
	data : 0.11568336486816407
	model : 0.06389079093933106
			 train-loss:  2.194267906533911 	 ± 0.230749347238772
	data : 0.11549339294433594
	model : 0.06388401985168457
			 train-loss:  2.1941910603288877 	 ± 0.23026296595862233
	data : 0.11542482376098633
	model : 0.06383671760559081
			 train-loss:  2.193747498818088 	 ± 0.22987768116813473
	data : 0.11552472114562988
	model : 0.06385278701782227
			 train-loss:  2.193656433029335 	 ± 0.22939852043794287
	data : 0.11553077697753907
	model : 0.06387228965759277
			 train-loss:  2.192969357618228 	 ± 0.22916337337447235
	data : 0.1154594898223877
	model : 0.06391391754150391
			 train-loss:  2.192523210744063 	 ± 0.22878944007230353
	data : 0.1155693531036377
	model : 0.06390552520751953
			 train-loss:  2.1927154079017797 	 ± 0.22833369415151197
	data : 0.11554460525512696
	model : 0.06392369270324708
			 train-loss:  2.1935521044021797 	 ± 0.2282313550708325
	data : 0.11550741195678711
	model : 0.06392149925231934
			 train-loss:  2.1931578460544223 	 ± 0.22784382268897022
	data : 0.11567196846008301
	model : 0.06395497322082519
			 train-loss:  2.1926005897951906 	 ± 0.22754232578231745
	data : 0.11587624549865723
	model : 0.06392269134521485
			 train-loss:  2.193280458936886 	 ± 0.22732567736674228
	data : 0.11590023040771484
	model : 0.0639373779296875
			 train-loss:  2.193988888728909 	 ± 0.22713399891092864
	data : 0.11601724624633789
	model : 0.06401124000549316
			 train-loss:  2.1927939233509637 	 ± 0.22744727180686775
	data : 0.11599769592285156
	model : 0.0640453815460205
			 train-loss:  2.1932687922831504 	 ± 0.22711090297062128
	data : 0.11592593193054199
	model : 0.06400585174560547
			 train-loss:  2.193351186422938 	 ± 0.22665811222187748
	data : 0.11575441360473633
	model : 0.06397933959960937
			 train-loss:  2.19208789396286 	 ± 0.22708101040977402
	data : 0.11568899154663086
	model : 0.06397700309753418
			 train-loss:  2.191965458877533 	 ± 0.22663647433853487
	data : 0.11563272476196289
	model : 0.06390299797058105
			 train-loss:  2.1935397268287717 	 ± 0.2275572981818757
	data : 0.11571478843688965
	model : 0.06385493278503418
			 train-loss:  2.1926517123761387 	 ± 0.22754421613166967
	data : 0.11552906036376953
	model : 0.06388468742370605
			 train-loss:  2.1919790717560477 	 ± 0.22734774026718893
	data : 0.11557774543762207
	model : 0.06390533447265626
			 train-loss:  2.192582574077681 	 ± 0.22710528732242907
	data : 0.11562161445617676
	model : 0.06390256881713867
			 train-loss:  2.192494106013328 	 ± 0.2266656908143785
	data : 0.11536116600036621
	model : 0.05551819801330567
#epoch  23    val-loss:  2.3899952486941687  train-loss:  2.192494106013328  lr:  0.00015625
			 train-loss:  2.1264700889587402 	 ± 0.0
	data : 5.728711843490601
	model : 0.07094216346740723
			 train-loss:  2.0917282104492188 	 ± 0.034741878509521484
	data : 2.9283305406570435
	model : 0.0678873062133789
			 train-loss:  2.0697999000549316 	 ± 0.042028169225056276
	data : 1.9904797077178955
	model : 0.06676451365152995
			 train-loss:  2.0597110986709595 	 ± 0.04037483122445457
	data : 1.5215864181518555
	model : 0.06621909141540527
			 train-loss:  2.156766986846924 	 ± 0.19744235441167063
	data : 1.2402104377746581
	model : 0.06591730117797852
			 train-loss:  2.1323008934656777 	 ± 0.18835919070502596
	data : 0.11738386154174804
	model : 0.06469054222106933
			 train-loss:  2.142047507422311 	 ± 0.176013376378996
	data : 0.11473598480224609
	model : 0.06467342376708984
			 train-loss:  2.182078629732132 	 ± 0.19576913790222217
	data : 0.11463851928710937
	model : 0.0647130012512207
			 train-loss:  2.172897868686252 	 ± 0.18639058802039735
	data : 0.11442680358886718
	model : 0.06477770805358887
			 train-loss:  2.1735995292663572 	 ± 0.17683816646996087
	data : 0.11437630653381348
	model : 0.06481590270996093
			 train-loss:  2.1799404404380103 	 ± 0.16979671219595704
	data : 0.1144322395324707
	model : 0.06482367515563965
			 train-loss:  2.2018197973569236 	 ± 0.17802841168630162
	data : 0.11445999145507812
	model : 0.0648881435394287
			 train-loss:  2.209563347009512 	 ± 0.17313479496561593
	data : 0.11449952125549316
	model : 0.0651782512664795
			 train-loss:  2.1817442093576704 	 ± 0.19466714234911373
	data : 0.11475934982299804
	model : 0.06541109085083008
			 train-loss:  2.1650589148203534 	 ± 0.19815784106658632
	data : 0.11461892127990722
	model : 0.06541910171508789
			 train-loss:  2.192904219031334 	 ± 0.2200972202270868
	data : 0.11460752487182617
	model : 0.06543431282043458
			 train-loss:  2.180896113900577 	 ± 0.21886142671847197
	data : 0.11457033157348633
	model : 0.06541123390197753
			 train-loss:  2.19002812438541 	 ± 0.21600205196178945
	data : 0.11463680267333984
	model : 0.06514334678649902
			 train-loss:  2.1836334529675936 	 ± 0.21198422973121234
	data : 0.11442937850952148
	model : 0.06494336128234864
			 train-loss:  2.164847493171692 	 ± 0.222251617307074
	data : 0.11471500396728515
	model : 0.06498589515686035
			 train-loss:  2.1592305160704113 	 ± 0.2183451624476225
	data : 0.11478223800659179
	model : 0.06500010490417481
			 train-loss:  2.1855369372801348 	 ± 0.24503094901003977
	data : 0.1148299217224121
	model : 0.06500663757324218
			 train-loss:  2.178571265676747 	 ± 0.24186189603458066
	data : 0.1147282600402832
	model : 0.06504912376403808
			 train-loss:  2.180128296216329 	 ± 0.2368872185303439
	data : 0.114849853515625
	model : 0.06501860618591308
			 train-loss:  2.1890471458435057 	 ± 0.23617796937702265
	data : 0.11479654312133789
	model : 0.0649874210357666
			 train-loss:  2.1782949475141673 	 ± 0.23774964048100922
	data : 0.11466307640075683
	model : 0.06500420570373536
			 train-loss:  2.1714593525286072 	 ± 0.23589454503233384
	data : 0.11456294059753418
	model : 0.06497993469238281
			 train-loss:  2.18512618967465 	 ± 0.2422849491935442
	data : 0.11469755172729493
	model : 0.06498222351074219
			 train-loss:  2.191864691931626 	 ± 0.24072639527250503
	data : 0.11463160514831543
	model : 0.0649810791015625
			 train-loss:  2.1849398612976074 	 ± 0.23960008883175157
	data : 0.11454782485961915
	model : 0.06503562927246094
			 train-loss:  2.1875375778444353 	 ± 0.23613294773011592
	data : 0.11458430290222169
	model : 0.06502728462219239
			 train-loss:  2.1883383691310883 	 ± 0.23245684919910573
	data : 0.1146928310394287
	model : 0.06505022048950196
			 train-loss:  2.196486913796627 	 ± 0.23350264987932629
	data : 0.11462936401367188
	model : 0.0650179386138916
			 train-loss:  2.1899360803996815 	 ± 0.2331008329366784
	data : 0.11462569236755371
	model : 0.06501483917236328
			 train-loss:  2.1853665590286253 	 ± 0.2312865735188603
	data : 0.11468391418457032
	model : 0.06493058204650878
			 train-loss:  2.18793683913019 	 ± 0.22855802547359325
	data : 0.11469368934631348
	model : 0.0648918628692627
			 train-loss:  2.18646074952306 	 ± 0.22562214118189577
	data : 0.11477875709533691
	model : 0.06490111351013184
			 train-loss:  2.1853417064014233 	 ± 0.22273766998433883
	data : 0.11473326683044434
	model : 0.06495170593261719
			 train-loss:  2.1804811159769693 	 ± 0.22189575970033065
	data : 0.11479172706604004
	model : 0.06500916481018067
			 train-loss:  2.174550396203995 	 ± 0.22221284619179593
	data : 0.11481075286865235
	model : 0.06504955291748046
			 train-loss:  2.179967897694285 	 ± 0.22214447338293947
	data : 0.11471414566040039
	model : 0.06506991386413574
			 train-loss:  2.178352270807539 	 ± 0.21972762902253634
	data : 0.11462812423706055
	model : 0.065059232711792
			 train-loss:  2.1776182485181232 	 ± 0.2172097235701509
	data : 0.11459636688232422
	model : 0.06499218940734863
			 train-loss:  2.1873567700386047 	 ± 0.2240220040346664
	data : 0.11470632553100586
	model : 0.06492228507995605
			 train-loss:  2.1850994480980765 	 ± 0.22202436600390763
	data : 0.11463122367858887
	model : 0.06493897438049316
			 train-loss:  2.1876948035281636 	 ± 0.2202868737320886
	data : 0.11480221748352051
	model : 0.0649336814880371
			 train-loss:  2.182094695720267 	 ± 0.22121583955575946
	data : 0.1147343635559082
	model : 0.0649782657623291
			 train-loss:  2.1771763985355697 	 ± 0.2214810415411503
	data : 0.11480512619018554
	model : 0.06502819061279297
			 train-loss:  2.176719147331861 	 ± 0.21923227071404586
	data : 0.1146843433380127
	model : 0.06506328582763672
			 train-loss:  2.1774648022651673 	 ± 0.2170916323000189
	data : 0.11476058959960937
	model : 0.06501312255859375
			 train-loss:  2.1901461680730185 	 ± 0.23290670898297536
	data : 0.11457591056823731
	model : 0.06496834754943848
			 train-loss:  2.1890620291233063 	 ± 0.23078625379895407
	data : 0.11452336311340332
	model : 0.06491541862487793
			 train-loss:  2.1884337123834863 	 ± 0.2286435536845571
	data : 0.11444239616394043
	model : 0.06489105224609375
			 train-loss:  2.194479419125451 	 ± 0.23075299794881737
	data : 0.11441826820373535
	model : 0.06493616104125977
			 train-loss:  2.1965796362270007 	 ± 0.22916589844804697
	data : 0.11441984176635742
	model : 0.06494321823120117
			 train-loss:  2.1967521671737944 	 ± 0.22711416173407248
	data : 0.11450257301330566
	model : 0.06500611305236817
			 train-loss:  2.1907833605481866 	 ± 0.229501651282434
	data : 0.1146230697631836
	model : 0.06503028869628906
			 train-loss:  2.1951843356264047 	 ± 0.22992802410366864
	data : 0.1147913932800293
	model : 0.06502776145935059
			 train-loss:  2.195735901089038 	 ± 0.2280098510976674
	data : 0.11470003128051758
	model : 0.06494460105895997
			 train-loss:  2.2043493926525115 	 ± 0.23558301998562783
	data : 0.11470088958740235
	model : 0.06492419242858886
			 train-loss:  2.203197012182142 	 ± 0.23381448327268364
	data : 0.11484751701354981
	model : 0.06490764617919922
			 train-loss:  2.1975497507279917 	 ± 0.2360780302623251
	data : 0.11487503051757812
	model : 0.06487579345703125
			 train-loss:  2.2003317068493558 	 ± 0.23521909685980796
	data : 0.11484241485595703
	model : 0.06489262580871583
			 train-loss:  2.204900372773409 	 ± 0.23617473146896775
	data : 0.11497602462768555
	model : 0.06491460800170898
			 train-loss:  2.201259781764104 	 ± 0.2361538116194533
	data : 0.11492280960083008
	model : 0.0649874210357666
			 train-loss:  2.2049683224071157 	 ± 0.23625750019944816
	data : 0.11482677459716797
	model : 0.06499028205871582
			 train-loss:  2.2013711573472663 	 ± 0.236301758699284
	data : 0.11465764045715332
	model : 0.06498885154724121
			 train-loss:  2.2028733912636254 	 ± 0.23487989652780603
	data : 0.11445345878601074
	model : 0.06497254371643066
			 train-loss:  2.2019213351650513 	 ± 0.23330378757435566
	data : 0.11453256607055665
	model : 0.06498703956604004
			 train-loss:  2.198210394382477 	 ± 0.23367344971337795
	data : 0.11465659141540527
	model : 0.06496701240539551
			 train-loss:  2.200189484676845 	 ± 0.23261211638967208
	data : 0.11470141410827636
	model : 0.0649958610534668
			 train-loss:  2.202135481768184 	 ± 0.2315723675526203
	data : 0.11475472450256348
	model : 0.0650136947631836
			 train-loss:  2.203606260965948 	 ± 0.23031915188885108
	data : 0.11492233276367188
	model : 0.06501574516296386
			 train-loss:  2.2050581155596554 	 ± 0.22909372958879073
	data : 0.11489052772521972
	model : 0.06497678756713868
			 train-loss:  2.199688754081726 	 ± 0.23220158110884212
	data : 0.11481246948242188
	model : 0.06495389938354493
			 train-loss:  2.2008192398046194 	 ± 0.23087655218854045
	data : 0.11473603248596191
	model : 0.06487083435058594
			 train-loss:  2.199675864987559 	 ± 0.2295889328633169
	data : 0.11479239463806153
	model : 0.06486124992370605
			 train-loss:  2.1986578412545033 	 ± 0.2282873095674839
	data : 0.1149061679840088
	model : 0.06484336853027343
			 train-loss:  2.2036383227456975 	 ± 0.23106322994519252
	data : 0.11501293182373047
	model : 0.06485686302185059
			 train-loss:  2.2089002653956413 	 ± 0.23432924214755763
	data : 0.1150209903717041
	model : 0.06485638618469239
			 train-loss:  2.2125075172494957 	 ± 0.23510268261799902
	data : 0.1149674892425537
	model : 0.06491694450378419
			 train-loss:  2.2106418973062096 	 ± 0.23426722060229507
	data : 0.11498665809631348
	model : 0.06493921279907226
			 train-loss:  2.2111371181097375 	 ± 0.2328948736790952
	data : 0.11490836143493652
	model : 0.06496920585632324
			 train-loss:  2.20962787100247 	 ± 0.23191241291298842
	data : 0.11485118865966797
	model : 0.06498560905456544
			 train-loss:  2.2074512383517098 	 ± 0.23140568502179243
	data : 0.11469469070434571
	model : 0.06505098342895507
			 train-loss:  2.205185362072878 	 ± 0.2310028996057225
	data : 0.11462860107421875
	model : 0.06506247520446777
			 train-loss:  2.2045816100876907 	 ± 0.22973969577804684
	data : 0.1146078109741211
	model : 0.06503725051879883
			 train-loss:  2.201440430500291 	 ± 0.23030193478162278
	data : 0.11468029022216797
	model : 0.0650390625
			 train-loss:  2.2061178644051713 	 ± 0.23317018611694584
	data : 0.11457366943359375
	model : 0.06508059501647949
			 train-loss:  2.208500307136112 	 ± 0.23295795767602326
	data : 0.11469035148620606
	model : 0.06503667831420898
			 train-loss:  2.210121436433478 	 ± 0.23218434124446638
	data : 0.11479992866516113
	model : 0.06503057479858398
			 train-loss:  2.21176719535952 	 ± 0.23145209070717354
	data : 0.11480026245117188
	model : 0.06503472328186036
			 train-loss:  2.210981226736499 	 ± 0.23032776803011829
	data : 0.11459908485412598
	model : 0.06504597663879394
			 train-loss:  2.210108387977519 	 ± 0.22925392356862553
	data : 0.11453723907470703
	model : 0.06498565673828124
			 train-loss:  2.213485239681445 	 ± 0.23038233453575885
	data : 0.1145925521850586
	model : 0.0649794578552246
			 train-loss:  2.2163870371878147 	 ± 0.23091791959499722
	data : 0.11463589668273926
	model : 0.06496429443359375
			 train-loss:  2.213450450258157 	 ± 0.23151937967821867
	data : 0.11464881896972656
	model : 0.0650266170501709
			 train-loss:  2.213367993734321 	 ± 0.2303365611890188
	data : 0.11470971107482911
	model : 0.06501183509826661
			 train-loss:  2.2119687940135146 	 ± 0.2295885090511934
	data : 0.11473674774169922
	model : 0.06510930061340332
			 train-loss:  2.2092606711387632 	 ± 0.23002137655973795
	data : 0.1148336410522461
	model : 0.06512327194213867
			 train-loss:  2.2085246071957125 	 ± 0.22899815053382386
	data : 0.11486773490905762
	model : 0.06514954566955566
			 train-loss:  2.209193830396615 	 ± 0.227972076426739
	data : 0.11472849845886231
	model : 0.06507601737976074
			 train-loss:  2.213014174433588 	 ± 0.2301203709575437
	data : 0.11485958099365234
	model : 0.06505517959594727
			 train-loss:  2.2120351309959707 	 ± 0.22922680235433338
	data : 0.11490988731384277
	model : 0.06508512496948242
			 train-loss:  2.214292371840704 	 ± 0.22929106657308926
	data : 0.11475977897644044
	model : 0.06508064270019531
			 train-loss:  2.2115869375894652 	 ± 0.22988462908562488
	data : 0.11479701995849609
	model : 0.06511387825012208
			 train-loss:  2.212153039245962 	 ± 0.22888210057635436
	data : 0.11491546630859376
	model : 0.0651616096496582
			 train-loss:  2.2093112700515323 	 ± 0.2297086105964126
	data : 0.11480717658996582
	model : 0.06521291732788086
			 train-loss:  2.2121220761482867 	 ± 0.23051078447148438
	data : 0.11485095024108886
	model : 0.065120267868042
			 train-loss:  2.209908699989319 	 ± 0.23062126743956976
	data : 0.11494059562683105
	model : 0.06512341499328614
			 train-loss:  2.2108974220516444 	 ± 0.22981415788549076
	data : 0.11499114036560058
	model : 0.06508398056030273
			 train-loss:  2.2083186230489185 	 ± 0.2303934924964189
	data : 0.11485772132873535
	model : 0.06508870124816894
			 train-loss:  2.206680496182062 	 ± 0.2300260073827679
	data : 0.11484727859497071
	model : 0.06505112648010254
			 train-loss:  2.20671878781235 	 ± 0.22901526089513172
	data : 0.1148369312286377
	model : 0.06504454612731933
			 train-loss:  2.2058712731237 	 ± 0.2281968542680349
	data : 0.11488804817199708
	model : 0.06502809524536132
			 train-loss:  2.208587895179617 	 ± 0.22907115982201054
	data : 0.11480965614318847
	model : 0.06502752304077149
			 train-loss:  2.210290994399633 	 ± 0.2288265007227731
	data : 0.1149569034576416
	model : 0.06503424644470215
			 train-loss:  2.206645933248229 	 ± 0.23124087532147927
	data : 0.11498479843139649
	model : 0.06507563591003418
			 train-loss:  2.207018972444935 	 ± 0.2303028781338683
	data : 0.11501340866088867
	model : 0.06509346961975097
			 train-loss:  2.206532287597656 	 ± 0.22940271825074737
	data : 0.1149622917175293
	model : 0.06506581306457519
			 train-loss:  2.205673988200416 	 ± 0.22864620292146418
	data : 0.11485223770141602
	model : 0.06506509780883789
			 train-loss:  2.204286479559101 	 ± 0.22821813209938957
	data : 0.11478681564331054
	model : 0.06500759124755859
			 train-loss:  2.2043311867287487 	 ± 0.22728905923092402
	data : 0.1148193359375
	model : 0.06496458053588867
			 train-loss:  2.2072163820266724 	 ± 0.2286210757464168
	data : 0.1148465633392334
	model : 0.0649526596069336
			 train-loss:  2.2074973945617677 	 ± 0.2277262557168375
	data : 0.11475777626037598
	model : 0.06503734588623047
			 train-loss:  2.208256585257394 	 ± 0.22697954208947185
	data : 0.11484031677246094
	model : 0.06502327919006348
			 train-loss:  2.2108746776430626 	 ± 0.22798618571001267
	data : 0.11487975120544433
	model : 0.06506133079528809
			 train-loss:  2.212836543098092 	 ± 0.22816756436136937
	data : 0.11489605903625488
	model : 0.06505661010742188
			 train-loss:  2.212014161339102 	 ± 0.2274718357273828
	data : 0.11485748291015625
	model : 0.06506209373474121
			 train-loss:  2.211427332804753 	 ± 0.22669325763031303
	data : 0.11484208106994628
	model : 0.06499838829040527
			 train-loss:  2.209395187501689 	 ± 0.22701188105990439
	data : 0.11474556922912597
	model : 0.0649576187133789
			 train-loss:  2.2075233260790506 	 ± 0.2271629115961245
	data : 0.11485829353332519
	model : 0.06491312980651856
			 train-loss:  2.20851983522114 	 ± 0.22659672555640056
	data : 0.1149980068206787
	model : 0.06490535736083984
			 train-loss:  2.2081093859316696 	 ± 0.2257992530708476
	data : 0.11503300666809083
	model : 0.06487770080566406
			 train-loss:  2.2075680414835612 	 ± 0.22504866804604334
	data : 0.11517267227172852
	model : 0.0649350643157959
			 train-loss:  2.205856341649504 	 ± 0.22510006218192258
	data : 0.11518144607543945
	model : 0.06500134468078614
			 train-loss:  2.206221630103397 	 ± 0.22431747803368907
	data : 0.11507773399353027
	model : 0.06504497528076172
			 train-loss:  2.2061258183009382 	 ± 0.22350606929408975
	data : 0.1149911880493164
	model : 0.06506967544555664
			 train-loss:  2.207131105361225 	 ± 0.22301353742395463
	data : 0.11490654945373535
	model : 0.06510438919067382
			 train-loss:  2.205834926026208 	 ± 0.22274047419483406
	data : 0.11474418640136719
	model : 0.06508026123046876
			 train-loss:  2.204197144677453 	 ± 0.22279357535505817
	data : 0.11479930877685547
	model : 0.06506261825561524
			 train-loss:  2.202451593439344 	 ± 0.22297318419149367
	data : 0.11486797332763672
	model : 0.06509909629821778
			 train-loss:  2.2008552067763323 	 ± 0.22300504300952997
	data : 0.11483302116394042
	model : 0.06514387130737305
			 train-loss:  2.200734661685096 	 ± 0.22223404613251044
	data : 0.11497635841369629
	model : 0.06512942314147949
			 train-loss:  2.200801296891837 	 ± 0.22146783955732932
	data : 0.11502585411071778
	model : 0.06515970230102539
			 train-loss:  2.1985447210808324 	 ± 0.22237449991460007
	data : 0.115104341506958
	model : 0.06517205238342286
			 train-loss:  2.199499783872747 	 ± 0.2219170880536083
	data : 0.11501240730285645
	model : 0.06510257720947266
			 train-loss:  2.1984168742154098 	 ± 0.22155547517156712
	data : 0.11499958038330078
	model : 0.06504940986633301
			 train-loss:  2.2003281260496816 	 ± 0.22203155979480024
	data : 0.11481852531433105
	model : 0.0650336742401123
			 train-loss:  2.2006110938390098 	 ± 0.22131717209918209
	data : 0.11474919319152832
	model : 0.06490201950073242
			 train-loss:  2.1994223578876215 	 ± 0.2210630571396164
	data : 0.11471762657165527
	model : 0.06497273445129395
			 train-loss:  2.1985381559321753 	 ± 0.2206024098952885
	data : 0.11486382484436035
	model : 0.06502308845520019
			 train-loss:  2.2006282806396484 	 ± 0.22138513732032145
	data : 0.11492691040039063
	model : 0.06503348350524903
			 train-loss:  2.202294992162036 	 ± 0.22162614059514654
	data : 0.11506547927856445
	model : 0.06505374908447266
			 train-loss:  2.203924028335079 	 ± 0.22183312149609163
	data : 0.11514043807983398
	model : 0.0651176929473877
			 train-loss:  2.2032885872400723 	 ± 0.22126245102562345
	data : 0.1151352882385254
	model : 0.06506543159484864
			 train-loss:  2.2025779827385192 	 ± 0.22073517461992848
	data : 0.11511383056640626
	model : 0.06502904891967773
			 train-loss:  2.2015415369709834 	 ± 0.22041844290174348
	data : 0.11499156951904296
	model : 0.06502704620361328
			 train-loss:  2.202429287088742 	 ± 0.22000738224430727
	data : 0.11484065055847167
	model : 0.06502490043640137
			 train-loss:  2.2030941039323806 	 ± 0.2194789351822593
	data : 0.11480669975280762
	model : 0.06499257087707519
			 train-loss:  2.2038062033445938 	 ± 0.21898159261470065
	data : 0.1148231029510498
	model : 0.0650336742401123
			 train-loss:  2.2020182535972124 	 ± 0.21948032068207027
	data : 0.11477155685424804
	model : 0.0650726318359375
			 train-loss:  2.2011093639888646 	 ± 0.21911162639558815
	data : 0.11489591598510743
	model : 0.06508355140686035
			 train-loss:  2.2008852595236243 	 ± 0.21846131881384553
	data : 0.1149592399597168
	model : 0.06509652137756347
			 train-loss:  2.200738866401441 	 ± 0.2178063772271959
	data : 0.1149895191192627
	model : 0.06511726379394531
			 train-loss:  2.2002463943987007 	 ± 0.21724146556509627
	data : 0.11499924659729004
	model : 0.06507554054260253
			 train-loss:  2.1986356588180906 	 ± 0.21758202660820367
	data : 0.1150050163269043
	model : 0.06503009796142578
			 train-loss:  2.2018342450970696 	 ± 0.22083638830561816
	data : 0.1149064540863037
	model : 0.06501326560974122
			 train-loss:  2.2022737716076644 	 ± 0.2202557445016442
	data : 0.11480374336242676
	model : 0.06497468948364257
			 train-loss:  2.2035988997010625 	 ± 0.2202815988373512
	data : 0.11481480598449707
	model : 0.06498055458068848
			 train-loss:  2.2021764091580933 	 ± 0.22041825762344489
	data : 0.11476755142211914
	model : 0.0649409294128418
			 train-loss:  2.2001256291256395 	 ± 0.22140667661853475
	data : 0.11488547325134277
	model : 0.06515040397644042
			 train-loss:  2.1998770319657517 	 ± 0.22078991880731721
	data : 0.11486573219299316
	model : 0.06513452529907227
			 train-loss:  2.198871787937208 	 ± 0.22055123086682182
	data : 0.1150125503540039
	model : 0.0651747703552246
			 train-loss:  2.1984513023921424 	 ± 0.21999011552425735
	data : 0.11515154838562011
	model : 0.06519141197204589
			 train-loss:  2.198724998669191 	 ± 0.2193941312526901
	data : 0.11524195671081543
	model : 0.0652310848236084
			 train-loss:  2.199249068222477 	 ± 0.2188839435563377
	data : 0.11518268585205078
	model : 0.06501665115356445
			 train-loss:  2.1983964657515624 	 ± 0.21856278093551396
	data : 0.11513347625732422
	model : 0.06504268646240234
			 train-loss:  2.1968157744274457 	 ± 0.2189693302250372
	data : 0.11509504318237304
	model : 0.06505331993103028
			 train-loss:  2.1949184060096742 	 ± 0.21983082914174043
	data : 0.11494522094726563
	model : 0.06512537002563476
			 train-loss:  2.195884615018223 	 ± 0.2196056510767986
	data : 0.11500215530395508
	model : 0.06507992744445801
			 train-loss:  2.1952920811516896 	 ± 0.21914654658656652
	data : 0.11501731872558593
	model : 0.06511521339416504
			 train-loss:  2.1981069038474494 	 ± 0.22182155739369078
	data : 0.11517863273620606
	model : 0.06511731147766113
			 train-loss:  2.197394873784936 	 ± 0.2214275603925711
	data : 0.11511731147766113
	model : 0.06509480476379395
			 train-loss:  2.1973663317190635 	 ± 0.22082863600695987
	data : 0.11508173942565918
	model : 0.06504154205322266
			 train-loss:  2.199417893604566 	 ± 0.22199494400228847
	data : 0.11497993469238281
	model : 0.06503829956054688
			 train-loss:  2.196765305524204 	 ± 0.22433669771215942
	data : 0.11490540504455567
	model : 0.06516590118408203
			 train-loss:  2.19530133554276 	 ± 0.22463311708831446
	data : 0.11479763984680176
	model : 0.06516695022583008
			 train-loss:  2.195925685463759 	 ± 0.22420155614847076
	data : 0.11473550796508789
	model : 0.06515812873840332
			 train-loss:  2.1942247829939188 	 ± 0.2248300873724941
	data : 0.11477646827697754
	model : 0.0651763916015625
			 train-loss:  2.1943060190889847 	 ± 0.22424355033323629
	data : 0.11492462158203125
	model : 0.06521859169006347
			 train-loss:  2.1948213589688144 	 ± 0.22377218951713326
	data : 0.1150787353515625
	model : 0.06507797241210937
			 train-loss:  2.193324675214105 	 ± 0.2241531485739555
	data : 0.11504073143005371
	model : 0.06524605751037597
			 train-loss:  2.191639780383749 	 ± 0.22479667052810426
	data : 0.11498007774353028
	model : 0.06530680656433105
			 train-loss:  2.192678737029051 	 ± 0.22468601714937161
	data : 0.11506567001342774
	model : 0.06528201103210449
			 train-loss:  2.191721413208514 	 ± 0.22451046144248135
	data : 0.11485333442687988
	model : 0.0652775764465332
			 train-loss:  2.1927747901926185 	 ± 0.22442496924880384
	data : 0.1145261287689209
	model : 0.06524558067321777
			 train-loss:  2.1929313889657607 	 ± 0.22386831237992677
	data : 0.11447629928588868
	model : 0.06505517959594727
			 train-loss:  2.1927303123713737 	 ± 0.22332304506146544
	data : 0.11463913917541504
	model : 0.06499671936035156
			 train-loss:  2.1919591814279555 	 ± 0.2230294834471335
	data : 0.1145400047302246
	model : 0.06500310897827148
			 train-loss:  2.1918253323331993 	 ± 0.22248204470748348
	data : 0.11448993682861328
	model : 0.0650141716003418
			 train-loss:  2.1907398505966262 	 ± 0.22246359638319577
	data : 0.11459388732910156
	model : 0.0650907039642334
			 train-loss:  2.1901332480566844 	 ± 0.22208238925929163
	data : 0.11471304893493653
	model : 0.0651254653930664
			 train-loss:  2.193005411648283 	 ± 0.22528522531053594
	data : 0.11468849182128907
	model : 0.06509547233581543
			 train-loss:  2.192934172909434 	 ± 0.22473738076682861
	data : 0.11465706825256347
	model : 0.06506624221801757
			 train-loss:  2.1922446217351745 	 ± 0.22440852212702353
	data : 0.11470527648925781
	model : 0.0650266170501709
			 train-loss:  2.191192138598161 	 ± 0.2243748965292032
	data : 0.11488056182861328
	model : 0.06497716903686523
			 train-loss:  2.1915237720196066 	 ± 0.22388573265938835
	data : 0.11501584053039551
	model : 0.06493563652038574
			 train-loss:  2.1915348043852445 	 ± 0.2233495353116909
	data : 0.11508960723876953
	model : 0.06495733261108398
			 train-loss:  2.1909621556599936 	 ± 0.22297085882555004
	data : 0.1152486801147461
	model : 0.06515378952026367
			 train-loss:  2.1923069694030906 	 ± 0.2232939162956962
	data : 0.11517372131347656
	model : 0.06516380310058593
			 train-loss:  2.193502459885939 	 ± 0.2234424852853305
	data : 0.11501522064208984
	model : 0.06513018608093261
			 train-loss:  2.192321068804029 	 ± 0.22358003601743962
	data : 0.11476011276245117
	model : 0.06515789031982422
			 train-loss:  2.1922104525788924 	 ± 0.22306288312000916
	data : 0.11484775543212891
	model : 0.06514086723327636
			 train-loss:  2.193854707895323 	 ± 0.22383964540234397
	data : 0.11469564437866211
	model : 0.06495089530944824
			 train-loss:  2.1926164522215172 	 ± 0.2240577550661175
	data : 0.11497864723205567
	model : 0.06498942375183106
			 train-loss:  2.1944576699612877 	 ± 0.22517280125377606
	data : 0.11503863334655762
	model : 0.06502017974853516
			 train-loss:  2.1946198202054434 	 ± 0.22466845424599632
	data : 0.11506304740905762
	model : 0.06502785682678222
			 train-loss:  2.1949174442247714 	 ± 0.22419799543839086
	data : 0.11489167213439941
	model : 0.06504578590393066
			 train-loss:  2.1962880356745287 	 ± 0.22460556792824107
	data : 0.11511201858520508
	model : 0.06499137878417968
			 train-loss:  2.196962819919327 	 ± 0.2243202278337482
	data : 0.11492958068847656
	model : 0.06487445831298828
			 train-loss:  2.1980163412051157 	 ± 0.22436173777835583
	data : 0.11490254402160645
	model : 0.06475071907043457
			 train-loss:  2.198465556307224 	 ± 0.22395815658768578
	data : 0.11500778198242187
	model : 0.06460704803466796
			 train-loss:  2.1980928273073266 	 ± 0.2235270012358796
	data : 0.11517148017883301
	model : 0.06446309089660644
			 train-loss:  2.197421515252855 	 ± 0.2232559169376985
	data : 0.11510767936706542
	model : 0.0643430233001709
			 train-loss:  2.19682766114716 	 ± 0.22293947250881935
	data : 0.11524653434753418
	model : 0.0641974925994873
			 train-loss:  2.1963110857598056 	 ± 0.22258338876297637
	data : 0.11531691551208496
	model : 0.06410050392150879
			 train-loss:  2.1952412682667113 	 ± 0.22267885652267402
	data : 0.11537132263183594
	model : 0.06402873992919922
			 train-loss:  2.1951326199494074 	 ± 0.22219818262371185
	data : 0.1153557300567627
	model : 0.06394710540771484
			 train-loss:  2.196708059310913 	 ± 0.22299271614481464
	data : 0.11564846038818359
	model : 0.06386737823486328
			 train-loss:  2.1957102322475217 	 ± 0.2230235189290052
	data : 0.11574959754943848
	model : 0.06387453079223633
			 train-loss:  2.1957469824059257 	 ± 0.22254304667788327
	data : 0.1157304286956787
	model : 0.06386833190917969
			 train-loss:  2.196344487656851 	 ± 0.22225138724632684
	data : 0.11570205688476562
	model : 0.0638308048248291
			 train-loss:  2.1957059339580374 	 ± 0.22199007281761549
	data : 0.11573724746704102
	model : 0.06383714675903321
			 train-loss:  2.19410594777858 	 ± 0.2228652545541256
	data : 0.11545600891113281
	model : 0.06386799812316894
			 train-loss:  2.1943577932099165 	 ± 0.22242608950706105
	data : 0.11539187431335449
	model : 0.0638352394104004
			 train-loss:  2.193381604263048 	 ± 0.22246238312037128
	data : 0.11548328399658203
	model : 0.0638617992401123
			 train-loss:  2.195662886154752 	 ± 0.22475537665070636
	data : 0.11555089950561523
	model : 0.06387138366699219
			 train-loss:  2.195780114648731 	 ± 0.22429197557413547
	data : 0.11558732986450196
	model : 0.06388564109802246
			 train-loss:  2.1962532738844556 	 ± 0.22394371056876075
	data : 0.11557731628417969
	model : 0.06387252807617187
			 train-loss:  2.195283900158039 	 ± 0.22398262311795486
	data : 0.11542234420776368
	model : 0.06393346786499024
			 train-loss:  2.195114148057197 	 ± 0.22353490417673852
	data : 0.11543164253234864
	model : 0.06390814781188965
			 train-loss:  2.1934716485655357 	 ± 0.22453305562302528
	data : 0.11549696922302247
	model : 0.06391191482543945
			 train-loss:  2.193317861830602 	 ± 0.224085298203544
	data : 0.11566047668457032
	model : 0.0638650894165039
			 train-loss:  2.1928613487555055 	 ± 0.2237411796903362
	data : 0.11576709747314454
	model : 0.06387896537780761
			 train-loss:  2.191891128939342 	 ± 0.22380179640757236
	data : 0.11580548286437989
	model : 0.06386041641235352
			 train-loss:  2.1914142264045684 	 ± 0.22347351291235146
	data : 0.11595110893249512
	model : 0.0638587474822998
			 train-loss:  2.190323281672693 	 ± 0.22368059363581783
	data : 0.11593389511108398
	model : 0.06385107040405273
			 train-loss:  2.191067534756948 	 ± 0.22353845873214673
	data : 0.11570172309875489
	model : 0.06382980346679687
			 train-loss:  2.1903295421600344 	 ± 0.2233946695814741
	data : 0.1156224250793457
	model : 0.06381192207336425
			 train-loss:  2.191252171755787 	 ± 0.22342597057686425
	data : 0.11576828956604004
	model : 0.06379847526550293
			 train-loss:  2.191539258237869 	 ± 0.22302860688031304
	data : 0.11560678482055664
	model : 0.06381759643554688
			 train-loss:  2.1915859525853936 	 ± 0.22258863671178789
	data : 0.11556906700134277
	model : 0.06383514404296875
			 train-loss:  2.1915327995780887 	 ± 0.22215164677256824
	data : 0.11568260192871094
	model : 0.06385178565979004
			 train-loss:  2.191150754105811 	 ± 0.22179921767726452
	data : 0.1155433177947998
	model : 0.06390714645385742
			 train-loss:  2.195260649546981 	 ± 0.23088956613202916
	data : 0.11508746147155761
	model : 0.05545997619628906
#epoch  24    val-loss:  2.4100702310863293  train-loss:  2.195260649546981  lr:  0.00015625
			 train-loss:  2.161255121231079 	 ± 0.0
	data : 5.764887571334839
	model : 0.0710148811340332
			 train-loss:  2.0950127840042114 	 ± 0.06624233722686768
	data : 2.947402000427246
	model : 0.06814813613891602
			 train-loss:  2.3205953439076743 	 ± 0.3235743307073622
	data : 2.0033625761667886
	model : 0.06696399052937825
			 train-loss:  2.3669809103012085 	 ± 0.2915135038527118
	data : 1.5311164855957031
	model : 0.06638383865356445
			 train-loss:  2.2960931301116942 	 ± 0.29679017480970543
	data : 1.24780592918396
	model : 0.06606478691101074
			 train-loss:  2.2302358746528625 	 ± 0.30836595743125633
	data : 0.11760296821594238
	model : 0.0648308277130127
			 train-loss:  2.2085340533937727 	 ± 0.29039826892079357
	data : 0.11442499160766602
	model : 0.06472053527832031
			 train-loss:  2.171270325779915 	 ± 0.2889807226060726
	data : 0.11430449485778808
	model : 0.06475787162780762
			 train-loss:  2.144495368003845 	 ± 0.28278290583747284
	data : 0.11436495780944825
	model : 0.06486759185791016
			 train-loss:  2.2010187983512877 	 ± 0.31736987609834105
	data : 0.11442828178405762
	model : 0.06488938331604004
			 train-loss:  2.168077078732577 	 ± 0.32002892491502233
	data : 0.11456255912780762
	model : 0.06490626335144042
			 train-loss:  2.145089944203695 	 ± 0.3157469376089254
	data : 0.11471219062805176
	model : 0.06496286392211914
			 train-loss:  2.144451544835017 	 ± 0.30336790582477047
	data : 0.114630126953125
	model : 0.0651310920715332
			 train-loss:  2.1446952308927263 	 ± 0.29233394912880856
	data : 0.11449222564697266
	model : 0.06505646705627441
			 train-loss:  2.126364262898763 	 ± 0.29063069931679353
	data : 0.11441159248352051
	model : 0.0650489330291748
			 train-loss:  2.1677618473768234 	 ± 0.3238726071822919
	data : 0.1144568920135498
	model : 0.06503114700317383
			 train-loss:  2.1669242662542008 	 ± 0.31422044251375114
	data : 0.11442279815673828
	model : 0.0650794506072998
			 train-loss:  2.183439360724555 	 ± 0.31286731870730294
	data : 0.11448121070861816
	model : 0.06493101119995118
			 train-loss:  2.190468512083355 	 ± 0.3059794567760072
	data : 0.11457619667053223
	model : 0.06494083404541015
			 train-loss:  2.1857000470161436 	 ± 0.2989553202291948
	data : 0.11457915306091308
	model : 0.06492042541503906
			 train-loss:  2.182155892962501 	 ± 0.2921807416495341
	data : 0.11446337699890137
	model : 0.06490883827209473
			 train-loss:  2.1764597567644985 	 ± 0.28665399943558256
	data : 0.11446909904479981
	model : 0.06481733322143554
			 train-loss:  2.177242672961691 	 ± 0.2803771912252798
	data : 0.11453366279602051
	model : 0.06482620239257812
			 train-loss:  2.175800363222758 	 ± 0.2745609978782115
	data : 0.1146914005279541
	model : 0.06482763290405273
			 train-loss:  2.1710444641113282 	 ± 0.27002080980220483
	data : 0.1147545337677002
	model : 0.06486263275146484
			 train-loss:  2.1765026037509623 	 ± 0.2661799045012528
	data : 0.11483187675476074
	model : 0.0649099349975586
			 train-loss:  2.1864677711769387 	 ± 0.2661005854775783
	data : 0.11483721733093262
	model : 0.06490540504455566
			 train-loss:  2.1807052237646922 	 ± 0.2630155847633661
	data : 0.11481180191040039
	model : 0.06489248275756836
			 train-loss:  2.1792306571171203 	 ± 0.25855881134471154
	data : 0.1145486831665039
	model : 0.06489381790161133
			 train-loss:  2.1778560241063434 	 ± 0.25432073415546036
	data : 0.1144676685333252
	model : 0.06487855911254883
			 train-loss:  2.1709724318596626 	 ± 0.253010139278177
	data : 0.11449971199035644
	model : 0.06487102508544922
			 train-loss:  2.18039807677269 	 ± 0.25449521653625345
	data : 0.1144676685333252
	model : 0.06492457389831544
			 train-loss:  2.178606979774706 	 ± 0.25081429600913646
	data : 0.11452736854553222
	model : 0.06498374938964843
			 train-loss:  2.1907951761694515 	 ± 0.25682640378203614
	data : 0.11460661888122559
	model : 0.06500616073608398
			 train-loss:  2.1856043679373607 	 ± 0.25493400828610235
	data : 0.11466007232666016
	model : 0.06507062911987305
			 train-loss:  2.188553386264377 	 ± 0.2519730504651643
	data : 0.11470322608947754
	model : 0.0651216983795166
			 train-loss:  2.184017162065248 	 ± 0.2500304877261665
	data : 0.1145749568939209
	model : 0.0650869369506836
			 train-loss:  2.1911121167634664 	 ± 0.25046482170558115
	data : 0.11436052322387695
	model : 0.06500158309936524
			 train-loss:  2.1953055675213156 	 ± 0.2485806290293997
	data : 0.11439399719238282
	model : 0.06500101089477539
			 train-loss:  2.19687642455101 	 ± 0.245649663099581
	data : 0.11452317237854004
	model : 0.06497426033020019
			 train-loss:  2.1969253900574475 	 ± 0.24263564028191717
	data : 0.11442713737487793
	model : 0.06496591567993164
			 train-loss:  2.196807174455552 	 ± 0.23973091443784292
	data : 0.1145596981048584
	model : 0.06501212120056152
			 train-loss:  2.193329090295836 	 ± 0.2379967550853072
	data : 0.11469087600708008
	model : 0.06510653495788574
			 train-loss:  2.1852400194514883 	 ± 0.2411819827121828
	data : 0.11478395462036133
	model : 0.06510872840881347
			 train-loss:  2.1887864589691164 	 ± 0.23964454549642425
	data : 0.11463780403137207
	model : 0.06509575843811036
			 train-loss:  2.185019887011984 	 ± 0.23836832530937124
	data : 0.11459765434265137
	model : 0.06503453254699706
			 train-loss:  2.1877092097667936 	 ± 0.23652320579192032
	data : 0.11464061737060546
	model : 0.06501731872558594
			 train-loss:  2.1852256705363593 	 ± 0.23466494718837252
	data : 0.1147317886352539
	model : 0.06498908996582031
			 train-loss:  2.1954096093469735 	 ± 0.24273857000681578
	data : 0.11480298042297363
	model : 0.06497774124145508
			 train-loss:  2.197129974365234 	 ± 0.24060049092963515
	data : 0.11483573913574219
	model : 0.06497621536254883
			 train-loss:  2.203736473532284 	 ± 0.2427670020479918
	data : 0.11489038467407227
	model : 0.06500101089477539
			 train-loss:  2.2044937839874854 	 ± 0.24048219405429963
	data : 0.11482129096984864
	model : 0.0649914264678955
			 train-loss:  2.205481578718941 	 ± 0.2383091693059321
	data : 0.11467413902282715
	model : 0.06498174667358399
			 train-loss:  2.2003077643888966 	 ± 0.23907800604047916
	data : 0.1145409107208252
	model : 0.06494641304016113
			 train-loss:  2.2008693196556783 	 ± 0.23693053808033945
	data : 0.11452732086181641
	model : 0.06497073173522949
			 train-loss:  2.198983688439642 	 ± 0.23522161498644148
	data : 0.11448864936828614
	model : 0.06494765281677246
			 train-loss:  2.1978306958549902 	 ± 0.23330873588625833
	data : 0.11464877128601074
	model : 0.0649488925933838
			 train-loss:  2.197090335961046 	 ± 0.231356241305339
	data : 0.11477432250976563
	model : 0.06496472358703613
			 train-loss:  2.1961941011881425 	 ± 0.22948874197470512
	data : 0.11473627090454101
	model : 0.06504693031311035
			 train-loss:  2.1975487132867175 	 ± 0.22780604640182994
	data : 0.11479449272155762
	model : 0.06502280235290528
			 train-loss:  2.2010930190320876 	 ± 0.2275929997053246
	data : 0.11480531692504883
	model : 0.0650291919708252
			 train-loss:  2.2043119149823345 	 ± 0.2271456612202273
	data : 0.1145702838897705
	model : 0.06499872207641602
			 train-loss:  2.2100132173962064 	 ± 0.22976396890101872
	data : 0.11442480087280274
	model : 0.06492681503295898
			 train-loss:  2.2083642426878214 	 ± 0.2283372919964961
	data : 0.11457524299621583
	model : 0.06487350463867188
			 train-loss:  2.2104907971162064 	 ± 0.2272118395814184
	data : 0.11452536582946778
	model : 0.06490325927734375
			 train-loss:  2.2167481780052185 	 ± 0.2310586132469569
	data : 0.11461482048034669
	model : 0.0648993968963623
			 train-loss:  2.2118976169557714 	 ± 0.23268882634149898
	data : 0.11472668647766113
	model : 0.06497139930725097
			 train-loss:  2.21030429706854 	 ± 0.23133945634407127
	data : 0.11476035118103027
	model : 0.06503810882568359
			 train-loss:  2.2074076835659966 	 ± 0.23089579214251255
	data : 0.11470775604248047
	model : 0.06506576538085937
			 train-loss:  2.204584917000362 	 ± 0.23043664467841432
	data : 0.11465387344360352
	model : 0.06498727798461915
			 train-loss:  2.2010899748600705 	 ± 0.23066895884954985
	data : 0.11461143493652344
	model : 0.06503877639770508
			 train-loss:  2.1972653137312994 	 ± 0.2313174367846854
	data : 0.11472277641296387
	model : 0.06499662399291992
			 train-loss:  2.1999638635818273 	 ± 0.23086595662277548
	data : 0.11483974456787109
	model : 0.06496009826660157
			 train-loss:  2.1983144702138127 	 ± 0.22973338535044685
	data : 0.114811372756958
	model : 0.06497445106506347
			 train-loss:  2.193081266085307 	 ± 0.23259476516750718
	data : 0.1148918628692627
	model : 0.06503911018371582
			 train-loss:  2.1922368768014406 	 ± 0.23117515631940844
	data : 0.11493372917175293
	model : 0.06503701210021973
			 train-loss:  2.192115624229629 	 ± 0.22967154575414284
	data : 0.11488585472106934
	model : 0.06501765251159668
			 train-loss:  2.19196886741198 	 ± 0.22819817669744624
	data : 0.11484651565551758
	model : 0.06507201194763183
			 train-loss:  2.1910176020634324 	 ± 0.2269048716488577
	data : 0.11482739448547363
	model : 0.06502013206481934
			 train-loss:  2.189825974404812 	 ± 0.22573087108308018
	data : 0.11476435661315917
	model : 0.06497330665588379
			 train-loss:  2.186058700820546 	 ± 0.22684961360115646
	data : 0.11480822563171386
	model : 0.06496391296386719
			 train-loss:  2.1830336567832203 	 ± 0.22709997646179436
	data : 0.11479392051696777
	model : 0.06501803398132325
			 train-loss:  2.1844270186251906 	 ± 0.22608011963965005
	data : 0.11478934288024903
	model : 0.06500263214111328
			 train-loss:  2.1874588429927826 	 ± 0.2264214527766934
	data : 0.11474275588989258
	model : 0.0650062084197998
			 train-loss:  2.1911205698462095 	 ± 0.22757378889316776
	data : 0.11476473808288574
	model : 0.06502375602722169
			 train-loss:  2.1901888168135355 	 ± 0.22640984074984702
	data : 0.11462883949279785
	model : 0.06497101783752442
			 train-loss:  2.191861671962957 	 ± 0.22563880544916312
	data : 0.11450252532958985
	model : 0.0648913860321045
			 train-loss:  2.1925592869520187 	 ± 0.22444744413381829
	data : 0.11453013420104981
	model : 0.06487073898315429
			 train-loss:  2.1916639309250905 	 ± 0.22334093139694902
	data : 0.11465873718261718
	model : 0.06488099098205566
			 train-loss:  2.1912975880834793 	 ± 0.2221235710741864
	data : 0.11466951370239258
	model : 0.06490902900695801
			 train-loss:  2.1924510277234592 	 ± 0.22117059794067434
	data : 0.11475205421447754
	model : 0.06494379043579102
			 train-loss:  2.1906846722830897 	 ± 0.22060973251595237
	data : 0.11483330726623535
	model : 0.06501283645629882
			 train-loss:  2.196094868003681 	 ± 0.22547326948289545
	data : 0.11484270095825196
	model : 0.0650151252746582
			 train-loss:  2.200021288496383 	 ± 0.22744477072529698
	data : 0.11485881805419922
	model : 0.06501250267028809
			 train-loss:  2.2055186058345595 	 ± 0.23243776024974022
	data : 0.11480507850646973
	model : 0.06498560905456544
			 train-loss:  2.2065726233025393 	 ± 0.23145208662706135
	data : 0.11473932266235351
	model : 0.06497020721435547
			 train-loss:  2.2063585741003764 	 ± 0.23026549479989045
	data : 0.11490435600280761
	model : 0.06492757797241211
			 train-loss:  2.202437063869165 	 ± 0.23232056469369855
	data : 0.11494321823120117
	model : 0.06491942405700683
			 train-loss:  2.2030487722820706 	 ± 0.23122356037648306
	data : 0.11495308876037598
	model : 0.06494193077087403
			 train-loss:  2.200751645565033 	 ± 0.23119708875481979
	data : 0.11507673263549804
	model : 0.0649648666381836
			 train-loss:  2.198497728546067 	 ± 0.23115120460333985
	data : 0.11511530876159667
	model : 0.06498723030090332
			 train-loss:  2.196057559228411 	 ± 0.23131892333935578
	data : 0.11504321098327637
	model : 0.06501765251159668
			 train-loss:  2.1951808941017075 	 ± 0.23036348689328284
	data : 0.11498556137084961
	model : 0.06498932838439941
			 train-loss:  2.196689162116784 	 ± 0.22976375933624468
	data : 0.11488175392150879
	model : 0.06500239372253418
			 train-loss:  2.1956054222016106 	 ± 0.2289339581566726
	data : 0.11481528282165528
	model : 0.06493554115295411
			 train-loss:  2.196257572129088 	 ± 0.2279494955951356
	data : 0.11490592956542969
	model : 0.06490445137023926
			 train-loss:  2.1972075076860804 	 ± 0.227092509779344
	data : 0.11492938995361328
	model : 0.06487412452697754
			 train-loss:  2.1968153615792594 	 ± 0.22607510471403278
	data : 0.1149911880493164
	model : 0.06489071846008301
			 train-loss:  2.1947616043440794 	 ± 0.22604554750912573
	data : 0.11494240760803223
	model : 0.06488885879516601
			 train-loss:  2.1944724343039774 	 ± 0.22503597402032363
	data : 0.11495599746704102
	model : 0.0649674415588379
			 train-loss:  2.199956829483445 	 ± 0.23128685138419572
	data : 0.11484999656677246
	model : 0.06500463485717774
			 train-loss:  2.2030090476785387 	 ± 0.23248670093715176
	data : 0.11472640037536622
	model : 0.06500115394592285
			 train-loss:  2.2026685972129347 	 ± 0.231483754232753
	data : 0.11470627784729004
	model : 0.06503524780273437
			 train-loss:  2.200365280895902 	 ± 0.23176320255858787
	data : 0.11476359367370606
	model : 0.06501169204711914
			 train-loss:  2.2012539728828098 	 ± 0.23094834138663312
	data : 0.11474452018737794
	model : 0.0649949073791504
			 train-loss:  2.201185295294071 	 ± 0.22995189909982705
	data : 0.11491141319274903
	model : 0.0649932861328125
			 train-loss:  2.2018967968785863 	 ± 0.2290952890063618
	data : 0.11510286331176758
	model : 0.06502118110656738
			 train-loss:  2.200685235403352 	 ± 0.22849859597936226
	data : 0.11506466865539551
	model : 0.06505188941955567
			 train-loss:  2.1975084483122624 	 ± 0.23013845516836665
	data : 0.11510848999023438
	model : 0.06511492729187011
			 train-loss:  2.1981591055790584 	 ± 0.2292874256036941
	data : 0.11528377532958985
	model : 0.0651465892791748
			 train-loss:  2.197390799680032 	 ± 0.2284930491196501
	data : 0.11516194343566895
	model : 0.06513328552246093
			 train-loss:  2.1947553685454073 	 ± 0.22939384350876812
	data : 0.11504864692687988
	model : 0.06514086723327636
			 train-loss:  2.1965142672624047 	 ± 0.22928399964109691
	data : 0.11496152877807617
	model : 0.0650914192199707
			 train-loss:  2.195438438846219 	 ± 0.22866908977450706
	data : 0.11492137908935547
	model : 0.06502480506896972
			 train-loss:  2.195399929046631 	 ± 0.2277529804194306
	data : 0.11478209495544434
	model : 0.06500630378723145
			 train-loss:  2.1932535162047735 	 ± 0.2281131915599647
	data : 0.11484813690185547
	model : 0.06506309509277344
			 train-loss:  2.193647674688204 	 ± 0.22725640647233966
	data : 0.1149519443511963
	model : 0.06512007713317872
			 train-loss:  2.1941494150087237 	 ± 0.22643755302564186
	data : 0.11495819091796874
	model : 0.06513056755065919
			 train-loss:  2.1916599892830666 	 ± 0.22730978772780497
	data : 0.1149740219116211
	model : 0.065191650390625
			 train-loss:  2.1918624226863566 	 ± 0.22644550423661833
	data : 0.11508407592773437
	model : 0.06521406173706054
			 train-loss:  2.189084457077143 	 ± 0.22779235427216846
	data : 0.11514720916748047
	model : 0.06516509056091309
			 train-loss:  2.186300229845625 	 ± 0.2291544395495169
	data : 0.11496148109436036
	model : 0.06514439582824708
			 train-loss:  2.1884855982056237 	 ± 0.22966789528817874
	data : 0.11496467590332031
	model : 0.06515145301818848
			 train-loss:  2.191253093641196 	 ± 0.2310245795704215
	data : 0.11504225730895996
	model : 0.06509962081909179
			 train-loss:  2.1912488257443465 	 ± 0.2301673478549867
	data : 0.11491618156433106
	model : 0.0650439739227295
			 train-loss:  2.189862287219833 	 ± 0.22988476944053263
	data : 0.11487026214599609
	model : 0.06505765914916992
			 train-loss:  2.1891950386284043 	 ± 0.22917637892097265
	data : 0.1150425910949707
	model : 0.06503500938415527
			 train-loss:  2.187817202961963 	 ± 0.22891331301160703
	data : 0.11507167816162109
	model : 0.0650820255279541
			 train-loss:  2.185501764146544 	 ± 0.22970452873724193
	data : 0.11500992774963378
	model : 0.06514182090759277
			 train-loss:  2.187460172176361 	 ± 0.2300443411815634
	data : 0.11506066322326661
	model : 0.0651865005493164
			 train-loss:  2.1863225926744176 	 ± 0.229621970048231
	data : 0.11497621536254883
	model : 0.06521453857421874
			 train-loss:  2.1906802620686276 	 ± 0.23458991102494012
	data : 0.11481947898864746
	model : 0.06518878936767578
			 train-loss:  2.192387754266912 	 ± 0.2346520603288445
	data : 0.11475515365600586
	model : 0.06507339477539062
			 train-loss:  2.1901046045952373 	 ± 0.23542439067431614
	data : 0.1146881103515625
	model : 0.06502337455749511
			 train-loss:  2.194759673907839 	 ± 0.24116972721731503
	data : 0.11468973159790039
	model : 0.06497139930725097
			 train-loss:  2.194231151717983 	 ± 0.24042663206308915
	data : 0.11484251022338868
	model : 0.06493973731994629
			 train-loss:  2.1964929290369253 	 ± 0.2411609779743239
	data : 0.11489200592041016
	model : 0.06496686935424804
			 train-loss:  2.1982486094977403 	 ± 0.24128565748418468
	data : 0.11495027542114258
	model : 0.06501975059509277
			 train-loss:  2.1969968208530606 	 ± 0.24095632548163598
	data : 0.11507639884948731
	model : 0.06501212120056152
			 train-loss:  2.197729202111562 	 ± 0.24031813376205619
	data : 0.11508708000183106
	model : 0.06504526138305664
			 train-loss:  2.1977474602642437 	 ± 0.23952116087908837
	data : 0.11471095085144042
	model : 0.06501832008361816
			 train-loss:  2.198985328015528 	 ± 0.2392160729498302
	data : 0.11469941139221192
	model : 0.06499204635620118
			 train-loss:  2.198570543644475 	 ± 0.23848787251135364
	data : 0.11469712257385253
	model : 0.06500701904296875
			 train-loss:  2.2008574078609415 	 ± 0.23938940885830404
	data : 0.11475157737731934
	model : 0.06499881744384765
			 train-loss:  2.20055188286689 	 ± 0.2386460555682391
	data : 0.11470398902893067
	model : 0.06508493423461914
			 train-loss:  2.201739130111841 	 ± 0.23833871797098752
	data : 0.11495161056518555
	model : 0.0651102066040039
			 train-loss:  2.2025985740552283 	 ± 0.23782084738388365
	data : 0.11539478302001953
	model : 0.06513314247131348
			 train-loss:  2.202927439273158 	 ± 0.23710286493375826
	data : 0.11548333168029785
	model : 0.0651543140411377
			 train-loss:  2.204447757523015 	 ± 0.23712737877799897
	data : 0.11540021896362304
	model : 0.0651639461517334
			 train-loss:  2.2024874106049537 	 ± 0.23767412711801728
	data : 0.11540193557739258
	model : 0.0651005744934082
			 train-loss:  2.2017667589720733 	 ± 0.2371101465757755
	data : 0.11527175903320312
	model : 0.06509904861450196
			 train-loss:  2.201236698362562 	 ± 0.23647285737598261
	data : 0.11477413177490234
	model : 0.06507792472839355
			 train-loss:  2.2006328120553422 	 ± 0.2358716310653794
	data : 0.11472406387329101
	model : 0.06504502296447753
			 train-loss:  2.2011604207318003 	 ± 0.23524787012031317
	data : 0.1147500991821289
	model : 0.06504740715026855
			 train-loss:  2.2005983294862688 	 ± 0.23464435266378172
	data : 0.11478462219238281
	model : 0.06507344245910644
			 train-loss:  2.200740343116852 	 ± 0.23394363723815345
	data : 0.11483383178710938
	model : 0.0650712013244629
			 train-loss:  2.19863004170492 	 ± 0.23482155534910884
	data : 0.11483163833618164
	model : 0.06512889862060547
			 train-loss:  2.1981562674045563 	 ± 0.23420167979273426
	data : 0.11493911743164062
	model : 0.06518735885620117
			 train-loss:  2.198296019311487 	 ± 0.2335147730213538
	data : 0.11495285034179688
	model : 0.06516947746276855
			 train-loss:  2.198385012851042 	 ± 0.23282982620916254
	data : 0.11487665176391601
	model : 0.06510124206542969
			 train-loss:  2.197804163771066 	 ± 0.2322715384644947
	data : 0.11490154266357422
	model : 0.06506681442260742
			 train-loss:  2.196949305922486 	 ± 0.23186497818561003
	data : 0.11503357887268066
	model : 0.06501178741455078
			 train-loss:  2.197062394522518 	 ± 0.23119863422864928
	data : 0.1149777889251709
	model : 0.06495809555053711
			 train-loss:  2.19624090879813 	 ± 0.23078638506662133
	data : 0.11506552696228027
	model : 0.06500015258789063
			 train-loss:  2.196642487389701 	 ± 0.23018700959945287
	data : 0.11515083312988281
	model : 0.06506643295288086
			 train-loss:  2.1961991407654504 	 ± 0.22960705471051857
	data : 0.1151200771331787
	model : 0.06509771347045898
			 train-loss:  2.195458686957925 	 ± 0.22916816031414156
	data : 0.1150395393371582
	model : 0.06512103080749512
			 train-loss:  2.19682429613692 	 ± 0.22924459808960213
	data : 0.11507568359375
	model : 0.06514158248901367
			 train-loss:  2.1984352665906512 	 ± 0.22961150513055964
	data : 0.1148566722869873
	model : 0.06510906219482422
			 train-loss:  2.196321294042799 	 ± 0.23071297295380583
	data : 0.11469197273254395
	model : 0.0650568962097168
			 train-loss:  2.1959752549124025 	 ± 0.23012159756583697
	data : 0.11469578742980957
	model : 0.06506357192993165
			 train-loss:  2.1954382278107025 	 ± 0.22960222770562555
	data : 0.11482205390930175
	model : 0.0650404930114746
			 train-loss:  2.196999981103699 	 ± 0.2299413453006713
	data : 0.114825439453125
	model : 0.06505522727966309
			 train-loss:  2.197441931651986 	 ± 0.22939357556234702
	data : 0.11485238075256347
	model : 0.06505961418151855
			 train-loss:  2.197311459360896 	 ± 0.22877959847221754
	data : 0.1148766040802002
	model : 0.06507310867309571
			 train-loss:  2.197164800859267 	 ± 0.228172490404398
	data : 0.11499300003051757
	model : 0.06505775451660156
			 train-loss:  2.1962171506116737 	 ± 0.22792830214692805
	data : 0.11499414443969727
	model : 0.06508984565734863
			 train-loss:  2.194437922949487 	 ± 0.22861966431212427
	data : 0.11508173942565918
	model : 0.06512775421142578
			 train-loss:  2.1953165966366965 	 ± 0.2283321152985574
	data : 0.11509723663330078
	model : 0.06512794494628907
			 train-loss:  2.1950842662861474 	 ± 0.22775284618751854
	data : 0.11514906883239746
	model : 0.06509733200073242
			 train-loss:  2.1949401045344885 	 ± 0.2271645435515979
	data : 0.11505093574523925
	model : 0.0651026725769043
			 train-loss:  2.1969211269170046 	 ± 0.22822035472237104
	data : 0.11453499794006347
	model : 0.06505784988403321
			 train-loss:  2.19800186218993 	 ± 0.22812039828367084
	data : 0.11450085639953614
	model : 0.06496305465698242
			 train-loss:  2.196826603609262 	 ± 0.22811675123636346
	data : 0.11464891433715821
	model : 0.06493468284606933
			 train-loss:  2.19776287873586 	 ± 0.22790449106354146
	data : 0.11474428176879883
	model : 0.06495370864868164
			 train-loss:  2.198338116310081 	 ± 0.22746423867739035
	data : 0.11480717658996582
	model : 0.06495161056518554
			 train-loss:  2.1987212509068135 	 ± 0.22694957951203354
	data : 0.11518416404724122
	model : 0.06513071060180664
			 train-loss:  2.1996787334933425 	 ± 0.22677430195175635
	data : 0.11499428749084473
	model : 0.06514787673950195
			 train-loss:  2.2030644326952835 	 ± 0.23116622780887255
	data : 0.11478371620178222
	model : 0.06515297889709473
			 train-loss:  2.2025665909051897 	 ± 0.23069451055049284
	data : 0.1146005630493164
	model : 0.06515254974365234
			 train-loss:  2.2009544847023426 	 ± 0.2312465320248597
	data : 0.11453619003295898
	model : 0.06517581939697266
			 train-loss:  2.20242178322065 	 ± 0.23160953566107947
	data : 0.11459102630615234
	model : 0.0649984359741211
			 train-loss:  2.201931294549275 	 ± 0.2311435114306525
	data : 0.11474661827087403
	model : 0.0650437355041504
			 train-loss:  2.2008436666984186 	 ± 0.23109642988665655
	data : 0.11492800712585449
	model : 0.06510705947875976
			 train-loss:  2.1999069266202973 	 ± 0.23092001061552325
	data : 0.11515688896179199
	model : 0.06515789031982422
			 train-loss:  2.2016399387017036 	 ± 0.23169134535828229
	data : 0.11523714065551757
	model : 0.06516590118408203
			 train-loss:  2.2014251129638747 	 ± 0.23115159206392297
	data : 0.11520199775695801
	model : 0.0652653694152832
			 train-loss:  2.2007460152873626 	 ± 0.23080216933795628
	data : 0.115059232711792
	model : 0.06522173881530761
			 train-loss:  2.2004911323483483 	 ± 0.23027869091446607
	data : 0.11496481895446778
	model : 0.06517953872680664
			 train-loss:  2.2001464611008053 	 ± 0.2297837869169813
	data : 0.11487016677856446
	model : 0.06515402793884277
			 train-loss:  2.2010675115042955 	 ± 0.22962686924771317
	data : 0.11478891372680664
	model : 0.06507396697998047
			 train-loss:  2.2005603915115572 	 ± 0.22920306010214014
	data : 0.11473078727722168
	model : 0.06495189666748047
			 train-loss:  2.2002538207551123 	 ± 0.22870795573190403
	data : 0.11480045318603516
	model : 0.06495680809020996
			 train-loss:  2.1987791857986805 	 ± 0.22918569113761372
	data : 0.1149831771850586
	model : 0.06498064994812011
			 train-loss:  2.1978337664936864 	 ± 0.2290699688923816
	data : 0.11507558822631836
	model : 0.06499457359313965
			 train-loss:  2.198277293531983 	 ± 0.22863161165631385
	data : 0.11516432762145996
	model : 0.06508340835571289
			 train-loss:  2.198994529961441 	 ± 0.22834763764374866
	data : 0.11520323753356934
	model : 0.06512384414672852
			 train-loss:  2.1986213835007553 	 ± 0.22788960460247404
	data : 0.11521868705749512
	model : 0.06512160301208496
			 train-loss:  2.1986553244394798 	 ± 0.2273692656557492
	data : 0.11506509780883789
	model : 0.06509966850280761
			 train-loss:  2.1983169544826855 	 ± 0.22690718768602183
	data : 0.11503911018371582
	model : 0.06500697135925293
			 train-loss:  2.197816583365876 	 ± 0.22651485873752905
	data : 0.11502413749694824
	model : 0.06489734649658203
			 train-loss:  2.1999745454874127 	 ± 0.2282696050863768
	data : 0.11508736610412598
	model : 0.06479754447937011
			 train-loss:  2.199231233297442 	 ± 0.2280263290965329
	data : 0.11508612632751465
	model : 0.06470313072204589
			 train-loss:  2.199533130441393 	 ± 0.2275614342799071
	data : 0.1151803970336914
	model : 0.06456260681152344
			 train-loss:  2.199898689058092 	 ± 0.22712108699571015
	data : 0.11528348922729492
	model : 0.06442704200744628
			 train-loss:  2.199795939226066 	 ± 0.2266232906536657
	data : 0.11541852951049805
	model : 0.06434159278869629
			 train-loss:  2.1993358880937888 	 ± 0.22622931041809907
	data : 0.11546077728271484
	model : 0.0642430305480957
			 train-loss:  2.197974259393257 	 ± 0.22666295230347094
	data : 0.11561670303344726
	model : 0.06413145065307617
			 train-loss:  2.1985969991142573 	 ± 0.22636290260774672
	data : 0.11554903984069824
	model : 0.0640416145324707
			 train-loss:  2.1993382910023564 	 ± 0.22614866567278094
	data : 0.11542606353759766
	model : 0.06399350166320801
			 train-loss:  2.1992885911619506 	 ± 0.22565989427951705
	data : 0.11546640396118164
	model : 0.06392173767089844
			 train-loss:  2.1989515701244615 	 ± 0.2252312867909831
	data : 0.11562833786010743
	model : 0.06391630172729493
			 train-loss:  2.1980638237981838 	 ± 0.22515383330477712
	data : 0.11558160781860352
	model : 0.0639233112335205
			 train-loss:  2.1979114387789345 	 ± 0.22468426091251473
	data : 0.11563248634338379
	model : 0.06394038200378419
			 train-loss:  2.1975948536649663 	 ± 0.22425799577245845
	data : 0.11576881408691406
	model : 0.06397395133972168
			 train-loss:  2.1975708553346536 	 ± 0.22378267091717965
	data : 0.11572790145874023
	model : 0.06396117210388183
			 train-loss:  2.1964122876839296 	 ± 0.22401821172358172
	data : 0.11552057266235352
	model : 0.06396369934082032
			 train-loss:  2.195691894082462 	 ± 0.22382201977723304
	data : 0.11547775268554687
	model : 0.06397738456726074
			 train-loss:  2.1967931432205265 	 ± 0.2239984888830971
	data : 0.11563024520874024
	model : 0.06393866539001465
			 train-loss:  2.1973366528749465 	 ± 0.22368920499637304
	data : 0.11562418937683105
	model : 0.06392378807067871
			 train-loss:  2.1962822467954326 	 ± 0.22382150014624358
	data : 0.11565017700195312
	model : 0.06397910118103027
			 train-loss:  2.1953899589451877 	 ± 0.22378769895269596
	data : 0.11569223403930665
	model : 0.06395516395568848
			 train-loss:  2.195480928008939 	 ± 0.22333123933472362
	data : 0.11574535369873047
	model : 0.06391801834106445
			 train-loss:  2.1935089953610154 	 ± 0.22498297521682123
	data : 0.11568093299865723
	model : 0.06398749351501465
			 train-loss:  2.1932156893671775 	 ± 0.2245700974846109
	data : 0.11560649871826172
	model : 0.06398272514343262
			 train-loss:  2.193367797184766 	 ± 0.22412583557050694
	data : 0.11560816764831543
	model : 0.0639230728149414
			 train-loss:  2.1937527704818045 	 ± 0.2237531641376037
	data : 0.11575202941894532
	model : 0.06390719413757324
			 train-loss:  2.1934511969166417 	 ± 0.22335188682047097
	data : 0.11579937934875488
	model : 0.06390719413757324
			 train-loss:  2.1932263316878355 	 ± 0.22293106483023595
	data : 0.11573376655578613
	model : 0.06386966705322265
			 train-loss:  2.1922763233184814 	 ± 0.22298922190822645
	data : 0.11579556465148926
	model : 0.06390328407287597
			 train-loss:  2.192071360895833 	 ± 0.22256817187320718
	data : 0.11572751998901368
	model : 0.06390295028686524
			 train-loss:  2.1935923979395913 	 ± 0.22342945045911405
	data : 0.11556649208068848
	model : 0.06389799118041992
			 train-loss:  2.194085431193175 	 ± 0.2231247652035717
	data : 0.11550564765930176
	model : 0.0638723373413086
			 train-loss:  2.193624187642195 	 ± 0.22280593099277807
	data : 0.11553835868835449
	model : 0.06383123397827148
			 train-loss:  2.1940724606607476 	 ± 0.22248336443804728
	data : 0.11551899909973144
	model : 0.06380949020385743
			 train-loss:  2.1931723509915173 	 ± 0.22251312977003954
	data : 0.11527924537658692
	model : 0.05545659065246582
#epoch  25    val-loss:  2.3724710627606043  train-loss:  2.1931723509915173  lr:  7.8125e-05
			 train-loss:  2.051732301712036 	 ± 0.0
	data : 5.736049652099609
	model : 0.07143616676330566
			 train-loss:  2.122145414352417 	 ± 0.07041311264038086
	data : 2.9334903955459595
	model : 0.06823420524597168
			 train-loss:  2.0712039470672607 	 ± 0.09217051480526331
	data : 1.9939671357472737
	model : 0.06703511873881023
			 train-loss:  2.1720452904701233 	 ± 0.1920377110556221
	data : 1.524304747581482
	model : 0.06644958257675171
			 train-loss:  2.124270534515381 	 ± 0.19655150775337968
	data : 1.2423309803009033
	model : 0.0661435604095459
			 train-loss:  2.0618977745374045 	 ± 0.2272565773227632
	data : 0.11800608634948731
	model : 0.06493892669677734
			 train-loss:  2.05902145590101 	 ± 0.2105166387697729
	data : 0.11476778984069824
	model : 0.06491174697875976
			 train-loss:  2.067871704697609 	 ± 0.19830755564372632
	data : 0.11456942558288574
	model : 0.06494283676147461
			 train-loss:  2.058475957976447 	 ± 0.18884539739850362
	data : 0.11439080238342285
	model : 0.06499552726745605
			 train-loss:  2.0516012787818907 	 ± 0.18033767391477432
	data : 0.11445927619934082
	model : 0.06498889923095703
			 train-loss:  2.0456938418475064 	 ± 0.17295704289442534
	data : 0.11458120346069336
	model : 0.06492495536804199
			 train-loss:  2.036697338024775 	 ± 0.16826051731637862
	data : 0.11463251113891601
	model : 0.06498217582702637
			 train-loss:  2.05212049300854 	 ± 0.17025942173338104
	data : 0.11469612121582032
	model : 0.06522045135498047
			 train-loss:  2.056305008275168 	 ± 0.1647583445829155
	data : 0.11446280479431152
	model : 0.06521377563476563
			 train-loss:  2.0696748018264772 	 ± 0.1668476669006408
	data : 0.11394023895263672
	model : 0.06524410247802734
			 train-loss:  2.0756795182824135 	 ± 0.16321491736064872
	data : 0.11373658180236816
	model : 0.06521992683410645
			 train-loss:  2.0881851350559906 	 ± 0.16605526199681334
	data : 0.11361651420593262
	model : 0.06516532897949219
			 train-loss:  2.0809134244918823 	 ± 0.16413824620858383
	data : 0.11373438835144042
	model : 0.06496477127075195
			 train-loss:  2.0713670881170976 	 ± 0.16481438965177875
	data : 0.11412158012390136
	model : 0.06499533653259278
			 train-loss:  2.092465376853943 	 ± 0.1851032464580687
	data : 0.11458196640014648
	model : 0.06498508453369141
			 train-loss:  2.1079068865094865 	 ± 0.19339192005233158
	data : 0.11463203430175781
	model : 0.06498579978942871
			 train-loss:  2.1220857446843926 	 ± 0.19980554392352343
	data : 0.11470966339111328
	model : 0.06497259140014648
			 train-loss:  2.129179508789726 	 ± 0.1982260777762516
	data : 0.11468992233276368
	model : 0.0649576187133789
			 train-loss:  2.1232612977425256 	 ± 0.1961171221581592
	data : 0.11450314521789551
	model : 0.06492581367492675
			 train-loss:  2.1187122583389284 	 ± 0.19344275330791277
	data : 0.11442890167236328
	model : 0.06510190963745117
			 train-loss:  2.1097224813241224 	 ± 0.1949391353916163
	data : 0.11448297500610352
	model : 0.06514539718627929
			 train-loss:  2.105142699347602 	 ± 0.19271519573949614
	data : 0.11449751853942872
	model : 0.06519808769226074
			 train-loss:  2.1119591082845415 	 ± 0.19252859840832715
	data : 0.11448459625244141
	model : 0.06521382331848144
			 train-loss:  2.1198792375367264 	 ± 0.19376655106237256
	data : 0.11456694602966308
	model : 0.06525330543518067
			 train-loss:  2.119409680366516 	 ± 0.1905265190668574
	data : 0.11467447280883789
	model : 0.06505546569824219
			 train-loss:  2.1318735153444353 	 ± 0.19947378464865761
	data : 0.11466064453125
	model : 0.06494216918945313
			 train-loss:  2.1284858882427216 	 ± 0.19723619429387926
	data : 0.11469354629516601
	model : 0.0649254322052002
			 train-loss:  2.1396159908988257 	 ± 0.20417492072626717
	data : 0.1147885799407959
	model : 0.06493253707885742
			 train-loss:  2.1383210981593415 	 ± 0.20128743369313018
	data : 0.11485772132873535
	model : 0.06493020057678223
			 train-loss:  2.139334399359567 	 ± 0.198479025169461
	data : 0.11483397483825683
	model : 0.06500053405761719
			 train-loss:  2.1486478447914124 	 ± 0.20331147799557037
	data : 0.11475768089294433
	model : 0.06507554054260253
			 train-loss:  2.1518126371744515 	 ± 0.20144218191837454
	data : 0.11467866897583008
	model : 0.06506810188293458
			 train-loss:  2.1430297619418095 	 ± 0.20582814356376686
	data : 0.11448712348937988
	model : 0.06507859230041504
			 train-loss:  2.135360231766334 	 ± 0.2086004822832404
	data : 0.114337158203125
	model : 0.06510338783264161
			 train-loss:  2.1359825283288956 	 ± 0.20601313076122052
	data : 0.11441411972045898
	model : 0.0650665283203125
			 train-loss:  2.142301248341072 	 ± 0.20737237667332772
	data : 0.11459846496582031
	model : 0.0650667667388916
			 train-loss:  2.1488972760382152 	 ± 0.2091966187261934
	data : 0.11458649635314941
	model : 0.06506524085998536
			 train-loss:  2.150140504504359 	 ± 0.20690672240409397
	data : 0.11466870307922364
	model : 0.06504721641540527
			 train-loss:  2.148820549249649 	 ± 0.20472505085394635
	data : 0.1147463321685791
	model : 0.06500968933105469
			 train-loss:  2.1440803474850125 	 ± 0.20486488115927523
	data : 0.11476273536682129
	model : 0.06499767303466797
			 train-loss:  2.146623056867848 	 ± 0.2033425144782593
	data : 0.11467294692993164
	model : 0.06492052078247071
			 train-loss:  2.14186714811528 	 ± 0.20373730255652225
	data : 0.1147110939025879
	model : 0.06494441032409667
			 train-loss:  2.144702119131883 	 ± 0.20253854397329976
	data : 0.11479077339172364
	model : 0.06492280960083008
			 train-loss:  2.1467174194297014 	 ± 0.20094683335600794
	data : 0.11475977897644044
	model : 0.06489377021789551
			 train-loss:  2.1490152049064637 	 ± 0.19957642226940256
	data : 0.11474037170410156
	model : 0.06489315032958984
			 train-loss:  2.1503573796328377 	 ± 0.19783787531827138
	data : 0.11473188400268555
	model : 0.06498022079467773
			 train-loss:  2.153086774624311 	 ± 0.19689353828365172
	data : 0.11472530364990234
	model : 0.06496129035949708
			 train-loss:  2.1492735169968515 	 ± 0.19695618778678586
	data : 0.11474499702453614
	model : 0.06493611335754394
			 train-loss:  2.1470663944880166 	 ± 0.19578446825889498
	data : 0.11475834846496583
	model : 0.06490592956542969
			 train-loss:  2.146417591788552 	 ± 0.19405502214563217
	data : 0.11472120285034179
	model : 0.0648963451385498
			 train-loss:  2.1494538145405904 	 ± 0.1936283155570038
	data : 0.11488261222839355
	model : 0.06484637260437012
			 train-loss:  2.1478513709285805 	 ± 0.19229656718777324
	data : 0.11491775512695312
	model : 0.06486263275146484
			 train-loss:  2.1534579334587884 	 ± 0.19527450044404257
	data : 0.11485705375671387
	model : 0.06489529609680175
			 train-loss:  2.156320911342815 	 ± 0.19483641467599314
	data : 0.11490011215209961
	model : 0.0649329662322998
			 train-loss:  2.154685739676158 	 ± 0.1936137772442494
	data : 0.11493072509765626
	model : 0.06494703292846679
			 train-loss:  2.1534788061360843 	 ± 0.19224767002761764
	data : 0.11490101814270019
	model : 0.06499629020690918
			 train-loss:  2.154620132138652 	 ± 0.1908992173056683
	data : 0.11483683586120605
	model : 0.06494970321655273
			 train-loss:  2.1555720283871604 	 ± 0.189526349647502
	data : 0.11472916603088379
	model : 0.06492404937744141
			 train-loss:  2.1515094116330147 	 ± 0.19078466752448184
	data : 0.11467523574829101
	model : 0.06495976448059082
			 train-loss:  2.153083522503193 	 ± 0.18972977834183988
	data : 0.11474456787109374
	model : 0.06493992805480957
			 train-loss:  2.156866871949398 	 ± 0.1907416222408449
	data : 0.1147160530090332
	model : 0.06493759155273438
			 train-loss:  2.158473964947373 	 ± 0.18976250267608363
	data : 0.11470460891723633
	model : 0.06497478485107422
			 train-loss:  2.1575462046791527 	 ± 0.18851504171051336
	data : 0.11482906341552734
	model : 0.0650299072265625
			 train-loss:  2.157119063363559 	 ± 0.18717714924191156
	data : 0.11481261253356934
	model : 0.06499419212341309
			 train-loss:  2.155356499126979 	 ± 0.18641120883558426
	data : 0.11464543342590332
	model : 0.06498641967773437
			 train-loss:  2.1585202687223193 	 ± 0.1869769369557618
	data : 0.11445708274841308
	model : 0.06490998268127442
			 train-loss:  2.158279753393597 	 ± 0.1856850057796151
	data : 0.11451849937438965
	model : 0.06487212181091309
			 train-loss:  2.157612330293002 	 ± 0.18449574531365745
	data : 0.11458020210266114
	model : 0.06484928131103515
			 train-loss:  2.1596909858085014 	 ± 0.18410354943634114
	data : 0.1146824836730957
	model : 0.06486940383911133
			 train-loss:  2.1544375165303546 	 ± 0.1883733443881263
	data : 0.11475334167480469
	model : 0.06490812301635743
			 train-loss:  2.161778151988983 	 ± 0.19763346125684622
	data : 0.11486678123474121
	model : 0.06495499610900879
			 train-loss:  2.1608474068827443 	 ± 0.1965135192123179
	data : 0.1147191047668457
	model : 0.06496729850769042
			 train-loss:  2.172123004228641 	 ± 0.2188885027193169
	data : 0.11473193168640136
	model : 0.06495676040649415
			 train-loss:  2.1725236735766447 	 ± 0.21752750441398733
	data : 0.1144986629486084
	model : 0.06494355201721191
			 train-loss:  2.1772072583436968 	 ± 0.2201355919023835
	data : 0.11446595191955566
	model : 0.06490015983581543
			 train-loss:  2.1759876292428855 	 ± 0.21904431239800773
	data : 0.11451206207275391
	model : 0.06492099761962891
			 train-loss:  2.1745265460595853 	 ± 0.21810135195033725
	data : 0.1147127628326416
	model : 0.0649378776550293
			 train-loss:  2.1763834924582977 	 ± 0.21743469221033845
	data : 0.11460866928100585
	model : 0.06497220993041992
			 train-loss:  2.1783324707122076 	 ± 0.21686468299717634
	data : 0.11472973823547364
	model : 0.06498913764953614
			 train-loss:  2.1809112071990966 	 ± 0.21687688505331504
	data : 0.1147416114807129
	model : 0.06500301361083985
			 train-loss:  2.1811124502226362 	 ± 0.21562026870797157
	data : 0.11470718383789062
	model : 0.0649871826171875
			 train-loss:  2.179307046977953 	 ± 0.21503028548511832
	data : 0.11452689170837402
	model : 0.06501679420471192
			 train-loss:  2.178091423078017 	 ± 0.2141054765673609
	data : 0.11442384719848633
	model : 0.06496253013610839
			 train-loss:  2.174795912892631 	 ± 0.21513205253371143
	data : 0.11452875137329102
	model : 0.06500005722045898
			 train-loss:  2.172640608416663 	 ± 0.21489763337931042
	data : 0.11469736099243164
	model : 0.06503376960754395
			 train-loss:  2.171353687296857 	 ± 0.2140620571971788
	data : 0.11474041938781739
	model : 0.06509318351745605
			 train-loss:  2.1696199502633964 	 ± 0.21353694011404087
	data : 0.11492080688476562
	model : 0.06509628295898437
			 train-loss:  2.175692134006049 	 ± 0.22022691404236094
	data : 0.11507439613342285
	model : 0.06514034271240235
			 train-loss:  2.1750193162167326 	 ± 0.21914843587011187
	data : 0.11503787040710449
	model : 0.06508970260620117
			 train-loss:  2.1707629354376543 	 ± 0.22186364457241634
	data : 0.11494274139404297
	model : 0.06508941650390625
			 train-loss:  2.1738471314311028 	 ± 0.22274289528760155
	data : 0.11482052803039551
	model : 0.06504974365234376
			 train-loss:  2.1714321817319417 	 ± 0.2228514730596417
	data : 0.11465802192687988
	model : 0.06500706672668458
			 train-loss:  2.1769300346471825 	 ± 0.22822790254722014
	data : 0.11470022201538085
	model : 0.06499500274658203
			 train-loss:  2.177874286969503 	 ± 0.22726463067171251
	data : 0.11473026275634765
	model : 0.0650019645690918
			 train-loss:  2.177550107240677 	 ± 0.22614845652351462
	data : 0.11474599838256835
	model : 0.06501851081848145
			 train-loss:  2.179391576512025 	 ± 0.22577833759575397
	data : 0.11480832099914551
	model : 0.0650641918182373
			 train-loss:  2.183965069406173 	 ± 0.22932225138216109
	data : 0.1148921012878418
	model : 0.06505265235900878
			 train-loss:  2.1850595647848925 	 ± 0.22847387810970898
	data : 0.1148341178894043
	model : 0.06505212783813477
			 train-loss:  2.1836927177814336 	 ± 0.22779556355960393
	data : 0.11482257843017578
	model : 0.06502332687377929
			 train-loss:  2.185908298265366 	 ± 0.22783137487836957
	data : 0.11470580101013184
	model : 0.06495718955993653
			 train-loss:  2.1900879290868653 	 ± 0.23076334928230086
	data : 0.11473050117492675
	model : 0.06489291191101074
			 train-loss:  2.1892576696716737 	 ± 0.22984149487882638
	data : 0.11475787162780762
	model : 0.06498603820800782
			 train-loss:  2.1910950508382587 	 ± 0.22956306666597445
	data : 0.11489911079406738
	model : 0.06497979164123535
			 train-loss:  2.1896759271621704 	 ± 0.22898302373844887
	data : 0.11489109992980957
	model : 0.0650256633758545
			 train-loss:  2.188460648059845 	 ± 0.22829266728576825
	data : 0.11491274833679199
	model : 0.0650357723236084
			 train-loss:  2.1873143116633096 	 ± 0.22757979725001942
	data : 0.11493768692016601
	model : 0.06506876945495606
			 train-loss:  2.1880463947142874 	 ± 0.22669278874516466
	data : 0.11499838829040528
	model : 0.06500372886657715
			 train-loss:  2.1851707359330845 	 ± 0.227730145941157
	data : 0.11471114158630372
	model : 0.065020751953125
			 train-loss:  2.1861777378801714 	 ± 0.22698168676780037
	data : 0.11465845108032227
	model : 0.0649688720703125
			 train-loss:  2.1882539759511532 	 ± 0.22707731572693204
	data : 0.1146094799041748
	model : 0.06497430801391602
			 train-loss:  2.1870418653405945 	 ± 0.2264697519458676
	data : 0.1146127700805664
	model : 0.06497006416320801
			 train-loss:  2.1876361563674407 	 ± 0.22559067794411466
	data : 0.11456847190856934
	model : 0.06498837471008301
			 train-loss:  2.186384341474307 	 ± 0.22504047802787291
	data : 0.11470756530761719
	model : 0.06505718231201171
			 train-loss:  2.1845643850935605 	 ± 0.22496330359835934
	data : 0.1147007942199707
	model : 0.06511344909667968
			 train-loss:  2.1858116298913957 	 ± 0.2244367824062564
	data : 0.11484785079956054
	model : 0.06515698432922364
			 train-loss:  2.1853085007549318 	 ± 0.22357537773144495
	data : 0.11484112739562988
	model : 0.06516432762145996
			 train-loss:  2.187480823915513 	 ± 0.223935763484941
	data : 0.11475367546081543
	model : 0.06512408256530762
			 train-loss:  2.188287854194641 	 ± 0.22320166550038367
	data : 0.11464266777038574
	model : 0.06502904891967773
			 train-loss:  2.1873998920763693 	 ± 0.22251786472845314
	data : 0.11467885971069336
	model : 0.06499161720275878
			 train-loss:  2.1873425340652464 	 ± 0.22162692633130623
	data : 0.11471414566040039
	model : 0.06497387886047364
			 train-loss:  2.1885864611655945 	 ± 0.22118337297976115
	data : 0.11480536460876464
	model : 0.06499347686767579
			 train-loss:  2.1889443331816065 	 ± 0.22034747182330275
	data : 0.11484847068786622
	model : 0.06505055427551269
			 train-loss:  2.186765738762915 	 ± 0.22085394430188435
	data : 0.11492938995361328
	model : 0.06515269279479981
			 train-loss:  2.188078624333522 	 ± 0.22049712504903388
	data : 0.11496434211730958
	model : 0.06518502235412597
			 train-loss:  2.188451795394604 	 ± 0.21968831123010166
	data : 0.114982271194458
	model : 0.06519932746887207
			 train-loss:  2.1884068723853307 	 ± 0.2188487992752855
	data : 0.11489949226379395
	model : 0.0651707649230957
			 train-loss:  2.1898239191734428 	 ± 0.21862069527609942
	data : 0.11486649513244629
	model : 0.06512885093688965
			 train-loss:  2.1898171068134165 	 ± 0.2177972762994475
	data : 0.11496658325195312
	model : 0.06505446434020996
			 train-loss:  2.1898127233804163 	 ± 0.21698308391285182
	data : 0.11504597663879394
	model : 0.06500711441040039
			 train-loss:  2.191242965945491 	 ± 0.21681101274016448
	data : 0.11503729820251465
	model : 0.06497864723205567
			 train-loss:  2.1893147545702316 	 ± 0.21717114289649883
	data : 0.11502056121826172
	model : 0.06500487327575684
			 train-loss:  2.189031872436078 	 ± 0.2164022426802664
	data : 0.11501784324645996
	model : 0.06503791809082031
			 train-loss:  2.187986854193867 	 ± 0.21596341347999165
	data : 0.11491317749023437
	model : 0.06506538391113281
			 train-loss:  2.1872970423252465 	 ± 0.21533769052567392
	data : 0.11488132476806641
	model : 0.06518373489379883
			 train-loss:  2.187413166250501 	 ± 0.21457161686745724
	data : 0.11482439041137696
	model : 0.0651883602142334
			 train-loss:  2.1861402481160264 	 ± 0.21433919819205668
	data : 0.11479864120483399
	model : 0.06516890525817871
			 train-loss:  2.184779605395357 	 ± 0.21419337480784056
	data : 0.11475553512573242
	model : 0.06507563591003418
			 train-loss:  2.183888772150853 	 ± 0.2137069487385562
	data : 0.11482362747192383
	model : 0.06506490707397461
			 train-loss:  2.1836824913819632 	 ± 0.21297790373535977
	data : 0.11484241485595703
	model : 0.06498136520385742
			 train-loss:  2.182359739829754 	 ± 0.21283494886551807
	data : 0.11493978500366211
	model : 0.06500372886657715
			 train-loss:  2.1822550835674757 	 ± 0.21210855348036473
	data : 0.11508092880249024
	model : 0.06501398086547852
			 train-loss:  2.180809860326806 	 ± 0.21210593963295526
	data : 0.1151125431060791
	model : 0.06512689590454102
			 train-loss:  2.179387857785096 	 ± 0.21209006997958904
	data : 0.11511831283569336
	model : 0.06515974998474121
			 train-loss:  2.177384483734233 	 ± 0.21277759036624014
	data : 0.11498069763183594
	model : 0.06526331901550293
			 train-loss:  2.178864672978719 	 ± 0.2128354466374761
	data : 0.11499624252319336
	model : 0.0652341365814209
			 train-loss:  2.1816830082444953 	 ± 0.21491948841274222
	data : 0.11492767333984374
	model : 0.06523041725158692
			 train-loss:  2.1802163469163993 	 ± 0.21496817855449715
	data : 0.11481714248657227
	model : 0.06523137092590332
			 train-loss:  2.179240501004886 	 ± 0.2146020243874057
	data : 0.11471743583679199
	model : 0.06522760391235352
			 train-loss:  2.1799615915719563 	 ± 0.2140900100223059
	data : 0.11482677459716797
	model : 0.06509976387023926
			 train-loss:  2.181139810623661 	 ± 0.21389859408459622
	data : 0.11468219757080078
	model : 0.06509561538696289
			 train-loss:  2.1815147354052615 	 ± 0.21326300835084486
	data : 0.11470565795898438
	model : 0.06512746810913086
			 train-loss:  2.182340284821334 	 ± 0.2128326598392695
	data : 0.11489291191101074
	model : 0.06509971618652344
			 train-loss:  2.1811740028707285 	 ± 0.21266076286831773
	data : 0.11496877670288086
	model : 0.06508541107177734
			 train-loss:  2.1811266627701573 	 ± 0.2119917986815585
	data : 0.11503252983093262
	model : 0.06515207290649414
			 train-loss:  2.1802336759865284 	 ± 0.21162805834585807
	data : 0.11510171890258789
	model : 0.06516242027282715
			 train-loss:  2.1815615981261924 	 ± 0.21163742300755897
	data : 0.11497387886047364
	model : 0.06511754989624023
			 train-loss:  2.1823957165082297 	 ± 0.21124850575443027
	data : 0.11490001678466796
	model : 0.06506280899047852
			 train-loss:  2.1826305045671988 	 ± 0.210620708347136
	data : 0.11490936279296875
	model : 0.06503996849060059
			 train-loss:  2.183830961221602 	 ± 0.21053618941792213
	data : 0.11498727798461914
	model : 0.06496858596801758
			 train-loss:  2.1836311318657615 	 ± 0.20991283083617202
	data : 0.11497931480407715
	model : 0.06500930786132812
			 train-loss:  2.184266947120069 	 ± 0.20943891107664678
	data : 0.11508364677429199
	model : 0.06503424644470215
			 train-loss:  2.186179772822443 	 ± 0.21026024972211182
	data : 0.11515998840332031
	model : 0.06510372161865234
			 train-loss:  2.1860677280596326 	 ± 0.20963854156277445
	data : 0.1151494026184082
	model : 0.06511588096618652
			 train-loss:  2.189235078512564 	 ± 0.213010946157218
	data : 0.11494994163513184
	model : 0.06512551307678223
			 train-loss:  2.1893133507055396 	 ± 0.2123859568482215
	data : 0.11481032371520997
	model : 0.06509604454040527
			 train-loss:  2.1892663965448302 	 ± 0.2117649196426958
	data : 0.1147542953491211
	model : 0.06501073837280273
			 train-loss:  2.18846299828485 	 ± 0.21140962556961376
	data : 0.1147801399230957
	model : 0.06497769355773926
			 train-loss:  2.1873872218104458 	 ± 0.21126934830791677
	data : 0.11487798690795899
	model : 0.06499524116516113
			 train-loss:  2.187436901974952 	 ± 0.21066239113546528
	data : 0.11498427391052246
	model : 0.06502695083618164
			 train-loss:  2.188886328424726 	 ± 0.21092794133724949
	data : 0.11505522727966308
	model : 0.06503362655639648
			 train-loss:  2.1875238926573233 	 ± 0.2110986722378141
	data : 0.11503663063049316
	model : 0.06511754989624023
			 train-loss:  2.186769694931763 	 ± 0.2107391618577031
	data : 0.11506462097167969
	model : 0.06516466140747071
			 train-loss:  2.187003035893601 	 ± 0.21016929287855934
	data : 0.1149777889251709
	model : 0.06511788368225098
			 train-loss:  2.1883311930981426 	 ± 0.21032916547174227
	data : 0.11477923393249512
	model : 0.06510071754455567
			 train-loss:  2.187797704670164 	 ± 0.20986551495166392
	data : 0.1146367073059082
	model : 0.06506624221801757
			 train-loss:  2.189688778055307 	 ± 0.21081723947665074
	data : 0.11478242874145508
	model : 0.06502327919006348
			 train-loss:  2.189410848276956 	 ± 0.21027052218365835
	data : 0.1147878646850586
	model : 0.06496992111206054
			 train-loss:  2.1895777273699233 	 ± 0.20970731049085156
	data : 0.11489415168762207
	model : 0.06497883796691895
			 train-loss:  2.1909147766621215 	 ± 0.20991736240360592
	data : 0.11502480506896973
	model : 0.06497030258178711
			 train-loss:  2.192761899973895 	 ± 0.2108432861302214
	data : 0.11511306762695313
	model : 0.06499772071838379
			 train-loss:  2.194172682941601 	 ± 0.21114945797203896
	data : 0.11494898796081543
	model : 0.06500706672668458
			 train-loss:  2.195245172250717 	 ± 0.21109149529580984
	data : 0.11495857238769532
	model : 0.06500544548034667
			 train-loss:  2.1974906052680727 	 ± 0.21275678230926498
	data : 0.11477885246276856
	model : 0.06498126983642578
			 train-loss:  2.19715818084737 	 ± 0.21224213488056426
	data : 0.11474499702453614
	model : 0.06496052742004395
			 train-loss:  2.1977421766833256 	 ± 0.2118350642914048
	data : 0.11475977897644044
	model : 0.06494250297546386
			 train-loss:  2.1969381583298686 	 ± 0.21157026305179624
	data : 0.11485071182250976
	model : 0.06495809555053711
			 train-loss:  2.1979669872671366 	 ± 0.21149707400020862
	data : 0.11487460136413574
	model : 0.06497039794921874
			 train-loss:  2.196812554962277 	 ± 0.21155407461264952
	data : 0.11495246887207031
	model : 0.06501359939575195
			 train-loss:  2.1966592018137274 	 ± 0.21101888250641695
	data : 0.11494135856628418
	model : 0.06506738662719727
			 train-loss:  2.195012172063192 	 ± 0.21172359397589613
	data : 0.1149892807006836
	model : 0.06512789726257324
			 train-loss:  2.1938861760557913 	 ± 0.21176733869999637
	data : 0.114971923828125
	model : 0.06522493362426758
			 train-loss:  2.1940560250112853 	 ± 0.2112425582723372
	data : 0.11487340927124023
	model : 0.06521902084350586
			 train-loss:  2.1956283474209335 	 ± 0.21186096994021783
	data : 0.11476469039916992
	model : 0.06514163017272949
			 train-loss:  2.1945005044266206 	 ± 0.2119230505863428
	data : 0.1147346019744873
	model : 0.06510953903198242
			 train-loss:  2.1939413398504257 	 ± 0.21153969559542127
	data : 0.11474308967590333
	model : 0.06505870819091797
			 train-loss:  2.192912314068619 	 ± 0.21151404107229896
	data : 0.11486339569091797
	model : 0.06496610641479492
			 train-loss:  2.192514662695403 	 ± 0.21106514831747827
	data : 0.11500530242919922
	model : 0.0649878978729248
			 train-loss:  2.191846393012061 	 ± 0.21075876293438364
	data : 0.11511693000793458
	model : 0.06506128311157226
			 train-loss:  2.1912059176201915 	 ± 0.21043950929803815
	data : 0.11512603759765624
	model : 0.06513667106628418
			 train-loss:  2.1929412609193384 	 ± 0.21138375685152105
	data : 0.11508216857910156
	model : 0.06515250205993653
			 train-loss:  2.194391472825726 	 ± 0.21188988397612724
	data : 0.11504368782043457
	model : 0.06511931419372559
			 train-loss:  2.193486979618165 	 ± 0.2117757254546302
	data : 0.11479568481445312
	model : 0.0651052474975586
			 train-loss:  2.1937927443247576 	 ± 0.21131183298036213
	data : 0.11471400260925294
	model : 0.0651127815246582
			 train-loss:  2.1930571651915045 	 ± 0.21107246496166168
	data : 0.11469249725341797
	model : 0.06501851081848145
			 train-loss:  2.1924320266360326 	 ± 0.2107631653886537
	data : 0.11481299400329589
	model : 0.06501669883728027
			 train-loss:  2.191898394534938 	 ± 0.21040528882157938
	data : 0.11482577323913574
	model : 0.065055513381958
			 train-loss:  2.1925320625305176 	 ± 0.2101101780086245
	data : 0.11495513916015625
	model : 0.06504979133605956
			 train-loss:  2.192044995760134 	 ± 0.20973631291724368
	data : 0.11494112014770508
	model : 0.06503334045410156
			 train-loss:  2.1919043598888077 	 ± 0.20925576740211332
	data : 0.11501245498657227
	model : 0.06508493423461914
			 train-loss:  2.191191722071448 	 ± 0.2090286861999473
	data : 0.11495518684387207
	model : 0.06509146690368653
			 train-loss:  2.190651681688097 	 ± 0.20869454392261935
	data : 0.11470866203308105
	model : 0.065057373046875
			 train-loss:  2.19004137614905 	 ± 0.2084062374980332
	data : 0.1147456169128418
	model : 0.06504731178283692
			 train-loss:  2.19012102293312 	 ± 0.2079310022901642
	data : 0.11487846374511719
	model : 0.0650062084197998
			 train-loss:  2.188567067934498 	 ± 0.20872063013130565
	data : 0.11494431495666504
	model : 0.06497349739074706
			 train-loss:  2.1872780360958792 	 ± 0.20911760558387063
	data : 0.11513271331787109
	model : 0.0649482250213623
			 train-loss:  2.1894310550991767 	 ± 0.2110737000708503
	data : 0.11542797088623047
	model : 0.06489710807800293
			 train-loss:  2.189975976406991 	 ± 0.2107535178016347
	data : 0.115391206741333
	model : 0.06483745574951172
			 train-loss:  2.188099758507425 	 ± 0.21213049915384133
	data : 0.1158724308013916
	model : 0.06481571197509765
			 train-loss:  2.1875894208039557 	 ± 0.21179362091738144
	data : 0.11585721969604493
	model : 0.06468229293823242
			 train-loss:  2.1879330529106986 	 ± 0.21138501855229147
	data : 0.11572117805480957
	model : 0.06448416709899903
			 train-loss:  2.188448038776364 	 ± 0.21105824612691867
	data : 0.11567502021789551
	model : 0.06430091857910156
			 train-loss:  2.190353064810127 	 ± 0.21253124305701646
	data : 0.11570935249328614
	model : 0.06408753395080566
			 train-loss:  2.19149277816739 	 ± 0.2127587322971515
	data : 0.11535172462463379
	model : 0.06389155387878417
			 train-loss:  2.1914447290928605 	 ± 0.2122949250886657
	data : 0.11561169624328613
	model : 0.06379027366638183
			 train-loss:  2.1910515826681385 	 ± 0.21191644022601036
	data : 0.11578459739685058
	model : 0.06378679275512696
			 train-loss:  2.192947311318798 	 ± 0.2134027629810676
	data : 0.11576299667358399
	model : 0.06380596160888671
			 train-loss:  2.191589632938648 	 ± 0.21393981182327623
	data : 0.11586928367614746
	model : 0.06388025283813477
			 train-loss:  2.1929912311324746 	 ± 0.2145450140430988
	data : 0.11593546867370605
	model : 0.06388444900512695
			 train-loss:  2.1943018110389385 	 ± 0.21501874547212
	data : 0.11582279205322266
	model : 0.06387386322021485
			 train-loss:  2.193985240002896 	 ± 0.21461541255162767
	data : 0.11560015678405762
	model : 0.06386294364929199
			 train-loss:  2.1950658509286782 	 ± 0.21479995567717167
	data : 0.11560039520263672
	model : 0.0638308048248291
			 train-loss:  2.195736713047269 	 ± 0.21459392991735787
	data : 0.11566119194030762
	model : 0.06378717422485351
			 train-loss:  2.1958589103041577 	 ± 0.21415089062550655
	data : 0.11559200286865234
	model : 0.06382102966308593
			 train-loss:  2.196560052648249 	 ± 0.21397597884605515
	data : 0.11562013626098633
	model : 0.06385178565979004
			 train-loss:  2.197217745582263 	 ± 0.21377167148781429
	data : 0.11570777893066406
	model : 0.06385412216186523
			 train-loss:  2.197418790635232 	 ± 0.21335043590670644
	data : 0.11574711799621581
	model : 0.06386756896972656
			 train-loss:  2.197128555991433 	 ± 0.21295684261428832
	data : 0.11563448905944824
	model : 0.06392126083374024
			 train-loss:  2.1976276493857427 	 ± 0.21265998580794646
	data : 0.11544933319091796
	model : 0.06388998031616211
			 train-loss:  2.1974065137691183 	 ± 0.21225175412768546
	data : 0.11538758277893066
	model : 0.06395063400268555
			 train-loss:  2.1969714495600488 	 ± 0.2119271356295768
	data : 0.11549215316772461
	model : 0.0639838695526123
			 train-loss:  2.1979892253875732 	 ± 0.21209508425597948
	data : 0.11552658081054687
	model : 0.0640148639678955
			 train-loss:  2.196664690488746 	 ± 0.2126823513998775
	data : 0.11554856300354004
	model : 0.06401548385620118
			 train-loss:  2.195118867101208 	 ± 0.21363897605269544
	data : 0.11571192741394043
	model : 0.06403880119323731
			 train-loss:  2.1942522310348878 	 ± 0.21364590996230143
	data : 0.1156285285949707
	model : 0.06398844718933105
			 train-loss:  2.1947889256477358 	 ± 0.2133863134786362
	data : 0.11540889739990234
	model : 0.06395139694213867
			 train-loss:  2.194730435234617 	 ± 0.2129628249811271
	data : 0.11540145874023437
	model : 0.06395554542541504
			 train-loss:  2.1947387092643313 	 ± 0.2125399000917432
	data : 0.11546521186828614
	model : 0.06393094062805176
			 train-loss:  2.193830841614795 	 ± 0.2126084741065033
	data : 0.11546912193298339
	model : 0.06396403312683105
			 train-loss:  2.1926521560338537 	 ± 0.21301618254392124
	data : 0.11546163558959961
	model : 0.0640103816986084
			 train-loss:  2.191815642749562 	 ± 0.21301569655715583
	data : 0.11566300392150879
	model : 0.06405062675476074
			 train-loss:  2.190753463190049 	 ± 0.21327478914234277
	data : 0.11543045043945313
	model : 0.055610942840576175
#epoch  26    val-loss:  2.4187139398173283  train-loss:  2.190753463190049  lr:  7.8125e-05
			 train-loss:  2.134636878967285 	 ± 0.0
	data : 5.66961932182312
	model : 0.07615327835083008
			 train-loss:  2.1278791427612305 	 ± 0.0067577362060546875
	data : 2.8993502855300903
	model : 0.07040190696716309
			 train-loss:  2.1781105995178223 	 ± 0.07125196952739238
	data : 1.971049388249715
	model : 0.06838877995808919
			 train-loss:  2.243086576461792 	 ± 0.12834821818248665
	data : 1.506937563419342
	model : 0.06743866205215454
			 train-loss:  2.2590868949890135 	 ± 0.11917488351320328
	data : 1.2285516262054443
	model : 0.06687650680541993
			 train-loss:  2.2250982522964478 	 ± 0.13270901706642196
	data : 0.11757750511169433
	model : 0.06462645530700684
			 train-loss:  2.2278317723955428 	 ± 0.12304698801327067
	data : 0.11472625732421875
	model : 0.06466703414916992
			 train-loss:  2.203814685344696 	 ± 0.131475223500861
	data : 0.11473841667175293
	model : 0.06480989456176758
			 train-loss:  2.201949199040731 	 ± 0.12406827778625727
	data : 0.11473221778869629
	model : 0.06485400199890137
			 train-loss:  2.184536647796631 	 ± 0.12877273106142761
	data : 0.11451544761657714
	model : 0.06486954689025878
			 train-loss:  2.168501312082464 	 ± 0.13283916512817343
	data : 0.11456990242004395
	model : 0.06484804153442383
			 train-loss:  2.1942376295725503 	 ± 0.1531720010100086
	data : 0.11456170082092285
	model : 0.06488962173461914
			 train-loss:  2.1828792095184326 	 ± 0.1523321474421913
	data : 0.11457653045654297
	model : 0.06553750038146973
			 train-loss:  2.174565179007394 	 ± 0.1498204844100688
	data : 0.11416053771972656
	model : 0.06565632820129394
			 train-loss:  2.1638640403747558 	 ± 0.15017645243101746
	data : 0.11423726081848144
	model : 0.06577801704406738
			 train-loss:  2.159516006708145 	 ± 0.1463796003109979
	data : 0.11416711807250976
	model : 0.06580653190612792
			 train-loss:  2.1706526840434357 	 ± 0.14883207207304647
	data : 0.11406359672546387
	model : 0.06575956344604492
			 train-loss:  2.1755787796444364 	 ± 0.14605788053347238
	data : 0.1140367031097412
	model : 0.06507697105407714
			 train-loss:  2.2038028491170785 	 ± 0.1858733057141132
	data : 0.114414644241333
	model : 0.06495232582092285
			 train-loss:  2.2003613352775573 	 ± 0.18178690222588165
	data : 0.11450767517089844
	model : 0.06483864784240723
			 train-loss:  2.189809657278515 	 ± 0.18357449221446134
	data : 0.11449475288391113
	model : 0.06483840942382812
			 train-loss:  2.203331345861608 	 ± 0.1897560322116475
	data : 0.11470112800598145
	model : 0.06486358642578124
			 train-loss:  2.1960347994514136 	 ± 0.18871429648277419
	data : 0.1147087574005127
	model : 0.06488924026489258
			 train-loss:  2.1899645874897637 	 ± 0.1870205844586249
	data : 0.1148289680480957
	model : 0.06495156288146972
			 train-loss:  2.1768347072601317 	 ± 0.19420371048737303
	data : 0.11479024887084961
	model : 0.0650609016418457
			 train-loss:  2.177140153371371 	 ± 0.19043852959773694
	data : 0.11476554870605468
	model : 0.06506304740905762
			 train-loss:  2.1833148620746754 	 ± 0.18951232206529461
	data : 0.11448054313659668
	model : 0.06503190994262695
			 train-loss:  2.183930371488844 	 ± 0.1861248868160348
	data : 0.11448335647583008
	model : 0.06503887176513672
			 train-loss:  2.1679981453665373 	 ± 0.2013834865592716
	data : 0.1143721580505371
	model : 0.06499409675598145
			 train-loss:  2.16142760515213 	 ± 0.20113540961097437
	data : 0.11440553665161132
	model : 0.0649611473083496
			 train-loss:  2.158476195027751 	 ± 0.19852396086659155
	data : 0.11446852684020996
	model : 0.06496882438659668
			 train-loss:  2.1498836055397987 	 ± 0.20116897340439985
	data : 0.1146623134613037
	model : 0.06504077911376953
			 train-loss:  2.150315118558479 	 ± 0.19811254976990803
	data : 0.11468210220336914
	model : 0.06507658958435059
			 train-loss:  2.1411587560878083 	 ± 0.20214078865571497
	data : 0.11456089019775391
	model : 0.06504244804382324
			 train-loss:  2.140309579031808 	 ± 0.1992936568336319
	data : 0.11442708969116211
	model : 0.06499528884887695
			 train-loss:  2.133498320976893 	 ± 0.2005952347681257
	data : 0.11435661315917969
	model : 0.0649949073791504
			 train-loss:  2.1346775551100037 	 ± 0.19799238353381604
	data : 0.11433806419372558
	model : 0.06500067710876464
			 train-loss:  2.1381845882064416 	 ± 0.19653104716330644
	data : 0.11425542831420898
	model : 0.06495747566223145
			 train-loss:  2.139487917606647 	 ± 0.1941613537452094
	data : 0.1144676685333252
	model : 0.06501312255859375
			 train-loss:  2.140823182463646 	 ± 0.19190023414071994
	data : 0.11459317207336425
	model : 0.065010404586792
			 train-loss:  2.141490491425119 	 ± 0.1895925212441741
	data : 0.11460671424865723
	model : 0.06495079994201661
			 train-loss:  2.13778608469736 	 ± 0.18881766564290195
	data : 0.11459097862243653
	model : 0.06494274139404296
			 train-loss:  2.132873293965362 	 ± 0.18930579372608133
	data : 0.11483988761901856
	model : 0.0649406909942627
			 train-loss:  2.1303936649452555 	 ± 0.1878472824961085
	data : 0.11482443809509277
	model : 0.06494779586791992
			 train-loss:  2.1293549405203924 	 ± 0.18587611081366356
	data : 0.11479172706604004
	model : 0.06499195098876953
			 train-loss:  2.130306179108827 	 ± 0.18395532510867152
	data : 0.11488947868347169
	model : 0.06504998207092286
			 train-loss:  2.127685382011089 	 ± 0.1828538365044597
	data : 0.11492681503295898
	model : 0.0650064468383789
			 train-loss:  2.125487633049488 	 ± 0.1815653237801506
	data : 0.11467657089233399
	model : 0.06497578620910645
			 train-loss:  2.123150944709778 	 ± 0.18043081045598422
	data : 0.11461915969848632
	model : 0.06500406265258789
			 train-loss:  2.1279717087745667 	 ± 0.1817771172530163
	data : 0.11470098495483398
	model : 0.06496701240539551
			 train-loss:  2.129738076060426 	 ± 0.18041901958244666
	data : 0.11473202705383301
	model : 0.06497468948364257
			 train-loss:  2.1324233206418843 	 ± 0.17970191675187824
	data : 0.11472172737121582
	model : 0.06500735282897949
			 train-loss:  2.132719631464976 	 ± 0.1780113669822877
	data : 0.11484279632568359
	model : 0.06504130363464355
			 train-loss:  2.136284022419541 	 ± 0.1782542843422455
	data : 0.11485652923583985
	model : 0.06500754356384278
			 train-loss:  2.1355364777825097 	 ± 0.17671176110096776
	data : 0.11486077308654785
	model : 0.06507930755615235
			 train-loss:  2.1349172187703 	 ± 0.17518707765036742
	data : 0.11476354598999024
	model : 0.06507110595703125
			 train-loss:  2.133803453361779 	 ± 0.1738434601792034
	data : 0.1148611068725586
	model : 0.0650179386138916
			 train-loss:  2.1314572918004004 	 ± 0.1732461903927966
	data : 0.11485309600830078
	model : 0.0650069236755371
			 train-loss:  2.13393180249101 	 ± 0.17280240824260903
	data : 0.11493964195251465
	model : 0.06499176025390625
			 train-loss:  2.1398276646931964 	 ± 0.1772396814999233
	data : 0.1150315284729004
	model : 0.0649183750152588
			 train-loss:  2.1377740688011295 	 ± 0.1764991728570887
	data : 0.11497983932495118
	model : 0.06492557525634765
			 train-loss:  2.132628392788672 	 ± 0.1796236731685958
	data : 0.11482009887695313
	model : 0.0649838924407959
			 train-loss:  2.1327229435481723 	 ± 0.1781939412825884
	data : 0.11477346420288086
	model : 0.0649794101715088
			 train-loss:  2.1323786210268736 	 ± 0.17681744251529036
	data : 0.11470837593078613
	model : 0.0650146484375
			 train-loss:  2.1286864995956423 	 ± 0.17792090874350203
	data : 0.11452102661132812
	model : 0.06495933532714844
			 train-loss:  2.134354289734002 	 ± 0.182384937999633
	data : 0.1145707130432129
	model : 0.06495823860168456
			 train-loss:  2.1305047415975316 	 ± 0.18370040666613868
	data : 0.11472787857055664
	model : 0.06491365432739257
			 train-loss:  2.1349048912525177 	 ± 0.18586765534414726
	data : 0.11488995552062989
	model : 0.06488251686096191
			 train-loss:  2.138318823731464 	 ± 0.18665112598983674
	data : 0.1148148536682129
	model : 0.0648775577545166
			 train-loss:  2.141387641429901 	 ± 0.1870581867714599
	data : 0.11472530364990234
	model : 0.0649505615234375
			 train-loss:  2.147365944486269 	 ± 0.19235317798045085
	data : 0.11470794677734375
	model : 0.06494159698486328
			 train-loss:  2.1547432127926083 	 ± 0.2008730194799228
	data : 0.11453533172607422
	model : 0.06491727828979492
			 train-loss:  2.1512015143485916 	 ± 0.20174333160491537
	data : 0.1143798828125
	model : 0.06497859954833984
			 train-loss:  2.156770134294355 	 ± 0.20594674645018435
	data : 0.11453442573547364
	model : 0.06493663787841797
			 train-loss:  2.1622018829981484 	 ± 0.209837626979474
	data : 0.11467194557189941
	model : 0.06491293907165527
			 train-loss:  2.1581571776615944 	 ± 0.21137510877732868
	data : 0.11463990211486816
	model : 0.06490054130554199
			 train-loss:  2.156127785707449 	 ± 0.21074198679091632
	data : 0.11473760604858399
	model : 0.06496644020080566
			 train-loss:  2.154722172480363 	 ± 0.20974968569069968
	data : 0.11479802131652832
	model : 0.06494078636169434
			 train-loss:  2.151004466829421 	 ± 0.2109883810663555
	data : 0.11464767456054688
	model : 0.06500124931335449
			 train-loss:  2.1508180245757105 	 ± 0.2096721055114214
	data : 0.11465530395507813
	model : 0.06499500274658203
			 train-loss:  2.150533957245909 	 ± 0.2083893034473659
	data : 0.114768648147583
	model : 0.06500306129455566
			 train-loss:  2.1505855569025365 	 ± 0.20711525981577822
	data : 0.11481614112854004
	model : 0.0649869441986084
			 train-loss:  2.155169718236808 	 ± 0.2100073680788663
	data : 0.1147690773010254
	model : 0.06498689651489258
			 train-loss:  2.1530565278870717 	 ± 0.20963945335521178
	data : 0.11484036445617676
	model : 0.0649637222290039
			 train-loss:  2.157540091346292 	 ± 0.21241528555524883
	data : 0.11484351158142089
	model : 0.06498713493347168
			 train-loss:  2.1612538487412207 	 ± 0.2139343799290559
	data : 0.11476311683654786
	model : 0.064967679977417
			 train-loss:  2.16554558962241 	 ± 0.21639290261966948
	data : 0.1146517276763916
	model : 0.06491670608520508
			 train-loss:  2.163102851672606 	 ± 0.21636289472371598
	data : 0.11468029022216797
	model : 0.06489219665527343
			 train-loss:  2.16432066713826 	 ± 0.2154470354974668
	data : 0.11472492218017578
	model : 0.06487727165222168
			 train-loss:  2.1618505822287664 	 ± 0.2155103091937066
	data : 0.11479806900024414
	model : 0.06493706703186035
			 train-loss:  2.1588109744774115 	 ± 0.21625411325185268
	data : 0.11487045288085937
	model : 0.0650360107421875
			 train-loss:  2.158402030882628 	 ± 0.21511098416127128
	data : 0.1149665355682373
	model : 0.06509542465209961
			 train-loss:  2.155324384730349 	 ± 0.21597822702349614
	data : 0.11506710052490235
	model : 0.0652010440826416
			 train-loss:  2.1511704173493893 	 ± 0.2185294272309158
	data : 0.1149970531463623
	model : 0.06524772644042968
			 train-loss:  2.1518734643333834 	 ± 0.21748307301326847
	data : 0.11487836837768554
	model : 0.0651926040649414
			 train-loss:  2.154294554144144 	 ± 0.2176305344271263
	data : 0.11475772857666015
	model : 0.06509037017822265
			 train-loss:  2.155986430718727 	 ± 0.217139507006543
	data : 0.11466817855834961
	model : 0.06507115364074707
			 train-loss:  2.1567815530056857 	 ± 0.21617070285881843
	data : 0.11467928886413574
	model : 0.06499476432800293
			 train-loss:  2.158596162844186 	 ± 0.21582504520219362
	data : 0.11469345092773438
	model : 0.06497178077697754
			 train-loss:  2.158966437578201 	 ± 0.21477480967201437
	data : 0.11477346420288086
	model : 0.0650094985961914
			 train-loss:  2.1581112051954365 	 ± 0.21387998053625681
	data : 0.11486048698425293
	model : 0.06505670547485351
			 train-loss:  2.160666099950379 	 ± 0.2143722121917526
	data : 0.11498332023620605
	model : 0.06503210067749024
			 train-loss:  2.1623174484493664 	 ± 0.21397996419711757
	data : 0.11484289169311523
	model : 0.06502904891967773
			 train-loss:  2.165212155534671 	 ± 0.21496565370642112
	data : 0.11481795310974122
	model : 0.06494646072387696
			 train-loss:  2.164666265533084 	 ± 0.21401197722595908
	data : 0.11476249694824218
	model : 0.06496038436889648
			 train-loss:  2.1633423600556716 	 ± 0.2134316670458376
	data : 0.11486940383911133
	model : 0.06498579978942871
			 train-loss:  2.1666980981826782 	 ± 0.2152231644573495
	data : 0.11485638618469238
	model : 0.06505565643310547
			 train-loss:  2.1689554905449904 	 ± 0.21549330626071228
	data : 0.1149129867553711
	model : 0.06505169868469238
			 train-loss:  2.1678225020749853 	 ± 0.21482544021023006
	data : 0.11490373611450196
	model : 0.06519126892089844
			 train-loss:  2.1673696528781545 	 ± 0.2138989889478948
	data : 0.11505746841430664
	model : 0.06514468193054199
			 train-loss:  2.1676414625064746 	 ± 0.2129523823314406
	data : 0.11497764587402344
	model : 0.06510858535766602
			 train-loss:  2.1674945365105356 	 ± 0.21200522178713233
	data : 0.11487698554992676
	model : 0.0650418758392334
			 train-loss:  2.167070318112331 	 ± 0.21111280306549668
	data : 0.11492204666137695
	model : 0.06502885818481445
			 train-loss:  2.1722631423096908 	 ± 0.21731257594750467
	data : 0.11502785682678222
	model : 0.06491765975952149
			 train-loss:  2.1753447522287783 	 ± 0.21885311526393536
	data : 0.11496825218200683
	model : 0.06490521430969239
			 train-loss:  2.1800864821877974 	 ± 0.22376202524150127
	data : 0.11489405632019042
	model : 0.06489663124084473
			 train-loss:  2.1766900598493395 	 ± 0.22578671218699684
	data : 0.1148597240447998
	model : 0.06497607231140137
			 train-loss:  2.1756976044784158 	 ± 0.22508409489442452
	data : 0.11489357948303222
	model : 0.06497950553894043
			 train-loss:  2.176106740446652 	 ± 0.22418042694540294
	data : 0.11478781700134277
	model : 0.06500873565673829
			 train-loss:  2.177964789668719 	 ± 0.22416263272312206
	data : 0.11470599174499511
	model : 0.0650148868560791
			 train-loss:  2.1841894486718925 	 ± 0.23341630496865157
	data : 0.11466851234436035
	model : 0.06494507789611817
			 train-loss:  2.184503628582251 	 ± 0.23248340122216357
	data : 0.11474041938781739
	model : 0.06488852500915528
			 train-loss:  2.182713451424265 	 ± 0.23237919542225882
	data : 0.11474642753601075
	model : 0.064910888671875
			 train-loss:  2.181596378164907 	 ± 0.23177163705377024
	data : 0.1149073600769043
	model : 0.06492385864257813
			 train-loss:  2.183061764717102 	 ± 0.23141871006752235
	data : 0.1148691177368164
	model : 0.06490864753723144
			 train-loss:  2.183894693851471 	 ± 0.23068659275197334
	data : 0.11505780220031739
	model : 0.06500015258789063
			 train-loss:  2.182482076442148 	 ± 0.23032305473021508
	data : 0.11510257720947266
	model : 0.06504602432250976
			 train-loss:  2.1814676923677325 	 ± 0.22970621743518096
	data : 0.11499333381652832
	model : 0.06505165100097657
			 train-loss:  2.1829892544783362 	 ± 0.22946079207025366
	data : 0.1149984359741211
	model : 0.06503510475158691
			 train-loss:  2.184579782302563 	 ± 0.22928929153058533
	data : 0.11497340202331544
	model : 0.06498093605041504
			 train-loss:  2.184616575714286 	 ± 0.2284128502659708
	data : 0.11475987434387207
	model : 0.06495137214660644
			 train-loss:  2.1852001863898654 	 ± 0.2276440275290557
	data : 0.11476750373840332
	model : 0.0648984432220459
			 train-loss:  2.18552823801686 	 ± 0.22681792523942462
	data : 0.11480751037597656
	model : 0.06490368843078613
			 train-loss:  2.186358936687014 	 ± 0.2261729892889044
	data : 0.11479554176330567
	model : 0.06491732597351074
			 train-loss:  2.1845781529391255 	 ± 0.22627470138993824
	data : 0.11485934257507324
	model : 0.06503658294677735
			 train-loss:  2.1837392230244244 	 ± 0.22565190358560794
	data : 0.11503195762634277
	model : 0.06509861946105958
			 train-loss:  2.1817586752620057 	 ± 0.22601013494291872
	data : 0.1150601863861084
	model : 0.0651205062866211
			 train-loss:  2.1823364047036655 	 ± 0.2252912750811626
	data : 0.11497645378112793
	model : 0.0650742530822754
			 train-loss:  2.181108438711372 	 ± 0.2249424296866834
	data : 0.11514220237731934
	model : 0.06508822441101074
			 train-loss:  2.179322738306863 	 ± 0.2251242045164658
	data : 0.11508140563964844
	model : 0.06503801345825196
			 train-loss:  2.1795815914235215 	 ± 0.22434537902577345
	data : 0.11501460075378418
	model : 0.06501193046569824
			 train-loss:  2.178111750475118 	 ± 0.22423431374637848
	data : 0.11499648094177246
	model : 0.06502561569213867
			 train-loss:  2.180182156029281 	 ± 0.22480681957003348
	data : 0.11520180702209473
	model : 0.06507306098937989
			 train-loss:  2.1800237521529198 	 ± 0.22403288861363718
	data : 0.11507248878479004
	model : 0.06509671211242676
			 train-loss:  2.180470350693012 	 ± 0.2233233373747376
	data : 0.11511859893798829
	model : 0.06511063575744629
			 train-loss:  2.1813994213326335 	 ± 0.22283822598148947
	data : 0.11513924598693848
	model : 0.06510944366455078
			 train-loss:  2.1821467154691008 	 ± 0.22226247225513582
	data : 0.11501731872558593
	model : 0.06511287689208985
			 train-loss:  2.1825081585226833 	 ± 0.22155365699897564
	data : 0.11491184234619141
	model : 0.06509394645690918
			 train-loss:  2.181415019419369 	 ± 0.22120904048614212
	data : 0.11471576690673828
	model : 0.06507887840270996
			 train-loss:  2.1831320325533548 	 ± 0.2214644184590847
	data : 0.11467828750610351
	model : 0.06503834724426269
			 train-loss:  2.1817516120064337 	 ± 0.22137640252137505
	data : 0.11469521522521972
	model : 0.0650421142578125
			 train-loss:  2.1811086386442184 	 ± 0.2207884040892966
	data : 0.11487317085266113
	model : 0.06501555442810059
			 train-loss:  2.18133342577741 	 ± 0.22008314017506336
	data : 0.11480278968811035
	model : 0.06505322456359863
			 train-loss:  2.1808793459619795 	 ± 0.21943931307776685
	data : 0.1149442195892334
	model : 0.06507844924926758
			 train-loss:  2.1808954969529184 	 ± 0.21873039073324962
	data : 0.11494450569152832
	model : 0.06514396667480468
			 train-loss:  2.180598482871667 	 ± 0.2180595600473125
	data : 0.1153778076171875
	model : 0.06510815620422364
			 train-loss:  2.1817432733098414 	 ± 0.21783376852327066
	data : 0.11528096199035645
	model : 0.06514172554016114
			 train-loss:  2.1828823927082595 	 ± 0.2176119172755578
	data : 0.11541557312011719
	model : 0.06508984565734863
			 train-loss:  2.1832171603568695 	 ± 0.21696733313957905
	data : 0.11522350311279297
	model : 0.06509275436401367
			 train-loss:  2.1813183091580868 	 ± 0.21760951749873964
	data : 0.11520180702209473
	model : 0.06507320404052734
			 train-loss:  2.180909942395939 	 ± 0.21699414885304444
	data : 0.1148488998413086
	model : 0.06511330604553223
			 train-loss:  2.1864153963548167 	 ± 0.22732293597932332
	data : 0.11489381790161132
	model : 0.06506342887878418
			 train-loss:  2.187915502881711 	 ± 0.2274274391012546
	data : 0.1148655891418457
	model : 0.06506881713867188
			 train-loss:  2.1897551526383654 	 ± 0.22794625940357777
	data : 0.11506304740905762
	model : 0.06503691673278808
			 train-loss:  2.1929071534763684 	 ± 0.23081150113234394
	data : 0.11514081954956054
	model : 0.0650395393371582
			 train-loss:  2.193080864038812 	 ± 0.2301260536865365
	data : 0.11497688293457031
	model : 0.06502785682678222
			 train-loss:  2.1938501853428916 	 ± 0.22965002674508378
	data : 0.11498923301696777
	model : 0.06503934860229492
			 train-loss:  2.192131824436642 	 ± 0.2300398291269872
	data : 0.11492776870727539
	model : 0.06505613327026367
			 train-loss:  2.1937123459471755 	 ± 0.23027129416944142
	data : 0.11482782363891601
	model : 0.06501784324645996
			 train-loss:  2.1937644551782047 	 ± 0.22959402610410481
	data : 0.1148721694946289
	model : 0.06499032974243164
			 train-loss:  2.1949066864816764 	 ± 0.2294056427528436
	data : 0.11491050720214843
	model : 0.06497092247009277
			 train-loss:  2.197112507598345 	 ± 0.23054935240883678
	data : 0.11498117446899414
	model : 0.06495299339294433
			 train-loss:  2.1998794630083736 	 ± 0.2327285980804972
	data : 0.11501951217651367
	model : 0.06496376991271972
			 train-loss:  2.1996953555907326 	 ± 0.23207150833462245
	data : 0.11500954627990723
	model : 0.06499776840209961
			 train-loss:  2.203129017693656 	 ± 0.23579842197098502
	data : 0.11497035026550292
	model : 0.06501641273498535
			 train-loss:  2.204061126167124 	 ± 0.23545068742606504
	data : 0.11496028900146485
	model : 0.06498026847839355
			 train-loss:  2.203386102019057 	 ± 0.23495535407643667
	data : 0.11488456726074218
	model : 0.06502962112426758
			 train-loss:  2.2029275572701787 	 ± 0.23437384682607135
	data : 0.11489710807800294
	model : 0.06498961448669434
			 train-loss:  2.2035077457321424 	 ± 0.23384640373959795
	data : 0.11488604545593262
	model : 0.0649421215057373
			 train-loss:  2.2024270203378467 	 ± 0.23364375857658645
	data : 0.11491522789001465
	model : 0.06494598388671875
			 train-loss:  2.202292974482584 	 ± 0.23300438040192692
	data : 0.11498842239379883
	model : 0.06503424644470215
			 train-loss:  2.2029390937679416 	 ± 0.23252591458916963
	data : 0.11502079963684082
	model : 0.06502490043640137
			 train-loss:  2.2013431298928183 	 ± 0.2328871359366135
	data : 0.11494307518005371
	model : 0.06512131690979003
			 train-loss:  2.2010163457497307 	 ± 0.23229549527344145
	data : 0.11492438316345215
	model : 0.06516618728637695
			 train-loss:  2.2006407054694925 	 ± 0.23172284827314885
	data : 0.11490354537963868
	model : 0.06518030166625977
			 train-loss:  2.200888387618526 	 ± 0.23112365118416817
	data : 0.11499209403991699
	model : 0.06517658233642579
			 train-loss:  2.200965840548755 	 ± 0.23050726545706018
	data : 0.11489286422729492
	model : 0.06514363288879395
			 train-loss:  2.1995013996641686 	 ± 0.2307639739823971
	data : 0.11484007835388184
	model : 0.06504607200622559
			 train-loss:  2.1979179558930575 	 ± 0.23117444996731099
	data : 0.11484642028808593
	model : 0.06509108543395996
			 train-loss:  2.1976188973376627 	 ± 0.230601947103495
	data : 0.11493821144104004
	model : 0.06507043838500977
			 train-loss:  2.1975510270183625 	 ± 0.22999938751391666
	data : 0.11482152938842774
	model : 0.06505584716796875
			 train-loss:  2.1981633938848972 	 ± 0.22955570693861863
	data : 0.11493782997131348
	model : 0.06508207321166992
			 train-loss:  2.199092721692021 	 ± 0.22932206189358575
	data : 0.11502518653869628
	model : 0.06511411666870118
			 train-loss:  2.200532435141888 	 ± 0.2296030886369155
	data : 0.11519618034362793
	model : 0.06507906913757325
			 train-loss:  2.2012139026935285 	 ± 0.22921022004441965
	data : 0.11515045166015625
	model : 0.06509809494018555
			 train-loss:  2.2015534213611057 	 ± 0.22867390672509602
	data : 0.1152043342590332
	model : 0.06507382392883301
			 train-loss:  2.2016152548911 	 ± 0.22809442039782624
	data : 0.11509661674499512
	model : 0.06506257057189942
			 train-loss:  2.1996473862667276 	 ± 0.22918810075525467
	data : 0.1149564266204834
	model : 0.06503787040710449
			 train-loss:  2.1992636176209954 	 ± 0.2286752958663735
	data : 0.11488962173461914
	model : 0.06501789093017578
			 train-loss:  2.1977114659547805 	 ± 0.2291513795923282
	data : 0.11496143341064453
	model : 0.06497392654418946
			 train-loss:  2.197584299898859 	 ± 0.22858771503321923
	data : 0.11505179405212403
	model : 0.06495356559753418
			 train-loss:  2.199589749964157 	 ± 0.2297869803085219
	data : 0.11515798568725585
	model : 0.06496391296386719
			 train-loss:  2.1990373387125324 	 ± 0.2293547244380544
	data : 0.11529436111450195
	model : 0.06498680114746094
			 train-loss:  2.1983853444164874 	 ± 0.22898039979595572
	data : 0.11523685455322266
	model : 0.06509580612182617
			 train-loss:  2.197623055737193 	 ± 0.2286805603122109
	data : 0.11507811546325683
	model : 0.06516199111938477
			 train-loss:  2.19679897792131 	 ± 0.22842976336134663
	data : 0.11504898071289063
	model : 0.06518692970275879
			 train-loss:  2.197646191154701 	 ± 0.22820153244740077
	data : 0.11487832069396972
	model : 0.0651780128479004
			 train-loss:  2.1999945405584116 	 ± 0.23014588094696176
	data : 0.11480479240417481
	model : 0.0651512622833252
			 train-loss:  2.1984261548119868 	 ± 0.23070617835293017
	data : 0.11468439102172852
	model : 0.06506690979003907
			 train-loss:  2.1970465716861542 	 ± 0.2310187565834492
	data : 0.1148451805114746
	model : 0.06505208015441895
			 train-loss:  2.1971659377852886 	 ± 0.23047715987481684
	data : 0.11490349769592285
	model : 0.06504149436950683
			 train-loss:  2.1973866831581548 	 ± 0.22995529613133886
	data : 0.11500000953674316
	model : 0.06506195068359374
			 train-loss:  2.1960012890363525 	 ± 0.23029996335766417
	data : 0.11504077911376953
	model : 0.06512994766235351
			 train-loss:  2.1947798244306975 	 ± 0.2304517791233186
	data : 0.11517610549926757
	model : 0.06513004302978516
			 train-loss:  2.1944109423216 	 ± 0.2299785387404363
	data : 0.11513814926147461
	model : 0.06512446403503418
			 train-loss:  2.1959422214163675 	 ± 0.23054154060469528
	data : 0.11503000259399414
	model : 0.06511416435241699
			 train-loss:  2.1944674407282183 	 ± 0.23102871918812318
	data : 0.11505837440490722
	model : 0.06510634422302246
			 train-loss:  2.1937521745305544 	 ± 0.23073892484177208
	data : 0.11501688957214355
	model : 0.06506471633911133
			 train-loss:  2.1937645782618764 	 ± 0.23021159374107367
	data : 0.11502599716186523
	model : 0.06499481201171875
			 train-loss:  2.194676884196021 	 ± 0.230084234323633
	data : 0.11497635841369629
	model : 0.0649022102355957
			 train-loss:  2.1940943660779237 	 ± 0.22972562988427872
	data : 0.11503496170043945
	model : 0.06486806869506836
			 train-loss:  2.193698272511766 	 ± 0.22928326940273386
	data : 0.11514930725097657
	model : 0.06481246948242188
			 train-loss:  2.1934188337069456 	 ± 0.22880648844237647
	data : 0.11544270515441894
	model : 0.0647697925567627
			 train-loss:  2.192887270557029 	 ± 0.22843314948427168
	data : 0.11552662849426269
	model : 0.06474246978759765
			 train-loss:  2.192010055647956 	 ± 0.22830276924094242
	data : 0.1155848503112793
	model : 0.06461615562438965
			 train-loss:  2.191414072977758 	 ± 0.22797246411208769
	data : 0.11548385620117188
	model : 0.0644301414489746
			 train-loss:  2.1929298051128305 	 ± 0.22860821862236064
	data : 0.11526250839233398
	model : 0.06423110961914062
			 train-loss:  2.192759563525518 	 ± 0.22812075425287875
	data : 0.11517086029052734
	model : 0.0640336036682129
			 train-loss:  2.1930309294613166 	 ± 0.2276590069291024
	data : 0.1153982162475586
	model : 0.0638587474822998
			 train-loss:  2.193046845042187 	 ± 0.22716368460193598
	data : 0.11554875373840331
	model : 0.06382131576538086
			 train-loss:  2.1930720821603553 	 ± 0.22667177815332618
	data : 0.1156930923461914
	model : 0.06384282112121582
			 train-loss:  2.194722909865708 	 ± 0.22757011601365001
	data : 0.11580257415771485
	model : 0.06383557319641113
			 train-loss:  2.1939972761874547 	 ± 0.2273500587062483
	data : 0.11576776504516602
	model : 0.06386923789978027
			 train-loss:  2.1941655716325483 	 ± 0.22687829212324298
	data : 0.11559109687805176
	model : 0.06387290954589844
			 train-loss:  2.194385110063756 	 ± 0.22641996442295106
	data : 0.11541938781738281
	model : 0.0638361930847168
			 train-loss:  2.193865634122137 	 ± 0.22608004657460726
	data : 0.11535873413085937
	model : 0.06379127502441406
			 train-loss:  2.1926224261899536 	 ± 0.2264095355503743
	data : 0.11553487777709961
	model : 0.06381196975708008
			 train-loss:  2.1925108653156697 	 ± 0.2259399122098905
	data : 0.11564884185791016
	model : 0.06380290985107422
			 train-loss:  2.1938644443096975 	 ± 0.22643168583552975
	data : 0.11560769081115722
	model : 0.0638209342956543
			 train-loss:  2.1933444092671075 	 ± 0.2261024374245886
	data : 0.11574935913085938
	model : 0.06386723518371581
			 train-loss:  2.192643933276418 	 ± 0.22589366175212985
	data : 0.11587758064270019
	model : 0.06389904022216797
			 train-loss:  2.1928263794292104 	 ± 0.22544424855679604
	data : 0.11568198204040528
	model : 0.06390128135681153
			 train-loss:  2.1918883985943265 	 ± 0.22545258021722478
	data : 0.11551737785339355
	model : 0.06390876770019531
			 train-loss:  2.1914189697289075 	 ± 0.22510908294206852
	data : 0.11564450263977051
	model : 0.06390533447265626
			 train-loss:  2.1919686224995827 	 ± 0.22481321802339338
	data : 0.11569666862487793
	model : 0.06392173767089844
			 train-loss:  2.1907879579357985 	 ± 0.2251156462945621
	data : 0.11559958457946777
	model : 0.06395702362060547
			 train-loss:  2.190400330161276 	 ± 0.224741733437298
	data : 0.11568317413330079
	model : 0.06396961212158203
			 train-loss:  2.1905601264968997 	 ± 0.22430222726576723
	data : 0.11582021713256836
	model : 0.06395769119262695
			 train-loss:  2.191550564095677 	 ± 0.2243941057805642
	data : 0.11567745208740235
	model : 0.0639434814453125
			 train-loss:  2.190903967857361 	 ± 0.22417717867209097
	data : 0.11554489135742188
	model : 0.0638948917388916
			 train-loss:  2.191706028592539 	 ± 0.2240892948053181
	data : 0.11550779342651367
	model : 0.06384615898132324
			 train-loss:  2.191332877628387 	 ± 0.2237223542908172
	data : 0.11557374000549317
	model : 0.06382365226745605
			 train-loss:  2.191730668893445 	 ± 0.2233690555647666
	data : 0.11558122634887695
	model : 0.06381001472473144
			 train-loss:  2.191919706937835 	 ± 0.22294919602479743
	data : 0.11564207077026367
	model : 0.06382198333740234
			 train-loss:  2.1933104094337015 	 ± 0.2236127606837017
	data : 0.11560273170471191
	model : 0.06384768486022949
			 train-loss:  2.1914196657016873 	 ± 0.22520867289687013
	data : 0.1154250144958496
	model : 0.055440330505371095
#epoch  27    val-loss:  2.4381752767060934  train-loss:  2.1914196657016873  lr:  7.8125e-05
			 train-loss:  2.5198636054992676 	 ± 0.0
	data : 5.700987100601196
	model : 0.07244753837585449
			 train-loss:  2.270414113998413 	 ± 0.2494494915008545
	data : 2.9143229722976685
	model : 0.07064175605773926
			 train-loss:  2.334432045618693 	 ± 0.2228900110171702
	data : 1.9819444020589192
	model : 0.06871589024861653
			 train-loss:  2.3043424487113953 	 ± 0.1999402892979719
	data : 1.5149958729743958
	model : 0.06771266460418701
			 train-loss:  2.245944452285767 	 ± 0.21359353776216242
	data : 1.2347496032714844
	model : 0.06708292961120606
			 train-loss:  2.2419486840566 	 ± 0.1951879358383513
	data : 0.1174539566040039
	model : 0.0655787467956543
			 train-loss:  2.208057233265468 	 ± 0.1988655181356709
	data : 0.1148442268371582
	model : 0.06479415893554688
			 train-loss:  2.2101496756076813 	 ± 0.18610401854913333
	data : 0.11443657875061035
	model : 0.0647921085357666
			 train-loss:  2.206791056527032 	 ± 0.17571752235220225
	data : 0.11460089683532715
	model : 0.06485271453857422
			 train-loss:  2.167625975608826 	 ± 0.20394635315544066
	data : 0.11481900215148926
	model : 0.06497435569763184
			 train-loss:  2.1664514541625977 	 ± 0.1944906856201786
	data : 0.11487846374511719
	model : 0.06499075889587402
			 train-loss:  2.1594049533208213 	 ± 0.18767150238870742
	data : 0.1148759365081787
	model : 0.06502513885498047
			 train-loss:  2.159286535703219 	 ± 0.18030941381888568
	data : 0.1147737979888916
	model : 0.06519012451171875
			 train-loss:  2.162160192217146 	 ± 0.17405915237818237
	data : 0.11437401771545411
	model : 0.06514978408813477
			 train-loss:  2.1684935410817463 	 ± 0.16981865111918923
	data : 0.11424374580383301
	model : 0.0651282787322998
			 train-loss:  2.176169902086258 	 ± 0.16709241051067633
	data : 0.11422872543334961
	model : 0.06509604454040527
			 train-loss:  2.1837577118593106 	 ± 0.1649203621626558
	data : 0.11429929733276367
	model : 0.0650634765625
			 train-loss:  2.189184400770399 	 ± 0.16182804986675642
	data : 0.1142641544342041
	model : 0.06497001647949219
			 train-loss:  2.178174890969929 	 ± 0.16429166966326889
	data : 0.11457958221435546
	model : 0.06507644653320313
			 train-loss:  2.1630737125873565 	 ± 0.17313298722950107
	data : 0.11463165283203125
	model : 0.06510000228881836
			 train-loss:  2.1753627402441844 	 ± 0.17767400721751583
	data : 0.11458740234375
	model : 0.06512126922607422
			 train-loss:  2.1747915907339617 	 ± 0.17360873190883283
	data : 0.11437358856201171
	model : 0.06511116027832031
			 train-loss:  2.168589534966842 	 ± 0.17226664876107012
	data : 0.11436581611633301
	model : 0.06507587432861328
			 train-loss:  2.1663941393295922 	 ± 0.16896792952112563
	data : 0.11436328887939454
	model : 0.06499505043029785
			 train-loss:  2.1563215923309325 	 ± 0.17275156621968693
	data : 0.11446762084960938
	model : 0.06498212814331054
			 train-loss:  2.1485251555076013 	 ± 0.1738243441861087
	data : 0.11448373794555664
	model : 0.0650472640991211
			 train-loss:  2.1375588355241 	 ± 0.17950653661463967
	data : 0.11467447280883789
	model : 0.06515960693359375
			 train-loss:  2.135883625064577 	 ± 0.176486715010205
	data : 0.11481819152832032
	model : 0.06512665748596191
			 train-loss:  2.1341886561492394 	 ± 0.17364892315136884
	data : 0.11490707397460938
	model : 0.0651369571685791
			 train-loss:  2.1483058015505474 	 ± 0.18689124358499
	data : 0.11480331420898438
	model : 0.06506404876708985
			 train-loss:  2.1472468030068184 	 ± 0.1839436346279049
	data : 0.11468958854675293
	model : 0.06497125625610352
			 train-loss:  2.149902042001486 	 ± 0.1816492993113466
	data : 0.11466012001037598
	model : 0.06489048004150391
			 train-loss:  2.14815282460415 	 ± 0.17914934367871893
	data : 0.11454377174377442
	model : 0.0649261474609375
			 train-loss:  2.140271891565884 	 ± 0.182209034665602
	data : 0.11457223892211914
	model : 0.06497673988342285
			 train-loss:  2.1528874158859255 	 ± 0.19406881907295312
	data : 0.11463732719421386
	model : 0.0650050163269043
			 train-loss:  2.1472447216510773 	 ± 0.19424448558884033
	data : 0.11467313766479492
	model : 0.06503734588623047
			 train-loss:  2.1431135905755534 	 ± 0.19319821522167918
	data : 0.11463637351989746
	model : 0.06505956649780273
			 train-loss:  2.1445879089204887 	 ± 0.19085000073092373
	data : 0.11466035842895508
	model : 0.0649867057800293
			 train-loss:  2.140804724815564 	 ± 0.18982532887947928
	data : 0.11449675559997559
	model : 0.06493997573852539
			 train-loss:  2.13616039454937 	 ± 0.18966822727683072
	data : 0.11463122367858887
	model : 0.06494770050048829
			 train-loss:  2.1386640159095207 	 ± 0.18800889795977582
	data : 0.11478490829467773
	model : 0.06493959426879883
			 train-loss:  2.1418326440311612 	 ± 0.186861956036584
	data : 0.11474719047546386
	model : 0.06492800712585449
			 train-loss:  2.1417043957599375 	 ± 0.18467823118592794
	data : 0.11473889350891113
	model : 0.0650031566619873
			 train-loss:  2.1410418071530084 	 ± 0.18261924765061519
	data : 0.11487536430358887
	model : 0.0650069236755371
			 train-loss:  2.1428229252497357 	 ± 0.18096482522943913
	data : 0.1147000789642334
	model : 0.06499505043029785
			 train-loss:  2.141597491243611 	 ± 0.17917568217828245
	data : 0.11463136672973633
	model : 0.06499686241149902
			 train-loss:  2.1489314246684947 	 ± 0.18410607134022122
	data : 0.11466040611267089
	model : 0.06494297981262206
			 train-loss:  2.155624933540821 	 ± 0.1878686844344056
	data : 0.11456618309020997
	model : 0.06488494873046875
			 train-loss:  2.156266210030536 	 ± 0.18599484688685528
	data : 0.11455516815185547
	model : 0.0648543357849121
			 train-loss:  2.15213751077652 	 ± 0.18637988927475824
	data : 0.1145808219909668
	model : 0.06487531661987304
			 train-loss:  2.148233210339266 	 ± 0.18659719851414186
	data : 0.11461701393127441
	model : 0.06492147445678711
			 train-loss:  2.1476369981582346 	 ± 0.1848433298589701
	data : 0.11469378471374511
	model : 0.0649489402770996
			 train-loss:  2.149662541893293 	 ± 0.18367292172274993
	data : 0.11479501724243164
	model : 0.06498403549194336
			 train-loss:  2.1531239063651473 	 ± 0.18370084795763678
	data : 0.11472845077514648
	model : 0.06497764587402344
			 train-loss:  2.154144943844188 	 ± 0.18217775336961273
	data : 0.11457891464233398
	model : 0.06492652893066406
			 train-loss:  2.158182678478105 	 ± 0.1830102713380975
	data : 0.11465415954589844
	model : 0.06487746238708496
			 train-loss:  2.1633116048679017 	 ± 0.18541385265365345
	data : 0.11471524238586425
	model : 0.06485557556152344
			 train-loss:  2.1647201838164496 	 ± 0.18411588985406388
	data : 0.11461615562438965
	model : 0.06487345695495605
			 train-loss:  2.1725356720261653 	 ± 0.19200743466775647
	data : 0.11471362113952636
	model : 0.06489510536193847
			 train-loss:  2.169272659222285 	 ± 0.19204321202749364
	data : 0.11492295265197754
	model : 0.06495046615600586
			 train-loss:  2.173026282279218 	 ± 0.1926690855491107
	data : 0.11492009162902832
	model : 0.06496214866638184
			 train-loss:  2.1686321343145063 	 ± 0.19416607675738848
	data : 0.11486344337463379
	model : 0.06499333381652832
			 train-loss:  2.168863989057995 	 ± 0.19262756341913345
	data : 0.1149402141571045
	model : 0.06493983268737794
			 train-loss:  2.1665225215256214 	 ± 0.19201823423184963
	data : 0.11484279632568359
	model : 0.06496710777282715
			 train-loss:  2.164232173332801 	 ± 0.19141442097885755
	data : 0.11474819183349609
	model : 0.06494369506835937
			 train-loss:  2.1649591344775576 	 ± 0.19004917160233029
	data : 0.11468534469604492
	model : 0.06496353149414062
			 train-loss:  2.1617011757039313 	 ± 0.19047347907581147
	data : 0.11470675468444824
	model : 0.06498045921325683
			 train-loss:  2.161331634311115 	 ± 0.18909194653756867
	data : 0.11479392051696777
	model : 0.0650545597076416
			 train-loss:  2.1643521250158115 	 ± 0.18936196429630903
	data : 0.11483798027038575
	model : 0.06502537727355957
			 train-loss:  2.173771096978869 	 ± 0.20363489953661607
	data : 0.11499252319335937
	model : 0.06499624252319336
			 train-loss:  2.1709688515730305 	 ± 0.20355050523369636
	data : 0.11492018699645996
	model : 0.0649634838104248
			 train-loss:  2.177338888247808 	 ± 0.209137145398928
	data : 0.11489510536193848
	model : 0.06492385864257813
			 train-loss:  2.179357646262809 	 ± 0.20840493710345934
	data : 0.1147989273071289
	model : 0.06487956047058105
			 train-loss:  2.1788463109248393 	 ± 0.20703810615642684
	data : 0.11495914459228515
	model : 0.06488475799560547
			 train-loss:  2.1782111326853433 	 ± 0.20572579429617985
	data : 0.1148458480834961
	model : 0.0649953842163086
			 train-loss:  2.1729968864666787 	 ± 0.20929726893615266
	data : 0.11488170623779297
	model : 0.06506156921386719
			 train-loss:  2.17241523018131 	 ± 0.2079955739989521
	data : 0.11485905647277832
	model : 0.0650895118713379
			 train-loss:  2.171217583692991 	 ± 0.2069250135655919
	data : 0.11492905616760254
	model : 0.06511116027832031
			 train-loss:  2.1744356049767024 	 ± 0.2075661419557468
	data : 0.11489462852478027
	model : 0.06510744094848633
			 train-loss:  2.1766934052109717 	 ± 0.2072386843358277
	data : 0.11484622955322266
	model : 0.06499123573303223
			 train-loss:  2.178421295719382 	 ± 0.20653450096405773
	data : 0.11486635208129883
	model : 0.06492867469787597
			 train-loss:  2.184194662222048 	 ± 0.2117455347594932
	data : 0.11476368904113769
	model : 0.0649266242980957
			 train-loss:  2.1826179156820458 	 ± 0.210949850165302
	data : 0.11461114883422852
	model : 0.06498403549194336
			 train-loss:  2.186727965161914 	 ± 0.2130074114945677
	data : 0.1144951343536377
	model : 0.06499414443969727
			 train-loss:  2.187299539061154 	 ± 0.21181550885097872
	data : 0.11451659202575684
	model : 0.06505880355834961
			 train-loss:  2.185562237750652 	 ± 0.21118868973934632
	data : 0.11441941261291504
	model : 0.06502041816711426
			 train-loss:  2.18407072006971 	 ± 0.21042654093075086
	data : 0.11443161964416504
	model : 0.06506028175354003
			 train-loss:  2.185194431380792 	 ± 0.20948988565577717
	data : 0.1147080421447754
	model : 0.06498656272888184
			 train-loss:  2.1857329848107327 	 ± 0.20837090591925927
	data : 0.11475262641906739
	model : 0.06497621536254883
			 train-loss:  2.182915339205 	 ± 0.20890808903904678
	data : 0.1148834228515625
	model : 0.06504335403442382
			 train-loss:  2.1827465033793185 	 ± 0.20776324570713017
	data : 0.11498303413391113
	model : 0.06511220932006836
			 train-loss:  2.18604966479799 	 ± 0.20901977176008718
	data : 0.11509203910827637
	model : 0.06504316329956054
			 train-loss:  2.190471940143134 	 ± 0.21217607714530964
	data : 0.1148350715637207
	model : 0.06504354476928711
			 train-loss:  2.1898355572781663 	 ± 0.21113367551408918
	data : 0.1146547794342041
	model : 0.06508712768554688
			 train-loss:  2.1909258227599295 	 ± 0.21028535053867317
	data : 0.1145700454711914
	model : 0.06499738693237304
			 train-loss:  2.1890738867223263 	 ± 0.20996457503038474
	data : 0.11451663970947265
	model : 0.06502242088317871
			 train-loss:  2.1857495049840394 	 ± 0.21140383701432497
	data : 0.1145517349243164
	model : 0.06507740020751954
			 train-loss:  2.188966384955815 	 ± 0.21269539834062098
	data : 0.11472020149230958
	model : 0.06514620780944824
			 train-loss:  2.186205698986246 	 ± 0.21337587832084476
	data : 0.11471338272094726
	model : 0.06514620780944824
			 train-loss:  2.1863538563251494 	 ± 0.21231143611169434
	data : 0.11467819213867188
	model : 0.06516094207763672
			 train-loss:  2.18616523246954 	 ± 0.21126619541565428
	data : 0.11483726501464844
	model : 0.06511578559875489
			 train-loss:  2.1875049378357683 	 ± 0.2106587258519162
	data : 0.11467680931091309
	model : 0.06505746841430664
			 train-loss:  2.184931794416557 	 ± 0.2112382588406933
	data : 0.11462216377258301
	model : 0.06499056816101074
			 train-loss:  2.184929982973979 	 ± 0.21022023801398929
	data : 0.11490521430969239
	model : 0.06494317054748536
			 train-loss:  2.186237219401768 	 ± 0.20964109624016586
	data : 0.11489496231079102
	model : 0.06495628356933594
			 train-loss:  2.19208640197538 	 ± 0.21708784101157128
	data : 0.11475887298583984
	model : 0.06495757102966308
			 train-loss:  2.1944799891142086 	 ± 0.21747181825826634
	data : 0.11485352516174316
	model : 0.06501131057739258
			 train-loss:  2.1947300279581987 	 ± 0.21647811433024194
	data : 0.11481060981750488
	model : 0.0650205135345459
			 train-loss:  2.195049449938153 	 ± 0.2155083746888744
	data : 0.11461586952209472
	model : 0.06502838134765625
			 train-loss:  2.194112747365778 	 ± 0.21474934349662428
	data : 0.11477575302124024
	model : 0.06497139930725097
			 train-loss:  2.1919298644538396 	 ± 0.2150022257913643
	data : 0.1147432804107666
	model : 0.06496615409851074
			 train-loss:  2.1876906411988393 	 ± 0.21865042389142206
	data : 0.11495742797851563
	model : 0.06494612693786621
			 train-loss:  2.18635080345964 	 ± 0.2181421248887988
	data : 0.11506166458129882
	model : 0.06494731903076172
			 train-loss:  2.1874058372096012 	 ± 0.2174726318128264
	data : 0.11503190994262695
	model : 0.06498260498046875
			 train-loss:  2.1851703135863594 	 ± 0.21783666568730511
	data : 0.11486492156982422
	model : 0.06506729125976562
			 train-loss:  2.188130402359469 	 ± 0.21920625306312122
	data : 0.11492257118225098
	model : 0.06505799293518066
			 train-loss:  2.189521417658553 	 ± 0.21878102510919678
	data : 0.11465616226196289
	model : 0.06509456634521485
			 train-loss:  2.186298836085756 	 ± 0.220623092484118
	data : 0.11449251174926758
	model : 0.06506266593933105
			 train-loss:  2.1855475391660417 	 ± 0.21984568193967108
	data : 0.11451244354248047
	model : 0.06504507064819336
			 train-loss:  2.185160391529401 	 ± 0.21896847325685323
	data : 0.11464800834655761
	model : 0.06499075889587402
			 train-loss:  2.183456674095028 	 ± 0.21885897940120752
	data : 0.11472024917602539
	model : 0.06499576568603516
			 train-loss:  2.181478509160339 	 ± 0.21904366129917663
	data : 0.11479902267456055
	model : 0.06500210762023925
			 train-loss:  2.1858319780690882 	 ± 0.2233881635216443
	data : 0.11483712196350097
	model : 0.06505651473999023
			 train-loss:  2.1875135235248075 	 ± 0.2232658239119537
	data : 0.11494340896606445
	model : 0.0650245189666748
			 train-loss:  2.1861062955856325 	 ± 0.2229224150252581
	data : 0.11493659019470215
	model : 0.06507415771484375
			 train-loss:  2.187684126316555 	 ± 0.22273571070409096
	data : 0.11492753028869629
	model : 0.06505436897277832
			 train-loss:  2.1873857458745403 	 ± 0.22188234578195923
	data : 0.11485409736633301
	model : 0.06501812934875488
			 train-loss:  2.186778566800058 	 ± 0.2211198153966469
	data : 0.11495122909545899
	model : 0.06498928070068359
			 train-loss:  2.1859533980835315 	 ± 0.22045885183625957
	data : 0.11495575904846192
	model : 0.0650357723236084
			 train-loss:  2.186194758231823 	 ± 0.21962640520466925
	data : 0.11484827995300292
	model : 0.06500611305236817
			 train-loss:  2.18529175893041 	 ± 0.21902864875861897
	data : 0.11476325988769531
	model : 0.06506891250610351
			 train-loss:  2.185425728559494 	 ± 0.21820280513729995
	data : 0.11487627029418945
	model : 0.06506876945495606
			 train-loss:  2.18506988457271 	 ± 0.21741938784259526
	data : 0.11477551460266114
	model : 0.06507358551025391
			 train-loss:  2.186273504548998 	 ± 0.21705090963400583
	data : 0.11469526290893554
	model : 0.06497669219970703
			 train-loss:  2.1858547219523676 	 ± 0.21629985448208724
	data : 0.11473345756530762
	model : 0.06497268676757813
			 train-loss:  2.1854771489606186 	 ± 0.21554781573824872
	data : 0.11487207412719727
	model : 0.06495499610900879
			 train-loss:  2.188656019468377 	 ± 0.2179358623680667
	data : 0.11482157707214355
	model : 0.06495656967163085
			 train-loss:  2.187188850796741 	 ± 0.21782279471684457
	data : 0.11484732627868652
	model : 0.06497902870178222
			 train-loss:  2.185676964066869 	 ± 0.21776332757080724
	data : 0.1149454116821289
	model : 0.06507158279418945
			 train-loss:  2.1872466019221712 	 ± 0.21777192062769452
	data : 0.1149519920349121
	model : 0.06504569053649903
			 train-loss:  2.1889464026647256 	 ± 0.21792835938737537
	data : 0.11473379135131836
	model : 0.06502432823181152
			 train-loss:  2.187191900233148 	 ± 0.21815671203990744
	data : 0.11467785835266113
	model : 0.06496410369873047
			 train-loss:  2.1888185589463562 	 ± 0.2182550618003726
	data : 0.11478877067565918
	model : 0.06490774154663086
			 train-loss:  2.188598893582821 	 ± 0.2175117736173404
	data : 0.11476655006408691
	model : 0.06491212844848633
			 train-loss:  2.1917512309962306 	 ± 0.22003646886037456
	data : 0.11478056907653808
	model : 0.06494784355163574
			 train-loss:  2.191234925838366 	 ± 0.2193697419000373
	data : 0.11495332717895508
	model : 0.06501555442810059
			 train-loss:  2.190853881998127 	 ± 0.2186707893817111
	data : 0.11494927406311035
	model : 0.06511330604553223
			 train-loss:  2.1937708025043077 	 ± 0.22078171432748186
	data : 0.11496601104736329
	model : 0.06515474319458008
			 train-loss:  2.194268096213373 	 ± 0.2201227412153815
	data : 0.11485795974731446
	model : 0.06514091491699218
			 train-loss:  2.197506592273712 	 ± 0.22292080442472137
	data : 0.11482272148132325
	model : 0.06510319709777831
			 train-loss:  2.1957008743917705 	 ± 0.22327937735891193
	data : 0.11479229927062988
	model : 0.0650517463684082
			 train-loss:  2.2003010997646735 	 ± 0.22961090721113422
	data : 0.11496357917785645
	model : 0.06499457359313965
			 train-loss:  2.2003755631789663 	 ± 0.22886115598305934
	data : 0.11484222412109375
	model : 0.06502737998962402
			 train-loss:  2.1997594755965393 	 ± 0.2282441430190928
	data : 0.11487836837768554
	model : 0.06507964134216308
			 train-loss:  2.201700679717525 	 ± 0.22877850346886053
	data : 0.11489315032958984
	model : 0.06514539718627929
			 train-loss:  2.2023600248190074 	 ± 0.22819175586310894
	data : 0.11486477851867676
	model : 0.06513032913208008
			 train-loss:  2.2023037117757616 	 ± 0.22746495704446798
	data : 0.11476588249206543
	model : 0.06516609191894532
			 train-loss:  2.200507788718501 	 ± 0.22785788270172083
	data : 0.11482176780700684
	model : 0.06516861915588379
			 train-loss:  2.1988295234224333 	 ± 0.2281177273330002
	data : 0.11476449966430664
	model : 0.06511316299438477
			 train-loss:  2.1969901598989963 	 ± 0.2285834638885659
	data : 0.11474695205688476
	model : 0.06504969596862793
			 train-loss:  2.1952828087421676 	 ± 0.22889358006112007
	data : 0.11487517356872559
	model : 0.06505832672119141
			 train-loss:  2.194238986498044 	 ± 0.22857008084664374
	data : 0.1149794578552246
	model : 0.06508102416992187
			 train-loss:  2.1929862272520007 	 ± 0.22842506105865626
	data : 0.11489496231079102
	model : 0.06506834030151368
			 train-loss:  2.1945342792243494 	 ± 0.2285836252037718
	data : 0.11501317024230957
	model : 0.06507635116577148
			 train-loss:  2.2002553874796087 	 ± 0.23937773755984804
	data : 0.1150552749633789
	model : 0.06507992744445801
			 train-loss:  2.199500726648124 	 ± 0.2388524228641942
	data : 0.11496319770812988
	model : 0.0650665283203125
			 train-loss:  2.1988704397292906 	 ± 0.23827464341288238
	data : 0.11477222442626953
	model : 0.06502361297607422
			 train-loss:  2.196432596161252 	 ± 0.2396442258689807
	data : 0.11489152908325195
	model : 0.0650111198425293
			 train-loss:  2.195862493571445 	 ± 0.2390484034113536
	data : 0.11489911079406738
	model : 0.06500411033630371
			 train-loss:  2.1942159435328317 	 ± 0.23930352618879475
	data : 0.11498713493347168
	model : 0.0650705337524414
			 train-loss:  2.192936689532988 	 ± 0.23918505629318493
	data : 0.11500015258789062
	model : 0.06511564254760742
			 train-loss:  2.1932321260141774 	 ± 0.23852002678491538
	data : 0.11507091522216797
	model : 0.06510457992553711
			 train-loss:  2.1932023798110167 	 ± 0.23782998342459968
	data : 0.11491379737854004
	model : 0.06509909629821778
			 train-loss:  2.193452985807397 	 ± 0.23716848590141434
	data : 0.11489653587341309
	model : 0.06517705917358399
			 train-loss:  2.192729158401489 	 ± 0.23668255452383294
	data : 0.11463875770568847
	model : 0.06515793800354004
			 train-loss:  2.19084103202278 	 ± 0.2373272480145033
	data : 0.11458854675292969
	model : 0.06509242057800294
			 train-loss:  2.190684401382834 	 ± 0.2366650048880457
	data : 0.11461806297302246
	model : 0.06507673263549804
			 train-loss:  2.1907675540849065 	 ± 0.23600187210672513
	data : 0.11485300064086915
	model : 0.06506466865539551
			 train-loss:  2.189589828752273 	 ± 0.23586568305306826
	data : 0.11489033699035645
	model : 0.06505336761474609
			 train-loss:  2.1908217635419636 	 ± 0.23578636954643134
	data : 0.11521754264831544
	model : 0.06505017280578614
			 train-loss:  2.190210195536113 	 ± 0.2352772385352446
	data : 0.11528782844543457
	model : 0.06510210037231445
			 train-loss:  2.190220426072131 	 ± 0.23463002255255835
	data : 0.11524796485900879
	model : 0.06512217521667481
			 train-loss:  2.190078277405494 	 ± 0.23399593699536642
	data : 0.11513795852661132
	model : 0.06517319679260254
			 train-loss:  2.190792094754136 	 ± 0.23355891516020907
	data : 0.11521806716918945
	model : 0.06507458686828613
			 train-loss:  2.1907730044545355 	 ± 0.23292696345225875
	data : 0.11493916511535644
	model : 0.06506438255310058
			 train-loss:  2.18906738937542 	 ± 0.2334554874746215
	data : 0.11490178108215332
	model : 0.0650407314300537
			 train-loss:  2.19157011496192 	 ± 0.23531903689819655
	data : 0.11495904922485352
	model : 0.06503138542175294
			 train-loss:  2.1932442454581564 	 ± 0.2358062940398081
	data : 0.11487135887145997
	model : 0.0649655818939209
			 train-loss:  2.19469440172589 	 ± 0.23602067543709307
	data : 0.11490902900695801
	model : 0.06497654914855958
			 train-loss:  2.196492032000893 	 ± 0.23669245811336653
	data : 0.11509103775024414
	model : 0.06496720314025879
			 train-loss:  2.1961269765624203 	 ± 0.23612565374722627
	data : 0.11503205299377442
	model : 0.06498255729675292
			 train-loss:  2.1957010788222155 	 ± 0.23558348269827623
	data : 0.11505165100097656
	model : 0.06503000259399414
			 train-loss:  2.1942110537247337 	 ± 0.23587769776593176
	data : 0.11511850357055664
	model : 0.06515588760375976
			 train-loss:  2.19424102048284 	 ± 0.23526934844576677
	data : 0.1149364948272705
	model : 0.06517186164855956
			 train-loss:  2.1951836665471394 	 ± 0.23503233064819212
	data : 0.11479449272155762
	model : 0.06517591476440429
			 train-loss:  2.1956788927924875 	 ± 0.23453396827448603
	data : 0.1148447036743164
	model : 0.06518182754516602
			 train-loss:  2.193839838057 	 ± 0.23535050254876808
	data : 0.114776611328125
	model : 0.06517210006713867
			 train-loss:  2.194576950988384 	 ± 0.23498329594969303
	data : 0.1148859977722168
	model : 0.06507043838500977
			 train-loss:  2.195985877933215 	 ± 0.23522908225779404
	data : 0.11496567726135254
	model : 0.06506929397583008
			 train-loss:  2.197778582572937 	 ± 0.23599915620458467
	data : 0.11507320404052734
	model : 0.06507997512817383
			 train-loss:  2.1987834202116403 	 ± 0.23583987983246188
	data : 0.11498279571533203
	model : 0.06506357192993165
			 train-loss:  2.20016878784293 	 ± 0.2360738615361775
	data : 0.11496992111206054
	model : 0.06503558158874512
			 train-loss:  2.198683997093163 	 ± 0.23643532250025595
	data : 0.11489243507385254
	model : 0.06503415107727051
			 train-loss:  2.1977758699772405 	 ± 0.23620975218765422
	data : 0.11488232612609864
	model : 0.06504325866699219
			 train-loss:  2.1971756667625613 	 ± 0.23578881615296346
	data : 0.11471877098083497
	model : 0.06503443717956543
			 train-loss:  2.196935102777574 	 ± 0.23524103407603117
	data : 0.11469659805297852
	model : 0.0649953842163086
			 train-loss:  2.1954572091355993 	 ± 0.23562883537153165
	data : 0.11477808952331543
	model : 0.06500744819641113
			 train-loss:  2.1941460325167728 	 ± 0.23581749625012802
	data : 0.11492156982421875
	model : 0.06503872871398926
			 train-loss:  2.194050657692138 	 ± 0.23525668437667377
	data : 0.11497573852539063
	model : 0.06503114700317383
			 train-loss:  2.195720806575957 	 ± 0.23593461083544495
	data : 0.11507444381713867
	model : 0.06501946449279786
			 train-loss:  2.1969386853312995 	 ± 0.2360355964194014
	data : 0.11508297920227051
	model : 0.06507253646850586
			 train-loss:  2.197659484620364 	 ± 0.23571090752940907
	data : 0.11508212089538575
	model : 0.0651237964630127
			 train-loss:  2.1979383784280695 	 ± 0.2351920030915551
	data : 0.11502246856689453
	model : 0.065071439743042
			 train-loss:  2.1982723329668845 	 ± 0.23469245976233538
	data : 0.11490459442138672
	model : 0.06506137847900391
			 train-loss:  2.198376576845036 	 ± 0.23415099306593493
	data : 0.11473684310913086
	model : 0.06505112648010254
			 train-loss:  2.1989348464541965 	 ± 0.23375172354919713
	data : 0.11483554840087891
	model : 0.06499910354614258
			 train-loss:  2.199485749143609 	 ± 0.23335300788568264
	data : 0.1147995948791504
	model : 0.06490721702575683
			 train-loss:  2.1992397352096136 	 ± 0.23284538318899578
	data : 0.11483521461486816
	model : 0.06490778923034668
			 train-loss:  2.198417125771579 	 ± 0.23263044512437706
	data : 0.11483020782470703
	model : 0.06491026878356934
			 train-loss:  2.1973841119896282 	 ± 0.23260403415962078
	data : 0.11503543853759765
	model : 0.065386962890625
			 train-loss:  2.1974284416949588 	 ± 0.23207811545614074
	data : 0.11512360572814942
	model : 0.06542096138000489
			 train-loss:  2.1967804458764224 	 ± 0.2317551197142402
	data : 0.11511363983154296
	model : 0.06538548469543456
			 train-loss:  2.196783745235392 	 ± 0.23123491077146832
	data : 0.11506319046020508
	model : 0.06531100273132324
			 train-loss:  2.196054915764502 	 ± 0.23097475265565912
	data : 0.11514825820922851
	model : 0.06515617370605468
			 train-loss:  2.195898168881734 	 ± 0.23047284393138656
	data : 0.11502928733825683
	model : 0.06448745727539062
			 train-loss:  2.196642703187149 	 ± 0.2302334079321425
	data : 0.1151388168334961
	model : 0.06423859596252442
			 train-loss:  2.197188535450839 	 ± 0.22987222990525985
	data : 0.11529526710510254
	model : 0.06406106948852539
			 train-loss:  2.1978587576171806 	 ± 0.22958974310209834
	data : 0.11541647911071777
	model : 0.06392202377319336
			 train-loss:  2.1971105264263904 	 ± 0.22936633343477025
	data : 0.1153653621673584
	model : 0.06387414932250976
			 train-loss:  2.1981658132179924 	 ± 0.22942362960103518
	data : 0.11561570167541504
	model : 0.06387438774108886
			 train-loss:  2.1982043331319634 	 ± 0.2289272484285058
	data : 0.11563806533813477
	model : 0.0639190673828125
			 train-loss:  2.199032209556678 	 ± 0.22877961560046964
	data : 0.1155977725982666
	model : 0.0639218807220459
			 train-loss:  2.200923857770764 	 ± 0.23009921791772772
	data : 0.11552228927612304
	model : 0.06389961242675782
			 train-loss:  2.202549961387602 	 ± 0.23094477315384218
	data : 0.11573500633239746
	model : 0.06387958526611329
			 train-loss:  2.203223871170206 	 ± 0.23068333457593998
	data : 0.11574802398681641
	model : 0.06385307312011719
			 train-loss:  2.2035115639031946 	 ± 0.23023632354021636
	data : 0.11568474769592285
	model : 0.06384882926940919
			 train-loss:  2.2031885667189264 	 ± 0.22980365579837467
	data : 0.11571650505065918
	model : 0.06386275291442871
			 train-loss:  2.203366958293594 	 ± 0.22933681086957228
	data : 0.11582117080688477
	model : 0.06387343406677246
			 train-loss:  2.201341610074542 	 ± 0.23097963746964265
	data : 0.11575236320495605
	model : 0.06388330459594727
			 train-loss:  2.1998622785011928 	 ± 0.2316297205817615
	data : 0.11562533378601074
	model : 0.06387438774108886
			 train-loss:  2.2000744817662534 	 ± 0.23117203755119833
	data : 0.11558313369750976
	model : 0.06388611793518066
			 train-loss:  2.2004761942162 	 ± 0.23077819059656582
	data : 0.11569709777832031
	model : 0.06386466026306152
			 train-loss:  2.1993398112034113 	 ± 0.23098032996079057
	data : 0.11565141677856446
	model : 0.06390457153320313
			 train-loss:  2.199724624880025 	 ± 0.2305845642466491
	data : 0.11558585166931153
	model : 0.06389031410217286
			 train-loss:  2.1998244524002075 	 ± 0.2301187857341497
	data : 0.11570549011230469
	model : 0.06395206451416016
			 train-loss:  2.1984339070513967 	 ± 0.23067971027003878
	data : 0.11569347381591796
	model : 0.06394467353820801
			 train-loss:  2.1986924878016176 	 ± 0.23024799566382537
	data : 0.11564321517944336
	model : 0.06395049095153808
			 train-loss:  2.199457872298456 	 ± 0.2300979546461981
	data : 0.11551427841186523
	model : 0.06389198303222657
			 train-loss:  2.198798687103762 	 ± 0.22986996359730452
	data : 0.1156074047088623
	model : 0.06389117240905762
			 train-loss:  2.199013522148132 	 ± 0.2294348093327282
	data : 0.11569514274597167
	model : 0.06384215354919434
			 train-loss:  2.199564197623872 	 ± 0.22914279427085468
	data : 0.11583070755004883
	model : 0.06380658149719239
			 train-loss:  2.1986669051268746 	 ± 0.22912911195696842
	data : 0.11576299667358399
	model : 0.06385784149169922
			 train-loss:  2.196848877333841 	 ± 0.23048981796657847
	data : 0.11584839820861817
	model : 0.06399240493774414
			 train-loss:  2.196926420598518 	 ± 0.23003895698684948
	data : 0.11583061218261718
	model : 0.06401481628417968
			 train-loss:  2.197420656447317 	 ± 0.22972253876091803
	data : 0.11574797630310059
	model : 0.06400465965270996
			 train-loss:  2.194190539419651 	 ± 0.23500401004104257
	data : 0.11522316932678223
	model : 0.055581188201904295
#epoch  28    val-loss:  2.425755914888884  train-loss:  2.194190539419651  lr:  7.8125e-05
			 train-loss:  2.2641379833221436 	 ± 0.0
	data : 5.668603420257568
	model : 0.07400131225585938
			 train-loss:  2.402858853340149 	 ± 0.13872087001800537
	data : 2.899897575378418
	model : 0.06936872005462646
			 train-loss:  2.3530520598093667 	 ± 0.13338073280152124
	data : 1.971510410308838
	model : 0.06774719556172688
			 train-loss:  2.3625981211662292 	 ± 0.1166884603304459
	data : 1.5073121190071106
	model : 0.06692516803741455
			 train-loss:  2.349870777130127 	 ± 0.10742857424486872
	data : 1.2288373470306397
	model : 0.06644492149353028
			 train-loss:  2.2745922406514487 	 ± 0.19481197535597314
	data : 0.11811823844909668
	model : 0.06466436386108398
			 train-loss:  2.238391467503139 	 ± 0.20098012178431388
	data : 0.11494793891906738
	model : 0.0646677017211914
			 train-loss:  2.2651397585868835 	 ± 0.20087852228321484
	data : 0.11493101119995117
	model : 0.06475210189819336
			 train-loss:  2.262467304865519 	 ± 0.18954086934185296
	data : 0.11483912467956543
	model : 0.0648125171661377
			 train-loss:  2.2762064218521116 	 ± 0.18447774135629572
	data : 0.11477489471435547
	model : 0.06487269401550293
			 train-loss:  2.2578608989715576 	 ± 0.18521284405513433
	data : 0.11468758583068847
	model : 0.06481347084045411
			 train-loss:  2.2518965005874634 	 ± 0.17842775101331143
	data : 0.11431927680969238
	model : 0.06481914520263672
			 train-loss:  2.236471121127789 	 ± 0.17956283487534408
	data : 0.11417675018310547
	model : 0.06480531692504883
			 train-loss:  2.245195133345468 	 ± 0.17586688929817212
	data : 0.11435704231262207
	model : 0.0648183822631836
			 train-loss:  2.2737947622934978 	 ± 0.2007943261934903
	data : 0.11433639526367187
	model : 0.06483407020568847
			 train-loss:  2.2402335330843925 	 ± 0.23386706786638556
	data : 0.11431159973144531
	model : 0.06487364768981933
			 train-loss:  2.233122944831848 	 ± 0.22866020686748453
	data : 0.11461553573608399
	model : 0.06487674713134765
			 train-loss:  2.203699847062429 	 ± 0.2531757459726247
	data : 0.11481404304504395
	model : 0.06493034362792968
			 train-loss:  2.225958090079458 	 ± 0.2638979167657215
	data : 0.11465730667114257
	model : 0.06494216918945313
			 train-loss:  2.2090233743190764 	 ± 0.26759841611907337
	data : 0.11451926231384277
	model : 0.06493206024169922
			 train-loss:  2.2028816597802297 	 ± 0.262589750854368
	data : 0.11448049545288086
	model : 0.06490459442138671
			 train-loss:  2.1989406726577063 	 ± 0.25718726990780755
	data : 0.11445741653442383
	model : 0.06494784355163574
			 train-loss:  2.2007971069087153 	 ± 0.25168478144132217
	data : 0.11442832946777344
	model : 0.06491756439208984
			 train-loss:  2.1950091073910394 	 ± 0.24794428051956827
	data : 0.11450576782226562
	model : 0.06495776176452636
			 train-loss:  2.1948044443130494 	 ± 0.2429368578048286
	data : 0.11465821266174317
	model : 0.06496548652648926
			 train-loss:  2.19803649187088 	 ± 0.23876669554371882
	data : 0.1146970272064209
	model : 0.06497015953063964
			 train-loss:  2.1891217584963196 	 ± 0.23867206559399917
	data : 0.11451849937438965
	model : 0.06493906974792481
			 train-loss:  2.205806170191084 	 ± 0.2498917619207757
	data : 0.11453990936279297
	model : 0.06490874290466309
			 train-loss:  2.202858094511361 	 ± 0.24604052153628791
	data : 0.11453914642333984
	model : 0.0649083137512207
			 train-loss:  2.1967783610026044 	 ± 0.2441106414241456
	data : 0.1146392822265625
	model : 0.06491327285766602
			 train-loss:  2.191648583258352 	 ± 0.241779208398054
	data : 0.11472015380859375
	model : 0.06494612693786621
			 train-loss:  2.1842716820538044 	 ± 0.24148991154082664
	data : 0.11482820510864258
	model : 0.06500487327575684
			 train-loss:  2.180775219743902 	 ± 0.2386239553776703
	data : 0.11487269401550293
	model : 0.06503686904907227
			 train-loss:  2.1884118774357963 	 ± 0.23914672713086443
	data : 0.11496539115905761
	model : 0.06500883102416992
			 train-loss:  2.1887805768421718 	 ± 0.23571539188617954
	data : 0.11478710174560547
	model : 0.06501073837280273
			 train-loss:  2.188873824146059 	 ± 0.2324191654459395
	data : 0.11468944549560547
	model : 0.06495404243469238
			 train-loss:  2.1954944423727087 	 ± 0.23267289816239078
	data : 0.11472458839416504
	model : 0.06488924026489258
			 train-loss:  2.1943406462669373 	 ± 0.22969824599274663
	data : 0.1146636962890625
	model : 0.06487889289855957
			 train-loss:  2.1932241030228443 	 ± 0.226838718445094
	data : 0.11458039283752441
	model : 0.06491727828979492
			 train-loss:  2.1907383471727373 	 ± 0.2245225810133018
	data : 0.11469182968139649
	model : 0.06496477127075195
			 train-loss:  2.1832903507279187 	 ± 0.2267151819668056
	data : 0.11469545364379882
	model : 0.06501951217651367
			 train-loss:  2.1928120028404963 	 ± 0.23214888717110682
	data : 0.11466894149780274
	model : 0.06503190994262695
			 train-loss:  2.190900450529054 	 ± 0.22976781133708213
	data : 0.11460509300231933
	model : 0.06504616737365723
			 train-loss:  2.193185256286101 	 ± 0.22763539950316897
	data : 0.11446843147277833
	model : 0.06497354507446289
			 train-loss:  2.1920680708355373 	 ± 0.22521386129148174
	data : 0.11447210311889648
	model : 0.06493644714355469
			 train-loss:  2.188461606917174 	 ± 0.2240623638938582
	data : 0.11453022956848144
	model : 0.06490740776062012
			 train-loss:  2.196198095666601 	 ± 0.227791623619835
	data : 0.11458349227905273
	model : 0.06492013931274414
			 train-loss:  2.197044732669989 	 ± 0.22548102312723045
	data : 0.11459116935729981
	model : 0.06491203308105468
			 train-loss:  2.195134160469989 	 ± 0.22356055103274702
	data : 0.11477413177490234
	model : 0.06498708724975585
			 train-loss:  2.1943383765220643 	 ± 0.22138374807334246
	data : 0.11482553482055664
	model : 0.06500110626220704
			 train-loss:  2.203262733478172 	 ± 0.2281051860309161
	data : 0.11480770111083985
	model : 0.06500544548034667
			 train-loss:  2.2000363812996793 	 ± 0.22707319908020757
	data : 0.11454753875732422
	model : 0.06496305465698242
			 train-loss:  2.208046753451509 	 ± 0.23221972473259792
	data : 0.11464548110961914
	model : 0.06493840217590333
			 train-loss:  2.20902791067406 	 ± 0.2301703551681505
	data : 0.1146463394165039
	model : 0.06492080688476562
			 train-loss:  2.2073533383282746 	 ± 0.22840003348798207
	data : 0.11467204093933106
	model : 0.06487550735473632
			 train-loss:  2.207738635795457 	 ± 0.22636959664995052
	data : 0.11468887329101562
	model : 0.06490473747253418
			 train-loss:  2.208698780913102 	 ± 0.22449012468956808
	data : 0.11485223770141602
	model : 0.06496920585632324
			 train-loss:  2.2070478098145845 	 ± 0.22289523978716555
	data : 0.11478395462036133
	model : 0.06498079299926758
			 train-loss:  2.2073260384090876 	 ± 0.22100838239740586
	data : 0.11473670005798339
	model : 0.06496963500976563
			 train-loss:  2.202749427159627 	 ± 0.22196036477926684
	data : 0.1145564079284668
	model : 0.06502695083618164
			 train-loss:  2.2042811503175828 	 ± 0.22045300660990264
	data : 0.11458187103271485
	model : 0.06494970321655273
			 train-loss:  2.202225681274168 	 ± 0.21925644095171365
	data : 0.11460928916931153
	model : 0.06493945121765136
			 train-loss:  2.2074755184234136 	 ± 0.22140254346167954
	data : 0.11474652290344238
	model : 0.0649186134338379
			 train-loss:  2.2025025840848684 	 ± 0.22318413432087775
	data : 0.11479897499084472
	model : 0.06497788429260254
			 train-loss:  2.2005632895689744 	 ± 0.2220034404098078
	data : 0.11490859985351562
	model : 0.06500868797302246
			 train-loss:  2.2023682178873005 	 ± 0.22079522626528542
	data : 0.1149369239807129
	model : 0.06509394645690918
			 train-loss:  2.1977831730202064 	 ± 0.22228451602936655
	data : 0.11499619483947754
	model : 0.06509985923767089
			 train-loss:  2.1949667983195362 	 ± 0.22184504547147899
	data : 0.11502037048339844
	model : 0.06514906883239746
			 train-loss:  2.192913157352503 	 ± 0.22088174683404455
	data : 0.11495585441589355
	model : 0.06508145332336426
			 train-loss:  2.1888473204204013 	 ± 0.22188376476135072
	data : 0.11493749618530273
	model : 0.06503682136535645
			 train-loss:  2.184144360918394 	 ± 0.2238017847879153
	data : 0.11497669219970703
	model : 0.06504464149475098
			 train-loss:  2.1824226793315677 	 ± 0.22271515460407326
	data : 0.11502070426940918
	model : 0.06502399444580079
			 train-loss:  2.182962125294829 	 ± 0.22123180654978875
	data : 0.11487278938293458
	model : 0.06501965522766114
			 train-loss:  2.184461253720361 	 ± 0.2201049127030545
	data : 0.11477046012878418
	model : 0.0650303840637207
			 train-loss:  2.180675808588664 	 ± 0.22104437031195553
	data : 0.1148045539855957
	model : 0.06500010490417481
			 train-loss:  2.1847398563435205 	 ± 0.2223880553154799
	data : 0.11466612815856933
	model : 0.0649576187133789
			 train-loss:  2.1832330598459615 	 ± 0.22132941227427397
	data : 0.11456809043884278
	model : 0.064935302734375
			 train-loss:  2.181375735845321 	 ± 0.220509177243052
	data : 0.11463489532470703
	model : 0.06494030952453614
			 train-loss:  2.189122248299514 	 ± 0.22954184655682053
	data : 0.11480917930603027
	model : 0.06491332054138184
			 train-loss:  2.190712186694145 	 ± 0.2285400304170038
	data : 0.11478652954101562
	model : 0.064910888671875
			 train-loss:  2.1876881740711354 	 ± 0.22872974476686467
	data : 0.11476244926452636
	model : 0.06495933532714844
			 train-loss:  2.187640254090472 	 ± 0.2273311820600165
	data : 0.11478357315063477
	model : 0.06501665115356445
			 train-loss:  2.1852170099695045 	 ± 0.22702056152206937
	data : 0.11471333503723144
	model : 0.06501626968383789
			 train-loss:  2.1810276692821864 	 ± 0.22887001004976074
	data : 0.11457333564758301
	model : 0.06503381729125976
			 train-loss:  2.1774606480317957 	 ± 0.22985650868989604
	data : 0.1145944595336914
	model : 0.06508145332336426
			 train-loss:  2.179323678792909 	 ± 0.22916084021427846
	data : 0.11467351913452148
	model : 0.06508755683898926
			 train-loss:  2.180615205874388 	 ± 0.22815460806821586
	data : 0.11478791236877442
	model : 0.06504902839660645
			 train-loss:  2.1837309219620447 	 ± 0.2287084721513068
	data : 0.11486024856567383
	model : 0.06504850387573242
			 train-loss:  2.1848258945379366 	 ± 0.22765181461304493
	data : 0.1150583267211914
	model : 0.06505880355834961
			 train-loss:  2.181796079211765 	 ± 0.22818087560074946
	data : 0.1149383544921875
	model : 0.06508355140686035
			 train-loss:  2.192347301231636 	 ± 0.24801995538190627
	data : 0.11489949226379395
	model : 0.06503357887268066
			 train-loss:  2.1940614010976707 	 ± 0.24720970759370295
	data : 0.11478734016418457
	model : 0.06506986618041992
			 train-loss:  2.191385813938674 	 ± 0.24721270376685484
	data : 0.11470799446105957
	model : 0.06507434844970703
			 train-loss:  2.1925413037868258 	 ± 0.24614658287843974
	data : 0.11453890800476074
	model : 0.06506834030151368
			 train-loss:  2.1919616084349784 	 ± 0.24491214497268146
	data : 0.11464533805847169
	model : 0.06497502326965332
			 train-loss:  2.1911339722573757 	 ± 0.24376673263755494
	data : 0.11474418640136719
	model : 0.06498055458068848
			 train-loss:  2.1923399844120457 	 ± 0.2427946630502129
	data : 0.11489500999450683
	model : 0.0649451732635498
			 train-loss:  2.1938244870730808 	 ± 0.24199481199678974
	data : 0.11488966941833496
	model : 0.06494736671447754
			 train-loss:  2.1962900005205714 	 ± 0.2420034648180921
	data : 0.11492738723754883
	model : 0.06495151519775391
			 train-loss:  2.1937967848777773 	 ± 0.24206490145006115
	data : 0.11483573913574219
	model : 0.06503024101257324
			 train-loss:  2.19054648899796 	 ± 0.24304671609777154
	data : 0.11484894752502442
	model : 0.06507501602172852
			 train-loss:  2.1938932012109196 	 ± 0.24417989388485029
	data : 0.11465654373168946
	model : 0.06509585380554199
			 train-loss:  2.192779851191252 	 ± 0.24325168541314526
	data : 0.11451940536499024
	model : 0.06504359245300292
			 train-loss:  2.1896759156997385 	 ± 0.24412040401397875
	data : 0.11447176933288575
	model : 0.06507906913757325
			 train-loss:  2.1902782281239825 	 ± 0.24303277875370538
	data : 0.1147162914276123
	model : 0.06502909660339355
			 train-loss:  2.192397670925788 	 ± 0.24285670503682835
	data : 0.11473441123962402
	model : 0.06499462127685547
			 train-loss:  2.1926839485346714 	 ± 0.2417371656288466
	data : 0.11478490829467773
	model : 0.06508121490478516
			 train-loss:  2.195778027728752 	 ± 0.24273467227214934
	data : 0.1151212215423584
	model : 0.06516671180725098
			 train-loss:  2.1971751746781374 	 ± 0.2420545138387932
	data : 0.11511011123657226
	model : 0.06514053344726563
			 train-loss:  2.2007300528613003 	 ± 0.24379335122520204
	data : 0.11493844985961914
	model : 0.06514167785644531
			 train-loss:  2.2012972208830686 	 ± 0.2427655877973768
	data : 0.1149179458618164
	model : 0.06515030860900879
			 train-loss:  2.202344170638493 	 ± 0.24193096456625388
	data : 0.11496510505676269
	model : 0.06505179405212402
			 train-loss:  2.2000139360934234 	 ± 0.24211728769043486
	data : 0.11483054161071778
	model : 0.0649787425994873
			 train-loss:  2.200408484852105 	 ± 0.24108951488417898
	data : 0.11492881774902344
	model : 0.06496152877807618
			 train-loss:  2.2027404215024866 	 ± 0.2413268543140066
	data : 0.11510353088378907
	model : 0.06495819091796876
			 train-loss:  2.200365410796527 	 ± 0.24163044303929448
	data : 0.11524162292480469
	model : 0.06493535041809081
			 train-loss:  2.1970324404219275 	 ± 0.243258832951989
	data : 0.1155590534210205
	model : 0.06495671272277832
			 train-loss:  2.200249588085433 	 ± 0.24471275512274313
	data : 0.11563959121704101
	model : 0.0649834156036377
			 train-loss:  2.202596569261631 	 ± 0.2450124173800422
	data : 0.11561112403869629
	model : 0.06501202583312989
			 train-loss:  2.204236434896787 	 ± 0.24464430422217961
	data : 0.11543836593627929
	model : 0.06505956649780273
			 train-loss:  2.203377137499407 	 ± 0.24381305876746878
	data : 0.11525893211364746
	model : 0.06508564949035645
			 train-loss:  2.2019294783717296 	 ± 0.24333338623167197
	data : 0.1149296760559082
	model : 0.06509885787963868
			 train-loss:  2.200140258161033 	 ± 0.2431466742356064
	data : 0.114837646484375
	model : 0.06507940292358398
			 train-loss:  2.203601486259891 	 ± 0.24518784804961105
	data : 0.1148386001586914
	model : 0.06502780914306641
			 train-loss:  2.2008104343414305 	 ± 0.2461749389462314
	data : 0.11489281654357911
	model : 0.06501178741455078
			 train-loss:  2.1990636407382906 	 ± 0.24597264583602096
	data : 0.11504998207092285
	model : 0.06506390571594238
			 train-loss:  2.200026344126604 	 ± 0.24524053712973845
	data : 0.11517877578735351
	model : 0.06504430770874023
			 train-loss:  2.200957422144711 	 ± 0.24450593362532794
	data : 0.11520752906799317
	model : 0.06506519317626953
			 train-loss:  2.1999687233636545 	 ± 0.24381312419605253
	data : 0.11525030136108398
	model : 0.06513371467590331
			 train-loss:  2.199352945731236 	 ± 0.242974249713415
	data : 0.1155097484588623
	model : 0.06512198448181153
			 train-loss:  2.2004210357447618 	 ± 0.24235125735417803
	data : 0.11538872718811036
	model : 0.0650369644165039
			 train-loss:  2.198942190769947 	 ± 0.2420241126382687
	data : 0.1152867317199707
	model : 0.06503634452819824
			 train-loss:  2.1993611082994846 	 ± 0.24116056362722274
	data : 0.1151125431060791
	model : 0.06497502326965332
			 train-loss:  2.1995428453630477 	 ± 0.24026816722206834
	data : 0.11507034301757812
	model : 0.06493253707885742
			 train-loss:  2.2013383309046426 	 ± 0.2402772491387095
	data : 0.11483163833618164
	model : 0.06494965553283691
			 train-loss:  2.2040648416561237 	 ± 0.24147923134061014
	data : 0.11482801437377929
	model : 0.06495466232299804
			 train-loss:  2.204811305895339 	 ± 0.24075373919754803
	data : 0.114784574508667
	model : 0.06498026847839355
			 train-loss:  2.204359955545785 	 ± 0.23993802337848952
	data : 0.11488947868347169
	model : 0.06502971649169922
			 train-loss:  2.205440441481501 	 ± 0.2394100846794613
	data : 0.11500020027160644
	model : 0.06502132415771485
			 train-loss:  2.2058158116681237 	 ± 0.23859456331169895
	data : 0.11498880386352539
	model : 0.06500959396362305
			 train-loss:  2.205648255686388 	 ± 0.23775524381393784
	data : 0.11490392684936523
	model : 0.06501979827880859
			 train-loss:  2.205555285366488 	 ± 0.23691917041841867
	data : 0.11500091552734375
	model : 0.06498246192932129
			 train-loss:  2.2054039823425398 	 ± 0.23609621286843682
	data : 0.11501169204711914
	model : 0.06503267288208008
			 train-loss:  2.2067133958141008 	 ± 0.23579548580242896
	data : 0.11497364044189454
	model : 0.06503310203552246
			 train-loss:  2.206827664375305 	 ± 0.2349849920149837
	data : 0.11502995491027831
	model : 0.06501379013061523
			 train-loss:  2.205936564974589 	 ± 0.23442457164308067
	data : 0.11507964134216309
	model : 0.06503415107727051
			 train-loss:  2.205501472868887 	 ± 0.2336849922885219
	data : 0.11506357192993164
	model : 0.06507210731506348
			 train-loss:  2.206179608364363 	 ± 0.23303926416419746
	data : 0.11502246856689453
	model : 0.06501946449279786
			 train-loss:  2.2070481033133182 	 ± 0.23249613777935071
	data : 0.11480460166931153
	model : 0.06505632400512695
			 train-loss:  2.2060186727841695 	 ± 0.2320603163189185
	data : 0.11468420028686524
	model : 0.06503901481628419
			 train-loss:  2.206110108767124 	 ± 0.23129333927461818
	data : 0.11471371650695801
	model : 0.06501479148864746
			 train-loss:  2.205727295655953 	 ± 0.2305792398433942
	data : 0.11475701332092285
	model : 0.06498537063598633
			 train-loss:  2.2054737401164437 	 ± 0.22984573665176256
	data : 0.11483922004699706
	model : 0.06505308151245118
			 train-loss:  2.2059677754129683 	 ± 0.22917975399091614
	data : 0.11511812210083008
	model : 0.0650240421295166
			 train-loss:  2.2053858072526995 	 ± 0.22855340085691178
	data : 0.11515569686889648
	model : 0.06507019996643067
			 train-loss:  2.203785627316206 	 ± 0.22868908135527036
	data : 0.11508121490478515
	model : 0.06513404846191406
			 train-loss:  2.2026908139514316 	 ± 0.228369366075128
	data : 0.11501989364624024
	model : 0.06512837409973145
			 train-loss:  2.2016338109970093 	 ± 0.22803047361013185
	data : 0.11486573219299316
	model : 0.06507787704467774
			 train-loss:  2.201105272245107 	 ± 0.22740933117248632
	data : 0.11464462280273438
	model : 0.06511383056640625
			 train-loss:  2.1992208518087866 	 ± 0.22793946685571267
	data : 0.11464362144470215
	model : 0.06509513854980468
			 train-loss:  2.1981018574341484 	 ± 0.22767088907832886
	data : 0.11477599143981934
	model : 0.06506314277648925
			 train-loss:  2.1977376179930603 	 ± 0.2270141636377837
	data : 0.11476144790649415
	model : 0.06512551307678223
			 train-loss:  2.196502134112493 	 ± 0.2268623862936198
	data : 0.11487288475036621
	model : 0.06514949798583984
			 train-loss:  2.196553987700765 	 ± 0.22617064400224354
	data : 0.11503129005432129
	model : 0.06509895324707031
			 train-loss:  2.1954560742233737 	 ± 0.2259221745474327
	data : 0.11510920524597168
	model : 0.06510615348815918
			 train-loss:  2.195909886475069 	 ± 0.2253160782021547
	data : 0.11505675315856934
	model : 0.06506190299987794
			 train-loss:  2.1960050017533903 	 ± 0.22464380887516666
	data : 0.11492481231689453
	model : 0.06502137184143067
			 train-loss:  2.19613577354522 	 ± 0.22398060366711212
	data : 0.11508698463439941
	model : 0.06499943733215333
			 train-loss:  2.19635678184103 	 ± 0.22333532799379693
	data : 0.1151127815246582
	model : 0.06499061584472657
			 train-loss:  2.1966105643440694 	 ± 0.2227019293491547
	data : 0.11504020690917968
	model : 0.06501736640930175
			 train-loss:  2.1959371943222847 	 ± 0.22222330210054056
	data : 0.1149686336517334
	model : 0.06506791114807128
			 train-loss:  2.195833452912264 	 ± 0.22158051525741987
	data : 0.11499977111816406
	model : 0.06507606506347656
			 train-loss:  2.19548086623925 	 ± 0.220987565928077
	data : 0.11487698554992676
	model : 0.06509280204772949
			 train-loss:  2.1954806700520133 	 ± 0.22035162918319562
	data : 0.11483421325683593
	model : 0.0651423454284668
			 train-loss:  2.194811432702201 	 ± 0.21989842003534268
	data : 0.11482357978820801
	model : 0.0651172161102295
			 train-loss:  2.1988106058402495 	 ± 0.22556465046243332
	data : 0.11486334800720215
	model : 0.06507043838500977
			 train-loss:  2.2002005967716713 	 ± 0.22568119637821915
	data : 0.11499848365783691
	model : 0.06504058837890625
			 train-loss:  2.1983447161953102 	 ± 0.22639679185910577
	data : 0.11505541801452637
	model : 0.06506729125976562
			 train-loss:  2.197403185194431 	 ± 0.2261127092821095
	data : 0.11516990661621093
	model : 0.06504335403442382
			 train-loss:  2.1967717044883304 	 ± 0.22564196883147647
	data : 0.11517400741577148
	model : 0.06505627632141113
			 train-loss:  2.1983193514755417 	 ± 0.22597376318718673
	data : 0.11514825820922851
	model : 0.06507248878479004
			 train-loss:  2.200569463955177 	 ± 0.2273762818744935
	data : 0.11508917808532715
	model : 0.06506385803222656
			 train-loss:  2.20215715801781 	 ± 0.22776356298977998
	data : 0.11508369445800781
	model : 0.06501975059509277
			 train-loss:  2.205465734652851 	 ± 0.2315114400121099
	data : 0.11479806900024414
	model : 0.06500849723815919
			 train-loss:  2.2052042980451843 	 ± 0.23091211882739
	data : 0.11474790573120117
	model : 0.0649942398071289
			 train-loss:  2.2037067156966015 	 ± 0.23118963439857595
	data : 0.11482486724853516
	model : 0.06499414443969727
			 train-loss:  2.204518216179016 	 ± 0.23083611622855119
	data : 0.11489248275756836
	model : 0.06504015922546387
			 train-loss:  2.2040852295591478 	 ± 0.23029749959154244
	data : 0.11479363441467286
	model : 0.06506242752075195
			 train-loss:  2.2049306019273383 	 ± 0.2299797265668127
	data : 0.11484723091125489
	model : 0.0651137351989746
			 train-loss:  2.2026266443101985 	 ± 0.2315503322662227
	data : 0.11489109992980957
	model : 0.06514406204223633
			 train-loss:  2.2007253244909317 	 ± 0.23242568682020878
	data : 0.11488628387451172
	model : 0.06516590118408203
			 train-loss:  2.2017941561837993 	 ± 0.2322897662412441
	data : 0.11472544670104981
	model : 0.06512832641601562
			 train-loss:  2.2011934438517673 	 ± 0.2318366705570729
	data : 0.11462135314941406
	model : 0.06540155410766602
			 train-loss:  2.2004674906583177 	 ± 0.23145820696979058
	data : 0.11447086334228515
	model : 0.06534848213195801
			 train-loss:  2.1999610900878905 	 ± 0.23097168312026234
	data : 0.11456341743469238
	model : 0.0653083324432373
			 train-loss:  2.2013025758217792 	 ± 0.23114206458630768
	data : 0.11460247039794921
	model : 0.0654573917388916
			 train-loss:  2.201537104427512 	 ± 0.23057804199551776
	data : 0.11452341079711914
	model : 0.06549181938171386
			 train-loss:  2.2004116460530443 	 ± 0.23053687006811416
	data : 0.11467189788818359
	model : 0.06524477005004883
			 train-loss:  2.198469638824463 	 ± 0.23157485289106397
	data : 0.11491665840148926
	model : 0.06546387672424317
			 train-loss:  2.198655779361725 	 ± 0.2310101143687937
	data : 0.1146977424621582
	model : 0.06546945571899414
			 train-loss:  2.198828866825768 	 ± 0.23044774664622675
	data : 0.11454658508300782
	model : 0.06525273323059082
			 train-loss:  2.1979989651406164 	 ± 0.2301775363298489
	data : 0.11478028297424317
	model : 0.06522216796875
			 train-loss:  2.19928731237139 	 ± 0.23033886349923063
	data : 0.11479406356811524
	model : 0.06517810821533203
			 train-loss:  2.1986070684358183 	 ± 0.22997792981846224
	data : 0.11485838890075684
	model : 0.06562862396240235
			 train-loss:  2.198475572539539 	 ± 0.22942400995293952
	data : 0.11455349922180176
	model : 0.06568198204040528
			 train-loss:  2.1977937649754646 	 ± 0.2290745761229285
	data : 0.11458373069763184
	model : 0.06592693328857421
			 train-loss:  2.1969385642360373 	 ± 0.22884999475939866
	data : 0.11433491706848145
	model : 0.06593732833862305
			 train-loss:  2.198255322300471 	 ± 0.22908390913274818
	data : 0.1143277645111084
	model : 0.06640677452087403
			 train-loss:  2.197705223229513 	 ± 0.22867287181977303
	data : 0.11377601623535157
	model : 0.06585988998413086
			 train-loss:  2.1980105547677904 	 ± 0.22817046416192782
	data : 0.11422700881958008
	model : 0.06578707695007324
			 train-loss:  2.197289034088641 	 ± 0.22786914414289866
	data : 0.1141770362854004
	model : 0.06563639640808105
			 train-loss:  2.1991558401089795 	 ± 0.2289426745790889
	data : 0.1145296573638916
	model : 0.06559453010559083
			 train-loss:  2.2001221829177067 	 ± 0.22883758240083588
	data : 0.11454963684082031
	model : 0.06512765884399414
			 train-loss:  2.1996120749232926 	 ± 0.22842364142814311
	data : 0.11507792472839355
	model : 0.06500768661499023
			 train-loss:  2.2000960993212324 	 ± 0.22800177719992493
	data : 0.11508116722106934
	model : 0.06503849029541016
			 train-loss:  2.198964722730495 	 ± 0.22807749292869942
	data : 0.11516475677490234
	model : 0.06503310203552246
			 train-loss:  2.198090743359333 	 ± 0.2279136058560358
	data : 0.1150136947631836
	model : 0.06507148742675781
			 train-loss:  2.1976646294287585 	 ± 0.22747688906255725
	data : 0.11493124961853027
	model : 0.0650568962097168
			 train-loss:  2.1972116150268137 	 ± 0.2270554812570783
	data : 0.11476659774780273
	model : 0.06505265235900878
			 train-loss:  2.196604304963892 	 ± 0.2267170640285432
	data : 0.11489081382751465
	model : 0.06534552574157715
			 train-loss:  2.1989833531875957 	 ± 0.22893932951265217
	data : 0.11515884399414063
	model : 0.06523404121398926
			 train-loss:  2.1988296379914156 	 ± 0.2284345483838695
	data : 0.11530885696411133
	model : 0.06519479751586914
			 train-loss:  2.2012215323512327 	 ± 0.23069121910733414
	data : 0.11549129486083984
	model : 0.06509695053100586
			 train-loss:  2.200199391692877 	 ± 0.23068125261569833
	data : 0.11562271118164062
	model : 0.06497106552124024
			 train-loss:  2.1999545685450235 	 ± 0.23019722098048012
	data : 0.11562161445617676
	model : 0.0644752025604248
			 train-loss:  2.2028302381523943 	 ± 0.23370263196154734
	data : 0.1152878761291504
	model : 0.06429738998413086
			 train-loss:  2.202243209935495 	 ± 0.23335423093213994
	data : 0.11518092155456543
	model : 0.06412668228149414
			 train-loss:  2.201748827570363 	 ± 0.2329610372072131
	data : 0.1153829574584961
	model : 0.06400113105773926
			 train-loss:  2.2012144649913736 	 ± 0.23259182736109776
	data : 0.11571645736694336
	model : 0.0639333724975586
			 train-loss:  2.2020957423293073 	 ± 0.23246848837110326
	data : 0.1157754898071289
	model : 0.0638777732849121
			 train-loss:  2.201888728967477 	 ± 0.23198600887950485
	data : 0.11594834327697753
	model : 0.06390972137451172
			 train-loss:  2.20171260474057 	 ± 0.23150097593302282
	data : 0.11599397659301758
	model : 0.063908052444458
			 train-loss:  2.20298195345719 	 ± 0.23181134435597003
	data : 0.11582741737365723
	model : 0.06388292312622071
			 train-loss:  2.203026931000571 	 ± 0.23131650949859672
	data : 0.11558375358581544
	model : 0.06384158134460449
			 train-loss:  2.201464905130102 	 ± 0.23205727572927964
	data : 0.11559724807739258
	model : 0.06388030052185059
			 train-loss:  2.200971484689389 	 ± 0.23168861166517127
	data : 0.11572656631469727
	model : 0.06384768486022949
			 train-loss:  2.2006108252811027 	 ± 0.2312656789672257
	data : 0.1157681941986084
	model : 0.0638723373413086
			 train-loss:  2.2003232135492214 	 ± 0.2308217864932879
	data : 0.11573514938354493
	model : 0.06392741203308105
			 train-loss:  2.1990172638553953 	 ± 0.23121782759231246
	data : 0.11580386161804199
	model : 0.06402626037597656
			 train-loss:  2.1977654293179514 	 ± 0.2315458080405983
	data : 0.11579666137695313
	model : 0.06405739784240723
			 train-loss:  2.197650891121987 	 ± 0.23107173623375113
	data : 0.11579365730285644
	model : 0.06406068801879883
			 train-loss:  2.1967059910789994 	 ± 0.23105991390072853
	data : 0.11577606201171875
	model : 0.06399149894714355
			 train-loss:  2.1962453185776134 	 ± 0.2306953279310681
	data : 0.11576924324035645
	model : 0.06395602226257324
			 train-loss:  2.1982309666813395 	 ± 0.23229360347228273
	data : 0.11590156555175782
	model : 0.06386346817016601
			 train-loss:  2.198047336753534 	 ± 0.23183679536527393
	data : 0.11598968505859375
	model : 0.06382288932800292
			 train-loss:  2.200497649064878 	 ± 0.23452248957359065
	data : 0.11596345901489258
	model : 0.06382613182067871
			 train-loss:  2.2007044264179494 	 ± 0.23406973535833372
	data : 0.1158945083618164
	model : 0.06385998725891114
			 train-loss:  2.1991447042072974 	 ± 0.23487997831794444
	data : 0.1158841609954834
	model : 0.06388654708862304
			 train-loss:  2.1984359763233536 	 ± 0.2346734172591441
	data : 0.11572575569152832
	model : 0.06388425827026367
			 train-loss:  2.1978939490318297 	 ± 0.2343597254641173
	data : 0.11550140380859375
	model : 0.06385526657104493
			 train-loss:  2.197177311813689 	 ± 0.2341667152096488
	data : 0.11545982360839843
	model : 0.06387100219726563
			 train-loss:  2.197630956532463 	 ± 0.23381212382976552
	data : 0.11562147140502929
	model : 0.06386208534240723
			 train-loss:  2.1961319521952993 	 ± 0.2345597520788668
	data : 0.11572260856628418
	model : 0.06386470794677734
			 train-loss:  2.1961410158262478 	 ± 0.2340976093175084
	data : 0.11584091186523438
	model : 0.06388592720031738
			 train-loss:  2.195856408511891 	 ± 0.23368216969503397
	data : 0.11580052375793456
	model : 0.06390171051025391
			 train-loss:  2.1943571493029594 	 ± 0.23445091145499986
	data : 0.11557307243347167
	model : 0.05547399520874023
#epoch  29    val-loss:  2.4569585950751054  train-loss:  2.1943571493029594  lr:  3.90625e-05
			 train-loss:  1.9676547050476074 	 ± 0.0
	data : 5.778272867202759
	model : 0.07145881652832031
			 train-loss:  2.2471975088119507 	 ± 0.27954280376434326
	data : 2.954744577407837
	model : 0.0680915117263794
			 train-loss:  2.257280429204305 	 ± 0.22869073000793858
	data : 2.008072296778361
	model : 0.06693069140116374
			 train-loss:  2.1768637895584106 	 ± 0.24212619703263377
	data : 1.5345415472984314
	model : 0.06635892391204834
			 train-loss:  2.1422688007354735 	 ± 0.22734847533736655
	data : 1.250572633743286
	model : 0.06602001190185547
			 train-loss:  2.157567818959554 	 ± 0.2103403769091692
	data : 0.11790361404418945
	model : 0.06464753150939942
			 train-loss:  2.168423754828317 	 ± 0.1965445065483676
	data : 0.11467142105102539
	model : 0.0646897792816162
			 train-loss:  2.1813654601573944 	 ± 0.18701186949047213
	data : 0.1146397590637207
	model : 0.06476097106933594
			 train-loss:  2.241519874996609 	 ± 0.24502230638074374
	data : 0.11482014656066894
	model : 0.06479740142822266
			 train-loss:  2.2218915939331056 	 ± 0.239791080566597
	data : 0.11476683616638184
	model : 0.06486625671386718
			 train-loss:  2.213845578106967 	 ± 0.2300432457302429
	data : 0.11473541259765625
	model : 0.06495327949523926
			 train-loss:  2.2104820211728415 	 ± 0.22053197575786979
	data : 0.1144566535949707
	model : 0.06492271423339843
			 train-loss:  2.1997663424565244 	 ± 0.2151073163571444
	data : 0.11451044082641601
	model : 0.06490206718444824
			 train-loss:  2.1893913064684187 	 ± 0.21063098703897165
	data : 0.11449122428894043
	model : 0.06493620872497559
			 train-loss:  2.180389165878296 	 ± 0.206257746554656
	data : 0.11461281776428223
	model : 0.06496477127075195
			 train-loss:  2.182356208562851 	 ± 0.19985346067002402
	data : 0.114650297164917
	model : 0.06498522758483886
			 train-loss:  2.194677521200741 	 ± 0.2000523611665601
	data : 0.11485829353332519
	model : 0.06504769325256347
			 train-loss:  2.20281720161438 	 ± 0.1972913733677001
	data : 0.11475210189819336
	model : 0.06503634452819824
			 train-loss:  2.1955825529600443 	 ± 0.19446691998079374
	data : 0.11472039222717285
	model : 0.06501994132995606
			 train-loss:  2.1912482142448426 	 ± 0.19048217021112365
	data : 0.11453313827514648
	model : 0.0649559497833252
			 train-loss:  2.178620911779858 	 ± 0.19427982099152974
	data : 0.11441226005554199
	model : 0.06490139961242676
			 train-loss:  2.1814316998828542 	 ± 0.19024955745780714
	data : 0.11442160606384277
	model : 0.06489439010620117
			 train-loss:  2.1856305754703023 	 ± 0.18710712252845965
	data : 0.1145777702331543
	model : 0.06489930152893067
			 train-loss:  2.195946072538694 	 ± 0.1897308142202362
	data : 0.11446957588195801
	model : 0.06486120223999023
			 train-loss:  2.1866359519958496 	 ± 0.19141094759344812
	data : 0.11445469856262207
	model : 0.06487970352172852
			 train-loss:  2.1772047189565806 	 ± 0.19352698810821772
	data : 0.11458749771118164
	model : 0.06494665145874023
			 train-loss:  2.1685680989865905 	 ± 0.1949485306096657
	data : 0.11458725929260254
	model : 0.06492800712585449
			 train-loss:  2.1709427748407637 	 ± 0.1918329116162961
	data : 0.11446599960327149
	model : 0.06497006416320801
			 train-loss:  2.1778856145924537 	 ± 0.19204319761513491
	data : 0.11467409133911133
	model : 0.06502633094787598
			 train-loss:  2.1749761501948037 	 ± 0.18946430077438184
	data : 0.11484770774841309
	model : 0.06503748893737793
			 train-loss:  2.170932662102484 	 ± 0.1876945827559474
	data : 0.11491036415100098
	model : 0.06501030921936035
			 train-loss:  2.176955834031105 	 ± 0.1877577661139214
	data : 0.11486024856567383
	model : 0.06503276824951172
			 train-loss:  2.1676716082023852 	 ± 0.19220563422755224
	data : 0.11494455337524415
	model : 0.0650177001953125
			 train-loss:  2.180132781758028 	 ± 0.2024369402172377
	data : 0.11489968299865723
	model : 0.06498584747314454
			 train-loss:  2.176414224079677 	 ± 0.20069872451229614
	data : 0.11479558944702148
	model : 0.06497864723205567
			 train-loss:  2.176226748360528 	 ± 0.1978947192134993
	data : 0.11463732719421386
	model : 0.06496977806091309
			 train-loss:  2.1882104358157597 	 ± 0.20802354279095436
	data : 0.11463532447814942
	model : 0.06492719650268555
			 train-loss:  2.196382472389623 	 ± 0.21120120979437368
	data : 0.11466574668884277
	model : 0.06490259170532227
			 train-loss:  2.1882288333697195 	 ± 0.2144493371333618
	data : 0.11460504531860352
	model : 0.06498441696166993
			 train-loss:  2.191346788406372 	 ± 0.21264512536812988
	data : 0.11459054946899414
	model : 0.0649754524230957
			 train-loss:  2.1860615189482524 	 ± 0.2126791843584726
	data : 0.11460456848144532
	model : 0.06497583389282227
			 train-loss:  2.1837985543977645 	 ± 0.21063103774415481
	data : 0.11469650268554688
	model : 0.06498680114746094
			 train-loss:  2.177723718243976 	 ± 0.21185757579348657
	data : 0.11463322639465331
	model : 0.06505446434020996
			 train-loss:  2.1758373975753784 	 ± 0.2098012218974273
	data : 0.11463642120361328
	model : 0.06496663093566894
			 train-loss:  2.1823219193352594 	 ± 0.21186921456850613
	data : 0.11464619636535645
	model : 0.06498451232910156
			 train-loss:  2.17762096291003 	 ± 0.21191314338970418
	data : 0.11485805511474609
	model : 0.06496963500976563
			 train-loss:  2.16855651774305 	 ± 0.21847484995685168
	data : 0.11487379074096679
	model : 0.06499896049499512
			 train-loss:  2.167067145307859 	 ± 0.21642808420841703
	data : 0.11493878364562989
	model : 0.06498174667358399
			 train-loss:  2.1641993230702927 	 ± 0.21512774323475045
	data : 0.11493477821350098
	model : 0.0650629997253418
			 train-loss:  2.168947672843933 	 ± 0.21554382720031667
	data : 0.11502232551574706
	model : 0.06503911018371582
			 train-loss:  2.16993906918694 	 ± 0.21353529268455299
	data : 0.11490697860717773
	model : 0.06503715515136718
			 train-loss:  2.171467152925638 	 ± 0.21175348095239113
	data : 0.11480178833007812
	model : 0.06501522064208984
			 train-loss:  2.175168811150317 	 ± 0.21143799547158526
	data : 0.1148139476776123
	model : 0.06496524810791016
			 train-loss:  2.170527160167694 	 ± 0.2121792108660587
	data : 0.11487832069396972
	model : 0.06492938995361328
			 train-loss:  2.1687651309100064 	 ± 0.21063980652823333
	data : 0.11489253044128418
	model : 0.06493959426879883
			 train-loss:  2.168210525597845 	 ± 0.2087911385171817
	data : 0.11482229232788085
	model : 0.06494736671447754
			 train-loss:  2.1665492371508948 	 ± 0.20732460132976502
	data : 0.1148911952972412
	model : 0.06497974395751953
			 train-loss:  2.1688321931608794 	 ± 0.20625099582813344
	data : 0.11481924057006836
	model : 0.06504082679748535
			 train-loss:  2.173294978626704 	 ± 0.20730079497438192
	data : 0.11469588279724122
	model : 0.06501140594482421
			 train-loss:  2.1684918185075124 	 ± 0.20885052809229876
	data : 0.11459784507751465
	model : 0.06502151489257812
			 train-loss:  2.1718512929853846 	 ± 0.2087597880989756
	data : 0.11442914009094238
	model : 0.06497912406921387
			 train-loss:  2.1680395583952627 	 ± 0.20919852768900593
	data : 0.11434497833251953
	model : 0.06495809555053711
			 train-loss:  2.1645476591019404 	 ± 0.20934503887520925
	data : 0.11433916091918946
	model : 0.06493215560913086
			 train-loss:  2.1703758127987385 	 ± 0.2127921966113302
	data : 0.11440844535827636
	model : 0.06497502326965332
			 train-loss:  2.169270064280583 	 ± 0.21133420706703743
	data : 0.11444568634033203
	model : 0.06498312950134277
			 train-loss:  2.1749463803840405 	 ± 0.21466202405510923
	data : 0.11466016769409179
	model : 0.06500849723815919
			 train-loss:  2.17348232553966 	 ± 0.21338578848537101
	data : 0.1146474838256836
	model : 0.06503167152404785
			 train-loss:  2.176162186790915 	 ± 0.21294378479961046
	data : 0.11450362205505371
	model : 0.06496496200561523
			 train-loss:  2.17620811600616 	 ± 0.21139542145733273
	data : 0.11446447372436523
	model : 0.06494479179382324
			 train-loss:  2.175927938733782 	 ± 0.2098929258436182
	data : 0.11466841697692871
	model : 0.06492538452148437
			 train-loss:  2.1756489008245334 	 ± 0.20842264067132438
	data : 0.11468396186828614
	model : 0.06492266654968262
			 train-loss:  2.1703446673022375 	 ± 0.211740976837569
	data : 0.11482062339782714
	model : 0.06492123603820801
			 train-loss:  2.1701465534837276 	 ± 0.2102924143130102
	data : 0.11489944458007813
	model : 0.06497278213500976
			 train-loss:  2.169529537896852 	 ± 0.20893320575837543
	data : 0.11503496170043945
	model : 0.06494460105895997
			 train-loss:  2.1683194923400877 	 ± 0.20779652283107217
	data : 0.11480126380920411
	model : 0.06497359275817871
			 train-loss:  2.1698529814418994 	 ± 0.20685167180780445
	data : 0.11473569869995118
	model : 0.06496219635009766
			 train-loss:  2.1756512251767246 	 ± 0.21162944677038573
	data : 0.11463027000427246
	model : 0.0649221420288086
			 train-loss:  2.175379059253595 	 ± 0.21028203409091398
	data : 0.11471953392028808
	model : 0.06491999626159668
			 train-loss:  2.1763772783400137 	 ± 0.20913279993715922
	data : 0.11464443206787109
	model : 0.0649566650390625
			 train-loss:  2.1781464546918867 	 ± 0.20841566692932317
	data : 0.114680814743042
	model : 0.06495366096496583
			 train-loss:  2.178614154274081 	 ± 0.2071673943467534
	data : 0.11475248336791992
	model : 0.06498346328735352
			 train-loss:  2.17895097267337 	 ± 0.20592261701490666
	data : 0.11491403579711915
	model : 0.06497669219970703
			 train-loss:  2.1789688518248407 	 ± 0.20467842423687674
	data : 0.1149259090423584
	model : 0.06498441696166993
			 train-loss:  2.1770084159714833 	 ± 0.20423888640257187
	data : 0.11495294570922851
	model : 0.0650062084197998
			 train-loss:  2.1779443264007567 	 ± 0.2032150421910581
	data : 0.11492414474487304
	model : 0.06497063636779785
			 train-loss:  2.17822619371636 	 ± 0.20204681734148472
	data : 0.11484785079956054
	model : 0.06495890617370606
			 train-loss:  2.1799116189452423 	 ± 0.20148941431026998
	data : 0.11485085487365723
	model : 0.06496734619140625
			 train-loss:  2.1794662990353326 	 ± 0.2003843714101781
	data : 0.11497912406921387
	model : 0.06499438285827637
			 train-loss:  2.1822097542580594 	 ± 0.2009105896373686
	data : 0.11498494148254394
	model : 0.06498360633850098
			 train-loss:  2.183446979522705 	 ± 0.2001319538492903
	data : 0.11510052680969238
	model : 0.06499614715576171
			 train-loss:  2.184471381889595 	 ± 0.19926641544061416
	data : 0.11509814262390136
	model : 0.06501460075378418
			 train-loss:  2.183615513469862 	 ± 0.1983485917291732
	data : 0.11502647399902344
	model : 0.06504940986633301
			 train-loss:  2.181535897716399 	 ± 0.19828517739836166
	data : 0.11516523361206055
	model : 0.06501731872558594
			 train-loss:  2.1810187507182994 	 ± 0.19729069291263068
	data : 0.11513233184814453
	model : 0.06499366760253907
			 train-loss:  2.1890922621676796 	 ± 0.21128408461111148
	data : 0.11499395370483398
	model : 0.06492729187011718
			 train-loss:  2.1866942420601845 	 ± 0.2114763625217571
	data : 0.11503705978393555
	model : 0.06495904922485352
			 train-loss:  2.1856554119857314 	 ± 0.21062952756615844
	data : 0.11499824523925781
	model : 0.06492524147033692
			 train-loss:  2.1837479107233944 	 ± 0.21039257659650404
	data : 0.11476674079895019
	model : 0.06494989395141601
			 train-loss:  2.1810116551139136 	 ± 0.21107261782329942
	data : 0.11474709510803223
	model : 0.06497955322265625
			 train-loss:  2.186543321609497 	 ± 0.21710704176154744
	data : 0.11483573913574219
	model : 0.06505718231201171
			 train-loss:  2.185031548585042 	 ± 0.21655790346314147
	data : 0.11477422714233398
	model : 0.06503725051879883
			 train-loss:  2.1829960287785997 	 ± 0.2164625262718282
	data : 0.11485261917114258
	model : 0.06508355140686035
			 train-loss:  2.1828860507428067 	 ± 0.2154120380117099
	data : 0.11476669311523438
	model : 0.06507463455200195
			 train-loss:  2.1827619958382387 	 ± 0.2143775987507736
	data : 0.11468462944030762
	model : 0.0649846076965332
			 train-loss:  2.18086104847136 	 ± 0.21423322906491868
	data : 0.11470155715942383
	model : 0.06500873565673829
			 train-loss:  2.1793028880965033 	 ± 0.21381726348299068
	data : 0.11486454010009765
	model : 0.06498432159423828
			 train-loss:  2.1812709491943645 	 ± 0.2137782000877144
	data : 0.11486058235168457
	model : 0.0649714469909668
			 train-loss:  2.1821275309280113 	 ± 0.21297058406629524
	data : 0.11488394737243653
	model : 0.06498179435729981
			 train-loss:  2.1813622286560337 	 ± 0.21214054193886867
	data : 0.11500344276428223
	model : 0.06505756378173828
			 train-loss:  2.1805746295235373 	 ± 0.2113340953608101
	data : 0.11500554084777832
	model : 0.06507606506347656
			 train-loss:  2.18270798416825 	 ± 0.21156646905019524
	data : 0.11497173309326172
	model : 0.06508550643920899
			 train-loss:  2.182987692100661 	 ± 0.21064047311194017
	data : 0.11479740142822266
	model : 0.06506638526916504
			 train-loss:  2.186665370401028 	 ± 0.21328758720726368
	data : 0.11472454071044921
	model : 0.06503734588623047
			 train-loss:  2.185212308900398 	 ± 0.2129110908146568
	data : 0.11478748321533203
	model : 0.0650141716003418
			 train-loss:  2.186553768489672 	 ± 0.2124666877578672
	data : 0.1148770809173584
	model : 0.06498608589172364
			 train-loss:  2.188182935632508 	 ± 0.21226909518527076
	data : 0.11484599113464355
	model : 0.06497130393981934
			 train-loss:  2.187063317013602 	 ± 0.2117037258434206
	data : 0.11494121551513672
	model : 0.06498689651489258
			 train-loss:  2.1890002549704857 	 ± 0.21184334369152705
	data : 0.11502242088317871
	model : 0.06501107215881348
			 train-loss:  2.189212187999437 	 ± 0.21096392961859403
	data : 0.11503162384033203
	model : 0.06504759788513184
			 train-loss:  2.1913646678129832 	 ± 0.21139121376213976
	data : 0.11499557495117188
	model : 0.06501827239990235
			 train-loss:  2.190150499343872 	 ± 0.21093563485661934
	data : 0.11496086120605468
	model : 0.06506781578063965
			 train-loss:  2.195624720854837 	 ± 0.21852953500218217
	data : 0.11538157463073731
	model : 0.06498265266418457
			 train-loss:  2.196088397405981 	 ± 0.2176996413848025
	data : 0.11525702476501465
	model : 0.06492862701416016
			 train-loss:  2.194247611107365 	 ± 0.2177790530627592
	data : 0.11519927978515625
	model : 0.0649263858795166
			 train-loss:  2.1953040142059326 	 ± 0.21722494487538738
	data : 0.11531391143798828
	model : 0.06494026184082032
			 train-loss:  2.1949827727817355 	 ± 0.2163910320432785
	data : 0.11539754867553711
	model : 0.06491298675537109
			 train-loss:  2.1937639488009957 	 ± 0.2159711887270707
	data : 0.1150120735168457
	model : 0.06505913734436035
			 train-loss:  2.193776622414589 	 ± 0.21512594448496028
	data : 0.11507768630981445
	model : 0.06512174606323243
			 train-loss:  2.193405548731486 	 ± 0.21433162101314632
	data : 0.11500329971313476
	model : 0.06512432098388672
			 train-loss:  2.1922453183394213 	 ± 0.21391195749016884
	data : 0.11491084098815918
	model : 0.06512489318847656
			 train-loss:  2.19457734814127 	 ± 0.21474639124268052
	data : 0.11472773551940918
	model : 0.06512689590454102
			 train-loss:  2.198447948152369 	 ± 0.21847020653333277
	data : 0.11468095779418945
	model : 0.06498713493347168
			 train-loss:  2.2005058984111128 	 ± 0.21892785516341462
	data : 0.11463613510131836
	model : 0.06497211456298828
			 train-loss:  2.198740570402857 	 ± 0.21905753255657032
	data : 0.1147986888885498
	model : 0.0649505615234375
			 train-loss:  2.1977908266915214 	 ± 0.2185214382506024
	data : 0.11481590270996093
	model : 0.0649388313293457
			 train-loss:  2.196939132669393 	 ± 0.21794134714458416
	data : 0.11498355865478516
	model : 0.06493206024169922
			 train-loss:  2.1964544649541815 	 ± 0.21721803263261902
	data : 0.11494898796081543
	model : 0.06499257087707519
			 train-loss:  2.196304510469022 	 ± 0.21643669663765017
	data : 0.11506524085998535
	model : 0.06502208709716797
			 train-loss:  2.1944620643588277 	 ± 0.21674013529122085
	data : 0.11497383117675782
	model : 0.06503062248229981
			 train-loss:  2.195688050133841 	 ± 0.2164478321907004
	data : 0.11487832069396972
	model : 0.06504306793212891
			 train-loss:  2.1943874392949096 	 ± 0.21622724004312874
	data : 0.11472315788269043
	model : 0.06504206657409668
			 train-loss:  2.19308082822343 	 ± 0.21602241562464217
	data : 0.11463446617126465
	model : 0.06500048637390136
			 train-loss:  2.192408234923036 	 ± 0.2154149224301157
	data : 0.11472229957580567
	model : 0.06497936248779297
			 train-loss:  2.1934764948156147 	 ± 0.2150454144742844
	data : 0.11487016677856446
	model : 0.06495647430419922
			 train-loss:  2.1933465711001694 	 ± 0.21430826684027446
	data : 0.11493043899536133
	model : 0.06497917175292969
			 train-loss:  2.1937893776044453 	 ± 0.21363962394182726
	data : 0.11500496864318847
	model : 0.06499981880187988
			 train-loss:  2.192360137595611 	 ± 0.21361095014105247
	data : 0.11507716178894042
	model : 0.06510634422302246
			 train-loss:  2.190553438019108 	 ± 0.21401205963459963
	data : 0.114937162399292
	model : 0.06513404846191406
			 train-loss:  2.1914349690379713 	 ± 0.213562126017184
	data : 0.11479573249816895
	model : 0.06512784957885742
			 train-loss:  2.189494585990906 	 ± 0.21416283756420307
	data : 0.11468181610107422
	model : 0.06510019302368164
			 train-loss:  2.1909195587335044 	 ± 0.21416478840948044
	data : 0.11458826065063477
	model : 0.0650327205657959
			 train-loss:  2.189567779239855 	 ± 0.2141044754562331
	data : 0.1147183895111084
	model : 0.0649482250213623
			 train-loss:  2.188647025550892 	 ± 0.21370535205638513
	data : 0.11487812995910644
	model : 0.0649348258972168
			 train-loss:  2.1866714366070634 	 ± 0.2144074877619168
	data : 0.1150289535522461
	model : 0.06495256423950195
			 train-loss:  2.186448474084177 	 ± 0.21373264180447202
	data : 0.11514477729797364
	model : 0.06498379707336426
			 train-loss:  2.1853586389468265 	 ± 0.21347812752194
	data : 0.11523880958557128
	model : 0.06508822441101074
			 train-loss:  2.1849760751056064 	 ± 0.2128508139676555
	data : 0.11516542434692383
	model : 0.06513857841491699
			 train-loss:  2.185040330585045 	 ± 0.21217769380481477
	data : 0.11510591506958008
	model : 0.06511497497558594
			 train-loss:  2.184062287492572 	 ± 0.21186639852243544
	data : 0.11499252319335937
	model : 0.06516780853271484
			 train-loss:  2.183203986287117 	 ± 0.21148039395591958
	data : 0.1149984359741211
	model : 0.06517505645751953
			 train-loss:  2.184229736742766 	 ± 0.21122148260616347
	data : 0.11491069793701172
	model : 0.06511015892028808
			 train-loss:  2.1822610858045977 	 ± 0.21204500914782407
	data : 0.11494135856628418
	model : 0.065106201171875
			 train-loss:  2.1810832784219754 	 ± 0.2119244441990191
	data : 0.11502156257629395
	model : 0.06510796546936035
			 train-loss:  2.1846994743114565 	 ± 0.21626291134881065
	data : 0.11514015197753906
	model : 0.0650442123413086
			 train-loss:  2.1848656220869587 	 ± 0.2156170718049131
	data : 0.11515984535217286
	model : 0.06501297950744629
			 train-loss:  2.185578462589218 	 ± 0.2151615681910312
	data : 0.11526226997375488
	model : 0.06501650810241699
			 train-loss:  2.1872464939505756 	 ± 0.21559024653344216
	data : 0.11525425910949708
	model : 0.06499180793762208
			 train-loss:  2.1854801057350066 	 ± 0.21615631850771239
	data : 0.11522736549377441
	model : 0.06499681472778321
			 train-loss:  2.1877491340129334 	 ± 0.21751328580955828
	data : 0.11508150100708008
	model : 0.06500134468078614
			 train-loss:  2.186294198737425 	 ± 0.2176958172373022
	data : 0.11492905616760254
	model : 0.0649996280670166
			 train-loss:  2.1857018700817177 	 ± 0.21719569701113617
	data : 0.11479034423828124
	model : 0.06499910354614258
			 train-loss:  2.188272629366365 	 ± 0.21915704162150668
	data : 0.11474714279174805
	model : 0.06500258445739746
			 train-loss:  2.1882685133487505 	 ± 0.21852272845887577
	data : 0.11468329429626464
	model : 0.06498050689697266
			 train-loss:  2.188057779580697 	 ± 0.21791151353096294
	data : 0.11477060317993164
	model : 0.06499781608581542
			 train-loss:  2.186700853620257 	 ± 0.21802398856743466
	data : 0.11486544609069824
	model : 0.06508612632751465
			 train-loss:  2.1883939911018717 	 ± 0.21855446112382476
	data : 0.11499838829040528
	model : 0.06509413719177246
			 train-loss:  2.188442199243664 	 ± 0.2179371395660992
	data : 0.11500158309936523
	model : 0.06510434150695801
			 train-loss:  2.190133347939909 	 ± 0.21848564973774648
	data : 0.11504802703857422
	model : 0.06510777473449707
			 train-loss:  2.189284288683417 	 ± 0.21816878370232654
	data : 0.11484365463256836
	model : 0.06513047218322754
			 train-loss:  2.187867137458589 	 ± 0.21838652847499776
	data : 0.11478137969970703
	model : 0.065045166015625
			 train-loss:  2.188313608670103 	 ± 0.21786477675506213
	data : 0.1147425651550293
	model : 0.06503634452819824
			 train-loss:  2.1884943083092407 	 ± 0.21727902335344781
	data : 0.11486053466796875
	model : 0.06501154899597168
			 train-loss:  2.187769633824708 	 ± 0.2169049855635934
	data : 0.11482977867126465
	model : 0.06501216888427734
			 train-loss:  2.1870011330946633 	 ± 0.21656444059966665
	data : 0.11494407653808594
	model : 0.06499724388122559
			 train-loss:  2.1870641972567584 	 ± 0.21598003230563265
	data : 0.11490979194641113
	model : 0.06503095626831054
			 train-loss:  2.1872497418875336 	 ± 0.21541344196533632
	data : 0.11500511169433594
	model : 0.0650589942932129
			 train-loss:  2.1890166309428087 	 ± 0.2161839024676172
	data : 0.11499533653259278
	model : 0.06506199836730957
			 train-loss:  2.189350931568349 	 ± 0.21565663739128124
	data : 0.11484880447387695
	model : 0.06507210731506348
			 train-loss:  2.188735022746697 	 ± 0.21525108344507288
	data : 0.11488323211669922
	model : 0.06507740020751954
			 train-loss:  2.1899919453420136 	 ± 0.21537818682706641
	data : 0.11493883132934571
	model : 0.06505422592163086
			 train-loss:  2.188490394522382 	 ± 0.2158084333710408
	data : 0.11498804092407226
	model : 0.06504764556884765
			 train-loss:  2.189392122129599 	 ± 0.2156061577721489
	data : 0.11496410369873047
	model : 0.06501688957214355
			 train-loss:  2.190580830055197 	 ± 0.2156767400086141
	data : 0.11510844230651855
	model : 0.06500616073608398
			 train-loss:  2.190312104126842 	 ± 0.21515254556468075
	data : 0.11495285034179688
	model : 0.0650404453277588
			 train-loss:  2.189563635068062 	 ± 0.2148532290421715
	data : 0.114984130859375
	model : 0.06501951217651367
			 train-loss:  2.190272169453757 	 ± 0.21453271119104386
	data : 0.11487832069396972
	model : 0.06499972343444824
			 train-loss:  2.1897846006499933 	 ± 0.21409636180311992
	data : 0.11482372283935546
	model : 0.06503658294677735
			 train-loss:  2.1897070781149046 	 ± 0.21355780197260538
	data : 0.11469464302062989
	model : 0.06501483917236328
			 train-loss:  2.190809458344426 	 ± 0.2135845784608916
	data : 0.11486215591430664
	model : 0.06495580673217774
			 train-loss:  2.189751769900322 	 ± 0.21357177378962935
	data : 0.11488394737243653
	model : 0.06501574516296386
			 train-loss:  2.1881191831323044 	 ± 0.2142872852205397
	data : 0.11503462791442871
	model : 0.06502642631530761
			 train-loss:  2.1874374345977707 	 ± 0.21397462377094426
	data : 0.11517367362976075
	model : 0.06504216194152831
			 train-loss:  2.188714844252676 	 ± 0.21421768296376628
	data : 0.11518816947937012
	model : 0.0650947093963623
			 train-loss:  2.1877233415257695 	 ± 0.2141584304272721
	data : 0.11506023406982421
	model : 0.06512255668640136
			 train-loss:  2.1883928990945583 	 ± 0.21384939093623523
	data : 0.11507759094238282
	model : 0.06506190299987794
			 train-loss:  2.1877666636578086 	 ± 0.21351805344877353
	data : 0.11490168571472167
	model : 0.06508851051330566
			 train-loss:  2.1872571752843073 	 ± 0.2131271708416658
	data : 0.1147308349609375
	model : 0.06511240005493164
			 train-loss:  2.186924604842296 	 ± 0.21266806310943584
	data : 0.11474699974060058
	model : 0.06508898735046387
			 train-loss:  2.1876399636838997 	 ± 0.2124093833253078
	data : 0.11492409706115722
	model : 0.06517624855041504
			 train-loss:  2.188848770800091 	 ± 0.21262242020809716
	data : 0.11487436294555664
	model : 0.065177583694458
			 train-loss:  2.1892468019684346 	 ± 0.21219638618489928
	data : 0.11484007835388184
	model : 0.0651555061340332
			 train-loss:  2.1898900827146925 	 ± 0.2119014565562107
	data : 0.1148383617401123
	model : 0.06513848304748535
			 train-loss:  2.189731260420571 	 ± 0.21141609756659893
	data : 0.11490612030029297
	model : 0.06518025398254394
			 train-loss:  2.190870815905455 	 ± 0.2115762318505625
	data : 0.11473073959350585
	model : 0.06506438255310058
			 train-loss:  2.189825442225434 	 ± 0.21163684795088047
	data : 0.11460957527160645
	model : 0.06516466140747071
			 train-loss:  2.190286202011285 	 ± 0.2112544389823203
	data : 0.11468281745910644
	model : 0.06514110565185546
			 train-loss:  2.189928175117563 	 ± 0.2108327881725364
	data : 0.11471381187438964
	model : 0.06513299942016601
			 train-loss:  2.18977295860238 	 ± 0.21036109753264076
	data : 0.11483402252197265
	model : 0.0650822639465332
			 train-loss:  2.1886742887975963 	 ± 0.2105062250980257
	data : 0.11494464874267578
	model : 0.06506552696228027
			 train-loss:  2.187954447486184 	 ± 0.21029723780040413
	data : 0.11503767967224121
	model : 0.0649533748626709
			 train-loss:  2.1859564533061033 	 ± 0.21190340061611754
	data : 0.11512198448181152
	model : 0.06493067741394043
			 train-loss:  2.185994354454247 	 ± 0.21142635280909328
	data : 0.11518621444702148
	model : 0.06483845710754395
			 train-loss:  2.1873306137563935 	 ± 0.21188924000005288
	data : 0.11508479118347167
	model : 0.0646998405456543
			 train-loss:  2.1878091810005054 	 ± 0.21153649732880322
	data : 0.11517801284790039
	model : 0.0645512580871582
			 train-loss:  2.1869581429163616 	 ± 0.2114498678766882
	data : 0.11540689468383789
	model : 0.06437397003173828
			 train-loss:  2.1869995641497386 	 ± 0.21098245465501791
	data : 0.1155247688293457
	model : 0.06418228149414062
			 train-loss:  2.1867835505943467 	 ± 0.21054226799960027
	data : 0.11555652618408203
	model : 0.06409287452697754
			 train-loss:  2.1865194842480777 	 ± 0.210117715346261
	data : 0.11569776535034179
	model : 0.06411805152893066
			 train-loss:  2.1863621265086546 	 ± 0.20967190450302575
	data : 0.11569027900695801
	model : 0.0640843391418457
			 train-loss:  2.1856134160705234 	 ± 0.20952216347367128
	data : 0.11561398506164551
	model : 0.06401004791259765
			 train-loss:  2.1857703542296503 	 ± 0.20908170772634058
	data : 0.11545820236206054
	model : 0.06401748657226562
			 train-loss:  2.1845681220293045 	 ± 0.20942925242828314
	data : 0.11572575569152832
	model : 0.06394791603088379
			 train-loss:  2.1839123790356223 	 ± 0.20921789728032641
	data : 0.11577796936035156
	model : 0.06392397880554199
			 train-loss:  2.182456515283666 	 ± 0.20994980545975137
	data : 0.11574010848999024
	model : 0.0639732837677002
			 train-loss:  2.1821956964249307 	 ± 0.20954061451762535
	data : 0.1158329963684082
	model : 0.06402053833007812
			 train-loss:  2.1813470064583473 	 ± 0.20950056307040593
	data : 0.11589221954345703
	model : 0.0640188217163086
			 train-loss:  2.1812882946513374 	 ± 0.20906005719189383
	data : 0.11564292907714843
	model : 0.06403684616088867
			 train-loss:  2.1807620194779727 	 ± 0.20877765503796103
	data : 0.11573567390441894
	model : 0.06398639678955079
			 train-loss:  2.1799974247002702 	 ± 0.2086740718164603
	data : 0.11577119827270507
	model : 0.06393775939941407
			 train-loss:  2.1823756232857705 	 ± 0.2114596280896475
	data : 0.11570448875427246
	model : 0.06394939422607422
			 train-loss:  2.182394608917078 	 ± 0.21102066410294146
	data : 0.11571683883666992
	model : 0.06400384902954101
			 train-loss:  2.1822789606969217 	 ± 0.2105918726478157
	data : 0.11587224006652833
	model : 0.06398959159851074
			 train-loss:  2.182439565167996 	 ± 0.2101729597524755
	data : 0.11554169654846191
	model : 0.06398863792419433
			 train-loss:  2.1820779643097863 	 ± 0.20981756593500003
	data : 0.11552386283874512
	model : 0.063995361328125
			 train-loss:  2.1836883423279745 	 ± 0.21089450684963543
	data : 0.11562523841857911
	model : 0.0639608383178711
			 train-loss:  2.18436564517215 	 ± 0.21073225988729935
	data : 0.1157369613647461
	model : 0.06389827728271484
			 train-loss:  2.1848868816970333 	 ± 0.2104640840300474
	data : 0.11562738418579102
	model : 0.06393971443176269
			 train-loss:  2.1856370226990793 	 ± 0.21036993847726077
	data : 0.11574330329895019
	model : 0.06392192840576172
			 train-loss:  2.1853920259628907 	 ± 0.20998253223148625
	data : 0.11582717895507813
	model : 0.06388130187988281
			 train-loss:  2.1851233878135683 	 ± 0.20960501576617957
	data : 0.1155942440032959
	model : 0.06389374732971191
			 train-loss:  2.186083595591237 	 ± 0.20973727726727276
	data : 0.11551799774169921
	model : 0.06387619972229004
			 train-loss:  2.18573377860917 	 ± 0.2093940744743389
	data : 0.11555209159851074
	model : 0.06384730339050293
			 train-loss:  2.1877680941532724 	 ± 0.21146030684358422
	data : 0.11576590538024903
	model : 0.06384882926940919
			 train-loss:  2.188021358073227 	 ± 0.21108207959939307
	data : 0.11567068099975586
	model : 0.06389989852905273
			 train-loss:  2.1879932641983033 	 ± 0.21066826240453793
	data : 0.11586899757385254
	model : 0.06391849517822265
			 train-loss:  2.1915202741511166 	 ± 0.2176692487352198
	data : 0.11554718017578125
	model : 0.05550985336303711
#epoch  30    val-loss:  2.4163480557893453  train-loss:  2.1915202741511166  lr:  3.90625e-05
			 train-loss:  2.3854687213897705 	 ± 0.0
	data : 4.8448805809021
	model : 0.09195518493652344
			 train-loss:  2.3570488691329956 	 ± 0.028419852256774902
	data : 2.5200966596603394
	model : 0.07904684543609619
			 train-loss:  2.30002760887146 	 ± 0.0839124955642812
	data : 1.855687936147054
	model : 0.07549691200256348
			 train-loss:  2.240768015384674 	 ± 0.12576199125541263
	data : 1.4195903539657593
	model : 0.07312649488449097
			 train-loss:  2.1545255899429323 	 ± 0.20592203995667596
	data : 1.1584476947784423
	model : 0.07148637771606445
			 train-loss:  2.10502690076828 	 ± 0.2181448446941691
	data : 0.21246614456176757
	model : 0.06606860160827636
			 train-loss:  2.1102663619177684 	 ± 0.2023702482479692
	data : 0.19637303352355956
	model : 0.06578497886657715
			 train-loss:  2.0909553915262222 	 ± 0.19607370488210232
	data : 0.11376733779907226
	model : 0.06509251594543457
			 train-loss:  2.1232020457585654 	 ± 0.2061359166048057
	data : 0.11434388160705566
	model : 0.06486001014709472
			 train-loss:  2.124399697780609 	 ± 0.19559070488227762
	data : 0.1145472526550293
	model : 0.06485986709594727
			 train-loss:  2.1064325029199775 	 ± 0.19495161366840202
	data : 0.11450610160827637
	model : 0.06486244201660156
			 train-loss:  2.122662127017975 	 ± 0.19425849200118134
	data : 0.1144345760345459
	model : 0.06491427421569824
			 train-loss:  2.121307318027203 	 ± 0.18669652045076682
	data : 0.11464271545410157
	model : 0.06494307518005371
			 train-loss:  2.144638725689479 	 ± 0.19860139798226004
	data : 0.1147489070892334
	model : 0.0649484634399414
			 train-loss:  2.1315675020217895 	 ± 0.19800255224667765
	data : 0.11465620994567871
	model : 0.06498937606811524
			 train-loss:  2.1322394981980324 	 ± 0.1917328120343495
	data : 0.11456141471862794
	model : 0.0649956226348877
			 train-loss:  2.114544889506172 	 ± 0.19901914117931446
	data : 0.11456184387207032
	model : 0.06499910354614258
			 train-loss:  2.1109125283029346 	 ± 0.1939908208178478
	data : 0.11459155082702636
	model : 0.06496362686157227
			 train-loss:  2.1108142639461316 	 ± 0.18881726008592314
	data : 0.1145263671875
	model : 0.06500043869018554
			 train-loss:  2.12387974858284 	 ± 0.19264680300510192
	data : 0.11463413238525391
	model : 0.06493511199951171
			 train-loss:  2.128582994143168 	 ± 0.1891769683024267
	data : 0.11468439102172852
	model : 0.06493821144104003
			 train-loss:  2.125860197977586 	 ± 0.1852481780081232
	data : 0.11474089622497559
	model : 0.06496844291687012
			 train-loss:  2.139511569686558 	 ± 0.19215818421321082
	data : 0.11468329429626464
	model : 0.0649521827697754
			 train-loss:  2.154282127817472 	 ± 0.20100779010422354
	data : 0.11478452682495117
	model : 0.06489090919494629
			 train-loss:  2.149412627220154 	 ± 0.1983861267349483
	data : 0.11467623710632324
	model : 0.06494779586791992
			 train-loss:  2.1509277683037977 	 ± 0.19468105634103075
	data : 0.11471333503723144
	model : 0.06493606567382812
			 train-loss:  2.1522255252908775 	 ± 0.19115640733239836
	data : 0.114801025390625
	model : 0.0648930549621582
			 train-loss:  2.1581242297376906 	 ± 0.19019779644512244
	data : 0.11491513252258301
	model : 0.06490449905395508
			 train-loss:  2.155814051628113 	 ± 0.18728912104803316
	data : 0.11489505767822265
	model : 0.0649409294128418
			 train-loss:  2.169786512851715 	 ± 0.1989211775408047
	data : 0.1149317741394043
	model : 0.06491169929504395
			 train-loss:  2.1656190541482743 	 ± 0.19701326453037282
	data : 0.11488981246948242
	model : 0.06494660377502441
			 train-loss:  2.1619361601769924 	 ± 0.1949916838585709
	data : 0.1147651195526123
	model : 0.06498255729675292
			 train-loss:  2.1572372660492407 	 ± 0.19384563373743072
	data : 0.11463365554809571
	model : 0.06502952575683593
			 train-loss:  2.157837254159591 	 ± 0.19100478779964072
	data : 0.11456480026245117
	model : 0.06499924659729003
			 train-loss:  2.1635745014463152 	 ± 0.19120566581351675
	data : 0.11454839706420898
	model : 0.06498570442199707
			 train-loss:  2.150943570666843 	 ± 0.20280033639710146
	data : 0.11468019485473632
	model : 0.064996337890625
			 train-loss:  2.15772548881737 	 ± 0.20413771922089158
	data : 0.11478171348571778
	model : 0.06496968269348144
			 train-loss:  2.154637888858193 	 ± 0.20230744540207393
	data : 0.11476449966430664
	model : 0.06497197151184082
			 train-loss:  2.1443431194011984 	 ± 0.20953802848820308
	data : 0.1147308349609375
	model : 0.06502280235290528
			 train-loss:  2.1400333642959595 	 ± 0.20864543206543734
	data : 0.11481289863586426
	model : 0.06503195762634277
			 train-loss:  2.145726674940528 	 ± 0.2092072879197156
	data : 0.11468677520751953
	model : 0.065008544921875
			 train-loss:  2.152959420567467 	 ± 0.21182638754240493
	data : 0.11461677551269531
	model : 0.06495957374572754
			 train-loss:  2.1532013638075007 	 ± 0.20935467248160475
	data : 0.1145594596862793
	model : 0.06491322517395019
			 train-loss:  2.15753450718793 	 ± 0.20890339944400882
	data : 0.11460733413696289
	model : 0.06489782333374024
			 train-loss:  2.1591992060343426 	 ± 0.20686413969995354
	data : 0.11466269493103028
	model : 0.06493067741394043
			 train-loss:  2.1656607337619946 	 ± 0.20914422048224487
	data : 0.11460742950439454
	model : 0.06497049331665039
			 train-loss:  2.170807752203434 	 ± 0.2098315077499157
	data : 0.11456942558288574
	model : 0.06504735946655274
			 train-loss:  2.169954185684522 	 ± 0.20771670232083925
	data : 0.11469831466674804
	model : 0.0651057243347168
			 train-loss:  2.1642555898549607 	 ± 0.20934288829975056
	data : 0.1146583080291748
	model : 0.0651273250579834
			 train-loss:  2.1660982894897463 	 ± 0.20763992279739005
	data : 0.11450996398925781
	model : 0.06508593559265137
			 train-loss:  2.168311128429338 	 ± 0.20618872709364458
	data : 0.11437907218933105
	model : 0.06501421928405762
			 train-loss:  2.169511309036842 	 ± 0.20437632050020427
	data : 0.11446738243103027
	model : 0.06499290466308594
			 train-loss:  2.1795335670687117 	 ± 0.21495289558222355
	data : 0.11453719139099121
	model : 0.06494226455688476
			 train-loss:  2.176520727298878 	 ± 0.21407988143372858
	data : 0.11471285820007324
	model : 0.06494083404541015
			 train-loss:  2.175971508026123 	 ± 0.21216316364564541
	data : 0.11474142074584961
	model : 0.0649843692779541
			 train-loss:  2.17604997754097 	 ± 0.21026112190043905
	data : 0.11491975784301758
	model : 0.06505537033081055
			 train-loss:  2.1757627244581257 	 ± 0.20841965051946612
	data : 0.11479606628417968
	model : 0.06508054733276367
			 train-loss:  2.1797052704054733 	 ± 0.20874816370894286
	data : 0.11470584869384766
	model : 0.06509485244750976
			 train-loss:  2.1824603565668657 	 ± 0.20803238215887332
	data : 0.11462955474853516
	model : 0.06504197120666504
			 train-loss:  2.180654537677765 	 ± 0.20675729432357234
	data : 0.1145824909210205
	model : 0.06497960090637207
			 train-loss:  2.1756816535699564 	 ± 0.2086421744612934
	data : 0.11459259986877442
	model : 0.06494512557983398
			 train-loss:  2.169368065172626 	 ± 0.21274628171351595
	data : 0.11474742889404296
	model : 0.06493172645568848
			 train-loss:  2.16788010937827 	 ± 0.2113760177847281
	data : 0.1147369384765625
	model : 0.06493191719055176
			 train-loss:  2.164963759481907 	 ± 0.2109917528869023
	data : 0.11465668678283691
	model : 0.0650062084197998
			 train-loss:  2.1638427404256966 	 ± 0.20955443833252335
	data : 0.11475000381469727
	model : 0.06509871482849121
			 train-loss:  2.1676988023700137 	 ± 0.21027175769191142
	data : 0.1148838996887207
	model : 0.06509757041931152
			 train-loss:  2.1695646527987806 	 ± 0.2092464353960567
	data : 0.11472620964050292
	model : 0.06509671211242676
			 train-loss:  2.165925160926931 	 ± 0.20982769766081835
	data : 0.1147153377532959
	model : 0.06508884429931641
			 train-loss:  2.1712512849033745 	 ± 0.21288160373101347
	data : 0.1146761417388916
	model : 0.06500225067138672
			 train-loss:  2.177145847252437 	 ± 0.2169530678881378
	data : 0.11453943252563477
	model : 0.06494727134704589
			 train-loss:  2.1779603773439433 	 ± 0.21552757900703498
	data : 0.11441702842712402
	model : 0.06496353149414062
			 train-loss:  2.178240641951561 	 ± 0.21403865460382898
	data : 0.11453232765197754
	model : 0.06496958732604981
			 train-loss:  2.1763376258823968 	 ± 0.21318002344827613
	data : 0.11454787254333496
	model : 0.06502242088317871
			 train-loss:  2.1780056164071366 	 ± 0.21221378621139153
	data : 0.11469869613647461
	model : 0.06508188247680664
			 train-loss:  2.178759446144104 	 ± 0.21089400109269288
	data : 0.11488986015319824
	model : 0.06508560180664062
			 train-loss:  2.1805111718805215 	 ± 0.21005048493045375
	data : 0.114829683303833
	model : 0.06507925987243653
			 train-loss:  2.1824912049553613 	 ± 0.20939475613212033
	data : 0.11467561721801758
	model : 0.06500892639160157
			 train-loss:  2.1816964409290214 	 ± 0.20816500817454478
	data : 0.11475491523742676
	model : 0.06494531631469727
			 train-loss:  2.177924199949337 	 ± 0.20950914148770644
	data : 0.11476588249206543
	model : 0.06492905616760254
			 train-loss:  2.1859114304184915 	 ± 0.2199665350846647
	data : 0.11470928192138671
	model : 0.06492838859558106
			 train-loss:  2.1846761188389343 	 ± 0.21888354675430008
	data : 0.11469225883483887
	model : 0.06496939659118653
			 train-loss:  2.1876796207776885 	 ± 0.21921779489081747
	data : 0.11491060256958008
	model : 0.06503996849060059
			 train-loss:  2.189474891467267 	 ± 0.21849881937014778
	data : 0.11481151580810547
	model : 0.06506471633911133
			 train-loss:  2.1856080251080647 	 ± 0.22003283957328834
	data : 0.11475653648376465
	model : 0.0650754451751709
			 train-loss:  2.1864846047233133 	 ± 0.21888219121994096
	data : 0.11460943222045898
	model : 0.06507511138916015
			 train-loss:  2.1865121777667555 	 ± 0.21760604782814905
	data : 0.11479454040527344
	model : 0.06500616073608398
			 train-loss:  2.1858586155135056 	 ± 0.2164367021422269
	data : 0.11477675437927246
	model : 0.0649653434753418
			 train-loss:  2.1859893568537454 	 ± 0.2152068896557217
	data : 0.11485939025878907
	model : 0.06496920585632324
			 train-loss:  2.1842511410123846 	 ± 0.2146147850732285
	data : 0.11493444442749023
	model : 0.0649836540222168
			 train-loss:  2.1860339522361754 	 ± 0.21408085550782216
	data : 0.11514143943786621
	model : 0.06500096321105957
			 train-loss:  2.184705436884702 	 ± 0.21327406162480728
	data : 0.11505169868469238
	model : 0.06504840850830078
			 train-loss:  2.186581719180812 	 ± 0.21286562386839483
	data : 0.11496162414550781
	model : 0.06508827209472656
			 train-loss:  2.1880958631474483 	 ± 0.21221562758189227
	data : 0.11485962867736817
	model : 0.06507267951965331
			 train-loss:  2.188944660602732 	 ± 0.21124245390051571
	data : 0.11478643417358399
	model : 0.06504759788513184
			 train-loss:  2.1979587291416367 	 ± 0.22757740918814015
	data : 0.11461162567138672
	model : 0.06496748924255372
			 train-loss:  2.1969474516808987 	 ± 0.2266034805969443
	data : 0.11459832191467285
	model : 0.06495223045349122
			 train-loss:  2.1999529550984964 	 ± 0.2273476143355991
	data : 0.11461634635925293
	model : 0.06492795944213867
			 train-loss:  2.201740241780573 	 ± 0.22686863203040408
	data : 0.11475496292114258
	model : 0.0649381160736084
			 train-loss:  2.2022403478622437 	 ± 0.22577421001427764
	data : 0.11476411819458007
	model : 0.06497030258178711
			 train-loss:  2.2017237174510957 	 ± 0.22470130787266462
	data : 0.11481890678405762
	model : 0.06505513191223145
			 train-loss:  2.2043429542296002 	 ± 0.22511510431173265
	data : 0.11485171318054199
	model : 0.06506247520446777
			 train-loss:  2.204659187326244 	 ± 0.22403142426309539
	data : 0.11491446495056153
	model : 0.06508336067199708
			 train-loss:  2.207462073529808 	 ± 0.2247312309972939
	data : 0.11476364135742187
	model : 0.06508665084838867
			 train-loss:  2.2043924434826923 	 ± 0.22580752940527718
	data : 0.11471791267395019
	model : 0.06503305435180665
			 train-loss:  2.2046958367029825 	 ± 0.22475098078217232
	data : 0.11486997604370117
	model : 0.06506328582763672
			 train-loss:  2.203095923054893 	 ± 0.22428828899770537
	data : 0.11480174064636231
	model : 0.0650367259979248
			 train-loss:  2.2015612136537785 	 ± 0.22379624544621454
	data : 0.11477646827697754
	model : 0.06502785682678222
			 train-loss:  2.2031930607778056 	 ± 0.22339638466584666
	data : 0.1149141788482666
	model : 0.0650456428527832
			 train-loss:  2.202286340774746 	 ± 0.2225688280599393
	data : 0.11493310928344727
	model : 0.06509151458740234
			 train-loss:  2.2011642011729156 	 ± 0.2218643739452379
	data : 0.11481986045837403
	model : 0.06505556106567383
			 train-loss:  2.2043992216522628 	 ± 0.2234536464007997
	data : 0.11480727195739746
	model : 0.0650482177734375
			 train-loss:  2.2057821229100227 	 ± 0.22293046674391306
	data : 0.11471872329711914
	model : 0.06502509117126465
			 train-loss:  2.206764026025755 	 ± 0.22218499214046958
	data : 0.11446976661682129
	model : 0.06495194435119629
			 train-loss:  2.206583850216447 	 ± 0.22121664155868434
	data : 0.1145359992980957
	model : 0.06494550704956055
			 train-loss:  2.209435526184414 	 ± 0.22234729468476522
	data : 0.11461296081542968
	model : 0.06491093635559082
			 train-loss:  2.2055668080675193 	 ± 0.22524060408676208
	data : 0.11473255157470703
	model : 0.06498074531555176
			 train-loss:  2.2066478800569844 	 ± 0.2245780101825144
	data : 0.11482582092285157
	model : 0.06504020690917969
			 train-loss:  2.2080422609539356 	 ± 0.22413243380629655
	data : 0.11494197845458984
	model : 0.06510400772094727
			 train-loss:  2.2066987111788836 	 ± 0.22366539064228413
	data : 0.11490964889526367
	model : 0.06511950492858887
			 train-loss:  2.2053879072268803 	 ± 0.22319002788239095
	data : 0.11487627029418945
	model : 0.06515364646911621
			 train-loss:  2.204043081969269 	 ± 0.2227535206009164
	data : 0.11482377052307129
	model : 0.06509270668029785
			 train-loss:  2.205298207822393 	 ± 0.2222679306644081
	data : 0.11467337608337402
	model : 0.0649869441986084
			 train-loss:  2.2043315462949797 	 ± 0.22161990735733397
	data : 0.11471858024597167
	model : 0.06497435569763184
			 train-loss:  2.2063681800519266 	 ± 0.22187717416250788
	data : 0.11478137969970703
	model : 0.06492137908935547
			 train-loss:  2.208363112449646 	 ± 0.2221016303962838
	data : 0.11487083435058594
	model : 0.06487669944763183
			 train-loss:  2.2084866544556996 	 ± 0.22122283107269836
	data : 0.11485228538513184
	model : 0.06494083404541015
			 train-loss:  2.2072702383431864 	 ± 0.2207727982777467
	data : 0.11497325897216797
	model : 0.06503725051879883
			 train-loss:  2.207047733478248 	 ± 0.21992300895595868
	data : 0.11493978500366211
	model : 0.06502447128295899
			 train-loss:  2.205498825672061 	 ± 0.21976870887171043
	data : 0.11487956047058105
	model : 0.06503963470458984
			 train-loss:  2.2067969496433553 	 ± 0.21941773255783822
	data : 0.11471037864685059
	model : 0.06507024765014649
			 train-loss:  2.205726151247971 	 ± 0.2189193642138894
	data : 0.11459465026855468
	model : 0.06497116088867187
			 train-loss:  2.204173777139548 	 ± 0.2188111217407862
	data : 0.1145787239074707
	model : 0.06493940353393554
			 train-loss:  2.206699153534452 	 ± 0.21990942033107003
	data : 0.1146425724029541
	model : 0.06492753028869629
			 train-loss:  2.2075683136484514 	 ± 0.21931650624712976
	data : 0.1146812915802002
	model : 0.06493191719055176
			 train-loss:  2.2064296378029717 	 ± 0.21889992621301674
	data : 0.1146359920501709
	model : 0.06498613357543945
			 train-loss:  2.204722844502505 	 ± 0.21899342424406812
	data : 0.11483845710754395
	model : 0.06506547927856446
			 train-loss:  2.2052604038349903 	 ± 0.21828275359717217
	data : 0.11483097076416016
	model : 0.06506919860839844
			 train-loss:  2.2042621149533037 	 ± 0.21780408936537607
	data : 0.11484203338623047
	model : 0.06510810852050782
			 train-loss:  2.2070499958751872 	 ± 0.21947644735463848
	data : 0.11471076011657715
	model : 0.06515011787414551
			 train-loss:  2.205601106371198 	 ± 0.21935733446008426
	data : 0.11472415924072266
	model : 0.06504855155944825
			 train-loss:  2.2065958790745297 	 ± 0.2188947703906411
	data : 0.11467657089233399
	model : 0.06498045921325683
			 train-loss:  2.208323809462534 	 ± 0.21908555705425053
	data : 0.11472964286804199
	model : 0.06497783660888672
			 train-loss:  2.20899702785732 	 ± 0.21846552419391835
	data : 0.11478734016418457
	model : 0.06499681472778321
			 train-loss:  2.208062301079432 	 ± 0.21799240261563385
	data : 0.11488046646118164
	model : 0.06497979164123535
			 train-loss:  2.206384168000057 	 ± 0.2181707623151438
	data : 0.11493134498596191
	model : 0.06509251594543457
			 train-loss:  2.209099337662736 	 ± 0.21986683639303173
	data : 0.11501116752624511
	model : 0.06512589454650879
			 train-loss:  2.208978061773339 	 ± 0.21912261374388842
	data : 0.11495018005371094
	model : 0.06514649391174317
			 train-loss:  2.208617058154699 	 ± 0.21842493823329565
	data : 0.1148064136505127
	model : 0.06509337425231934
			 train-loss:  2.206514430526119 	 ± 0.21918843564966914
	data : 0.11476917266845703
	model : 0.06511106491088867
			 train-loss:  2.20652942498525 	 ± 0.21845666241607065
	data : 0.11486024856567383
	model : 0.06501874923706055
			 train-loss:  2.2062319351347868 	 ± 0.2177625770256005
	data : 0.11479420661926269
	model : 0.06502175331115723
			 train-loss:  2.203326402526153 	 ± 0.21996209400033878
	data : 0.11488609313964844
	model : 0.0650515079498291
			 train-loss:  2.2039286840974897 	 ± 0.2193677936227596
	data : 0.11487226486206055
	model : 0.06507534980773926
			 train-loss:  2.206577996154884 	 ± 0.22109643052967035
	data : 0.11482877731323242
	model : 0.06503710746765137
			 train-loss:  2.2051323552285473 	 ± 0.22111104560504521
	data : 0.11478128433227539
	model : 0.06505866050720215
			 train-loss:  2.204717897451841 	 ± 0.22046161028738603
	data : 0.11479501724243164
	model : 0.06504077911376953
			 train-loss:  2.20563859089165 	 ± 0.22005904624350656
	data : 0.11465249061584473
	model : 0.06498785018920898
			 train-loss:  2.204848370974577 	 ± 0.21958490049467616
	data : 0.11479496955871582
	model : 0.06499052047729492
			 train-loss:  2.2043070583223545 	 ± 0.21899901984276002
	data : 0.11488866806030273
	model : 0.06500082015991211
			 train-loss:  2.2028659887611868 	 ± 0.2190685040817231
	data : 0.11503000259399414
	model : 0.06504702568054199
			 train-loss:  2.2013293578758004 	 ± 0.21925037330149355
	data : 0.11510162353515625
	model : 0.06505427360534669
			 train-loss:  2.200491103861067 	 ± 0.21883126639449518
	data : 0.1152984619140625
	model : 0.06508517265319824
			 train-loss:  2.1992492756229236 	 ± 0.21873080115668758
	data : 0.11521105766296387
	model : 0.0651008129119873
			 train-loss:  2.197574297102486 	 ± 0.21910897269364282
	data : 0.11519832611083984
	model : 0.06512923240661621
			 train-loss:  2.1951925414981264 	 ± 0.22056317109416004
	data : 0.11509056091308593
	model : 0.0650984764099121
			 train-loss:  2.1941648527800317 	 ± 0.22029370236457968
	data : 0.1150909423828125
	model : 0.06511759757995605
			 train-loss:  2.193542304153214 	 ± 0.21977956385134365
	data : 0.11488971710205079
	model : 0.0651252269744873
			 train-loss:  2.191245597742853 	 ± 0.22112539467038628
	data : 0.1148681640625
	model : 0.06508450508117676
			 train-loss:  2.1905600735421715 	 ± 0.22064918448975696
	data : 0.11489953994750976
	model : 0.0650559425354004
			 train-loss:  2.1911116761319778 	 ± 0.22011609406823923
	data : 0.11494107246398926
	model : 0.06505556106567383
			 train-loss:  2.1909420817916154 	 ± 0.21948267519928924
	data : 0.11479010581970214
	model : 0.06506428718566895
			 train-loss:  2.19293403694796 	 ± 0.22038847426950728
	data : 0.11475458145141601
	model : 0.06502375602722169
			 train-loss:  2.191390872690719 	 ± 0.2206805726030618
	data : 0.11485104560852051
	model : 0.06531205177307128
			 train-loss:  2.191217512919985 	 ± 0.2200573330504656
	data : 0.11466293334960938
	model : 0.0653005599975586
			 train-loss:  2.1924711390904017 	 ± 0.22004992365900927
	data : 0.11453819274902344
	model : 0.06529879570007324
			 train-loss:  2.1924488354812968 	 ± 0.2194240896823966
	data : 0.11453356742858886
	model : 0.06528477668762207
			 train-loss:  2.191154793830915 	 ± 0.2194758170742961
	data : 0.11466546058654785
	model : 0.06532855033874511
			 train-loss:  2.1903595964560347 	 ± 0.21911399393150116
	data : 0.11464114189147949
	model : 0.06504583358764648
			 train-loss:  2.1914063565557895 	 ± 0.21894693552412833
	data : 0.11487531661987305
	model : 0.0650402545928955
			 train-loss:  2.1918727411164176 	 ± 0.21842704683549027
	data : 0.11499309539794922
	model : 0.06504573822021484
			 train-loss:  2.190640266428995 	 ± 0.2184495375420108
	data : 0.11507577896118164
	model : 0.06506533622741699
			 train-loss:  2.190202221765623 	 ± 0.2179282735092922
	data : 0.11504044532775878
	model : 0.06507105827331543
			 train-loss:  2.189401647432254 	 ± 0.21760022190432626
	data : 0.11501564979553222
	model : 0.0650792121887207
			 train-loss:  2.188257338560146 	 ± 0.21755952846803045
	data : 0.1149775505065918
	model : 0.06528072357177735
			 train-loss:  2.189057346936819 	 ± 0.21724194218055395
	data : 0.11479306221008301
	model : 0.0652885913848877
			 train-loss:  2.1879801910410643 	 ± 0.2171519719497
	data : 0.11468234062194824
	model : 0.06523852348327637
			 train-loss:  2.188155707828501 	 ± 0.21658380179851341
	data : 0.11469912528991699
	model : 0.06518778800964356
			 train-loss:  2.1877768642090736 	 ± 0.21606912870440945
	data : 0.11478147506713868
	model : 0.06525168418884278
			 train-loss:  2.1868607449153115 	 ± 0.21586254149302894
	data : 0.11482319831848145
	model : 0.06512694358825684
			 train-loss:  2.1862726882884376 	 ± 0.2154454677304785
	data : 0.1151078224182129
	model : 0.06516618728637695
			 train-loss:  2.187316304726126 	 ± 0.21536170884231892
	data : 0.11530723571777343
	model : 0.06521129608154297
			 train-loss:  2.1866112084438405 	 ± 0.2150210626315137
	data : 0.11531314849853516
	model : 0.0652653694152832
			 train-loss:  2.1884351820525727 	 ± 0.21594736298697226
	data : 0.11526975631713868
	model : 0.06524448394775391
			 train-loss:  2.1901375122906006 	 ± 0.2166845312519746
	data : 0.11521077156066895
	model : 0.06518206596374512
			 train-loss:  2.189014888421083 	 ± 0.2166931028131716
	data : 0.11504683494567872
	model : 0.06516704559326172
			 train-loss:  2.1898648288785196 	 ± 0.21646523426787864
	data : 0.11481513977050781
	model : 0.06513371467590331
			 train-loss:  2.1895153812950636 	 ± 0.21597054919536882
	data : 0.11485071182250976
	model : 0.06509556770324706
			 train-loss:  2.188003654431815 	 ± 0.21646688817350251
	data : 0.11490302085876465
	model : 0.06506552696228027
			 train-loss:  2.1879515432233188 	 ± 0.2159235616070689
	data : 0.11500606536865235
	model : 0.06506104469299316
			 train-loss:  2.187644053697586 	 ± 0.21542675076850343
	data : 0.11516342163085938
	model : 0.0650254249572754
			 train-loss:  2.1888038151299773 	 ± 0.2155152089362912
	data : 0.11525607109069824
	model : 0.06522912979125976
			 train-loss:  2.187220245894819 	 ± 0.21615021767110415
	data : 0.11499857902526855
	model : 0.06540617942810059
			 train-loss:  2.1886706252403445 	 ± 0.21660030367112695
	data : 0.11481709480285644
	model : 0.06539549827575683
			 train-loss:  2.1868724624315896 	 ± 0.21758237689687449
	data : 0.11467475891113281
	model : 0.06598982810974122
			 train-loss:  2.1877262313191483 	 ± 0.2173933155734984
	data : 0.1140334129333496
	model : 0.0660168170928955
			 train-loss:  2.1891037410902747 	 ± 0.21776003120516732
	data : 0.1140439510345459
	model : 0.06583304405212402
			 train-loss:  2.188006321013262 	 ± 0.2178036821607533
	data : 0.11420135498046875
	model : 0.06567139625549316
			 train-loss:  2.1883491077102146 	 ± 0.21733544944074937
	data : 0.11427035331726074
	model : 0.06565136909484863
			 train-loss:  2.187790425200211 	 ± 0.21696455121624014
	data : 0.11435017585754395
	model : 0.06517772674560547
			 train-loss:  2.1870177410897753 	 ± 0.21673540993451385
	data : 0.11483335494995117
	model : 0.06513161659240722
			 train-loss:  2.1878520871790665 	 ± 0.21655899707191664
	data : 0.11485061645507813
	model : 0.06513333320617676
			 train-loss:  2.188319186557014 	 ± 0.21615415689887915
	data : 0.11490426063537598
	model : 0.06526713371276856
			 train-loss:  2.1901956684712514 	 ± 0.21737008904335076
	data : 0.11484789848327637
	model : 0.06529016494750976
			 train-loss:  2.189755736667419 	 ± 0.21695664631095718
	data : 0.11481823921203613
	model : 0.06514387130737305
			 train-loss:  2.1879149259522905 	 ± 0.218120178511972
	data : 0.11472930908203124
	model : 0.06521081924438477
			 train-loss:  2.187654815338276 	 ± 0.21764810469065965
	data : 0.11475234031677246
	model : 0.06517601013183594
			 train-loss:  2.1873738545975927 	 ± 0.21718528988159902
	data : 0.11480851173400879
	model : 0.06502270698547363
			 train-loss:  2.1878219821037503 	 ± 0.21678711695868733
	data : 0.11492242813110351
	model : 0.06496152877807618
			 train-loss:  2.185880912493353 	 ± 0.21818209654771223
	data : 0.1150285243988037
	model : 0.06498298645019532
			 train-loss:  2.187777988477187 	 ± 0.21948851119211818
	data : 0.11517863273620606
	model : 0.06486048698425292
			 train-loss:  2.187052872385914 	 ± 0.2192553162901186
	data : 0.11529350280761719
	model : 0.06482734680175781
			 train-loss:  2.186196358891221 	 ± 0.21913118936195206
	data : 0.11526279449462891
	model : 0.06474332809448242
			 train-loss:  2.1852066826927286 	 ± 0.21913600467707986
	data : 0.11530332565307617
	model : 0.06474037170410156
			 train-loss:  2.186252656791891 	 ± 0.2192035280359122
	data : 0.11532316207885743
	model : 0.06457037925720215
			 train-loss:  2.1858056365119087 	 ± 0.21881817016720273
	data : 0.11530656814575195
	model : 0.06443476676940918
			 train-loss:  2.1855364478794876 	 ± 0.21837085679573268
	data : 0.11529598236083985
	model : 0.0642399787902832
			 train-loss:  2.1850246227785353 	 ± 0.21802514862340966
	data : 0.11540279388427735
	model : 0.06408886909484864
			 train-loss:  2.1848892318575004 	 ± 0.21755606130606361
	data : 0.11564183235168457
	model : 0.06390104293823243
			 train-loss:  2.1851251052456653 	 ± 0.2171097438176404
	data : 0.11573381423950195
	model : 0.06385684013366699
			 train-loss:  2.1846242427825926 	 ± 0.21676980098288579
	data : 0.11578302383422852
	model : 0.0638310432434082
			 train-loss:  2.1855628934257476 	 ± 0.2167680219864301
	data : 0.1156923770904541
	model : 0.06385598182678223
			 train-loss:  2.1861622960403047 	 ± 0.21649211011588745
	data : 0.1157979965209961
	model : 0.06385469436645508
			 train-loss:  2.1869318710376264 	 ± 0.21634481955085544
	data : 0.11560349464416504
	model : 0.06385364532470703
			 train-loss:  2.1874693275516868 	 ± 0.21603787514599274
	data : 0.11556344032287598
	model : 0.06382265090942382
			 train-loss:  2.1886677336185536 	 ± 0.2163557790703457
	data : 0.11556954383850097
	model : 0.06379709243774415
			 train-loss:  2.1878766154838822 	 ± 0.21623726641385035
	data : 0.11580238342285157
	model : 0.0638082504272461
			 train-loss:  2.189517580507174 	 ± 0.21724814090919622
	data : 0.1157526969909668
	model : 0.06380648612976074
			 train-loss:  2.1886544903787244 	 ± 0.21719805744852866
	data : 0.11572957038879395
	model : 0.06384501457214356
			 train-loss:  2.1892574716312616 	 ± 0.21694272239299053
	data : 0.11578803062438965
	model : 0.06386818885803222
			 train-loss:  2.1897792771458624 	 ± 0.21664053026562935
	data : 0.1160761833190918
	model : 0.06388282775878906
			 train-loss:  2.189798783959195 	 ± 0.21619081257935988
	data : 0.11588282585144043
	model : 0.06388254165649414
			 train-loss:  2.1914268421732688 	 ± 0.21721906340351707
	data : 0.11595463752746582
	model : 0.06386322975158691
			 train-loss:  2.1919891701804266 	 ± 0.2169480851747213
	data : 0.11609234809875488
	model : 0.06383614540100098
			 train-loss:  2.1923550772862357 	 ± 0.21657818697128334
	data : 0.11611499786376953
	model : 0.06383476257324219
			 train-loss:  2.1931798881413984 	 ± 0.21651940771258218
	data : 0.115940523147583
	model : 0.06384739875793458
			 train-loss:  2.1932430310947137 	 ± 0.21608113979543878
	data : 0.1160362720489502
	model : 0.06384043693542481
			 train-loss:  2.1928826811825215 	 ± 0.21571733809416666
	data : 0.11596918106079102
	model : 0.06387004852294922
			 train-loss:  2.192226116215029 	 ± 0.21552913755516975
	data : 0.11590633392333985
	model : 0.06390085220336914
			 train-loss:  2.192856664638443 	 ± 0.21532499695763926
	data : 0.11575026512145996
	model : 0.06387991905212402
			 train-loss:  2.1924674372673034 	 ± 0.2149816687213416
	data : 0.11578865051269531
	model : 0.06386299133300781
			 train-loss:  2.1930265925320023 	 ± 0.21473506838649561
	data : 0.11578497886657715
	model : 0.06383366584777832
			 train-loss:  2.1930625150128016 	 ± 0.21430933889864287
	data : 0.1157233715057373
	model : 0.06382503509521484
			 train-loss:  2.192001347014084 	 ± 0.21454773065723223
	data : 0.11563324928283691
	model : 0.06385540962219238
			 train-loss:  2.191705246140638 	 ± 0.21417676659177254
	data : 0.11559104919433594
	model : 0.06389565467834472
			 train-loss:  2.1911654336779725 	 ± 0.2139294587975372
	data : 0.11551661491394043
	model : 0.06394028663635254
			 train-loss:  2.191790719050914 	 ± 0.21374456950162268
	data : 0.11522383689880371
	model : 0.055521154403686525
#epoch  31    val-loss:  2.4408773497531286  train-loss:  2.191790719050914  lr:  3.90625e-05
			 train-loss:  2.8928864002227783 	 ± 0.0
	data : 5.717116594314575
	model : 0.07805609703063965
			 train-loss:  2.5344786643981934 	 ± 0.35840773582458496
	data : 2.922168493270874
	model : 0.07321786880493164
			 train-loss:  2.368316968282064 	 ± 0.37530896860006996
	data : 1.9864472548166912
	model : 0.07034913698832194
			 train-loss:  2.2556733191013336 	 ± 0.37908889661396106
	data : 1.5185064673423767
	model : 0.06895327568054199
			 train-loss:  2.2131304502487184 	 ± 0.34958017104755196
	data : 1.237729549407959
	model : 0.06811056137084961
			 train-loss:  2.270141899585724 	 ± 0.343642411190605
	data : 0.11731033325195313
	model : 0.06544308662414551
			 train-loss:  2.218767932483128 	 ± 0.34213418199786094
	data : 0.11480660438537597
	model : 0.06477847099304199
			 train-loss:  2.210197016596794 	 ± 0.32083959835693837
	data : 0.11481962203979493
	model : 0.06484079360961914
			 train-loss:  2.1596945656670465 	 ± 0.33452125104133945
	data : 0.11479396820068359
	model : 0.06484808921813964
			 train-loss:  2.1607342958450317 	 ± 0.3173700521599725
	data : 0.11475114822387696
	model : 0.06486611366271973
			 train-loss:  2.141695737838745 	 ± 0.3085315419392758
	data : 0.11462249755859374
	model : 0.06488933563232421
			 train-loss:  2.1383939186731973 	 ± 0.2955993771671888
	data : 0.11454577445983886
	model : 0.06484637260437012
			 train-loss:  2.124761260472811 	 ± 0.28790228083614305
	data : 0.11456494331359864
	model : 0.06493468284606933
			 train-loss:  2.1174314277512685 	 ± 0.2786855135561267
	data : 0.11469101905822754
	model : 0.0651625633239746
			 train-loss:  2.131587354342143 	 ± 0.2743963763888892
	data : 0.11457319259643554
	model : 0.06523499488830567
			 train-loss:  2.1208973601460457 	 ± 0.2688897084365302
	data : 0.11461639404296875
	model : 0.06531176567077637
			 train-loss:  2.1381526764701393 	 ± 0.2698380452094999
	data : 0.11467676162719727
	model : 0.06532707214355468
			 train-loss:  2.1393344865904913 	 ± 0.2622807108933907
	data : 0.11456766128540039
	model : 0.06524715423583985
			 train-loss:  2.1521789713909754 	 ± 0.2610368509606394
	data : 0.114404296875
	model : 0.0650184154510498
			 train-loss:  2.1817601382732392 	 ± 0.2852351467293002
	data : 0.11450333595275879
	model : 0.06495671272277832
			 train-loss:  2.1935442231950306 	 ± 0.2833057354671455
	data : 0.11444659233093261
	model : 0.06488652229309082
			 train-loss:  2.192259143699299 	 ± 0.2768547284852051
	data : 0.11454939842224121
	model : 0.06489067077636719
			 train-loss:  2.1851998049279917 	 ± 0.27278627026116486
	data : 0.11460795402526855
	model : 0.06491875648498535
			 train-loss:  2.1899666637182236 	 ± 0.2680195186435196
	data : 0.11461472511291504
	model : 0.06495342254638672
			 train-loss:  2.1910629510879516 	 ± 0.26265933865537006
	data : 0.11467370986938477
	model : 0.06497025489807129
			 train-loss:  2.1868175038924584 	 ± 0.25843193485684957
	data : 0.11467933654785156
	model : 0.0649296760559082
			 train-loss:  2.1927877223050154 	 ± 0.2554216145137261
	data : 0.11460289955139161
	model : 0.06494078636169434
			 train-loss:  2.196135925395148 	 ± 0.2514217104839381
	data : 0.11465516090393066
	model : 0.06491026878356934
			 train-loss:  2.1901781189030616 	 ± 0.24905219326484238
	data : 0.11467947959899902
	model : 0.06492843627929687
			 train-loss:  2.1984217286109926 	 ± 0.2488577593737265
	data : 0.11477360725402833
	model : 0.06493539810180664
			 train-loss:  2.2008025838482763 	 ± 0.24515809321017976
	data : 0.11483368873596192
	model : 0.06500067710876464
			 train-loss:  2.190692737698555 	 ± 0.24777563741705266
	data : 0.11473579406738281
	model : 0.0650524616241455
			 train-loss:  2.1864668600486987 	 ± 0.24516083899154478
	data : 0.11461920738220215
	model : 0.06508712768554688
			 train-loss:  2.1902908086776733 	 ± 0.2425255085694489
	data : 0.11461086273193359
	model : 0.06500635147094727
			 train-loss:  2.1865229674748012 	 ± 0.24004327853572852
	data : 0.11485676765441895
	model : 0.0650094985961914
			 train-loss:  2.1838435199525623 	 ± 0.23721610155481604
	data : 0.1148843765258789
	model : 0.06499533653259278
			 train-loss:  2.180636605700931 	 ± 0.2347783269855065
	data : 0.11504530906677246
	model : 0.06496896743774414
			 train-loss:  2.1751453343190645 	 ± 0.234064123846187
	data : 0.11512489318847656
	model : 0.06498489379882813
			 train-loss:  2.1754353932845287 	 ± 0.23105073377946106
	data : 0.11520915031433106
	model : 0.06506557464599609
			 train-loss:  2.177326795458794 	 ± 0.2284498834005709
	data : 0.11484127044677735
	model : 0.06505670547485351
			 train-loss:  2.1790830652888227 	 ± 0.2259199362996636
	data : 0.11474947929382324
	model : 0.06508851051330566
			 train-loss:  2.184677223364512 	 ± 0.22607003926316907
	data : 0.11446590423583984
	model : 0.0650357723236084
			 train-loss:  2.1836894140687098 	 ± 0.2235175487088484
	data : 0.11444301605224609
	model : 0.06502976417541503
			 train-loss:  2.1912746727466583 	 ± 0.22649214622561148
	data : 0.11452364921569824
	model : 0.06500487327575684
			 train-loss:  2.1939421309365166 	 ± 0.2246592887516698
	data : 0.11454715728759765
	model : 0.06501288414001465
			 train-loss:  2.188268588936847 	 ± 0.22543977380308902
	data : 0.11463723182678223
	model : 0.06503987312316895
			 train-loss:  2.1906314301998058 	 ± 0.2236035959103017
	data : 0.1148827075958252
	model : 0.0650914192199707
			 train-loss:  2.19147419432799 	 ± 0.2213375546155184
	data : 0.11482291221618653
	model : 0.06503067016601563
			 train-loss:  2.195926096974587 	 ± 0.22122803565494778
	data : 0.11476421356201172
	model : 0.06503329277038575
			 train-loss:  2.1893280720710755 	 ± 0.22382173617254528
	data : 0.1147082805633545
	model : 0.06506290435791015
			 train-loss:  2.186669440830455 	 ± 0.2224124721143265
	data : 0.11461834907531739
	model : 0.0649935245513916
			 train-loss:  2.1806105031416965 	 ± 0.2244732955028035
	data : 0.11457033157348633
	model : 0.06502747535705566
			 train-loss:  2.1900194613438733 	 ± 0.23246724545853883
	data : 0.11467885971069336
	model : 0.0650324821472168
			 train-loss:  2.1910660951225847 	 ± 0.2304307249862292
	data : 0.11456890106201172
	model : 0.06502327919006348
			 train-loss:  2.1911939989436755 	 ± 0.22832822524265778
	data : 0.11473293304443359
	model : 0.06500759124755859
			 train-loss:  2.194667358483587 	 ± 0.22774185072198372
	data : 0.11475415229797363
	model : 0.06503381729125976
			 train-loss:  2.192808479593511 	 ± 0.22616347785644678
	data : 0.11475214958190919
	model : 0.0649709701538086
			 train-loss:  2.194522399326851 	 ± 0.22457841003629075
	data : 0.11468386650085449
	model : 0.06498446464538574
			 train-loss:  2.1961954953306813 	 ± 0.22303134404145736
	data : 0.11471390724182129
	model : 0.06494512557983398
			 train-loss:  2.1983345131079357 	 ± 0.22177438801002952
	data : 0.11468815803527832
	model : 0.06491527557373047
			 train-loss:  2.1978432174588813 	 ± 0.2199819728087826
	data : 0.1148144245147705
	model : 0.06491093635559082
			 train-loss:  2.2041839034326616 	 ± 0.2237498901562213
	data : 0.11496706008911133
	model : 0.06489052772521972
			 train-loss:  2.205714291996426 	 ± 0.22229385055396508
	data : 0.11491584777832031
	model : 0.06491060256958008
			 train-loss:  2.2091601695865393 	 ± 0.22223978067790073
	data : 0.11496052742004395
	model : 0.06496491432189941
			 train-loss:  2.2105193890058077 	 ± 0.22079154086068645
	data : 0.11483378410339355
	model : 0.06497197151184082
			 train-loss:  2.208963190064286 	 ± 0.21947140778404683
	data : 0.1147310733795166
	model : 0.06497220993041992
			 train-loss:  2.2061649454173757 	 ± 0.2190104321292544
	data : 0.11451492309570313
	model : 0.06494803428649902
			 train-loss:  2.2016755868406857 	 ± 0.2204779675385823
	data : 0.11454849243164063
	model : 0.06494941711425781
			 train-loss:  2.208287491314653 	 ± 0.2255633143205754
	data : 0.11460719108581544
	model : 0.06493768692016602
			 train-loss:  2.2073283467973983 	 ± 0.22408803126936389
	data : 0.11466407775878906
	model : 0.06499238014221191
			 train-loss:  2.2083723545074463 	 ± 0.22267573420935036
	data : 0.1146552562713623
	model : 0.06498980522155762
			 train-loss:  2.2101060483190746 	 ± 0.22160598711825458
	data : 0.11474013328552246
	model : 0.06502285003662109
			 train-loss:  2.2139819746148097 	 ± 0.2225266871838832
	data : 0.11473474502563477
	model : 0.06498703956604004
			 train-loss:  2.2170499530998438 	 ± 0.22256701137448062
	data : 0.11459412574768066
	model : 0.06495547294616699
			 train-loss:  2.216553897857666 	 ± 0.2211194310719501
	data : 0.11461267471313477
	model : 0.06486425399780274
			 train-loss:  2.2205367872589514 	 ± 0.22235156596211247
	data : 0.11474366188049316
	model : 0.0648587703704834
			 train-loss:  2.219451418170681 	 ± 0.22110555873031246
	data : 0.11494402885437012
	model : 0.06486954689025878
			 train-loss:  2.215409903954237 	 ± 0.22252777037313043
	data : 0.11498723030090333
	model : 0.06489834785461426
			 train-loss:  2.217487602294246 	 ± 0.22187497273825274
	data : 0.11506075859069824
	model : 0.06496772766113282
			 train-loss:  2.2189530029892923 	 ± 0.2208682679111504
	data : 0.11511521339416504
	model : 0.0650017261505127
			 train-loss:  2.217113893709065 	 ± 0.22011615323987863
	data : 0.11503820419311524
	model : 0.06500225067138672
			 train-loss:  2.217310136411248 	 ± 0.21877699375371973
	data : 0.11487860679626465
	model : 0.06501932144165039
			 train-loss:  2.217000870819551 	 ± 0.21747309893017813
	data : 0.11467370986938477
	model : 0.06501526832580566
			 train-loss:  2.2159713747955503 	 ± 0.21637811110923255
	data : 0.11465373039245605
	model : 0.06502261161804199
			 train-loss:  2.2194697057499604 	 ± 0.2174780179895446
	data : 0.11473269462585449
	model : 0.06501297950744629
			 train-loss:  2.217537529246752 	 ± 0.2169425219500812
	data : 0.11488137245178223
	model : 0.06502385139465332
			 train-loss:  2.2193096793931106 	 ± 0.21631730406107919
	data : 0.11484780311584472
	model : 0.0650186538696289
			 train-loss:  2.2253900346430866 	 ± 0.22243625611961715
	data : 0.11500024795532227
	model : 0.06502065658569336
			 train-loss:  2.2281942193427784 	 ± 0.22274186959639555
	data : 0.11491780281066895
	model : 0.06498522758483886
			 train-loss:  2.22606727414661 	 ± 0.22240796118455128
	data : 0.11481413841247559
	model : 0.06498174667358399
			 train-loss:  2.225661169041644 	 ± 0.2212161146523722
	data : 0.11463212966918945
	model : 0.06499862670898438
			 train-loss:  2.2242349865643876 	 ± 0.22043081415082855
	data : 0.11450605392456055
	model : 0.06497564315795898
			 train-loss:  2.222671199870366 	 ± 0.21975498382359498
	data : 0.11461820602416992
	model : 0.0649989128112793
			 train-loss:  2.226069043291376 	 ± 0.22102538803459496
	data : 0.11484875679016113
	model : 0.0649907112121582
			 train-loss:  2.22299478430497 	 ± 0.22187020678782013
	data : 0.11487307548522949
	model : 0.06503572463989257
			 train-loss:  2.225726068019867 	 ± 0.2223112801853834
	data : 0.11499872207641601
	model : 0.06504325866699219
			 train-loss:  2.2269006193298653 	 ± 0.22146158987618805
	data : 0.11503739356994629
	model : 0.06506924629211426
			 train-loss:  2.2261153167607834 	 ± 0.2204644962639466
	data : 0.11494488716125488
	model : 0.06502952575683593
			 train-loss:  2.2294199129547736 	 ± 0.22177428504077856
	data : 0.11482157707214355
	model : 0.06504549980163574
			 train-loss:  2.2282746171951295 	 ± 0.22095667834583285
	data : 0.1147695541381836
	model : 0.06502485275268555
			 train-loss:  2.2256984273986062 	 ± 0.22136427986972113
	data : 0.11463837623596192
	model : 0.06494650840759278
			 train-loss:  2.22724780031279 	 ± 0.22082614750290339
	data : 0.11476340293884277
	model : 0.06492915153503417
			 train-loss:  2.2232987221004894 	 ± 0.2233415792419534
	data : 0.11482963562011719
	model : 0.06493577957153321
			 train-loss:  2.221575997196711 	 ± 0.2229518177501894
	data : 0.11485915184020996
	model : 0.06501030921936035
			 train-loss:  2.217009897459121 	 ± 0.22672104423862569
	data : 0.11483263969421387
	model : 0.06508879661560059
			 train-loss:  2.218913497789851 	 ± 0.2264905984601298
	data : 0.11480960845947266
	model : 0.06514744758605957
			 train-loss:  2.215276486405702 	 ± 0.22851853973824934
	data : 0.11473031044006347
	model : 0.0651768684387207
			 train-loss:  2.216188938529403 	 ± 0.22765386590403383
	data : 0.11467185020446777
	model : 0.06516928672790527
			 train-loss:  2.2166321671337164 	 ± 0.22665398507473122
	data : 0.11466155052185059
	model : 0.06505560874938965
			 train-loss:  2.2156754341992464 	 ± 0.2258423838370804
	data : 0.11475372314453125
	model : 0.06497402191162109
			 train-loss:  2.212938393558468 	 ± 0.22664803739555478
	data : 0.11467700004577637
	model : 0.06497006416320801
			 train-loss:  2.217866336660726 	 ± 0.23153028155068553
	data : 0.11490674018859863
	model : 0.06493620872497559
			 train-loss:  2.2170777247015354 	 ± 0.2306545756793684
	data : 0.11488833427429199
	model : 0.06498031616210938
			 train-loss:  2.2186708879052546 	 ± 0.2302643399695416
	data : 0.11486239433288574
	model : 0.06503710746765137
			 train-loss:  2.2160846648008925 	 ± 0.23091795726601136
	data : 0.11475839614868164
	model : 0.06509952545166016
			 train-loss:  2.2150061993763366 	 ± 0.23021115556782581
	data : 0.11492819786071777
	model : 0.06510066986083984
			 train-loss:  2.217908097128583 	 ± 0.23134616066805955
	data : 0.11484799385070801
	model : 0.0650862216949463
			 train-loss:  2.2161835775537004 	 ± 0.23111778836200253
	data : 0.11478056907653808
	model : 0.06508898735046387
			 train-loss:  2.2139887779700658 	 ± 0.23137628671734942
	data : 0.1146881103515625
	model : 0.06506638526916504
			 train-loss:  2.215619394183159 	 ± 0.23109580560929796
	data : 0.11476149559020996
	model : 0.06498379707336426
			 train-loss:  2.2179817749449042 	 ± 0.23158930595935046
	data : 0.11485762596130371
	model : 0.06495509147644044
			 train-loss:  2.2143219748481373 	 ± 0.23412534560521878
	data : 0.1147979736328125
	model : 0.06504201889038086
			 train-loss:  2.2155257725134128 	 ± 0.23355047288859548
	data : 0.11499209403991699
	model : 0.06500830650329589
			 train-loss:  2.2193509897878094 	 ± 0.23644388137392106
	data : 0.11504788398742676
	model : 0.06501398086547852
			 train-loss:  2.219141569137573 	 ± 0.2355077528149253
	data : 0.11506462097167969
	model : 0.06503710746765137
			 train-loss:  2.2158894160437206 	 ± 0.23737264882173056
	data : 0.11504263877868652
	model : 0.06511502265930176
			 train-loss:  2.213932209127531 	 ± 0.2374547744658466
	data : 0.1149489402770996
	model : 0.06510300636291504
			 train-loss:  2.212265861220658 	 ± 0.237269692018784
	data : 0.11478629112243652
	model : 0.06512012481689453
			 train-loss:  2.2117472850075064 	 ± 0.23642106197589646
	data : 0.1148198127746582
	model : 0.06515493392944335
			 train-loss:  2.2111590137848487 	 ± 0.2356047532732974
	data : 0.11479463577270507
	model : 0.0651423454284668
			 train-loss:  2.210182312790674 	 ± 0.23496781735026634
	data : 0.1148076057434082
	model : 0.06512918472290039
			 train-loss:  2.209925471833258 	 ± 0.23409455413028823
	data : 0.11493983268737792
	model : 0.06513457298278809
			 train-loss:  2.2096534859865233 	 ± 0.23323377352569669
	data : 0.11489410400390625
	model : 0.06516146659851074
			 train-loss:  2.2080314764335975 	 ± 0.23311359813230909
	data : 0.1149247169494629
	model : 0.06513838768005371
			 train-loss:  2.20728849834866 	 ± 0.23240780295891067
	data : 0.11488261222839355
	model : 0.06514286994934082
			 train-loss:  2.207150431240306 	 ± 0.23155734292138208
	data : 0.11471447944641114
	model : 0.06508522033691407
			 train-loss:  2.207003172296677 	 ± 0.2307170867599541
	data : 0.11460313796997071
	model : 0.06502470970153809
			 train-loss:  2.207864899566208 	 ± 0.2301008026861468
	data : 0.11480908393859864
	model : 0.0649693489074707
			 train-loss:  2.2092145518433277 	 ± 0.2298191585739882
	data : 0.11495962142944335
	model : 0.06494054794311524
			 train-loss:  2.207597391945975 	 ± 0.22978924282269936
	data : 0.11502594947814941
	model : 0.06496119499206543
			 train-loss:  2.2066559757746704 	 ± 0.22924371985009642
	data : 0.11517291069030762
	model : 0.06503028869628906
			 train-loss:  2.20499656233989 	 ± 0.22928335927138466
	data : 0.11521725654602051
	model : 0.06505889892578125
			 train-loss:  2.2050143922125542 	 ± 0.22848036151429285
	data : 0.11515388488769532
	model : 0.06506919860839844
			 train-loss:  2.2058272196186914 	 ± 0.2278930259805588
	data : 0.11504645347595215
	model : 0.06508336067199708
			 train-loss:  2.205912673884425 	 ± 0.22710814347313865
	data : 0.11502070426940918
	model : 0.0650871753692627
			 train-loss:  2.206128504178295 	 ± 0.22634396083742478
	data : 0.11488456726074218
	model : 0.06500773429870606
			 train-loss:  2.2063024092693717 	 ± 0.22558255667170155
	data : 0.11486163139343261
	model : 0.06498212814331054
			 train-loss:  2.2052593714482076 	 ± 0.2251745564762257
	data : 0.11488275527954102
	model : 0.06498069763183593
			 train-loss:  2.206817649354871 	 ± 0.22521693195716702
	data : 0.11495513916015625
	model : 0.06496305465698242
			 train-loss:  2.20646333694458 	 ± 0.2245066154613861
	data : 0.11505680084228516
	model : 0.06496505737304688
			 train-loss:  2.2067758500181287 	 ± 0.2237947137168376
	data : 0.1151052474975586
	model : 0.06503434181213379
			 train-loss:  2.206041180773785 	 ± 0.22323994708650344
	data : 0.11522402763366699
	model : 0.06503634452819824
			 train-loss:  2.206861778022417 	 ± 0.22273908893108338
	data : 0.1151641845703125
	model : 0.06503515243530274
			 train-loss:  2.2051997471165348 	 ± 0.22296452719427476
	data : 0.11508164405822754
	model : 0.06519064903259278
			 train-loss:  2.2049217877849454 	 ± 0.22227088978312978
	data : 0.11471977233886718
	model : 0.06519584655761719
			 train-loss:  2.204052039445975 	 ± 0.2218217877948586
	data : 0.1146012783050537
	model : 0.06515684127807617
			 train-loss:  2.204221814301363 	 ± 0.22112438799256276
	data : 0.1146538257598877
	model : 0.06516065597534179
			 train-loss:  2.205917659439618 	 ± 0.2214453465024578
	data : 0.1147080898284912
	model : 0.06516642570495605
			 train-loss:  2.2040053583541006 	 ± 0.22205273160245015
	data : 0.11475815773010253
	model : 0.06508831977844239
			 train-loss:  2.204046680033207 	 ± 0.22135834240895275
	data : 0.1151167869567871
	model : 0.06506915092468261
			 train-loss:  2.204491126611366 	 ± 0.22074142379133346
	data : 0.11526756286621094
	model : 0.0650813102722168
			 train-loss:  2.2036963833702936 	 ± 0.22028999958310574
	data : 0.11516103744506836
	model : 0.0651021957397461
			 train-loss:  2.2035274768899553 	 ± 0.2196237460447562
	data : 0.1151921272277832
	model : 0.06511564254760742
			 train-loss:  2.203743684582594 	 ± 0.21897053690861149
	data : 0.11515045166015625
	model : 0.0650491714477539
			 train-loss:  2.2064102707487163 	 ± 0.22096075064311638
	data : 0.11500940322875977
	model : 0.06502647399902343
			 train-loss:  2.2062415918671943 	 ± 0.22030485606196598
	data : 0.11480069160461426
	model : 0.06498231887817382
			 train-loss:  2.205126075687523 	 ± 0.22011399762108602
	data : 0.11477813720703126
	model : 0.06495151519775391
			 train-loss:  2.206095407406489 	 ± 0.2198151312648451
	data : 0.11488986015319824
	model : 0.0649287223815918
			 train-loss:  2.2060394399970242 	 ± 0.2191650262889
	data : 0.11492605209350586
	model : 0.0649289608001709
			 train-loss:  2.20427544187097 	 ± 0.21971944697709136
	data : 0.11504936218261719
	model : 0.06499509811401367
			 train-loss:  2.205958128672594 	 ± 0.22017188694988848
	data : 0.11524295806884766
	model : 0.0651336669921875
			 train-loss:  2.2054417098677437 	 ± 0.21963476110348246
	data : 0.11507549285888671
	model : 0.06516842842102051
			 train-loss:  2.204253021003194 	 ± 0.21955322914576364
	data : 0.11490874290466309
	model : 0.06522798538208008
			 train-loss:  2.204721851595517 	 ± 0.21900825074531663
	data : 0.11504020690917968
	model : 0.06524252891540527
			 train-loss:  2.2060009350095475 	 ± 0.21903242647764107
	data : 0.11488962173461914
	model : 0.06518425941467285
			 train-loss:  2.204722164029425 	 ± 0.21906343174206433
	data : 0.11471190452575683
	model : 0.06504859924316406
			 train-loss:  2.2066607333846013 	 ± 0.21995245176398726
	data : 0.11488547325134277
	model : 0.06501355171203613
			 train-loss:  2.2068393705935963 	 ± 0.2193466132370754
	data : 0.11501574516296387
	model : 0.06494970321655273
			 train-loss:  2.2085523345616944 	 ± 0.21992372562200332
	data : 0.11480627059936524
	model : 0.06494040489196777
			 train-loss:  2.208258193731308 	 ± 0.21934728055975947
	data : 0.11479172706604004
	model : 0.06497769355773926
			 train-loss:  2.20684528877722 	 ± 0.21956034390385873
	data : 0.11496100425720215
	model : 0.0650561809539795
			 train-loss:  2.2060681712496413 	 ± 0.21920579463365605
	data : 0.11499185562133789
	model : 0.06508550643920899
			 train-loss:  2.2054087425190243 	 ± 0.21878699152902578
	data : 0.11488432884216308
	model : 0.06510744094848633
			 train-loss:  2.2043542058571526 	 ± 0.21865749830578593
	data : 0.11494412422180175
	model : 0.06510734558105469
			 train-loss:  2.204537329802642 	 ± 0.21807987872409684
	data : 0.11494779586791992
	model : 0.06507172584533691
			 train-loss:  2.2046324988847137 	 ± 0.2174967043993806
	data : 0.11493582725524902
	model : 0.06500201225280762
			 train-loss:  2.2045643699360404 	 ± 0.2169163728740136
	data : 0.11498074531555176
	model : 0.06498804092407226
			 train-loss:  2.204324467385069 	 ± 0.21636357107476378
	data : 0.11503887176513672
	model : 0.06500158309936524
			 train-loss:  2.2063628441442256 	 ± 0.2175928358910027
	data : 0.11503314971923828
	model : 0.0651017189025879
			 train-loss:  2.2073469525889347 	 ± 0.21744077377458454
	data : 0.11507654190063477
	model : 0.0651423454284668
			 train-loss:  2.2073412990070764 	 ± 0.21687082408584854
	data : 0.11509857177734376
	model : 0.06519932746887207
			 train-loss:  2.207590273271004 	 ± 0.2163326854152775
	data : 0.11507639884948731
	model : 0.06524543762207032
			 train-loss:  2.208331788759775 	 ± 0.2160160061000175
	data : 0.11501035690307618
	model : 0.06522750854492188
			 train-loss:  2.2087680285738918 	 ± 0.21554376191652586
	data : 0.11497879028320312
	model : 0.06512327194213867
			 train-loss:  2.2093481993063904 	 ± 0.21514218894801132
	data : 0.11508440971374512
	model : 0.0650719165802002
			 train-loss:  2.2088526426529396 	 ± 0.2147042034467105
	data : 0.11510863304138183
	model : 0.06505851745605469
			 train-loss:  2.210717503794559 	 ± 0.21574412181560973
	data : 0.11516804695129394
	model : 0.06499042510986328
			 train-loss:  2.2125019280597416 	 ± 0.21665116931397477
	data : 0.11516017913818359
	model : 0.0649724006652832
			 train-loss:  2.211105838492887 	 ± 0.2169971803110283
	data : 0.11525888442993164
	model : 0.06497673988342285
			 train-loss:  2.2104066318273543 	 ± 0.21667862488922662
	data : 0.1152461051940918
	model : 0.06503558158874512
			 train-loss:  2.2087520111852617 	 ± 0.2174019322510058
	data : 0.11514763832092285
	model : 0.06504497528076172
			 train-loss:  2.20732583149825 	 ± 0.2178037043219254
	data : 0.11496453285217285
	model : 0.06510272026062011
			 train-loss:  2.2068512439727783 	 ± 0.2173712579510701
	data : 0.11511421203613281
	model : 0.06518392562866211
			 train-loss:  2.207052528858185 	 ± 0.21685679493838944
	data : 0.11502699851989746
	model : 0.06518659591674805
			 train-loss:  2.208363342285156 	 ± 0.21713587783438662
	data : 0.114990234375
	model : 0.06511435508728028
			 train-loss:  2.207190072652206 	 ± 0.2172586266232683
	data : 0.114955472946167
	model : 0.06507091522216797
			 train-loss:  2.2071756855877127 	 ± 0.21673331037345586
	data : 0.11497974395751953
	model : 0.06506390571594238
			 train-loss:  2.20491627145272 	 ± 0.21864176094904197
	data : 0.11489176750183105
	model : 0.0650151252746582
			 train-loss:  2.207887513215462 	 ± 0.22228759086089012
	data : 0.11483626365661621
	model : 0.06502227783203125
			 train-loss:  2.209031507514772 	 ± 0.2223735640568286
	data : 0.1149709701538086
	model : 0.06508293151855468
			 train-loss:  2.209288538914721 	 ± 0.22187725316454357
	data : 0.115028715133667
	model : 0.06509819030761718
			 train-loss:  2.209008179183276 	 ± 0.22139079861049643
	data : 0.11507959365844726
	model : 0.06523332595825196
			 train-loss:  2.2075671987354477 	 ± 0.22186476871267743
	data : 0.11484527587890625
	model : 0.06526131629943847
			 train-loss:  2.2089414056216445 	 ± 0.22225254939134426
	data : 0.11495347023010254
	model : 0.06523737907409669
			 train-loss:  2.208653409536495 	 ± 0.2217751011653524
	data : 0.11472063064575196
	model : 0.06522035598754883
			 train-loss:  2.2073894926795252 	 ± 0.22203592128236058
	data : 0.11471800804138184
	model : 0.06520729064941407
			 train-loss:  2.206151430507959 	 ± 0.2222697594897857
	data : 0.11466093063354492
	model : 0.06503286361694335
			 train-loss:  2.2060267640910016 	 ± 0.22176698444358295
	data : 0.114845609664917
	model : 0.06496143341064453
			 train-loss:  2.205387767591433 	 ± 0.2214611466222712
	data : 0.114933443069458
	model : 0.06512813568115235
			 train-loss:  2.204206318205053 	 ± 0.22164790237357368
	data : 0.11490478515625
	model : 0.06511316299438477
			 train-loss:  2.203395860766933 	 ± 0.22147234590213158
	data : 0.1149409294128418
	model : 0.06507892608642578
			 train-loss:  2.203971382733938 	 ± 0.22113854231358188
	data : 0.11496787071228028
	model : 0.06502389907836914
			 train-loss:  2.2031639424140144 	 ± 0.22096990149093637
	data : 0.11490678787231445
	model : 0.0649423599243164
			 train-loss:  2.2035972082189152 	 ± 0.22057102703567563
	data : 0.114754056930542
	model : 0.06465415954589844
			 train-loss:  2.2044207043117945 	 ± 0.2204251650667832
	data : 0.11484556198120117
	model : 0.06440320014953613
			 train-loss:  2.2039429012653047 	 ± 0.22005370274518726
	data : 0.11504960060119629
	model : 0.06420869827270508
			 train-loss:  2.2033588686703585 	 ± 0.21974394050113805
	data : 0.1153233528137207
	model : 0.06406464576721191
			 train-loss:  2.202555867663601 	 ± 0.219595046751007
	data : 0.11564040184020996
	model : 0.06396360397338867
			 train-loss:  2.2008917222376994 	 ± 0.22055118871923207
	data : 0.11569170951843262
	model : 0.06391658782958984
			 train-loss:  2.1995984746062236 	 ± 0.22093966682078842
	data : 0.1159134864807129
	model : 0.06399345397949219
			 train-loss:  2.1983362854301154 	 ± 0.22129039032488992
	data : 0.1159337043762207
	model : 0.06404991149902343
			 train-loss:  2.197000450615225 	 ± 0.22174438147577233
	data : 0.11586508750915528
	model : 0.06404585838317871
			 train-loss:  2.1970911726931135 	 ± 0.2212723383043298
	data : 0.11567368507385253
	model : 0.06400589942932129
			 train-loss:  2.1963940486948714 	 ± 0.221055297421591
	data : 0.11568846702575683
	model : 0.06397628784179688
			 train-loss:  2.196957216364272 	 ± 0.22075262487035463
	data : 0.11571602821350098
	model : 0.06392545700073242
			 train-loss:  2.196633966797489 	 ± 0.2203401603400085
	data : 0.11566529273986817
	model : 0.06390624046325684
			 train-loss:  2.196331898874371 	 ± 0.21992377922127807
	data : 0.11561264991760253
	model : 0.06391849517822265
			 train-loss:  2.1964420796442434 	 ± 0.2194678230253157
	data : 0.11571927070617676
	model : 0.06396598815917968
			 train-loss:  2.1966802330695434 	 ± 0.21903901957668695
	data : 0.11591668128967285
	model : 0.06395998001098632
			 train-loss:  2.1963725462555885 	 ± 0.21863396303178764
	data : 0.1159639835357666
	model : 0.06396265029907226
			 train-loss:  2.196878680055072 	 ± 0.2183207441435857
	data : 0.11579627990722656
	model : 0.06391711235046386
			 train-loss:  2.1968855242098657 	 ± 0.21786922718095492
	data : 0.11582431793212891
	model : 0.0638845443725586
			 train-loss:  2.1961114372245567 	 ± 0.21775369499312563
	data : 0.1158607006072998
	model : 0.06387410163879395
			 train-loss:  2.1953913867473602 	 ± 0.21759671395751062
	data : 0.11580038070678711
	model : 0.06385841369628906
			 train-loss:  2.1952665304651067 	 ± 0.21716094302123876
	data : 0.11560077667236328
	model : 0.06388978958129883
			 train-loss:  2.1952623715245627 	 ± 0.21671911929263912
	data : 0.11572704315185547
	model : 0.06393318176269532
			 train-loss:  2.1958774907386256 	 ± 0.21649504747093556
	data : 0.11574282646179199
	model : 0.06392445564270019
			 train-loss:  2.194923334544705 	 ± 0.2165778970410123
	data : 0.11561708450317383
	model : 0.06391139030456543
			 train-loss:  2.1941212348669885 	 ± 0.21651134446124423
	data : 0.11544108390808105
	model : 0.06390743255615235
			 train-loss:  2.194508725643158 	 ± 0.216164383647252
	data : 0.11559276580810547
	model : 0.06388812065124512
			 train-loss:  2.1949421195869903 	 ± 0.21584215257043637
	data : 0.11571707725524902
	model : 0.06385197639465331
			 train-loss:  2.1944761621573616 	 ± 0.21553992359584043
	data : 0.11570425033569336
	model : 0.06387691497802735
			 train-loss:  2.1953654171449863 	 ± 0.21557622202365428
	data : 0.11570429801940918
	model : 0.06386685371398926
			 train-loss:  2.195202903954063 	 ± 0.21516696860286033
	data : 0.11586384773254395
	model : 0.06389708518981933
			 train-loss:  2.1941987472421984 	 ± 0.21534015928761996
	data : 0.11585421562194824
	model : 0.0638878345489502
			 train-loss:  2.1930977553129196 	 ± 0.21563708564554537
	data : 0.11540260314941406
	model : 0.05547623634338379
#epoch  32    val-loss:  2.433354139328003  train-loss:  2.1930977553129196  lr:  1.953125e-05
			 train-loss:  2.052614450454712 	 ± 0.0
	data : 5.889231204986572
	model : 0.07110333442687988
			 train-loss:  2.0721815824508667 	 ± 0.019567131996154785
	data : 3.0101290941238403
	model : 0.06783771514892578
			 train-loss:  2.242502450942993 	 ± 0.2413993473980293
	data : 2.044956604639689
	model : 0.06672406196594238
			 train-loss:  2.250801742076874 	 ± 0.20955158885849698
	data : 1.5624215006828308
	model : 0.06618398427963257
			 train-loss:  2.2306176662445067 	 ± 0.19172658235968065
	data : 1.2728919982910156
	model : 0.06588187217712402
			 train-loss:  2.1711097160975137 	 ± 0.21986029497445536
	data : 0.11794466972351074
	model : 0.06466021537780761
			 train-loss:  2.155474696840559 	 ± 0.2071225850203111
	data : 0.11470727920532227
	model : 0.0647587776184082
			 train-loss:  2.1627645194530487 	 ± 0.19470306923199748
	data : 0.1146733283996582
	model : 0.06482009887695313
			 train-loss:  2.1844739649030895 	 ± 0.193565344382361
	data : 0.11447348594665527
	model : 0.06481585502624512
			 train-loss:  2.1631263375282286 	 ± 0.19447950802616465
	data : 0.11432771682739258
	model : 0.06483421325683594
			 train-loss:  2.19616745818745 	 ± 0.21284042516036294
	data : 0.11437263488769531
	model : 0.06479821205139161
			 train-loss:  2.199802448352178 	 ± 0.20413550415279244
	data : 0.11431622505187988
	model : 0.06480846405029297
			 train-loss:  2.219688736475431 	 ± 0.2078735014190229
	data : 0.11442389488220214
	model : 0.06503653526306152
			 train-loss:  2.2531196985925948 	 ± 0.23378204489424378
	data : 0.11436939239501953
	model : 0.06509709358215332
			 train-loss:  2.261893630027771 	 ± 0.2282283675770398
	data : 0.11456236839294434
	model : 0.0651522159576416
			 train-loss:  2.2511493042111397 	 ± 0.22486503527729543
	data : 0.11447877883911133
	model : 0.0651740550994873
			 train-loss:  2.2415881086798275 	 ± 0.22147816551005395
	data : 0.1144484519958496
	model : 0.0650794506072998
			 train-loss:  2.2459943095842996 	 ± 0.2160034326016494
	data : 0.1143876075744629
	model : 0.06487474441528321
			 train-loss:  2.2398936434795984 	 ± 0.21182953567848842
	data : 0.11458368301391601
	model : 0.06488976478576661
			 train-loss:  2.214055234193802 	 ± 0.23518717879693607
	data : 0.11454710960388184
	model : 0.06489429473876954
			 train-loss:  2.2288062969843545 	 ± 0.23881150166313064
	data : 0.11462388038635254
	model : 0.06489815711975097
			 train-loss:  2.221443929455497 	 ± 0.2357475593587766
	data : 0.11464848518371581
	model : 0.06497669219970703
			 train-loss:  2.210090264030125 	 ± 0.2366356906417847
	data : 0.1146775245666504
	model : 0.0649653434753418
			 train-loss:  2.199159026145935 	 ± 0.2375112203327283
	data : 0.11459197998046874
	model : 0.06500444412231446
			 train-loss:  2.198402690887451 	 ± 0.2327420151694609
	data : 0.11458320617675781
	model : 0.06498355865478515
			 train-loss:  2.1958415783368626 	 ± 0.22858130046688555
	data : 0.11461153030395507
	model : 0.06500630378723145
			 train-loss:  2.1940924238275596 	 ± 0.22448562456099933
	data : 0.11456589698791504
	model : 0.06501259803771972
			 train-loss:  2.2006591728755405 	 ± 0.2230657222246838
	data : 0.11449952125549316
	model : 0.0650320053100586
			 train-loss:  2.2008454306372283 	 ± 0.2191882382598173
	data : 0.11459259986877442
	model : 0.06498746871948242
			 train-loss:  2.1982929627100627 	 ± 0.21594205707732042
	data : 0.11452627182006836
	model : 0.06494150161743165
			 train-loss:  2.193117218632852 	 ± 0.21431378143662502
	data : 0.11449089050292968
	model : 0.06489791870117187
			 train-loss:  2.1974447295069695 	 ± 0.2123101972855925
	data : 0.11456413269042968
	model : 0.06485686302185059
			 train-loss:  2.191755193652529 	 ± 0.21153145907926274
	data : 0.11459550857543946
	model : 0.06485471725463868
			 train-loss:  2.1915685289046345 	 ± 0.20840024518795522
	data : 0.1146817684173584
	model : 0.06488003730773925
			 train-loss:  2.190332862309047 	 ± 0.20552785645140176
	data : 0.11469612121582032
	model : 0.06496453285217285
			 train-loss:  2.1840057373046875 	 ± 0.20608119057027147
	data : 0.11470389366149902
	model : 0.06500091552734374
			 train-loss:  2.1784508228302 	 ± 0.20599147709380972
	data : 0.1147679328918457
	model : 0.0650299072265625
			 train-loss:  2.179528606565375 	 ± 0.2033686899583405
	data : 0.1146385669708252
	model : 0.06500182151794434
			 train-loss:  2.1771538318731847 	 ± 0.20127753096447348
	data : 0.11459283828735352
	model : 0.06498022079467773
			 train-loss:  2.1802136182785032 	 ± 0.19966210902516407
	data : 0.11462807655334473
	model : 0.06494503021240235
			 train-loss:  2.1806907595657723 	 ± 0.19723526127260646
	data : 0.11463966369628906
	model : 0.06493949890136719
			 train-loss:  2.1827827124368575 	 ± 0.19533290333826595
	data : 0.11453685760498047
	model : 0.06493315696716309
			 train-loss:  2.190511847651282 	 ± 0.19944091650571383
	data : 0.1146965503692627
	model : 0.06494693756103516
			 train-loss:  2.1971020806919443 	 ± 0.2018420276270695
	data : 0.11467070579528808
	model : 0.06494579315185547
			 train-loss:  2.201633172565036 	 ± 0.20183711542750093
	data : 0.1144686222076416
	model : 0.06494083404541015
			 train-loss:  2.19966008870498 	 ± 0.20006947632478508
	data : 0.11443681716918945
	model : 0.06492857933044434
			 train-loss:  2.191742057495929 	 ± 0.20508564107460098
	data : 0.11452665328979492
	model : 0.0649601936340332
			 train-loss:  2.1898508643110595 	 ± 0.2033518340696046
	data : 0.11460094451904297
	model : 0.06499199867248535
			 train-loss:  2.1896322041141745 	 ± 0.2012718203975592
	data : 0.11470279693603516
	model : 0.06502032279968262
			 train-loss:  2.192928125858307 	 ± 0.20058023501694577
	data : 0.11489653587341309
	model : 0.06503257751464844
			 train-loss:  2.192634416561501 	 ± 0.19861488551267129
	data : 0.11485042572021484
	model : 0.06504330635070801
			 train-loss:  2.188758673576208 	 ± 0.1986337090673569
	data : 0.1148108959197998
	model : 0.06499485969543457
			 train-loss:  2.193974398217111 	 ± 0.200313520632617
	data : 0.114680814743042
	model : 0.0649726390838623
			 train-loss:  2.190880596637726 	 ± 0.1997241518881071
	data : 0.11461796760559081
	model : 0.06490869522094726
			 train-loss:  2.1867178851907902 	 ± 0.200250318103686
	data : 0.11468162536621093
	model : 0.06492052078247071
			 train-loss:  2.1913933349507198 	 ± 0.20146068042211757
	data : 0.11491122245788574
	model : 0.06491494178771973
			 train-loss:  2.185400232934115 	 ± 0.20466003706062005
	data : 0.11488032341003418
	model : 0.06496038436889648
			 train-loss:  2.187656478635196 	 ± 0.2036018903242099
	data : 0.11487140655517578
	model : 0.06496472358703613
			 train-loss:  2.1870197542643144 	 ± 0.20192730967934153
	data : 0.11491398811340332
	model : 0.06505541801452637
			 train-loss:  2.1841370284557344 	 ± 0.201458078678727
	data : 0.11494317054748535
	model : 0.06503186225891114
			 train-loss:  2.188212998577806 	 ± 0.20227910342433347
	data : 0.11482133865356445
	model : 0.0650331974029541
			 train-loss:  2.1877800091620414 	 ± 0.2006696862742871
	data : 0.11483154296875
	model : 0.06494522094726562
			 train-loss:  2.1881299870354787 	 ± 0.19908977196020786
	data : 0.11481847763061523
	model : 0.0649594783782959
			 train-loss:  2.1882697511464357 	 ± 0.1975313745238738
	data : 0.11476426124572754
	model : 0.0649261474609375
			 train-loss:  2.1851900834303635 	 ± 0.1975483629372394
	data : 0.1146728515625
	model : 0.06493711471557617
			 train-loss:  2.184231602784359 	 ± 0.1961983103028249
	data : 0.11466016769409179
	model : 0.0649674415588379
			 train-loss:  2.1832394137311337 	 ± 0.19489539749020643
	data : 0.11466512680053711
	model : 0.06506781578063965
			 train-loss:  2.18205561357386 	 ± 0.19369955394908686
	data : 0.11482129096984864
	model : 0.06506400108337403
			 train-loss:  2.1798624957817188 	 ± 0.19313938010009116
	data : 0.11481547355651855
	model : 0.06504716873168945
			 train-loss:  2.179416697365897 	 ± 0.19179060330083728
	data : 0.1148590087890625
	model : 0.06500492095947266
			 train-loss:  2.1854779015124683 	 ± 0.19707163631660546
	data : 0.11497507095336915
	model : 0.06498026847839355
			 train-loss:  2.1896811723709106 	 ± 0.19887738777967431
	data : 0.11503887176513672
	model : 0.06494455337524414
			 train-loss:  2.1953189405676437 	 ± 0.20322126654929784
	data : 0.1149148941040039
	model : 0.06495418548583984
			 train-loss:  2.1979078634365186 	 ± 0.2030519003612499
	data : 0.11484036445617676
	model : 0.06495323181152343
			 train-loss:  2.200655466715495 	 ± 0.20307385325720842
	data : 0.11481752395629882
	model : 0.06497917175292969
			 train-loss:  2.1942571197685443 	 ± 0.2092051262825432
	data : 0.11461224555969238
	model : 0.06502137184143067
			 train-loss:  2.195514717659393 	 ± 0.2081311678871421
	data : 0.1144948959350586
	model : 0.06495985984802247
			 train-loss:  2.1985425383616715 	 ± 0.20849251631306473
	data : 0.11449942588806153
	model : 0.06494002342224121
			 train-loss:  2.19745181029356 	 ± 0.20739258148936435
	data : 0.11465568542480468
	model : 0.06499056816101074
			 train-loss:  2.1958155170083047 	 ± 0.20660483010478403
	data : 0.11465535163879395
	model : 0.0649979591369629
			 train-loss:  2.1996445641105558 	 ± 0.2081622006726428
	data : 0.11469602584838867
	model : 0.06496200561523438
			 train-loss:  2.1976730256545833 	 ± 0.20764853217585322
	data : 0.11476750373840332
	model : 0.06512374877929687
			 train-loss:  2.1984235680246926 	 ± 0.20650571838025983
	data : 0.11479120254516602
	model : 0.06519322395324707
			 train-loss:  2.1979715526103973 	 ± 0.2053141401931199
	data : 0.11464867591857911
	model : 0.06516327857971191
			 train-loss:  2.1988880536135507 	 ± 0.20427561233916766
	data : 0.11463232040405273
	model : 0.06514201164245606
			 train-loss:  2.196346726528434 	 ± 0.2044315767325454
	data : 0.11475696563720703
	model : 0.06514053344726563
			 train-loss:  2.1945764826632095 	 ± 0.2039151840796895
	data : 0.11486396789550782
	model : 0.06509456634521485
			 train-loss:  2.196468637748198 	 ± 0.2035199454312959
	data : 0.11482357978820801
	model : 0.06504311561584472
			 train-loss:  2.1953304001454557 	 ± 0.20265483527152506
	data : 0.11486620903015136
	model : 0.06506118774414063
			 train-loss:  2.1940493371751573 	 ± 0.20188788998239626
	data : 0.11499290466308594
	model : 0.06509943008422851
			 train-loss:  2.194274517206045 	 ± 0.2007869160139635
	data : 0.11499195098876953
	model : 0.06508908271789551
			 train-loss:  2.194485527017842 	 ± 0.19970284604054966
	data : 0.11479358673095703
	model : 0.06499481201171875
			 train-loss:  2.1933311108619935 	 ± 0.19893466911198174
	data : 0.11484718322753906
	model : 0.06498870849609376
			 train-loss:  2.1937705988579608 	 ± 0.1979190613128891
	data : 0.11490325927734375
	model : 0.06494965553283691
			 train-loss:  2.1931236769023696 	 ± 0.19697451159451107
	data : 0.11479406356811524
	model : 0.06496920585632324
			 train-loss:  2.1930301785469055 	 ± 0.19594803619240866
	data : 0.11481738090515137
	model : 0.06501498222351074
			 train-loss:  2.1920118209013006 	 ± 0.19519057006176593
	data : 0.11488575935363769
	model : 0.06512060165405273
			 train-loss:  2.191258114211413 	 ± 0.19433397240685227
	data : 0.11535072326660156
	model : 0.0651740550994873
			 train-loss:  2.1910937675321946 	 ± 0.19335684148770224
	data : 0.11532583236694335
	model : 0.06517205238342286
			 train-loss:  2.190407648086548 	 ± 0.19250871330429478
	data : 0.11574106216430664
	model : 0.0651120662689209
			 train-loss:  2.1889885675789107 	 ± 0.19207825714432622
	data : 0.11563258171081543
	model : 0.0651789665222168
			 train-loss:  2.1879538484648164 	 ± 0.19141704583415695
	data : 0.11553215980529785
	model : 0.0651235580444336
			 train-loss:  2.189795757960347 	 ± 0.191391747177663
	data : 0.11509995460510254
	model : 0.06507267951965331
			 train-loss:  2.195798580463116 	 ± 0.19997517616541147
	data : 0.11515488624572753
	model : 0.0650787353515625
			 train-loss:  2.196280661083403 	 ± 0.19908134780008535
	data : 0.11473379135131836
	model : 0.06514229774475097
			 train-loss:  2.1950882560801954 	 ± 0.19851643589261428
	data : 0.11487360000610351
	model : 0.0650360107421875
			 train-loss:  2.192752128449556 	 ± 0.19904512853273462
	data : 0.11545042991638184
	model : 0.06508865356445312
			 train-loss:  2.1948438695183508 	 ± 0.19929948990303478
	data : 0.11538381576538086
	model : 0.06509971618652344
			 train-loss:  2.194604157307826 	 ± 0.19839880610974472
	data : 0.11534457206726074
	model : 0.06510639190673828
			 train-loss:  2.1948861653154546 	 ± 0.1975168795981381
	data : 0.11546335220336915
	model : 0.06508545875549317
			 train-loss:  2.1920265474834957 	 ± 0.19889938550648353
	data : 0.11526517868041992
	model : 0.06509013175964355
			 train-loss:  2.192464571978365 	 ± 0.19806322150759498
	data : 0.1146965503692627
	model : 0.06501917839050293
			 train-loss:  2.189718759165401 	 ± 0.19931458232379737
	data : 0.11473679542541504
	model : 0.06500387191772461
			 train-loss:  2.188086936348363 	 ± 0.199195200624918
	data : 0.11482529640197754
	model : 0.06507468223571777
			 train-loss:  2.1876977132714313 	 ± 0.19837077889608548
	data : 0.11481361389160157
	model : 0.0650571346282959
			 train-loss:  2.1874386401012025 	 ± 0.19753342028471957
	data : 0.11493616104125977
	model : 0.0650564193725586
			 train-loss:  2.187674768969544 	 ± 0.1967038899774382
	data : 0.11503682136535645
	model : 0.06506409645080566
			 train-loss:  2.187644493781914 	 ± 0.19586889928140952
	data : 0.11494512557983398
	model : 0.06509318351745605
			 train-loss:  2.188493412081935 	 ± 0.1952620599146463
	data : 0.11490330696105958
	model : 0.065010404586792
			 train-loss:  2.1853805810213087 	 ± 0.19738951624471096
	data : 0.11490421295166016
	model : 0.06506619453430176
			 train-loss:  2.1834921649664887 	 ± 0.19765765812030348
	data : 0.1148564338684082
	model : 0.06508359909057618
			 train-loss:  2.1830658590207332 	 ± 0.19690176725114206
	data : 0.11482906341552734
	model : 0.06511445045471191
			 train-loss:  2.181028967950402 	 ± 0.19738609141875135
	data : 0.11483311653137207
	model : 0.0650935173034668
			 train-loss:  2.1826422089530575 	 ± 0.19740106009375746
	data : 0.1148834228515625
	model : 0.06509599685668946
			 train-loss:  2.1797865543365478 	 ± 0.19916483561547837
	data : 0.11476526260375977
	model : 0.06511297225952148
			 train-loss:  2.178402745534503 	 ± 0.19897533278814464
	data : 0.11471915245056152
	model : 0.06513638496398926
			 train-loss:  2.1767704899855485 	 ± 0.1990355194169713
	data : 0.1147573471069336
	model : 0.06513500213623047
			 train-loss:  2.177135495468974 	 ± 0.19829918004725117
	data : 0.11483159065246581
	model : 0.06511812210083008
			 train-loss:  2.173892854720123 	 ± 0.20090700027369815
	data : 0.11481680870056152
	model : 0.06518435478210449
			 train-loss:  2.1723967277086698 	 ± 0.20085289751849505
	data : 0.11468162536621093
	model : 0.06509985923767089
			 train-loss:  2.1730161714189835 	 ± 0.20020942903942396
	data : 0.11471066474914551
	model : 0.0650522232055664
			 train-loss:  2.1725714622121868 	 ± 0.199514554832549
	data : 0.11477389335632324
	model : 0.06501774787902832
			 train-loss:  2.172090155737741 	 ± 0.19883999221909346
	data : 0.11482176780700684
	model : 0.06506237983703614
			 train-loss:  2.1716502033062834 	 ± 0.19816162833341186
	data : 0.11489377021789551
	model : 0.06500892639160157
			 train-loss:  2.1705527340924298 	 ± 0.19783465677924084
	data : 0.11499996185302734
	model : 0.06503534317016602
			 train-loss:  2.169621623614255 	 ± 0.19740265566643533
	data : 0.11502079963684082
	model : 0.06504192352294921
			 train-loss:  2.1706374265851767 	 ± 0.19703731696794988
	data : 0.11503582000732422
	model : 0.06506743431091308
			 train-loss:  2.1709393452906953 	 ± 0.19635391849784473
	data : 0.11513943672180176
	model : 0.06502752304077149
			 train-loss:  2.1743180837562615 	 ± 0.1996318580935734
	data : 0.11499757766723633
	model : 0.0650139331817627
			 train-loss:  2.1779518365859984 	 ± 0.2034787299311051
	data : 0.11493387222290039
	model : 0.06495623588562012
			 train-loss:  2.1781198234422834 	 ± 0.20276563261258904
	data : 0.11481924057006836
	model : 0.06495079994201661
			 train-loss:  2.177033630894943 	 ± 0.2024616533038306
	data : 0.11487131118774414
	model : 0.06493573188781739
			 train-loss:  2.1761010209997216 	 ± 0.2020583548192854
	data : 0.11475410461425781
	model : 0.06495075225830078
			 train-loss:  2.173431573642625 	 ± 0.20387021517212978
	data : 0.11472010612487793
	model : 0.06500515937805176
			 train-loss:  2.1748154689525734 	 ± 0.20384358402026664
	data : 0.1148331642150879
	model : 0.06505603790283203
			 train-loss:  2.17511481618228 	 ± 0.2031762679576572
	data : 0.11503968238830567
	model : 0.065067720413208
			 train-loss:  2.1728527432396296 	 ± 0.20432046810068424
	data : 0.11495413780212402
	model : 0.06511039733886718
			 train-loss:  2.172400970716734 	 ± 0.20370268239717232
	data : 0.11495776176452636
	model : 0.06514716148376465
			 train-loss:  2.1731334740683534 	 ± 0.2032134478626579
	data : 0.11493840217590331
	model : 0.06514616012573242
			 train-loss:  2.170949957370758 	 ± 0.2042811658468481
	data : 0.11493635177612305
	model : 0.06518425941467285
			 train-loss:  2.1718491205316504 	 ± 0.20390121627713423
	data : 0.11490707397460938
	model : 0.06516366004943848
			 train-loss:  2.1726984671856227 	 ± 0.20349720260546836
	data : 0.1149592399597168
	model : 0.06510930061340332
			 train-loss:  2.172037577317431 	 ± 0.20299468089705935
	data : 0.11499948501586914
	model : 0.06507787704467774
			 train-loss:  2.1748848294282888 	 ± 0.20537675332782881
	data : 0.11513252258300781
	model : 0.06505718231201171
			 train-loss:  2.1743030125094998 	 ± 0.2048404622356081
	data : 0.11505928039550781
	model : 0.06507339477539062
			 train-loss:  2.172248269502933 	 ± 0.20577912490250722
	data : 0.11500058174133301
	model : 0.06510586738586426
			 train-loss:  2.1731844791181527 	 ± 0.2054557537762197
	data : 0.11497321128845214
	model : 0.06524362564086914
			 train-loss:  2.1726678964457933 	 ± 0.2049068042610383
	data : 0.11476211547851563
	model : 0.06541967391967773
			 train-loss:  2.1722033586142198 	 ± 0.2043448707223208
	data : 0.11430716514587402
	model : 0.06542634963989258
			 train-loss:  2.171120434999466 	 ± 0.20416245754824394
	data : 0.11435518264770508
	model : 0.06540532112121582
			 train-loss:  2.171085976665805 	 ± 0.20352789175633712
	data : 0.11439633369445801
	model : 0.06537446975708008
			 train-loss:  2.1711924694202565 	 ± 0.20290324621649344
	data : 0.1145087718963623
	model : 0.06524677276611328
			 train-loss:  2.1729493111920504 	 ± 0.2035120737910343
	data : 0.114719820022583
	model : 0.06505703926086426
			 train-loss:  2.1723008969934976 	 ± 0.20305947989023065
	data : 0.11510963439941406
	model : 0.06503725051879883
			 train-loss:  2.1725710984432336 	 ± 0.20247278317631634
	data : 0.11511855125427246
	model : 0.06498875617980956
			 train-loss:  2.1750403915543153 	 ± 0.20433879005239083
	data : 0.11507163047790528
	model : 0.06499099731445312
			 train-loss:  2.1751932541053454 	 ± 0.20373559836371818
	data : 0.11493954658508301
	model : 0.06498804092407226
			 train-loss:  2.1756728007679893 	 ± 0.2032228471710876
	data : 0.11483688354492187
	model : 0.06492986679077148
			 train-loss:  2.1773283735534847 	 ± 0.2037538330702084
	data : 0.11476917266845703
	model : 0.06491646766662598
			 train-loss:  2.1807275211109833 	 ± 0.2079040053266332
	data : 0.11482396125793456
	model : 0.06490931510925294
			 train-loss:  2.1839919341237923 	 ± 0.2116196818682259
	data : 0.11484918594360352
	model : 0.06489472389221192
			 train-loss:  2.186706383560979 	 ± 0.21396843792636583
	data : 0.11488962173461914
	model : 0.06488661766052246
			 train-loss:  2.1862324538258457 	 ± 0.2134396555465536
	data : 0.11493606567382812
	model : 0.06499719619750977
			 train-loss:  2.1930970433114587 	 ± 0.23118580305674616
	data : 0.11502103805541992
	model : 0.06497893333435059
			 train-loss:  2.191549623353141 	 ± 0.23142625024274505
	data : 0.11507878303527833
	model : 0.06501083374023438
			 train-loss:  2.193614526905797 	 ± 0.23237893951801028
	data : 0.1151153564453125
	model : 0.06501574516296386
			 train-loss:  2.193144668293538 	 ± 0.2318053967424436
	data : 0.11503124237060547
	model : 0.06505537033081055
			 train-loss:  2.194336840945683 	 ± 0.2316968553758397
	data : 0.11499857902526855
	model : 0.06498527526855469
			 train-loss:  2.1924240669058688 	 ± 0.2324538120813848
	data : 0.11495509147644042
	model : 0.06556515693664551
			 train-loss:  2.1920184943411085 	 ± 0.23187070774022306
	data : 0.11440138816833496
	model : 0.06552467346191407
			 train-loss:  2.1958745513831714 	 ± 0.236946065507103
	data : 0.11438884735107421
	model : 0.06551542282104492
			 train-loss:  2.197263505432632 	 ± 0.23703194199848346
	data : 0.11444439888000488
	model : 0.06548314094543457
			 train-loss:  2.199615034249311 	 ± 0.2385026782573465
	data : 0.11440801620483398
	model : 0.06552023887634277
			 train-loss:  2.201623911442964 	 ± 0.23940110788537225
	data : 0.11435003280639648
	model : 0.0649827003479004
			 train-loss:  2.2007037446305557 	 ± 0.23907924445672465
	data : 0.11488409042358398
	model : 0.06507720947265624
			 train-loss:  2.199246930819686 	 ± 0.2392576142877825
	data : 0.11492691040039063
	model : 0.06510324478149414
			 train-loss:  2.2007775395949256 	 ± 0.23952837244363173
	data : 0.11488213539123535
	model : 0.06510214805603028
			 train-loss:  2.200098619816151 	 ± 0.23907081690458806
	data : 0.11477885246276856
	model : 0.06508269309997558
			 train-loss:  2.1982397515937766 	 ± 0.2397958764729016
	data : 0.11490216255187988
	model : 0.06510310173034668
			 train-loss:  2.198486628030476 	 ± 0.2391880831210814
	data : 0.11489009857177734
	model : 0.06502528190612793
			 train-loss:  2.1971871097674542 	 ± 0.2392326638662883
	data : 0.11497893333435058
	model : 0.06505441665649414
			 train-loss:  2.196245228871703 	 ± 0.23896365079644144
	data : 0.11516156196594238
	model : 0.06525492668151855
			 train-loss:  2.1968198372292393 	 ± 0.23847672055114744
	data : 0.1152574062347412
	model : 0.06526756286621094
			 train-loss:  2.1955678954566875 	 ± 0.23849632311508215
	data : 0.11513867378234863
	model : 0.06526226997375488
			 train-loss:  2.1959215567662165 	 ± 0.23793500392724284
	data : 0.11504983901977539
	model : 0.06527605056762695
			 train-loss:  2.1959291319457854 	 ± 0.23732727426417127
	data : 0.11495795249938964
	model : 0.06530933380126953
			 train-loss:  2.1941956317969384 	 ± 0.23796493386403722
	data : 0.1147535800933838
	model : 0.06512079238891602
			 train-loss:  2.193742189142439 	 ± 0.2374485595923743
	data : 0.11476249694824218
	model : 0.06523852348327637
			 train-loss:  2.191955012292718 	 ± 0.23818250344615544
	data : 0.11470122337341308
	model : 0.06518692970275879
			 train-loss:  2.194769425988197 	 ± 0.24088070572079717
	data : 0.11480021476745605
	model : 0.06516995429992675
			 train-loss:  2.1945928560560617 	 ± 0.24029372770798268
	data : 0.11477451324462891
	model : 0.06526079177856445
			 train-loss:  2.194719094451111 	 ± 0.2397048848680735
	data : 0.11478729248046875
	model : 0.06527194976806641
			 train-loss:  2.1933363112322803 	 ± 0.2399200438319361
	data : 0.11478581428527831
	model : 0.06519932746887207
			 train-loss:  2.194609391338685 	 ± 0.24001764806426273
	data : 0.11480622291564942
	model : 0.06521687507629395
			 train-loss:  2.1946243885086805 	 ± 0.23943161932253246
	data : 0.11482205390930175
	model : 0.06526689529418946
			 train-loss:  2.1932173892132285 	 ± 0.23969780794453918
	data : 0.1149294376373291
	model : 0.06517925262451171
			 train-loss:  2.192477866071434 	 ± 0.23935358535064177
	data : 0.1148653507232666
	model : 0.06520185470581055
			 train-loss:  2.1928391427947926 	 ± 0.2388340915379512
	data : 0.11493935585021972
	model : 0.0651066780090332
			 train-loss:  2.191587156085877 	 ± 0.23894524536956918
	data : 0.11497130393981933
	model : 0.06518893241882324
			 train-loss:  2.1917768319447837 	 ± 0.23839142039759764
	data : 0.1150059700012207
	model : 0.06514506340026856
			 train-loss:  2.195882814190399 	 ± 0.24515615338862418
	data : 0.11488070487976074
	model : 0.06507244110107421
			 train-loss:  2.1956766495164834 	 ± 0.2445956050983379
	data : 0.11498141288757324
	model : 0.06507644653320313
			 train-loss:  2.1950836909209057 	 ± 0.24417344497305846
	data : 0.11508936882019043
	model : 0.06513948440551758
			 train-loss:  2.1951539193358376 	 ± 0.24360443445271565
	data : 0.11509208679199219
	model : 0.06509609222412109
			 train-loss:  2.196398802690728 	 ± 0.24371858601375077
	data : 0.11495790481567383
	model : 0.06515178680419922
			 train-loss:  2.1977930433220334 	 ± 0.24401166970269034
	data : 0.11499404907226562
	model : 0.06513957977294922
			 train-loss:  2.1971677019848803 	 ± 0.24362220023280665
	data : 0.1148488998413086
	model : 0.06510539054870605
			 train-loss:  2.196764966763488 	 ± 0.24313518266372094
	data : 0.11466069221496582
	model : 0.06505393981933594
			 train-loss:  2.197885040823183 	 ± 0.2431425141765871
	data : 0.11476120948791504
	model : 0.06512031555175782
			 train-loss:  2.197219754349102 	 ± 0.24278898959764292
	data : 0.11469879150390624
	model : 0.06502890586853027
			 train-loss:  2.1960800896942345 	 ± 0.24282815131419674
	data : 0.11470069885253906
	model : 0.0650251865386963
			 train-loss:  2.1949354442390234 	 ± 0.24287745424730156
	data : 0.11504068374633789
	model : 0.06494336128234864
			 train-loss:  2.1962410248983066 	 ± 0.24311178321009638
	data : 0.1152425765991211
	model : 0.06487326622009278
			 train-loss:  2.1956148307238306 	 ± 0.2427486920915708
	data : 0.11521801948547364
	model : 0.06462435722351074
			 train-loss:  2.195746235317654 	 ± 0.24221663423891607
	data : 0.11544008255004883
	model : 0.06451172828674316
			 train-loss:  2.1959139847122464 	 ± 0.24169326113282286
	data : 0.11554923057556152
	model : 0.06429123878479004
			 train-loss:  2.1944930437903047 	 ± 0.24210453405592983
	data : 0.11534128189086915
	model : 0.06415863037109375
			 train-loss:  2.1956339509863603 	 ± 0.2421838201097961
	data : 0.11543717384338378
	model : 0.06402277946472168
			 train-loss:  2.19509867080955 	 ± 0.241789585680114
	data : 0.1155503273010254
	model : 0.06395497322082519
			 train-loss:  2.196747286423393 	 ± 0.24254984437655308
	data : 0.11560235023498536
	model : 0.06387805938720703
			 train-loss:  2.19572837786241 	 ± 0.24251707157372934
	data : 0.11550397872924804
	model : 0.06394157409667969
			 train-loss:  2.19535338621715 	 ± 0.24206094692672364
	data : 0.11562023162841797
	model : 0.06395497322082519
			 train-loss:  2.1960458975493142 	 ± 0.24177114894196108
	data : 0.11555275917053223
	model : 0.06394224166870117
			 train-loss:  2.1957511642040353 	 ± 0.24129593504081578
	data : 0.11552243232727051
	model : 0.06396880149841308
			 train-loss:  2.1981003989564614 	 ± 0.24344894996518912
	data : 0.11548895835876465
	model : 0.06394672393798828
			 train-loss:  2.1975462340702445 	 ± 0.2430811107940929
	data : 0.11558413505554199
	model : 0.06389532089233399
			 train-loss:  2.1969240913914225 	 ± 0.2427559570576102
	data : 0.11565566062927246
	model : 0.06390242576599121
			 train-loss:  2.196261108923359 	 ± 0.24246034743425884
	data : 0.11561117172241211
	model : 0.06393303871154785
			 train-loss:  2.1953300078044875 	 ± 0.2423785947943929
	data : 0.11574921607971192
	model : 0.06392536163330079
			 train-loss:  2.195135319729646 	 ± 0.24189183817322313
	data : 0.11578521728515626
	model : 0.06393642425537109
			 train-loss:  2.194176915275605 	 ± 0.24184566107657948
	data : 0.11564955711364747
	model : 0.06393814086914062
			 train-loss:  2.193655499742051 	 ± 0.2414811670362993
	data : 0.1155311107635498
	model : 0.06388764381408692
			 train-loss:  2.193696820686874 	 ± 0.24098463727315134
	data : 0.1156318187713623
	model : 0.06385998725891114
			 train-loss:  2.1937521109815505 	 ± 0.24049185374601662
	data : 0.11559581756591797
	model : 0.06382074356079101
			 train-loss:  2.19644205229623 	 ± 0.24365097254385717
	data : 0.11548728942871093
	model : 0.06383538246154785
			 train-loss:  2.1955162000849966 	 ± 0.2435867127196108
	data : 0.11567292213439942
	model : 0.06384696960449218
			 train-loss:  2.19514193853386 	 ± 0.2431639851217675
	data : 0.11578412055969238
	model : 0.06386451721191407
			 train-loss:  2.1969050856367236 	 ± 0.24425017349213932
	data : 0.11564679145812988
	model : 0.06384906768798829
			 train-loss:  2.196436477473462 	 ± 0.2438708992757697
	data : 0.11543655395507812
	model : 0.0638270378112793
			 train-loss:  2.197426475048065 	 ± 0.24388351144706497
	data : 0.11556820869445801
	model : 0.06381049156188964
			 train-loss:  2.1978788181130153 	 ± 0.24350226266793415
	data : 0.11557774543762207
	model : 0.06381263732910156
			 train-loss:  2.197966962106644 	 ± 0.24302265523116426
	data : 0.11556034088134766
	model : 0.0638082504272461
			 train-loss:  2.198225922735312 	 ± 0.24257673311787162
	data : 0.11569499969482422
	model : 0.06388463973999023
			 train-loss:  2.199228308801576 	 ± 0.2426231914505349
	data : 0.1159172534942627
	model : 0.06389799118041992
			 train-loss:  2.198262661111121 	 ± 0.2426355594035863
	data : 0.11584672927856446
	model : 0.06396651268005371
			 train-loss:  2.196999962441623 	 ± 0.24299921790379794
	data : 0.11538057327270508
	model : 0.055519914627075194
#epoch  33    val-loss:  2.4067061951285913  train-loss:  2.196999962441623  lr:  1.953125e-05
			 train-loss:  2.5770058631896973 	 ± 0.0
	data : 5.533687114715576
	model : 0.08032536506652832
			 train-loss:  2.3778538703918457 	 ± 0.19915199279785156
	data : 2.8288832902908325
	model : 0.07849419116973877
			 train-loss:  2.3417216142018638 	 ± 0.17044673859692167
	data : 1.923127253850301
	model : 0.0738660494486491
			 train-loss:  2.3058976531028748 	 ± 0.16012225513716422
	data : 1.4709582328796387
	model : 0.07154250144958496
			 train-loss:  2.281506156921387 	 ± 0.15129801590257247
	data : 1.1996494293212892
	model : 0.07015447616577149
			 train-loss:  2.2554821968078613 	 ± 0.14987374857969965
	data : 0.11580262184143067
	model : 0.0670018196105957
			 train-loss:  2.2541202136448453 	 ± 0.13879622936506206
	data : 0.11389646530151368
	model : 0.06462388038635254
			 train-loss:  2.2272716760635376 	 ± 0.1479940941963504
	data : 0.11452755928039551
	model : 0.06469535827636719
			 train-loss:  2.213773171106974 	 ± 0.1446594119273406
	data : 0.11461348533630371
	model : 0.06480708122253417
			 train-loss:  2.2163559198379517 	 ± 0.13745452423370394
	data : 0.11466612815856933
	model : 0.06488776206970215
			 train-loss:  2.2140872695229272 	 ± 0.13125395491559788
	data : 0.11478290557861329
	model : 0.06550989151000977
			 train-loss:  2.2172011733055115 	 ± 0.1260897603112274
	data : 0.11427512168884277
	model : 0.06551160812377929
			 train-loss:  2.2323873043060303 	 ± 0.1320722499029168
	data : 0.11425809860229492
	model : 0.06549229621887206
			 train-loss:  2.2152213879993985 	 ± 0.14151974364253045
	data : 0.11423420906066895
	model : 0.06547598838806153
			 train-loss:  2.220733976364136 	 ± 0.1382681795568943
	data : 0.11418814659118652
	model : 0.06552696228027344
			 train-loss:  2.205615520477295 	 ± 0.1461222930002101
	data : 0.11399121284484863
	model : 0.06500010490417481
			 train-loss:  2.193484018830692 	 ± 0.1498349559751041
	data : 0.11459264755249024
	model : 0.06508150100708007
			 train-loss:  2.188740352789561 	 ± 0.14692107915533661
	data : 0.11466050148010254
	model : 0.06511034965515136
			 train-loss:  2.1685860596205058 	 ± 0.1666170104632271
	data : 0.11457767486572265
	model : 0.06516695022583008
			 train-loss:  2.1799404203891752 	 ± 0.16977242245946456
	data : 0.11470160484313965
	model : 0.06511764526367188
			 train-loss:  2.165878114246187 	 ± 0.17721494337452195
	data : 0.11479725837707519
	model : 0.06510138511657715
			 train-loss:  2.1519710529934275 	 ± 0.18449705518736226
	data : 0.11469473838806152
	model : 0.06507811546325684
			 train-loss:  2.1520188424898232 	 ± 0.18044181885664728
	data : 0.11453385353088379
	model : 0.06508097648620606
			 train-loss:  2.15813514093558 	 ± 0.1790615090675088
	data : 0.11461529731750489
	model : 0.06497282981872558
			 train-loss:  2.160419430732727 	 ± 0.1758002690140394
	data : 0.11461329460144043
	model : 0.06496467590332031
			 train-loss:  2.1443016620782704 	 ± 0.19029349475464039
	data : 0.11462621688842774
	model : 0.06500205993652344
			 train-loss:  2.1611356293713606 	 ± 0.20551979728747752
	data : 0.1146052360534668
	model : 0.06498351097106933
			 train-loss:  2.1528942244393483 	 ± 0.20630980016651124
	data : 0.11468300819396973
	model : 0.06495604515075684
			 train-loss:  2.152367789169838 	 ± 0.2027406679629972
	data : 0.11476664543151856
	model : 0.06499485969543457
			 train-loss:  2.1691916942596436 	 ± 0.21895643902277137
	data : 0.11456003189086914
	model : 0.06492724418640136
			 train-loss:  2.166069099980016 	 ± 0.21607389111498485
	data : 0.11450252532958985
	model : 0.06486854553222657
			 train-loss:  2.1586241237819195 	 ± 0.2166729944159935
	data : 0.11461706161499023
	model : 0.06487159729003907
			 train-loss:  2.1574033932252363 	 ± 0.21347653349421733
	data : 0.11474027633666992
	model : 0.06490278244018555
			 train-loss:  2.167646923485924 	 ± 0.21839083712258922
	data : 0.11467900276184081
	model : 0.06493968963623047
			 train-loss:  2.1613962411880494 	 ± 0.218312324621954
	data : 0.11472129821777344
	model : 0.06503167152404785
			 train-loss:  2.1626117328802743 	 ± 0.21537893203557953
	data : 0.11482696533203125
	model : 0.06506476402282715
			 train-loss:  2.1671015668559717 	 ± 0.2141496259244537
	data : 0.11486654281616211
	model : 0.06504755020141602
			 train-loss:  2.1699747819649544 	 ± 0.21203458926927612
	data : 0.11457090377807617
	model : 0.06496472358703613
			 train-loss:  2.1631673880112476 	 ± 0.2134638704437342
	data : 0.11455082893371582
	model : 0.06494011878967285
			 train-loss:  2.1705582231283187 	 ± 0.21577304440216896
	data : 0.11461338996887208
	model : 0.06489977836608887
			 train-loss:  2.168832223589827 	 ± 0.21340479968374806
	data : 0.11459784507751465
	model : 0.06490077972412109
			 train-loss:  2.165946384270986 	 ± 0.21165711735500128
	data : 0.11458415985107422
	model : 0.06496520042419433
			 train-loss:  2.1675530838411907 	 ± 0.2094405084511721
	data : 0.11482524871826172
	model : 0.06508021354675293
			 train-loss:  2.1746048683469947 	 ± 0.2121477714297869
	data : 0.11479582786560058
	model : 0.06505646705627441
			 train-loss:  2.18610524336497 	 ± 0.2232171769173374
	data : 0.11485772132873535
	model : 0.06506175994873047
			 train-loss:  2.185881555080414 	 ± 0.22078267096071932
	data : 0.11488199234008789
	model : 0.06504716873168945
			 train-loss:  2.1903558715860894 	 ± 0.22051929115952396
	data : 0.11475090980529785
	model : 0.06496672630310059
			 train-loss:  2.1909039790431657 	 ± 0.2182424763355871
	data : 0.11469001770019531
	model : 0.06491827964782715
			 train-loss:  2.1935960589622963 	 ± 0.21680777732269166
	data : 0.11474709510803223
	model : 0.06493659019470215
			 train-loss:  2.2038450503349303 	 ± 0.22630189889860328
	data : 0.11479744911193848
	model : 0.06493558883666992
			 train-loss:  2.1979339052649105 	 ± 0.22793741160870448
	data : 0.11472702026367188
	model : 0.06499419212341309
			 train-loss:  2.2014584243297577 	 ± 0.2271340001991327
	data : 0.11479721069335938
	model : 0.06502904891967773
			 train-loss:  2.200321276232881 	 ± 0.2251304112624166
	data : 0.11479096412658692
	model : 0.06505050659179687
			 train-loss:  2.2057655453681946 	 ± 0.22653043618873345
	data : 0.11491045951843262
	model : 0.06501045227050781
			 train-loss:  2.2095243258909747 	 ± 0.2261547145488544
	data : 0.11479921340942383
	model : 0.06498608589172364
			 train-loss:  2.203778939587729 	 ± 0.22814064696317646
	data : 0.11483631134033204
	model : 0.06487274169921875
			 train-loss:  2.2083619602939537 	 ± 0.22871654440917147
	data : 0.11477861404418946
	model : 0.06486988067626953
			 train-loss:  2.2122337160439325 	 ± 0.22861276763992425
	data : 0.11488680839538574
	model : 0.06482634544372559
			 train-loss:  2.2125635510784085 	 ± 0.22668101009554945
	data : 0.11476964950561523
	model : 0.06486763954162597
			 train-loss:  2.2152997255325317 	 ± 0.22576445259074787
	data : 0.11482281684875488
	model : 0.06489334106445313
			 train-loss:  2.2143625118693366 	 ± 0.22402393444307833
	data : 0.11478986740112304
	model : 0.06495976448059082
			 train-loss:  2.2076934787534896 	 ± 0.22823298642089226
	data : 0.11485967636108399
	model : 0.06493406295776367
			 train-loss:  2.2075467734109786 	 ± 0.22641731467864673
	data : 0.11468610763549805
	model : 0.06496844291687012
			 train-loss:  2.2143990602344275 	 ± 0.23113173617449098
	data : 0.11459102630615234
	model : 0.0648921012878418
			 train-loss:  2.215679364938002 	 ± 0.22957550376049846
	data : 0.11454529762268066
	model : 0.06488914489746093
			 train-loss:  2.2133546074231467 	 ± 0.2285993110071731
	data : 0.11458439826965332
	model : 0.06487717628479003
			 train-loss:  2.2078500043100386 	 ± 0.23125207761419242
	data : 0.11458673477172851
	model : 0.06492624282836915
			 train-loss:  2.205117534188663 	 ± 0.23063247291268948
	data : 0.11466960906982422
	model : 0.06497173309326172
			 train-loss:  2.202495381452035 	 ± 0.22997390110596708
	data : 0.11465964317321778
	model : 0.06505188941955567
			 train-loss:  2.2012164899281093 	 ± 0.22857232178248948
	data : 0.11468086242675782
	model : 0.06504635810852051
			 train-loss:  2.1979672086070963 	 ± 0.22857931870131556
	data : 0.11473922729492188
	model : 0.06504559516906738
			 train-loss:  2.1949124154117374 	 ± 0.22844120977262156
	data : 0.11470413208007812
	model : 0.06495246887207032
			 train-loss:  2.206055812639733 	 ± 0.24578677699342935
	data : 0.11464142799377441
	model : 0.0648737907409668
			 train-loss:  2.204811355552158 	 ± 0.24435184908822558
	data : 0.11475067138671875
	model : 0.06482396125793458
			 train-loss:  2.201441618601481 	 ± 0.24444222468871954
	data : 0.11477890014648437
	model : 0.06485595703125
			 train-loss:  2.2002529188206323 	 ± 0.24304683953139106
	data : 0.11477560997009277
	model : 0.06487751007080078
			 train-loss:  2.198197129484895 	 ± 0.24212764591342062
	data : 0.11484870910644532
	model : 0.06494307518005371
			 train-loss:  2.198222380418044 	 ± 0.24057064083247356
	data : 0.11493535041809082
	model : 0.06495938301086426
			 train-loss:  2.201744846150845 	 ± 0.2410590252556491
	data : 0.11474781036376953
	model : 0.06496562957763671
			 train-loss:  2.2008998453617097 	 ± 0.23966537832329088
	data : 0.11477317810058593
	model : 0.06493906974792481
			 train-loss:  2.20151330512247 	 ± 0.23824456058194465
	data : 0.11481199264526368
	model : 0.06492366790771484
			 train-loss:  2.202427756495592 	 ± 0.2369303773511425
	data : 0.11474380493164063
	model : 0.06495122909545899
			 train-loss:  2.2002557415560067 	 ± 0.2363186694428846
	data : 0.11467189788818359
	model : 0.06503443717956543
			 train-loss:  2.1989831044560386 	 ± 0.2351937527042846
	data : 0.11479792594909669
	model : 0.06508665084838867
			 train-loss:  2.1949844514622407 	 ± 0.23666097822176835
	data : 0.1148305892944336
	model : 0.06514568328857422
			 train-loss:  2.193104165931081 	 ± 0.23591878516563233
	data : 0.11477994918823242
	model : 0.06517410278320312
			 train-loss:  2.1944744956904443 	 ± 0.2349030032541724
	data : 0.11471114158630372
	model : 0.06512637138366699
			 train-loss:  2.1938156309452923 	 ± 0.23364534877244056
	data : 0.11477832794189453
	model : 0.06503405570983886
			 train-loss:  2.192134483476703 	 ± 0.2328636664266632
	data : 0.11482467651367187
	model : 0.06504559516906738
			 train-loss:  2.1928897658983866 	 ± 0.23167596318075798
	data : 0.11489653587341309
	model : 0.0650111198425293
			 train-loss:  2.1926660655619026 	 ± 0.23040927548984957
	data : 0.11487584114074707
	model : 0.06501045227050781
			 train-loss:  2.1873757502307063 	 ± 0.2346449290677376
	data : 0.11487827301025391
	model : 0.06506910324096679
			 train-loss:  2.1882239439154185 	 ± 0.23352174719758773
	data : 0.11491875648498535
	model : 0.06509299278259277
			 train-loss:  2.1847334821173487 	 ± 0.2347026278560088
	data : 0.11482090950012207
	model : 0.06507730484008789
			 train-loss:  2.1850077603992664 	 ± 0.2334792271228651
	data : 0.11448369026184083
	model : 0.0651026725769043
			 train-loss:  2.185860849916935 	 ± 0.2324087945937299
	data : 0.11449966430664063
	model : 0.06506986618041992
			 train-loss:  2.1863834661306796 	 ± 0.23126440338402895
	data : 0.11462163925170898
	model : 0.06501522064208984
			 train-loss:  2.1843045962100125 	 ± 0.23099065349139175
	data : 0.11468720436096191
	model : 0.06497712135314941
			 train-loss:  2.18294286366665 	 ± 0.23021609096121834
	data : 0.1147552490234375
	model : 0.06501507759094238
			 train-loss:  2.182064982652664 	 ± 0.22922859962161413
	data : 0.1149775505065918
	model : 0.06499934196472168
			 train-loss:  2.185112756077606 	 ± 0.23011820486483037
	data : 0.11494359970092774
	model : 0.06499543190002441
			 train-loss:  2.1823571371097192 	 ± 0.2306559441397477
	data : 0.11492486000061035
	model : 0.06502780914306641
			 train-loss:  2.1805108322680575 	 ± 0.2302896883001644
	data : 0.11461958885192872
	model : 0.0650747299194336
			 train-loss:  2.1810842053248334 	 ± 0.22925371645164574
	data : 0.11460633277893066
	model : 0.06499090194702148
			 train-loss:  2.1785772153309413 	 ± 0.22958737114626565
	data : 0.11462655067443847
	model : 0.0650014877319336
			 train-loss:  2.1792817756814777 	 ± 0.2286158699402507
	data : 0.114703369140625
	model : 0.06502604484558105
			 train-loss:  2.1765057795515683 	 ± 0.2293329640597589
	data : 0.11464338302612305
	model : 0.06508884429931641
			 train-loss:  2.1767517041277 	 ± 0.22828294240006766
	data : 0.1148076057434082
	model : 0.06510834693908692
			 train-loss:  2.179696290864857 	 ± 0.2292845912250224
	data : 0.11486740112304687
	model : 0.06513571739196777
			 train-loss:  2.1767217256806113 	 ± 0.23034308733880884
	data : 0.11483359336853027
	model : 0.06513853073120117
			 train-loss:  2.177109221080402 	 ± 0.22933917074870838
	data : 0.11511673927307128
	model : 0.06513423919677734
			 train-loss:  2.1769751980900764 	 ± 0.2283174058896577
	data : 0.11511116027832032
	model : 0.06505470275878907
			 train-loss:  2.177377056231541 	 ± 0.227344688874409
	data : 0.11507115364074708
	model : 0.06503515243530274
			 train-loss:  2.173339709900973 	 ± 0.23037826226958608
	data : 0.11510138511657715
	model : 0.06509203910827636
			 train-loss:  2.1753181706304137 	 ± 0.23034508984323165
	data : 0.11518616676330566
	model : 0.06508374214172363
			 train-loss:  2.1739153409826346 	 ± 0.22984292139017842
	data : 0.11526246070861816
	model : 0.06510238647460938
			 train-loss:  2.172856557063567 	 ± 0.22914250552242416
	data : 0.11540460586547852
	model : 0.06514892578125
			 train-loss:  2.173587605104608 	 ± 0.22830647768073525
	data : 0.11550045013427734
	model : 0.06514492034912109
			 train-loss:  2.174772661273219 	 ± 0.22770934649713193
	data : 0.11548447608947754
	model : 0.06509261131286621
			 train-loss:  2.1739342391490935 	 ± 0.22694294769565945
	data : 0.11549510955810546
	model : 0.06510720252990723
			 train-loss:  2.174690766768022 	 ± 0.22615511518947382
	data : 0.11509485244750976
	model : 0.06509361267089844
			 train-loss:  2.1766583763185094 	 ± 0.22626390669061885
	data : 0.11495146751403809
	model : 0.06503710746765137
			 train-loss:  2.1768130383840423 	 ± 0.2253487327498192
	data : 0.11502671241760254
	model : 0.06502857208251953
			 train-loss:  2.1748549015291276 	 ± 0.22548644653879532
	data : 0.11501955986022949
	model : 0.06503934860229492
			 train-loss:  2.174033905029297 	 ± 0.2247686918666621
	data : 0.11496262550354004
	model : 0.06501255035400391
			 train-loss:  2.1725376125365967 	 ± 0.22449914522771397
	data : 0.11490683555603028
	model : 0.06500978469848633
			 train-loss:  2.172885753977017 	 ± 0.22364768806771446
	data : 0.11497092247009277
	model : 0.06503987312316895
			 train-loss:  2.1754074785858393 	 ± 0.22457766043270688
	data : 0.11488633155822754
	model : 0.0650240421295166
			 train-loss:  2.1755655070607975 	 ± 0.22371265530511147
	data : 0.11472649574279785
	model : 0.06506614685058594
			 train-loss:  2.1762589986507708 	 ± 0.22298971401681558
	data : 0.1146009922027588
	model : 0.06503376960754395
			 train-loss:  2.177298076280201 	 ± 0.22245268168876375
	data : 0.1146690845489502
	model : 0.0650217056274414
			 train-loss:  2.178974800037615 	 ± 0.22243785981780742
	data : 0.11479945182800293
	model : 0.06500449180603027
			 train-loss:  2.1782065172840777 	 ± 0.22177577872193943
	data : 0.11474051475524902
	model : 0.06505193710327148
			 train-loss:  2.1792609335771247 	 ± 0.2212810794318767
	data : 0.11494050025939942
	model : 0.06499247550964356
			 train-loss:  2.1792566705633094 	 ± 0.22046000205131108
	data : 0.11499495506286621
	model : 0.06501569747924804
			 train-loss:  2.1786714985090145 	 ± 0.21975319772114402
	data : 0.11502881050109863
	model : 0.06499791145324707
			 train-loss:  2.179357474737794 	 ± 0.21909580567126077
	data : 0.11471118927001953
	model : 0.06495718955993653
			 train-loss:  2.1778435387473176 	 ± 0.21901855797672118
	data : 0.11484317779541016
	model : 0.06499476432800293
			 train-loss:  2.1837415086279672 	 ± 0.2289639749283814
	data : 0.1148653507232666
	model : 0.06498217582702637
			 train-loss:  2.1822930344513485 	 ± 0.22878302789315558
	data : 0.11488337516784668
	model : 0.06499347686767579
			 train-loss:  2.1826361400861267 	 ± 0.2280064413862849
	data : 0.1147761344909668
	model : 0.06503753662109375
			 train-loss:  2.1838722942580637 	 ± 0.22767584632010554
	data : 0.11491618156433106
	model : 0.06510276794433593
			 train-loss:  2.182968955773574 	 ± 0.2271336046282725
	data : 0.11482925415039062
	model : 0.06503100395202636
			 train-loss:  2.1820698935124607 	 ± 0.2265987675953909
	data : 0.1147395133972168
	model : 0.06504721641540527
			 train-loss:  2.1853167147471986 	 ± 0.2291525918573967
	data : 0.11471772193908691
	model : 0.0649953842163086
			 train-loss:  2.1842689065084064 	 ± 0.22871476207423475
	data : 0.11493048667907715
	model : 0.06496329307556152
			 train-loss:  2.1854729222602587 	 ± 0.22839929622255994
	data : 0.11500144004821777
	model : 0.06489801406860352
			 train-loss:  2.1867768514800714 	 ± 0.22817470926400163
	data : 0.11512694358825684
	model : 0.0649324893951416
			 train-loss:  2.1864369803627066 	 ± 0.2274453186764377
	data : 0.1151463508605957
	model : 0.06494579315185547
			 train-loss:  2.188513220945994 	 ± 0.2280982302362324
	data : 0.11513643264770508
	model : 0.06497406959533691
			 train-loss:  2.187544277172215 	 ± 0.2276511999431283
	data : 0.11496105194091796
	model : 0.0650259017944336
			 train-loss:  2.1858307052599755 	 ± 0.22787606295697901
	data : 0.11491255760192871
	model : 0.0650670051574707
			 train-loss:  2.186269070587906 	 ± 0.22719444006383205
	data : 0.11472716331481933
	model : 0.06499648094177246
			 train-loss:  2.1847749754980015 	 ± 0.22720845322296096
	data : 0.11478991508483886
	model : 0.06499571800231933
			 train-loss:  2.1832006739031886 	 ± 0.2273154256465798
	data : 0.11497893333435058
	model : 0.0650169849395752
			 train-loss:  2.1820695912226653 	 ± 0.22702283799589798
	data : 0.1150660514831543
	model : 0.06499800682067872
			 train-loss:  2.1853912537264977 	 ± 0.23007022765965215
	data : 0.11508064270019532
	model : 0.06501245498657227
			 train-loss:  2.1838479313669326 	 ± 0.23015482730210327
	data : 0.11514806747436523
	model : 0.0651017665863037
			 train-loss:  2.186339201417359 	 ± 0.2315571364274344
	data : 0.11513566970825195
	model : 0.06507067680358887
			 train-loss:  2.187615256011486 	 ± 0.23139250775696385
	data : 0.11495471000671387
	model : 0.06506667137145997
			 train-loss:  2.18729041081778 	 ± 0.23070937233652172
	data : 0.11481375694274902
	model : 0.06498603820800782
			 train-loss:  2.1872549778149453 	 ± 0.22999664353160865
	data : 0.11483497619628906
	model : 0.06496992111206054
			 train-loss:  2.1877271646370917 	 ± 0.22936879738680596
	data : 0.11491837501525878
	model : 0.06489911079406738
			 train-loss:  2.1877068121258807 	 ± 0.22866858064603404
	data : 0.1149632453918457
	model : 0.06497788429260254
			 train-loss:  2.1869154597773695 	 ± 0.228199732530118
	data : 0.11507797241210938
	model : 0.06501307487487792
			 train-loss:  2.189286822296051 	 ± 0.2295414272616462
	data : 0.11515207290649414
	model : 0.06507320404052734
			 train-loss:  2.187687291356618 	 ± 0.22977918336757794
	data : 0.11509003639221191
	model : 0.06511292457580567
			 train-loss:  2.190754213503429 	 ± 0.2324973126647704
	data : 0.11501774787902833
	model : 0.06512503623962403
			 train-loss:  2.1914424331935907 	 ± 0.23198000109585412
	data : 0.11494951248168946
	model : 0.06520590782165528
			 train-loss:  2.193142923186807 	 ± 0.23235071619708034
	data : 0.11465730667114257
	model : 0.06583318710327149
			 train-loss:  2.1920056831069856 	 ± 0.2321443651329003
	data : 0.11413073539733887
	model : 0.06582670211791992
			 train-loss:  2.1955050465672517 	 ± 0.2359484574866455
	data : 0.11421184539794922
	model : 0.0657620906829834
			 train-loss:  2.1944890986977286 	 ± 0.2356425318480457
	data : 0.1141843318939209
	model : 0.06578259468078614
			 train-loss:  2.1935458142181923 	 ± 0.235291760844879
	data : 0.11423778533935547
	model : 0.06566843986511231
			 train-loss:  2.1927819865090505 	 ± 0.23483478132948718
	data : 0.11452283859252929
	model : 0.06504554748535156
			 train-loss:  2.1900797581130806 	 ± 0.23687949229518906
	data : 0.11507315635681152
	model : 0.06507172584533691
			 train-loss:  2.191084701462654 	 ± 0.23658533741630786
	data : 0.11490859985351562
	model : 0.06515707969665527
			 train-loss:  2.19090309035912 	 ± 0.23593220820274624
	data : 0.11487779617309571
	model : 0.06511058807373046
			 train-loss:  2.193611563251005 	 ± 0.2380311149499183
	data : 0.11481900215148926
	model : 0.06508779525756836
			 train-loss:  2.1908300108379786 	 ± 0.24026853635714618
	data : 0.11461191177368164
	model : 0.06502676010131836
			 train-loss:  2.189547052699558 	 ± 0.24022135971905653
	data : 0.11460151672363281
	model : 0.06502537727355957
			 train-loss:  2.190957479424529 	 ± 0.24031083624261243
	data : 0.11481661796569824
	model : 0.06496891975402833
			 train-loss:  2.1898131806993746 	 ± 0.24015004152255964
	data : 0.11492319107055664
	model : 0.06504526138305664
			 train-loss:  2.188199336114137 	 ± 0.2404895629652113
	data : 0.11497602462768555
	model : 0.06508989334106445
			 train-loss:  2.187194298409127 	 ± 0.24022586366996893
	data : 0.11520342826843262
	model : 0.06513385772705078
			 train-loss:  2.18728078949836 	 ± 0.2395821131458801
	data : 0.11522941589355469
	model : 0.0651698112487793
			 train-loss:  2.1877627767981056 	 ± 0.2390310633637308
	data : 0.11517014503479003
	model : 0.06515512466430665
			 train-loss:  2.188707044784059 	 ± 0.2387439469819165
	data : 0.11516828536987304
	model : 0.06510429382324219
			 train-loss:  2.188846531368437 	 ± 0.23811919235450726
	data : 0.11500563621520996
	model : 0.0650876522064209
			 train-loss:  2.18913981036136 	 ± 0.23752595888466504
	data : 0.11491351127624512
	model : 0.06500849723815919
			 train-loss:  2.1891172019598995 	 ± 0.23690355214528278
	data : 0.11484050750732422
	model : 0.06494240760803223
			 train-loss:  2.18791461798052 	 ± 0.23686960542182298
	data : 0.1148911476135254
	model : 0.0650029182434082
			 train-loss:  2.188802502315897 	 ± 0.23657527353898714
	data : 0.11498150825500489
	model : 0.06500287055969238
			 train-loss:  2.187938001352487 	 ± 0.2362701986620707
	data : 0.11508760452270508
	model : 0.06497502326965332
			 train-loss:  2.1881966107930895 	 ± 0.23569112491727137
	data : 0.11503009796142578
	model : 0.06504302024841309
			 train-loss:  2.189298411413115 	 ± 0.23559204046945784
	data : 0.11504616737365722
	model : 0.06506118774414063
			 train-loss:  2.1888625639949355 	 ± 0.23507253787764426
	data : 0.11509337425231933
	model : 0.06503901481628419
			 train-loss:  2.1882530011311925 	 ± 0.23463420549163969
	data : 0.1150331974029541
	model : 0.06504945755004883
			 train-loss:  2.1883057283995737 	 ± 0.23404510582557356
	data : 0.11494731903076172
	model : 0.06505441665649414
			 train-loss:  2.1894785541296007 	 ± 0.23404477033573923
	data : 0.11489949226379395
	model : 0.06503820419311523
			 train-loss:  2.189335854492377 	 ± 0.23347056559018972
	data : 0.11494693756103516
	model : 0.06504716873168945
			 train-loss:  2.188904027537544 	 ± 0.23297240671625152
	data : 0.11495814323425294
	model : 0.06505990028381348
			 train-loss:  2.188305090800882 	 ± 0.23255372417019365
	data : 0.11491231918334961
	model : 0.06508078575134277
			 train-loss:  2.190472341635648 	 ± 0.2340290937016732
	data : 0.11494154930114746
	model : 0.06510910987854004
			 train-loss:  2.192004983599593 	 ± 0.23448164550848902
	data : 0.11496953964233399
	model : 0.06517095565795898
			 train-loss:  2.1922900184844303 	 ± 0.23394742167154564
	data : 0.11496267318725586
	model : 0.06516447067260742
			 train-loss:  2.192753844215098 	 ± 0.23347657461629348
	data : 0.11485404968261718
	model : 0.06512293815612794
			 train-loss:  2.1916141217717757 	 ± 0.2334911636546123
	data : 0.1147038459777832
	model : 0.06513118743896484
			 train-loss:  2.1936783784884586 	 ± 0.23482672685200195
	data : 0.11472864151000976
	model : 0.06510438919067382
			 train-loss:  2.1939745624860127 	 ± 0.2343060768079045
	data : 0.11480069160461426
	model : 0.06507024765014649
			 train-loss:  2.1932553934260004 	 ± 0.23398240118349736
	data : 0.11483078002929688
	model : 0.06511158943176269
			 train-loss:  2.193274588517423 	 ± 0.23343007012113423
	data : 0.11488757133483887
	model : 0.06515960693359375
			 train-loss:  2.19336783550155 	 ± 0.23288542519778507
	data : 0.11505522727966308
	model : 0.06517767906188965
			 train-loss:  2.1934227280527634 	 ± 0.23234204440203932
	data : 0.1151151180267334
	model : 0.0652120590209961
			 train-loss:  2.1931281206219695 	 ± 0.23184114497724032
	data : 0.11513962745666503
	model : 0.06515345573425294
			 train-loss:  2.191538005515381 	 ± 0.23247600445570799
	data : 0.1150369644165039
	model : 0.06514124870300293
			 train-loss:  2.194015766069087 	 ± 0.23478101531275367
	data : 0.11496796607971191
	model : 0.06504659652709961
			 train-loss:  2.193225527575257 	 ± 0.23453098483681709
	data : 0.11490187644958497
	model : 0.06494641304016113
			 train-loss:  2.1911566181269957 	 ± 0.2359803883186856
	data : 0.11494131088256836
	model : 0.0653219223022461
			 train-loss:  2.1905408620834352 	 ± 0.23561972999842615
	data : 0.11474847793579102
	model : 0.06533298492431641
			 train-loss:  2.189993142002848 	 ± 0.23522638026823398
	data : 0.11490454673767089
	model : 0.06522650718688965
			 train-loss:  2.189890435150078 	 ± 0.23470095967127544
	data : 0.11496238708496094
	model : 0.06522159576416016
			 train-loss:  2.190172685101428 	 ± 0.23421189173875448
	data : 0.11498847007751464
	model : 0.06519541740417481
			 train-loss:  2.188968051224947 	 ± 0.234379874860414
	data : 0.11502442359924317
	model : 0.06464090347290039
			 train-loss:  2.1892442136340673 	 ± 0.23389497316101043
	data : 0.11537561416625977
	model : 0.06444134712219238
			 train-loss:  2.191168366807752 	 ± 0.23515489564880993
	data : 0.11534085273742675
	model : 0.06425213813781738
			 train-loss:  2.192446471836073 	 ± 0.23542175993578532
	data : 0.11543326377868653
	model : 0.06406927108764648
			 train-loss:  2.191881438619212 	 ± 0.23505912561969614
	data : 0.11568093299865723
	model : 0.06398735046386719
			 train-loss:  2.191048453988987 	 ± 0.23488234250564813
	data : 0.11576423645019532
	model : 0.06394166946411133
			 train-loss:  2.1908700346946715 	 ± 0.23438672392958165
	data : 0.11588549613952637
	model : 0.06392412185668946
			 train-loss:  2.1902016380648592 	 ± 0.23409841239515422
	data : 0.11595311164855956
	model : 0.06399540901184082
			 train-loss:  2.1896508587845442 	 ± 0.23374329196077523
	data : 0.11607480049133301
	model : 0.0640559196472168
			 train-loss:  2.1887825108393066 	 ± 0.23361586442982582
	data : 0.11596946716308594
	model : 0.06402087211608887
			 train-loss:  2.189850982437786 	 ± 0.23368598568711924
	data : 0.11586785316467285
	model : 0.06394371986389161
			 train-loss:  2.190372602990333 	 ± 0.2333247290366061
	data : 0.11566281318664551
	model : 0.06394233703613281
			 train-loss:  2.1915680553953525 	 ± 0.23354997197970584
	data : 0.11572322845458985
	model : 0.06386208534240723
			 train-loss:  2.190234904550802 	 ± 0.23395486736242152
	data : 0.11567163467407227
	model : 0.06386055946350097
			 train-loss:  2.1918346310863974 	 ± 0.23475820347253015
	data : 0.11585440635681152
	model : 0.06387166976928711
			 train-loss:  2.1921520801767644 	 ± 0.23431774708930758
	data : 0.11582889556884765
	model : 0.06394896507263184
			 train-loss:  2.1923378258943558 	 ± 0.23384670710792094
	data : 0.11595311164855956
	model : 0.06402897834777832
			 train-loss:  2.1926868644492754 	 ± 0.23342368214919934
	data : 0.1159430980682373
	model : 0.064044189453125
			 train-loss:  2.193366701937904 	 ± 0.23317986453909686
	data : 0.11573867797851563
	model : 0.06397309303283691
			 train-loss:  2.19286888715171 	 ± 0.23282840243222072
	data : 0.11541152000427246
	model : 0.06390323638916015
			 train-loss:  2.193458746691219 	 ± 0.23253267449801165
	data : 0.11551599502563477
	model : 0.06385436058044433
			 train-loss:  2.1929318807563005 	 ± 0.2322035235239534
	data : 0.1155728816986084
	model : 0.06380219459533691
			 train-loss:  2.1923673375835264 	 ± 0.2318995024624532
	data : 0.11557292938232422
	model : 0.06386742591857911
			 train-loss:  2.193002731693901 	 ± 0.23164406656622724
	data : 0.11562485694885254
	model : 0.06391119956970215
			 train-loss:  2.1918057554191157 	 ± 0.23194071772847116
	data : 0.11580772399902343
	model : 0.0640249252319336
			 train-loss:  2.1907650122202065 	 ± 0.23205401669050243
	data : 0.11575021743774414
	model : 0.06404695510864258
			 train-loss:  2.190744915008545 	 ± 0.23158966074962933
	data : 0.11565399169921875
	model : 0.06401095390319825
			 train-loss:  2.1908722341773044 	 ± 0.23113663306883434
	data : 0.11555471420288085
	model : 0.06394877433776855
			 train-loss:  2.1909277476961653 	 ± 0.23067924938711012
	data : 0.1156914234161377
	model : 0.06393842697143555
			 train-loss:  2.1918601735307295 	 ± 0.23069824812144407
	data : 0.11588487625122071
	model : 0.06383657455444336
			 train-loss:  2.1908893007931747 	 ± 0.23076096670837848
	data : 0.11601238250732422
	model : 0.06386995315551758
			 train-loss:  2.1917439446729774 	 ± 0.23071047545709109
	data : 0.11599936485290527
	model : 0.063934326171875
			 train-loss:  2.194500099401921 	 ± 0.2344279919723549
	data : 0.11577696800231933
	model : 0.05556092262268066
#epoch  34    val-loss:  2.4079977085715845  train-loss:  2.194500099401921  lr:  1.953125e-05
			 train-loss:  2.237476110458374 	 ± 0.0
	data : 5.70196795463562
	model : 0.07285666465759277
			 train-loss:  2.197458028793335 	 ± 0.04001808166503906
	data : 2.914137363433838
	model : 0.07038843631744385
			 train-loss:  2.1914478143056235 	 ± 0.033762058534343255
	data : 1.9820979436238606
	model : 0.06847325960795085
			 train-loss:  2.1587204933166504 	 ± 0.06378197288186631
	data : 1.5152555108070374
	model : 0.06761044263839722
			 train-loss:  2.16669340133667 	 ± 0.05923496518902725
	data : 1.235036516189575
	model : 0.06701889038085937
			 train-loss:  2.1475557486216226 	 ± 0.06895819754022355
	data : 0.11758260726928711
	model : 0.06542267799377441
			 train-loss:  2.170163665499006 	 ± 0.08451403102480776
	data : 0.11520719528198242
	model : 0.06481318473815918
			 train-loss:  2.135893866419792 	 ± 0.12029433761637406
	data : 0.11458387374877929
	model : 0.06491193771362305
			 train-loss:  2.1270907587475247 	 ± 0.11611557462676955
	data : 0.11471066474914551
	model : 0.06489887237548828
			 train-loss:  2.1259002327919005 	 ± 0.11021479107757512
	data : 0.11475186347961426
	model : 0.06496076583862305
			 train-loss:  2.131878386844288 	 ± 0.10677257801211452
	data : 0.1147646427154541
	model : 0.06499242782592773
			 train-loss:  2.1403945982456207 	 ± 0.10605722930110753
	data : 0.11481275558471679
	model : 0.06499719619750977
			 train-loss:  2.1343894646717954 	 ± 0.103998241407355
	data : 0.11479349136352539
	model : 0.06496267318725586
			 train-loss:  2.134372583457402 	 ± 0.10021523147392568
	data : 0.11458034515380859
	model : 0.0649714469909668
			 train-loss:  2.1606414238611857 	 ± 0.13796478093211653
	data : 0.11473684310913086
	model : 0.06494083404541015
			 train-loss:  2.1731426641345024 	 ± 0.14208748486271372
	data : 0.11463923454284668
	model : 0.06492280960083008
			 train-loss:  2.1620304654626286 	 ± 0.14483428148987454
	data : 0.11462259292602539
	model : 0.06495900154113769
			 train-loss:  2.1441344221433005 	 ± 0.15892181773766836
	data : 0.1147162914276123
	model : 0.06500716209411621
			 train-loss:  2.148142231138129 	 ± 0.1556149061017296
	data : 0.11474618911743165
	model : 0.06504898071289063
			 train-loss:  2.1538218915462495 	 ± 0.15368185109399696
	data : 0.11471571922302246
	model : 0.06505980491638183
			 train-loss:  2.161821825163705 	 ± 0.15418630984634596
	data : 0.11466364860534668
	model : 0.06502575874328613
			 train-loss:  2.1750536235896023 	 ± 0.16238687911789768
	data : 0.11465568542480468
	model : 0.06496119499206543
			 train-loss:  2.1722653896912285 	 ± 0.15935505023824448
	data : 0.11440548896789551
	model : 0.0649439811706543
			 train-loss:  2.180758699774742 	 ± 0.16122990615970456
	data : 0.1144932746887207
	model : 0.06489195823669433
			 train-loss:  2.1721169090270998 	 ± 0.1635469732843791
	data : 0.11451945304870606
	model : 0.06491694450378419
			 train-loss:  2.1642275039966288 	 ± 0.16515122142468108
	data : 0.11470341682434082
	model : 0.06500029563903809
			 train-loss:  2.173756484632139 	 ± 0.16919096242608084
	data : 0.11473646163940429
	model : 0.06504588127136231
			 train-loss:  2.1768931320735385 	 ± 0.1669397514205215
	data : 0.11480898857116699
	model : 0.06502437591552734
			 train-loss:  2.187977100240773 	 ± 0.17420621840433984
	data : 0.11470298767089844
	model : 0.06501426696777343
			 train-loss:  2.1877324263254803 	 ± 0.17128324224551503
	data : 0.11458120346069336
	model : 0.06503467559814453
			 train-loss:  2.188601109289354 	 ± 0.1685651266510459
	data : 0.11449112892150878
	model : 0.06496992111206054
			 train-loss:  2.1853893101215363 	 ± 0.1668713378130566
	data : 0.11443791389465333
	model : 0.06495981216430664
			 train-loss:  2.1872067090236778 	 ± 0.16464482356780358
	data : 0.11449904441833496
	model : 0.06494450569152832
			 train-loss:  2.193599252139821 	 ± 0.1663104274275839
	data : 0.11459784507751465
	model : 0.06494808197021484
			 train-loss:  2.1957888398851666 	 ± 0.16441381517096162
	data : 0.11459450721740723
	model : 0.06496434211730957
			 train-loss:  2.2101089557011924 	 ± 0.18291614616993626
	data : 0.11471724510192871
	model : 0.06498174667358399
			 train-loss:  2.2043911830799 	 ± 0.1836599655872021
	data : 0.11468949317932128
	model : 0.06505656242370605
			 train-loss:  2.2007911393516943 	 ± 0.18254549340494441
	data : 0.11472325325012207
	model : 0.06507439613342285
			 train-loss:  2.2042890328627367 	 ± 0.1814755211318441
	data : 0.11467900276184081
	model : 0.06510467529296875
			 train-loss:  2.195668378472328 	 ± 0.18710516362051158
	data : 0.11469950675964355
	model : 0.06503100395202636
			 train-loss:  2.1971728947104476 	 ± 0.1850541083351221
	data : 0.11459217071533204
	model : 0.0650407314300537
			 train-loss:  2.196974393867311 	 ± 0.18284222933153582
	data : 0.11467099189758301
	model : 0.06499838829040527
			 train-loss:  2.196951336638872 	 ± 0.18070371193888357
	data : 0.11465330123901367
	model : 0.0649993896484375
			 train-loss:  2.195081355896863 	 ± 0.17905882434874013
	data : 0.11470870971679688
	model : 0.06499896049499512
			 train-loss:  2.194612251387702 	 ± 0.1770854451823846
	data : 0.11470742225646972
	model : 0.06503729820251465
			 train-loss:  2.1877341503682346 	 ± 0.1811253827503584
	data : 0.11471128463745117
	model : 0.06505231857299805
			 train-loss:  2.183824252575002 	 ± 0.18113975684398798
	data : 0.11477041244506836
	model : 0.06501989364624024
			 train-loss:  2.181000796457132 	 ± 0.1802850958311478
	data : 0.11464109420776367
	model : 0.0649653434753418
			 train-loss:  2.1760112193165995 	 ± 0.1817536720142103
	data : 0.11451444625854493
	model : 0.06493034362792968
			 train-loss:  2.17621248960495 	 ± 0.17993247155962655
	data : 0.11462459564208985
	model : 0.06490597724914551
			 train-loss:  2.1692052425122728 	 ± 0.18492147624527175
	data : 0.11475062370300293
	model : 0.06488533020019531
			 train-loss:  2.1730695894131293 	 ± 0.18520240261245388
	data : 0.11480002403259278
	model : 0.06489048004150391
			 train-loss:  2.177190467996417 	 ± 0.18583811971611752
	data : 0.115016508102417
	model : 0.06495809555053711
			 train-loss:  2.176925387647417 	 ± 0.18411946921851569
	data : 0.115069580078125
	model : 0.06501445770263672
			 train-loss:  2.173483280702071 	 ± 0.18418309668319102
	data : 0.11496949195861816
	model : 0.06501808166503906
			 train-loss:  2.175320999962943 	 ± 0.1830392977868715
	data : 0.1147979736328125
	model : 0.06505074501037597
			 train-loss:  2.178202666734394 	 ± 0.1827036673075966
	data : 0.11455702781677246
	model : 0.06500177383422852
			 train-loss:  2.174321848770668 	 ± 0.18347633248794623
	data : 0.11445684432983398
	model : 0.06494622230529785
			 train-loss:  2.1734994629682123 	 ± 0.18202258694913367
	data : 0.1145927906036377
	model : 0.06487383842468261
			 train-loss:  2.1775677879651387 	 ± 0.18318445153874416
	data : 0.11455740928649902
	model : 0.06488976478576661
			 train-loss:  2.1767216236864932 	 ± 0.18179492748083373
	data : 0.11467156410217286
	model : 0.0648688793182373
			 train-loss:  2.1739218542652745 	 ± 0.18164388821708136
	data : 0.11479954719543457
	model : 0.06493988037109374
			 train-loss:  2.172867914986989 	 ± 0.1803874960377933
	data : 0.11477179527282715
	model : 0.06495676040649415
			 train-loss:  2.1706502623856068 	 ± 0.17983617447064182
	data : 0.11466155052185059
	model : 0.06497836112976074
			 train-loss:  2.175988241342398 	 ± 0.18348600021860936
	data : 0.1146435260772705
	model : 0.06489100456237792
			 train-loss:  2.1793565352757773 	 ± 0.18410446973354327
	data : 0.11462302207946777
	model : 0.0648580551147461
			 train-loss:  2.1756321298542307 	 ± 0.1852135721142926
	data : 0.1146960735321045
	model : 0.06487526893615722
			 train-loss:  2.1818777901284836 	 ± 0.19082229942796536
	data : 0.1149674415588379
	model : 0.06487293243408203
			 train-loss:  2.178067521772523 	 ± 0.1920225429583672
	data : 0.11498665809631348
	model : 0.0649294376373291
			 train-loss:  2.1762926612581524 	 ± 0.1912152287884476
	data : 0.11510910987854003
	model : 0.06505990028381348
			 train-loss:  2.176230998106406 	 ± 0.1898645684148503
	data : 0.11505217552185058
	model : 0.06508827209472656
			 train-loss:  2.178337126970291 	 ± 0.18937481423809932
	data : 0.11494426727294922
	model : 0.06503710746765137
			 train-loss:  2.1767936700010955 	 ± 0.18852870188682852
	data : 0.11475205421447754
	model : 0.06502828598022461
			 train-loss:  2.1743914919930534 	 ± 0.18837198087638068
	data : 0.11467518806457519
	model : 0.0649376392364502
			 train-loss:  2.1721921443939207 	 ± 0.18806602762507363
	data : 0.11466727256774903
	model : 0.06489219665527343
			 train-loss:  2.171611255721042 	 ± 0.18689237215177065
	data : 0.11469264030456543
	model : 0.06487050056457519
			 train-loss:  2.169625272998562 	 ± 0.18648027296524458
	data : 0.11480598449707032
	model : 0.06487560272216797
			 train-loss:  2.1695631467379055 	 ± 0.18528183248394262
	data : 0.114862060546875
	model : 0.06494240760803223
			 train-loss:  2.1710208246979534 	 ± 0.18455499173116574
	data : 0.11489682197570801
	model : 0.06502676010131836
			 train-loss:  2.1703368782997132 	 ± 0.18349861850190344
	data : 0.11503376960754394
	model : 0.06502714157104492
			 train-loss:  2.171369614424529 	 ± 0.1825961828013398
	data : 0.11505165100097656
	model : 0.06501922607421876
			 train-loss:  2.1751808044387073 	 ± 0.18469245427144498
	data : 0.11494951248168946
	model : 0.06502084732055664
			 train-loss:  2.176319024649011 	 ± 0.18386559717602738
	data : 0.11486725807189942
	model : 0.06495280265808105
			 train-loss:  2.179380924928756 	 ± 0.18488440488313138
	data : 0.11493129730224609
	model : 0.06491594314575196
			 train-loss:  2.181917538362391 	 ± 0.18525816953508412
	data : 0.11486878395080566
	model : 0.06491875648498535
			 train-loss:  2.1815690467523976 	 ± 0.18420596000940978
	data : 0.11486105918884278
	model : 0.06494436264038086
			 train-loss:  2.1847522395780716 	 ± 0.18550802739391198
	data : 0.11484622955322266
	model : 0.06497244834899903
			 train-loss:  2.18742679736831 	 ± 0.18613033707964546
	data : 0.11487107276916504
	model : 0.06500258445739746
			 train-loss:  2.1859263248657914 	 ± 0.18561617135905253
	data : 0.11480493545532226
	model : 0.06505069732666016
			 train-loss:  2.1848203500111896 	 ± 0.18487674536972726
	data : 0.11470131874084473
	model : 0.06505613327026367
			 train-loss:  2.1864768253577935 	 ± 0.18452849375452504
	data : 0.11466221809387207
	model : 0.06505460739135742
			 train-loss:  2.182675988777824 	 ± 0.18707021847092434
	data : 0.11466794013977051
	model : 0.06503963470458984
			 train-loss:  2.18394075926914 	 ± 0.1864568066161706
	data : 0.11466574668884277
	model : 0.06503300666809082
			 train-loss:  2.1845056604831776 	 ± 0.18554235558760193
	data : 0.1148031234741211
	model : 0.06498403549194336
			 train-loss:  2.188347460094251 	 ± 0.18828428774105388
	data : 0.11478571891784668
	model : 0.06498231887817382
			 train-loss:  2.1922665213545165 	 ± 0.1911564828920609
	data : 0.11481986045837403
	model : 0.06500458717346191
			 train-loss:  2.1910372566931025 	 ± 0.19054961678746604
	data : 0.1148714542388916
	model : 0.06500167846679687
			 train-loss:  2.1948307339026005 	 ± 0.1932214542622594
	data : 0.11495490074157715
	model : 0.0650108814239502
			 train-loss:  2.1909816048362036 	 ± 0.1959830650502815
	data : 0.11472296714782715
	model : 0.06506462097167968
			 train-loss:  2.195367982387543 	 ± 0.19982507015613418
	data : 0.11482486724853516
	model : 0.0649876594543457
			 train-loss:  2.1931843675009093 	 ± 0.20002882028533356
	data : 0.11483244895935059
	model : 0.0650111198425293
			 train-loss:  2.193110539632685 	 ± 0.19904725458818548
	data : 0.11496624946594239
	model : 0.06501898765563965
			 train-loss:  2.2009102522748187 	 ± 0.21316747870734373
	data : 0.11498584747314453
	model : 0.06498532295227051
			 train-loss:  2.2015579354304533 	 ± 0.21224197307082246
	data : 0.11518259048461914
	model : 0.06493520736694336
			 train-loss:  2.2045348950794765 	 ± 0.2133994315831773
	data : 0.11507468223571778
	model : 0.06500344276428223
			 train-loss:  2.206136893551305 	 ± 0.21302387918735502
	data : 0.11510491371154785
	model : 0.06495647430419922
			 train-loss:  2.205028675426947 	 ± 0.2123328808924288
	data : 0.1150827407836914
	model : 0.06501684188842774
			 train-loss:  2.202549107648708 	 ± 0.21289823789652346
	data : 0.11489424705505372
	model : 0.06507287025451661
			 train-loss:  2.2008683145593064 	 ± 0.21263803757393898
	data : 0.11479501724243164
	model : 0.0650965690612793
			 train-loss:  2.198404178836129 	 ± 0.2132269549785672
	data : 0.11487851142883301
	model : 0.06510825157165527
			 train-loss:  2.1992177157788664 	 ± 0.21243572152194343
	data : 0.11495747566223144
	model : 0.06508455276489258
			 train-loss:  2.1998896119850024 	 ± 0.2116036606317729
	data : 0.1148672103881836
	model : 0.06501355171203613
			 train-loss:  2.200307081230974 	 ± 0.21071160353155072
	data : 0.11503543853759765
	model : 0.06498932838439941
			 train-loss:  2.1994362766282602 	 ± 0.20998952257807224
	data : 0.11507039070129395
	model : 0.06498823165893555
			 train-loss:  2.1979872091956763 	 ± 0.2096462172052375
	data : 0.11505446434020997
	model : 0.06503262519836425
			 train-loss:  2.1988250191869407 	 ± 0.20893387775863648
	data : 0.11500349044799804
	model : 0.06505823135375977
			 train-loss:  2.1992812329887323 	 ± 0.20809709942468965
	data : 0.11509966850280762
	model : 0.0650726318359375
			 train-loss:  2.2044782143528177 	 ± 0.21470310737061538
	data : 0.11492061614990234
	model : 0.06505942344665527
			 train-loss:  2.2044151159895566 	 ± 0.21380018897708347
	data : 0.11490335464477539
	model : 0.06503653526306152
			 train-loss:  2.203971692919731 	 ± 0.2129624334434555
	data : 0.1149606704711914
	model : 0.06496543884277343
			 train-loss:  2.20237225047813 	 ± 0.21280311517284659
	data : 0.11500740051269531
	model : 0.06496853828430176
			 train-loss:  2.2025479713424305 	 ± 0.21193799130858534
	data : 0.11491403579711915
	model : 0.06498880386352539
			 train-loss:  2.2041417534758403 	 ± 0.21180751820896734
	data : 0.1148949146270752
	model : 0.06501359939575195
			 train-loss:  2.2036653632117855 	 ± 0.21101787977986972
	data : 0.11491885185241699
	model : 0.06500191688537597
			 train-loss:  2.2016872987747194 	 ± 0.21132320470745367
	data : 0.11482205390930175
	model : 0.06508493423461914
			 train-loss:  2.2002314981960116 	 ± 0.21111132533640095
	data : 0.1149416446685791
	model : 0.06514248847961426
			 train-loss:  2.199871668665428 	 ± 0.21031732395496974
	data : 0.11492323875427246
	model : 0.06509785652160645
			 train-loss:  2.200648154132068 	 ± 0.20967683603351248
	data : 0.114984130859375
	model : 0.06512255668640136
			 train-loss:  2.1982580598934676 	 ± 0.21060572859295257
	data : 0.11509757041931153
	model : 0.06513805389404297
			 train-loss:  2.1958551636108985 	 ± 0.21156185104019104
	data : 0.11525506973266601
	model : 0.06511459350585938
			 train-loss:  2.1954342345245013 	 ± 0.21080745499167544
	data : 0.11506638526916504
	model : 0.06505169868469238
			 train-loss:  2.195329663428393 	 ± 0.2100108343792908
	data : 0.11508283615112305
	model : 0.06507086753845215
			 train-loss:  2.196521890790839 	 ± 0.20966774429478283
	data : 0.1150893211364746
	model : 0.06505250930786133
			 train-loss:  2.197942499794177 	 ± 0.20952544088192115
	data : 0.11501235961914062
	model : 0.06503071784973144
			 train-loss:  2.199546230280841 	 ± 0.2095718491246932
	data : 0.11465978622436523
	model : 0.06498680114746094
			 train-loss:  2.202689725686522 	 ± 0.21197034504090886
	data : 0.11469640731811523
	model : 0.06497211456298828
			 train-loss:  2.202375407636601 	 ± 0.21122712133814317
	data : 0.11471190452575683
	model : 0.06498856544494629
			 train-loss:  2.199346088844797 	 ± 0.21342634268854555
	data : 0.11478800773620605
	model : 0.0649658203125
			 train-loss:  2.200503832144703 	 ± 0.21309169652281879
	data : 0.11473913192749023
	model : 0.0649721622467041
			 train-loss:  2.1982263922691345 	 ± 0.21402028773821813
	data : 0.11493144035339356
	model : 0.0650557518005371
			 train-loss:  2.1971968589945043 	 ± 0.213607628793195
	data : 0.11498780250549316
	model : 0.06510205268859863
			 train-loss:  2.197712794156142 	 ± 0.21294230733702255
	data : 0.11501655578613282
	model : 0.06509284973144532
			 train-loss:  2.196659189837796 	 ± 0.21256755165498611
	data : 0.11483354568481445
	model : 0.06510229110717773
			 train-loss:  2.197953369882372 	 ± 0.21239277419090796
	data : 0.1148310661315918
	model : 0.0651017665863037
			 train-loss:  2.200699571083332 	 ± 0.21420918688789928
	data : 0.11479454040527344
	model : 0.06501989364624024
			 train-loss:  2.1977836105921496 	 ± 0.21634278516228136
	data : 0.11474270820617676
	model : 0.0650439739227295
			 train-loss:  2.1974382400512695 	 ± 0.21564605205529241
	data : 0.11472368240356445
	model : 0.06506457328796386
			 train-loss:  2.195746469336587 	 ± 0.21589288033853146
	data : 0.11494536399841308
	model : 0.06507983207702636
			 train-loss:  2.1963200513148466 	 ± 0.21528030614178348
	data : 0.1149672508239746
	model : 0.06512255668640136
			 train-loss:  2.1958614214261374 	 ± 0.21463452737758165
	data : 0.11505002975463867
	model : 0.06513452529907227
			 train-loss:  2.1940483375890367 	 ± 0.21507204683574635
	data : 0.11506237983703613
	model : 0.06508173942565917
			 train-loss:  2.194755801244786 	 ± 0.21453961374443492
	data : 0.1149521827697754
	model : 0.06503605842590332
			 train-loss:  2.1967524741989335 	 ± 0.21524960634559218
	data : 0.1148451328277588
	model : 0.06500377655029296
			 train-loss:  2.1983011504272363 	 ± 0.21540308317226248
	data : 0.11501779556274414
	model : 0.06497011184692383
			 train-loss:  2.1961136602586318 	 ± 0.21641638245130332
	data : 0.11501073837280273
	model : 0.06500010490417481
			 train-loss:  2.1979556649159164 	 ± 0.21693716043861183
	data : 0.11494746208190917
	model : 0.06501584053039551
			 train-loss:  2.1972796385455284 	 ± 0.2164099562058343
	data : 0.11498479843139649
	model : 0.06505513191223145
			 train-loss:  2.1970830884160875 	 ± 0.2157380848757071
	data : 0.1150932788848877
	model : 0.06506032943725586
			 train-loss:  2.196731525397151 	 ± 0.21510399054056112
	data : 0.1148879051208496
	model : 0.06508002281188965
			 train-loss:  2.19642403870821 	 ± 0.21446578770625535
	data : 0.11476354598999024
	model : 0.0650439739227295
			 train-loss:  2.196952089759874 	 ± 0.21390301860174984
	data : 0.11486349105834961
	model : 0.06517190933227539
			 train-loss:  2.1974242513562428 	 ± 0.21332594535495253
	data : 0.11478853225708008
	model : 0.06514840126037598
			 train-loss:  2.199093666544721 	 ± 0.21372939539261146
	data : 0.1147979736328125
	model : 0.06514720916748047
			 train-loss:  2.200377759410114 	 ± 0.21370654206023829
	data : 0.11484460830688477
	model : 0.06516942977905274
			 train-loss:  2.200416069319754 	 ± 0.21305852714417145
	data : 0.11497292518615723
	model : 0.06518874168395997
			 train-loss:  2.2031719842589044 	 ± 0.2153454541502565
	data : 0.11488189697265624
	model : 0.06517534255981446
			 train-loss:  2.205471025969454 	 ± 0.2167334455299564
	data : 0.11484966278076172
	model : 0.06514883041381836
			 train-loss:  2.205220196928297 	 ± 0.21611175294384263
	data : 0.11465907096862793
	model : 0.06512966156005859
			 train-loss:  2.207413236065024 	 ± 0.21733825177909216
	data : 0.11455826759338379
	model : 0.06508326530456543
			 train-loss:  2.206142642918755 	 ± 0.2173266944768874
	data : 0.11454229354858399
	model : 0.06509451866149903
			 train-loss:  2.2057787479712947 	 ± 0.21674224185715885
	data : 0.11461200714111328
	model : 0.06498074531555176
			 train-loss:  2.2045187797657277 	 ± 0.2167384184961834
	data : 0.11477417945861816
	model : 0.06503639221191407
			 train-loss:  2.2045652673423635 	 ± 0.21611195902904531
	data : 0.11481742858886719
	model : 0.06509552001953126
			 train-loss:  2.202566453780251 	 ± 0.2170878672186822
	data : 0.11496844291687011
	model : 0.06509814262390137
			 train-loss:  2.202267528261457 	 ± 0.21650263777557693
	data : 0.11487274169921875
	model : 0.06510767936706544
			 train-loss:  2.2036046385765076 	 ± 0.21661011580948955
	data : 0.1149184226989746
	model : 0.06507534980773926
			 train-loss:  2.2018443251733726 	 ± 0.2172561382980639
	data : 0.11486268043518066
	model : 0.06507091522216797
			 train-loss:  2.1997997091057595 	 ± 0.21834605631825202
	data : 0.11502509117126465
	model : 0.06505389213562011
			 train-loss:  2.2007350681880333 	 ± 0.21809262051467262
	data : 0.11504898071289063
	model : 0.0650702953338623
			 train-loss:  2.200509614414639 	 ± 0.21750688027948015
	data : 0.11503357887268066
	model : 0.0650547981262207
			 train-loss:  2.199246611384397 	 ± 0.21756607670711606
	data : 0.1148791790008545
	model : 0.0651216983795166
			 train-loss:  2.1973051075096968 	 ± 0.21853417004916526
	data : 0.11497173309326172
	model : 0.06511931419372559
			 train-loss:  2.1967351026222355 	 ± 0.21807188740734018
	data : 0.11477055549621581
	model : 0.06513209342956543
			 train-loss:  2.1962741801272267 	 ± 0.21756785922031374
	data : 0.11458802223205566
	model : 0.06510481834411622
			 train-loss:  2.195284779651745 	 ± 0.21739370820668383
	data : 0.1147188663482666
	model : 0.06513776779174804
			 train-loss:  2.194022681764377 	 ± 0.21748706413172642
	data : 0.11477627754211425
	model : 0.06509208679199219
			 train-loss:  2.195560574531555 	 ± 0.21791647434430386
	data : 0.11482052803039551
	model : 0.06507158279418945
			 train-loss:  2.196067967947493 	 ± 0.21744686481672224
	data : 0.11484198570251465
	model : 0.06507201194763183
			 train-loss:  2.1982035756741882 	 ± 0.21883874864387035
	data : 0.11500711441040039
	model : 0.06507530212402343
			 train-loss:  2.1965898331842926 	 ± 0.2193867141233984
	data : 0.11506519317626954
	model : 0.06504764556884765
			 train-loss:  2.1962884165229597 	 ± 0.21885109071959716
	data : 0.11491904258728028
	model : 0.06503176689147949
			 train-loss:  2.197030913705627 	 ± 0.21852148936116372
	data : 0.1148636817932129
	model : 0.06501169204711914
			 train-loss:  2.1979638041609926 	 ± 0.21833762416384994
	data : 0.1150503158569336
	model : 0.06496162414550781
			 train-loss:  2.198721554475961 	 ± 0.21802845569043686
	data : 0.11511907577514649
	model : 0.06506824493408203
			 train-loss:  2.201303850687467 	 ± 0.2204229395152283
	data : 0.11502032279968262
	model : 0.06507372856140137
			 train-loss:  2.2015607071166134 	 ± 0.21988917274217937
	data : 0.1152113914489746
	model : 0.06512589454650879
			 train-loss:  2.1999553750614225 	 ± 0.22047884324379136
	data : 0.11511626243591308
	model : 0.06519069671630859
			 train-loss:  2.199960966302891 	 ± 0.21992138772332773
	data : 0.11511473655700684
	model : 0.06520781517028809
			 train-loss:  2.2002446256091246 	 ± 0.2194044349551422
	data : 0.11494479179382325
	model : 0.0651087760925293
			 train-loss:  2.200337532758713 	 ± 0.21885916079420975
	data : 0.114833402633667
	model : 0.0650589942932129
			 train-loss:  2.201670028677034 	 ± 0.21912584558279485
	data : 0.11476550102233887
	model : 0.0649986743927002
			 train-loss:  2.201304296455761 	 ± 0.2186442734887008
	data : 0.11492319107055664
	model : 0.0649864673614502
			 train-loss:  2.2003566349668455 	 ± 0.21852055448984437
	data : 0.11493086814880371
	model : 0.06502408981323242
			 train-loss:  2.2004247027284958 	 ± 0.21798646427697613
	data : 0.11492409706115722
	model : 0.06505622863769531
			 train-loss:  2.1993859756283642 	 ± 0.21795965135472978
	data : 0.115073823928833
	model : 0.06508097648620606
			 train-loss:  2.2016118313502338 	 ± 0.2197531680935193
	data : 0.11517844200134278
	model : 0.06508479118347169
			 train-loss:  2.200076799461807 	 ± 0.22032604328955782
	data : 0.1150784969329834
	model : 0.0650761604309082
			 train-loss:  2.199006398709921 	 ± 0.22033464249685825
	data : 0.11486244201660156
	model : 0.06509709358215332
			 train-loss:  2.198023073410874 	 ± 0.22026391408583718
	data : 0.11494722366333007
	model : 0.06507711410522461
			 train-loss:  2.1988391121228537 	 ± 0.2200553100289234
	data : 0.11507148742675781
	model : 0.06505675315856933
			 train-loss:  2.199896568935629 	 ± 0.22006741095402602
	data : 0.11502556800842285
	model : 0.06508145332336426
			 train-loss:  2.2004286258850456 	 ± 0.2196837599739897
	data : 0.11508278846740723
	model : 0.06507296562194824
			 train-loss:  2.1997275279721182 	 ± 0.21940506662729323
	data : 0.11516928672790527
	model : 0.06506009101867676
			 train-loss:  2.1985124110061434 	 ± 0.2196090459869639
	data : 0.11512207984924316
	model : 0.06507310867309571
			 train-loss:  2.199752260363379 	 ± 0.21984718018885258
	data : 0.11498885154724121
	model : 0.06507892608642578
			 train-loss:  2.1994420648724944 	 ± 0.21938483840403075
	data : 0.11483516693115234
	model : 0.06507091522216797
			 train-loss:  2.201545358802866 	 ± 0.22105081323649572
	data : 0.11474475860595704
	model : 0.06503582000732422
			 train-loss:  2.200328406937625 	 ± 0.22127062299602202
	data : 0.11488075256347656
	model : 0.06502904891967773
			 train-loss:  2.199294733674559 	 ± 0.22129178146360706
	data : 0.1148381233215332
	model : 0.06502833366394042
			 train-loss:  2.199291320280595 	 ± 0.22078827855445168
	data : 0.11489715576171874
	model : 0.06501049995422363
			 train-loss:  2.200090955285465 	 ± 0.22060724996726572
	data : 0.11499600410461426
	model : 0.06495046615600586
			 train-loss:  2.2012295508169912 	 ± 0.22075968801598614
	data : 0.11500253677368164
	model : 0.06488828659057617
			 train-loss:  2.200380072999963 	 ± 0.22062750499166572
	data : 0.11492509841918945
	model : 0.06476802825927734
			 train-loss:  2.2001694376979555 	 ± 0.22015695325314533
	data : 0.11489205360412598
	model : 0.06462945938110351
			 train-loss:  2.1996969159444175 	 ± 0.21978098170496796
	data : 0.11486868858337403
	model : 0.06445512771606446
			 train-loss:  2.199409107191373 	 ± 0.21933669197112435
	data : 0.11507935523986816
	model : 0.06430487632751465
			 train-loss:  2.199518798206346 	 ± 0.21885925069233791
	data : 0.11536974906921386
	model : 0.06418776512145996
			 train-loss:  2.198450172679466 	 ± 0.21897148601515412
	data : 0.11551961898803711
	model : 0.0641016960144043
			 train-loss:  2.1973542521614173 	 ± 0.21911861414961006
	data : 0.11574363708496094
	model : 0.06403570175170899
			 train-loss:  2.1982860689577848 	 ± 0.2190959874991702
	data : 0.11597633361816406
	model : 0.06399736404418946
			 train-loss:  2.1981158690019087 	 ± 0.21863647674896738
	data : 0.1158836841583252
	model : 0.06399021148681641
			 train-loss:  2.197066714537555 	 ± 0.21874673426122715
	data : 0.11571011543273926
	model : 0.06396183967590333
			 train-loss:  2.197583666686848 	 ± 0.2184187902602972
	data : 0.11571660041809081
	model : 0.06388554573059083
			 train-loss:  2.1961055708746624 	 ± 0.21911627878069756
	data : 0.11583409309387208
	model : 0.06392521858215332
			 train-loss:  2.1949633177290573 	 ± 0.21934663701500193
	data : 0.11575689315795898
	model : 0.06394233703613281
			 train-loss:  2.1951467359470107 	 ± 0.21889948532741962
	data : 0.1157829761505127
	model : 0.06394195556640625
			 train-loss:  2.1954787818691397 	 ± 0.21849673536999287
	data : 0.11583356857299805
	model : 0.06395463943481446
			 train-loss:  2.196248756236389 	 ± 0.21835919860916828
	data : 0.11584806442260742
	model : 0.06396818161010742
			 train-loss:  2.19590351322206 	 ± 0.2179669848544133
	data : 0.11578788757324218
	model : 0.06390461921691895
			 train-loss:  2.19606114278237 	 ± 0.21752606336836963
	data : 0.11560220718383789
	model : 0.06388540267944336
			 train-loss:  2.1963514732622014 	 ± 0.21712088737178356
	data : 0.11564822196960449
	model : 0.06385045051574707
			 train-loss:  2.1959925973710934 	 ± 0.2167434406833087
	data : 0.11567921638488769
	model : 0.06383004188537597
			 train-loss:  2.196817897965388 	 ± 0.2166777017405783
	data : 0.11574230194091797
	model : 0.06383652687072754
			 train-loss:  2.195648593492195 	 ± 0.21700013570561302
	data : 0.11569557189941407
	model : 0.06385469436645508
			 train-loss:  2.19556545773331 	 ± 0.21656071914162206
	data : 0.11587519645690918
	model : 0.06392536163330079
			 train-loss:  2.195982838549265 	 ± 0.21621882719970373
	data : 0.11588773727416993
	model : 0.0639491081237793
			 train-loss:  2.195445666429002 	 ± 0.2159451131272105
	data : 0.11591105461120606
	model : 0.06400794982910156
			 train-loss:  2.1962732600588954 	 ± 0.21590143936278117
	data : 0.11575436592102051
	model : 0.06399602890014648
			 train-loss:  2.195513241262321 	 ± 0.21579963129798052
	data : 0.11569418907165527
	model : 0.0639772891998291
			 train-loss:  2.1944715576171876 	 ± 0.21599396761064668
	data : 0.11585049629211426
	model : 0.06390891075134278
			 train-loss:  2.1956424304688595 	 ± 0.21635678810033
	data : 0.11586418151855468
	model : 0.06392135620117187
			 train-loss:  2.194523409245506 	 ± 0.21665366161487346
	data : 0.11580824851989746
	model : 0.06389899253845215
			 train-loss:  2.196064499527098 	 ± 0.21760461836542022
	data : 0.1158712387084961
	model : 0.06391716003417969
			 train-loss:  2.195031984584538 	 ± 0.2177959266856337
	data : 0.11600112915039062
	model : 0.0639181137084961
			 train-loss:  2.1953549674912995 	 ± 0.21742939676314718
	data : 0.11574559211730957
	model : 0.06392993927001953
			 train-loss:  2.194059514440596 	 ± 0.21798810319206086
	data : 0.11539387702941895
	model : 0.05548152923583984
#epoch  35    val-loss:  2.4533668317292867  train-loss:  2.194059514440596  lr:  9.765625e-06
			 train-loss:  2.4191582202911377 	 ± 0.0
	data : 6.198307991027832
	model : 0.07085585594177246
			 train-loss:  2.5073009729385376 	 ± 0.0881427526473999
	data : 3.164697766304016
	model : 0.07027018070220947
			 train-loss:  2.3843531608581543 	 ± 0.1881801235170184
	data : 2.1489561398824057
	model : 0.0683903694152832
			 train-loss:  2.311993658542633 	 ± 0.20558820951409154
	data : 1.6404098868370056
	model : 0.06749933958053589
			 train-loss:  2.2654516220092775 	 ± 0.2061015627588905
	data : 1.3352390766143798
	model : 0.06696038246154785
			 train-loss:  2.2422062953313193 	 ± 0.19519205381249266
	data : 0.11848950386047363
	model : 0.06575822830200195
			 train-loss:  2.193620358194624 	 ± 0.21638079704281032
	data : 0.11521906852722168
	model : 0.06477246284484864
			 train-loss:  2.180863603949547 	 ± 0.2052004181644293
	data : 0.11456303596496582
	model : 0.06480093002319336
			 train-loss:  2.176527910762363 	 ± 0.19385308465393677
	data : 0.11447587013244628
	model : 0.06481828689575195
			 train-loss:  2.1653356671333315 	 ± 0.1869452150870039
	data : 0.11447792053222657
	model : 0.06484732627868653
			 train-loss:  2.167746966535395 	 ± 0.17840829438728065
	data : 0.11448054313659668
	model : 0.06484775543212891
			 train-loss:  2.1806093951066337 	 ± 0.17605942832913765
	data : 0.1144979476928711
	model : 0.06488323211669922
			 train-loss:  2.1883008755170383 	 ± 0.17123799261755349
	data : 0.11453924179077149
	model : 0.06492986679077148
			 train-loss:  2.1758576716695512 	 ± 0.17099948122439299
	data : 0.11463894844055175
	model : 0.0649031162261963
			 train-loss:  2.173424013455709 	 ± 0.16545196311710134
	data : 0.11469821929931641
	model : 0.06490497589111328
			 train-loss:  2.1822498962283134 	 ± 0.1638044510806487
	data : 0.11468725204467774
	model : 0.06488323211669922
			 train-loss:  2.1732171914156746 	 ± 0.16296928342779984
	data : 0.11452865600585938
	model : 0.06484465599060059
			 train-loss:  2.1644871830940247 	 ± 0.16241647026789888
	data : 0.11454558372497559
	model : 0.06480426788330078
			 train-loss:  2.16265732991068 	 ± 0.15827509640381923
	data : 0.11443252563476562
	model : 0.06482019424438476
			 train-loss:  2.1654101431369783 	 ± 0.15473344004149564
	data : 0.11432428359985351
	model : 0.06485095024108886
			 train-loss:  2.1617071004140946 	 ± 0.15190974921105113
	data : 0.1144493579864502
	model : 0.06494293212890626
			 train-loss:  2.1589428023858503 	 ± 0.1489567220172524
	data : 0.11450228691101075
	model : 0.06498298645019532
			 train-loss:  2.159001428148021 	 ± 0.14568280759516472
	data : 0.11452546119689941
	model : 0.06501364707946777
			 train-loss:  2.1542021284500756 	 ± 0.1444608394301447
	data : 0.11456565856933594
	model : 0.06503505706787109
			 train-loss:  2.154152817726135 	 ± 0.14154234391463025
	data : 0.11450209617614746
	model : 0.06502265930175781
			 train-loss:  2.171968996524811 	 ± 0.1649214761359444
	data : 0.11439709663391114
	model : 0.06496605873107911
			 train-loss:  2.1630704093862465 	 ± 0.16807892683171133
	data : 0.11447548866271973
	model : 0.06499743461608887
			 train-loss:  2.162346214056015 	 ± 0.16509312126827072
	data : 0.11451253890991211
	model : 0.06501145362854004
			 train-loss:  2.15838295426862 	 ± 0.16357167731550395
	data : 0.11456670761108398
	model : 0.06501598358154297
			 train-loss:  2.1581884264945983 	 ± 0.1608257894630239
	data : 0.11470208168029786
	model : 0.06503243446350097
			 train-loss:  2.150339357314571 	 ± 0.16394760882234444
	data : 0.11471328735351563
	model : 0.0650515079498291
			 train-loss:  2.151595614850521 	 ± 0.16151711684912806
	data : 0.11468486785888672
	model : 0.06497035026550294
			 train-loss:  2.1456313530604043 	 ± 0.16259015245409006
	data : 0.11457648277282714
	model : 0.0649792194366455
			 train-loss:  2.135444767334882 	 ± 0.17053544124717857
	data : 0.11469950675964355
	model : 0.0649409294128418
			 train-loss:  2.1302059377942766 	 ± 0.1708348740846923
	data : 0.11487922668457032
	model : 0.06488246917724609
			 train-loss:  2.1271404027938843 	 ± 0.16941896237338813
	data : 0.11494903564453125
	model : 0.06486129760742188
			 train-loss:  2.136110563535948 	 ± 0.17556688208601318
	data : 0.11496334075927735
	model : 0.06492290496826172
			 train-loss:  2.137939879768773 	 ± 0.17359837561230504
	data : 0.1150479793548584
	model : 0.06490492820739746
			 train-loss:  2.13713970551124 	 ± 0.17142928136198968
	data : 0.11490941047668457
	model : 0.06496639251708984
			 train-loss:  2.1336500376462935 	 ± 0.170669950594939
	data : 0.11471881866455078
	model : 0.06502017974853516
			 train-loss:  2.129847032267873 	 ± 0.17028300369856061
	data : 0.11458077430725097
	model : 0.06500740051269531
			 train-loss:  2.1335158745447793 	 ± 0.169875806903225
	data : 0.11457457542419433
	model : 0.0649984359741211
			 train-loss:  2.131888361864312 	 ± 0.16821987955367082
	data : 0.11464438438415528
	model : 0.06498904228210449
			 train-loss:  2.1303559108213945 	 ± 0.16660064411378314
	data : 0.11480674743652344
	model : 0.06495695114135742
			 train-loss:  2.134993977016873 	 ± 0.1675872685530478
	data : 0.1147305965423584
	model : 0.0649491310119629
			 train-loss:  2.129604274811952 	 ± 0.16965299419144986
	data : 0.11475944519042969
	model : 0.06500201225280762
			 train-loss:  2.1297206396752215 	 ± 0.16784032706668944
	data : 0.11477231979370117
	model : 0.06501798629760742
			 train-loss:  2.130441926419735 	 ± 0.16615638577532446
	data : 0.11472740173339843
	model : 0.06502957344055176
			 train-loss:  2.126622438430786 	 ± 0.1665675975162681
	data : 0.11447505950927735
	model : 0.06505241394042968
			 train-loss:  2.126980018615723 	 ± 0.16491250579425126
	data : 0.11462445259094238
	model : 0.06503634452819824
			 train-loss:  2.125967086530199 	 ± 0.16344472642853008
	data : 0.11476855278015137
	model : 0.06500282287597656
			 train-loss:  2.1206737665029674 	 ± 0.16622101394212563
	data : 0.11487736701965331
	model : 0.06502323150634766
			 train-loss:  2.1237597870376876 	 ± 0.16614252539258267
	data : 0.11492390632629394
	model : 0.06500096321105957
			 train-loss:  2.126857841456378 	 ± 0.166135055562414
	data : 0.11501240730285645
	model : 0.06498928070068359
			 train-loss:  2.1275828751650723 	 ± 0.16470400516756734
	data : 0.11499695777893067
	model : 0.065028715133667
			 train-loss:  2.129613288811275 	 ± 0.16391989864353193
	data : 0.11496953964233399
	model : 0.06503195762634277
			 train-loss:  2.129672460388719 	 ± 0.16247624569509025
	data : 0.11482071876525879
	model : 0.06498551368713379
			 train-loss:  2.1321473080536415 	 ± 0.16214962529721838
	data : 0.11465764045715332
	model : 0.06499834060668945
			 train-loss:  2.130763255943686 	 ± 0.16111477333946655
	data : 0.11457018852233887
	model : 0.06499838829040527
			 train-loss:  2.1358325044314066 	 ± 0.16444292896121224
	data : 0.11451454162597656
	model : 0.06495237350463867
			 train-loss:  2.134671664628826 	 ± 0.1633371572191027
	data : 0.11454129219055176
	model : 0.06497044563293457
			 train-loss:  2.1347711970729213 	 ± 0.16201643235063762
	data : 0.11466751098632813
	model : 0.06500554084777832
			 train-loss:  2.142850081125895 	 ± 0.17285631951741295
	data : 0.1147587776184082
	model : 0.06500449180603027
			 train-loss:  2.1460407115519047 	 ± 0.17336029170666495
	data : 0.1147857666015625
	model : 0.06496844291687012
			 train-loss:  2.1469461624438946 	 ± 0.1721740240964512
	data : 0.11487202644348145
	model : 0.06495885848999024
			 train-loss:  2.143220769636559 	 ± 0.173484438805807
	data : 0.1147343635559082
	model : 0.06494855880737305
			 train-loss:  2.1494076305360936 	 ± 0.17937096235991504
	data : 0.11455559730529785
	model : 0.0648766040802002
			 train-loss:  2.154435040319667 	 ± 0.18274082965186916
	data : 0.11453170776367187
	model : 0.06487827301025391
			 train-loss:  2.158625528432321 	 ± 0.1846735761127926
	data : 0.11460890769958496
	model : 0.06489834785461426
			 train-loss:  2.1563145773751393 	 ± 0.18435188945194658
	data : 0.11460704803466797
	model : 0.06491675376892089
			 train-loss:  2.1575239074062296 	 ± 0.18332845384995028
	data : 0.11470627784729004
	model : 0.06492977142333985
			 train-loss:  2.1587966779867807 	 ± 0.1823665044457183
	data : 0.11478161811828613
	model : 0.06498966217041016
			 train-loss:  2.1583426978490126 	 ± 0.18115407318333362
	data : 0.11473608016967773
	model : 0.0649789810180664
			 train-loss:  2.1582737226743958 	 ± 0.17992686093939111
	data : 0.11464653015136719
	model : 0.06496133804321289
			 train-loss:  2.1608427143096924 	 ± 0.18008443968681542
	data : 0.1145505428314209
	model : 0.06496295928955079
			 train-loss:  2.1675418709453784 	 ± 0.18806805158976173
	data : 0.11460533142089843
	model : 0.0649418830871582
			 train-loss:  2.1671122018392985 	 ± 0.18688038275549682
	data : 0.11470265388488769
	model : 0.06491508483886718
			 train-loss:  2.1653092763362785 	 ± 0.18635134000156686
	data : 0.11477785110473633
	model : 0.0649383544921875
			 train-loss:  2.164611037773422 	 ± 0.18527080123198703
	data : 0.11487016677856446
	model : 0.06498885154724121
			 train-loss:  2.1636184364557267 	 ± 0.1843204797342757
	data : 0.11495776176452636
	model : 0.06498312950134277
			 train-loss:  2.1633713834079695 	 ± 0.1831924929761566
	data : 0.11484909057617188
	model : 0.06499056816101074
			 train-loss:  2.1634148533751323 	 ± 0.18207245941283157
	data : 0.1146923542022705
	model : 0.06497077941894532
			 train-loss:  2.166517892515803 	 ± 0.18314077688685165
	data : 0.11461997032165527
	model : 0.06496081352233887
			 train-loss:  2.16456633522397 	 ± 0.18291354177044497
	data : 0.11470003128051758
	model : 0.06493668556213379
			 train-loss:  2.1637360965504366 	 ± 0.1819935400066722
	data : 0.11472678184509277
	model : 0.06492700576782226
			 train-loss:  2.163266583930614 	 ± 0.1809841173522949
	data : 0.11480445861816406
	model : 0.06495094299316406
			 train-loss:  2.164383712856249 	 ± 0.18023895146181965
	data : 0.11494140625
	model : 0.0651031494140625
			 train-loss:  2.1605387817729604 	 ± 0.18276510524822298
	data : 0.11500625610351563
	model : 0.06514563560485839
			 train-loss:  2.163599450936478 	 ± 0.1839894708219803
	data : 0.11507515907287598
	model : 0.0651512622833252
			 train-loss:  2.1617545525232953 	 ± 0.18379041191197565
	data : 0.11499252319335937
	model : 0.06517367362976074
			 train-loss:  2.161695794744806 	 ± 0.1827786348562569
	data : 0.11487035751342774
	model : 0.06517462730407715
			 train-loss:  2.1594946164151896 	 ± 0.18299128540871296
	data : 0.11474242210388183
	model : 0.0651012897491455
			 train-loss:  2.16177212935622 	 ± 0.18331109637646054
	data : 0.1147653579711914
	model : 0.06506910324096679
			 train-loss:  2.1615007874813488 	 ± 0.18235220600249505
	data : 0.1146923542022705
	model : 0.06507725715637207
			 train-loss:  2.16021383812553 	 ± 0.18181856058020984
	data : 0.11509432792663574
	model : 0.06507458686828613
			 train-loss:  2.1606276718278727 	 ± 0.18091408032910783
	data : 0.11512985229492187
	model : 0.0650866985321045
			 train-loss:  2.158700543580596 	 ± 0.18096687487180801
	data : 0.11512298583984375
	model : 0.06506180763244629
			 train-loss:  2.1562926282688064 	 ± 0.1815963867424153
	data : 0.11558785438537597
	model : 0.06507568359375
			 train-loss:  2.1620334808272546 	 ± 0.18940424546348292
	data : 0.11556963920593262
	model : 0.06505846977233887
			 train-loss:  2.1617229533195497 	 ± 0.1884801709028472
	data : 0.1150352954864502
	model : 0.06504769325256347
			 train-loss:  2.1608407969522005 	 ± 0.1877521354483465
	data : 0.11491217613220214
	model : 0.06498732566833496
			 train-loss:  2.1624241927090813 	 ± 0.18750597059924032
	data : 0.11512079238891601
	model : 0.06498546600341797
			 train-loss:  2.17102801684037 	 ± 0.20583433873978185
	data : 0.11467843055725098
	model : 0.06493620872497559
			 train-loss:  2.1714534736596622 	 ± 0.20488786431485131
	data : 0.11510014533996582
	model : 0.06496400833129883
			 train-loss:  2.168847598348345 	 ± 0.2056342810671989
	data : 0.11526641845703126
	model : 0.06497154235839844
			 train-loss:  2.170112651474071 	 ± 0.20507212359879612
	data : 0.1155013084411621
	model : 0.06501326560974122
			 train-loss:  2.1711686849594116 	 ± 0.2044009647004014
	data : 0.11533880233764648
	model : 0.06501784324645996
			 train-loss:  2.1693238847785525 	 ± 0.20434543444283593
	data : 0.1153170108795166
	model : 0.06506009101867676
			 train-loss:  2.1694151491200158 	 ± 0.20340812143789566
	data : 0.1147921085357666
	model : 0.0650151252746582
			 train-loss:  2.1696504126895557 	 ± 0.20249632533967296
	data : 0.114711332321167
	model : 0.06498432159423828
			 train-loss:  2.17187031002732 	 ± 0.2029222101415743
	data : 0.1146130084991455
	model : 0.06492719650268555
			 train-loss:  2.1709569298795293 	 ± 0.20224334606761463
	data : 0.11466522216796875
	model : 0.06493091583251953
			 train-loss:  2.1700014129149174 	 ± 0.20160024941060425
	data : 0.11469011306762696
	model : 0.06493339538574219
			 train-loss:  2.168089797622279 	 ± 0.20174012701527028
	data : 0.11468405723571777
	model : 0.06497664451599121
			 train-loss:  2.1722861891207486 	 ± 0.20579766713458186
	data : 0.11467642784118652
	model : 0.0650033950805664
			 train-loss:  2.1717720504464775 	 ± 0.20498285154060147
	data : 0.11507511138916016
	model : 0.06507725715637207
			 train-loss:  2.1723580584566817 	 ± 0.2042025379541534
	data : 0.11511602401733398
	model : 0.06509079933166503
			 train-loss:  2.1736918829255183 	 ± 0.20384663635917857
	data : 0.11509261131286622
	model : 0.06509084701538086
			 train-loss:  2.1738980778125154 	 ± 0.203000688217778
	data : 0.1151115894317627
	model : 0.0651130199432373
			 train-loss:  2.1731300711631776 	 ± 0.20232661472599678
	data : 0.11511526107788086
	model : 0.06507172584533691
			 train-loss:  2.1766275197021234 	 ± 0.20509900546560877
	data : 0.11481742858886719
	model : 0.06502156257629395
			 train-loss:  2.1765791568599764 	 ± 0.20425739898701498
	data : 0.11474041938781739
	model : 0.06498827934265136
			 train-loss:  2.1762127488609253 	 ± 0.20346564406762982
	data : 0.11489429473876953
	model : 0.0649764060974121
			 train-loss:  2.1757246621193422 	 ± 0.20271584407585708
	data : 0.11501636505126953
	model : 0.06497564315795898
			 train-loss:  2.1742669630050657 	 ± 0.20255480661696149
	data : 0.11519317626953125
	model : 0.0650716781616211
			 train-loss:  2.177407447307829 	 ± 0.2047819749735934
	data : 0.11514248847961425
	model : 0.06515083312988282
			 train-loss:  2.1741198000945445 	 ± 0.20728565289807535
	data : 0.1151623249053955
	model : 0.06518921852111817
			 train-loss:  2.17693308647722 	 ± 0.20889425849749563
	data : 0.11502857208251953
	model : 0.06515893936157227
			 train-loss:  2.1776939057564553 	 ± 0.20826097489347675
	data : 0.11486229896545411
	model : 0.0651369571685791
			 train-loss:  2.179081569268153 	 ± 0.20805624651236615
	data : 0.11472725868225098
	model : 0.06505756378173828
			 train-loss:  2.1815642791849967 	 ± 0.209184760058795
	data : 0.11466140747070312
	model : 0.06497669219970703
			 train-loss:  2.181740192752896 	 ± 0.2084006135151307
	data : 0.11469054222106934
	model : 0.06494154930114746
			 train-loss:  2.1826925842385543 	 ± 0.207903820952029
	data : 0.11474180221557617
	model : 0.06500234603881835
			 train-loss:  2.1830074760451246 	 ± 0.20715844050511
	data : 0.11480340957641602
	model : 0.06499638557434081
			 train-loss:  2.182185585410507 	 ± 0.206608932447726
	data : 0.11483020782470703
	model : 0.06503543853759766
			 train-loss:  2.1822318180518994 	 ± 0.2058486402154617
	data : 0.1148913860321045
	model : 0.06505084037780762
			 train-loss:  2.1842749945438693 	 ± 0.20647544022183362
	data : 0.11504225730895996
	model : 0.06503481864929199
			 train-loss:  2.1843380815740945 	 ± 0.2057273058117308
	data : 0.11494603157043456
	model : 0.06502618789672851
			 train-loss:  2.1856922151373444 	 ± 0.2056022486612179
	data : 0.11486783027648925
	model : 0.06500115394592285
			 train-loss:  2.1840153736727577 	 ± 0.20581831694116115
	data : 0.11481027603149414
	model : 0.06498241424560547
			 train-loss:  2.183073824179088 	 ± 0.20538952712727526
	data : 0.11485500335693359
	model : 0.06497426033020019
			 train-loss:  2.1852621939820303 	 ± 0.20630808294253244
	data : 0.11482725143432618
	model : 0.06499533653259278
			 train-loss:  2.186023164462376 	 ± 0.20578535020151703
	data : 0.11485943794250489
	model : 0.06498179435729981
			 train-loss:  2.1873651701543064 	 ± 0.20569654777539106
	data : 0.11493339538574218
	model : 0.06507811546325684
			 train-loss:  2.189636152366112 	 ± 0.20678957699863995
	data : 0.1149226188659668
	model : 0.06506972312927246
			 train-loss:  2.187582682256829 	 ± 0.20755834470231366
	data : 0.11488156318664551
	model : 0.06511917114257812
			 train-loss:  2.1857414521327634 	 ± 0.208044131856425
	data : 0.11457247734069824
	model : 0.06514115333557129
			 train-loss:  2.186869924132888 	 ± 0.20779102345194117
	data : 0.11459388732910156
	model : 0.06512351036071777
			 train-loss:  2.1910118656670488 	 ± 0.21313463059958143
	data : 0.11458086967468262
	model : 0.06503024101257324
			 train-loss:  2.190934739112854 	 ± 0.21242508000794105
	data : 0.11474246978759765
	model : 0.06505751609802246
			 train-loss:  2.188263718655567 	 ± 0.21423288724184444
	data : 0.11465339660644532
	model : 0.06502814292907715
			 train-loss:  2.187489841329424 	 ± 0.21373866329593566
	data : 0.11481947898864746
	model : 0.06500930786132812
			 train-loss:  2.189880722488453 	 ± 0.21506860782590315
	data : 0.114833402633667
	model : 0.06500773429870606
			 train-loss:  2.1881085936125224 	 ± 0.2154869823720994
	data : 0.11497225761413574
	model : 0.06500434875488281
			 train-loss:  2.1877055098933558 	 ± 0.21484897633140446
	data : 0.11485333442687988
	model : 0.06499533653259278
			 train-loss:  2.1852388351391525 	 ± 0.216349901174556
	data : 0.11488089561462403
	model : 0.06497225761413575
			 train-loss:  2.181658982471296 	 ± 0.22024608518151056
	data : 0.11492047309875489
	model : 0.06498494148254394
			 train-loss:  2.180476462539238 	 ± 0.22004741471387204
	data : 0.11505851745605469
	model : 0.06500363349914551
			 train-loss:  2.183143820402757 	 ± 0.22190193568658942
	data : 0.11500840187072754
	model : 0.06498961448669434
			 train-loss:  2.182462126761675 	 ± 0.22137435357070523
	data : 0.11513314247131348
	model : 0.06494631767272949
			 train-loss:  2.1809735868288125 	 ± 0.2214875521391706
	data : 0.11522483825683594
	model : 0.06500558853149414
			 train-loss:  2.180545561843448 	 ± 0.22086967302820845
	data : 0.1153759479522705
	model : 0.06498475074768066
			 train-loss:  2.181634305444963 	 ± 0.22062673562055285
	data : 0.11516904830932617
	model : 0.06498560905456544
			 train-loss:  2.18108340516323 	 ± 0.22006548955043148
	data : 0.11518783569335937
	model : 0.06503992080688477
			 train-loss:  2.1808944897218185 	 ± 0.21941094935339936
	data : 0.1151510238647461
	model : 0.06509809494018555
			 train-loss:  2.1805612143263757 	 ± 0.21879096105789486
	data : 0.11507573127746581
	model : 0.06503539085388184
			 train-loss:  2.1803385861619504 	 ± 0.21815377266453015
	data : 0.1149515151977539
	model : 0.06502976417541503
			 train-loss:  2.1786925679161433 	 ± 0.2185411955694
	data : 0.1150275707244873
	model : 0.06498250961303711
			 train-loss:  2.177843078353701 	 ± 0.21817168324395703
	data : 0.11505756378173829
	model : 0.06496171951293946
			 train-loss:  2.1798896312713625 	 ± 0.2191500079002627
	data : 0.11512556076049804
	model : 0.06493577957153321
			 train-loss:  2.180702454862539 	 ± 0.21876513407872658
	data : 0.11515860557556153
	model : 0.06499080657958985
			 train-loss:  2.180679594361505 	 ± 0.21812846673083824
	data : 0.11508593559265137
	model : 0.065024995803833
			 train-loss:  2.1841902429657862 	 ± 0.22231697946374718
	data : 0.11506123542785644
	model : 0.06503701210021973
			 train-loss:  2.1841648090844865 	 ± 0.22167746946322223
	data : 0.11499056816101075
	model : 0.06502308845520019
			 train-loss:  2.184220882143293 	 ± 0.22104443537620969
	data : 0.11471977233886718
	model : 0.06500749588012696
			 train-loss:  2.183871475133029 	 ± 0.22046403344912344
	data : 0.11466193199157715
	model : 0.06500039100646973
			 train-loss:  2.1836722530214128 	 ± 0.21985625830099717
	data : 0.11480875015258789
	model : 0.06499347686767579
			 train-loss:  2.183057197024313 	 ± 0.21939046793085898
	data : 0.11496176719665527
	model : 0.0650594711303711
			 train-loss:  2.182598568207725 	 ± 0.21886233838345887
	data : 0.11509542465209961
	model : 0.06512298583984374
			 train-loss:  2.1830653773413764 	 ± 0.2183428818655048
	data : 0.11529607772827148
	model : 0.06514215469360352
			 train-loss:  2.1827415590128187 	 ± 0.2177822270622238
	data : 0.11542015075683594
	model : 0.06513509750366211
			 train-loss:  2.1837668222385447 	 ± 0.2176206786813644
	data : 0.1154219627380371
	model : 0.06527490615844726
			 train-loss:  2.1833122362856003 	 ± 0.2171119037716796
	data : 0.11502861976623535
	model : 0.06525993347167969
			 train-loss:  2.182661817125652 	 ± 0.21669982352648462
	data : 0.11481118202209473
	model : 0.0652092456817627
			 train-loss:  2.184806690989314 	 ± 0.21806299807152998
	data : 0.11466193199157715
	model : 0.06523432731628417
			 train-loss:  2.1850768968623173 	 ± 0.21750706915389614
	data : 0.11418228149414063
	model : 0.06523866653442383
			 train-loss:  2.182855051469038 	 ± 0.2190309136205114
	data : 0.11421899795532227
	model : 0.06510739326477051
			 train-loss:  2.180898126135481 	 ± 0.22008063182932586
	data : 0.11456413269042968
	model : 0.06525473594665528
			 train-loss:  2.181465503399965 	 ± 0.21963545362743042
	data : 0.11449031829833985
	model : 0.06526494026184082
			 train-loss:  2.183435330892864 	 ± 0.22072426420246216
	data : 0.11459112167358398
	model : 0.06526370048522949
			 train-loss:  2.1848342081639154 	 ± 0.2209885277692892
	data : 0.11498856544494629
	model : 0.06539998054504395
			 train-loss:  2.184124949077765 	 ± 0.22063013815171767
	data : 0.11476941108703613
	model : 0.06541604995727539
			 train-loss:  2.1848631535169374 	 ± 0.2202954189479619
	data : 0.11461272239685058
	model : 0.0652625560760498
			 train-loss:  2.1843291594810093 	 ± 0.21985211047390563
	data : 0.1147068977355957
	model : 0.06524167060852051
			 train-loss:  2.1848030151465 	 ± 0.21938696301763563
	data : 0.11466879844665527
	model : 0.06523022651672364
			 train-loss:  2.184405143163642 	 ± 0.21889710817417546
	data : 0.11475272178649902
	model : 0.06550569534301758
			 train-loss:  2.1843423770768995 	 ± 0.2183425931490568
	data : 0.11448874473571777
	model : 0.06549930572509766
			 train-loss:  2.1845850029377023 	 ± 0.2178171472840511
	data : 0.11453127861022949
	model : 0.06565241813659668
			 train-loss:  2.1834619644299225 	 ± 0.2178431011109529
	data : 0.11437382698059081
	model : 0.06572604179382324
			 train-loss:  2.184037877321243 	 ± 0.2174496311198255
	data : 0.1144533634185791
	model : 0.0656933307647705
			 train-loss:  2.185276068265165 	 ± 0.21761369431613056
	data : 0.11432576179504395
	model : 0.0653470516204834
			 train-loss:  2.1866135644440603 	 ± 0.21790101835112446
	data : 0.11453108787536621
	model : 0.06527619361877442
			 train-loss:  2.1858643111336993 	 ± 0.217624347977429
	data : 0.11440510749816894
	model : 0.06512312889099121
			 train-loss:  2.186439729204365 	 ± 0.21724505225609717
	data : 0.11471829414367676
	model : 0.06509528160095215
			 train-loss:  2.1872884180487655 	 ± 0.21705328112282452
	data : 0.11474013328552246
	model : 0.06524620056152344
			 train-loss:  2.1875373842646773 	 ± 0.21655515223036353
	data : 0.11465911865234375
	model : 0.06520743370056152
			 train-loss:  2.1878952323526577 	 ± 0.21609248475613194
	data : 0.11482806205749511
	model : 0.06530637741088867
			 train-loss:  2.1882445239103756 	 ± 0.21563097435556328
	data : 0.1149792194366455
	model : 0.06530475616455078
			 train-loss:  2.188306839272166 	 ± 0.21511636961746491
	data : 0.11493906974792481
	model : 0.06526947021484375
			 train-loss:  2.188990633828299 	 ± 0.2148311389408034
	data : 0.11478362083435059
	model : 0.06566972732543945
			 train-loss:  2.188830712395257 	 ± 0.21433398503296525
	data : 0.11436867713928223
	model : 0.06563024520874024
			 train-loss:  2.1876439800802268 	 ± 0.21452161164163938
	data : 0.11443881988525391
	model : 0.06558775901794434
			 train-loss:  2.1869866881572024 	 ± 0.214231320526575
	data : 0.1145249843597412
	model : 0.06561450958251953
			 train-loss:  2.1857765255687394 	 ± 0.21445869746164167
	data : 0.114589262008667
	model : 0.06565537452697753
			 train-loss:  2.1864412729130236 	 ± 0.21418024748323913
	data : 0.11464595794677734
	model : 0.06518969535827637
			 train-loss:  2.187589089075724 	 ± 0.21434565823412882
	data : 0.11525917053222656
	model : 0.06520771980285645
			 train-loss:  2.1890229838235036 	 ± 0.21488705199564218
	data : 0.11513447761535645
	model : 0.06515626907348633
			 train-loss:  2.1875281197215437 	 ± 0.21552155069524936
	data : 0.11500983238220215
	model : 0.06512393951416015
			 train-loss:  2.1877519991844214 	 ± 0.21505433513966998
	data : 0.11488299369812012
	model : 0.06558647155761718
			 train-loss:  2.1868470197374172 	 ± 0.2149825703171597
	data : 0.11443395614624023
	model : 0.06550960540771485
			 train-loss:  2.1871060866575975 	 ± 0.21453004918434349
	data : 0.11450181007385254
	model : 0.06540088653564453
			 train-loss:  2.1882316856770903 	 ± 0.21469939827422818
	data : 0.11468944549560547
	model : 0.06535816192626953
			 train-loss:  2.1881568822090935 	 ± 0.21422036800515867
	data : 0.11481270790100098
	model : 0.06530804634094238
			 train-loss:  2.188181098550558 	 ± 0.21374196859007344
	data : 0.11492199897766113
	model : 0.06469087600708008
			 train-loss:  2.1881181775199043 	 ± 0.21326853664973228
	data : 0.11544322967529297
	model : 0.0645437240600586
			 train-loss:  2.1873114884427163 	 ± 0.21313993659907535
	data : 0.11550087928771972
	model : 0.06439228057861328
			 train-loss:  2.186248406439626 	 ± 0.21326959191316916
	data : 0.11539635658264161
	model : 0.06426973342895508
			 train-loss:  2.1867566411955313 	 ± 0.21293910548666564
	data : 0.11555671691894531
	model : 0.06406078338623047
			 train-loss:  2.188147579218102 	 ± 0.2135091837882589
	data : 0.11568984985351563
	model : 0.06393709182739257
			 train-loss:  2.188504856565724 	 ± 0.21311312020291578
	data : 0.11584134101867676
	model : 0.06389045715332031
			 train-loss:  2.1883758765794497 	 ± 0.21266033241067658
	data : 0.11573243141174316
	model : 0.06393346786499024
			 train-loss:  2.189999183704113 	 ± 0.21363098618892282
	data : 0.11584954261779785
	model : 0.06391024589538574
			 train-loss:  2.189012670209991 	 ± 0.21370098414210156
	data : 0.11578774452209473
	model : 0.06392621994018555
			 train-loss:  2.188853435536735 	 ± 0.21325772115237257
	data : 0.11559505462646484
	model : 0.06390104293823243
			 train-loss:  2.1881324945612155 	 ± 0.21308906932118116
	data : 0.11556668281555176
	model : 0.06385068893432617
			 train-loss:  2.18815441303334 	 ± 0.21263739564342474
	data : 0.11574020385742187
	model : 0.06380839347839355
			 train-loss:  2.1884168446818486 	 ± 0.21222661536363524
	data : 0.11576266288757324
	model : 0.06381897926330567
			 train-loss:  2.1880830441202437 	 ± 0.21184262839618861
	data : 0.11573071479797363
	model : 0.06384592056274414
			 train-loss:  2.1887562130285607 	 ± 0.21165391396724328
	data : 0.11586503982543946
	model : 0.0639491081237793
			 train-loss:  2.1879845852653186 	 ± 0.2115491110791442
	data : 0.11585426330566406
	model : 0.06399712562561036
			 train-loss:  2.1879879127399557 	 ± 0.211109762566075
	data : 0.11573348045349122
	model : 0.06403164863586426
			 train-loss:  2.187004186890342 	 ± 0.2112259193618779
	data : 0.11566696166992188
	model : 0.0639988899230957
			 train-loss:  2.1862284416033897 	 ± 0.21113600690713308
	data : 0.11566243171691895
	model : 0.06400179862976074
			 train-loss:  2.187440235595234 	 ± 0.211547978137301
	data : 0.11569547653198242
	model : 0.06392502784729004
			 train-loss:  2.186824401057496 	 ± 0.21133485544986952
	data : 0.11575145721435547
	model : 0.06395483016967773
			 train-loss:  2.187763569316244 	 ± 0.21141656898184386
	data : 0.11577949523925782
	model : 0.06394572257995605
			 train-loss:  2.187667284417249 	 ± 0.21099357070136118
	data : 0.11573777198791504
	model : 0.06396617889404296
			 train-loss:  2.1885453746203454 	 ± 0.2110194899292716
	data : 0.1157045841217041
	model : 0.06393165588378906
			 train-loss:  2.1876547666917365 	 ± 0.21106184438745063
	data : 0.11566376686096191
	model : 0.06394128799438477
			 train-loss:  2.1887465777397157 	 ± 0.21134269454458401
	data : 0.11561007499694824
	model : 0.06388821601867675
			 train-loss:  2.187940048031598 	 ± 0.21130642665633576
	data : 0.11561102867126465
	model : 0.06393599510192871
			 train-loss:  2.1884706280534227 	 ± 0.21105421602218347
	data : 0.11574759483337402
	model : 0.06395320892333985
			 train-loss:  2.1885687904395605 	 ± 0.21064246381216184
	data : 0.11577939987182617
	model : 0.06399178504943848
			 train-loss:  2.1882443789422044 	 ± 0.2102907225252742
	data : 0.11584672927856446
	model : 0.06399760246276856
			 train-loss:  2.1875359446394675 	 ± 0.21018145717962392
	data : 0.11596360206604003
	model : 0.06400976181030274
			 train-loss:  2.1882336041890085 	 ± 0.21006617371996494
	data : 0.11551570892333984
	model : 0.05549626350402832
#epoch  36    val-loss:  2.438249374690809  train-loss:  2.1882336041890085  lr:  9.765625e-06
			 train-loss:  2.5285604000091553 	 ± 0.0
	data : 5.469604253768921
	model : 0.09026479721069336
			 train-loss:  2.3823543787002563 	 ± 0.14620602130889893
	data : 2.856947422027588
	model : 0.07758915424346924
			 train-loss:  2.4142876466115317 	 ± 0.12763333280784725
	data : 1.9427063465118408
	model : 0.07325450579325359
			 train-loss:  2.2396797835826874 	 ± 0.3219959910111116
	data : 1.4857463836669922
	model : 0.07114547491073608
			 train-loss:  2.2311538457870483 	 ± 0.2885063275613946
	data : 1.211402416229248
	model : 0.06983113288879395
			 train-loss:  2.2639316519101462 	 ± 0.27337734679441256
	data : 0.14038443565368652
	model : 0.06471271514892578
			 train-loss:  2.227083223206656 	 ± 0.26871092373844907
	data : 0.11457643508911133
	model : 0.06468095779418945
			 train-loss:  2.2129130512475967 	 ± 0.2541366205070995
	data : 0.11465444564819335
	model : 0.06472806930541992
			 train-loss:  2.2116030984454684 	 ± 0.23963094897476314
	data : 0.11463022232055664
	model : 0.06479382514953613
			 train-loss:  2.21434508562088 	 ± 0.22748265643543486
	data : 0.11477131843566894
	model : 0.0649035930633545
			 train-loss:  2.249218236316334 	 ± 0.24332145220343687
	data : 0.11476354598999024
	model : 0.0649482250213623
			 train-loss:  2.2338701585928598 	 ± 0.23845911612906343
	data : 0.11464600563049317
	model : 0.06502332687377929
			 train-loss:  2.2490385587398825 	 ± 0.2350524552604898
	data : 0.11466560363769532
	model : 0.06503143310546874
			 train-loss:  2.2920053090367998 	 ± 0.27441409197831124
	data : 0.11458044052124024
	model : 0.0649449348449707
			 train-loss:  2.2742755651474 	 ± 0.27328318842408567
	data : 0.11448612213134765
	model : 0.06492352485656738
			 train-loss:  2.2901562228798866 	 ± 0.27165952791735104
	data : 0.11463809013366699
	model : 0.06492676734924316
			 train-loss:  2.2874878224204567 	 ± 0.26376450324525574
	data : 0.11457176208496093
	model : 0.0648878574371338
			 train-loss:  2.283002025551266 	 ± 0.2569994131245443
	data : 0.11456179618835449
	model : 0.06490745544433593
			 train-loss:  2.2670994181382027 	 ± 0.259084008905577
	data : 0.11464800834655761
	model : 0.06500010490417481
			 train-loss:  2.2645646572113036 	 ± 0.25276545003862766
	data : 0.11473021507263184
	model : 0.0649787425994873
			 train-loss:  2.257623967670259 	 ± 0.2486190609300722
	data : 0.11449742317199707
	model : 0.06499295234680176
			 train-loss:  2.2554829987612637 	 ± 0.2431009777221634
	data : 0.11446266174316407
	model : 0.06499657630920411
			 train-loss:  2.241382236066072 	 ± 0.24678512229423508
	data : 0.11447415351867676
	model : 0.06497755050659179
			 train-loss:  2.2278387248516083 	 ± 0.25016812327529125
	data : 0.11449241638183594
	model : 0.06494760513305664
			 train-loss:  2.22469575881958 	 ± 0.24559683214252107
	data : 0.11445355415344238
	model : 0.06501688957214355
			 train-loss:  2.2228106260299683 	 ± 0.24101189091503403
	data : 0.11457996368408203
	model : 0.06501960754394531
			 train-loss:  2.225243330001831 	 ± 0.23683167102012428
	data : 0.11465034484863282
	model : 0.0650057315826416
			 train-loss:  2.2322409067835127 	 ± 0.23538933158307251
	data : 0.11464891433715821
	model : 0.06500844955444336
			 train-loss:  2.2358546092592437 	 ± 0.23208438049784597
	data : 0.11449275016784669
	model : 0.06498985290527344
			 train-loss:  2.2262596766153973 	 ± 0.23396055277786734
	data : 0.11458172798156738
	model : 0.0649268627166748
			 train-loss:  2.2224615158573275 	 ± 0.23109433983854555
	data : 0.11471877098083497
	model : 0.06492376327514648
			 train-loss:  2.23499146848917 	 ± 0.23791320149901707
	data : 0.11485404968261718
	model : 0.06492886543273926
			 train-loss:  2.2238679401802295 	 ± 0.24258382179901983
	data : 0.1148289680480957
	model : 0.06496753692626953
			 train-loss:  2.219898858491112 	 ± 0.24007496375598011
	data : 0.11504135131835938
	model : 0.06496272087097169
			 train-loss:  2.2171725307192123 	 ± 0.23715388052458367
	data : 0.1150540828704834
	model : 0.06496305465698242
			 train-loss:  2.212510257959366 	 ± 0.23545800868008498
	data : 0.11499042510986328
	model : 0.06500329971313476
			 train-loss:  2.207077628857381 	 ± 0.23453052506862163
	data : 0.11486830711364746
	model : 0.06500287055969238
			 train-loss:  2.1974488653634725 	 ± 0.23872046995665752
	data : 0.11487483978271484
	model : 0.0649946689605713
			 train-loss:  2.2062561236895046 	 ± 0.24181361035408733
	data : 0.11466407775878906
	model : 0.06495342254638672
			 train-loss:  2.2038056284189222 	 ± 0.23926171602335614
	data : 0.11454625129699707
	model : 0.06494712829589844
			 train-loss:  2.2031319112312504 	 ± 0.23636428776361015
	data : 0.11454591751098633
	model : 0.06489729881286621
			 train-loss:  2.1983261817977544 	 ± 0.2355520763707512
	data : 0.11467242240905762
	model : 0.06489853858947754
			 train-loss:  2.199636994406234 	 ± 0.23295193193648198
	data : 0.11464333534240723
	model : 0.06490149497985839
			 train-loss:  2.19996456395496 	 ± 0.2302995542627578
	data : 0.11479101181030274
	model : 0.06494908332824707
			 train-loss:  2.196541534529792 	 ± 0.2288554538860099
	data : 0.11491255760192871
	model : 0.06498417854309083
			 train-loss:  2.1927540691002556 	 ± 0.22777567035058013
	data : 0.11489019393920899
	model : 0.06501131057739258
			 train-loss:  2.192467438413742 	 ± 0.22534788235872766
	data : 0.11468682289123536
	model : 0.0650479793548584
			 train-loss:  2.19233280668656 	 ± 0.22299006384399217
	data : 0.11466794013977051
	model : 0.06499018669128417
			 train-loss:  2.1932677711759294 	 ± 0.22079796437475663
	data : 0.11458659172058105
	model : 0.06499133110046387
			 train-loss:  2.1957944416999817 	 ± 0.21929324062431602
	data : 0.1146608829498291
	model : 0.06496081352233887
			 train-loss:  2.1952781326630535 	 ± 0.2171633538706056
	data : 0.11467103958129883
	model : 0.0649599552154541
			 train-loss:  2.1915574234265547 	 ± 0.21670031892148775
	data : 0.11481308937072754
	model : 0.06494722366333008
			 train-loss:  2.1893623752414055 	 ± 0.21522908056327097
	data : 0.11488070487976074
	model : 0.06500158309936524
			 train-loss:  2.1983156402905784 	 ± 0.22296691253838
	data : 0.11492609977722168
	model : 0.06500654220581055
			 train-loss:  2.1966661604967985 	 ± 0.2212629003103047
	data : 0.11476173400878906
	model : 0.06496801376342773
			 train-loss:  2.198945422257696 	 ± 0.2199289906998017
	data : 0.11471271514892578
	model : 0.06494369506835937
			 train-loss:  2.1935869162542776 	 ± 0.22164870188134644
	data : 0.11474008560180664
	model : 0.06493010520935058
			 train-loss:  2.1943476015123826 	 ± 0.21980466925246403
	data : 0.11476278305053711
	model : 0.0649181842803955
			 train-loss:  2.1905988151744262 	 ± 0.21979605738451785
	data : 0.11469202041625977
	model : 0.06496310234069824
			 train-loss:  2.1884878555933636 	 ± 0.2185590261963616
	data : 0.11473393440246582
	model : 0.06504764556884765
			 train-loss:  2.1852902271708503 	 ± 0.21817070112787296
	data : 0.1146782398223877
	model : 0.06506071090698243
			 train-loss:  2.183942844790797 	 ± 0.21665982464036387
	data : 0.11462182998657226
	model : 0.06504583358764648
			 train-loss:  2.190324030225239 	 ± 0.22072831143786636
	data : 0.11450624465942383
	model : 0.06500868797302246
			 train-loss:  2.1952002495527267 	 ± 0.22239088443900934
	data : 0.11466059684753419
	model : 0.06496338844299317
			 train-loss:  2.1919682429387017 	 ± 0.22218315232511163
	data : 0.11463947296142578
	model : 0.0649658203125
			 train-loss:  2.1888683636983237 	 ± 0.22190537311052438
	data : 0.11478133201599121
	model : 0.06498675346374512
			 train-loss:  2.183186792615634 	 ± 0.22502785598603955
	data : 0.11491966247558594
	model : 0.06506953239440919
			 train-loss:  2.1817495910560383 	 ± 0.22367668197801832
	data : 0.11497340202331544
	model : 0.06512365341186524
			 train-loss:  2.177264261936796 	 ± 0.2251093143810415
	data : 0.1149559497833252
	model : 0.06515254974365234
			 train-loss:  2.178979938370841 	 ± 0.22394952752258668
	data : 0.11497230529785156
	model : 0.06510896682739258
			 train-loss:  2.1765618223539542 	 ± 0.22328527727004355
	data : 0.11479802131652832
	model : 0.06510567665100098
			 train-loss:  2.1759473515881433 	 ± 0.22178970670963877
	data : 0.11456284523010254
	model : 0.06504330635070801
			 train-loss:  2.180401093339267 	 ± 0.2234837977911841
	data : 0.11457061767578125
	model : 0.06500663757324218
			 train-loss:  2.18369557084264 	 ± 0.22374625544083201
	data : 0.11466584205627442
	model : 0.06497330665588379
			 train-loss:  2.181134901046753 	 ± 0.22333855139288897
	data : 0.1147392749786377
	model : 0.0649846076965332
			 train-loss:  2.1764890033947792 	 ± 0.22548307851169502
	data : 0.11480531692504883
	model : 0.06501545906066894
			 train-loss:  2.1760074674309076 	 ± 0.22405344806328292
	data : 0.11496458053588868
	model : 0.065101957321167
			 train-loss:  2.175264971378522 	 ± 0.22270789983126552
	data : 0.11501641273498535
	model : 0.06513738632202148
			 train-loss:  2.1728194921831543 	 ± 0.22234532684320105
	data : 0.11488857269287109
	model : 0.06516413688659668
			 train-loss:  2.171603538095951 	 ± 0.22121546355856309
	data : 0.11493411064147949
	model : 0.06519503593444824
			 train-loss:  2.170919566978643 	 ± 0.21993079596857748
	data : 0.11489715576171874
	model : 0.06515350341796874
			 train-loss:  2.169911487800319 	 ± 0.21877384785948428
	data : 0.11487250328063965
	model : 0.06502666473388671
			 train-loss:  2.1703400654965135 	 ± 0.21748656880813103
	data : 0.11487135887145997
	model : 0.06502270698547363
			 train-loss:  2.1693574715228308 	 ± 0.21637338853804652
	data : 0.1149148941040039
	model : 0.0650393009185791
			 train-loss:  2.166888779752395 	 ± 0.21628357010659516
	data : 0.11484651565551758
	model : 0.06499705314636231
			 train-loss:  2.1678885512573776 	 ± 0.2152199034713622
	data : 0.11493911743164062
	model : 0.06500935554504395
			 train-loss:  2.1652962599677603 	 ± 0.21532560310209606
	data : 0.11491875648498535
	model : 0.06505374908447266
			 train-loss:  2.1616242338310587 	 ± 0.21682095735422136
	data : 0.11492657661437988
	model : 0.0650904655456543
			 train-loss:  2.1625185468223656 	 ± 0.21576258343017998
	data : 0.11488285064697265
	model : 0.06505370140075684
			 train-loss:  2.1650829633076984 	 ± 0.2159201612210324
	data : 0.11481337547302246
	model : 0.06501545906066894
			 train-loss:  2.1684754287803565 	 ± 0.2171289608696075
	data : 0.11487836837768554
	model : 0.06494851112365722
			 train-loss:  2.166447091361751 	 ± 0.21681081164544455
	data : 0.11483201980590821
	model : 0.06494736671447754
			 train-loss:  2.166897880133762 	 ± 0.21568535549431647
	data : 0.11494150161743164
	model : 0.06489701271057129
			 train-loss:  2.169443458952802 	 ± 0.2159349774203202
	data : 0.11495432853698731
	model : 0.06490774154663086
			 train-loss:  2.1714359120318765 	 ± 0.2156623797345256
	data : 0.11493406295776368
	model : 0.0649496078491211
			 train-loss:  2.1700262489418187 	 ± 0.21497571813856994
	data : 0.11492767333984374
	model : 0.0650177001953125
			 train-loss:  2.1694006170194173 	 ± 0.2139525566606478
	data : 0.11493282318115235
	model : 0.06500825881958008
			 train-loss:  2.1688946862610017 	 ± 0.2129164771763043
	data : 0.1148484230041504
	model : 0.06498508453369141
			 train-loss:  2.170340329709679 	 ± 0.21232126974182308
	data : 0.11500959396362305
	model : 0.06495885848999024
			 train-loss:  2.1686318635940554 	 ± 0.21193981494586503
	data : 0.11488380432128906
	model : 0.06495671272277832
			 train-loss:  2.1681315780866264 	 ± 0.21094733049118494
	data : 0.11529316902160644
	model : 0.06495013236999511
			 train-loss:  2.1671872793459426 	 ± 0.2101251423330022
	data : 0.11570310592651367
	model : 0.06496715545654297
			 train-loss:  2.166070292296919 	 ± 0.20940671189432555
	data : 0.11557435989379883
	model : 0.06501102447509766
			 train-loss:  2.1661909291377435 	 ± 0.20840111347111945
	data : 0.1154592514038086
	model : 0.06502366065979004
			 train-loss:  2.1670741558074953 	 ± 0.2076018414838963
	data : 0.11579546928405762
	model : 0.06504960060119629
			 train-loss:  2.1671354523244895 	 ± 0.20662122167270297
	data : 0.11535110473632812
	model : 0.06505107879638672
			 train-loss:  2.1637506262164248 	 ± 0.20858519261713704
	data : 0.11488780975341797
	model : 0.06511344909667968
			 train-loss:  2.1658590612588107 	 ± 0.2087596729771442
	data : 0.11504898071289063
	model : 0.0651057243347168
			 train-loss:  2.167670674280289 	 ± 0.2086509734971601
	data : 0.1150123119354248
	model : 0.06513772010803223
			 train-loss:  2.164792670986869 	 ± 0.2098625511063836
	data : 0.11491012573242188
	model : 0.06513314247131348
			 train-loss:  2.1711267125499143 	 ± 0.21922299615628585
	data : 0.11487183570861817
	model : 0.06510577201843262
			 train-loss:  2.17353414531265 	 ± 0.21971106621991734
	data : 0.11484303474426269
	model : 0.0649756908416748
			 train-loss:  2.1717053546314746 	 ± 0.21959130125837392
	data : 0.11524629592895508
	model : 0.06499681472778321
			 train-loss:  2.1745685286689223 	 ± 0.22073445961748972
	data : 0.11524252891540528
	model : 0.06494121551513672
			 train-loss:  2.1741878955260567 	 ± 0.2198102222407742
	data : 0.11525802612304688
	model : 0.06493816375732422
			 train-loss:  2.172089250950978 	 ± 0.2200147892744038
	data : 0.1153571605682373
	model : 0.06497249603271485
			 train-loss:  2.171487725698031 	 ± 0.21916831260746564
	data : 0.11557259559631347
	model : 0.06505637168884278
			 train-loss:  2.1704046938378934 	 ± 0.2185518501229988
	data : 0.1151695728302002
	model : 0.06509833335876465
			 train-loss:  2.174429605988895 	 ± 0.22197998938416827
	data : 0.115057373046875
	model : 0.06513562202453613
			 train-loss:  2.1753146876891454 	 ± 0.22126389381743958
	data : 0.11503586769104004
	model : 0.06510767936706544
			 train-loss:  2.1784451608815467 	 ± 0.22300019062067572
	data : 0.11493911743164062
	model : 0.06505722999572754
			 train-loss:  2.1779900525437026 	 ± 0.2221407921148209
	data : 0.11474037170410156
	model : 0.06503105163574219
			 train-loss:  2.1778747366695868 	 ± 0.22123960435197487
	data : 0.11452698707580566
	model : 0.06498093605041504
			 train-loss:  2.178450039317531 	 ± 0.2204380608261995
	data : 0.11472010612487793
	model : 0.0649747371673584
			 train-loss:  2.18187571811676 	 ± 0.22284382262187422
	data : 0.1147681713104248
	model : 0.06497058868408204
			 train-loss:  2.1812262431023615 	 ± 0.22207650602124018
	data : 0.11485629081726074
	model : 0.06500682830810547
			 train-loss:  2.1827858667674027 	 ± 0.22189215779645
	data : 0.11499977111816406
	model : 0.06502203941345215
			 train-loss:  2.1849759882315993 	 ± 0.22239749255757588
	data : 0.1151430606842041
	model : 0.06501178741455078
			 train-loss:  2.1833548203919286 	 ± 0.22229178245401301
	data : 0.11511726379394531
	model : 0.06505179405212402
			 train-loss:  2.1829533494435824 	 ± 0.22148210702234003
	data : 0.1150627613067627
	model : 0.06506595611572266
			 train-loss:  2.1833173127574774 	 ± 0.22067415855228875
	data : 0.114935302734375
	model : 0.06508946418762207
			 train-loss:  2.1836752936695563 	 ± 0.21987486132228162
	data : 0.11484284400939941
	model : 0.0650700569152832
			 train-loss:  2.185764985873287 	 ± 0.2203585217883993
	data : 0.11492376327514649
	model : 0.06507158279418945
			 train-loss:  2.1858125951752734 	 ± 0.21953543532611378
	data : 0.1149785041809082
	model : 0.0650601863861084
			 train-loss:  2.1877035361749155 	 ± 0.21981341982951488
	data : 0.1150099754333496
	model : 0.06505537033081055
			 train-loss:  2.1881862309049156 	 ± 0.21907559149134262
	data : 0.11510229110717773
	model : 0.06504478454589843
			 train-loss:  2.1909932106950856 	 ± 0.2207155527263644
	data : 0.11512389183044433
	model : 0.06509737968444824
			 train-loss:  2.1889576402263367 	 ± 0.22120129147940842
	data : 0.1150604248046875
	model : 0.06515531539916992
			 train-loss:  2.1885824177762587 	 ± 0.22044823919099613
	data : 0.1149789810180664
	model : 0.06514153480529786
			 train-loss:  2.1909391633101873 	 ± 0.2214098967344214
	data : 0.11490182876586914
	model : 0.06513080596923829
			 train-loss:  2.1932922449517758 	 ± 0.22237321159390896
	data : 0.11466622352600098
	model : 0.06507620811462403
			 train-loss:  2.192526851741361 	 ± 0.22177513039552102
	data : 0.11471748352050781
	model : 0.06503257751464844
			 train-loss:  2.191785057941517 	 ± 0.22117504287356113
	data : 0.11475067138671875
	model : 0.06498227119445801
			 train-loss:  2.1925997311870256 	 ± 0.22062093397708304
	data : 0.11477699279785156
	model : 0.06495251655578613
			 train-loss:  2.1921070929231314 	 ± 0.2199383191876769
	data : 0.11483912467956543
	model : 0.06499319076538086
			 train-loss:  2.191143177143515 	 ± 0.21949092836717576
	data : 0.11503567695617675
	model : 0.06503973007202149
			 train-loss:  2.1907798003177255 	 ± 0.2187871480696175
	data : 0.11508612632751465
	model : 0.06507840156555175
			 train-loss:  2.192482485159023 	 ± 0.219021820633185
	data : 0.11502041816711425
	model : 0.06504034996032715
			 train-loss:  2.1931200067468937 	 ± 0.21842335053529555
	data : 0.11496801376342773
	model : 0.06506509780883789
			 train-loss:  2.1939521845181784 	 ± 0.2179309228064729
	data : 0.11488814353942871
	model : 0.06506805419921875
			 train-loss:  2.1928741308237543 	 ± 0.21760902560956988
	data : 0.11477694511413575
	model : 0.06506934165954589
			 train-loss:  2.191299862767521 	 ± 0.21775301814753878
	data : 0.11471495628356934
	model : 0.06503510475158691
			 train-loss:  2.1925079425175986 	 ± 0.217550691490278
	data : 0.11476378440856934
	model : 0.06505694389343261
			 train-loss:  2.1934961769487953 	 ± 0.21718747118140913
	data : 0.11484060287475586
	model : 0.0650561809539795
			 train-loss:  2.192763516979833 	 ± 0.21667657522021413
	data : 0.11489043235778809
	model : 0.06505036354064941
			 train-loss:  2.195550037500186 	 ± 0.21874942919571244
	data : 0.11492695808410644
	model : 0.0650240421295166
			 train-loss:  2.1969943752714025 	 ± 0.2187966196058809
	data : 0.11487555503845215
	model : 0.06502742767333984
			 train-loss:  2.1969702175900907 	 ± 0.21810333624635356
	data : 0.11492738723754883
	model : 0.06505217552185058
			 train-loss:  2.1961232813649207 	 ± 0.21767687595095056
	data : 0.11485795974731446
	model : 0.06506128311157226
			 train-loss:  2.1958278484642504 	 ± 0.2170275438727788
	data : 0.11475272178649902
	model : 0.06504888534545898
			 train-loss:  2.196363451317971 	 ± 0.2164585446497198
	data : 0.1148033618927002
	model : 0.06505007743835449
			 train-loss:  2.1962700868830267 	 ± 0.2157926803484606
	data : 0.11488399505615235
	model : 0.06502137184143067
			 train-loss:  2.1955946610749133 	 ± 0.21530141979031947
	data : 0.11488170623779297
	model : 0.06498274803161622
			 train-loss:  2.193230803419904 	 ± 0.21675531342229812
	data : 0.11495490074157715
	model : 0.0650099277496338
			 train-loss:  2.192171822172223 	 ± 0.2165226030358082
	data : 0.11505146026611328
	model : 0.06506714820861817
			 train-loss:  2.1902381174535637 	 ± 0.21729377812520373
	data : 0.11507568359375
	model : 0.0651336669921875
			 train-loss:  2.187718048780978 	 ± 0.2190618082162304
	data : 0.11501951217651367
	model : 0.06520137786865235
			 train-loss:  2.1908832646551586 	 ± 0.22220606944018373
	data : 0.11504292488098145
	model : 0.0652092456817627
			 train-loss:  2.191705929457083 	 ± 0.22180413195587284
	data : 0.11492209434509278
	model : 0.0651613712310791
			 train-loss:  2.1898358043502357 	 ± 0.2224831085354499
	data : 0.11491055488586426
	model : 0.06514515876770019
			 train-loss:  2.189840442953054 	 ± 0.2218316275086992
	data : 0.11485629081726074
	model : 0.06507883071899415
			 train-loss:  2.1875626832939856 	 ± 0.22318232654179315
	data : 0.11484684944152831
	model : 0.06513113975524902
			 train-loss:  2.1864054196142737 	 ± 0.22305331731190406
	data : 0.1148869514465332
	model : 0.0651628017425537
			 train-loss:  2.1856113103614456 	 ± 0.22265655678074506
	data : 0.11501951217651367
	model : 0.06516728401184083
			 train-loss:  2.1857013981682916 	 ± 0.22202266400117981
	data : 0.11505026817321777
	model : 0.06514759063720703
			 train-loss:  2.1852101988413115 	 ± 0.22148635818304946
	data : 0.11505379676818847
	model : 0.06517267227172852
			 train-loss:  2.1871568462942954 	 ± 0.22236455189621038
	data : 0.11511144638061524
	model : 0.0650815486907959
			 train-loss:  2.187353430839067 	 ± 0.22175447618419672
	data : 0.11502108573913575
	model : 0.06504983901977539
			 train-loss:  2.1861718433529305 	 ± 0.2216953794975642
	data : 0.11493630409240722
	model : 0.06509795188903808
			 train-loss:  2.1845987843142614 	 ± 0.22207820764450034
	data : 0.11485557556152344
	model : 0.0650705337524414
			 train-loss:  2.184296033000419 	 ± 0.22150112785729353
	data : 0.11485471725463867
	model : 0.06503086090087891
			 train-loss:  2.184302874318846 	 ± 0.2208917892680209
	data : 0.11486763954162597
	model : 0.06500129699707032
			 train-loss:  2.193290071409257 	 ± 0.25144908163229024
	data : 0.11488323211669922
	model : 0.06496758460998535
			 train-loss:  2.191628780701886 	 ± 0.2517698887749001
	data : 0.11504936218261719
	model : 0.06489548683166504
			 train-loss:  2.1920523392187583 	 ± 0.25115423264323755
	data : 0.11512889862060546
	model : 0.06492347717285156
			 train-loss:  2.193458684670028 	 ± 0.25120750690096966
	data : 0.11516461372375489
	model : 0.06495280265808105
			 train-loss:  2.192581872251582 	 ± 0.2508201493150014
	data : 0.1150315284729004
	model : 0.06501145362854004
			 train-loss:  2.192982759247435 	 ± 0.25021224692152144
	data : 0.11500864028930664
	model : 0.06506633758544922
			 train-loss:  2.1954769312389315 	 ± 0.25188181156692924
	data : 0.11484284400939941
	model : 0.06508030891418456
			 train-loss:  2.194213921145389 	 ± 0.25181743509129734
	data : 0.1146815299987793
	model : 0.06507058143615722
			 train-loss:  2.194319778712008 	 ± 0.25116160059742565
	data : 0.11482119560241699
	model : 0.0651254653930664
			 train-loss:  2.193863570690155 	 ± 0.25058601072703995
	data : 0.11496014595031738
	model : 0.06506361961364746
			 train-loss:  2.194119544844553 	 ± 0.2499611469904451
	data : 0.11500797271728516
	model : 0.06504817008972168
			 train-loss:  2.1950719504012275 	 ± 0.24966692996797302
	data : 0.11510262489318848
	model : 0.06506838798522949
			 train-loss:  2.1960792529277313 	 ± 0.24942084973322315
	data : 0.1151817798614502
	model : 0.06507859230041504
			 train-loss:  2.1958441430208633 	 ± 0.2488054207484061
	data : 0.11497015953063965
	model : 0.06502690315246581
			 train-loss:  2.196125311294788 	 ± 0.24820434744597483
	data : 0.11495556831359863
	model : 0.06503882408142089
			 train-loss:  2.1967132175811614 	 ± 0.24771424984403217
	data : 0.1149406909942627
	model : 0.06499814987182617
			 train-loss:  2.1966964683341024 	 ± 0.2470911807546279
	data : 0.114825439453125
	model : 0.06498122215270996
			 train-loss:  2.1968300628662107 	 ± 0.24647988356250394
	data : 0.11474113464355469
	model : 0.06494812965393067
			 train-loss:  2.19908162135983 	 ± 0.24791931291870503
	data : 0.11481652259826661
	model : 0.06492714881896973
			 train-loss:  2.199359712034169 	 ± 0.2473363151260174
	data : 0.11485309600830078
	model : 0.06492433547973633
			 train-loss:  2.199323598974444 	 ± 0.24672689414414087
	data : 0.11481156349182128
	model : 0.06499271392822266
			 train-loss:  2.1976994667567458 	 ± 0.24720686085942173
	data : 0.1148144245147705
	model : 0.06502656936645508
			 train-loss:  2.19839466315944 	 ± 0.24680300096388996
	data : 0.11505603790283203
	model : 0.06505732536315918
			 train-loss:  2.1988928508990018 	 ± 0.24630654168590244
	data : 0.11503863334655762
	model : 0.06508865356445312
			 train-loss:  2.2007296022009735 	 ± 0.24712104039625782
	data : 0.11501107215881348
	model : 0.06509842872619628
			 train-loss:  2.2002704114868092 	 ± 0.24661479229829286
	data : 0.1149662971496582
	model : 0.06506891250610351
			 train-loss:  2.199892215751575 	 ± 0.24608455280516786
	data : 0.11488189697265624
	model : 0.06516294479370117
			 train-loss:  2.1997428581828165 	 ± 0.24550743344628081
	data : 0.1146707534790039
	model : 0.06513032913208008
			 train-loss:  2.200482405192479 	 ± 0.2451593294937346
	data : 0.11475248336791992
	model : 0.06511321067810058
			 train-loss:  2.200395071843885 	 ± 0.24458373000888906
	data : 0.11474957466125488
	model : 0.06511554718017579
			 train-loss:  2.2000699283931175 	 ± 0.24405483512540652
	data : 0.1147921085357666
	model : 0.06518101692199707
			 train-loss:  2.1998309047422677 	 ± 0.24350893425721284
	data : 0.11483049392700195
	model : 0.06506853103637696
			 train-loss:  2.1993498098018556 	 ± 0.2430438925785336
	data : 0.11494646072387696
	model : 0.06510071754455567
			 train-loss:  2.199040539286755 	 ± 0.24252303875421746
	data : 0.11493058204650879
	model : 0.06507587432861328
			 train-loss:  2.1999034898072343 	 ± 0.24229574479544677
	data : 0.11485438346862793
	model : 0.06505746841430664
			 train-loss:  2.200377819188144 	 ± 0.24184034239490496
	data : 0.11482396125793456
	model : 0.06493721008300782
			 train-loss:  2.19953034838585 	 ± 0.24161179088077353
	data : 0.11487393379211426
	model : 0.06491923332214355
			 train-loss:  2.200977923111482 	 ± 0.24201202318076148
	data : 0.11508188247680665
	model : 0.06536316871643066
			 train-loss:  2.200725013314329 	 ± 0.24149300092834453
	data : 0.11458659172058105
	model : 0.06560120582580567
			 train-loss:  2.2001224557558694 	 ± 0.24111493441327234
	data : 0.11450934410095215
	model : 0.06558032035827636
			 train-loss:  2.199856079747324 	 ± 0.24060644727712574
	data : 0.11462187767028809
	model : 0.06554193496704101
			 train-loss:  2.199133222124406 	 ± 0.24031134201972607
	data : 0.11471467018127442
	model : 0.06545214653015137
			 train-loss:  2.1986323648028905 	 ± 0.23989386961916198
	data : 0.11462993621826172
	model : 0.06482648849487305
			 train-loss:  2.1995319534192044 	 ± 0.23974259156796257
	data : 0.11526226997375488
	model : 0.0643796443939209
			 train-loss:  2.2002130106157143 	 ± 0.2394329495260003
	data : 0.11548118591308594
	model : 0.06415338516235351
			 train-loss:  2.1992041086941434 	 ± 0.23939038740263194
	data : 0.11554780006408691
	model : 0.06404008865356445
			 train-loss:  2.1997195326084653 	 ± 0.23899388330516733
	data : 0.11573071479797363
	model : 0.06392040252685546
			 train-loss:  2.1991833930430205 	 ± 0.23861173869991867
	data : 0.11577119827270507
	model : 0.06385111808776855
			 train-loss:  2.1996561505577783 	 ± 0.23820262902680142
	data : 0.11573433876037598
	model : 0.06385998725891114
			 train-loss:  2.200085546949814 	 ± 0.23777828634765935
	data : 0.11568570137023926
	model : 0.06391668319702148
			 train-loss:  2.1999420920155077 	 ± 0.23727754486265074
	data : 0.11571693420410156
	model : 0.06392865180969239
			 train-loss:  2.2002229298281875 	 ± 0.23680880258217532
	data : 0.11553382873535156
	model : 0.06392641067504883
			 train-loss:  2.2005429729502253 	 ± 0.23635512584079707
	data : 0.11548762321472168
	model : 0.06392230987548828
			 train-loss:  2.2011331900701685 	 ± 0.23602732562903167
	data : 0.11552309989929199
	model : 0.06389360427856446
			 train-loss:  2.2006012332087326 	 ± 0.23567058075318645
	data : 0.11571211814880371
	model : 0.06388916969299316
			 train-loss:  2.199030986353129 	 ± 0.2364140936817193
	data : 0.11562681198120117
	model : 0.06385550498962403
			 train-loss:  2.197973612960911 	 ± 0.23648226294170943
	data : 0.11579279899597168
	model : 0.06388936042785645
			 train-loss:  2.198400920132796 	 ± 0.23608151971091565
	data : 0.11582355499267578
	model : 0.06388845443725585
			 train-loss:  2.197643727682438 	 ± 0.23588306897229994
	data : 0.11581439971923828
	model : 0.06392617225646972
			 train-loss:  2.1974611435054747 	 ± 0.235412267495489
	data : 0.11563191413879395
	model : 0.06389899253845215
			 train-loss:  2.197488473276052 	 ± 0.2349277654458764
	data : 0.11577348709106446
	model : 0.06389398574829101
			 train-loss:  2.195686696494212 	 ± 0.23612229294375858
	data : 0.11577377319335938
	model : 0.06390256881713867
			 train-loss:  2.1945586087752362 	 ± 0.2362978648541837
	data : 0.11588730812072753
	model : 0.06396098136901855
			 train-loss:  2.1945303901424253 	 ± 0.23581750920619995
	data : 0.11580066680908203
	model : 0.06394619941711426
			 train-loss:  2.1938749836524005 	 ± 0.23556406251445483
	data : 0.11597743034362792
	model : 0.0639756202697754
			 train-loss:  2.1971960298476683 	 ± 0.24081304935283449
	data : 0.11587629318237305
	model : 0.0639531135559082
			 train-loss:  2.196203298358075 	 ± 0.24083695185648463
	data : 0.11555829048156738
	model : 0.06383752822875977
			 train-loss:  2.19718226480484 	 ± 0.24085070703793637
	data : 0.11555275917053223
	model : 0.06376304626464843
			 train-loss:  2.1975672857694892 	 ± 0.24044752350851192
	data : 0.11573662757873535
	model : 0.06376829147338867
			 train-loss:  2.1964382013631245 	 ± 0.24063576135540193
	data : 0.11567063331604004
	model : 0.06379141807556152
			 train-loss:  2.195338498933513 	 ± 0.2407933739184788
	data : 0.1156421184539795
	model : 0.06382722854614258
			 train-loss:  2.194984849982374 	 ± 0.24038472830025998
	data : 0.11591124534606934
	model : 0.06389775276184081
			 train-loss:  2.19547534643435 	 ± 0.24004024560805326
	data : 0.11572766304016113
	model : 0.06389884948730469
			 train-loss:  2.1960115339607 	 ± 0.23972391577792737
	data : 0.11526427268981934
	model : 0.05544896125793457
#epoch  37    val-loss:  2.3903220013568274  train-loss:  2.1960115339607  lr:  9.765625e-06
			 train-loss:  2.383842706680298 	 ± 0.0
	data : 5.1170854568481445
	model : 0.0733175277709961
			 train-loss:  2.4167275428771973 	 ± 0.032884836196899414
	data : 2.754043698310852
	model : 0.0694047212600708
			 train-loss:  2.356223980585734 	 ± 0.08967889235700084
	data : 1.8737825552622478
	model : 0.06796566645304362
			 train-loss:  2.349373996257782 	 ± 0.07856522549234791
	data : 1.4339189529418945
	model : 0.06716108322143555
			 train-loss:  2.2487790107727053 	 ± 0.213108892691746
	data : 1.169981336593628
	model : 0.06671290397644043
			 train-loss:  2.2420262893040976 	 ± 0.19512601831401521
	data : 0.16946816444396973
	model : 0.06501855850219726
			 train-loss:  2.2409720761435374 	 ± 0.18067004474802761
	data : 0.11436376571655274
	model : 0.06489944458007812
			 train-loss:  2.217246353626251 	 ± 0.18028262894235897
	data : 0.1145627498626709
	model : 0.06486883163452148
			 train-loss:  2.2354633808135986 	 ± 0.1776102274689515
	data : 0.11445679664611816
	model : 0.06485991477966309
			 train-loss:  2.224480938911438 	 ± 0.17168686578368314
	data : 0.11448755264282226
	model : 0.06482157707214356
			 train-loss:  2.227554516358809 	 ± 0.16398529741246287
	data : 0.11455836296081542
	model : 0.06481208801269531
			 train-loss:  2.228330890337626 	 ± 0.15702508273819685
	data : 0.11446709632873535
	model : 0.0653611660003662
			 train-loss:  2.2017425023592434 	 ± 0.17675831605990552
	data : 0.11448545455932617
	model : 0.0654172420501709
			 train-loss:  2.199376259531294 	 ± 0.17054211188868215
	data : 0.1146618366241455
	model : 0.06549935340881348
			 train-loss:  2.224234135945638 	 ± 0.18919945580414996
	data : 0.11471338272094726
	model : 0.06550469398498535
			 train-loss:  2.2189896404743195 	 ± 0.18431420981039429
	data : 0.11472158432006836
	model : 0.06551556587219239
			 train-loss:  2.2336963485268986 	 ± 0.18823917548799346
	data : 0.11442608833312988
	model : 0.0649033546447754
			 train-loss:  2.221441970931159 	 ± 0.18978492679610237
	data : 0.1144094467163086
	model : 0.06486067771911622
			 train-loss:  2.2135712723982963 	 ± 0.18771701996793458
	data : 0.11442799568176269
	model : 0.06483073234558105
			 train-loss:  2.2103325605392454 	 ± 0.18350774218537208
	data : 0.11448373794555664
	model : 0.0649221420288086
			 train-loss:  2.2142896425156366 	 ± 0.17995745501195407
	data : 0.11450223922729492
	model : 0.06500058174133301
			 train-loss:  2.215141924944791 	 ± 0.1758633234708404
	data : 0.11473674774169922
	model : 0.06505179405212402
			 train-loss:  2.2146332367606787 	 ± 0.17201427169619277
	data : 0.11473956108093261
	model : 0.06502981185913086
			 train-loss:  2.226256181796392 	 ± 0.17737861768266594
	data : 0.11456656455993652
	model : 0.06498222351074219
			 train-loss:  2.22192364692688 	 ± 0.17508611496079785
	data : 0.11443958282470704
	model : 0.06492142677307129
			 train-loss:  2.2219851200397196 	 ± 0.1716863360468494
	data : 0.11444287300109864
	model : 0.0648463249206543
			 train-loss:  2.212534502700523 	 ± 0.17523314560634123
	data : 0.11443953514099121
	model : 0.0649287223815918
			 train-loss:  2.2013755440711975 	 ± 0.18158219272647305
	data : 0.11451511383056641
	model : 0.06496543884277343
			 train-loss:  2.1908210968149118 	 ± 0.1869604824390209
	data : 0.11465086936950683
	model : 0.06501846313476563
			 train-loss:  2.190000470479329 	 ± 0.183871179526865
	data : 0.11468939781188965
	model : 0.0650254726409912
			 train-loss:  2.20225009610576 	 ± 0.1929238454840187
	data : 0.11453213691711425
	model : 0.06499261856079101
			 train-loss:  2.2139496132731438 	 ± 0.20074794419720188
	data : 0.11448163986206054
	model : 0.06491007804870605
			 train-loss:  2.211977684136593 	 ± 0.19799738657240407
	data : 0.1144979476928711
	model : 0.06489377021789551
			 train-loss:  2.2170605378992416 	 ± 0.19723717951443195
	data : 0.11460494995117188
	model : 0.06486263275146484
			 train-loss:  2.2178895201001847 	 ± 0.1944591731728916
	data : 0.11466417312622071
	model : 0.06486015319824219
			 train-loss:  2.2123749256134033 	 ± 0.19449510841132214
	data : 0.11477265357971192
	model : 0.06494121551513672
			 train-loss:  2.203636752592551 	 ± 0.19888379389131872
	data : 0.11491537094116211
	model : 0.06499810218811035
			 train-loss:  2.2034776618606164 	 ± 0.19625184103400062
	data : 0.11487231254577637
	model : 0.06497931480407715
			 train-loss:  2.2068567000902615 	 ± 0.19483610212758687
	data : 0.11480660438537597
	model : 0.0650097370147705
			 train-loss:  2.204589435458183 	 ± 0.19290556781111254
	data : 0.11478190422058106
	model : 0.0649794578552246
			 train-loss:  2.1977025910121637 	 ± 0.19545352332808955
	data : 0.11470470428466797
	model : 0.06495928764343262
			 train-loss:  2.197023610273997 	 ± 0.1931616112528346
	data : 0.11459388732910156
	model : 0.06492075920104981
			 train-loss:  2.190264485603155 	 ± 0.19586348072786516
	data : 0.11465845108032227
	model : 0.0649524211883545
			 train-loss:  2.1915778409351003 	 ± 0.19381640490054544
	data : 0.11471314430236816
	model : 0.06497755050659179
			 train-loss:  2.1889840443929036 	 ± 0.1924215368502619
	data : 0.11471481323242187
	model : 0.0650259017944336
			 train-loss:  2.196213406065236 	 ± 0.19640009908036682
	data : 0.11476893424987793
	model : 0.06502041816711426
			 train-loss:  2.190234590083995 	 ± 0.19848582790428673
	data : 0.11485509872436524
	model : 0.06504850387573242
			 train-loss:  2.1909830619891486 	 ± 0.19647440230329213
	data : 0.11481361389160157
	model : 0.06502838134765625
			 train-loss:  2.195282517647257 	 ± 0.19672744155020233
	data : 0.11464266777038574
	model : 0.0649418830871582
			 train-loss:  2.1928467988967895 	 ± 0.19549515620456684
	data : 0.11456851959228516
	model : 0.06492552757263184
			 train-loss:  2.20770198691125 	 ± 0.22023352859030088
	data : 0.11458868980407715
	model : 0.06488733291625977
			 train-loss:  2.2074925211759715 	 ± 0.21811074823415377
	data : 0.11452980041503906
	model : 0.06489686965942383
			 train-loss:  2.206931712492457 	 ± 0.21608114746397986
	data : 0.11444931030273438
	model : 0.06494112014770508
			 train-loss:  2.2024184156347206 	 ± 0.2165779650831923
	data : 0.1146611213684082
	model : 0.06504812240600585
			 train-loss:  2.197102358124473 	 ± 0.2181266686271301
	data : 0.11472873687744141
	model : 0.06510863304138184
			 train-loss:  2.1958436646631787 	 ± 0.21637178934554555
	data : 0.11484308242797851
	model : 0.06512327194213867
			 train-loss:  2.1970467379218652 	 ± 0.21465427619025887
	data : 0.11486978530883789
	model : 0.06507225036621093
			 train-loss:  2.1944498995254778 	 ± 0.21369702867052412
	data : 0.11473169326782226
	model : 0.06499042510986328
			 train-loss:  2.1928626379724276 	 ± 0.21222285008617112
	data : 0.11472892761230469
	model : 0.0649339199066162
			 train-loss:  2.1903483092784883 	 ± 0.21133122077710473
	data : 0.11482782363891601
	model : 0.06491670608520508
			 train-loss:  2.185570789165184 	 ± 0.21283378756069726
	data : 0.11475801467895508
	model : 0.06498188972473144
			 train-loss:  2.1892986201470896 	 ± 0.21310866901945524
	data : 0.11474037170410156
	model : 0.06499924659729003
			 train-loss:  2.1929447253545127 	 ± 0.2133510255763489
	data : 0.11498327255249023
	model : 0.06503810882568359
			 train-loss:  2.19547007791698 	 ± 0.21262456960749154
	data : 0.11488699913024902
	model : 0.06503162384033204
			 train-loss:  2.1956872885043803 	 ± 0.210989812261407
	data : 0.11481194496154785
	model : 0.06499485969543457
			 train-loss:  2.1947021863677283 	 ± 0.2095358754832453
	data : 0.11472067832946778
	model : 0.06493949890136719
			 train-loss:  2.198101198495324 	 ± 0.2097915562856406
	data : 0.11468043327331542
	model : 0.06494526863098145
			 train-loss:  2.2001862894086277 	 ± 0.2089414845365239
	data : 0.1148005485534668
	model : 0.06493129730224609
			 train-loss:  2.1985653811606807 	 ± 0.20785211037165677
	data : 0.11477255821228027
	model : 0.06497488021850586
			 train-loss:  2.1963786380631585 	 ± 0.20716000711629995
	data : 0.11477317810058593
	model : 0.06498942375183106
			 train-loss:  2.200076229135755 	 ± 0.2080093284642428
	data : 0.11482563018798828
	model : 0.06500678062438965
			 train-loss:  2.197625449962086 	 ± 0.20758946639432044
	data : 0.11486148834228516
	model : 0.06498780250549316
			 train-loss:  2.1996411999611007 	 ± 0.2068710227515832
	data : 0.11484146118164062
	model : 0.06498312950134277
			 train-loss:  2.2075921152089095 	 ± 0.21640736067344254
	data : 0.1147397518157959
	model : 0.06494402885437012
			 train-loss:  2.205240486462911 	 ± 0.2159095848214323
	data : 0.11456356048583985
	model : 0.06494727134704589
			 train-loss:  2.20328814261838 	 ± 0.2151498115239988
	data : 0.1146090030670166
	model : 0.06494030952453614
			 train-loss:  2.20067713012943 	 ± 0.21495674256633734
	data : 0.1146162509918213
	model : 0.06495943069458007
			 train-loss:  2.199711874509469 	 ± 0.21374226005587654
	data : 0.1145284652709961
	model : 0.0649899959564209
			 train-loss:  2.201878971691373 	 ± 0.21324578600500646
	data : 0.11470279693603516
	model : 0.06499800682067872
			 train-loss:  2.204387004673481 	 ± 0.21307808735059974
	data : 0.11485862731933594
	model : 0.06498231887817382
			 train-loss:  2.201910388322524 	 ± 0.21291416046135533
	data : 0.11494631767272949
	model : 0.06504440307617188
			 train-loss:  2.206151646811788 	 ± 0.2150271035324717
	data : 0.11495966911315918
	model : 0.06504588127136231
			 train-loss:  2.2074800586125938 	 ± 0.21406608957704135
	data : 0.11482667922973633
	model : 0.06503486633300781
			 train-loss:  2.206223889475777 	 ± 0.2130955987138423
	data : 0.11482410430908203
	model : 0.06504402160644532
			 train-loss:  2.2052733659744264 	 ± 0.2120174417308434
	data : 0.11493253707885742
	model : 0.06505870819091797
			 train-loss:  2.2023048983063807 	 ± 0.21255048336049898
	data : 0.11484127044677735
	model : 0.06507682800292969
			 train-loss:  2.2003464753600372 	 ± 0.21210438499618636
	data : 0.11488251686096192
	model : 0.06515889167785645
			 train-loss:  2.1982300877571106 	 ± 0.21181766023265902
	data : 0.11503486633300782
	model : 0.06516036987304688
			 train-loss:  2.1975886125243114 	 ± 0.21071025576702992
	data : 0.11493759155273438
	model : 0.0651773452758789
			 train-loss:  2.1947802305221558 	 ± 0.21120472436993404
	data : 0.11486706733703614
	model : 0.06517419815063477
			 train-loss:  2.195271536544129 	 ± 0.21009276127670903
	data : 0.11485409736633301
	model : 0.06510591506958008
			 train-loss:  2.192618517772011 	 ± 0.2104749420452157
	data : 0.11472563743591309
	model : 0.06506223678588867
			 train-loss:  2.193778004697574 	 ± 0.20963550765344643
	data : 0.11470546722412109
	model : 0.06505842208862304
			 train-loss:  2.192406540221356 	 ± 0.2089364721146394
	data : 0.11484928131103515
	model : 0.0650568962097168
			 train-loss:  2.1948496416995398 	 ± 0.20917932707177145
	data : 0.11509909629821777
	model : 0.0650559425354004
			 train-loss:  2.1935711974898973 	 ± 0.20845975430361596
	data : 0.11508431434631347
	model : 0.06507887840270996
			 train-loss:  2.1960376980378458 	 ± 0.20878578168148187
	data : 0.11516017913818359
	model : 0.06507267951965331
			 train-loss:  2.198931248820558 	 ± 0.20966362842852365
	data : 0.1151425838470459
	model : 0.06508069038391114
			 train-loss:  2.1982976426981917 	 ± 0.2086963133042243
	data : 0.11495389938354492
	model : 0.06507925987243653
			 train-loss:  2.197904806137085 	 ± 0.20768699384101474
	data : 0.11478853225708008
	model : 0.06507983207702636
			 train-loss:  2.1984624791853498 	 ± 0.20673151463047054
	data : 0.11471529006958008
	model : 0.06506524085998536
			 train-loss:  2.198222057492125 	 ± 0.20572981795497614
	data : 0.11509141921997071
	model : 0.06500606536865235
			 train-loss:  2.1991120241220714 	 ± 0.20492590406459749
	data : 0.11500339508056641
	model : 0.06501297950744629
			 train-loss:  2.198394449857565 	 ± 0.2040682917442104
	data : 0.11554203033447266
	model : 0.06500539779663086
			 train-loss:  2.195812248048328 	 ± 0.20479430440012622
	data : 0.11544184684753418
	model : 0.06508121490478516
			 train-loss:  2.196908764119418 	 ± 0.20413546128334986
	data : 0.11553215980529785
	model : 0.06506681442260742
			 train-loss:  2.19503656280375 	 ± 0.20409159679027392
	data : 0.11516375541687011
	model : 0.0651620864868164
			 train-loss:  2.1941579050487943 	 ± 0.2033477530199275
	data : 0.11534743309020996
	model : 0.0651465892791748
			 train-loss:  2.1916530318216445 	 ± 0.20407984255149506
	data : 0.11490845680236816
	model : 0.06516008377075196
			 train-loss:  2.1927159667015075 	 ± 0.2034529680117574
	data : 0.11498608589172363
	model : 0.0650827407836914
			 train-loss:  2.1917525809090415 	 ± 0.20278632012667713
	data : 0.11502256393432617
	model : 0.06509299278259277
			 train-loss:  2.191932151360171 	 ± 0.20188785889225117
	data : 0.11493830680847168
	model : 0.06501760482788085
			 train-loss:  2.1928921326071813 	 ± 0.20124916449104255
	data : 0.1148386001586914
	model : 0.06498408317565918
			 train-loss:  2.1933569500320838 	 ± 0.2004254637918648
	data : 0.11489777565002442
	model : 0.06496124267578125
			 train-loss:  2.1927604167357737 	 ± 0.19965376561957954
	data : 0.11487407684326172
	model : 0.06493949890136719
			 train-loss:  2.1916584403350434 	 ± 0.19914226547041095
	data : 0.11491765975952148
	model : 0.0649683952331543
			 train-loss:  2.1957105377800445 	 ± 0.2030353391951293
	data : 0.11503772735595703
	model : 0.06501669883728027
			 train-loss:  2.196688640925844 	 ± 0.20244982287351004
	data : 0.11505637168884278
	model : 0.0651008129119873
			 train-loss:  2.2007507306187093 	 ± 0.20637000453194881
	data : 0.11496915817260742
	model : 0.06511788368225098
			 train-loss:  2.199585361282031 	 ± 0.20590115624892089
	data : 0.11495761871337891
	model : 0.0651282787322998
			 train-loss:  2.1975756085608618 	 ± 0.20622707048060862
	data : 0.11480474472045898
	model : 0.06509690284729004
			 train-loss:  2.1989414340159934 	 ± 0.20592893012558652
	data : 0.11475901603698731
	model : 0.0649984359741211
			 train-loss:  2.19917148109374 	 ± 0.20510585215684507
	data : 0.11471128463745117
	model : 0.06493563652038574
			 train-loss:  2.2017497747175154 	 ± 0.20626876585791482
	data : 0.11472759246826172
	model : 0.06489129066467285
			 train-loss:  2.2031969928741457 	 ± 0.20607314222653075
	data : 0.11487302780151368
	model : 0.06488261222839356
			 train-loss:  2.203128311369154 	 ± 0.2052551990426401
	data : 0.11491122245788574
	model : 0.06488914489746093
			 train-loss:  2.203044194874801 	 ± 0.2044476910169265
	data : 0.11482720375061035
	model : 0.06495766639709473
			 train-loss:  2.2036808505654335 	 ± 0.20377384952610328
	data : 0.11499767303466797
	model : 0.0649949550628662
			 train-loss:  2.2021161271620167 	 ± 0.20375299252725484
	data : 0.11505703926086426
	model : 0.06505784988403321
			 train-loss:  2.2024976620307335 	 ± 0.20301406853576198
	data : 0.11493616104125977
	model : 0.06510658264160156
			 train-loss:  2.201227917926002 	 ± 0.20275524262771047
	data : 0.11486334800720215
	model : 0.06510767936706544
			 train-loss:  2.200627330577735 	 ± 0.20210270595894925
	data : 0.11487517356872559
	model : 0.06506929397583008
			 train-loss:  2.1998739852044817 	 ± 0.20152743874301113
	data : 0.11483583450317383
	model : 0.06505413055419922
			 train-loss:  2.1994118156717786 	 ± 0.20084479844912015
	data : 0.11493468284606934
	model : 0.06502246856689453
			 train-loss:  2.200721202073274 	 ± 0.200672794376798
	data : 0.11495027542114258
	model : 0.06499090194702148
			 train-loss:  2.2024712054168476 	 ± 0.20096494644703056
	data : 0.11502599716186523
	model : 0.06497058868408204
			 train-loss:  2.199972244074745 	 ± 0.20233983443328388
	data : 0.11510891914367676
	model : 0.06504101753234863
			 train-loss:  2.1982147374014924 	 ± 0.20265216849168152
	data : 0.11507139205932618
	model : 0.06506433486938476
			 train-loss:  2.201973108078936 	 ± 0.20669239750637944
	data : 0.1150092601776123
	model : 0.06503753662109375
			 train-loss:  2.2020150329385486 	 ± 0.20595348059533075
	data : 0.11485981941223145
	model : 0.06502761840820312
			 train-loss:  2.2051568614675645 	 ± 0.20856164473560374
	data : 0.11475434303283691
	model : 0.06497001647949219
			 train-loss:  2.2056234361420217 	 ± 0.20789980893922033
	data : 0.11475572586059571
	model : 0.06489887237548828
			 train-loss:  2.207729067002143 	 ± 0.20868555111563683
	data : 0.1147928237915039
	model : 0.06486310958862304
			 train-loss:  2.207341021133794 	 ± 0.20801145145750857
	data : 0.11478695869445801
	model : 0.06488099098205566
			 train-loss:  2.2061601219506097 	 ± 0.2077767310450494
	data : 0.1148597240447998
	model : 0.064933443069458
			 train-loss:  2.206423863156201 	 ± 0.20708829791956102
	data : 0.11497492790222168
	model : 0.06501121520996093
			 train-loss:  2.2066224926993963 	 ± 0.20639666869895024
	data : 0.11489291191101074
	model : 0.0650475025177002
			 train-loss:  2.205768608563655 	 ± 0.20595856423196507
	data : 0.11487002372741699
	model : 0.06504669189453124
			 train-loss:  2.2032498089259103 	 ± 0.20754084757692237
	data : 0.11472969055175782
	model : 0.06506109237670898
			 train-loss:  2.201879692077637 	 ± 0.2075229012182468
	data : 0.11473078727722168
	model : 0.06501102447509766
			 train-loss:  2.2026412771237607 	 ± 0.20704480825421775
	data : 0.11480507850646973
	model : 0.06498632431030274
			 train-loss:  2.202560117370204 	 ± 0.20636502580436913
	data : 0.11500062942504882
	model : 0.06495876312255859
			 train-loss:  2.201686251397226 	 ± 0.20597148862457249
	data : 0.11505537033081055
	model : 0.0649754524230957
			 train-loss:  2.201790784860586 	 ± 0.20530573256743415
	data : 0.11517596244812012
	model : 0.0649836540222168
			 train-loss:  2.2000615589080317 	 ± 0.20576442774838527
	data : 0.1151648998260498
	model : 0.06505160331726074
			 train-loss:  2.1994513066915364 	 ± 0.20524453485103655
	data : 0.11510119438171387
	model : 0.06509056091308593
			 train-loss:  2.200209121795217 	 ± 0.20480867465516286
	data : 0.11496968269348144
	model : 0.06513466835021972
			 train-loss:  2.199266203596622 	 ± 0.20450109124236476
	data : 0.11492619514465333
	model : 0.0651397705078125
			 train-loss:  2.198505477335468 	 ± 0.20408113184281243
	data : 0.11477513313293457
	model : 0.0651308536529541
			 train-loss:  2.1980725176632405 	 ± 0.20351561769375504
	data : 0.11474885940551757
	model : 0.06505823135375977
			 train-loss:  2.195859376688181 	 ± 0.2048048512486466
	data : 0.11476855278015137
	model : 0.06501493453979493
			 train-loss:  2.197930907761609 	 ± 0.20585673895633685
	data : 0.1149672031402588
	model : 0.06498470306396484
			 train-loss:  2.1997124088322457 	 ± 0.2064731514257714
	data : 0.11503810882568359
	model : 0.06495361328125
			 train-loss:  2.199092300199881 	 ± 0.20599489145861635
	data : 0.11519403457641601
	model : 0.06496973037719726
			 train-loss:  2.197775780793392 	 ± 0.20606059500300028
	data : 0.11515040397644043
	model : 0.06499419212341309
			 train-loss:  2.195807219987892 	 ± 0.20698935495733706
	data : 0.11522717475891113
	model : 0.0650705337524414
			 train-loss:  2.1969539890746157 	 ± 0.20689693517863295
	data : 0.11514182090759277
	model : 0.06510567665100098
			 train-loss:  2.197711419491541 	 ± 0.20651234871676372
	data : 0.11511120796203614
	model : 0.06512441635131835
			 train-loss:  2.1978225298887173 	 ± 0.205905495398938
	data : 0.1149383544921875
	model : 0.06508774757385254
			 train-loss:  2.1978613895528456 	 ± 0.2052996193143947
	data : 0.11491713523864747
	model : 0.06507787704467774
			 train-loss:  2.1959736863074943 	 ± 0.20617283174189646
	data : 0.11477336883544922
	model : 0.06501293182373047
			 train-loss:  2.1974500875140346 	 ± 0.20647721545399972
	data : 0.11472697257995605
	model : 0.06497478485107422
			 train-loss:  2.1966495569041697 	 ± 0.20614711729695012
	data : 0.11465635299682617
	model : 0.06501359939575195
			 train-loss:  2.1953277437166236 	 ± 0.2062878191485373
	data : 0.1147878646850586
	model : 0.06503958702087402
			 train-loss:  2.1956771687098913 	 ± 0.20574921589235268
	data : 0.1149219036102295
	model : 0.06507525444030762
			 train-loss:  2.1962883580814707 	 ± 0.20532312238955247
	data : 0.11500988006591797
	model : 0.06505813598632812
			 train-loss:  2.2024111505282127 	 ± 0.22026665094078837
	data : 0.1149622917175293
	model : 0.06506071090698243
			 train-loss:  2.2031257607963646 	 ± 0.2198527145035005
	data : 0.11484112739562988
	model : 0.06500372886657715
			 train-loss:  2.204775739648489 	 ± 0.2203401450310259
	data : 0.11484270095825196
	model : 0.06512737274169922
			 train-loss:  2.204353004031711 	 ± 0.21980001542744698
	data : 0.11481218338012696
	model : 0.06512956619262696
			 train-loss:  2.202786912575611 	 ± 0.22019674165883352
	data : 0.11491265296936035
	model : 0.06518020629882812
			 train-loss:  2.2006846096489454 	 ± 0.22140496101975643
	data : 0.11448373794555664
	model : 0.06520652770996094
			 train-loss:  2.1989054517016386 	 ± 0.2220999558787279
	data : 0.11452832221984863
	model : 0.06526222229003906
			 train-loss:  2.1991665201342623 	 ± 0.22152375494293294
	data : 0.11451396942138672
	model : 0.06512203216552734
			 train-loss:  2.2026003160992182 	 ± 0.22578098112171344
	data : 0.11458768844604492
	model : 0.06506667137145997
			 train-loss:  2.200200146244418 	 ± 0.22752743154105876
	data : 0.11448483467102051
	model : 0.06504926681518555
			 train-loss:  2.201537878118097 	 ± 0.22765049067666976
	data : 0.11480541229248047
	model : 0.06501936912536621
			 train-loss:  2.2015213611278126 	 ± 0.22704434231869752
	data : 0.11482572555541992
	model : 0.06494064331054687
			 train-loss:  2.199718507509383 	 ± 0.22778814609487158
	data : 0.11479310989379883
	model : 0.0651707649230957
			 train-loss:  2.198047445322338 	 ± 0.22834649229607415
	data : 0.11472625732421875
	model : 0.06522283554077149
			 train-loss:  2.1972018118304106 	 ± 0.2280460334864657
	data : 0.11473255157470703
	model : 0.06523513793945312
			 train-loss:  2.196407852694392 	 ± 0.2277159083821099
	data : 0.11490449905395508
	model : 0.06530036926269531
			 train-loss:  2.197347518075933 	 ± 0.22749810773085463
	data : 0.11492042541503907
	model : 0.0653813362121582
			 train-loss:  2.197376126481086 	 ± 0.22691136292487368
	data : 0.11500663757324218
	model : 0.06517572402954101
			 train-loss:  2.1967973372875114 	 ± 0.22647231855551678
	data : 0.11498446464538574
	model : 0.06514620780944824
			 train-loss:  2.197933470108071 	 ± 0.2264502913242236
	data : 0.11495752334594726
	model : 0.0650930404663086
			 train-loss:  2.196976684071691 	 ± 0.2262716450775897
	data : 0.11439757347106934
	model : 0.06507291793823242
			 train-loss:  2.1964424071889934 	 ± 0.22582407152588024
	data : 0.1145850658416748
	model : 0.06499977111816406
			 train-loss:  2.1960272004256893 	 ± 0.22533171535025961
	data : 0.11463584899902343
	model : 0.06499509811401367
			 train-loss:  2.1974151486158373 	 ± 0.2256188459958636
	data : 0.11473231315612793
	model : 0.06499762535095215
			 train-loss:  2.1955138160221614 	 ± 0.22665750316674474
	data : 0.11463842391967774
	model : 0.06508955955505372
			 train-loss:  2.19462122716526 	 ± 0.22644963802164578
	data : 0.1151113510131836
	model : 0.06511416435241699
			 train-loss:  2.1943619726913903 	 ± 0.22592124178708461
	data : 0.11497092247009277
	model : 0.06518959999084473
			 train-loss:  2.194785669153812 	 ± 0.22544766955818032
	data : 0.11488614082336426
	model : 0.06514053344726563
			 train-loss:  2.1962402163482295 	 ± 0.22585464609157094
	data : 0.11475028991699218
	model : 0.06553955078125
			 train-loss:  2.195354082630676 	 ± 0.22566273728906747
	data : 0.1142852783203125
	model : 0.06561074256896973
			 train-loss:  2.195706904798314 	 ± 0.22517394726214293
	data : 0.11420331001281739
	model : 0.06551480293273926
			 train-loss:  2.195907233426204 	 ± 0.22465050158323946
	data : 0.114300537109375
	model : 0.0654825210571289
			 train-loss:  2.1952796307477085 	 ± 0.22429512480371236
	data : 0.11442389488220214
	model : 0.06556763648986816
			 train-loss:  2.1950635927064077 	 ± 0.22378224728967722
	data : 0.11463694572448731
	model : 0.06520447731018067
			 train-loss:  2.195467825184501 	 ± 0.22332816702147124
	data : 0.11507120132446289
	model : 0.06510157585144043
			 train-loss:  2.1950642854537605 	 ± 0.22287792327565267
	data : 0.11518135070800781
	model : 0.0651792049407959
			 train-loss:  2.1943658131388992 	 ± 0.22258657152669115
	data : 0.11517491340637206
	model : 0.06517658233642579
			 train-loss:  2.192852726606565 	 ± 0.22316118148975728
	data : 0.11504521369934081
	model : 0.06517057418823242
			 train-loss:  2.1914733310078467 	 ± 0.2235541667367698
	data : 0.11484174728393555
	model : 0.06514053344726563
			 train-loss:  2.1915796796480813 	 ± 0.22304153110401656
	data : 0.1149707317352295
	model : 0.06508450508117676
			 train-loss:  2.192034284090666 	 ± 0.22262729620338567
	data : 0.11500215530395508
	model : 0.06499705314636231
			 train-loss:  2.19179892868077 	 ± 0.2221431527270227
	data : 0.1149704933166504
	model : 0.06493983268737794
			 train-loss:  2.1920760690349423 	 ± 0.221673166539703
	data : 0.11514511108398437
	model : 0.06489372253417969
			 train-loss:  2.192184847051447 	 ± 0.22117464831570882
	data : 0.11530880928039551
	model : 0.06488285064697266
			 train-loss:  2.1911386445636665 	 ± 0.22121861211237254
	data : 0.11531200408935546
	model : 0.06486744880676269
			 train-loss:  2.1902454076586544 	 ± 0.22111889136758694
	data : 0.11534948348999023
	model : 0.06486139297485352
			 train-loss:  2.189233043268657 	 ± 0.22113759109879058
	data : 0.11529483795166015
	model : 0.06486315727233886
			 train-loss:  2.193294786981174 	 ± 0.22882858947274037
	data : 0.11532492637634277
	model : 0.06467156410217285
			 train-loss:  2.194629766676161 	 ± 0.22919207510319672
	data : 0.11523022651672363
	model : 0.06444816589355469
			 train-loss:  2.1933770791619227 	 ± 0.22945512394842368
	data : 0.1153782844543457
	model : 0.06426568031311035
			 train-loss:  2.192541337748456 	 ± 0.22929363249627965
	data : 0.11555304527282714
	model : 0.06408872604370117
			 train-loss:  2.195128399029113 	 ± 0.23208675196219364
	data : 0.1158111572265625
	model : 0.06387891769409179
			 train-loss:  2.1958548658279353 	 ± 0.23183911067703278
	data : 0.11579442024230957
	model : 0.06387076377868653
			 train-loss:  2.1960416151129682 	 ± 0.23135182462010004
	data : 0.11598758697509766
	model : 0.06389384269714356
			 train-loss:  2.195384318694408 	 ± 0.2310656431488436
	data : 0.11583209037780762
	model : 0.06395788192749023
			 train-loss:  2.195057003662504 	 ± 0.23062078095765245
	data : 0.11575970649719239
	model : 0.06395936012268066
			 train-loss:  2.1946420915137033 	 ± 0.23021211531539323
	data : 0.1155787467956543
	model : 0.06397719383239746
			 train-loss:  2.19344915334995 	 ± 0.2304402636944958
	data : 0.11556854248046874
	model : 0.06393251419067383
			 train-loss:  2.1933086582954893 	 ± 0.2299594856150655
	data : 0.11564970016479492
	model : 0.06393241882324219
			 train-loss:  2.1931769014415092 	 ± 0.22948065498609682
	data : 0.116009521484375
	model : 0.0638606071472168
			 train-loss:  2.191671932296914 	 ± 0.23016015101866838
	data : 0.11612443923950196
	model : 0.063958740234375
			 train-loss:  2.1926243710918585 	 ± 0.23014366961747657
	data : 0.11630482673645019
	model : 0.06399545669555665
			 train-loss:  2.19142679888833 	 ± 0.23040361931411907
	data : 0.11623520851135254
	model : 0.06409344673156739
			 train-loss:  2.1922632346550626 	 ± 0.23028644590849123
	data : 0.11612429618835449
	model : 0.06403875350952148
			 train-loss:  2.193017855719412 	 ± 0.2301053383104543
	data : 0.11597323417663574
	model : 0.06400847434997559
			 train-loss:  2.1922749103593433 	 ± 0.229918889271168
	data : 0.11577367782592773
	model : 0.06391115188598633
			 train-loss:  2.1929315667093534 	 ± 0.2296726006651911
	data : 0.1156625747680664
	model : 0.06387834548950196
			 train-loss:  2.1927439072093025 	 ± 0.2292201442193672
	data : 0.11586465835571289
	model : 0.06380486488342285
			 train-loss:  2.192078191406873 	 ± 0.2289881071202033
	data : 0.11592593193054199
	model : 0.06386752128601074
			 train-loss:  2.1932351618278316 	 ± 0.22923863719532012
	data : 0.11582679748535156
	model : 0.06392140388488769
			 train-loss:  2.1920776835337343 	 ± 0.22949330773419072
	data : 0.11593804359436036
	model : 0.06399645805358886
			 train-loss:  2.1930427315735046 	 ± 0.22953179761630982
	data : 0.11593790054321289
	model : 0.06399149894714355
			 train-loss:  2.1933342496075303 	 ± 0.22911642480431085
	data : 0.11563396453857422
	model : 0.06396894454956055
			 train-loss:  2.193352220058441 	 ± 0.22865790863643526
	data : 0.11560616493225098
	model : 0.06389102935791016
			 train-loss:  2.1929750200286806 	 ± 0.22827988218199954
	data : 0.11563830375671387
	model : 0.06389250755310058
			 train-loss:  2.191980876146801 	 ± 0.2283702716356659
	data : 0.11574749946594239
	model : 0.06388492584228515
			 train-loss:  2.1927734622842237 	 ± 0.22826551926240535
	data : 0.11577229499816895
	model : 0.06394243240356445
			 train-loss:  2.1926141202919127 	 ± 0.22782983243254407
	data : 0.11597237586975098
	model : 0.06402301788330078
			 train-loss:  2.1930299398945827 	 ± 0.22747922106879104
	data : 0.11596312522888183
	model : 0.06407728195190429
			 train-loss:  2.1919483388774097 	 ± 0.2276905237158708
	data : 0.1157491683959961
	model : 0.05563220977783203
#epoch  38    val-loss:  2.411696333634226  train-loss:  2.1919483388774097  lr:  4.8828125e-06
			 train-loss:  2.133120536804199 	 ± 0.0
	data : 5.440281867980957
	model : 0.07491803169250488
			 train-loss:  2.0740150213241577 	 ± 0.059105515480041504
	data : 2.7829127311706543
	model : 0.0699228048324585
			 train-loss:  2.302041927973429 	 ± 0.32606980073625175
	data : 1.8933603763580322
	model : 0.06819017728169759
			 train-loss:  2.235766291618347 	 ± 0.3048253863065664
	data : 1.4486762285232544
	model : 0.06732523441314697
			 train-loss:  2.245701456069946 	 ± 0.2733672308091796
	data : 1.181800127029419
	model : 0.06674199104309082
			 train-loss:  2.306680917739868 	 ± 0.2843715459183637
	data : 0.11660890579223633
	model : 0.06473512649536133
			 train-loss:  2.298675911767142 	 ± 0.2640060664438772
	data : 0.11442947387695312
	model : 0.0647432804107666
			 train-loss:  2.2904233038425446 	 ± 0.24791841700884729
	data : 0.1144876480102539
	model : 0.06479945182800292
			 train-loss:  2.2970852851867676 	 ± 0.23449800653225117
	data : 0.11447644233703613
	model : 0.06482176780700684
			 train-loss:  2.265388071537018 	 ± 0.24193553632995027
	data : 0.11462969779968261
	model : 0.06494417190551757
			 train-loss:  2.2623241272839634 	 ± 0.23087987635743074
	data : 0.11471691131591796
	model : 0.06532430648803711
			 train-loss:  2.2306046982606254 	 ± 0.2448075464887055
	data : 0.11476998329162598
	model : 0.06534528732299805
			 train-loss:  2.222568521132836 	 ± 0.23684518151594824
	data : 0.11459503173828126
	model : 0.0653390884399414
			 train-loss:  2.2212755594934737 	 ± 0.22827733423544097
	data : 0.11446452140808105
	model : 0.06534314155578613
			 train-loss:  2.2130930026372275 	 ± 0.22265189096691523
	data : 0.11438331604003907
	model : 0.06534667015075683
			 train-loss:  2.2035075649619102 	 ± 0.2187549022264587
	data : 0.11444568634033203
	model : 0.06500487327575684
			 train-loss:  2.200624893693363 	 ± 0.21253644391166565
	data : 0.11444664001464844
	model : 0.06502342224121094
			 train-loss:  2.2127568787998624 	 ± 0.21251904607502894
	data : 0.11467070579528808
	model : 0.06501765251159668
			 train-loss:  2.2156003588124324 	 ± 0.20720234306723997
	data : 0.11479644775390625
	model : 0.06503915786743164
			 train-loss:  2.217764526605606 	 ± 0.2021760600230148
	data : 0.11460175514221191
	model : 0.06497797966003419
			 train-loss:  2.208920121192932 	 ± 0.20122920571803815
	data : 0.11440072059631348
	model : 0.06496753692626953
			 train-loss:  2.209427849812941 	 ± 0.19661639579863385
	data : 0.11447453498840332
	model : 0.06491174697875976
			 train-loss:  2.2116613025250644 	 ± 0.19257976794557444
	data : 0.11441011428833008
	model : 0.06492195129394532
			 train-loss:  2.2101750721534095 	 ± 0.1886596962413929
	data : 0.11442275047302246
	model : 0.06494259834289551
			 train-loss:  2.221917290687561 	 ± 0.1935920826194849
	data : 0.11466565132141113
	model : 0.06500563621520997
			 train-loss:  2.212080556612748 	 ± 0.19610066395974898
	data : 0.11492342948913574
	model : 0.06500535011291504
			 train-loss:  2.2052514067402593 	 ± 0.1955601268440854
	data : 0.11478047370910645
	model : 0.06503596305847167
			 train-loss:  2.207949846982956 	 ± 0.1925474413702497
	data : 0.11475286483764649
	model : 0.06499333381652832
			 train-loss:  2.2111648156725128 	 ± 0.18996182304423564
	data : 0.11471495628356934
	model : 0.06495857238769531
			 train-loss:  2.205573872725169 	 ± 0.18918019202686015
	data : 0.11473808288574219
	model : 0.0649350643157959
			 train-loss:  2.192811885187703 	 ± 0.19879815462509015
	data : 0.11463494300842285
	model : 0.06496229171752929
			 train-loss:  2.194658685475588 	 ± 0.19593727318052936
	data : 0.11465563774108886
	model : 0.06496806144714355
			 train-loss:  2.195746418201562 	 ± 0.1930437774395021
	data : 0.1147371768951416
	model : 0.06500611305236817
			 train-loss:  2.1848700011477753 	 ± 0.20018398264382067
	data : 0.11482977867126465
	model : 0.06505227088928223
			 train-loss:  2.1817000014441352 	 ± 0.19816742586631791
	data : 0.11473231315612793
	model : 0.06505751609802246
			 train-loss:  2.1875160965654583 	 ± 0.19840219226967298
	data : 0.11460399627685547
	model : 0.06495547294616699
			 train-loss:  2.1848199270867013 	 ± 0.19637018431674652
	data : 0.11462669372558594
	model : 0.06498007774353028
			 train-loss:  2.182939664313668 	 ± 0.1941063855982368
	data : 0.11477527618408204
	model : 0.06497201919555665
			 train-loss:  2.188044624450879 	 ± 0.19416876186083165
	data : 0.11468338966369629
	model : 0.06493539810180664
			 train-loss:  2.189889469742775 	 ± 0.19207213512738985
	data : 0.11471352577209473
	model : 0.06497712135314941
			 train-loss:  2.1954851237739006 	 ± 0.19298798192803615
	data : 0.11487994194030762
	model : 0.06503701210021973
			 train-loss:  2.196740522271111 	 ± 0.19084603141144726
	data : 0.11494593620300293
	model : 0.0649946689605713
			 train-loss:  2.193481758583424 	 ± 0.1897925191670015
	data : 0.11479377746582031
	model : 0.06499752998352051
			 train-loss:  2.192875550551848 	 ± 0.1876654967741476
	data : 0.11479005813598633
	model : 0.06497344970703126
			 train-loss:  2.1932819657855562 	 ± 0.18558819078300995
	data : 0.11473774909973145
	model : 0.06493978500366211
			 train-loss:  2.1897841666055764 	 ± 0.18505343325052934
	data : 0.11473140716552735
	model : 0.06495957374572754
			 train-loss:  2.185635969993916 	 ± 0.18522339911278668
	data : 0.11477537155151367
	model : 0.06495962142944336
			 train-loss:  2.187215012808641 	 ± 0.1836032471197215
	data : 0.11483702659606934
	model : 0.06498370170593262
			 train-loss:  2.1878911548731277 	 ± 0.18178045584818733
	data : 0.11494760513305664
	model : 0.06505718231201171
			 train-loss:  2.184196774959564 	 ± 0.1818021593660226
	data : 0.1149487018585205
	model : 0.06507315635681152
			 train-loss:  2.1856773904725615 	 ± 0.1803151611498667
	data : 0.11482381820678711
	model : 0.06506223678588867
			 train-loss:  2.1868696694190684 	 ± 0.1787758222070766
	data : 0.11456575393676757
	model : 0.06502456665039062
			 train-loss:  2.187477177044131 	 ± 0.17713540626335442
	data : 0.1143723964691162
	model : 0.06504530906677246
			 train-loss:  2.201930578108187 	 ± 0.20461583517772824
	data : 0.11431698799133301
	model : 0.0649984359741211
			 train-loss:  2.200298203121532 	 ± 0.20310170157937837
	data : 0.11440839767456054
	model : 0.06505107879638672
			 train-loss:  2.2012786843947003 	 ± 0.2014114260842096
	data : 0.11451263427734375
	model : 0.06506538391113281
			 train-loss:  2.2060510129259345 	 ± 0.20280600379546118
	data : 0.11448955535888672
	model : 0.06510720252990723
			 train-loss:  2.2010928741816818 	 ± 0.2045051869001347
	data : 0.11455349922180176
	model : 0.06505422592163086
			 train-loss:  2.1993383169174194 	 ± 0.20320450055196498
	data : 0.11455745697021484
	model : 0.06500744819641113
			 train-loss:  2.1961186309655507 	 ± 0.2030159702435294
	data : 0.11466059684753419
	model : 0.06493477821350098
			 train-loss:  2.1929303524924104 	 ± 0.20285395652113025
	data : 0.1146730899810791
	model : 0.06491594314575196
			 train-loss:  2.190582807986967 	 ± 0.2020450228753055
	data : 0.11481142044067383
	model : 0.06491508483886718
			 train-loss:  2.1909336873463223 	 ± 0.20045411742052738
	data : 0.11488056182861328
	model : 0.06498856544494629
			 train-loss:  2.1936184149235487 	 ± 0.20002025085241404
	data : 0.11491093635559083
	model : 0.06504836082458496
			 train-loss:  2.1945306282777053 	 ± 0.19860978836342055
	data : 0.11464290618896485
	model : 0.06506128311157226
			 train-loss:  2.194493512312571 	 ± 0.1970996530014226
	data : 0.11451454162597656
	model : 0.06506848335266113
			 train-loss:  2.1963061699226722 	 ± 0.19617672201947098
	data : 0.1145237922668457
	model : 0.0650245189666748
			 train-loss:  2.1978707436253044 	 ± 0.19514956972652867
	data : 0.11456642150878907
	model : 0.06497139930725097
			 train-loss:  2.1952667495478755 	 ± 0.19491668804943701
	data : 0.11460161209106445
	model : 0.0649996280670166
			 train-loss:  2.204035028389522 	 ± 0.20677202982311058
	data : 0.11474404335021973
	model : 0.06499037742614747
			 train-loss:  2.2002808611157914 	 ± 0.2076994411840566
	data : 0.11473665237426758
	model : 0.06502671241760254
			 train-loss:  2.1973537537786694 	 ± 0.2077215185276729
	data : 0.11477956771850586
	model : 0.06505141258239747
			 train-loss:  2.202790760014155 	 ± 0.21138957466178507
	data : 0.11477422714233398
	model : 0.0650205135345459
			 train-loss:  2.200203747362704 	 ± 0.2111166875844829
	data : 0.11458444595336914
	model : 0.06499705314636231
			 train-loss:  2.1978801250457765 	 ± 0.2106549976367278
	data : 0.11448817253112793
	model : 0.0650362491607666
			 train-loss:  2.203640624096519 	 ± 0.21512877701774036
	data : 0.11466522216796875
	model : 0.06498522758483886
			 train-loss:  2.202970315883686 	 ± 0.2138071431493423
	data : 0.11465620994567871
	model : 0.06503567695617676
			 train-loss:  2.2024548848470054 	 ± 0.21248030625140787
	data : 0.11467037200927735
	model : 0.06508984565734863
			 train-loss:  2.201782793938359 	 ± 0.2112146336518337
	data : 0.11474285125732422
	model : 0.06514840126037598
			 train-loss:  2.1996671676635744 	 ± 0.21073103779118563
	data : 0.11485109329223633
	model : 0.06514253616333007
			 train-loss:  2.1970065567228527 	 ± 0.2107738995375859
	data : 0.11484293937683106
	model : 0.06516704559326172
			 train-loss:  2.1964389274759992 	 ± 0.20954703316572007
	data : 0.11480860710144043
	model : 0.06511588096618652
			 train-loss:  2.1995379594435174 	 ± 0.21016291629536496
	data : 0.11463565826416015
	model : 0.06508083343505859
			 train-loss:  2.198509754169555 	 ± 0.20911811158627908
	data : 0.11471552848815918
	model : 0.06503190994262695
			 train-loss:  2.2004215478897096 	 ± 0.20862148911302325
	data : 0.11483259201049804
	model : 0.06500000953674316
			 train-loss:  2.201398698396461 	 ± 0.2076005904440116
	data : 0.11478924751281738
	model : 0.06499485969543457
			 train-loss:  2.2054953095556677 	 ± 0.20987114370548485
	data : 0.11482825279235839
	model : 0.06503834724426269
			 train-loss:  2.202479311011054 	 ± 0.2105629333765046
	data : 0.11487402915954589
	model : 0.06510515213012695
			 train-loss:  2.203930321704136 	 ± 0.20981863885930416
	data : 0.11493616104125977
	model : 0.06511096954345703
			 train-loss:  2.2013147632280985 	 ± 0.21010371059525731
	data : 0.11482458114624024
	model : 0.0651024341583252
			 train-loss:  2.2011313268116544 	 ± 0.20895335230978443
	data : 0.11468443870544434
	model : 0.06504178047180176
			 train-loss:  2.197457044020943 	 ± 0.21074974282007275
	data : 0.114750337600708
	model : 0.06500821113586426
			 train-loss:  2.196157657971946 	 ± 0.2099838130627731
	data : 0.1149825096130371
	model : 0.06501936912536621
			 train-loss:  2.202994204582052 	 ± 0.219022358453166
	data : 0.11505098342895508
	model : 0.06499800682067872
			 train-loss:  2.201848823145816 	 ± 0.2181493897606636
	data : 0.11501088142395019
	model : 0.06503419876098633
			 train-loss:  2.197492544849714 	 ± 0.22112500114563924
	data : 0.11511344909667968
	model : 0.0650984764099121
			 train-loss:  2.1981559758333815 	 ± 0.22007824612654361
	data : 0.11493825912475586
	model : 0.06512255668640136
			 train-loss:  2.19943951830572 	 ± 0.21931714748166928
	data : 0.11484751701354981
	model : 0.06505060195922852
			 train-loss:  2.1972122144217443 	 ± 0.2193178491931008
	data : 0.11462626457214356
	model : 0.06505761146545411
			 train-loss:  2.1984948205947874 	 ± 0.21859135061996066
	data : 0.1146519660949707
	model : 0.06498351097106933
			 train-loss:  2.197393294608239 	 ± 0.21778526956356137
	data : 0.11461501121520996
	model : 0.06498026847839355
			 train-loss:  2.1983429357117297 	 ± 0.2169251094878603
	data : 0.11484127044677735
	model : 0.06494283676147461
			 train-loss:  2.19843718380604 	 ± 0.21587160514621012
	data : 0.11486973762512206
	model : 0.06497530937194824
			 train-loss:  2.196620152546809 	 ± 0.21562127159730018
	data : 0.11499934196472168
	model : 0.06501641273498535
			 train-loss:  2.1947648843129475 	 ± 0.21542450380988937
	data : 0.11501569747924804
	model : 0.0650660514831543
			 train-loss:  2.1973786466526537 	 ± 0.2160723080752726
	data : 0.11504521369934081
	model : 0.06504855155944825
			 train-loss:  2.1940752100721697 	 ± 0.21773299803452537
	data : 0.11488842964172363
	model : 0.06509928703308106
			 train-loss:  2.193916351706893 	 ± 0.21672886029017793
	data : 0.11479287147521973
	model : 0.06507682800292969
			 train-loss:  2.192514789213828 	 ± 0.2162235449234733
	data : 0.11472082138061523
	model : 0.06505017280578614
			 train-loss:  2.189451050758362 	 ± 0.2176022180971838
	data : 0.11478338241577149
	model : 0.0650442123413086
			 train-loss:  2.189327482704644 	 ± 0.2166236870862579
	data : 0.11494965553283691
	model : 0.06507730484008789
			 train-loss:  2.187127531639167 	 ± 0.2168964207995205
	data : 0.11509804725646973
	model : 0.06512255668640136
			 train-loss:  2.186847527470209 	 ± 0.2159549011378969
	data : 0.11517181396484374
	model : 0.06514005661010742
			 train-loss:  2.189278971730617 	 ± 0.21655362879741877
	data : 0.11514501571655274
	model : 0.06513428688049316
			 train-loss:  2.187913891543513 	 ± 0.21610210554317913
	data : 0.11504020690917968
	model : 0.06520595550537109
			 train-loss:  2.190256658299216 	 ± 0.21663036889337356
	data : 0.11484308242797851
	model : 0.06515169143676758
			 train-loss:  2.18913590704274 	 ± 0.21604009409335556
	data : 0.11480493545532226
	model : 0.06509180068969726
			 train-loss:  2.187782118886204 	 ± 0.2156205381780542
	data : 0.114772367477417
	model : 0.0650641918182373
			 train-loss:  2.186792454799684 	 ± 0.2149816246097744
	data : 0.11473894119262695
	model : 0.06502780914306641
			 train-loss:  2.1882860948642096 	 ± 0.21470314563438378
	data : 0.11471676826477051
	model : 0.06493730545043945
			 train-loss:  2.186532936805536 	 ± 0.21467486518470336
	data : 0.11475124359130859
	model : 0.06499776840209961
			 train-loss:  2.186245550874804 	 ± 0.2138166103729492
	data : 0.11483101844787598
	model : 0.06500678062438965
			 train-loss:  2.187027944781916 	 ± 0.21312094368812734
	data : 0.11479692459106446
	model : 0.06502823829650879
			 train-loss:  2.1860790406503985 	 ± 0.2125205720194096
	data : 0.11491599082946777
	model : 0.06507601737976074
			 train-loss:  2.1849702949523926 	 ± 0.21202855678902874
	data : 0.11504364013671875
	model : 0.06513028144836426
			 train-loss:  2.1828153653750344 	 ± 0.2125553564707979
	data : 0.11518688201904297
	model : 0.06516213417053222
			 train-loss:  2.1835779383426575 	 ± 0.21188984043022152
	data : 0.11519942283630372
	model : 0.06513009071350098
			 train-loss:  2.1828252458944917 	 ± 0.2112309059260688
	data : 0.11513347625732422
	model : 0.06510715484619141
			 train-loss:  2.183979643407718 	 ± 0.2108155423836828
	data : 0.11509747505187988
	model : 0.06507258415222168
			 train-loss:  2.1824619458271908 	 ± 0.21070942447564162
	data : 0.11494755744934082
	model : 0.0650726318359375
			 train-loss:  2.184662979067737 	 ± 0.2113985174056805
	data : 0.1149327278137207
	model : 0.06501426696777343
			 train-loss:  2.1849616585355816 	 ± 0.21062398733686166
	data : 0.11484651565551758
	model : 0.06501679420471192
			 train-loss:  2.182133239911015 	 ± 0.2123320638371603
	data : 0.11494865417480468
	model : 0.06506590843200684
			 train-loss:  2.1806323448223854 	 ± 0.21224527806963214
	data : 0.11499848365783691
	model : 0.0651005744934082
			 train-loss:  2.178990296964292 	 ± 0.2123103289536789
	data : 0.11502370834350586
	model : 0.06508445739746094
			 train-loss:  2.177702206022599 	 ± 0.21205712922445194
	data : 0.1150435447692871
	model : 0.06512131690979003
			 train-loss:  2.1795006616272197 	 ± 0.21232021810614551
	data : 0.11497893333435058
	model : 0.06514616012573242
			 train-loss:  2.181637295778247 	 ± 0.21302263355923504
	data : 0.11492619514465333
	model : 0.06504039764404297
			 train-loss:  2.1805224384335307 	 ± 0.21265864315208274
	data : 0.11490068435668946
	model : 0.06503167152404785
			 train-loss:  2.180337335382189 	 ± 0.21190902450292745
	data : 0.11488804817199708
	model : 0.06502957344055176
			 train-loss:  2.1823498052908175 	 ± 0.21249461481265852
	data : 0.11480627059936524
	model : 0.06497759819030761
			 train-loss:  2.182132729342286 	 ± 0.2117607609512641
	data : 0.11491694450378417
	model : 0.06497812271118164
			 train-loss:  2.181901281530207 	 ± 0.21103706260513083
	data : 0.11487994194030762
	model : 0.06508016586303711
			 train-loss:  2.1806243144803577 	 ± 0.21085668538456054
	data : 0.11485776901245118
	model : 0.06510205268859863
			 train-loss:  2.1797266697061475 	 ± 0.21040424822139459
	data : 0.11491951942443848
	model : 0.06513805389404297
			 train-loss:  2.179221107535166 	 ± 0.2097708031417165
	data : 0.11492657661437988
	model : 0.06513738632202148
			 train-loss:  2.1792914737649514 	 ± 0.20905780838526736
	data : 0.11474447250366211
	model : 0.06513915061950684
			 train-loss:  2.1785302548795134 	 ± 0.20855464950201502
	data : 0.11476263999938965
	model : 0.06513853073120117
			 train-loss:  2.1818119263488973 	 ± 0.21165300660793623
	data : 0.11475253105163574
	model : 0.06507549285888672
			 train-loss:  2.1806188074747723 	 ± 0.21144846874828047
	data : 0.11479477882385254
	model : 0.06508836746215821
			 train-loss:  2.1812893886439846 	 ± 0.2109071148008407
	data : 0.1149402141571045
	model : 0.06512808799743652
			 train-loss:  2.1806055150533976 	 ± 0.21038010297074888
	data : 0.1151036262512207
	model : 0.06515998840332031
			 train-loss:  2.182238073909984 	 ± 0.21065522976540665
	data : 0.11513671875
	model : 0.06513819694519044
			 train-loss:  2.181603815648463 	 ± 0.21011668614135567
	data : 0.1152839183807373
	model : 0.06517372131347657
			 train-loss:  2.1818992707037155 	 ± 0.2094698847676134
	data : 0.11526646614074706
	model : 0.06513104438781739
			 train-loss:  2.1860513427318673 	 ± 0.2151011845119343
	data : 0.11512799263000488
	model : 0.06507973670959473
			 train-loss:  2.18657398072018 	 ± 0.21451439836268063
	data : 0.11490468978881836
	model : 0.06505913734436035
			 train-loss:  2.186709420590461 	 ± 0.21384121187660868
	data : 0.11487312316894531
	model : 0.06505680084228516
			 train-loss:  2.184778446671348 	 ± 0.21454508694996374
	data : 0.11466288566589355
	model : 0.06503338813781738
			 train-loss:  2.185454023629427 	 ± 0.21404316770632364
	data : 0.11470527648925781
	model : 0.0650242805480957
			 train-loss:  2.186047154924144 	 ± 0.21350926075013898
	data : 0.1147834300994873
	model : 0.06506061553955078
			 train-loss:  2.186970704131656 	 ± 0.2131716021138446
	data : 0.1149296760559082
	model : 0.06503882408142089
			 train-loss:  2.1864841035538656 	 ± 0.21260692425701389
	data : 0.11493453979492188
	model : 0.06502113342285157
			 train-loss:  2.188409039886986 	 ± 0.21337774254996947
	data : 0.11507811546325683
	model : 0.06501955986022949
			 train-loss:  2.188012401262919 	 ± 0.21279079419109775
	data : 0.11499714851379395
	model : 0.06497721672058106
			 train-loss:  2.190284642110388 	 ± 0.2141472844208693
	data : 0.1148747444152832
	model : 0.0649411678314209
			 train-loss:  2.1903117481106054 	 ± 0.2135054478101843
	data : 0.11478924751281738
	model : 0.06490797996520996
			 train-loss:  2.1884361448742093 	 ± 0.21424454880773477
	data : 0.11479630470275878
	model : 0.06489782333374024
			 train-loss:  2.186987072758421 	 ± 0.21443388920640638
	data : 0.11482090950012207
	model : 0.06494340896606446
			 train-loss:  2.185357428298277 	 ± 0.2148493224514116
	data : 0.11498732566833496
	model : 0.06501784324645996
			 train-loss:  2.186399641789888 	 ± 0.21465074891075822
	data : 0.11510677337646484
	model : 0.06506452560424805
			 train-loss:  2.1855427657448967 	 ± 0.2143189699435537
	data : 0.11518998146057129
	model : 0.06513099670410157
			 train-loss:  2.186247848361903 	 ± 0.21389862724209924
	data : 0.11524991989135742
	model : 0.06529726982116699
			 train-loss:  2.18946882020468 	 ± 0.21744997756092246
	data : 0.11513004302978516
	model : 0.06526379585266114
			 train-loss:  2.188301168169294 	 ± 0.21737416866280604
	data : 0.1149825096130371
	model : 0.06521115303039551
			 train-loss:  2.1880323656580667 	 ± 0.21678491458670662
	data : 0.11489682197570801
	model : 0.06518878936767578
			 train-loss:  2.1868793836421214 	 ± 0.2167121488349706
	data : 0.11476297378540039
	model : 0.06517534255981446
			 train-loss:  2.187749081113365 	 ± 0.21641208382644683
	data : 0.11466422080993652
	model : 0.06500415802001953
			 train-loss:  2.1885790392007243 	 ± 0.216090625253322
	data : 0.11486015319824219
	model : 0.06500678062438965
			 train-loss:  2.188433243830999 	 ± 0.21549836577574416
	data : 0.11495304107666016
	model : 0.06503281593322754
			 train-loss:  2.1871709902642182 	 ± 0.2155684687254237
	data : 0.11497683525085449
	model : 0.06505303382873535
			 train-loss:  2.188917712850885 	 ± 0.21625603930054269
	data : 0.11509151458740234
	model : 0.06503491401672364
			 train-loss:  2.1893258225070973 	 ± 0.21573463252538638
	data : 0.11520037651062012
	model : 0.06504974365234376
			 train-loss:  2.1882190276747164 	 ± 0.21566794651967916
	data : 0.11499576568603516
	model : 0.0650564193725586
			 train-loss:  2.188571905445408 	 ± 0.215137527028282
	data : 0.1149374008178711
	model : 0.0650670051574707
			 train-loss:  2.1891602905847694 	 ± 0.21470762084358813
	data : 0.11483259201049804
	model : 0.06501669883728027
			 train-loss:  2.191448953699938 	 ± 0.2163957159014262
	data : 0.1147923469543457
	model : 0.0655019760131836
			 train-loss:  2.191557175301491 	 ± 0.21582450194146624
	data : 0.11459174156188964
	model : 0.06552567481994628
			 train-loss:  2.197604908514275 	 ± 0.23067265540009302
	data : 0.11469354629516601
	model : 0.06551198959350586
			 train-loss:  2.1963142125230086 	 ± 0.23074807984464224
	data : 0.11469635963439942
	model : 0.06551709175109863
			 train-loss:  2.1963568239311897 	 ± 0.23014398406665793
	data : 0.11487703323364258
	model : 0.06555638313293458
			 train-loss:  2.197843395794431 	 ± 0.23046144496794557
	data : 0.11501111984252929
	model : 0.06507368087768554
			 train-loss:  2.1970767684551102 	 ± 0.23010894199885118
	data : 0.11513404846191407
	model : 0.06505541801452637
			 train-loss:  2.1956252025574754 	 ± 0.23039931815079592
	data : 0.11515769958496094
	model : 0.06513590812683105
			 train-loss:  2.1951155790915857 	 ± 0.22991738923580385
	data : 0.11514101028442383
	model : 0.06514153480529786
			 train-loss:  2.1948834627258536 	 ± 0.22935302041356045
	data : 0.11492371559143066
	model : 0.06512131690979003
			 train-loss:  2.19621122004417 	 ± 0.2295241268021622
	data : 0.11481266021728516
	model : 0.06508264541625977
			 train-loss:  2.1968104508188038 	 ± 0.22909822295053806
	data : 0.11476478576660157
	model : 0.06506991386413574
			 train-loss:  2.19589817763573 	 ± 0.22888213347502032
	data : 0.1147730827331543
	model : 0.06501908302307129
			 train-loss:  2.1976211148500444 	 ± 0.2295992811077731
	data : 0.11488451957702636
	model : 0.06498203277587891
			 train-loss:  2.1975501704571854 	 ± 0.22902962405812852
	data : 0.11491632461547852
	model : 0.06500272750854492
			 train-loss:  2.1982953625150246 	 ± 0.22870616556046242
	data : 0.11499924659729004
	model : 0.06499752998352051
			 train-loss:  2.199778774688984 	 ± 0.22911426403987806
	data : 0.11508193016052246
	model : 0.06502757072448731
			 train-loss:  2.200925746969148 	 ± 0.22913550877504577
	data : 0.11502790451049805
	model : 0.06500000953674316
			 train-loss:  2.200033733902908 	 ± 0.22893075154029385
	data : 0.1148451805114746
	model : 0.06496829986572265
			 train-loss:  2.1984892470165365 	 ± 0.22944256388838064
	data : 0.11493010520935058
	model : 0.06498045921325683
			 train-loss:  2.2007114311347262 	 ± 0.23109915810849502
	data : 0.11485176086425782
	model : 0.06501288414001465
			 train-loss:  2.1992394265073996 	 ± 0.2315136808581163
	data : 0.11493115425109864
	model : 0.06556196212768554
			 train-loss:  2.197416906151475 	 ± 0.2324500389342899
	data : 0.11494483947753906
	model : 0.06556839942932129
			 train-loss:  2.1958633666946774 	 ± 0.23298098384807028
	data : 0.11514296531677246
	model : 0.06564168930053711
			 train-loss:  2.1953575526368563 	 ± 0.2325437916669129
	data : 0.11515884399414063
	model : 0.06567816734313965
			 train-loss:  2.1976933934778535 	 ± 0.2344627578820456
	data : 0.11510190963745118
	model : 0.06568055152893067
			 train-loss:  2.1986887080008994 	 ± 0.23436022412922808
	data : 0.1150726318359375
	model : 0.06510796546936035
			 train-loss:  2.1976721710133775 	 ± 0.23428222321593536
	data : 0.11503753662109376
	model : 0.06510648727416993
			 train-loss:  2.1978984732960547 	 ± 0.2337601888042051
	data : 0.11495237350463867
	model : 0.06506972312927246
			 train-loss:  2.1979718252464577 	 ± 0.23322092960068908
	data : 0.11479721069335938
	model : 0.06513619422912598
			 train-loss:  2.1990090185596096 	 ± 0.2331817186873067
	data : 0.11472349166870117
	model : 0.06514062881469726
			 train-loss:  2.1987565373061995 	 ± 0.23267601138889824
	data : 0.11468157768249512
	model : 0.06576724052429199
			 train-loss:  2.1968080055768087 	 ± 0.23392010893911838
	data : 0.11422977447509766
	model : 0.06580333709716797
			 train-loss:  2.1966509185054086 	 ± 0.2333994440830431
	data : 0.11428322792053222
	model : 0.06578989028930664
			 train-loss:  2.196194861809053 	 ± 0.23296901773014186
	data : 0.11436433792114258
	model : 0.06628551483154296
			 train-loss:  2.1966269064593957 	 ± 0.23253243996868192
	data : 0.11399812698364258
	model : 0.06624884605407715
			 train-loss:  2.1974978730282975 	 ± 0.23237312377792008
	data : 0.11412215232849121
	model : 0.06561617851257324
			 train-loss:  2.197147556181465 	 ± 0.23191286381186937
	data : 0.11453533172607422
	model : 0.06547713279724121
			 train-loss:  2.19705768638187 	 ± 0.2314008371596783
	data : 0.1144256591796875
	model : 0.0654083251953125
			 train-loss:  2.1965650937198538 	 ± 0.2310065202363868
	data : 0.11439046859741211
	model : 0.06467609405517578
			 train-loss:  2.1951184582605237 	 ± 0.23152082211993338
	data : 0.11493196487426757
	model : 0.06455893516540527
			 train-loss:  2.1982559451931403 	 ± 0.2357993705820346
	data : 0.11504731178283692
	model : 0.06434979438781738
			 train-loss:  2.1969259206905116 	 ± 0.23613950665272396
	data : 0.11529068946838379
	model : 0.06433696746826172
			 train-loss:  2.197513438826022 	 ± 0.23579327675966347
	data : 0.11546168327331544
	model : 0.06428532600402832
			 train-loss:  2.1958909530144233 	 ± 0.23656552833945363
	data : 0.11551203727722167
	model : 0.06433439254760742
			 train-loss:  2.195825753540828 	 ± 0.23605721821454756
	data : 0.11571235656738281
	model : 0.06437878608703614
			 train-loss:  2.1963588577483346 	 ± 0.23569002979115705
	data : 0.11558828353881836
	model : 0.0644411563873291
			 train-loss:  2.1960850016683593 	 ± 0.23522302653849592
	data : 0.11549944877624511
	model : 0.0645209789276123
			 train-loss:  2.197100332950024 	 ± 0.23523532045653134
	data : 0.11534318923950196
	model : 0.06453638076782227
			 train-loss:  2.196815149258759 	 ± 0.23477711847635085
	data : 0.1154064655303955
	model : 0.06444478034973145
			 train-loss:  2.1957938635902567 	 ± 0.2348060349234865
	data : 0.11522936820983887
	model : 0.06441740989685059
			 train-loss:  2.1944588923654638 	 ± 0.23521179438985645
	data : 0.11530570983886719
	model : 0.06447014808654786
			 train-loss:  2.193946047308056 	 ± 0.23485250899227803
	data : 0.11534171104431153
	model : 0.06447305679321289
			 train-loss:  2.19386494855086 	 ± 0.23436607575378787
	data : 0.11535377502441406
	model : 0.06450395584106446
			 train-loss:  2.196572885473734 	 ± 0.23761196113359137
	data : 0.1153022289276123
	model : 0.064628267288208
			 train-loss:  2.196661009276209 	 ± 0.23712446551282484
	data : 0.11532421112060547
	model : 0.06466822624206543
			 train-loss:  2.1953052292144837 	 ± 0.23757409565354345
	data : 0.115155029296875
	model : 0.06474032402038574
			 train-loss:  2.196878975043531 	 ± 0.23835260803239133
	data : 0.11502685546875
	model : 0.06466197967529297
			 train-loss:  2.198069892124254 	 ± 0.23859199767313946
	data : 0.11503720283508301
	model : 0.06460008621215821
			 train-loss:  2.1974738734524424 	 ± 0.2382892520614121
	data : 0.11511120796203614
	model : 0.06453633308410645
			 train-loss:  2.1964713397778963 	 ± 0.23832568073139507
	data : 0.11516299247741699
	model : 0.06461796760559083
			 train-loss:  2.1975477635860443 	 ± 0.23844558568935212
	data : 0.1153146743774414
	model : 0.06447978019714355
			 train-loss:  2.195836248168026 	 ± 0.23948783158586875
	data : 0.11530265808105469
	model : 0.06444597244262695
			 train-loss:  2.196503922462463 	 ± 0.23924047576380184
	data : 0.11548552513122559
	model : 0.06447649002075195
			 train-loss:  2.1966398949642105 	 ± 0.2387731046005746
	data : 0.11546125411987304
	model : 0.06448135375976563
			 train-loss:  2.1957565721065277 	 ± 0.23870944702574648
	data : 0.11521797180175782
	model : 0.06439681053161621
			 train-loss:  2.194595076821067 	 ± 0.23894965992677558
	data : 0.11506319046020508
	model : 0.06452593803405762
			 train-loss:  2.194509525937358 	 ± 0.23848270497486534
	data : 0.11511435508728027
	model : 0.06460103988647461
			 train-loss:  2.195876781613219 	 ± 0.23901002039432037
	data : 0.11504526138305664
	model : 0.06464228630065919
			 train-loss:  2.194986229762435 	 ± 0.23896627020360972
	data : 0.11486611366271973
	model : 0.05626072883605957
#epoch  39    val-loss:  2.436180491196482  train-loss:  2.194986229762435  lr:  4.8828125e-06
			 train-loss:  2.0622644424438477 	 ± 0.0
	data : 5.884216070175171
	model : 0.07126617431640625
			 train-loss:  2.3254226446151733 	 ± 0.2631582021713257
	data : 3.0077342987060547
	model : 0.06798970699310303
			 train-loss:  2.270392656326294 	 ± 0.22852736991918046
	data : 2.0434609254201255
	model : 0.06684303283691406
			 train-loss:  2.2244154810905457 	 ± 0.21333136438322797
	data : 1.561334490776062
	model : 0.06625586748123169
			 train-loss:  2.2102198123931887 	 ± 0.19291004344706414
	data : 1.2719555854797364
	model : 0.06597781181335449
			 train-loss:  2.188987890879313 	 ± 0.18238935415244759
	data : 0.11804885864257812
	model : 0.06463923454284667
			 train-loss:  2.186535562787737 	 ± 0.16896654081198123
	data : 0.11467838287353516
	model : 0.06460614204406738
			 train-loss:  2.2157546281814575 	 ± 0.17594674450554237
	data : 0.11456847190856934
	model : 0.06465144157409668
			 train-loss:  2.259762340121799 	 ± 0.2073909143924751
	data : 0.11451468467712403
	model : 0.06468820571899414
			 train-loss:  2.2457788467407225 	 ± 0.20117090997343456
	data : 0.11458206176757812
	model : 0.06475739479064942
			 train-loss:  2.2301268577575684 	 ± 0.1980921901309322
	data : 0.1145287036895752
	model : 0.06487045288085938
			 train-loss:  2.194984962542852 	 ± 0.22260941938830953
	data : 0.11455302238464356
	model : 0.0649498462677002
			 train-loss:  2.1888251212927012 	 ± 0.2149380325196122
	data : 0.11458468437194824
	model : 0.06499123573303223
			 train-loss:  2.162325280053275 	 ± 0.22809562848950565
	data : 0.11449608802795411
	model : 0.06508798599243164
			 train-loss:  2.1865753809611004 	 ± 0.23831082966756603
	data : 0.11439895629882812
	model : 0.06500225067138672
			 train-loss:  2.199278712272644 	 ± 0.23593042981343817
	data : 0.11447381973266602
	model : 0.06494722366333008
			 train-loss:  2.186075441977557 	 ± 0.23490016022272017
	data : 0.11445608139038085
	model : 0.06496939659118653
			 train-loss:  2.197119745943281 	 ± 0.2327793786879103
	data : 0.11447086334228515
	model : 0.06497087478637695
			 train-loss:  2.1870872535203634 	 ± 0.23053426663933377
	data : 0.11449933052062988
	model : 0.06496500968933105
			 train-loss:  2.177012640237808 	 ± 0.22894803889366752
	data : 0.11462397575378418
	model : 0.06497883796691895
			 train-loss:  2.1758773156574795 	 ± 0.22348809004852177
	data : 0.1146009922027588
	model : 0.06499366760253907
			 train-loss:  2.175222846594724 	 ± 0.21837034249710405
	data : 0.11459784507751465
	model : 0.06546893119812011
			 train-loss:  2.1669926021410073 	 ± 0.21703117648585904
	data : 0.1139552116394043
	model : 0.0654693603515625
			 train-loss:  2.166891078154246 	 ± 0.21246214521331092
	data : 0.11394610404968261
	model : 0.06548843383789063
			 train-loss:  2.164445037841797 	 ± 0.20851415141361357
	data : 0.11405134201049805
	model : 0.06550111770629882
			 train-loss:  2.173401814240676 	 ± 0.20931199400951012
	data : 0.1140322208404541
	model : 0.06549215316772461
			 train-loss:  2.1704248940503157 	 ± 0.20595940300472093
	data : 0.11408209800720215
	model : 0.06503829956054688
			 train-loss:  2.1694564649036954 	 ± 0.20231071064019487
	data : 0.1146928310394287
	model : 0.06503281593322754
			 train-loss:  2.16877766313224 	 ± 0.1988244424586521
	data : 0.1147430419921875
	model : 0.06499896049499512
			 train-loss:  2.1762972116470336 	 ± 0.19963270851195153
	data : 0.11455516815185547
	model : 0.06499752998352051
			 train-loss:  2.1792109397149857 	 ± 0.19703381678810356
	data : 0.11452755928039551
	model : 0.06497516632080078
			 train-loss:  2.1834841072559357 	 ± 0.19538471303807683
	data : 0.11447281837463379
	model : 0.06489458084106445
			 train-loss:  2.1810739184870864 	 ± 0.19288403403325352
	data : 0.11449828147888183
	model : 0.06490812301635743
			 train-loss:  2.178564604590921 	 ± 0.19057228963375414
	data : 0.1146005630493164
	model : 0.06488590240478516
			 train-loss:  2.174087313243321 	 ± 0.18963574268189515
	data : 0.11466560363769532
	model : 0.06490273475646972
			 train-loss:  2.172980593310462 	 ± 0.1870979620220024
	data : 0.11468009948730469
	model : 0.06495685577392578
			 train-loss:  2.1827121811944084 	 ± 0.19356881389663536
	data : 0.11467323303222657
	model : 0.06501398086547852
			 train-loss:  2.183454394340515 	 ± 0.19105822405280432
	data : 0.11467008590698242
	model : 0.06499509811401367
			 train-loss:  2.1792086637937107 	 ± 0.19040026522312214
	data : 0.11447815895080567
	model : 0.06503243446350097
			 train-loss:  2.1740944653749468 	 ± 0.19069871647523587
	data : 0.11433043479919433
	model : 0.06505537033081055
			 train-loss:  2.178597488054415 	 ± 0.1904996414019453
	data : 0.11431760787963867
	model : 0.06512246131896973
			 train-loss:  2.17628603605997 	 ± 0.1887991479828825
	data : 0.11447162628173828
	model : 0.06511321067810058
			 train-loss:  2.1807249729023424 	 ± 0.18879548927932147
	data : 0.1144669532775879
	model : 0.06512517929077148
			 train-loss:  2.185201192444021 	 ± 0.18893179490267178
	data : 0.11475133895874023
	model : 0.06511940956115722
			 train-loss:  2.1869351731406317 	 ± 0.18717449148604642
	data : 0.11484951972961426
	model : 0.06505646705627441
			 train-loss:  2.1902637922245525 	 ± 0.18647053918140574
	data : 0.11493391990661621
	model : 0.06495919227600097
			 train-loss:  2.1912683451429324 	 ± 0.18460191689117403
	data : 0.11486248970031739
	model : 0.0649416446685791
			 train-loss:  2.189759356280168 	 ± 0.18296156254697776
	data : 0.11471543312072754
	model : 0.06488194465637206
			 train-loss:  2.182359751389951 	 ± 0.18820194150335517
	data : 0.11444249153137206
	model : 0.06485767364501953
			 train-loss:  2.190187451839447 	 ± 0.19420078061620805
	data : 0.11444263458251953
	model : 0.06491589546203613
			 train-loss:  2.1852235794067383 	 ± 0.19546471698534204
	data : 0.1143758773803711
	model : 0.06495895385742187
			 train-loss:  2.183604909823491 	 ± 0.19392096510656862
	data : 0.11436820030212402
	model : 0.06497049331665039
			 train-loss:  2.18519093855372 	 ± 0.19242300098597312
	data : 0.11459369659423828
	model : 0.06501297950744629
			 train-loss:  2.1873992107532643 	 ± 0.1913096600408357
	data : 0.11458845138549804
	model : 0.06503524780273437
			 train-loss:  2.187447873028842 	 ± 0.18956284054526606
	data : 0.1145538330078125
	model : 0.06496720314025879
			 train-loss:  2.186817262853895 	 ± 0.1879208942362597
	data : 0.11482963562011719
	model : 0.06495990753173828
			 train-loss:  2.1930177211761475 	 ± 0.19195747522965703
	data : 0.11484355926513672
	model : 0.06495232582092285
			 train-loss:  2.1923825288641043 	 ± 0.19035589129405292
	data : 0.11481409072875977
	model : 0.06498336791992188
			 train-loss:  2.18648393881523 	 ± 0.19400830332836994
	data : 0.11491265296936035
	model : 0.06500945091247559
			 train-loss:  2.1855598549048105 	 ± 0.19251567035183534
	data : 0.11504983901977539
	model : 0.06508030891418456
			 train-loss:  2.1856736882788237 	 ± 0.19093318830935682
	data : 0.11478180885314941
	model : 0.06509747505187988
			 train-loss:  2.189067461798268 	 ± 0.1912330263505084
	data : 0.11477608680725097
	model : 0.06508502960205079
			 train-loss:  2.185503397669111 	 ± 0.1917736982826292
	data : 0.11473731994628907
	model : 0.06508216857910157
			 train-loss:  2.1925698574632406 	 ± 0.19836432180837235
	data : 0.11469745635986328
	model : 0.06499676704406739
			 train-loss:  2.1909109243979823 	 ± 0.1972794360113494
	data : 0.11459813117980958
	model : 0.06499381065368652
			 train-loss:  2.186091229771123 	 ± 0.1995981113369663
	data : 0.11464419364929199
	model : 0.06495661735534668
			 train-loss:  2.183609601277024 	 ± 0.19912621054850643
	data : 0.1147087574005127
	model : 0.06500277519226075
			 train-loss:  2.1835948106120613 	 ± 0.19765666141153485
	data : 0.11472668647766113
	model : 0.06500043869018554
			 train-loss:  2.1863392660583276 	 ± 0.1975199460084777
	data : 0.11476078033447265
	model : 0.06508049964904786
			 train-loss:  2.1818812251091004 	 ± 0.19956978548472448
	data : 0.11477670669555665
	model : 0.06505475044250489
			 train-loss:  2.18893864289136 	 ± 0.20676953435259104
	data : 0.11476240158081055
	model : 0.06504335403442382
			 train-loss:  2.1984417918655605 	 ± 0.22039019589445275
	data : 0.11452250480651856
	model : 0.06494178771972656
			 train-loss:  2.1970503248580515 	 ± 0.21919369468034364
	data : 0.11469445228576661
	model : 0.06491484642028808
			 train-loss:  2.1980191388645687 	 ± 0.2178649238362579
	data : 0.11482915878295899
	model : 0.06485929489135742
			 train-loss:  2.204010885556539 	 ± 0.2224610815569107
	data : 0.11487026214599609
	model : 0.06487874984741211
			 train-loss:  2.2041794547909186 	 ± 0.220997497288905
	data : 0.11486320495605469
	model : 0.06490039825439453
			 train-loss:  2.206975672152135 	 ± 0.22090685971503968
	data : 0.1149972915649414
	model : 0.0649627685546875
			 train-loss:  2.2036730219156313 	 ± 0.22139123532164176
	data : 0.11489439010620117
	model : 0.06497244834899903
			 train-loss:  2.201468885699405 	 ± 0.2208451687956363
	data : 0.11472430229187011
	model : 0.06500072479248047
			 train-loss:  2.2047191455960276 	 ± 0.22135379415173845
	data : 0.11461868286132812
	model : 0.06538877487182618
			 train-loss:  2.202226416564282 	 ± 0.22111013227131718
	data : 0.11415820121765137
	model : 0.06537489891052246
			 train-loss:  2.2032166908426984 	 ± 0.21993841626456664
	data : 0.11428089141845703
	model : 0.06537909507751465
			 train-loss:  2.199913493121963 	 ± 0.22064635117330433
	data : 0.11410493850708008
	model : 0.06543126106262206
			 train-loss:  2.2069079393432256 	 ± 0.22839829302480824
	data : 0.11417145729064941
	model : 0.06544017791748047
			 train-loss:  2.20367831622853 	 ± 0.22897210374561605
	data : 0.11417212486267089
	model : 0.06509780883789062
			 train-loss:  2.205072979594386 	 ± 0.22799983789576553
	data : 0.11456899642944336
	model : 0.06511940956115722
			 train-loss:  2.2097433188865923 	 ± 0.23078615129972116
	data : 0.11441068649291992
	model : 0.065067720413208
			 train-loss:  2.2094567120075226 	 ± 0.22948669090496968
	data : 0.1145444393157959
	model : 0.06500697135925293
			 train-loss:  2.209800422861335 	 ± 0.22821657575000168
	data : 0.11453790664672851
	model : 0.06498131752014161
			 train-loss:  2.2086130168702867 	 ± 0.22722145947214145
	data : 0.11460413932800292
	model : 0.06492724418640136
			 train-loss:  2.211499096273066 	 ± 0.2276222405289781
	data : 0.1147535800933838
	model : 0.06492671966552735
			 train-loss:  2.2084804021793865 	 ± 0.22820593880074855
	data : 0.11476635932922363
	model : 0.06501593589782714
			 train-loss:  2.2090993030096895 	 ± 0.22705332426413874
	data : 0.11476845741271972
	model : 0.06503562927246094
			 train-loss:  2.207056334678163 	 ± 0.22670008724384966
	data : 0.11468009948730469
	model : 0.06506028175354003
			 train-loss:  2.2048307205501354 	 ± 0.2265338098953858
	data : 0.11461100578308106
	model : 0.06504392623901367
			 train-loss:  2.2054336853325367 	 ± 0.22542747804139818
	data : 0.11451277732849122
	model : 0.065028715133667
			 train-loss:  2.206748268038956 	 ± 0.2246320455068677
	data : 0.11468381881713867
	model : 0.06503629684448242
			 train-loss:  2.2083103863560423 	 ± 0.22401197102859652
	data : 0.11463255882263183
	model : 0.06506690979003907
			 train-loss:  2.2112738544290718 	 ± 0.22480020178785473
	data : 0.11475172042846679
	model : 0.06508941650390625
			 train-loss:  2.2127169811725618 	 ± 0.22413379549189238
	data : 0.11480574607849121
	model : 0.06511502265930176
			 train-loss:  2.211272953760506 	 ± 0.22348846518940943
	data : 0.11485772132873535
	model : 0.06512260437011719
			 train-loss:  2.2102565145960043 	 ± 0.22262471714959356
	data : 0.11471891403198242
	model : 0.06513991355895996
			 train-loss:  2.2154166270228264 	 ± 0.22758847082130348
	data : 0.11475481986999511
	model : 0.06504173278808593
			 train-loss:  2.212574744453797 	 ± 0.2283206675027461
	data : 0.11467447280883789
	model : 0.06497378349304199
			 train-loss:  2.2144320226850964 	 ± 0.22801884587618593
	data : 0.11480298042297363
	model : 0.06497716903686523
			 train-loss:  2.2169588815491155 	 ± 0.22841305688303426
	data : 0.11472544670104981
	model : 0.06500682830810547
			 train-loss:  2.2155520949408274 	 ± 0.22780410447294192
	data : 0.11471776962280274
	model : 0.06500215530395508
			 train-loss:  2.2168173138742095 	 ± 0.2271243861589795
	data : 0.11474261283874512
	model : 0.06506829261779785
			 train-loss:  2.217744163416941 	 ± 0.22628522454967892
	data : 0.11470823287963867
	model : 0.06511111259460449
			 train-loss:  2.215442687814886 	 ± 0.22653223375008108
	data : 0.11445698738098145
	model : 0.06505603790283203
			 train-loss:  2.2148362322970554 	 ± 0.22559919248508536
	data : 0.11451034545898438
	model : 0.06504526138305664
			 train-loss:  2.213894722717149 	 ± 0.22480874297497996
	data : 0.11457428932189942
	model : 0.0650320053100586
			 train-loss:  2.209932139489503 	 ± 0.22770672972351347
	data : 0.11448044776916504
	model : 0.06510334014892578
			 train-loss:  2.208978477277254 	 ± 0.226932363008309
	data : 0.11466541290283203
	model : 0.0651031494140625
			 train-loss:  2.207025282279305 	 ± 0.2269039284780379
	data : 0.11483736038208008
	model : 0.06519560813903809
			 train-loss:  2.206690633091433 	 ± 0.22595227820293598
	data : 0.1148144245147705
	model : 0.06516714096069336
			 train-loss:  2.2085871767793965 	 ± 0.22590995374621453
	data : 0.11473307609558106
	model : 0.06512322425842285
			 train-loss:  2.211554885920832 	 ± 0.2272295280357457
	data : 0.1147505760192871
	model : 0.06505303382873535
			 train-loss:  2.2145979053833904 	 ± 0.2286745321249476
	data : 0.11461024284362793
	model : 0.06503753662109375
			 train-loss:  2.2131615867217382 	 ± 0.22825812794578673
	data : 0.11471414566040039
	model : 0.06500163078308105
			 train-loss:  2.21241922319428 	 ± 0.2274583747437637
	data : 0.1147125244140625
	model : 0.0650702953338623
			 train-loss:  2.21817275832911 	 ± 0.23519931773605182
	data : 0.11479291915893555
	model : 0.06510305404663086
			 train-loss:  2.2185994696810964 	 ± 0.23428868400878142
	data : 0.11480669975280762
	model : 0.06511993408203125
			 train-loss:  2.218177688698615 	 ± 0.23338894187439785
	data : 0.11476845741271972
	model : 0.06510343551635742
			 train-loss:  2.215595724105835 	 ± 0.23422486227421452
	data : 0.11459932327270508
	model : 0.06508054733276367
			 train-loss:  2.2174702163726563 	 ± 0.23423299133997597
	data : 0.11458244323730468
	model : 0.06497468948364257
			 train-loss:  2.2147844100561667 	 ± 0.23524879089562686
	data : 0.11457490921020508
	model : 0.06533699035644532
			 train-loss:  2.213651468977332 	 ± 0.23467561833242281
	data : 0.11441783905029297
	model : 0.06534337997436523
			 train-loss:  2.2116735027742016 	 ± 0.2348329341106278
	data : 0.11461830139160156
	model : 0.06535472869873046
			 train-loss:  2.2102052239271313 	 ± 0.23452165520721158
	data : 0.11464571952819824
	model : 0.06540226936340332
			 train-loss:  2.208307142476089 	 ± 0.2346250392502083
	data : 0.1147303581237793
	model : 0.06549482345581055
			 train-loss:  2.206493650421952 	 ± 0.23465442368049622
	data : 0.11471896171569824
	model : 0.06515583992004395
			 train-loss:  2.2079240881410755 	 ± 0.2343475738589233
	data : 0.11495385169982911
	model : 0.0651102066040039
			 train-loss:  2.2101691160629047 	 ± 0.23490271111856809
	data : 0.11481046676635742
	model : 0.06505937576293945
			 train-loss:  2.208591923007259 	 ± 0.23474215288134034
	data : 0.11476893424987793
	model : 0.0650454044342041
			 train-loss:  2.2081364612368977 	 ± 0.23393740166039495
	data : 0.11481304168701172
	model : 0.0649958610534668
			 train-loss:  2.2066794633865356 	 ± 0.23370055516958355
	data : 0.11486225128173828
	model : 0.0650629997253418
			 train-loss:  2.2082528592883675 	 ± 0.23357939863272065
	data : 0.11481599807739258
	model : 0.06509461402893066
			 train-loss:  2.207871281843391 	 ± 0.23278083092611768
	data : 0.11482028961181641
	model : 0.06517877578735351
			 train-loss:  2.207138239485877 	 ± 0.23210893488757461
	data : 0.11477537155151367
	model : 0.06517128944396973
			 train-loss:  2.2058008097587747 	 ± 0.23182512623043044
	data : 0.11468725204467774
	model : 0.06515007019042969
			 train-loss:  2.2043192084406464 	 ± 0.23167635440216944
	data : 0.11459555625915527
	model : 0.06502881050109863
			 train-loss:  2.204676813178963 	 ± 0.23090420117894178
	data : 0.11452112197875977
	model : 0.06497392654418946
			 train-loss:  2.2044990244838925 	 ± 0.23011087539309744
	data : 0.1144904613494873
	model : 0.06492834091186524
			 train-loss:  2.2058532254449252 	 ± 0.22989108602887293
	data : 0.1147679328918457
	model : 0.0648913860321045
			 train-loss:  2.2049173377964593 	 ± 0.22937944364284388
	data : 0.11480765342712403
	model : 0.06491456031799317
			 train-loss:  2.20292207091844 	 ± 0.22986570776263548
	data : 0.11482758522033691
	model : 0.06500144004821777
			 train-loss:  2.202020190052084 	 ± 0.22934863558128304
	data : 0.11492767333984374
	model : 0.06505980491638183
			 train-loss:  2.199982324702628 	 ± 0.22991824557695037
	data : 0.1149362564086914
	model : 0.06505084037780762
			 train-loss:  2.202645220756531 	 ± 0.2314444748643001
	data : 0.11480445861816406
	model : 0.06503777503967285
			 train-loss:  2.200384064225961 	 ± 0.23233321747612706
	data : 0.1147343635559082
	model : 0.06501469612121583
			 train-loss:  2.198895404997625 	 ± 0.2322891147040467
	data : 0.11467599868774414
	model : 0.06491546630859375
			 train-loss:  2.1992607841304705 	 ± 0.23157257374766654
	data : 0.11460762023925782
	model : 0.06487202644348145
			 train-loss:  2.198531346661704 	 ± 0.2309957686890138
	data : 0.11472496986389161
	model : 0.06487622261047363
			 train-loss:  2.1973048710053966 	 ± 0.23075191569841763
	data : 0.11477665901184082
	model : 0.06492652893066406
			 train-loss:  2.198025338924848 	 ± 0.23018596763577245
	data : 0.11478910446166993
	model : 0.06498074531555176
			 train-loss:  2.19727918524651 	 ± 0.2296409027074112
	data : 0.11483159065246581
	model : 0.06504144668579101
			 train-loss:  2.197750487659551 	 ± 0.22898919735024725
	data : 0.1148961067199707
	model : 0.06508231163024902
			 train-loss:  2.196315564449478 	 ± 0.22897945047351184
	data : 0.11470942497253418
	model : 0.06515254974365234
			 train-loss:  2.195997679233551 	 ± 0.22829795973727424
	data : 0.11463336944580078
	model : 0.0651169776916504
			 train-loss:  2.1959184445209385 	 ± 0.2275900623329947
	data : 0.11450958251953125
	model : 0.0650780200958252
			 train-loss:  2.197799214610347 	 ± 0.22813812728680485
	data : 0.11450309753417968
	model : 0.06508760452270508
			 train-loss:  2.200056680141051 	 ± 0.22924501028998776
	data : 0.11448068618774414
	model : 0.06506071090698243
			 train-loss:  2.2012162920905323 	 ± 0.229024046826976
	data : 0.11452441215515137
	model : 0.06502003669738769
			 train-loss:  2.198603709538778 	 ± 0.23076724073323338
	data : 0.11454930305480956
	model : 0.06510887145996094
			 train-loss:  2.198221048676824 	 ± 0.2301236100495584
	data : 0.11479263305664063
	model : 0.06516265869140625
			 train-loss:  2.1983821192187465 	 ± 0.229442967899091
	data : 0.114841890335083
	model : 0.06511416435241699
			 train-loss:  2.1985595325628915 	 ± 0.22877057138885654
	data : 0.11490612030029297
	model : 0.06511125564575196
			 train-loss:  2.1988025558065383 	 ± 0.22811448034259335
	data : 0.11482067108154297
	model : 0.06509342193603515
			 train-loss:  2.1966775199946236 	 ± 0.22911413553079904
	data : 0.1147301197052002
	model : 0.06498217582702637
			 train-loss:  2.194918488201342 	 ± 0.22959164095705317
	data : 0.11460356712341309
	model : 0.0649071216583252
			 train-loss:  2.1945429797782454 	 ± 0.22897590863622733
	data : 0.11468963623046875
	model : 0.06490378379821778
			 train-loss:  2.1931895465520075 	 ± 0.22900211737424117
	data : 0.11474781036376953
	model : 0.06493029594421387
			 train-loss:  2.193507549406468 	 ± 0.22838142196146846
	data : 0.11489057540893555
	model : 0.06495580673217774
			 train-loss:  2.1920611790248326 	 ± 0.2285257831131075
	data : 0.11487946510314942
	model : 0.06503882408142089
			 train-loss:  2.192542241378264 	 ± 0.22796448134962116
	data : 0.11486921310424805
	model : 0.06505107879638672
			 train-loss:  2.1918389393111406 	 ± 0.22751100388481577
	data : 0.11469025611877441
	model : 0.0650390625
			 train-loss:  2.1893521547317505 	 ± 0.22927068843615883
	data : 0.11448578834533692
	model : 0.06498851776123046
			 train-loss:  2.1895785371684497 	 ± 0.22864931972367603
	data : 0.11450204849243165
	model : 0.06498675346374512
			 train-loss:  2.191044505437215 	 ± 0.2288552957552613
	data : 0.11457223892211914
	model : 0.06490883827209473
			 train-loss:  2.1927144606469087 	 ± 0.22931933679457622
	data : 0.11467642784118652
	model : 0.06489624977111816
			 train-loss:  2.1935622351510182 	 ± 0.2289727173934849
	data : 0.11483173370361328
	model : 0.06488852500915528
			 train-loss:  2.1951886539250776 	 ± 0.22939800362519616
	data : 0.11491899490356446
	model : 0.06496062278747558
			 train-loss:  2.1949720706628715 	 ± 0.22879255081674288
	data : 0.11486687660217285
	model : 0.06497292518615723
			 train-loss:  2.1951001528147103 	 ± 0.22817996913744085
	data : 0.11478610038757324
	model : 0.06500658988952637
			 train-loss:  2.198747964315517 	 ± 0.23291174481701082
	data : 0.11465954780578613
	model : 0.06501045227050781
			 train-loss:  2.198277280929892 	 ± 0.2323768323737686
	data : 0.11444230079650879
	model : 0.06503314971923828
			 train-loss:  2.1987534951656422 	 ± 0.23184945848914898
	data : 0.11443734169006348
	model : 0.06499199867248535
			 train-loss:  2.1963116794666915 	 ± 0.23364653006421524
	data : 0.11459007263183593
	model : 0.06499462127685547
			 train-loss:  2.195472963232743 	 ± 0.23331595020942847
	data : 0.1147162914276123
	model : 0.0650101661682129
			 train-loss:  2.1942654155311785 	 ± 0.2332989031711135
	data : 0.11478099822998047
	model : 0.0650550365447998
			 train-loss:  2.1946327748397985 	 ± 0.23274594106202798
	data : 0.11494765281677247
	model : 0.06507272720336914
			 train-loss:  2.1972766206672154 	 ± 0.23501502250709555
	data : 0.11490788459777831
	model : 0.06508278846740723
			 train-loss:  2.1970005539274706 	 ± 0.23443990389587083
	data : 0.1147623062133789
	model : 0.06507058143615722
			 train-loss:  2.1976838197463597 	 ± 0.2340315814152976
	data : 0.11466917991638184
	model : 0.06504173278808593
			 train-loss:  2.197494145558805 	 ± 0.2334488246212216
	data : 0.11450767517089844
	model : 0.06503181457519532
			 train-loss:  2.197787715698862 	 ± 0.23289182956755142
	data : 0.11452951431274414
	model : 0.06499967575073243
			 train-loss:  2.1967783070573903 	 ± 0.2327346050293239
	data : 0.11464123725891114
	model : 0.06499667167663574
			 train-loss:  2.195736602323139 	 ± 0.23261140967248936
	data : 0.1147425651550293
	model : 0.0650179386138916
			 train-loss:  2.1954122614860534 	 ± 0.23207425916937718
	data : 0.11482839584350586
	model : 0.06503658294677735
			 train-loss:  2.1954198144561614 	 ± 0.2314962648346021
	data : 0.11503591537475585
	model : 0.06501946449279786
			 train-loss:  2.1951056588994393 	 ± 0.2309654919284675
	data : 0.11502203941345215
	model : 0.06507701873779297
			 train-loss:  2.1972836003514935 	 ± 0.23246601380457868
	data : 0.11496858596801758
	model : 0.06506175994873047
			 train-loss:  2.1964621134832796 	 ± 0.23219073210696559
	data : 0.11494131088256836
	model : 0.06501379013061523
			 train-loss:  2.1971358706311483 	 ± 0.23182353985121187
	data : 0.11487035751342774
	model : 0.0650209903717041
			 train-loss:  2.1959255355075724 	 ± 0.23190855129998383
	data : 0.11480927467346191
	model : 0.06502056121826172
			 train-loss:  2.1958638573614295 	 ± 0.2313494012361995
	data : 0.11483397483825683
	model : 0.06499066352844238
			 train-loss:  2.1948946147010875 	 ± 0.23121351130190546
	data : 0.11489310264587402
	model : 0.06502866744995117
			 train-loss:  2.1971352322820272 	 ± 0.23291229292282153
	data : 0.11491813659667968
	model : 0.06504287719726562
			 train-loss:  2.1973631342252093 	 ± 0.23238043602958378
	data : 0.11487736701965331
	model : 0.06504082679748535
			 train-loss:  2.1964971154786963 	 ± 0.23216855363263098
	data : 0.1149064064025879
	model : 0.06506662368774414
			 train-loss:  2.1966250825603053 	 ± 0.23162779778272602
	data : 0.11497588157653808
	model : 0.06504707336425782
			 train-loss:  2.197086638128254 	 ± 0.23118113058222237
	data : 0.11489901542663575
	model : 0.06503925323486329
			 train-loss:  2.196134967781673 	 ± 0.23105818003545797
	data : 0.11481714248657227
	model : 0.06506805419921875
			 train-loss:  2.1983603361041046 	 ± 0.23280754146061963
	data : 0.11477560997009277
	model : 0.06500024795532226
			 train-loss:  2.19805816202252 	 ± 0.23231026647722836
	data : 0.1146540641784668
	model : 0.06495447158813476
			 train-loss:  2.196732518310371 	 ± 0.23259179418918036
	data : 0.11466178894042969
	model : 0.06496644020080566
			 train-loss:  2.1970153090057023 	 ± 0.2320951013091509
	data : 0.11474528312683105
	model : 0.06493444442749023
			 train-loss:  2.19663102822761 	 ± 0.2316340977962922
	data : 0.11479554176330567
	model : 0.0649001121520996
			 train-loss:  2.1966815043579446 	 ± 0.2311082642524825
	data : 0.11484522819519043
	model : 0.06492767333984376
			 train-loss:  2.197104756648724 	 ± 0.23067024578011505
	data : 0.11493053436279296
	model : 0.06492424011230469
			 train-loss:  2.198099144407221 	 ± 0.23062439057697479
	data : 0.11485524177551269
	model : 0.06484856605529785
			 train-loss:  2.197852232531047 	 ± 0.23013612146855594
	data : 0.11478180885314941
	model : 0.06476778984069824
			 train-loss:  2.1960299696241106 	 ± 0.23122866877005516
	data : 0.11479835510253907
	model : 0.06466856002807617
			 train-loss:  2.1952749813927546 	 ± 0.23099079876690243
	data : 0.11492218971252441
	model : 0.06458163261413574
			 train-loss:  2.1945806256437725 	 ± 0.23071440469465898
	data : 0.11513824462890625
	model : 0.06448163986206054
			 train-loss:  2.1928968660631893 	 ± 0.23159310744245695
	data : 0.1152101993560791
	model : 0.06439685821533203
			 train-loss:  2.1927646429915177 	 ± 0.2310932565066524
	data : 0.11532740592956543
	model : 0.06431822776794434
			 train-loss:  2.191116202346102 	 ± 0.2319276715954522
	data : 0.11538729667663575
	model : 0.06426925659179687
			 train-loss:  2.1909402458564093 	 ± 0.23143824953859163
	data : 0.11555299758911133
	model : 0.06417307853698731
			 train-loss:  2.1907741171973094 	 ± 0.23095050059955766
	data : 0.11543788909912109
	model : 0.06409621238708496
			 train-loss:  2.19164981050738 	 ± 0.23083623574949194
	data : 0.11549558639526367
	model : 0.06403059959411621
			 train-loss:  2.191386355350969 	 ± 0.230375298088399
	data : 0.11551752090454101
	model : 0.06404662132263184
			 train-loss:  2.189839411495078 	 ± 0.2310920778590083
	data : 0.11563000679016114
	model : 0.06401476860046387
			 train-loss:  2.191333605888042 	 ± 0.23172986785847133
	data : 0.11563272476196289
	model : 0.06408352851867676
			 train-loss:  2.191512321516619 	 ± 0.23125462241648473
	data : 0.1155466079711914
	model : 0.06410036087036133
			 train-loss:  2.19144015020459 	 ± 0.2307688911377288
	data : 0.11543483734130859
	model : 0.06413373947143555
			 train-loss:  2.1910742065485787 	 ± 0.23035247216217197
	data : 0.11541638374328614
	model : 0.06407647132873535
			 train-loss:  2.1934648642480123 	 ± 0.2328099476592511
	data : 0.11515350341796875
	model : 0.06405258178710938
			 train-loss:  2.1946423316995305 	 ± 0.23303646303633416
	data : 0.11493186950683594
	model : 0.06402621269226075
			 train-loss:  2.1947790354613943 	 ± 0.2325621253212862
	data : 0.11508030891418457
	model : 0.06399068832397461
			 train-loss:  2.1947653909360083 	 ± 0.23208122431337838
	data : 0.11518850326538085
	model : 0.06395921707153321
			 train-loss:  2.1940171900109497 	 ± 0.23189548148539957
	data : 0.11522998809814453
	model : 0.06398425102233887
			 train-loss:  2.193773269164758 	 ± 0.23145103314590496
	data : 0.11539435386657715
	model : 0.06401252746582031
			 train-loss:  2.195132837976728 	 ± 0.23195246424391328
	data : 0.11556415557861328
	model : 0.06396780014038086
			 train-loss:  2.1950370213849757 	 ± 0.23148539451405267
	data : 0.11548619270324707
	model : 0.06398658752441407
			 train-loss:  2.19426149153999 	 ± 0.23133633183580787
	data : 0.11540484428405762
	model : 0.06401643753051758
			 train-loss:  2.1944789766303954 	 ± 0.23089475770732132
	data : 0.11533665657043457
	model : 0.06397933959960937
			 train-loss:  2.1957270893227143 	 ± 0.23126740779636532
	data : 0.11541895866394043
	model : 0.06392388343811035
			 train-loss:  2.1950092654228213 	 ± 0.2310821887768034
	data : 0.1153602123260498
	model : 0.06395878791809081
			 train-loss:  2.1959394064557505 	 ± 0.231089860493731
	data : 0.11538534164428711
	model : 0.06395354270935058
			 train-loss:  2.1958226594660015 	 ± 0.23063830978719413
	data : 0.11546392440795898
	model : 0.06393294334411621
			 train-loss:  2.194661968781543 	 ± 0.2309183231809429
	data : 0.11544313430786132
	model : 0.06399054527282715
			 train-loss:  2.1953667092511036 	 ± 0.23073576358854628
	data : 0.11519250869750977
	model : 0.06396546363830566
			 train-loss:  2.194578591515036 	 ± 0.23062519191405906
	data : 0.1151965618133545
	model : 0.06396527290344238
			 train-loss:  2.194511872716248 	 ± 0.23017677708313314
	data : 0.11504545211791992
	model : 0.05552816390991211
#epoch  40    val-loss:  2.4207888276953446  train-loss:  2.194511872716248  lr:  4.8828125e-06
			 train-loss:  2.059809923171997 	 ± 0.0
	data : 5.120941877365112
	model : 0.08511471748352051
			 train-loss:  2.244049906730652 	 ± 0.18423998355865479
	data : 2.718788981437683
	model : 0.07576286792755127
			 train-loss:  2.3294740517934165 	 ± 0.19293561140236412
	data : 1.850178559621175
	model : 0.07204262415568034
			 train-loss:  2.264139771461487 	 ± 0.20180143004913642
	data : 1.416288673877716
	model : 0.07022345066070557
			 train-loss:  2.2924508094787597 	 ± 0.1891695356825805
	data : 1.1557453632354737
	model : 0.06907958984375
			 train-loss:  2.271946390469869 	 ± 0.17867032122439938
	data : 0.15438270568847656
	model : 0.06495800018310546
			 train-loss:  2.2317921263831004 	 ± 0.19244955886126885
	data : 0.11402192115783691
	model : 0.06462817192077637
			 train-loss:  2.278278663754463 	 ± 0.21802342928023524
	data : 0.11444897651672363
	model : 0.06468086242675782
			 train-loss:  2.248694141705831 	 ± 0.2219337463072714
	data : 0.11444120407104492
	model : 0.06478586196899414
			 train-loss:  2.238156831264496 	 ± 0.21290477490421836
	data : 0.11471142768859863
	model : 0.064906644821167
			 train-loss:  2.2730191946029663 	 ± 0.23100112422000088
	data : 0.1146895408630371
	model : 0.06499862670898438
			 train-loss:  2.246618707974752 	 ± 0.23786880574089742
	data : 0.11458868980407715
	model : 0.06502513885498047
			 train-loss:  2.237638473510742 	 ± 0.23064447904074037
	data : 0.11449828147888183
	model : 0.06502766609191894
			 train-loss:  2.2212717022214616 	 ± 0.22995526631731647
	data : 0.11446475982666016
	model : 0.06492457389831544
			 train-loss:  2.2576823075612387 	 ± 0.26060387579016536
	data : 0.11439929008483887
	model : 0.06488757133483887
			 train-loss:  2.2626104652881622 	 ± 0.2530494663119723
	data : 0.11452035903930664
	model : 0.06488337516784667
			 train-loss:  2.254475299049826 	 ± 0.24764131215954047
	data : 0.11455292701721191
	model : 0.06489567756652832
			 train-loss:  2.243048005633884 	 ± 0.24523278509643504
	data : 0.1146315574645996
	model : 0.0649709701538086
			 train-loss:  2.2604759617855676 	 ± 0.24988218804853315
	data : 0.11461901664733887
	model : 0.0650167465209961
			 train-loss:  2.2562904834747313 	 ± 0.24423738217939828
	data : 0.11458001136779786
	model : 0.06500439643859864
			 train-loss:  2.2437165748505365 	 ± 0.24489466266856377
	data : 0.11466774940490723
	model : 0.06502933502197265
			 train-loss:  2.2672499364072625 	 ± 0.2624452963710442
	data : 0.1146662712097168
	model : 0.06498489379882813
			 train-loss:  2.270179111024608 	 ± 0.2570440032768335
	data : 0.11450872421264649
	model : 0.06489787101745606
			 train-loss:  2.2728465547164283 	 ± 0.2519569136861519
	data : 0.11454524993896484
	model : 0.0648716926574707
			 train-loss:  2.2543249559402465 	 ± 0.2630136608800032
	data : 0.11457133293151855
	model : 0.06493229866027832
			 train-loss:  2.2460310138188877 	 ± 0.26121887408106254
	data : 0.11465835571289062
	model : 0.0649378776550293
			 train-loss:  2.247892145757322 	 ± 0.2565114538392357
	data : 0.11462006568908692
	model : 0.06500544548034667
			 train-loss:  2.257073483296803 	 ± 0.25636733562297814
	data : 0.11467609405517579
	model : 0.0650205135345459
			 train-loss:  2.25476899229247 	 ± 0.2522034053791668
	data : 0.1146097183227539
	model : 0.0650132179260254
			 train-loss:  2.248283080259959 	 ± 0.25041222751148573
	data : 0.11459293365478515
	model : 0.06494684219360351
			 train-loss:  2.2571394943421885 	 ± 0.2510708713777669
	data : 0.11432857513427734
	model : 0.06490764617919922
			 train-loss:  2.256225984543562 	 ± 0.2471690894052273
	data : 0.1144287109375
	model : 0.06485104560852051
			 train-loss:  2.249966444391193 	 ± 0.24595749240774453
	data : 0.11443839073181153
	model : 0.06490888595581054
			 train-loss:  2.236508579815135 	 ± 0.25434737407835095
	data : 0.1146355152130127
	model : 0.06491575241088868
			 train-loss:  2.2373873097555976 	 ± 0.25073986666467263
	data : 0.1147376537322998
	model : 0.06496548652648926
			 train-loss:  2.232089751296573 	 ± 0.2492114020050364
	data : 0.11477413177490234
	model : 0.06500802040100098
			 train-loss:  2.231243468619682 	 ± 0.24587304980279653
	data : 0.11477155685424804
	model : 0.06502580642700195
			 train-loss:  2.2309905792537488 	 ± 0.24262118582795
	data : 0.11474590301513672
	model : 0.06498794555664063
			 train-loss:  2.2337156197963615 	 ± 0.2400788661332029
	data : 0.11470570564270019
	model : 0.06500425338745117
			 train-loss:  2.2306982278823853 	 ± 0.23780663766860657
	data : 0.11447367668151856
	model : 0.06491570472717285
			 train-loss:  2.2266815348369318 	 ± 0.23625840269093756
	data : 0.11451420783996583
	model : 0.06485128402709961
			 train-loss:  2.2499357348396662 	 ± 0.2768756085114278
	data : 0.11468267440795898
	model : 0.06483120918273926
			 train-loss:  2.2538289303003354 	 ± 0.27479792990979235
	data : 0.11477646827697754
	model : 0.06481165885925293
			 train-loss:  2.2493872588331048 	 ± 0.27321420238341987
	data : 0.11474571228027344
	model : 0.06481022834777832
			 train-loss:  2.2504838095770943 	 ± 0.2702593328841339
	data : 0.1149606704711914
	model : 0.06487565040588379
			 train-loss:  2.2462230143339736 	 ± 0.2688293613472418
	data : 0.11500883102416992
	model : 0.06496033668518067
			 train-loss:  2.251958755736655 	 ± 0.2687841587227815
	data : 0.1147928237915039
	model : 0.06499757766723632
			 train-loss:  2.247282385826111 	 ± 0.26789482613104304
	data : 0.11473135948181153
	model : 0.0650099277496338
			 train-loss:  2.2471754356306426 	 ± 0.26514814960232636
	data : 0.11458845138549804
	model : 0.06502819061279297
			 train-loss:  2.2403758859634397 	 ± 0.266763809991063
	data : 0.11453480720520019
	model : 0.06502938270568848
			 train-loss:  2.2355960256913128 	 ± 0.26628918858466694
	data : 0.1144317626953125
	model : 0.06500453948974609
			 train-loss:  2.2362189201208262 	 ± 0.26375380052319697
	data : 0.11440911293029785
	model : 0.06501607894897461
			 train-loss:  2.2312674095045844 	 ± 0.2636823994258376
	data : 0.1144249439239502
	model : 0.06505885124206542
			 train-loss:  2.226073525570057 	 ± 0.2639518809696465
	data : 0.11455264091491699
	model : 0.06503825187683106
			 train-loss:  2.2255547133359044 	 ± 0.26156909668242434
	data : 0.11460556983947753
	model : 0.06502819061279297
			 train-loss:  2.238372428076608 	 ± 0.27610287022733765
	data : 0.11458516120910645
	model : 0.06496868133544922
			 train-loss:  2.2409542610770776 	 ± 0.2743513545520044
	data : 0.11444802284240722
	model : 0.06494240760803223
			 train-loss:  2.2347950318763994 	 ± 0.2759226124767315
	data : 0.1144214153289795
	model : 0.06492700576782226
			 train-loss:  2.2325288319991805 	 ± 0.27411815290504876
	data : 0.11445837020874024
	model : 0.06495518684387207
			 train-loss:  2.2312879959742227 	 ± 0.2719912801750886
	data : 0.11451983451843262
	model : 0.06496858596801758
			 train-loss:  2.234283990547305 	 ± 0.27074903657007077
	data : 0.11469035148620606
	model : 0.06500706672668458
			 train-loss:  2.238435249174795 	 ± 0.2705067675226479
	data : 0.11492924690246582
	model : 0.0649932861328125
			 train-loss:  2.235440799168178 	 ± 0.2693851479436019
	data : 0.11501951217651367
	model : 0.06497054100036621
			 train-loss:  2.2421748861670494 	 ± 0.27256448188552873
	data : 0.11489109992980957
	model : 0.06492795944213867
			 train-loss:  2.2409258695749137 	 ± 0.27064422206326716
	data : 0.11466732025146484
	model : 0.06488022804260254
			 train-loss:  2.240162665193731 	 ± 0.26865653453055405
	data : 0.1146505355834961
	model : 0.06488690376281739
			 train-loss:  2.2441106305193546 	 ± 0.26856615429040737
	data : 0.1146132469177246
	model : 0.06489882469177247
			 train-loss:  2.2384685067569507 	 ± 0.2705548468452698
	data : 0.11455717086791992
	model : 0.06492362022399903
			 train-loss:  2.236570800560108 	 ± 0.2690426440363307
	data : 0.11465253829956054
	model : 0.06498541831970214
			 train-loss:  2.238044943128313 	 ± 0.26739452426753135
	data : 0.11474432945251464
	model : 0.06504859924316406
			 train-loss:  2.2380594569192804 	 ± 0.26550481440448853
	data : 0.1145820140838623
	model : 0.06502299308776856
			 train-loss:  2.2348231375217438 	 ± 0.26506108146170926
	data : 0.11452708244323731
	model : 0.06506872177124023
			 train-loss:  2.2314376406473655 	 ± 0.2648021553901102
	data : 0.11444950103759766
	model : 0.06503686904907227
			 train-loss:  2.2272563077308036 	 ± 0.26542213596647146
	data : 0.11449337005615234
	model : 0.0649935245513916
			 train-loss:  2.2262885475158694 	 ± 0.2637781205581903
	data : 0.11456837654113769
	model : 0.06497249603271485
			 train-loss:  2.2239144318982174 	 ± 0.26284238044223057
	data : 0.11475048065185547
	model : 0.06503772735595703
			 train-loss:  2.224093861394114 	 ± 0.2611347190132811
	data : 0.11478195190429688
	model : 0.06502623558044433
			 train-loss:  2.2213379603165846 	 ± 0.26057994511130544
	data : 0.11486878395080566
	model : 0.06510100364685059
			 train-loss:  2.221146302887156 	 ± 0.2589309852345041
	data : 0.11475558280944824
	model : 0.06511917114257812
			 train-loss:  2.2206483334302902 	 ± 0.25734564177074637
	data : 0.11470551490783691
	model : 0.06510682106018066
			 train-loss:  2.2167729757450245 	 ± 0.2580903650928563
	data : 0.11459136009216309
	model : 0.0650761604309082
			 train-loss:  2.217738522262108 	 ± 0.2566589690697583
	data : 0.11459503173828126
	model : 0.06503067016601563
			 train-loss:  2.2148286046752013 	 ± 0.25646541816510043
	data : 0.1146148681640625
	model : 0.06498665809631347
			 train-loss:  2.212650927759352 	 ± 0.25570508363224903
	data : 0.11466259956359863
	model : 0.06495838165283203
			 train-loss:  2.21144257293028 	 ± 0.25443762230033673
	data : 0.11474833488464356
	model : 0.0650360107421875
			 train-loss:  2.2110334094180617 	 ± 0.25298213494230154
	data : 0.11487751007080078
	model : 0.0650339126586914
			 train-loss:  2.2117780501815094 	 ± 0.2516187890390091
	data : 0.11489281654357911
	model : 0.06507196426391601
			 train-loss:  2.211702437563376 	 ± 0.25018604614145645
	data : 0.11488580703735352
	model : 0.06508846282958984
			 train-loss:  2.2086181613836398 	 ± 0.2504533644046016
	data : 0.11482467651367187
	model : 0.06512904167175293
			 train-loss:  2.212001657485962 	 ± 0.2510951978932643
	data : 0.11469640731811523
	model : 0.06507358551025391
			 train-loss:  2.209977909758851 	 ± 0.2504487061340292
	data : 0.11463460922241211
	model : 0.065059232711792
			 train-loss:  2.210050966428674 	 ± 0.2490848278430231
	data : 0.11456613540649414
	model : 0.06504487991333008
			 train-loss:  2.2122841240257345 	 ± 0.24866628856350206
	data : 0.11454715728759765
	model : 0.06502776145935059
			 train-loss:  2.213005248536455 	 ± 0.2474378035982239
	data : 0.11458845138549804
	model : 0.06501364707946777
			 train-loss:  2.2113780573794717 	 ± 0.24663713559188696
	data : 0.11469411849975586
	model : 0.06503582000732422
			 train-loss:  2.213323123753071 	 ± 0.2460805640603997
	data : 0.11476149559020996
	model : 0.06506061553955078
			 train-loss:  2.2108602019929395 	 ± 0.24599531183291914
	data : 0.11474661827087403
	model : 0.06509475708007813
			 train-loss:  2.213364140111573 	 ± 0.2459763571318801
	data : 0.11469001770019531
	model : 0.06507282257080078
			 train-loss:  2.2122139220285897 	 ± 0.24499564729481732
	data : 0.11460409164428711
	model : 0.06505532264709472
			 train-loss:  2.213074365854263 	 ± 0.2439178847801229
	data : 0.11458020210266114
	model : 0.06504406929016113
			 train-loss:  2.2122399960414016 	 ± 0.24285074254673464
	data : 0.11450958251953125
	model : 0.06504096984863281
			 train-loss:  2.2176547529650668 	 ± 0.24770862154360057
	data : 0.11466093063354492
	model : 0.06501474380493164
			 train-loss:  2.215900961635182 	 ± 0.24713876115038388
	data : 0.11466069221496582
	model : 0.06503944396972657
			 train-loss:  2.215456594641392 	 ± 0.24598906795004882
	data : 0.11475148200988769
	model : 0.0650557518005371
			 train-loss:  2.212837188584464 	 ± 0.2462679513834205
	data : 0.11468086242675782
	model : 0.06506023406982422
			 train-loss:  2.2144511521987194 	 ± 0.2456608761881939
	data : 0.11469860076904297
	model : 0.06504135131835938
			 train-loss:  2.2167552531322587 	 ± 0.24565829042554965
	data : 0.1144791603088379
	model : 0.06501102447509766
			 train-loss:  2.2148413249739893 	 ± 0.24531851104987146
	data : 0.11457719802856445
	model : 0.06503233909606934
			 train-loss:  2.214298476866626 	 ± 0.24425576124269327
	data : 0.11465897560119628
	model : 0.06504058837890625
			 train-loss:  2.2148561835289002 	 ± 0.2432126811962492
	data : 0.114715576171875
	model : 0.06503815650939941
			 train-loss:  2.21284260191359 	 ± 0.24303394807475748
	data : 0.11468405723571777
	model : 0.06510868072509765
			 train-loss:  2.2110810535294667 	 ± 0.24265730615952746
	data : 0.11487579345703125
	model : 0.0651474952697754
			 train-loss:  2.210228894664123 	 ± 0.24174948837160692
	data : 0.11484065055847167
	model : 0.06510562896728515
			 train-loss:  2.2090622897733723 	 ± 0.24100611635935432
	data : 0.11485896110534669
	model : 0.06509790420532227
			 train-loss:  2.209919500350952 	 ± 0.24013046172847097
	data : 0.11483840942382813
	model : 0.06508803367614746
			 train-loss:  2.209296055908861 	 ± 0.23918663315338576
	data : 0.11464729309082031
	model : 0.06497998237609863
			 train-loss:  2.20718658887423 	 ± 0.23924349888326582
	data : 0.11464729309082031
	model : 0.06503596305847167
			 train-loss:  2.207955954438549 	 ± 0.23837290878346262
	data : 0.1146768569946289
	model : 0.06499061584472657
			 train-loss:  2.2079243479656574 	 ± 0.2373694772217625
	data : 0.11462984085083008
	model : 0.06501812934875488
			 train-loss:  2.210008901357651 	 ± 0.23746964294902956
	data : 0.11458134651184082
	model : 0.06503109931945801
			 train-loss:  2.215154588715104 	 ± 0.2431114043710651
	data : 0.11480073928833008
	model : 0.06507515907287598
			 train-loss:  2.2149081855523782 	 ± 0.24212816709831364
	data : 0.11470403671264648
	model : 0.06499743461608887
			 train-loss:  2.2174993821275915 	 ± 0.2428344277847878
	data : 0.11465082168579102
	model : 0.06500144004821777
			 train-loss:  2.2176750725315464 	 ± 0.24186112353034786
	data : 0.11459784507751465
	model : 0.06493496894836426
			 train-loss:  2.217036643981934 	 ± 0.2409966181015128
	data : 0.11464567184448242
	model : 0.06493239402770996
			 train-loss:  2.2177110777960882 	 ± 0.24015678249245523
	data : 0.11466770172119141
	model : 0.06492242813110352
			 train-loss:  2.2186531457375356 	 ± 0.23944303728765157
	data : 0.11470327377319336
	model : 0.06494812965393067
			 train-loss:  2.2176178246736526 	 ± 0.23879108920725403
	data : 0.11474275588989258
	model : 0.06499338150024414
			 train-loss:  2.218235359635464 	 ± 0.23796632603653853
	data : 0.11470346450805664
	model : 0.06502161026000977
			 train-loss:  2.2157118091216454 	 ± 0.23877580314691077
	data : 0.11470060348510742
	model : 0.06500778198242188
			 train-loss:  2.2161697058277277 	 ± 0.2379199880206875
	data : 0.11447052955627442
	model : 0.06501998901367187
			 train-loss:  2.219029971144416 	 ± 0.23926724469411323
	data : 0.11445808410644531
	model : 0.06498651504516602
			 train-loss:  2.2169560920026967 	 ± 0.23955396225020245
	data : 0.1144792079925537
	model : 0.06495447158813476
			 train-loss:  2.2159678775872758 	 ± 0.2389303870464567
	data : 0.11455655097961426
	model : 0.06501998901367187
			 train-loss:  2.2149563047620986 	 ± 0.23833165388798255
	data : 0.11454930305480956
	model : 0.06505999565124512
			 train-loss:  2.2122486154822742 	 ± 0.2395288690627118
	data : 0.11465911865234375
	model : 0.06508688926696778
			 train-loss:  2.2122674693156332 	 ± 0.2386531763026263
	data : 0.11471543312072754
	model : 0.06510448455810547
			 train-loss:  2.2100752073785532 	 ± 0.2391673941272201
	data : 0.11462907791137696
	model : 0.06515898704528808
			 train-loss:  2.209078756167734 	 ± 0.23859284648749335
	data : 0.1145705223083496
	model : 0.06508407592773438
			 train-loss:  2.211174726486206 	 ± 0.2390200157633686
	data : 0.11462688446044922
	model : 0.06503219604492187
			 train-loss:  2.2114837829102862 	 ± 0.23819898997456318
	data : 0.11462922096252441
	model : 0.06500906944274902
			 train-loss:  2.2100619766074168 	 ± 0.23795845493856482
	data : 0.11465024948120117
	model : 0.06504964828491211
			 train-loss:  2.2100045347547197 	 ± 0.23712596063032979
	data : 0.11462416648864746
	model : 0.06504087448120117
			 train-loss:  2.2087371432118945 	 ± 0.2367867019142574
	data : 0.11461863517761231
	model : 0.06506285667419434
			 train-loss:  2.206893071635016 	 ± 0.23700412050673975
	data : 0.11453890800476074
	model : 0.06506013870239258
			 train-loss:  2.2067746160781545 	 ± 0.23619537501967963
	data : 0.11453690528869628
	model : 0.06499772071838379
			 train-loss:  2.207800843277756 	 ± 0.23571699559302434
	data : 0.11441917419433593
	model : 0.06491236686706543
			 train-loss:  2.208524260166529 	 ± 0.2350829842922993
	data : 0.11446552276611328
	model : 0.06486625671386718
			 train-loss:  2.209713506218571 	 ± 0.23473906241746528
	data : 0.11453261375427246
	model : 0.0648430347442627
			 train-loss:  2.2096729429562885 	 ± 0.23395581435353402
	data : 0.1145625114440918
	model : 0.064872407913208
			 train-loss:  2.2088073049949495 	 ± 0.23342072926070231
	data : 0.11463031768798829
	model : 0.06493277549743652
			 train-loss:  2.2125997817830036 	 ± 0.23727325228814253
	data : 0.11474790573120117
	model : 0.06496801376342773
			 train-loss:  2.2140540539049636 	 ± 0.23717524638023837
	data : 0.11466717720031738
	model : 0.06496634483337402
			 train-loss:  2.212692877688965 	 ± 0.23700274817159633
	data : 0.11455225944519043
	model : 0.06503076553344726
			 train-loss:  2.214347121792455 	 ± 0.23712726165267947
	data : 0.11451396942138672
	model : 0.06497840881347657
			 train-loss:  2.2171969681214065 	 ± 0.23901411146295326
	data : 0.11447772979736329
	model : 0.06494579315185547
			 train-loss:  2.216472741904532 	 ± 0.2384233567664399
	data : 0.11447629928588868
	model : 0.06497607231140137
			 train-loss:  2.2185758239106286 	 ± 0.2391240638772476
	data : 0.11455116271972657
	model : 0.06498956680297852
			 train-loss:  2.218392887205448 	 ± 0.23838200621387381
	data : 0.11462211608886719
	model : 0.06493468284606933
			 train-loss:  2.2173473201692104 	 ± 0.238001342357445
	data : 0.11469244956970215
	model : 0.06496195793151856
			 train-loss:  2.21705985291404 	 ± 0.23728891861171328
	data : 0.11475324630737305
	model : 0.06500983238220215
			 train-loss:  2.2169947867040283 	 ± 0.2365568523958829
	data : 0.11482210159301758
	model : 0.06498732566833496
			 train-loss:  2.2148351713192245 	 ± 0.2374266093677422
	data : 0.1146852970123291
	model : 0.06497507095336914
			 train-loss:  2.213194421151789 	 ± 0.23762675097685568
	data : 0.11470370292663574
	model : 0.06499419212341309
			 train-loss:  2.214965565999349 	 ± 0.2379888909178668
	data : 0.11468467712402344
	model : 0.06498627662658692
			 train-loss:  2.2121205351438866 	 ± 0.24006886143912218
	data : 0.11464791297912598
	model : 0.0649712085723877
			 train-loss:  2.2091593292658915 	 ± 0.24237070982558448
	data : 0.11452546119689941
	model : 0.06504807472229004
			 train-loss:  2.206578857132367 	 ± 0.24393835822195759
	data : 0.11472072601318359
	model : 0.0651082992553711
			 train-loss:  2.2062816034407304 	 ± 0.2432460914388421
	data : 0.11476035118103027
	model : 0.06511712074279785
			 train-loss:  2.208228386850918 	 ± 0.24384649730397553
	data : 0.1147226333618164
	model : 0.0651167392730713
			 train-loss:  2.2066245413663093 	 ± 0.2440300860358211
	data : 0.11467785835266113
	model : 0.06512503623962403
			 train-loss:  2.207626848719841 	 ± 0.243672419480364
	data : 0.11467270851135254
	model : 0.06504392623901367
			 train-loss:  2.2080656682824813 	 ± 0.24303529265857382
	data : 0.11486430168151855
	model : 0.06501893997192383
			 train-loss:  2.2077135902711715 	 ± 0.2423801512493846
	data : 0.1149200439453125
	model : 0.06499605178833008
			 train-loss:  2.205788801738194 	 ± 0.24301660678148043
	data : 0.11493053436279296
	model : 0.06504359245300292
			 train-loss:  2.20505746521733 	 ± 0.2425182857528854
	data : 0.11501188278198242
	model : 0.06501402854919433
			 train-loss:  2.206182564719249 	 ± 0.24229242563798234
	data : 0.11503815650939941
	model : 0.06501221656799316
			 train-loss:  2.2064952769975985 	 ± 0.24164668695806912
	data : 0.11484475135803222
	model : 0.06499733924865722
			 train-loss:  2.2059942637075927 	 ± 0.24106344250289521
	data : 0.11465697288513184
	model : 0.06494231224060058
			 train-loss:  2.205549164613088 	 ± 0.24046663686537145
	data : 0.11472606658935547
	model : 0.06491050720214844
			 train-loss:  2.2052604875511888 	 ± 0.2398327185943723
	data : 0.11476731300354004
	model : 0.06491155624389648
			 train-loss:  2.203652247622773 	 ± 0.24014961019258454
	data : 0.11503019332885742
	model : 0.06494059562683105
			 train-loss:  2.2028200320207354 	 ± 0.23975558117780219
	data : 0.11500792503356934
	model : 0.06497316360473633
			 train-loss:  2.2040335050095683 	 ± 0.23966602402594173
	data : 0.11515040397644043
	model : 0.06502771377563477
			 train-loss:  2.2025971702627234 	 ± 0.2398101754123648
	data : 0.11509232521057129
	model : 0.06504006385803222
			 train-loss:  2.2009854637166506 	 ± 0.24016720866180108
	data : 0.11505141258239746
	model : 0.06502280235290528
			 train-loss:  2.199778801617138 	 ± 0.24008885770559438
	data : 0.11462998390197754
	model : 0.06500811576843261
			 train-loss:  2.1984507542975407 	 ± 0.24013717624236253
	data : 0.11457610130310059
	model : 0.06496610641479492
			 train-loss:  2.198289120638812 	 ± 0.2395113037162852
	data : 0.11469740867614746
	model : 0.06496515274047851
			 train-loss:  2.197479426233392 	 ± 0.2391393932584272
	data : 0.11479544639587402
	model : 0.06496644020080566
			 train-loss:  2.1963846820811326 	 ± 0.2389894264087756
	data : 0.11474881172180176
	model : 0.06503143310546874
			 train-loss:  2.1958800119658313 	 ± 0.23846826457171325
	data : 0.11491971015930176
	model : 0.0651336669921875
			 train-loss:  2.196483463821016 	 ± 0.23799660196745384
	data : 0.11484751701354981
	model : 0.06515007019042969
			 train-loss:  2.1958273845849576 	 ± 0.2375573329052208
	data : 0.11467623710632324
	model : 0.06517419815063477
			 train-loss:  2.1947774991011006 	 ± 0.2373982356110992
	data : 0.11456513404846191
	model : 0.06513013839721679
			 train-loss:  2.194112716280684 	 ± 0.2369737525764916
	data : 0.11453022956848144
	model : 0.06508588790893555
			 train-loss:  2.1934887207099023 	 ± 0.23653290964590287
	data : 0.11454105377197266
	model : 0.06496949195861816
			 train-loss:  2.194710515364252 	 ± 0.23655724613808726
	data : 0.11483325958251953
	model : 0.06494841575622559
			 train-loss:  2.196033997152319 	 ± 0.23669589272773944
	data : 0.11482129096984864
	model : 0.06495065689086914
			 train-loss:  2.1947713494300842 	 ± 0.23677432817100982
	data : 0.11482481956481934
	model : 0.06501264572143554
			 train-loss:  2.1952017053442807 	 ± 0.23626300575877696
	data : 0.1148310661315918
	model : 0.06506872177124023
			 train-loss:  2.196661486484037 	 ± 0.23658443182830644
	data : 0.11488680839538574
	model : 0.06505784988403321
			 train-loss:  2.19848280822115 	 ± 0.23741639642771054
	data : 0.1146402359008789
	model : 0.06507506370544433
			 train-loss:  2.1982383926709494 	 ± 0.2368593796058479
	data : 0.11460704803466797
	model : 0.06498498916625976
			 train-loss:  2.1975009801911143 	 ± 0.23651559393069888
	data : 0.114555025100708
	model : 0.06499533653259278
			 train-loss:  2.1976475912390403 	 ± 0.235950166397275
	data : 0.11468315124511719
	model : 0.06494703292846679
			 train-loss:  2.197151683954801 	 ± 0.2354871380571261
	data : 0.11461906433105469
	model : 0.06499676704406739
			 train-loss:  2.1967843152009525 	 ± 0.2349798335516969
	data : 0.11474013328552246
	model : 0.06500539779663086
			 train-loss:  2.198745871274665 	 ± 0.2361178839871499
	data : 0.11488447189331055
	model : 0.06503911018371582
			 train-loss:  2.19721877631687 	 ± 0.23658732645599742
	data : 0.11503129005432129
	model : 0.06499228477478028
			 train-loss:  2.196639068318769 	 ± 0.23617548216503964
	data : 0.11486139297485351
	model : 0.06498379707336426
			 train-loss:  2.1967831776951843 	 ± 0.23562710476189155
	data : 0.1148941993713379
	model : 0.06497159004211425
			 train-loss:  2.1961535528791902 	 ± 0.2352520291306914
	data : 0.11489734649658204
	model : 0.06495494842529297
			 train-loss:  2.194011010299219 	 ± 0.2367755830703284
	data : 0.11492257118225098
	model : 0.06502184867858887
			 train-loss:  2.1937495558760887 	 ± 0.23625526195873833
	data : 0.11499967575073242
	model : 0.0650601863861084
			 train-loss:  2.1929669164948993 	 ± 0.23598693099670087
	data : 0.11507892608642578
	model : 0.06508393287658691
			 train-loss:  2.192983682803844 	 ± 0.23544268336931032
	data : 0.11506619453430175
	model : 0.06507534980773926
			 train-loss:  2.192947017490317 	 ± 0.2349026774622683
	data : 0.11506309509277343
	model : 0.06504020690917969
			 train-loss:  2.1925935467628586 	 ± 0.23442385759702114
	data : 0.11485991477966309
	model : 0.06494855880737305
			 train-loss:  2.191424206170169 	 ± 0.23452974843207236
	data : 0.11475830078125
	model : 0.06491293907165527
			 train-loss:  2.192881374876963 	 ± 0.2349945745001331
	data : 0.11487007141113281
	model : 0.06482806205749511
			 train-loss:  2.1923208677016937 	 ± 0.23461272655675178
	data : 0.11484260559082031
	model : 0.06473770141601562
			 train-loss:  2.1936403165483687 	 ± 0.2349101767143337
	data : 0.11486263275146484
	model : 0.06472902297973633
			 train-loss:  2.1928431540727615 	 ± 0.23468734253184925
	data : 0.11480107307434081
	model : 0.06515312194824219
			 train-loss:  2.1938051478068035 	 ± 0.23460744601992975
	data : 0.11424317359924316
	model : 0.06498618125915527
			 train-loss:  2.1942526924926624 	 ± 0.23418406792029592
	data : 0.11420350074768067
	model : 0.06483931541442871
			 train-loss:  2.1943607992012595 	 ± 0.2336733263490392
	data : 0.11427288055419922
	model : 0.06471891403198242
			 train-loss:  2.193150137077298 	 ± 0.23387272212805013
	data : 0.1141573429107666
	model : 0.06463398933410644
			 train-loss:  2.195426699896567 	 ± 0.2358797688523639
	data : 0.11443095207214356
	model : 0.06404008865356445
			 train-loss:  2.1946329453717106 	 ± 0.23567273091037091
	data : 0.11513018608093262
	model : 0.06401219367980956
			 train-loss:  2.1959788639308053 	 ± 0.2360462674312655
	data : 0.11525259017944336
	model : 0.06404986381530761
			 train-loss:  2.1951927342291535 	 ± 0.2358398508350598
	data : 0.11527957916259765
	model : 0.06406917572021484
			 train-loss:  2.1953128780929827 	 ± 0.23534032748914918
	data : 0.11552672386169434
	model : 0.06398882865905761
			 train-loss:  2.19479174236966 	 ± 0.2349716155612863
	data : 0.11553888320922852
	model : 0.06399803161621094
			 train-loss:  2.1953949507246624 	 ± 0.23465263731674416
	data : 0.11538920402526856
	model : 0.06394062042236329
			 train-loss:  2.1968934611748843 	 ± 0.2352790845391276
	data : 0.11534252166748046
	model : 0.06391301155090331
			 train-loss:  2.1957301771590476 	 ± 0.23546133263458371
	data : 0.11549129486083984
	model : 0.06388993263244629
			 train-loss:  2.1955982516793644 	 ± 0.23497492259470554
	data : 0.11552643775939941
	model : 0.06392207145690917
			 train-loss:  2.1950432615798885 	 ± 0.23463909297057095
	data : 0.11554780006408691
	model : 0.06398038864135742
			 train-loss:  2.195239446560542 	 ± 0.23416939334501535
	data : 0.1155932903289795
	model : 0.06406035423278808
			 train-loss:  2.195154123781133 	 ± 0.23368679806313342
	data : 0.11565632820129394
	model : 0.06409368515014649
			 train-loss:  2.1963094039396807 	 ± 0.23389210424493848
	data : 0.11555795669555664
	model : 0.06410212516784668
			 train-loss:  2.196686222720048 	 ± 0.23348394592622407
	data : 0.11539368629455567
	model : 0.06406927108764648
			 train-loss:  2.1961474780176506 	 ± 0.23315630300083165
	data : 0.11529664993286133
	model : 0.06399455070495605
			 train-loss:  2.195611012711817 	 ± 0.2328308365862522
	data : 0.11536784172058105
	model : 0.0639714241027832
			 train-loss:  2.1955557489782813 	 ± 0.23235873139326021
	data : 0.11527853012084961
	model : 0.06396279335021973
			 train-loss:  2.1968587747952233 	 ± 0.23278675279130498
	data : 0.11523685455322266
	model : 0.06398143768310546
			 train-loss:  2.196654094803718 	 ± 0.23233922037639104
	data : 0.11532988548278808
	model : 0.06399898529052735
			 train-loss:  2.195857105006176 	 ± 0.23221164462140068
	data : 0.1153113842010498
	model : 0.06408877372741699
			 train-loss:  2.1962001881599424 	 ± 0.23180998190917884
	data : 0.11529111862182617
	model : 0.06399374008178711
			 train-loss:  2.1956207548004696 	 ± 0.23152908315131782
	data : 0.11532211303710938
	model : 0.06395702362060547
			 train-loss:  2.195211496145006 	 ± 0.231160195325013
	data : 0.11534609794616699
	model : 0.06394491195678711
			 train-loss:  2.196308389953945 	 ± 0.23135909445334465
	data : 0.11528964042663574
	model : 0.06396684646606446
			 train-loss:  2.1951028113290083 	 ± 0.23169809981972173
	data : 0.11534180641174316
	model : 0.06391572952270508
			 train-loss:  2.1952482873318244 	 ± 0.23125496622276842
	data : 0.11536736488342285
	model : 0.06399831771850586
			 train-loss:  2.1941189039498568 	 ± 0.23150639699182263
	data : 0.11505727767944336
	model : 0.05558919906616211
#epoch  41    val-loss:  2.424477062727276  train-loss:  2.1941189039498568  lr:  2.44140625e-06
			 train-loss:  1.9777485132217407 	 ± 0.0
	data : 5.5225865840911865
	model : 0.07858037948608398
			 train-loss:  2.1899091601371765 	 ± 0.2121606469154358
	data : 2.824252963066101
	model : 0.0717695951461792
			 train-loss:  2.2278697888056436 	 ± 0.18135631242812583
	data : 1.921077013015747
	model : 0.06942001978556316
			 train-loss:  2.2500798404216766 	 ± 0.16170171063259203
	data : 1.4694048762321472
	model : 0.0682024359703064
			 train-loss:  2.2297205686569215 	 ± 0.1502529679053624
	data : 1.198469352722168
	model : 0.06752901077270508
			 train-loss:  2.3056211272875466 	 ± 0.21821496123283246
	data : 0.11687889099121093
	model : 0.0647585391998291
			 train-loss:  2.3084886244365146 	 ± 0.20214986043274633
	data : 0.11454234123229981
	model : 0.0646742820739746
			 train-loss:  2.2622194290161133 	 ± 0.2252606588550596
	data : 0.11461105346679687
	model : 0.06468567848205567
			 train-loss:  2.341196378072103 	 ± 0.30822587787753164
	data : 0.11451659202575684
	model : 0.0646890640258789
			 train-loss:  2.345846080780029 	 ± 0.2927412684120338
	data : 0.11437578201293945
	model : 0.06468062400817871
			 train-loss:  2.3509604930877686 	 ± 0.279586024454461
	data : 0.11437616348266602
	model : 0.06471409797668456
			 train-loss:  2.3599012891451516 	 ± 0.2693206870812808
	data : 0.11451687812805175
	model : 0.06480641365051269
			 train-loss:  2.3410484607403097 	 ± 0.2668693912099119
	data : 0.11438789367675781
	model : 0.06492834091186524
			 train-loss:  2.3004550422940935 	 ± 0.29589510672714525
	data : 0.11453957557678222
	model : 0.06505670547485351
			 train-loss:  2.291063133875529 	 ± 0.2880137111061358
	data : 0.11417565345764161
	model : 0.06508355140686035
			 train-loss:  2.301742270588875 	 ± 0.2819185406951679
	data : 0.1141317367553711
	model : 0.06505188941955567
			 train-loss:  2.2784642191494213 	 ± 0.2889165385327358
	data : 0.11404204368591309
	model : 0.06497864723205567
			 train-loss:  2.2939135233561196 	 ± 0.2879113880061888
	data : 0.11414813995361328
	model : 0.06483249664306641
			 train-loss:  2.28913278328745 	 ± 0.2809654394219983
	data : 0.11404628753662109
	model : 0.06480536460876465
			 train-loss:  2.275343191623688 	 ± 0.28037011818118107
	data : 0.11453499794006347
	model : 0.0647965431213379
			 train-loss:  2.276051123936971 	 ± 0.2736315348373106
	data : 0.11459550857543946
	model : 0.06488509178161621
			 train-loss:  2.2750132842497393 	 ± 0.2673826148040433
	data : 0.11458206176757812
	model : 0.06499295234680176
			 train-loss:  2.259740471839905 	 ± 0.27113971463742476
	data : 0.11450438499450684
	model : 0.0650710105895996
			 train-loss:  2.2508291949828467 	 ± 0.26884939431131394
	data : 0.11464653015136719
	model : 0.06504530906677246
			 train-loss:  2.256087622642517 	 ± 0.26467418163384515
	data : 0.1146622657775879
	model : 0.0650263786315918
			 train-loss:  2.2583774465780992 	 ± 0.25978679879004174
	data : 0.11463556289672852
	model : 0.06494112014770508
			 train-loss:  2.259335478146871 	 ± 0.2549773420765015
	data : 0.11458845138549804
	model : 0.06487383842468261
			 train-loss:  2.2526496180466244 	 ± 0.25278143880820236
	data : 0.11455016136169434
	model : 0.06487913131713867
			 train-loss:  2.2515560717418275 	 ± 0.2484522980306971
	data : 0.11461105346679687
	model : 0.0648684024810791
			 train-loss:  2.2534579555193583 	 ± 0.2444909485959967
	data : 0.11464929580688477
	model : 0.0649071216583252
			 train-loss:  2.2487916061955113 	 ± 0.24186941939898232
	data : 0.11457228660583496
	model : 0.06494803428649902
			 train-loss:  2.2493077628314495 	 ± 0.23807755981401477
	data : 0.11465463638305665
	model : 0.06498017311096191
			 train-loss:  2.245091781471715 	 ± 0.23565250831909842
	data : 0.11477398872375488
	model : 0.06496639251708984
			 train-loss:  2.2418006903984966 	 ± 0.23292968839463354
	data : 0.11463122367858887
	model : 0.06493773460388183
			 train-loss:  2.2472370181764876 	 ± 0.23175609137106218
	data : 0.11447978019714355
	model : 0.06493215560913086
			 train-loss:  2.249990532795588 	 ± 0.22909448123356232
	data : 0.11460299491882324
	model : 0.06493034362792968
			 train-loss:  2.2498875405337357 	 ± 0.22597824919280574
	data : 0.1145787239074707
	model : 0.06497540473937988
			 train-loss:  2.2424770468159725 	 ± 0.22749547904734954
	data : 0.11454892158508301
	model : 0.06498203277587891
			 train-loss:  2.2404668575678115 	 ± 0.22490156759899044
	data : 0.11461257934570312
	model : 0.06504335403442382
			 train-loss:  2.241368126869202 	 ± 0.2221438191770428
	data : 0.11461291313171387
	model : 0.06505284309387208
			 train-loss:  2.233535036808107 	 ± 0.22494124044590658
	data : 0.11448640823364258
	model : 0.06509795188903808
			 train-loss:  2.2290624465261186 	 ± 0.22408480674062262
	data : 0.11445560455322265
	model : 0.06502976417541503
			 train-loss:  2.2285648750704388 	 ± 0.2214873165775794
	data : 0.11447534561157227
	model : 0.06502237319946289
			 train-loss:  2.2222351052544336 	 ± 0.22285543600826863
	data : 0.1145704746246338
	model : 0.06502351760864258
			 train-loss:  2.2207025157080755 	 ± 0.22059972200002737
	data : 0.11478047370910645
	model : 0.06503143310546874
			 train-loss:  2.21946672771288 	 ± 0.21834615151614864
	data : 0.11480145454406739
	model : 0.06503238677978515
			 train-loss:  2.218197756625236 	 ± 0.21618222063634057
	data : 0.11489357948303222
	model : 0.06508927345275879
			 train-loss:  2.213124749561151 	 ± 0.2167271918181268
	data : 0.11485915184020996
	model : 0.0650822639465332
			 train-loss:  2.20581409882526 	 ± 0.22040299439048439
	data : 0.11474194526672363
	model : 0.0650904655456543
			 train-loss:  2.213828921318054 	 ± 0.22528551204532657
	data : 0.11458625793457031
	model : 0.06504664421081544
			 train-loss:  2.2130930563982796 	 ± 0.22312657599534122
	data : 0.11454076766967773
	model : 0.06493172645568848
			 train-loss:  2.2092019823881297 	 ± 0.2227110669819142
	data : 0.1145176887512207
	model : 0.0648841381072998
			 train-loss:  2.204736005585149 	 ± 0.2229383401271975
	data : 0.11460337638854981
	model : 0.0648571491241455
			 train-loss:  2.2190687325265674 	 ± 0.2442718558273277
	data : 0.11471009254455566
	model : 0.06481547355651855
			 train-loss:  2.2153579993681474 	 ± 0.2435721834829348
	data : 0.1146845817565918
	model : 0.0648585319519043
			 train-loss:  2.2156488576105664 	 ± 0.24139727313076853
	data : 0.11480903625488281
	model : 0.06492414474487304
			 train-loss:  2.208609967900996 	 ± 0.24499977678295665
	data : 0.11485090255737304
	model : 0.06497712135314941
			 train-loss:  2.210141907478201 	 ± 0.24315375492038221
	data : 0.1147036075592041
	model : 0.0649810791015625
			 train-loss:  2.2134169340133667 	 ± 0.24237109446030863
	data : 0.11441683769226074
	model : 0.06497621536254883
			 train-loss:  2.2173280815283456 	 ± 0.24221315486751482
	data : 0.11454453468322753
	model : 0.06494197845458985
			 train-loss:  2.22017089851567 	 ± 0.24122676326664422
	data : 0.1145772933959961
	model : 0.06495203971862792
			 train-loss:  2.219936861145881 	 ± 0.23928045980717697
	data : 0.11464619636535645
	model : 0.06489391326904297
			 train-loss:  2.2243542235995095 	 ± 0.23990860419975252
	data : 0.11464695930480957
	model : 0.064923095703125
			 train-loss:  2.2231090050190687 	 ± 0.2382320498459023
	data : 0.1148716926574707
	model : 0.06497278213500976
			 train-loss:  2.222163011477544 	 ± 0.23651350302806018
	data : 0.11475195884704589
	model : 0.06501049995422363
			 train-loss:  2.224201597950675 	 ± 0.23528963272816644
	data : 0.11474142074584961
	model : 0.06497397422790527
			 train-loss:  2.2263508216658634 	 ± 0.23417896844963504
	data : 0.11445517539978027
	model : 0.06493253707885742
			 train-loss:  2.2249947628554176 	 ± 0.2327155512063637
	data : 0.11454224586486816
	model : 0.06493105888366699
			 train-loss:  2.2216649625612344 	 ± 0.23264910341352693
	data : 0.11443347930908203
	model : 0.06490826606750488
			 train-loss:  2.2202715141432625 	 ± 0.23127118190998736
	data : 0.11463069915771484
	model : 0.06489577293395996
			 train-loss:  2.2172270778199317 	 ± 0.23104508661887155
	data : 0.11466941833496094
	model : 0.06494746208190919
			 train-loss:  2.2187543594174914 	 ± 0.22979562994373812
	data : 0.11484179496765137
	model : 0.06501140594482421
			 train-loss:  2.2197058968348045 	 ± 0.22835904128154275
	data : 0.11466860771179199
	model : 0.06502537727355957
			 train-loss:  2.221156727623295 	 ± 0.22714931094813412
	data : 0.11477837562561036
	model : 0.0650212287902832
			 train-loss:  2.22556343237559 	 ± 0.22879217964335616
	data : 0.11463265419006348
	model : 0.06505308151245118
			 train-loss:  2.2241556189562144 	 ± 0.2276087553844588
	data : 0.1144831657409668
	model : 0.06498174667358399
			 train-loss:  2.2361517773046122 	 ± 0.24913839043150288
	data : 0.11444768905639649
	model : 0.06492185592651367
			 train-loss:  2.2379999298315782 	 ± 0.24806687722053025
	data : 0.11464066505432129
	model : 0.06489591598510742
			 train-loss:  2.2379531151131737 	 ± 0.24649218026256947
	data : 0.11457033157348633
	model : 0.06490559577941894
			 train-loss:  2.234879378974438 	 ± 0.2464656070289683
	data : 0.11456241607666015
	model : 0.06485419273376465
			 train-loss:  2.2327901507601324 	 ± 0.24565126397305725
	data : 0.11472110748291016
	model : 0.06495780944824218
			 train-loss:  2.2306306987273983 	 ± 0.24492112261747961
	data : 0.11479620933532715
	model : 0.0650148868560791
			 train-loss:  2.2307811946753997 	 ± 0.2434450376169298
	data : 0.11478757858276367
	model : 0.06505007743835449
			 train-loss:  2.227990296624956 	 ± 0.24332373728487472
	data : 0.11476368904113769
	model : 0.06498217582702637
			 train-loss:  2.226176100618699 	 ± 0.24245899562732887
	data : 0.11485586166381836
	model : 0.06498045921325683
			 train-loss:  2.22596353846927 	 ± 0.24105319476980125
	data : 0.11483831405639648
	model : 0.06491999626159668
			 train-loss:  2.2225631872812905 	 ± 0.24172942652713364
	data : 0.11488795280456543
	model : 0.0649569034576416
			 train-loss:  2.223553692752665 	 ± 0.2405295382401781
	data : 0.11480941772460937
	model : 0.06497678756713868
			 train-loss:  2.228534848502513 	 ± 0.2436962391517604
	data : 0.11483087539672851
	model : 0.06503872871398926
			 train-loss:  2.227620153956943 	 ± 0.2424921755090952
	data : 0.11472015380859375
	model : 0.06503210067749024
			 train-loss:  2.226187092917306 	 ± 0.24153903227396775
	data : 0.11459565162658691
	model : 0.06504878997802735
			 train-loss:  2.2244845836058906 	 ± 0.24077111253178407
	data : 0.11444892883300781
	model : 0.06505613327026367
			 train-loss:  2.2245132025851997 	 ± 0.23947330292531688
	data : 0.1144327163696289
	model : 0.06499090194702148
			 train-loss:  2.2236626173587557 	 ± 0.23833729980528667
	data : 0.11450915336608887
	model : 0.06499037742614747
			 train-loss:  2.224527140667564 	 ± 0.2372276970583173
	data : 0.11463494300842285
	model : 0.06501359939575195
			 train-loss:  2.2244985153277717 	 ± 0.23598906659263433
	data : 0.11483979225158691
	model : 0.06500520706176757
			 train-loss:  2.2280337392669365 	 ± 0.23731097063043494
	data : 0.11492404937744141
	model : 0.06497721672058106
			 train-loss:  2.2240504111562456 	 ± 0.2393343440176622
	data : 0.11492557525634765
	model : 0.06503872871398926
			 train-loss:  2.2258905654001717 	 ± 0.2388182937310624
	data : 0.11492042541503907
	model : 0.06503229141235352
			 train-loss:  2.225785003900528 	 ± 0.23762352330146172
	data : 0.1148463249206543
	model : 0.06504125595092773
			 train-loss:  2.2250781779242033 	 ± 0.23654986857324956
	data : 0.11466856002807617
	model : 0.06509265899658204
			 train-loss:  2.2228939568295196 	 ± 0.2364087702540547
	data : 0.11465978622436523
	model : 0.06510000228881836
			 train-loss:  2.2202313224088797 	 ± 0.23679027728561042
	data : 0.11463398933410644
	model : 0.06504850387573242
			 train-loss:  2.2226180755175076 	 ± 0.2368908042085563
	data : 0.11465964317321778
	model : 0.06503419876098633
			 train-loss:  2.224716125215803 	 ± 0.23672894118099197
	data : 0.1147186279296875
	model : 0.06503129005432129
			 train-loss:  2.2255724196164115 	 ± 0.23577297762229132
	data : 0.1148503303527832
	model : 0.06499419212341309
			 train-loss:  2.223010922146735 	 ± 0.2361458640748706
	data : 0.11479277610778808
	model : 0.06500411033630371
			 train-loss:  2.2232250692667783 	 ± 0.2350604914713552
	data : 0.1148198127746582
	model : 0.06500711441040039
			 train-loss:  2.2206082661217508 	 ± 0.23555481218993196
	data : 0.1147674560546875
	model : 0.06505980491638183
			 train-loss:  2.222256921638142 	 ± 0.23511256839662867
	data : 0.11472558975219727
	model : 0.06503691673278808
			 train-loss:  2.2221529215305775 	 ± 0.23405364834345205
	data : 0.11470990180969239
	model : 0.06503548622131347
			 train-loss:  2.224655369562762 	 ± 0.23449328896511024
	data : 0.11468343734741211
	model : 0.06499786376953125
			 train-loss:  2.2287847879713616 	 ± 0.23750858055774904
	data : 0.11463427543640137
	model : 0.06498308181762695
			 train-loss:  2.227317097939943 	 ± 0.23697871909545182
	data : 0.11477947235107422
	model : 0.06493625640869141
			 train-loss:  2.22721515427465 	 ± 0.23594863778756028
	data : 0.11484274864196778
	model : 0.06496710777282715
			 train-loss:  2.2269975635512123 	 ± 0.234941004231353
	data : 0.114780855178833
	model : 0.0649538516998291
			 train-loss:  2.2232527467939587 	 ± 0.23738628346915178
	data : 0.11480846405029296
	model : 0.06498336791992188
			 train-loss:  2.2219083915322515 	 ± 0.23682512369739317
	data : 0.11491971015930176
	model : 0.06504077911376953
			 train-loss:  2.2240071557149164 	 ± 0.23692740413297828
	data : 0.11524801254272461
	model : 0.06505088806152344
			 train-loss:  2.2237349251906076 	 ± 0.23595682987464853
	data : 0.11518759727478027
	model : 0.06506404876708985
			 train-loss:  2.2215423357388207 	 ± 0.23620412834683444
	data : 0.11516695022583008
	model : 0.06506900787353516
			 train-loss:  2.219179835475859 	 ± 0.23666521899196508
	data : 0.11521878242492675
	model : 0.06501483917236328
			 train-loss:  2.219855029408525 	 ± 0.23581915689721109
	data : 0.11511073112487794
	model : 0.06499981880187988
			 train-loss:  2.2181231264145143 	 ± 0.23565045851384803
	data : 0.11462879180908203
	model : 0.06502594947814941
			 train-loss:  2.2180059261322023 	 ± 0.23470959235056865
	data : 0.11474504470825195
	model : 0.06501641273498535
			 train-loss:  2.2168986759488543 	 ± 0.23410389159288358
	data : 0.11514339447021485
	model : 0.06506161689758301
			 train-loss:  2.2172511787865106 	 ± 0.23321397055623724
	data : 0.11550340652465821
	model : 0.06514067649841308
			 train-loss:  2.2168572656810284 	 ± 0.23234360370530874
	data : 0.11565542221069336
	model : 0.06517767906188965
			 train-loss:  2.2143537480701774 	 ± 0.23316802075827972
	data : 0.115604829788208
	model : 0.06520366668701172
			 train-loss:  2.213072272447439 	 ± 0.23272506718360847
	data : 0.11555643081665039
	model : 0.06521530151367187
			 train-loss:  2.2129060130082925 	 ± 0.2318428518689077
	data : 0.11520256996154785
	model : 0.06518673896789551
			 train-loss:  2.2116025180527656 	 ± 0.23144434445744042
	data : 0.11490440368652344
	model : 0.0651158332824707
			 train-loss:  2.212448666866561 	 ± 0.23077746165897095
	data : 0.11482510566711426
	model : 0.0650543212890625
			 train-loss:  2.2140905198766223 	 ± 0.2306931141042383
	data : 0.11472740173339843
	model : 0.06497268676757813
			 train-loss:  2.212677157366717 	 ± 0.23041869130533885
	data : 0.11458625793457031
	model : 0.06499004364013672
			 train-loss:  2.2117170849267174 	 ± 0.2298408580758212
	data : 0.11467037200927735
	model : 0.06499762535095215
			 train-loss:  2.2153255434801977 	 ± 0.2328348667198069
	data : 0.1145596981048584
	model : 0.06502385139465332
			 train-loss:  2.2150661858959473 	 ± 0.23200958929949625
	data : 0.11453022956848144
	model : 0.06504573822021484
			 train-loss:  2.2127368973313475 	 ± 0.2327872961618039
	data : 0.11463274955749511
	model : 0.06509809494018555
			 train-loss:  2.2103870025702883 	 ± 0.23360311022884309
	data : 0.11476316452026367
	model : 0.06505832672119141
			 train-loss:  2.2097754317817957 	 ± 0.2328857048817131
	data : 0.11457600593566894
	model : 0.06499552726745605
			 train-loss:  2.208702412289633 	 ± 0.2324137537090272
	data : 0.1144646167755127
	model : 0.06496057510375977
			 train-loss:  2.207081858928387 	 ± 0.23240339285816636
	data : 0.1146008014678955
	model : 0.06495018005371093
			 train-loss:  2.205532939069801 	 ± 0.23233453708583102
	data : 0.11479501724243164
	model : 0.06498527526855469
			 train-loss:  2.2053488394309735 	 ± 0.2315425368649896
	data : 0.11475462913513183
	model : 0.06499314308166504
			 train-loss:  2.2025707151791822 	 ± 0.23316056173303537
	data : 0.11483273506164551
	model : 0.06504673957824707
			 train-loss:  2.199228579495229 	 ± 0.23584916071777062
	data : 0.11506690979003906
	model : 0.06510100364685059
			 train-loss:  2.1964221314803973 	 ± 0.23750111011496453
	data : 0.11504735946655273
	model : 0.06508760452270508
			 train-loss:  2.195464399036945 	 ± 0.23698937010497578
	data : 0.11487183570861817
	model : 0.06504130363464355
			 train-loss:  2.1951532848676045 	 ± 0.23622861203587614
	data : 0.11471095085144042
	model : 0.06507501602172852
			 train-loss:  2.1930742824314446 	 ± 0.236817930452352
	data : 0.11470975875854492
	model : 0.06508135795593262
			 train-loss:  2.1925504842871115 	 ± 0.23612538169226535
	data : 0.11464385986328125
	model : 0.06508879661560059
			 train-loss:  2.1922763819788016 	 ± 0.2353767255057711
	data : 0.1147040843963623
	model : 0.06510210037231445
			 train-loss:  2.1914863733502177 	 ± 0.23481468806562097
	data : 0.11481747627258301
	model : 0.06511588096618652
			 train-loss:  2.1907016269622313 	 ± 0.23425850345808702
	data : 0.11491179466247559
	model : 0.06513152122497559
			 train-loss:  2.1892395256421504 	 ± 0.23421490184351465
	data : 0.1149472713470459
	model : 0.06515145301818848
			 train-loss:  2.190196774567768 	 ± 0.2337737406124694
	data : 0.11490297317504883
	model : 0.0651123046875
			 train-loss:  2.189216910283777 	 ± 0.2333559850106871
	data : 0.11485748291015625
	model : 0.06509513854980468
			 train-loss:  2.190432514784471 	 ± 0.23312230065850822
	data : 0.11480584144592285
	model : 0.06506400108337403
			 train-loss:  2.189924607425928 	 ± 0.23248088465905536
	data : 0.11491765975952148
	model : 0.06496381759643555
			 train-loss:  2.187625534786201 	 ± 0.23357521579427107
	data : 0.11486291885375977
	model : 0.06491851806640625
			 train-loss:  2.19178240461114 	 ± 0.2387522098730302
	data : 0.1149674892425537
	model : 0.06490364074707031
			 train-loss:  2.1936632497178996 	 ± 0.2392195562558066
	data : 0.11496686935424805
	model : 0.06492862701416016
			 train-loss:  2.191717144919605 	 ± 0.2397798800734824
	data : 0.11489171981811523
	model : 0.06492576599121094
			 train-loss:  2.189502555673773 	 ± 0.2407286076492333
	data : 0.1148287296295166
	model : 0.0650062084197998
			 train-loss:  2.189669917864972 	 ± 0.24001205436525
	data : 0.11493163108825684
	model : 0.06501359939575195
			 train-loss:  2.190763610565734 	 ± 0.23970691394069907
	data : 0.114892578125
	model : 0.06502571105957031
			 train-loss:  2.1892691141083125 	 ± 0.23977152063783347
	data : 0.11468544006347656
	model : 0.06498394012451172
			 train-loss:  2.189693556734796 	 ± 0.23912437724422791
	data : 0.11469874382019044
	model : 0.06497001647949219
			 train-loss:  2.191165242475622 	 ± 0.23918641737561278
	data : 0.11472878456115723
	model : 0.06490983963012695
			 train-loss:  2.192366407628645 	 ± 0.2389996979805251
	data : 0.11461663246154785
	model : 0.06492481231689454
			 train-loss:  2.1917603112930477 	 ± 0.23843568306828108
	data : 0.11457176208496093
	model : 0.06493606567382812
			 train-loss:  2.190559909522878 	 ± 0.23826623558143628
	data : 0.11467876434326171
	model : 0.06497001647949219
			 train-loss:  2.1902785205292976 	 ± 0.23760940250280832
	data : 0.11492762565612794
	model : 0.06502037048339844
			 train-loss:  2.190891605104719 	 ± 0.23706752531786743
	data : 0.1149284839630127
	model : 0.06501317024230957
			 train-loss:  2.1907916367053986 	 ± 0.23639677773854825
	data : 0.11493139266967774
	model : 0.0649688720703125
			 train-loss:  2.1917276719195695 	 ± 0.23605489910905142
	data : 0.11493854522705078
	model : 0.06495857238769531
			 train-loss:  2.1932416176528076 	 ± 0.236251054035335
	data : 0.11503996849060058
	model : 0.06491661071777344
			 train-loss:  2.1948113920968337 	 ± 0.23651928776906075
	data : 0.1149369716644287
	model : 0.0648646354675293
			 train-loss:  2.1953332742055256 	 ± 0.2359647022034346
	data : 0.11485376358032226
	model : 0.06489090919494629
			 train-loss:  2.195377590906554 	 ± 0.23531271434790885
	data : 0.11495904922485352
	model : 0.06492147445678711
			 train-loss:  2.194567765508379 	 ± 0.23491814378688972
	data : 0.11487908363342285
	model : 0.06492934226989747
			 train-loss:  2.193481411438822 	 ± 0.23473337764176633
	data : 0.11490330696105958
	model : 0.06501331329345703
			 train-loss:  2.195015610560127 	 ± 0.23501285649972578
	data : 0.11480903625488281
	model : 0.06502332687377929
			 train-loss:  2.1955046589310103 	 ± 0.2344706879594179
	data : 0.11491384506225585
	model : 0.06503233909606934
			 train-loss:  2.1948166111464142 	 ± 0.23402673291123008
	data : 0.11484498977661133
	model : 0.06503734588623047
			 train-loss:  2.1930451405877096 	 ± 0.23464722468464824
	data : 0.11505436897277832
	model : 0.06505250930786133
			 train-loss:  2.1931267596305686 	 ± 0.23402499238429908
	data : 0.11497716903686524
	model : 0.06499304771423339
			 train-loss:  2.19478164021931 	 ± 0.23450540138979922
	data : 0.11496806144714355
	model : 0.06504960060119629
			 train-loss:  2.194267047078986 	 ± 0.23399443568688746
	data : 0.11482963562011719
	model : 0.06503472328186036
			 train-loss:  2.1944615953255697 	 ± 0.2333964872311955
	data : 0.1148841381072998
	model : 0.06502456665039062
			 train-loss:  2.1945136661330857 	 ± 0.23278900272284042
	data : 0.11479811668395996
	model : 0.06502690315246581
			 train-loss:  2.194335783083822 	 ± 0.2321982217717361
	data : 0.11468448638916015
	model : 0.0650395393371582
			 train-loss:  2.193521488573133 	 ± 0.23187511763057922
	data : 0.11477823257446289
	model : 0.06499576568603516
			 train-loss:  2.192141685730372 	 ± 0.2320769156574242
	data : 0.11488180160522461
	model : 0.06503033638000488
			 train-loss:  2.1929857499745427 	 ± 0.23178400955922623
	data : 0.1148979663848877
	model : 0.06504406929016113
			 train-loss:  2.193529069726237 	 ± 0.2313200721733592
	data : 0.1147603988647461
	model : 0.06503763198852539
			 train-loss:  2.1932441766815955 	 ± 0.23076983718642322
	data : 0.1149104118347168
	model : 0.06504530906677246
			 train-loss:  2.1924244423008443 	 ± 0.23047810106412522
	data : 0.11493487358093261
	model : 0.06505084037780762
			 train-loss:  2.192441779375076 	 ± 0.22990131384798335
	data : 0.11492252349853516
	model : 0.06502456665039062
			 train-loss:  2.191926137724919 	 ± 0.22944461879101652
	data : 0.1147852897644043
	model : 0.06510705947875976
			 train-loss:  2.1918821936786763 	 ± 0.22887682987464986
	data : 0.11495532989501953
	model : 0.0651087760925293
			 train-loss:  2.191051857812064 	 ± 0.22861719416149145
	data : 0.11494760513305664
	model : 0.06512866020202637
			 train-loss:  2.1911065484963212 	 ± 0.22805750074594608
	data : 0.11498279571533203
	model : 0.06511268615722657
			 train-loss:  2.1924052471067847 	 ± 0.2282555263886131
	data : 0.11489129066467285
	model : 0.06511406898498535
			 train-loss:  2.192741689172763 	 ± 0.22775178261269766
	data : 0.11491475105285645
	model : 0.06502470970153809
			 train-loss:  2.1932040643000947 	 ± 0.22729789142177487
	data : 0.11500744819641114
	model : 0.06498656272888184
			 train-loss:  2.1929111778736115 	 ± 0.22678999582188622
	data : 0.11476521492004395
	model : 0.06491656303405761
			 train-loss:  2.194105265813581 	 ± 0.2269012646630941
	data : 0.11459355354309082
	model : 0.06492819786071777
			 train-loss:  2.195420007478623 	 ± 0.22715696591823856
	data : 0.11480975151062012
	model : 0.06493940353393554
			 train-loss:  2.195043470057266 	 ± 0.22668372224623334
	data : 0.11488399505615235
	model : 0.06496591567993164
			 train-loss:  2.196673578811142 	 ± 0.22738470960144933
	data : 0.11471247673034668
	model : 0.06499462127685547
			 train-loss:  2.196182649460197 	 ± 0.22696290391074384
	data : 0.11481275558471679
	model : 0.06504640579223633
			 train-loss:  2.197476655523354 	 ± 0.22721819235045293
	data : 0.11491479873657226
	model : 0.06506867408752441
			 train-loss:  2.1963422104369763 	 ± 0.22729581354489223
	data : 0.11474809646606446
	model : 0.06509175300598144
			 train-loss:  2.1952916140909546 	 ± 0.22729168778284028
	data : 0.11465620994567871
	model : 0.0650747299194336
			 train-loss:  2.1948980043560677 	 ± 0.22684114361600904
	data : 0.1146049976348877
	model : 0.06505584716796875
			 train-loss:  2.1948722885289325 	 ± 0.22632058477720166
	data : 0.11493110656738281
	model : 0.06500558853149414
			 train-loss:  2.193952626289298 	 ± 0.22621118663581927
	data : 0.11498193740844727
	model : 0.06495118141174316
			 train-loss:  2.194523312286897 	 ± 0.22585443917880652
	data : 0.11508073806762695
	model : 0.06488242149353027
			 train-loss:  2.193734987289118 	 ± 0.2256460338080424
	data : 0.11513042449951172
	model : 0.06484899520874024
			 train-loss:  2.19277059978193 	 ± 0.22559326272324792
	data : 0.1153419017791748
	model : 0.06481266021728516
			 train-loss:  2.1920861326525563 	 ± 0.22531779656617495
	data : 0.11518263816833496
	model : 0.06478080749511719
			 train-loss:  2.1919347191495553 	 ± 0.22482566270909218
	data : 0.11512346267700195
	model : 0.06531243324279785
			 train-loss:  2.195659850968255 	 ± 0.2311499235020856
	data : 0.11476869583129883
	model : 0.06513609886169433
			 train-loss:  2.196170064727817 	 ± 0.2307649052681278
	data : 0.11493968963623047
	model : 0.06492772102355956
			 train-loss:  2.196988486508441 	 ± 0.2305845338737508
	data : 0.11506619453430175
	model : 0.06475729942321777
			 train-loss:  2.196197172528819 	 ± 0.23038700323683192
	data : 0.11511349678039551
	model : 0.06465654373168946
			 train-loss:  2.197262279331424 	 ± 0.23044531677475003
	data : 0.11519513130187989
	model : 0.06396880149841308
			 train-loss:  2.19533117076625 	 ± 0.23179330191828804
	data : 0.1156552791595459
	model : 0.06396875381469727
			 train-loss:  2.1950301282849662 	 ± 0.23133609661779186
	data : 0.11566877365112305
	model : 0.06398701667785645
			 train-loss:  2.1938903665748137 	 ± 0.23148606354569132
	data : 0.11550207138061523
	model : 0.0639880657196045
			 train-loss:  2.1931125754450522 	 ± 0.23129238238536812
	data : 0.11546506881713867
	model : 0.0639315128326416
			 train-loss:  2.1933639553877025 	 ± 0.2308295339895881
	data : 0.11549406051635742
	model : 0.0639045238494873
			 train-loss:  2.192260866469525 	 ± 0.23095513220056735
	data : 0.11540956497192383
	model : 0.0639129638671875
			 train-loss:  2.1921683898416617 	 ± 0.2304696610971684
	data : 0.11543159484863282
	model : 0.06388025283813477
			 train-loss:  2.19175970956746 	 ± 0.23006860290718323
	data : 0.11534442901611328
	model : 0.06390442848205566
			 train-loss:  2.191841173071821 	 ± 0.22958818202637069
	data : 0.11544699668884277
	model : 0.06388425827026367
			 train-loss:  2.1906538593220413 	 ± 0.2298384176821368
	data : 0.11553802490234374
	model : 0.06390066146850586
			 train-loss:  2.190106488764286 	 ± 0.22951513901851708
	data : 0.11573071479797363
	model : 0.06389594078063965
			 train-loss:  2.1899990070904933 	 ± 0.2290445240641334
	data : 0.11575746536254883
	model : 0.06394271850585938
			 train-loss:  2.1912985473624933 	 ± 0.2294593937263965
	data : 0.11586813926696778
	model : 0.06393022537231445
			 train-loss:  2.190900791329121 	 ± 0.22907035349399393
	data : 0.11574826240539551
	model : 0.06396417617797852
			 train-loss:  2.1902890503406525 	 ± 0.2287992784329674
	data : 0.11564044952392578
	model : 0.06397275924682617
			 train-loss:  2.1891079226318673 	 ± 0.22907604690497402
	data : 0.11540837287902832
	model : 0.0639875888824463
			 train-loss:  2.1891750179655185 	 ± 0.22861238330222086
	data : 0.11535029411315918
	model : 0.06395602226257324
			 train-loss:  2.189659889410382 	 ± 0.2282758481899424
	data : 0.11543292999267578
	model : 0.06392731666564941
			 train-loss:  2.189623570730609 	 ± 0.22781586480431426
	data : 0.11559653282165527
	model : 0.06394462585449219
			 train-loss:  2.1915095288111983 	 ± 0.22928962044136744
	data : 0.1155817985534668
	model : 0.06394195556640625
			 train-loss:  2.19293484544754 	 ± 0.22993321959412982
	data : 0.11573276519775391
	model : 0.06390833854675293
			 train-loss:  2.192508370277891 	 ± 0.2295737814669314
	data : 0.1157759666442871
	model : 0.06390886306762696
			 train-loss:  2.191970767009826 	 ± 0.22927608059916205
	data : 0.11568541526794433
	model : 0.0639369010925293
			 train-loss:  2.1919552700321665 	 ± 0.22882264942729366
	data : 0.11555466651916504
	model : 0.06396169662475586
			 train-loss:  2.191601939558044 	 ± 0.22844090933991296
	data : 0.11562838554382324
	model : 0.06393446922302246
			 train-loss:  2.191495095514784 	 ± 0.2279989048208568
	data : 0.11564016342163086
	model : 0.0639681339263916
			 train-loss:  2.1910201185382903 	 ± 0.22767953102548322
	data : 0.11534452438354492
	model : 0.05561075210571289
#epoch  42    val-loss:  2.4444441418898735  train-loss:  2.1910201185382903  lr:  2.44140625e-06
			 train-loss:  2.3096604347229004 	 ± 0.0
	data : 5.433353424072266
	model : 0.07143211364746094
			 train-loss:  2.4176105260849 	 ± 0.10795009136199951
	data : 2.783074736595154
	model : 0.07162988185882568
			 train-loss:  2.4158101081848145 	 ± 0.08817764924253343
	data : 1.8923906485239665
	model : 0.06924875577290852
			 train-loss:  2.388509750366211 	 ± 0.0898187173173042
	data : 1.4479650259017944
	model : 0.068092942237854
			 train-loss:  2.334333896636963 	 ± 0.13488518856268897
	data : 1.1813075065612793
	model : 0.06741461753845215
			 train-loss:  2.2922529776891074 	 ± 0.15496998776797646
	data : 0.11755962371826172
	model : 0.06612024307250977
			 train-loss:  2.2339161123548235 	 ± 0.20249449962481014
	data : 0.11390061378479004
	model : 0.0647165298461914
			 train-loss:  2.1919695287942886 	 ± 0.21953389416630179
	data : 0.1147233486175537
	model : 0.0648122787475586
			 train-loss:  2.2007974651124744 	 ± 0.20847919762905726
	data : 0.1147073745727539
	model : 0.06483154296875
			 train-loss:  2.2040210366249084 	 ± 0.198017021943347
	data : 0.11477036476135254
	model : 0.06486759185791016
			 train-loss:  2.1973721222444014 	 ± 0.18996896795899065
	data : 0.11495184898376465
	model : 0.0648691177368164
			 train-loss:  2.2264095644156137 	 ± 0.20580515440929045
	data : 0.11491909027099609
	model : 0.06488323211669922
			 train-loss:  2.2299253665483914 	 ± 0.19810592230120233
	data : 0.11482172012329102
	model : 0.06542439460754394
			 train-loss:  2.219040640762874 	 ± 0.19489196537496753
	data : 0.114483642578125
	model : 0.06547660827636718
			 train-loss:  2.199067203203837 	 ± 0.20257300215185683
	data : 0.11452398300170899
	model : 0.06554613113403321
			 train-loss:  2.1738075092434883 	 ± 0.21918454430073847
	data : 0.11434702873229981
	model : 0.06564183235168457
			 train-loss:  2.159214363378637 	 ± 0.22050675938158865
	data : 0.11443066596984863
	model : 0.06572794914245605
			 train-loss:  2.1459655629263983 	 ± 0.22114692503803363
	data : 0.11448297500610352
	model : 0.06519427299499511
			 train-loss:  2.137809671853718 	 ± 0.218012152173461
	data : 0.114845609664917
	model : 0.06521525382995605
			 train-loss:  2.1474593341350556 	 ± 0.21661495154137586
	data : 0.1147228717803955
	model : 0.06515378952026367
			 train-loss:  2.141646629288083 	 ± 0.212986866399631
	data : 0.11446909904479981
	model : 0.06500606536865235
			 train-loss:  2.1335335319692437 	 ± 0.21138519721738377
	data : 0.11447944641113281
	model : 0.06497488021850586
			 train-loss:  2.1328224099200703 	 ± 0.206765706170654
	data : 0.11439895629882812
	model : 0.06499605178833008
			 train-loss:  2.1212174842755 	 ± 0.20992434449472605
	data : 0.1144021987915039
	model : 0.06499300003051758
			 train-loss:  2.125870175361633 	 ± 0.20694212239007898
	data : 0.11446495056152343
	model : 0.06501317024230957
			 train-loss:  2.11451839025204 	 ± 0.21071189014369202
	data : 0.11468887329101562
	model : 0.06510720252990723
			 train-loss:  2.1097115543153553 	 ± 0.2082206093612768
	data : 0.11471357345581054
	model : 0.06508474349975586
			 train-loss:  2.106845519372395 	 ± 0.20501020014288882
	data : 0.11482419967651367
	model : 0.06506538391113281
			 train-loss:  2.1031766718831557 	 ± 0.2023778456063529
	data : 0.11477999687194824
	model : 0.06506924629211426
			 train-loss:  2.092847748597463 	 ± 0.20660464421257127
	data : 0.11458878517150879
	model : 0.06506462097167968
			 train-loss:  2.0978329912308724 	 ± 0.2050709813645242
	data : 0.11461772918701171
	model : 0.06497607231140137
			 train-loss:  2.1009591929614544 	 ± 0.20259043362581713
	data : 0.11454291343688965
	model : 0.06495084762573242
			 train-loss:  2.1041899269277398 	 ± 0.20033263485520955
	data : 0.11450815200805664
	model : 0.06495356559753418
			 train-loss:  2.1058543464716744 	 ± 0.1975960451124129
	data : 0.11456665992736817
	model : 0.0649630069732666
			 train-loss:  2.120068199293954 	 ± 0.21165488741559924
	data : 0.1148000717163086
	model : 0.06497979164123535
			 train-loss:  2.124544928471247 	 ± 0.21036836006829485
	data : 0.1148188591003418
	model : 0.06501331329345703
			 train-loss:  2.1280965837272436 	 ± 0.20859741886266683
	data : 0.11480774879455566
	model : 0.06500144004821777
			 train-loss:  2.121539824887326 	 ± 0.20966277085733487
	data : 0.11480679512023925
	model : 0.06502947807312012
			 train-loss:  2.1237117693974423 	 ± 0.20738996134714288
	data : 0.11472611427307129
	model : 0.0650022029876709
			 train-loss:  2.1464076399803163 	 ± 0.249046843555481
	data : 0.11462922096252441
	model : 0.0649033546447754
			 train-loss:  2.144022325190102 	 ± 0.24645310062433676
	data : 0.11450681686401368
	model : 0.06490893363952636
			 train-loss:  2.144534247262137 	 ± 0.24352352177714454
	data : 0.11455440521240234
	model : 0.06492996215820312
			 train-loss:  2.145616198694983 	 ± 0.24077731559912116
	data : 0.11456775665283203
	model : 0.06486644744873046
			 train-loss:  2.1458175724202935 	 ± 0.23802914730858335
	data : 0.11465678215026856
	model : 0.064872407913208
			 train-loss:  2.148771709865994 	 ± 0.23618381806207506
	data : 0.11458792686462402
	model : 0.06496119499206543
			 train-loss:  2.1514431549155195 	 ± 0.23428886929888146
	data : 0.11466178894042969
	model : 0.06497378349304199
			 train-loss:  2.1477105034158583 	 ± 0.23316148574896148
	data : 0.11457781791687012
	model : 0.06498594284057617
			 train-loss:  2.1516340052088103 	 ± 0.23228258871377527
	data : 0.11448655128479004
	model : 0.06499123573303223
			 train-loss:  2.146768399647304 	 ± 0.23235841303297092
	data : 0.11430659294128417
	model : 0.06496763229370117
			 train-loss:  2.1479061365127565 	 ± 0.23016092474317212
	data : 0.11443214416503907
	model : 0.06495456695556641
			 train-loss:  2.146786512113085 	 ± 0.22803074865165182
	data : 0.11445384025573731
	model : 0.06495366096496583
			 train-loss:  2.1486073319728556 	 ± 0.22620155834075178
	data : 0.1145721435546875
	model : 0.06493072509765625
			 train-loss:  2.141632743601529 	 ± 0.22963288327033338
	data : 0.11451773643493653
	model : 0.06495084762573242
			 train-loss:  2.1405243277549744 	 ± 0.22763978405753413
	data : 0.11467113494873046
	model : 0.06495704650878906
			 train-loss:  2.141983662952076 	 ± 0.22581561816132842
	data : 0.11464934349060059
	model : 0.06494674682617188
			 train-loss:  2.1498650163412094 	 ± 0.23129737312215826
	data : 0.11452240943908691
	model : 0.0649381160736084
			 train-loss:  2.1544828143036154 	 ± 0.2318492016998453
	data : 0.11438660621643067
	model : 0.06493239402770996
			 train-loss:  2.152962078308237 	 ± 0.23012839609083693
	data : 0.11438632011413574
	model : 0.06494979858398438
			 train-loss:  2.1553157972077193 	 ± 0.2288728615592444
	data : 0.11448383331298828
	model : 0.06498117446899414
			 train-loss:  2.159893351793289 	 ± 0.22966503143794798
	data : 0.1145298957824707
	model : 0.06498169898986816
			 train-loss:  2.1663704328849667 	 ± 0.23323484403441067
	data : 0.1146623134613037
	model : 0.06499524116516113
			 train-loss:  2.1755086587321375 	 ± 0.24210542104950186
	data : 0.11463723182678223
	model : 0.0649881362915039
			 train-loss:  2.173881737012712 	 ± 0.24051765782910284
	data : 0.1146975040435791
	model : 0.06498265266418457
			 train-loss:  2.170016234740615 	 ± 0.2405955317524322
	data : 0.11456422805786133
	model : 0.06497931480407715
			 train-loss:  2.171522516470689 	 ± 0.239041547521525
	data : 0.11451349258422852
	model : 0.06495790481567383
			 train-loss:  2.173912946021918 	 ± 0.23800527470445795
	data : 0.11453022956848144
	model : 0.06493959426879883
			 train-loss:  2.172343611717224 	 ± 0.23656624008391666
	data : 0.11464715003967285
	model : 0.06498918533325196
			 train-loss:  2.1711908456157234 	 ± 0.23500984299669028
	data : 0.11466021537780761
	model : 0.06501045227050781
			 train-loss:  2.1741696768912715 	 ± 0.23459026103148783
	data : 0.11470484733581543
	model : 0.06500229835510254
			 train-loss:  2.179049370970045 	 ± 0.23640938706487677
	data : 0.11475253105163574
	model : 0.06505284309387208
			 train-loss:  2.1756484122343465 	 ± 0.23645693073176935
	data : 0.1146660327911377
	model : 0.06511216163635254
			 train-loss:  2.178999120990435 	 ± 0.23650044585977406
	data : 0.11469926834106445
	model : 0.06508488655090332
			 train-loss:  2.1787022874779898 	 ± 0.23488849849900845
	data : 0.11453628540039062
	model : 0.06502151489257812
			 train-loss:  2.180707018117647 	 ± 0.2339239490539166
	data : 0.11444945335388183
	model : 0.06501083374023438
			 train-loss:  2.18280952612559 	 ± 0.2330620697927973
	data : 0.11443767547607422
	model : 0.06498627662658692
			 train-loss:  2.1802837472212944 	 ± 0.23255469404574605
	data : 0.11467456817626953
	model : 0.0649646282196045
			 train-loss:  2.176609359778367 	 ± 0.2332496763599179
	data : 0.11470303535461426
	model : 0.06503181457519532
			 train-loss:  2.173564762641222 	 ± 0.23328451142135795
	data : 0.11501030921936035
	model : 0.0650777816772461
			 train-loss:  2.1784206960774677 	 ± 0.23573720206932547
	data : 0.11511316299438476
	model : 0.0650784969329834
			 train-loss:  2.1770387426018716 	 ± 0.23458101355665767
	data : 0.11502695083618164
	model : 0.06504640579223633
			 train-loss:  2.1747726098990734 	 ± 0.234007947259433
	data : 0.11487407684326172
	model : 0.06503615379333497
			 train-loss:  2.175942830923127 	 ± 0.23281503542848994
	data : 0.11480555534362794
	model : 0.06496005058288574
			 train-loss:  2.1793891464371278 	 ± 0.23350313890571742
	data : 0.11446585655212402
	model : 0.06491646766662598
			 train-loss:  2.1797545098123097 	 ± 0.23213294407727691
	data : 0.11452269554138184
	model : 0.06490473747253418
			 train-loss:  2.182389155556174 	 ± 0.23202333580659507
	data : 0.1146303653717041
	model : 0.0649271011352539
			 train-loss:  2.1821566792421563 	 ± 0.23068037590498208
	data : 0.11465167999267578
	model : 0.06496119499206543
			 train-loss:  2.18328569675314 	 ± 0.2295896549638918
	data : 0.11469893455505371
	model : 0.06497383117675781
			 train-loss:  2.1851647442037407 	 ± 0.22895326522186746
	data : 0.11494860649108887
	model : 0.06496200561523438
			 train-loss:  2.1855740359659945 	 ± 0.22769575127818434
	data : 0.11480913162231446
	model : 0.0649477481842041
			 train-loss:  2.1841577609380085 	 ± 0.2268211078349067
	data : 0.1146385669708252
	model : 0.06493558883666992
			 train-loss:  2.1820837219992835 	 ± 0.2264279172987412
	data : 0.1145561695098877
	model : 0.06487374305725098
			 train-loss:  2.1787939162357994 	 ± 0.22737018329376354
	data : 0.11463427543640137
	model : 0.06488175392150879
			 train-loss:  2.180781196522456 	 ± 0.2269463601991541
	data : 0.1146578311920166
	model : 0.06492247581481933
			 train-loss:  2.1789026856422424 	 ± 0.2264617126262529
	data : 0.11466684341430664
	model : 0.06497626304626465
			 train-loss:  2.179137484650863 	 ± 0.2252781579077108
	data : 0.11477112770080566
	model : 0.06502208709716797
			 train-loss:  2.1819656329850354 	 ± 0.22579072211707593
	data : 0.11482172012329102
	model : 0.06504807472229004
			 train-loss:  2.1835028006858432 	 ± 0.2251281971439055
	data : 0.11463203430175781
	model : 0.06506886482238769
			 train-loss:  2.1813167856664073 	 ± 0.22500903286331128
	data : 0.11438579559326172
	model : 0.06502280235290528
			 train-loss:  2.179736726211779 	 ± 0.22441552037833978
	data : 0.11443037986755371
	model : 0.06503615379333497
			 train-loss:  2.178750239610672 	 ± 0.22350625260698284
	data : 0.11446428298950195
	model : 0.06501007080078125
			 train-loss:  2.178708774028438 	 ± 0.22239742014775746
	data : 0.11452350616455079
	model : 0.06502151489257812
			 train-loss:  2.1784022169954635 	 ± 0.2213259953585816
	data : 0.11463866233825684
	model : 0.06504201889038086
			 train-loss:  2.175804101147698 	 ± 0.2218065210941513
	data : 0.1148453712463379
	model : 0.06515698432922364
			 train-loss:  2.17541297353231 	 ± 0.22077325665543587
	data : 0.11483430862426758
	model : 0.06510434150695801
			 train-loss:  2.17339954943884 	 ± 0.22067676730797042
	data : 0.11483197212219239
	model : 0.06514506340026856
			 train-loss:  2.17176076713598 	 ± 0.22027439046115696
	data : 0.11476402282714844
	model : 0.06517305374145507
			 train-loss:  2.1698651848552384 	 ± 0.22010957320750046
	data : 0.11458368301391601
	model : 0.0651237964630127
			 train-loss:  2.167607194847531 	 ± 0.22032968721559443
	data : 0.11450028419494629
	model : 0.06508235931396485
			 train-loss:  2.1694334537611093 	 ± 0.22013633607722877
	data : 0.11459689140319824
	model : 0.06513791084289551
			 train-loss:  2.1671929196877913 	 ± 0.22037840224938895
	data : 0.11496481895446778
	model : 0.06511054039001465
			 train-loss:  2.168416642927909 	 ± 0.21975856689016018
	data : 0.11494693756103516
	model : 0.06510224342346191
			 train-loss:  2.1664706276995793 	 ± 0.21973390063003959
	data : 0.11511397361755371
	model : 0.06511030197143555
			 train-loss:  2.1645823569424385 	 ± 0.2196703153755383
	data : 0.1151742935180664
	model : 0.0651200294494629
			 train-loss:  2.166529625131373 	 ± 0.2196821271778202
	data : 0.11513466835021972
	model : 0.0651017665863037
			 train-loss:  2.1683944132017055 	 ± 0.21962925648684517
	data : 0.11472339630126953
	model : 0.0651163101196289
			 train-loss:  2.1688492904449332 	 ± 0.21873492871388586
	data : 0.11508808135986329
	model : 0.06506199836730957
			 train-loss:  2.168520119455126 	 ± 0.2178270109303679
	data : 0.1149360179901123
	model : 0.06506595611572266
			 train-loss:  2.1690166612802924 	 ± 0.2169685383811206
	data : 0.11504912376403809
	model : 0.06505560874938965
			 train-loss:  2.16858952085511 	 ± 0.21610479974817082
	data : 0.1150585651397705
	model : 0.06502375602722169
			 train-loss:  2.1659707287947336 	 ± 0.21709034445221684
	data : 0.1151318073272705
	model : 0.06503539085388184
			 train-loss:  2.1648576180796977 	 ± 0.21653500896610495
	data : 0.11473283767700196
	model : 0.06504297256469727
			 train-loss:  2.1660228674529027 	 ± 0.21602634471886006
	data : 0.11492452621459961
	model : 0.06501612663269044
			 train-loss:  2.1653032012102083 	 ± 0.2152931912186078
	data : 0.11468801498413086
	model : 0.06498713493347168
			 train-loss:  2.1639540464647355 	 ± 0.21494474964780705
	data : 0.11456751823425293
	model : 0.06500563621520997
			 train-loss:  2.1636312713623047 	 ± 0.21411341439766465
	data : 0.11457514762878418
	model : 0.06495037078857421
			 train-loss:  2.16168859459105 	 ± 0.21436524365267354
	data : 0.11461858749389649
	model : 0.06491246223449706
			 train-loss:  2.162272571578739 	 ± 0.21362021669350173
	data : 0.11466822624206544
	model : 0.06492643356323242
			 train-loss:  2.1617877408862114 	 ± 0.21285426287820486
	data : 0.11488347053527832
	model : 0.06490168571472169
			 train-loss:  2.1598116177921147 	 ± 0.2132031159613101
	data : 0.11498446464538574
	model : 0.06492538452148437
			 train-loss:  2.162559759616852 	 ± 0.21466288688370933
	data : 0.11490826606750489
	model : 0.06496238708496094
			 train-loss:  2.164735183461022 	 ± 0.2152756815203311
	data : 0.11498594284057617
	model : 0.06501536369323731
			 train-loss:  2.1630691091219583 	 ± 0.21530481024825424
	data : 0.11483588218688964
	model : 0.06502647399902343
			 train-loss:  2.1645316270957315 	 ± 0.21515101978486467
	data : 0.11483640670776367
	model : 0.06505508422851562
			 train-loss:  2.164771277513077 	 ± 0.2143645314432129
	data : 0.11477479934692383
	model : 0.06501479148864746
			 train-loss:  2.1647489459426317 	 ± 0.21356926944882146
	data : 0.11471662521362305
	model : 0.06494965553283691
			 train-loss:  2.1669178482364204 	 ± 0.21426971378518567
	data : 0.1148305892944336
	model : 0.06492910385131836
			 train-loss:  2.1679965684013647 	 ± 0.21385659772675428
	data : 0.1148526668548584
	model : 0.06491069793701172
			 train-loss:  2.167856940324756 	 ± 0.21308661355568295
	data : 0.11487407684326172
	model : 0.06494522094726562
			 train-loss:  2.1701193284645357 	 ± 0.21397565960542902
	data : 0.11495962142944335
	model : 0.06494646072387696
			 train-loss:  2.1706028171948026 	 ± 0.21328627675887812
	data : 0.1149672031402588
	model : 0.06499333381652832
			 train-loss:  2.1696914797979043 	 ± 0.21280197229548312
	data : 0.11477141380310059
	model : 0.06499629020690918
			 train-loss:  2.1756384473451424 	 ± 0.22350042251394953
	data : 0.11493849754333496
	model : 0.06495494842529297
			 train-loss:  2.17605668181306 	 ± 0.22277333718284767
	data : 0.11487860679626465
	model : 0.06494441032409667
			 train-loss:  2.1772030029031963 	 ± 0.22242129077968936
	data : 0.11475830078125
	model : 0.06493501663208008
			 train-loss:  2.175005587216081 	 ± 0.22321597889164002
	data : 0.11475872993469238
	model : 0.06493816375732422
			 train-loss:  2.1752307317028308 	 ± 0.22246674719756554
	data : 0.11481242179870606
	model : 0.06492881774902344
			 train-loss:  2.180377869378953 	 ± 0.23026671250317532
	data : 0.11475191116333008
	model : 0.06496138572692871
			 train-loss:  2.183081498017182 	 ± 0.23181675813285385
	data : 0.11469922065734864
	model : 0.06492695808410645
			 train-loss:  2.1827441429931844 	 ± 0.23107398924142022
	data : 0.11473517417907715
	model : 0.06497955322265625
			 train-loss:  2.1827745183308918 	 ± 0.230302753038461
	data : 0.11472549438476562
	model : 0.06501684188842774
			 train-loss:  2.1833726380834517 	 ± 0.22965575567789823
	data : 0.11473865509033203
	model : 0.06505441665649414
			 train-loss:  2.181215730152632 	 ± 0.2304284505822116
	data : 0.11458563804626465
	model : 0.06512761116027832
			 train-loss:  2.180903006223292 	 ± 0.22970654079229116
	data : 0.11448283195495605
	model : 0.06519660949707032
			 train-loss:  2.1848457989754615 	 ± 0.23409601330127858
	data : 0.1145233154296875
	model : 0.06512961387634278
			 train-loss:  2.183522011387733 	 ± 0.23391720898780927
	data : 0.11451640129089355
	model : 0.06512837409973145
			 train-loss:  2.1833701049670196 	 ± 0.2331739387273934
	data : 0.11458954811096192
	model : 0.06514678001403809
			 train-loss:  2.1817125841310827 	 ± 0.2333503159917632
	data : 0.11471610069274903
	model : 0.06508512496948242
			 train-loss:  2.182158917565889 	 ± 0.23267791310190628
	data : 0.11493339538574218
	model : 0.06504297256469727
			 train-loss:  2.1826916348259404 	 ± 0.23204170481336522
	data : 0.11495351791381836
	model : 0.06506524085998536
			 train-loss:  2.1857552327215672 	 ± 0.23451897005652164
	data : 0.11502017974853515
	model : 0.06503157615661621
			 train-loss:  2.1861687217439925 	 ± 0.233848013489911
	data : 0.11491155624389648
	model : 0.06498708724975585
			 train-loss:  2.186245292057226 	 ± 0.23312716763131178
	data : 0.11479830741882324
	model : 0.0650029182434082
			 train-loss:  2.1858588106061783 	 ± 0.23246300540439063
	data : 0.11460862159729004
	model : 0.06493854522705078
			 train-loss:  2.1866679707678354 	 ± 0.23198332942967612
	data : 0.11453299522399903
	model : 0.06493101119995118
			 train-loss:  2.1878259723836724 	 ± 0.23175423331707642
	data : 0.11452078819274902
	model : 0.06492509841918945
			 train-loss:  2.1869348108050333 	 ± 0.23133851424533033
	data : 0.11453475952148437
	model : 0.06499161720275878
			 train-loss:  2.1867838141447056 	 ± 0.23065304864886674
	data : 0.11461167335510254
	model : 0.06503105163574219
			 train-loss:  2.188743844628334 	 ± 0.23135627159571562
	data : 0.11459050178527833
	model : 0.06508374214172363
			 train-loss:  2.1889999385416155 	 ± 0.23069465158827523
	data : 0.11459498405456543
	model : 0.0651052474975586
			 train-loss:  2.188972928243525 	 ± 0.23001540516212415
	data : 0.11451687812805175
	model : 0.06508865356445312
			 train-loss:  2.187465574309143 	 ± 0.23018242362259042
	data : 0.11451506614685059
	model : 0.0650336742401123
			 train-loss:  2.186618426511454 	 ± 0.22977950668088024
	data : 0.11440181732177734
	model : 0.06503791809082031
			 train-loss:  2.1868906641282098 	 ± 0.22914225894957238
	data : 0.11451897621154786
	model : 0.06501197814941406
			 train-loss:  2.186074507647547 	 ± 0.2287348956398567
	data : 0.11473236083984376
	model : 0.06500716209411621
			 train-loss:  2.1856033856528145 	 ± 0.2281650792148444
	data : 0.1148303508758545
	model : 0.0650320053100586
			 train-loss:  2.1859876025806773 	 ± 0.22757272648544224
	data : 0.11476640701293946
	model : 0.06504111289978028
			 train-loss:  2.1875928485460876 	 ± 0.22792601878031912
	data : 0.1147944450378418
	model : 0.06497821807861329
			 train-loss:  2.1903486734025934 	 ± 0.2302230518228645
	data : 0.11475310325622559
	model : 0.06498904228210449
			 train-loss:  2.189542466701742 	 ± 0.22983090285283742
	data : 0.11450400352478027
	model : 0.06498746871948242
			 train-loss:  2.1896856745084126 	 ± 0.22919960299430112
	data : 0.1144078254699707
	model : 0.06498312950134277
			 train-loss:  2.1887074359872725 	 ± 0.2289420768379044
	data : 0.1144442081451416
	model : 0.06499838829040527
			 train-loss:  2.1900135370401235 	 ± 0.22898744686395042
	data : 0.11460466384887695
	model : 0.06500167846679687
			 train-loss:  2.191492389460079 	 ± 0.22923078809657996
	data : 0.1146153450012207
	model : 0.0650219440460205
			 train-loss:  2.1946753198685856 	 ± 0.23262664387549178
	data : 0.11463680267333984
	model : 0.06503086090087891
			 train-loss:  2.1960796124226336 	 ± 0.2327777824477123
	data : 0.1146855354309082
	model : 0.0650402545928955
			 train-loss:  2.2000584640810565 	 ± 0.23837566848663624
	data : 0.11473627090454101
	model : 0.06496930122375488
			 train-loss:  2.202628626542933 	 ± 0.24030763608083128
	data : 0.11453070640563964
	model : 0.0649641513824463
			 train-loss:  2.200485479324422 	 ± 0.24145288770864978
	data : 0.11447443962097167
	model : 0.06497430801391602
			 train-loss:  2.200402961206184 	 ± 0.2408159342172692
	data : 0.11464419364929199
	model : 0.06496496200561523
			 train-loss:  2.2005677938461305 	 ± 0.2401920618147278
	data : 0.11478877067565918
	model : 0.06498675346374512
			 train-loss:  2.2011613221692787 	 ± 0.23970211812161654
	data : 0.11481571197509766
	model : 0.06505026817321777
			 train-loss:  2.203156499813 	 ± 0.24066194097958704
	data : 0.11482300758361816
	model : 0.06508865356445312
			 train-loss:  2.2034580719903345 	 ± 0.240074024586662
	data : 0.11484904289245605
	model : 0.0650965690612793
			 train-loss:  2.202786131003468 	 ± 0.23963636470289224
	data : 0.1146775722503662
	model : 0.06511235237121582
			 train-loss:  2.2020969953292457 	 ± 0.23921377310771497
	data : 0.11451888084411621
	model : 0.06508517265319824
			 train-loss:  2.2016618215307897 	 ± 0.23868012560533908
	data : 0.11447362899780274
	model : 0.0650557518005371
			 train-loss:  2.200863256067189 	 ± 0.23833592721101754
	data : 0.11450462341308594
	model : 0.06500887870788574
			 train-loss:  2.2002912458747326 	 ± 0.2378688352517112
	data : 0.11452155113220215
	model : 0.06497492790222167
			 train-loss:  2.201693249707246 	 ± 0.23808915334819605
	data : 0.11463031768798829
	model : 0.06495308876037598
			 train-loss:  2.2000660210847856 	 ± 0.23859995692017982
	data : 0.11472907066345214
	model : 0.06494250297546386
			 train-loss:  2.201477472461871 	 ± 0.2388412546247474
	data : 0.11467194557189941
	model : 0.0649503231048584
			 train-loss:  2.202175804293982 	 ± 0.23845495243366227
	data : 0.11470069885253906
	model : 0.06500191688537597
			 train-loss:  2.2020510035782612 	 ± 0.23787351312602342
	data : 0.11468987464904785
	model : 0.0650146484375
			 train-loss:  2.2012828045031605 	 ± 0.23754206572966616
	data : 0.11468873023986817
	model : 0.0650261402130127
			 train-loss:  2.2021360786949717 	 ± 0.237275179381989
	data : 0.11465487480163575
	model : 0.065032958984375
			 train-loss:  2.2021915351302876 	 ± 0.23669989990534915
	data : 0.11459927558898926
	model : 0.06496853828430176
			 train-loss:  2.201001393046356 	 ± 0.23674452011574695
	data : 0.11458048820495606
	model : 0.06498231887817382
			 train-loss:  2.201724050136713 	 ± 0.2364034870895489
	data : 0.11474332809448243
	model : 0.06495676040649415
			 train-loss:  2.2026844423923766 	 ± 0.23624364150101199
	data : 0.11476521492004395
	model : 0.06503334045410156
			 train-loss:  2.2015320959545317 	 ± 0.2362685395677278
	data : 0.11472225189208984
	model : 0.06509170532226563
			 train-loss:  2.200076940500341 	 ± 0.23664938178963324
	data : 0.11480984687805176
	model : 0.06516871452331544
			 train-loss:  2.2002446527750985 	 ± 0.23610315546520916
	data : 0.11499719619750977
	model : 0.06513948440551758
			 train-loss:  2.200852786990958 	 ± 0.2357146393110808
	data : 0.11493110656738281
	model : 0.06517505645751953
			 train-loss:  2.1989482763771697 	 ± 0.23680021904737003
	data : 0.11494097709655762
	model : 0.06509785652160645
			 train-loss:  2.1973943982013435 	 ± 0.23733993309221735
	data : 0.11493959426879882
	model : 0.06504588127136231
			 train-loss:  2.1966337158724114 	 ± 0.23705244727588126
	data : 0.11495375633239746
	model : 0.06500911712646484
			 train-loss:  2.1955509509908437 	 ± 0.23704037398923183
	data : 0.1147468090057373
	model : 0.06500430107116699
			 train-loss:  2.1947549958841517 	 ± 0.23678655854575903
	data : 0.11481833457946777
	model : 0.06495027542114258
			 train-loss:  2.1937675688364733 	 ± 0.23669475970345935
	data : 0.11488418579101563
	model : 0.06490535736083984
			 train-loss:  2.1918565419587224 	 ± 0.23784353273028158
	data : 0.11493010520935058
	model : 0.06487822532653809
			 train-loss:  2.1918355701196246 	 ± 0.23730501896278833
	data : 0.1149559497833252
	model : 0.0648427963256836
			 train-loss:  2.192683756351471 	 ± 0.2371054588784668
	data : 0.1150054931640625
	model : 0.0647697925567627
			 train-loss:  2.1927774385486485 	 ± 0.23657735277853958
	data : 0.11494088172912598
	model : 0.06473321914672851
			 train-loss:  2.191352632961103 	 ± 0.23700567242633716
	data : 0.11472845077514648
	model : 0.06470561027526855
			 train-loss:  2.19041382683648 	 ± 0.23689546379507298
	data : 0.1147353172302246
	model : 0.06457533836364746
			 train-loss:  2.1911086519207577 	 ± 0.23660044483388407
	data : 0.11484522819519043
	model : 0.06441421508789062
			 train-loss:  2.1906955242156982 	 ± 0.2361604031463878
	data : 0.11499032974243165
	model : 0.06429524421691894
			 train-loss:  2.1921153654131973 	 ± 0.23661095402778184
	data : 0.11511588096618652
	model : 0.06425280570983886
			 train-loss:  2.19065964638406 	 ± 0.23711479832041765
	data : 0.1151669979095459
	model : 0.0641031265258789
			 train-loss:  2.1912682040877964 	 ± 0.2367779261777389
	data : 0.11525702476501465
	model : 0.06401381492614747
			 train-loss:  2.190784591100949 	 ± 0.23637867649424377
	data : 0.11515274047851562
	model : 0.06403751373291015
			 train-loss:  2.1920407501787973 	 ± 0.23664011037234545
	data : 0.1152677059173584
	model : 0.0639836311340332
			 train-loss:  2.1913267097759657 	 ± 0.2363820864729005
	data : 0.11525201797485352
	model : 0.06388368606567382
			 train-loss:  2.1919288548648868 	 ± 0.236055466409967
	data : 0.11550278663635254
	model : 0.06389422416687011
			 train-loss:  2.1915738739865893 	 ± 0.2356152673787243
	data : 0.11548538208007812
	model : 0.06395759582519531
			 train-loss:  2.191623464984409 	 ± 0.23511678159533947
	data : 0.11553292274475098
	model : 0.0639564037322998
			 train-loss:  2.1916743708059254 	 ± 0.23462153360551719
	data : 0.11547684669494629
	model : 0.0639641284942627
			 train-loss:  2.190997129227935 	 ± 0.2343601384937041
	data : 0.11541581153869629
	model : 0.06397137641906739
			 train-loss:  2.19179179927794 	 ± 0.23419043805715714
	data : 0.1151693344116211
	model : 0.06393427848815918
			 train-loss:  2.1932229811946553 	 ± 0.2347470534283607
	data : 0.11517176628112794
	model : 0.06387920379638672
			 train-loss:  2.195086395592116 	 ± 0.23603151891328286
	data : 0.11522865295410156
	model : 0.06384625434875488
			 train-loss:  2.1943628950552507 	 ± 0.23581098333334657
	data : 0.11520142555236816
	model : 0.06386823654174804
			 train-loss:  2.194086962276035 	 ± 0.2353644213225097
	data : 0.11514472961425781
	model : 0.06385798454284668
			 train-loss:  2.194455651474781 	 ± 0.23495192649375918
	data : 0.11532878875732422
	model : 0.06387596130371094
			 train-loss:  2.1961880727690093 	 ± 0.23602839870872575
	data : 0.1152388572692871
	model : 0.06385445594787598
			 train-loss:  2.1950060705828474 	 ± 0.2362736562701499
	data : 0.1152738094329834
	model : 0.06386938095092773
			 train-loss:  2.1958633603354696 	 ± 0.23617795022893984
	data : 0.11534204483032226
	model : 0.06387877464294434
			 train-loss:  2.1963518122511525 	 ± 0.2358262820058219
	data : 0.11539649963378906
	model : 0.06390080451965333
			 train-loss:  2.195639038181688 	 ± 0.2356197813643213
	data : 0.11524467468261719
	model : 0.06393222808837891
			 train-loss:  2.196339132785797 	 ± 0.23540742899473432
	data : 0.11542444229125977
	model : 0.06394939422607422
			 train-loss:  2.1958264663399927 	 ± 0.23507781871235045
	data : 0.11529679298400879
	model : 0.06392383575439453
			 train-loss:  2.197218608761591 	 ± 0.23564537350450904
	data : 0.11511001586914063
	model : 0.06388654708862304
			 train-loss:  2.196485239055317 	 ± 0.23546718307221093
	data : 0.11511435508728027
	model : 0.06389360427856446
			 train-loss:  2.195752973162283 	 ± 0.2352916693951438
	data : 0.1152773380279541
	model : 0.06388463973999023
			 train-loss:  2.1961440586576275 	 ± 0.23491256223665077
	data : 0.11520938873291016
	model : 0.06390590667724609
			 train-loss:  2.194229995831847 	 ± 0.23643725816483738
	data : 0.11501226425170899
	model : 0.055554962158203124
#epoch  43    val-loss:  2.4155821298298084  train-loss:  2.194229995831847  lr:  2.44140625e-06
			 train-loss:  2.357175350189209 	 ± 0.0
	data : 5.4846274852752686
	model : 0.07174444198608398
			 train-loss:  2.146226406097412 	 ± 0.21094894409179688
	data : 2.8029842376708984
	model : 0.06875765323638916
			 train-loss:  2.181277275085449 	 ± 0.1792301077086728
	data : 1.9074129263559978
	model : 0.06734069188435872
			 train-loss:  2.223986566066742 	 ± 0.17194424747667783
	data : 1.4591469168663025
	model : 0.0666799545288086
			 train-loss:  2.1843326568603514 	 ± 0.1730363818786426
	data : 1.1901828289031982
	model : 0.06631951332092285
			 train-loss:  2.195716222127279 	 ± 0.15999766353181993
	data : 0.11614232063293457
	model : 0.06493487358093261
			 train-loss:  2.190474135535104 	 ± 0.1486845422992311
	data : 0.11490607261657715
	model : 0.06474161148071289
			 train-loss:  2.1793606281280518 	 ± 0.1421558187591932
	data : 0.11455631256103516
	model : 0.06479535102844239
			 train-loss:  2.1871747970581055 	 ± 0.13583594003782107
	data : 0.11456561088562012
	model : 0.06483640670776367
			 train-loss:  2.175302815437317 	 ± 0.1336965140675019
	data : 0.11456828117370606
	model : 0.06480231285095214
			 train-loss:  2.19317906553095 	 ± 0.13944670329332143
	data : 0.11454739570617675
	model : 0.06478652954101563
			 train-loss:  2.224583407243093 	 ± 0.16933249520749835
	data : 0.11430373191833496
	model : 0.06490159034729004
			 train-loss:  2.224530109992394 	 ± 0.16268950378215524
	data : 0.11427736282348633
	model : 0.06495780944824218
			 train-loss:  2.2148216451917375 	 ± 0.160631933066709
	data : 0.11442570686340332
	model : 0.06502642631530761
			 train-loss:  2.204094584782918 	 ± 0.16029167459010543
	data : 0.11445479393005371
	model : 0.06512818336486817
			 train-loss:  2.1985798478126526 	 ± 0.15666450456863934
	data : 0.11444525718688965
	model : 0.0651623249053955
			 train-loss:  2.1857450919992782 	 ± 0.1604235372407876
	data : 0.11456837654113769
	model : 0.06511545181274414
			 train-loss:  2.2162854737705655 	 ± 0.20040484819590038
	data : 0.11453385353088379
	model : 0.06503715515136718
			 train-loss:  2.2051766859857658 	 ± 0.20067287127324393
	data : 0.11435651779174805
	model : 0.06497573852539062
			 train-loss:  2.2100599467754365 	 ± 0.19674653665224773
	data : 0.11439476013183594
	model : 0.06491785049438477
			 train-loss:  2.2161898215611777 	 ± 0.1939520862464562
	data : 0.11441235542297364
	model : 0.06493697166442872
			 train-loss:  2.2303012880412014 	 ± 0.20022320698962087
	data : 0.11451077461242676
	model : 0.06492033004760742
			 train-loss:  2.226165683373161 	 ± 0.19678056084618165
	data : 0.11452980041503906
	model : 0.0650219440460205
			 train-loss:  2.2202273656924567 	 ± 0.19473112514820437
	data : 0.11465363502502442
	model : 0.06500849723815919
			 train-loss:  2.217351698875427 	 ± 0.19131615114476805
	data : 0.11461911201477051
	model : 0.06501908302307129
			 train-loss:  2.2166384687790504 	 ± 0.18763481259964884
	data : 0.11463212966918945
	model : 0.06497702598571778
			 train-loss:  2.205116073290507 	 ± 0.1932738265732893
	data : 0.11454086303710938
	model : 0.06492462158203124
			 train-loss:  2.2051378786563873 	 ± 0.18979116384782557
	data : 0.11454129219055176
	model : 0.06489396095275879
			 train-loss:  2.1986956061988043 	 ± 0.18958025673059778
	data : 0.11452913284301758
	model : 0.06492729187011718
			 train-loss:  2.200758707523346 	 ± 0.18672462748431407
	data : 0.1145902156829834
	model : 0.06490044593811035
			 train-loss:  2.1977049496866043 	 ± 0.1844481963828392
	data : 0.1145637035369873
	model : 0.06500563621520997
			 train-loss:  2.2023375518620014 	 ± 0.18336648467852626
	data : 0.11449294090270996
	model : 0.0650479793548584
			 train-loss:  2.2080983501492124 	 ± 0.18348394563080514
	data : 0.11456937789916992
	model : 0.06501355171203613
			 train-loss:  2.2023386429337894 	 ± 0.18376866107457998
	data : 0.11456232070922852
	model : 0.06499419212341309
			 train-loss:  2.200119015148708 	 ± 0.18158619649372465
	data : 0.1144035816192627
	model : 0.06500759124755859
			 train-loss:  2.1955135431554584 	 ± 0.18110764183908995
	data : 0.11443119049072266
	model : 0.06491060256958008
			 train-loss:  2.1879866090980737 	 ± 0.18426356815550893
	data : 0.11435012817382813
	model : 0.0649226188659668
			 train-loss:  2.1934510187098852 	 ± 0.18483606921370227
	data : 0.11435790061950683
	model : 0.06496295928955079
			 train-loss:  2.192493790235275 	 ± 0.18254638263450051
	data : 0.11438703536987305
	model : 0.0650064468383789
			 train-loss:  2.191095557808876 	 ± 0.180461489992821
	data : 0.11451048851013183
	model : 0.06504426002502442
			 train-loss:  2.187890608136247 	 ± 0.1793959761601014
	data : 0.1145214557647705
	model : 0.0651285171508789
			 train-loss:  2.1872176244145347 	 ± 0.1772998182235463
	data : 0.1147228717803955
	model : 0.06512455940246582
			 train-loss:  2.1856984620870548 	 ± 0.17550243206506666
	data : 0.11465644836425781
	model : 0.06512303352355957
			 train-loss:  2.1901456198909064 	 ± 0.17593038031120709
	data : 0.11442561149597168
	model : 0.06503806114196778
			 train-loss:  2.193008859952291 	 ± 0.17499830271868175
	data : 0.11439142227172852
	model : 0.06494317054748536
			 train-loss:  2.1892272415368454 	 ± 0.17493480700446715
	data : 0.11438198089599609
	model : 0.06490788459777833
			 train-loss:  2.1855120227691973 	 ± 0.17488855728418384
	data : 0.11430983543395996
	model : 0.06486945152282715
			 train-loss:  2.180132918059826 	 ± 0.1769427394704371
	data : 0.11439919471740723
	model : 0.06485157012939453
			 train-loss:  2.1794617589639156 	 ± 0.17518961479807868
	data : 0.11453571319580078
	model : 0.06488747596740722
			 train-loss:  2.1786366629600527 	 ± 0.1735250169220599
	data : 0.1145857334136963
	model : 0.06495428085327148
			 train-loss:  2.1743299797469495 	 ± 0.174493258010666
	data : 0.11473231315612793
	model : 0.06496181488037109
			 train-loss:  2.1752835856034207 	 ± 0.1729414299098354
	data : 0.11469058990478516
	model : 0.06496334075927734
			 train-loss:  2.174485145874743 	 ± 0.17139887011463262
	data : 0.11440362930297851
	model : 0.06493048667907715
			 train-loss:  2.17655571964052 	 ± 0.1704721947006818
	data : 0.11443142890930176
	model : 0.06492481231689454
			 train-loss:  2.1726597157391634 	 ± 0.17132439770923666
	data : 0.11442956924438477
	model : 0.06497535705566407
			 train-loss:  2.1702631392649243 	 ± 0.17071555977018052
	data : 0.1144038200378418
	model : 0.06497440338134766
			 train-loss:  2.174252859333105 	 ± 0.1718252253834524
	data : 0.11446166038513184
	model : 0.06501703262329102
			 train-loss:  2.1749557852745056 	 ± 0.17042018412818571
	data : 0.11477937698364257
	model : 0.06506376266479492
			 train-loss:  2.1692321502556235 	 ± 0.17450175967995427
	data : 0.11474180221557617
	model : 0.06508760452270508
			 train-loss:  2.167198089758555 	 ± 0.17374537822183594
	data : 0.11482572555541992
	model : 0.06502866744995117
			 train-loss:  2.180950797972132 	 ± 0.2025852884851549
	data : 0.11457982063293456
	model : 0.06491632461547851
			 train-loss:  2.182299571652566 	 ± 0.20122082733843577
	data : 0.11456947326660157
	model : 0.06484885215759277
			 train-loss:  2.1805475023057728 	 ± 0.200093603324682
	data : 0.11446232795715332
	model : 0.06483259201049804
			 train-loss:  2.1916198544204235 	 ± 0.21710705828894084
	data : 0.11449956893920898
	model : 0.06480674743652344
			 train-loss:  2.19829604442303 	 ± 0.22195245084837142
	data : 0.11448230743408203
	model : 0.06483001708984375
			 train-loss:  2.1971248424414433 	 ± 0.22046687828795228
	data : 0.11461901664733887
	model : 0.06493325233459472
			 train-loss:  2.1948428758934364 	 ± 0.21959934779077128
	data : 0.11462631225585937
	model : 0.06502013206481934
			 train-loss:  2.190533822073656 	 ± 0.22081383842992924
	data : 0.1145822525024414
	model : 0.06503410339355468
			 train-loss:  2.191806781119195 	 ± 0.2194590885681931
	data : 0.11445384025573731
	model : 0.06501665115356445
			 train-loss:  2.192545357772282 	 ± 0.21797224136637436
	data : 0.11440548896789551
	model : 0.06493105888366699
			 train-loss:  2.198292005230004 	 ± 0.22170790092925027
	data : 0.11429505348205567
	model : 0.06487827301025391
			 train-loss:  2.1998592797252865 	 ± 0.22055859591143653
	data : 0.11431794166564942
	model : 0.06484694480895996
			 train-loss:  2.200471131768945 	 ± 0.21910422978215752
	data : 0.11437439918518066
	model : 0.06485366821289062
			 train-loss:  2.1958381549732104 	 ± 0.22118958251606047
	data : 0.11452889442443848
	model : 0.0649439811706543
			 train-loss:  2.1971528784434002 	 ± 0.2200009299197188
	data : 0.11454668045043945
	model : 0.06503324508666992
			 train-loss:  2.196915915137843 	 ± 0.21855839733840995
	data : 0.11467838287353516
	model : 0.06507654190063476
			 train-loss:  2.1997833344843483 	 ± 0.21856873399476082
	data : 0.11469526290893554
	model : 0.06506204605102539
			 train-loss:  2.1988441974688797 	 ± 0.21731943892894465
	data : 0.1146470546722412
	model : 0.06501917839050293
			 train-loss:  2.200393402123753 	 ± 0.21637264567439246
	data : 0.11459951400756836
	model : 0.06489691734313965
			 train-loss:  2.198507708311081 	 ± 0.2156683083462689
	data : 0.11440744400024414
	model : 0.06486411094665527
			 train-loss:  2.1965679033302967 	 ± 0.2150339841929851
	data : 0.11439023017883301
	model : 0.06483945846557618
			 train-loss:  2.1939259389551675 	 ± 0.21503742618054308
	data : 0.11439690589904786
	model : 0.06491971015930176
			 train-loss:  2.1930486581411706 	 ± 0.21388567509149484
	data : 0.11439180374145508
	model : 0.06499228477478028
			 train-loss:  2.1920088472820463 	 ± 0.21281967493602208
	data : 0.11443767547607422
	model : 0.06513967514038085
			 train-loss:  2.1940566315370447 	 ± 0.21239494290767855
	data : 0.11453680992126465
	model : 0.06514554023742676
			 train-loss:  2.1937356022901313 	 ± 0.2111772199565289
	data : 0.11453356742858886
	model : 0.06511917114257812
			 train-loss:  2.1923534048014672 	 ± 0.21035095204905802
	data : 0.1144364356994629
	model : 0.06500978469848633
			 train-loss:  2.194970577955246 	 ± 0.21057213742285724
	data : 0.11441493034362793
	model : 0.06498956680297852
			 train-loss:  2.1953022051393316 	 ± 0.20940891511193716
	data : 0.11443943977355957
	model : 0.06493172645568848
			 train-loss:  2.190780136320326 	 ± 0.21256720987120292
	data : 0.11452093124389648
	model : 0.06497735977172851
			 train-loss:  2.1916954753163096 	 ± 0.21157430925337276
	data : 0.11453056335449219
	model : 0.06507158279418945
			 train-loss:  2.190788852131885 	 ± 0.2105989681260785
	data : 0.11461048126220703
	model : 0.06512041091918945
			 train-loss:  2.190380370745095 	 ± 0.20950029546604998
	data : 0.11483001708984375
	model : 0.06512007713317872
			 train-loss:  2.1872773297289583 	 ± 0.2105206385134088
	data : 0.11482343673706055
	model : 0.06510090827941895
			 train-loss:  2.1849154685672962 	 ± 0.21065799907910626
	data : 0.11477293968200683
	model : 0.0650254249572754
			 train-loss:  2.183159297953049 	 ± 0.2102558611942013
	data : 0.11456289291381835
	model : 0.06494655609130859
			 train-loss:  2.183831673307517 	 ± 0.20927297955309007
	data : 0.1145930290222168
	model : 0.0649538516998291
			 train-loss:  2.1860194388700993 	 ± 0.20931450795560846
	data : 0.11446223258972169
	model : 0.06495876312255859
			 train-loss:  2.1836318644610317 	 ± 0.20959165591601844
	data : 0.11442108154296875
	model : 0.06503472328186036
			 train-loss:  2.181674357652664 	 ± 0.20944862631418293
	data : 0.11447296142578126
	model : 0.06511368751525878
			 train-loss:  2.1869136590768794 	 ± 0.21489395332105324
	data : 0.114666748046875
	model : 0.0651885986328125
			 train-loss:  2.1860052125126708 	 ± 0.21403276534184776
	data : 0.11453547477722167
	model : 0.0651712417602539
			 train-loss:  2.1870347354018573 	 ± 0.2132448796379354
	data : 0.11448931694030762
	model : 0.06515860557556152
			 train-loss:  2.187220260500908 	 ± 0.2122255401238027
	data : 0.11443052291870118
	model : 0.0650702953338623
			 train-loss:  2.18554409685589 	 ± 0.21190309366167534
	data : 0.11437287330627441
	model : 0.06506247520446777
			 train-loss:  2.185683602432035 	 ± 0.21090602677438042
	data : 0.11431770324707032
	model : 0.0650324821472168
			 train-loss:  2.1826206857913006 	 ± 0.212273580888783
	data : 0.11444072723388672
	model : 0.06505794525146484
			 train-loss:  2.183562053574456 	 ± 0.21151281471180342
	data : 0.11458067893981934
	model : 0.06505985260009765
			 train-loss:  2.1851236032783437 	 ± 0.21116482824615937
	data : 0.11466097831726074
	model : 0.06509671211242676
			 train-loss:  2.1829928105527703 	 ± 0.2113766921853227
	data : 0.11469898223876954
	model : 0.06511931419372559
			 train-loss:  2.1818822987444766 	 ± 0.21074448636711218
	data : 0.11477575302124024
	model : 0.06511983871459961
			 train-loss:  2.180641569197178 	 ± 0.2102083876784431
	data : 0.11470284461975097
	model : 0.06513075828552246
			 train-loss:  2.1824857502911996 	 ± 0.21018429598502522
	data : 0.11461849212646484
	model : 0.06513428688049316
			 train-loss:  2.181441979450092 	 ± 0.2095543499800929
	data : 0.1143256664276123
	model : 0.0650601863861084
			 train-loss:  2.1806085182272867 	 ± 0.2088309463815811
	data : 0.11423449516296387
	model : 0.06502599716186523
			 train-loss:  2.1846503440676064 	 ± 0.2123984315045589
	data : 0.11449222564697266
	model : 0.06496257781982422
			 train-loss:  2.187821525793809 	 ± 0.21422897413473954
	data : 0.11454501152038574
	model : 0.06491575241088868
			 train-loss:  2.1846222150123724 	 ± 0.21610803935059195
	data : 0.11462244987487794
	model : 0.06492891311645507
			 train-loss:  2.1842934163678596 	 ± 0.21522774436776804
	data : 0.1149641990661621
	model : 0.06502842903137207
			 train-loss:  2.1859371503194174 	 ± 0.2150778430968828
	data : 0.11501975059509277
	model : 0.06500158309936524
			 train-loss:  2.1858881918851996 	 ± 0.21418791923598463
	data : 0.11480545997619629
	model : 0.06507973670959473
			 train-loss:  2.1836664598496234 	 ± 0.21470373874747317
	data : 0.11484990119934083
	model : 0.06510682106018066
			 train-loss:  2.1817868928599164 	 ± 0.21483462386890273
	data : 0.11470789909362793
	model : 0.06515302658081054
			 train-loss:  2.179433024698688 	 ± 0.2155532698439747
	data : 0.11460161209106445
	model : 0.06509037017822265
			 train-loss:  2.1778213243484497 	 ± 0.21543817237819485
	data : 0.11467690467834472
	model : 0.06508817672729492
			 train-loss:  2.180370693168943 	 ± 0.2164662891473056
	data : 0.11470680236816407
	model : 0.06505427360534669
			 train-loss:  2.179546706319794 	 ± 0.21581066858316306
	data : 0.11467890739440918
	model : 0.06508278846740723
			 train-loss:  2.1773690516129136 	 ± 0.2163622893450666
	data : 0.11482462882995606
	model : 0.06504979133605956
			 train-loss:  2.1780436935350878 	 ± 0.2156571577923827
	data : 0.1149078369140625
	model : 0.065089750289917
			 train-loss:  2.1787703725007863 	 ± 0.21498459439094764
	data : 0.11486587524414063
	model : 0.06509790420532227
			 train-loss:  2.1788943769367597 	 ± 0.21416713758010766
	data : 0.11481699943542481
	model : 0.06510987281799316
			 train-loss:  2.1778532564640045 	 ± 0.21368686518742494
	data : 0.11472625732421875
	model : 0.06508708000183105
			 train-loss:  2.181527637001267 	 ± 0.21702740384379027
	data : 0.11462535858154296
	model : 0.0650327205657959
			 train-loss:  2.1824760161229033 	 ± 0.2164925358148373
	data : 0.11433615684509277
	model : 0.06494789123535157
			 train-loss:  2.1832268388183027 	 ± 0.2158642643061307
	data : 0.11422457695007324
	model : 0.06496062278747558
			 train-loss:  2.180603908265338 	 ± 0.2172176812288338
	data : 0.1142085075378418
	model : 0.0649488925933838
			 train-loss:  2.179596035149846 	 ± 0.2167423947751797
	data : 0.11429352760314941
	model : 0.06493978500366211
			 train-loss:  2.178847873556441 	 ± 0.21613314390569466
	data : 0.11432385444641113
	model : 0.06497306823730468
			 train-loss:  2.181709623165268 	 ± 0.2179624591177601
	data : 0.11459112167358398
	model : 0.06504631042480469
			 train-loss:  2.1814564219542913 	 ± 0.21720314167472618
	data : 0.11471843719482422
	model : 0.06504058837890625
			 train-loss:  2.1817016745290014 	 ± 0.21645100019240754
	data : 0.11467018127441406
	model : 0.06511611938476562
			 train-loss:  2.1825739875645707 	 ± 0.2159360779240034
	data : 0.11455669403076171
	model : 0.06505179405212402
			 train-loss:  2.180691732393278 	 ± 0.21634557239946495
	data : 0.11444268226623536
	model : 0.0649803638458252
			 train-loss:  2.183622588713964 	 ± 0.21842327608296283
	data : 0.1143916130065918
	model : 0.0649752140045166
			 train-loss:  2.185926713614628 	 ± 0.21941785846190745
	data : 0.11433892250061035
	model : 0.06496601104736328
			 train-loss:  2.185018534529699 	 ± 0.21893843034512792
	data : 0.11442289352416993
	model : 0.06492948532104492
			 train-loss:  2.183522573133715 	 ± 0.2189399184143155
	data : 0.1145364761352539
	model : 0.06503710746765137
			 train-loss:  2.182749988259496 	 ± 0.21839997091368882
	data : 0.11463212966918945
	model : 0.06513142585754395
			 train-loss:  2.1828511689333308 	 ± 0.21766933171870045
	data : 0.11469507217407227
	model : 0.06513476371765137
			 train-loss:  2.181015056769053 	 ± 0.218097218212012
	data : 0.11469783782958984
	model : 0.0651611328125
			 train-loss:  2.1832473601726505 	 ± 0.21908643069739056
	data : 0.11462655067443847
	model : 0.0650944709777832
			 train-loss:  2.182416971576841 	 ± 0.2186028440876028
	data : 0.11436281204223633
	model : 0.06495413780212403
			 train-loss:  2.1816791358336904 	 ± 0.2180770914628811
	data : 0.11440305709838867
	model : 0.06491174697875976
			 train-loss:  2.1826143411846903 	 ± 0.21767548622938804
	data : 0.11443252563476562
	model : 0.06488866806030273
			 train-loss:  2.184490361521321 	 ± 0.21821759474143507
	data : 0.11448020935058593
	model : 0.0648622989654541
			 train-loss:  2.185966431330412 	 ± 0.21829196137005177
	data : 0.1145174503326416
	model : 0.06487627029418945
			 train-loss:  2.1885812457200067 	 ± 0.22003290683731605
	data : 0.11476335525512696
	model : 0.06494030952453614
			 train-loss:  2.1894662674469285 	 ± 0.21961564537613426
	data : 0.11473922729492188
	model : 0.06495456695556641
			 train-loss:  2.18942517229596 	 ± 0.2189245502546658
	data : 0.1147280216217041
	model : 0.06501431465148926
			 train-loss:  2.1877088874578474 	 ± 0.2193097460487028
	data : 0.11469588279724122
	model : 0.0650219440460205
			 train-loss:  2.1856352179687217 	 ± 0.2201954674078224
	data : 0.11443018913269043
	model : 0.06499304771423339
			 train-loss:  2.1871718146182872 	 ± 0.2203789670347103
	data : 0.11449666023254394
	model : 0.06495003700256348
			 train-loss:  2.1874045250605954 	 ± 0.219721882634988
	data : 0.11443557739257812
	model : 0.06491894721984863
			 train-loss:  2.1869424443419385 	 ± 0.2191304017807722
	data : 0.11439399719238282
	model : 0.06485543251037598
			 train-loss:  2.187283406835614 	 ± 0.21850899268918078
	data : 0.114447021484375
	model : 0.06486272811889648
			 train-loss:  2.1863768668059844 	 ± 0.21816083915896417
	data : 0.11467766761779785
	model : 0.0649024486541748
			 train-loss:  2.1849251271721846 	 ± 0.21830943585656276
	data : 0.11467704772949219
	model : 0.06497879028320312
			 train-loss:  2.186168276837894 	 ± 0.21825079730488725
	data : 0.11481409072875977
	model : 0.064996337890625
			 train-loss:  2.184732286182381 	 ± 0.21839868049833244
	data : 0.11476402282714844
	model : 0.06499390602111817
			 train-loss:  2.1851327952216653 	 ± 0.2178176212682123
	data : 0.1146012306213379
	model : 0.06494269371032715
			 train-loss:  2.184939594993814 	 ± 0.2171944020917178
	data : 0.11458749771118164
	model : 0.0649592399597168
			 train-loss:  2.184940225856249 	 ± 0.2165621027933802
	data : 0.11457686424255371
	model : 0.06492328643798828
			 train-loss:  2.1845429288169553 	 ± 0.2159981490094126
	data : 0.11451215744018554
	model : 0.06488633155822754
			 train-loss:  2.1832319361039962 	 ± 0.21606573644973134
	data : 0.11457514762878418
	model : 0.06491174697875976
			 train-loss:  2.183508736746652 	 ± 0.21547845852690045
	data : 0.11465072631835938
	model : 0.06495656967163085
			 train-loss:  2.182938589291139 	 ± 0.21499776906513332
	data : 0.11470866203308105
	model : 0.06493334770202637
			 train-loss:  2.1827967086080777 	 ± 0.2143978330889072
	data : 0.11456542015075684
	model : 0.06499357223510742
			 train-loss:  2.183690194333537 	 ± 0.21412495072040197
	data : 0.11452622413635254
	model : 0.06502137184143067
			 train-loss:  2.183043075007433 	 ± 0.21370047258412306
	data : 0.11430025100708008
	model : 0.06497459411621094
			 train-loss:  2.1837150163120693 	 ± 0.2132955716548833
	data : 0.11420440673828125
	model : 0.06499896049499512
			 train-loss:  2.184698937347581 	 ± 0.21311477048825445
	data : 0.11418142318725585
	model : 0.06497039794921874
			 train-loss:  2.1849314734175964 	 ± 0.21255150831459438
	data : 0.11425304412841797
	model : 0.06493535041809081
			 train-loss:  2.1872034932746263 	 ± 0.21417461589714484
	data : 0.11424217224121094
	model : 0.06494784355163574
			 train-loss:  2.1876005634017615 	 ± 0.21365935760079288
	data : 0.11438770294189453
	model : 0.06497111320495605
			 train-loss:  2.18857898969908 	 ± 0.21349404936332894
	data : 0.11452698707580566
	model : 0.0649444580078125
			 train-loss:  2.187163169025093 	 ± 0.21378844292309365
	data : 0.11451044082641601
	model : 0.06501402854919433
			 train-loss:  2.1852126045023055 	 ± 0.21486916591737332
	data : 0.11440949440002442
	model : 0.0649651050567627
			 train-loss:  2.185517189350534 	 ± 0.2143374169631545
	data : 0.11442747116088867
	model : 0.06494522094726562
			 train-loss:  2.1872130563019443 	 ± 0.2150305515475642
	data : 0.11447973251342773
	model : 0.06494650840759278
			 train-loss:  2.188401450608906 	 ± 0.2150853332789651
	data : 0.11445789337158203
	model : 0.06492800712585449
			 train-loss:  2.187446982448638 	 ± 0.214924601975497
	data : 0.11442794799804687
	model : 0.06494255065917968
			 train-loss:  2.1900095331172147 	 ± 0.21726995040498945
	data : 0.11451950073242187
	model : 0.06497817039489746
			 train-loss:  2.1905132871835344 	 ± 0.21681873296213136
	data : 0.11446156501770019
	model : 0.06500124931335449
			 train-loss:  2.19091534614563 	 ± 0.2163313204939765
	data : 0.11443243026733399
	model : 0.06500153541564942
			 train-loss:  2.1909326064280976 	 ± 0.2157760457486121
	data : 0.1144073486328125
	model : 0.06501498222351074
			 train-loss:  2.189966620231161 	 ± 0.21564719956449255
	data : 0.11431121826171875
	model : 0.06490740776062012
			 train-loss:  2.1888033048755626 	 ± 0.21571486394654257
	data : 0.11441216468811036
	model : 0.06491312980651856
			 train-loss:  2.1882645517888695 	 ± 0.21530227142616196
	data : 0.11457023620605469
	model : 0.06494884490966797
			 train-loss:  2.187888700159351 	 ± 0.21482573961525858
	data : 0.11459064483642578
	model : 0.06497397422790527
			 train-loss:  2.188160389661789 	 ± 0.21432227399688988
	data : 0.11473522186279297
	model : 0.06497778892517089
			 train-loss:  2.190173456324867 	 ± 0.21567567563654597
	data : 0.11489281654357911
	model : 0.06507000923156739
			 train-loss:  2.1905028312513144 	 ± 0.21519183512480808
	data : 0.11476187705993653
	model : 0.06509356498718262
			 train-loss:  2.1910119221128266 	 ± 0.2147830604616984
	data : 0.1146928310394287
	model : 0.0650289535522461
			 train-loss:  2.1915627881592394 	 ± 0.21439969218326146
	data : 0.11470985412597656
	model : 0.06500487327575684
			 train-loss:  2.1912698734097367 	 ± 0.21391704147647117
	data : 0.11454272270202637
	model : 0.06496796607971192
			 train-loss:  2.1922834301457823 	 ± 0.21389006105748923
	data : 0.11450634002685547
	model : 0.06488442420959473
			 train-loss:  2.1914460693580518 	 ± 0.21371099741351438
	data : 0.11457304954528809
	model : 0.06488947868347168
			 train-loss:  2.1902300503391485 	 ± 0.2139133071644957
	data : 0.11456599235534667
	model : 0.06493582725524902
			 train-loss:  2.189415773706573 	 ± 0.21372382768626225
	data : 0.11462931632995606
	model : 0.06501331329345703
			 train-loss:  2.1899728530929203 	 ± 0.21336640166201268
	data : 0.11480798721313476
	model : 0.06504707336425782
			 train-loss:  2.1889206600415196 	 ± 0.21340561183090456
	data : 0.11487231254577637
	model : 0.06513915061950684
			 train-loss:  2.188501887164026 	 ± 0.21298858617364425
	data : 0.11489605903625488
	model : 0.065128755569458
			 train-loss:  2.1874366578921465 	 ± 0.21305332644303562
	data : 0.11482348442077636
	model : 0.0650702953338623
			 train-loss:  2.188043859517463 	 ± 0.21273960808952722
	data : 0.11475396156311035
	model : 0.06500639915466308
			 train-loss:  2.1887223044107125 	 ± 0.2124762089944363
	data : 0.11503934860229492
	model : 0.06497077941894532
			 train-loss:  2.1886145483564445 	 ± 0.21198968354149939
	data : 0.11490116119384766
	model : 0.06485948562622071
			 train-loss:  2.190952845982143 	 ± 0.21427445066436668
	data : 0.11474804878234864
	model : 0.06475434303283692
			 train-loss:  2.191677116472787 	 ± 0.21404849646311896
	data : 0.11475038528442383
	model : 0.06476731300354004
			 train-loss:  2.1958305911930727 	 ± 0.22218988429855166
	data : 0.11486310958862304
	model : 0.0647355079650879
			 train-loss:  2.1970731951973654 	 ± 0.2224457086339734
	data : 0.1144641399383545
	model : 0.06475839614868165
			 train-loss:  2.1971530299380895 	 ± 0.22194502608515135
	data : 0.11460857391357422
	model : 0.06480917930603028
			 train-loss:  2.1963808085467362 	 ± 0.22174195037707714
	data : 0.11477327346801758
	model : 0.06487655639648438
			 train-loss:  2.1963119164710623 	 ± 0.22124659363984045
	data : 0.1147758960723877
	model : 0.06483616828918456
			 train-loss:  2.1973961432065283 	 ± 0.22134514967128088
	data : 0.11477575302124024
	model : 0.06487135887145996
			 train-loss:  2.197999004787869 	 ± 0.22103695763921188
	data : 0.11465463638305665
	model : 0.0646902084350586
			 train-loss:  2.1987679964673204 	 ± 0.22084883310217227
	data : 0.11472468376159668
	model : 0.06453351974487305
			 train-loss:  2.1972854342229566 	 ± 0.22148609133963906
	data : 0.11481876373291015
	model : 0.06441617012023926
			 train-loss:  2.1972887897700595 	 ± 0.2209998482562354
	data : 0.11508717536926269
	model : 0.06426472663879394
			 train-loss:  2.196452171521416 	 ± 0.2208783325896984
	data : 0.1150639533996582
	model : 0.06411561965942383
			 train-loss:  2.195958357790242 	 ± 0.2205242877567138
	data : 0.1152733325958252
	model : 0.06413455009460449
			 train-loss:  2.194684636025202 	 ± 0.22089269405930806
	data : 0.11517291069030762
	model : 0.06416511535644531
			 train-loss:  2.193820346018364 	 ± 0.22080720432227469
	data : 0.11510000228881836
	model : 0.06409807205200195
			 train-loss:  2.1946449279785156 	 ± 0.22069053908766026
	data : 0.11493725776672363
	model : 0.0640629768371582
			 train-loss:  2.194574498722696 	 ± 0.22022109729510397
	data : 0.11484313011169434
	model : 0.06402955055236817
			 train-loss:  2.195291156971708 	 ± 0.22002532182993342
	data : 0.11487040519714356
	model : 0.06400346755981445
			 train-loss:  2.1959191166748435 	 ± 0.21976960352617858
	data : 0.11495308876037598
	model : 0.06396889686584473
			 train-loss:  2.1946323928953726 	 ± 0.2201945095797391
	data : 0.11504478454589843
	model : 0.06400151252746582
			 train-loss:  2.1941320700805726 	 ± 0.2198663855158867
	data : 0.11517691612243652
	model : 0.06404814720153809
			 train-loss:  2.193849252856426 	 ± 0.21944930963271897
	data : 0.11530041694641113
	model : 0.06404180526733398
			 train-loss:  2.1932944233218827 	 ± 0.21915956270762657
	data : 0.11507916450500488
	model : 0.06399693489074706
			 train-loss:  2.193303816051404 	 ± 0.21870445057605106
	data : 0.11510491371154785
	model : 0.06398301124572754
			 train-loss:  2.193777354788189 	 ± 0.21837588441454472
	data : 0.11507182121276856
	model : 0.06397452354431152
			 train-loss:  2.1931966356779813 	 ± 0.21811325192051167
	data : 0.11494712829589844
	model : 0.0640416145324707
			 train-loss:  2.1921338258219545 	 ± 0.21829544657361102
	data : 0.11497735977172852
	model : 0.0641026496887207
			 train-loss:  2.193543864756214 	 ± 0.21896009578056122
	data : 0.11517324447631835
	model : 0.06415247917175293
			 train-loss:  2.1921210477991804 	 ± 0.219646559624642
	data : 0.11522474288940429
	model : 0.06417646408081054
			 train-loss:  2.1915444550726577 	 ± 0.21938795272063993
	data : 0.11510586738586426
	model : 0.06414198875427246
			 train-loss:  2.1922693776507534 	 ± 0.21924141610630848
	data : 0.11515917778015136
	model : 0.06405096054077149
			 train-loss:  2.1919067944867545 	 ± 0.2188752221914356
	data : 0.1150977611541748
	model : 0.06398439407348633
			 train-loss:  2.191563545703888 	 ± 0.21850417513127932
	data : 0.11518979072570801
	model : 0.06398911476135254
			 train-loss:  2.190879251377516 	 ± 0.21833672113188746
	data : 0.11510510444641113
	model : 0.06399579048156738
			 train-loss:  2.190443889016197 	 ± 0.21801222006783072
	data : 0.11513590812683105
	model : 0.06406259536743164
			 train-loss:  2.1909460960169556 	 ± 0.21772694489575137
	data : 0.11510758399963379
	model : 0.06407136917114258
			 train-loss:  2.1897633873571563 	 ± 0.21811071713574212
	data : 0.11498222351074219
	model : 0.06402459144592285
			 train-loss:  2.190137456445133 	 ± 0.21776425000611888
	data : 0.1148909091949463
	model : 0.06405329704284668
			 train-loss:  2.1928427778184414 	 ± 0.22159041947299568
	data : 0.11475038528442383
	model : 0.055595779418945314
#epoch  44    val-loss:  2.390870972683555  train-loss:  2.1928427778184414  lr:  1.220703125e-06
			 train-loss:  2.6752560138702393 	 ± 0.0
	data : 5.762418746948242
	model : 0.0709848403930664
			 train-loss:  2.387725353240967 	 ± 0.28753066062927246
	data : 2.946718454360962
	model : 0.06997227668762207
			 train-loss:  2.2655394872029624 	 ± 0.29150418908399633
	data : 2.00297482808431
	model : 0.06816474596659343
			 train-loss:  2.379270553588867 	 ± 0.32021131393809943
	data : 1.5308457612991333
	model : 0.06723451614379883
			 train-loss:  2.3593270778656006 	 ± 0.28916984171048915
	data : 1.2475245475769043
	model : 0.0666886329650879
			 train-loss:  2.3135181268056235 	 ± 0.2831518399171513
	data : 0.11794323921203613
	model : 0.06537866592407227
			 train-loss:  2.261417201587132 	 ± 0.29156206294788056
	data : 0.114532470703125
	model : 0.06450796127319336
			 train-loss:  2.2417112439870834 	 ± 0.2776700476035636
	data : 0.11434364318847656
	model : 0.06454486846923828
			 train-loss:  2.223872012562222 	 ± 0.2666079917725785
	data : 0.1143415927886963
	model : 0.06459851264953613
			 train-loss:  2.2057350993156435 	 ± 0.25871290603356417
	data : 0.11441593170166016
	model : 0.06466307640075683
			 train-loss:  2.210111130367626 	 ± 0.24706093078756333
	data : 0.11438450813293458
	model : 0.064739990234375
			 train-loss:  2.219689597686132 	 ± 0.23866656439344608
	data : 0.11442584991455078
	model : 0.06491198539733886
			 train-loss:  2.2180289030075073 	 ± 0.22937557058838054
	data : 0.11407470703125
	model : 0.06491889953613281
			 train-loss:  2.2440898844173978 	 ± 0.24017564790440005
	data : 0.1138638973236084
	model : 0.06494483947753907
			 train-loss:  2.2367226521174115 	 ± 0.2336634002603159
	data : 0.11384415626525879
	model : 0.06498923301696777
			 train-loss:  2.221442826092243 	 ± 0.2338552315702556
	data : 0.11391019821166992
	model : 0.06509594917297364
			 train-loss:  2.2241002741981957 	 ± 0.22712178407228134
	data : 0.11387920379638672
	model : 0.06497025489807129
			 train-loss:  2.2054377926720514 	 ± 0.23375074282916736
	data : 0.11425399780273438
	model : 0.06497302055358886
			 train-loss:  2.2111994090833162 	 ± 0.22882566156386924
	data : 0.11443815231323243
	model : 0.06493601799011231
			 train-loss:  2.198054713010788 	 ± 0.230273754220013
	data : 0.11433296203613282
	model : 0.06490659713745117
			 train-loss:  2.1950477361679077 	 ± 0.2251261698882586
	data : 0.11422605514526367
	model : 0.06481733322143554
			 train-loss:  2.180451913313432 	 ± 0.22989535331190689
	data : 0.11426301002502441
	model : 0.06487560272216797
			 train-loss:  2.182011604309082 	 ± 0.22496107201932303
	data : 0.11413650512695313
	model : 0.0649219036102295
			 train-loss:  2.176391194264094 	 ± 0.22186794714261238
	data : 0.11417098045349121
	model : 0.06498951911926269
			 train-loss:  2.1764041328430177 	 ± 0.21738531355238733
	data : 0.11421098709106445
	model : 0.06496362686157227
			 train-loss:  2.1702944865593543 	 ± 0.21534162654678635
	data : 0.11418046951293945
	model : 0.06489529609680175
			 train-loss:  2.1632262468338013 	 ± 0.21436766168995922
	data : 0.11428990364074706
	model : 0.06485552787780761
			 train-loss:  2.1625410616397858 	 ± 0.2105349711493858
	data : 0.1143381118774414
	model : 0.06482086181640626
			 train-loss:  2.167266397640623 	 ± 0.2083788211500732
	data : 0.11431779861450195
	model : 0.06484928131103515
			 train-loss:  2.168802567323049 	 ± 0.2050433532254435
	data : 0.11430726051330567
	model : 0.06490616798400879
			 train-loss:  2.169962425385752 	 ± 0.20180910836831503
	data : 0.11433701515197754
	model : 0.06494855880737305
			 train-loss:  2.163692496716976 	 ± 0.20167516520232243
	data : 0.11421093940734864
	model : 0.06493101119995118
			 train-loss:  2.1649556810205635 	 ± 0.19872448587305785
	data : 0.11408724784851074
	model : 0.06486554145812988
			 train-loss:  2.1569306149202236 	 ± 0.20113469250716054
	data : 0.11413521766662597
	model : 0.06484808921813964
			 train-loss:  2.1683268206460133 	 ± 0.20908132832548965
	data : 0.11422576904296874
	model : 0.0648303508758545
			 train-loss:  2.1674787865744696 	 ± 0.20621800820930591
	data : 0.11436924934387208
	model : 0.06484761238098144
			 train-loss:  2.1775141342266187 	 ± 0.2121367823963486
	data : 0.11446118354797363
	model : 0.06488423347473145
			 train-loss:  2.1772640567076835 	 ± 0.20933242663705867
	data : 0.11467061042785645
	model : 0.06494903564453125
			 train-loss:  2.17800340285668 	 ± 0.20668150734699506
	data : 0.1146538257598877
	model : 0.06490364074707031
			 train-loss:  2.1750170409679415 	 ± 0.20493201379514933
	data : 0.11464529037475586
	model : 0.06488027572631835
			 train-loss:  2.168788741274578 	 ± 0.20621464232751055
	data : 0.1145413875579834
	model : 0.0648695945739746
			 train-loss:  2.1594033667019437 	 ± 0.21242289568814882
	data : 0.11451802253723145
	model : 0.06483192443847656
			 train-loss:  2.1579133660294287 	 ± 0.2101602899862774
	data : 0.11442122459411622
	model : 0.06484837532043457
			 train-loss:  2.1560075960376044 	 ± 0.20813389532555965
	data : 0.11441826820373535
	model : 0.06485977172851562
			 train-loss:  2.148286708196004 	 ± 0.21208486691328166
	data : 0.11434283256530761
	model : 0.06491374969482422
			 train-loss:  2.1484234281208203 	 ± 0.20976893488418188
	data : 0.11439533233642578
	model : 0.06493978500366211
			 train-loss:  2.1519802123942275 	 ± 0.2089227243079025
	data : 0.11430659294128417
	model : 0.06491546630859375
			 train-loss:  2.1502639849980674 	 ± 0.2070695347735187
	data : 0.11429319381713868
	model : 0.06485724449157715
			 train-loss:  2.149137243932607 	 ± 0.20509430374240356
	data : 0.1141739845275879
	model : 0.06485786437988281
			 train-loss:  2.148493461608887 	 ± 0.20308300836693868
	data : 0.11433491706848145
	model : 0.06481075286865234
			 train-loss:  2.143900156021118 	 ± 0.20368836624570175
	data : 0.11428380012512207
	model : 0.06486973762512208
			 train-loss:  2.151955769612239 	 ± 0.20976325831067186
	data : 0.1143383502960205
	model : 0.06488218307495117
			 train-loss:  2.1551437557868236 	 ± 0.20904285057928002
	data : 0.11440935134887695
	model : 0.06491756439208984
			 train-loss:  2.1534901989830866 	 ± 0.2074477994511984
	data : 0.11441993713378906
	model : 0.06489238739013672
			 train-loss:  2.150778120214289 	 ± 0.20651714810828461
	data : 0.11426329612731934
	model : 0.06487531661987304
			 train-loss:  2.1464720325810567 	 ± 0.20714142205027125
	data : 0.11426091194152832
	model : 0.06481819152832032
			 train-loss:  2.152569289793048 	 ± 0.21032520275481903
	data : 0.11443133354187011
	model : 0.06483535766601563
			 train-loss:  2.1508686172551124 	 ± 0.20889913764783458
	data : 0.11443114280700684
	model : 0.06488680839538574
			 train-loss:  2.151551650742353 	 ± 0.20718655169844505
	data : 0.11457352638244629
	model : 0.06495909690856934
			 train-loss:  2.1535852471987407 	 ± 0.20604568613840496
	data : 0.11452951431274414
	model : 0.06498451232910156
			 train-loss:  2.1598812595742647 	 ± 0.2100886249947256
	data : 0.11454429626464843
	model : 0.06496133804321289
			 train-loss:  2.1601176377265685 	 ± 0.20839565228796564
	data : 0.11442537307739258
	model : 0.0649078369140625
			 train-loss:  2.154255153640868 	 ± 0.2118260213158404
	data : 0.1142967700958252
	model : 0.06485896110534668
			 train-loss:  2.158664410933852 	 ± 0.2130586376669719
	data : 0.11427912712097169
	model : 0.06482853889465331
			 train-loss:  2.1615544961049005 	 ± 0.2126738818494496
	data : 0.11439809799194336
	model : 0.06483888626098633
			 train-loss:  2.1591507503480623 	 ± 0.21194443576359756
	data : 0.11445589065551758
	model : 0.06490988731384277
			 train-loss:  2.1624811247213565 	 ± 0.2120896542363325
	data : 0.11443381309509278
	model : 0.06499457359313965
			 train-loss:  2.1653955070411457 	 ± 0.21187164498199215
	data : 0.11440739631652833
	model : 0.06501827239990235
			 train-loss:  2.1702535238818847 	 ± 0.21411175213085426
	data : 0.11432623863220215
	model : 0.06492676734924316
			 train-loss:  2.169150241783687 	 ± 0.21277433898860287
	data : 0.11428532600402833
	model : 0.06491475105285645
			 train-loss:  2.170206074983301 	 ± 0.21145521379966015
	data : 0.11426253318786621
	model : 0.06487140655517579
			 train-loss:  2.1724564449654684 	 ± 0.21083606191339752
	data : 0.11427264213562012
	model : 0.06482572555541992
			 train-loss:  2.173949285729291 	 ± 0.20976980935031259
	data : 0.11444668769836426
	model : 0.06482791900634766
			 train-loss:  2.171965755320884 	 ± 0.2090357467531801
	data : 0.1145319938659668
	model : 0.06489925384521485
			 train-loss:  2.1687818288803102 	 ± 0.20943614187279738
	data : 0.11459193229675294
	model : 0.06495375633239746
			 train-loss:  2.1673530481363597 	 ± 0.20842133364999407
	data : 0.1146092414855957
	model : 0.0649810791015625
			 train-loss:  2.1638665973366082 	 ± 0.2092823675482925
	data : 0.11446056365966797
	model : 0.06500034332275391
			 train-loss:  2.163259166937608 	 ± 0.20800479115529596
	data : 0.11447558403015137
	model : 0.06498589515686035
			 train-loss:  2.1648532982113995 	 ± 0.20716307633524605
	data : 0.11438913345336914
	model : 0.0650062084197998
			 train-loss:  2.1636130064725876 	 ± 0.20615918851318826
	data : 0.11441960334777831
	model : 0.06499533653259278
			 train-loss:  2.1602233383390637 	 ± 0.20711370733448065
	data : 0.11434702873229981
	model : 0.06503758430480958
			 train-loss:  2.159488388677923 	 ± 0.20595319122474334
	data : 0.11451401710510253
	model : 0.06507978439331055
			 train-loss:  2.159904349281127 	 ± 0.20474340061925506
	data : 0.11443057060241699
	model : 0.06511540412902832
			 train-loss:  2.1630685258479345 	 ± 0.20555245377107967
	data : 0.11454548835754394
	model : 0.06509513854980468
			 train-loss:  2.1620321063434376 	 ± 0.20456040919895715
	data : 0.11447024345397949
	model : 0.06510167121887207
			 train-loss:  2.16304743428563 	 ± 0.2035829497235035
	data : 0.11447052955627442
	model : 0.06500377655029296
			 train-loss:  2.1645841338168617 	 ± 0.20291059785520157
	data : 0.11446623802185059
	model : 0.06497983932495117
			 train-loss:  2.16471566259861 	 ± 0.20175813265785128
	data : 0.11452302932739258
	model : 0.06496686935424804
			 train-loss:  2.1652219925033913 	 ± 0.20067767688117205
	data : 0.11448755264282226
	model : 0.06502685546875
			 train-loss:  2.169855421119266 	 ± 0.20429091844650235
	data : 0.1144857406616211
	model : 0.06500897407531739
			 train-loss:  2.1695195462677503 	 ± 0.20319032576167814
	data : 0.11448535919189454
	model : 0.0650334358215332
			 train-loss:  2.1669676757377125 	 ± 0.2035439514991368
	data : 0.11452713012695312
	model : 0.06502056121826172
			 train-loss:  2.1663679089597476 	 ± 0.20252839077711315
	data : 0.114418363571167
	model : 0.06499738693237304
			 train-loss:  2.167049005944678 	 ± 0.20155528297880781
	data : 0.11439275741577148
	model : 0.06495022773742676
			 train-loss:  2.172566718804209 	 ± 0.20750602408566324
	data : 0.11437296867370605
	model : 0.06489109992980957
			 train-loss:  2.172656212002039 	 ± 0.20642427726350004
	data : 0.11443452835083008
	model : 0.06489391326904297
			 train-loss:  2.174378022705157 	 ± 0.20604926218835679
	data : 0.11441469192504883
	model : 0.06491451263427735
			 train-loss:  2.1761893338086655 	 ± 0.2057700492316777
	data : 0.11453776359558106
	model : 0.06492791175842286
			 train-loss:  2.1775820219155513 	 ± 0.205191866273641
	data : 0.11450843811035157
	model : 0.06494040489196777
			 train-loss:  2.1809520971775056 	 ± 0.2068986425399245
	data : 0.11449594497680664
	model : 0.0649651050567627
			 train-loss:  2.1793821696007605 	 ± 0.20646957011445918
	data : 0.1143467903137207
	model : 0.06496715545654297
			 train-loss:  2.1804406771472857 	 ± 0.20573018634049764
	data : 0.11423978805541993
	model : 0.06490068435668946
			 train-loss:  2.176439953081816 	 ± 0.20867817090065316
	data : 0.11422719955444335
	model : 0.06492762565612793
			 train-loss:  2.181292773439334 	 ± 0.21343265458470392
	data : 0.11435680389404297
	model : 0.06490559577941894
			 train-loss:  2.1821910438083467 	 ± 0.21261131608824607
	data : 0.11438207626342774
	model : 0.06493334770202637
			 train-loss:  2.1818151777645327 	 ± 0.21164110386119628
	data : 0.11450495719909667
	model : 0.06493124961853028
			 train-loss:  2.183030352414211 	 ± 0.21102100668887028
	data : 0.11456565856933594
	model : 0.06499629020690918
			 train-loss:  2.182420012023714 	 ± 0.21013664802622678
	data : 0.11456589698791504
	model : 0.06496233940124511
			 train-loss:  2.182670060647737 	 ± 0.20918663821740513
	data : 0.11429729461669921
	model : 0.064945650100708
			 train-loss:  2.1827686949209735 	 ± 0.20823616520015606
	data : 0.11430749893188477
	model : 0.06490278244018555
			 train-loss:  2.1833067810213245 	 ± 0.20737284820604757
	data : 0.11433982849121094
	model : 0.06488823890686035
			 train-loss:  2.182616986334324 	 ± 0.20657287823624218
	data : 0.11448163986206054
	model : 0.06487822532653809
			 train-loss:  2.185078746449631 	 ± 0.2073004390062682
	data : 0.11444101333618165
	model : 0.06490583419799804
			 train-loss:  2.1895918020030907 	 ± 0.2118916196781185
	data : 0.11463775634765624
	model : 0.06492962837219238
			 train-loss:  2.1897530918535977 	 ± 0.21097536878345513
	data : 0.1147191047668457
	model : 0.06491918563842773
			 train-loss:  2.18798418805517 	 ± 0.2109187793562834
	data : 0.11478919982910156
	model : 0.06495351791381836
			 train-loss:  2.1866620269596067 	 ± 0.2104977048962211
	data : 0.11456642150878907
	model : 0.06496610641479492
			 train-loss:  2.1884925496780268 	 ± 0.21053699563788145
	data : 0.11452665328979492
	model : 0.06490159034729004
			 train-loss:  2.1892060482201456 	 ± 0.20979373692985503
	data : 0.11440801620483398
	model : 0.06491971015930176
			 train-loss:  2.1903602590163547 	 ± 0.20929683639771016
	data : 0.11437358856201171
	model : 0.06499571800231933
			 train-loss:  2.1916603992793187 	 ± 0.2089162112605669
	data : 0.11429772377014161
	model : 0.0650242805480957
			 train-loss:  2.1911262459442264 	 ± 0.2081411854002025
	data : 0.11445722579956055
	model : 0.06505231857299805
			 train-loss:  2.191764667751343 	 ± 0.20741326044521385
	data : 0.11446099281311035
	model : 0.06510443687438965
			 train-loss:  2.1886572732079412 	 ± 0.2094301764963931
	data : 0.11459546089172364
	model : 0.06506538391113281
			 train-loss:  2.193626166343689 	 ± 0.21580467629782107
	data : 0.11448273658752442
	model : 0.06497778892517089
			 train-loss:  2.194798962464408 	 ± 0.2153461708942013
	data : 0.11440491676330566
	model : 0.0649024486541748
			 train-loss:  2.1918206646686462 	 ± 0.2170863339788431
	data : 0.11436581611633301
	model : 0.06486425399780274
			 train-loss:  2.192347737029195 	 ± 0.21631824258069213
	data : 0.11443452835083008
	model : 0.06483263969421386
			 train-loss:  2.1931877376497253 	 ± 0.21568764002680876
	data : 0.11444253921508789
	model : 0.06486034393310547
			 train-loss:  2.1937311594302837 	 ± 0.21494510376188186
	data : 0.11455683708190918
	model : 0.0649235725402832
			 train-loss:  2.1941087755538127 	 ± 0.21416641290902225
	data : 0.11460566520690918
	model : 0.06493644714355469
			 train-loss:  2.1945043964819475 	 ± 0.21340167951234715
	data : 0.11458625793457031
	model : 0.06490602493286132
			 train-loss:  2.1968235091159216 	 ± 0.21426106070003773
	data : 0.1145012378692627
	model : 0.06489171981811523
			 train-loss:  2.1967458333542096 	 ± 0.21346196160602202
	data : 0.11432137489318847
	model : 0.06482901573181152
			 train-loss:  2.1999551755410653 	 ± 0.2158904041735071
	data : 0.11423563957214355
	model : 0.06481943130493165
			 train-loss:  2.2008897381670334 	 ± 0.215369138494871
	data : 0.11418452262878417
	model : 0.0648425579071045
			 train-loss:  2.198080734614908 	 ± 0.21706774422377503
	data : 0.11414098739624023
	model : 0.06490349769592285
			 train-loss:  2.1971386360085527 	 ± 0.21656075863648033
	data : 0.11426057815551757
	model : 0.06494622230529785
			 train-loss:  2.1968563048959635 	 ± 0.21580584457231522
	data : 0.11436667442321777
	model : 0.06500434875488281
			 train-loss:  2.1954743010657176 	 ± 0.21565014527052329
	data : 0.11446156501770019
	model : 0.06502346992492676
			 train-loss:  2.1961163273939848 	 ± 0.215018302147308
	data : 0.11444621086120606
	model : 0.06497373580932617
			 train-loss:  2.202226635435937 	 ± 0.22621150178708108
	data : 0.11439766883850097
	model : 0.06487545967102051
			 train-loss:  2.20170878363656 	 ± 0.22550361435833158
	data : 0.11426324844360351
	model : 0.06486549377441406
			 train-loss:  2.202984905905194 	 ± 0.22523679944197336
	data : 0.11435885429382324
	model : 0.06486697196960449
			 train-loss:  2.203781984592306 	 ± 0.2246624818894087
	data : 0.11434307098388671
	model : 0.06487727165222168
			 train-loss:  2.2021287998108017 	 ± 0.22477502440731728
	data : 0.1144676685333252
	model : 0.06497197151184082
			 train-loss:  2.2016858527449523 	 ± 0.22407310794860683
	data : 0.11462903022766113
	model : 0.0650454044342041
			 train-loss:  2.2004626191951133 	 ± 0.22380675925792934
	data : 0.11460938453674316
	model : 0.06509342193603515
			 train-loss:  2.2029931537256946 	 ± 0.22516888799689805
	data : 0.11457304954528809
	model : 0.06506114006042481
			 train-loss:  2.2038508582115175 	 ± 0.2246611540929576
	data : 0.11438722610473633
	model : 0.06494979858398438
			 train-loss:  2.202309831088742 	 ± 0.2247100197854906
	data : 0.11430907249450684
	model : 0.06490507125854492
			 train-loss:  2.2049559009702584 	 ± 0.22631757837342073
	data : 0.11423277854919434
	model : 0.06486945152282715
			 train-loss:  2.2029227555966844 	 ± 0.22696518836068744
	data : 0.11432051658630371
	model : 0.0648655891418457
			 train-loss:  2.2042498851751353 	 ± 0.22682189093340216
	data : 0.11437058448791504
	model : 0.06490306854248047
			 train-loss:  2.2018083018641317 	 ± 0.22811026106643198
	data : 0.11453480720520019
	model : 0.06508135795593262
			 train-loss:  2.201590903294392 	 ± 0.2273940713529331
	data : 0.11457552909851074
	model : 0.06508302688598633
			 train-loss:  2.2017595525000506 	 ± 0.2266785168489904
	data : 0.11469030380249023
	model : 0.06513013839721679
			 train-loss:  2.2007651555387278 	 ± 0.2263033048045166
	data : 0.1148749828338623
	model : 0.06508708000183105
			 train-loss:  2.198068132940328 	 ± 0.22812358786214795
	data : 0.11477570533752442
	model : 0.06507034301757812
			 train-loss:  2.196541564911604 	 ± 0.22822281700061284
	data : 0.11481728553771972
	model : 0.06492819786071777
			 train-loss:  2.1956572702952792 	 ± 0.2277877457953491
	data : 0.11472969055175782
	model : 0.0649064064025879
			 train-loss:  2.1949421454358986 	 ± 0.22726482657398808
	data : 0.11463236808776855
	model : 0.06488842964172363
			 train-loss:  2.1919899900998074 	 ± 0.22966127305396264
	data : 0.11445355415344238
	model : 0.0649488925933838
			 train-loss:  2.1923929721843907 	 ± 0.22901781386320974
	data : 0.11446037292480468
	model : 0.06500163078308105
			 train-loss:  2.1924032377474236 	 ± 0.22832280363113797
	data : 0.11442217826843262
	model : 0.06507453918457032
			 train-loss:  2.1918399872550047 	 ± 0.22774899582744007
	data : 0.114540433883667
	model : 0.06508383750915528
			 train-loss:  2.191748629787011 	 ± 0.22706913953621127
	data : 0.11455507278442383
	model : 0.06506524085998536
			 train-loss:  2.190233100028265 	 ± 0.22723788792120708
	data : 0.11453428268432617
	model : 0.06501941680908203
			 train-loss:  2.190585816400291 	 ± 0.226610709525609
	data : 0.11451973915100097
	model : 0.06495180130004882
			 train-loss:  2.1892244423136993 	 ± 0.22663529074113206
	data : 0.11448206901550292
	model : 0.0649137020111084
			 train-loss:  2.190488967282033 	 ± 0.22657232249292522
	data : 0.11445960998535157
	model : 0.06491031646728515
			 train-loss:  2.19079087362733 	 ± 0.22594721536076362
	data : 0.11441650390625
	model : 0.06492643356323242
			 train-loss:  2.1915457097092115 	 ± 0.22551063558528206
	data : 0.114436674118042
	model : 0.0649339199066162
			 train-loss:  2.192282210821393 	 ± 0.2250702500133104
	data : 0.11447515487670898
	model : 0.06495194435119629
			 train-loss:  2.1925551346370153 	 ± 0.22445514443840642
	data : 0.11453747749328613
	model : 0.06494712829589844
			 train-loss:  2.191485567526384 	 ± 0.22426336447354775
	data : 0.11451940536499024
	model : 0.06491551399230958
			 train-loss:  2.1908797948373913 	 ± 0.22377331049354407
	data : 0.11457290649414062
	model : 0.06488733291625977
			 train-loss:  2.192880819352825 	 ± 0.22472628383698465
	data : 0.1145331859588623
	model : 0.06485357284545898
			 train-loss:  2.1913420274936954 	 ± 0.22503611226089482
	data : 0.11453289985656738
	model : 0.06488180160522461
			 train-loss:  2.1922468145688376 	 ± 0.224736396765614
	data : 0.11452875137329102
	model : 0.06490535736083984
			 train-loss:  2.1923312047568473 	 ± 0.2241175780445115
	data : 0.11453046798706054
	model : 0.06495819091796876
			 train-loss:  2.1907620515142168 	 ± 0.22449581903835217
	data : 0.11455883979797363
	model : 0.0650357723236084
			 train-loss:  2.1894719359653245 	 ± 0.22455710249554262
	data : 0.11465210914611816
	model : 0.06511626243591309
			 train-loss:  2.18895794969538 	 ± 0.22405397520213155
	data : 0.11455597877502441
	model : 0.06512265205383301
			 train-loss:  2.1894983568707027 	 ± 0.22356781253878524
	data : 0.11452651023864746
	model : 0.06510252952575683
			 train-loss:  2.1886373828816157 	 ± 0.22327332895974952
	data : 0.11451020240783691
	model : 0.06506414413452148
			 train-loss:  2.189256059294716 	 ± 0.22283534302453797
	data : 0.11440858840942383
	model : 0.06504874229431153
			 train-loss:  2.188413599070082 	 ± 0.22254030164810681
	data : 0.11443042755126953
	model : 0.06499590873718261
			 train-loss:  2.188460524750765 	 ± 0.2219517224711093
	data : 0.11463694572448731
	model : 0.06496725082397461
			 train-loss:  2.1877762712930378 	 ± 0.22156665084585095
	data : 0.11462574005126953
	model : 0.0650320053100586
			 train-loss:  2.1871777311045464 	 ± 0.22113982749547578
	data : 0.11463932991027832
	model : 0.06510004997253419
			 train-loss:  2.1879497511933246 	 ± 0.22082110399224728
	data : 0.11467142105102539
	model : 0.06507267951965331
			 train-loss:  2.1886953028990197 	 ± 0.22049043066862975
	data : 0.11466498374938965
	model : 0.06503968238830567
			 train-loss:  2.187959035032803 	 ± 0.22015915916704082
	data : 0.11452622413635254
	model : 0.06504302024841309
			 train-loss:  2.1875364297475572 	 ± 0.21967279861810707
	data : 0.1144944667816162
	model : 0.06494283676147461
			 train-loss:  2.1860694568984362 	 ± 0.22006720674829908
	data : 0.11441421508789062
	model : 0.0649146556854248
			 train-loss:  2.1875889361812377 	 ± 0.22053631960847045
	data : 0.11444034576416015
	model : 0.06489310264587403
			 train-loss:  2.1893295736023872 	 ± 0.2213312106739976
	data : 0.11450762748718261
	model : 0.06492290496826172
			 train-loss:  2.187697623243284 	 ± 0.22196545299542048
	data : 0.11448869705200196
	model : 0.06496095657348633
			 train-loss:  2.189001170992851 	 ± 0.22217215649624028
	data : 0.11452274322509766
	model : 0.06497364044189453
			 train-loss:  2.1877746795540425 	 ± 0.22229653373093702
	data : 0.11460165977478028
	model : 0.06495833396911621
			 train-loss:  2.1893270145548454 	 ± 0.2228350848123949
	data : 0.1145477294921875
	model : 0.06494545936584473
			 train-loss:  2.188615283355337 	 ± 0.2225155998007152
	data : 0.11433548927307129
	model : 0.06497106552124024
			 train-loss:  2.1877188063135335 	 ± 0.22233673933693307
	data : 0.1143162727355957
	model : 0.06490135192871094
			 train-loss:  2.187720530207564 	 ± 0.22179379303278035
	data : 0.1143725872039795
	model : 0.06489729881286621
			 train-loss:  2.1901202514333633 	 ± 0.22390670791919032
	data : 0.1144495964050293
	model : 0.06494932174682617
			 train-loss:  2.1919068230523004 	 ± 0.2248322446630217
	data : 0.11439704895019531
	model : 0.06500825881958008
			 train-loss:  2.1908358570474844 	 ± 0.22481978122050678
	data : 0.11442036628723144
	model : 0.06500358581542968
			 train-loss:  2.1921744956924583 	 ± 0.22511069129398495
	data : 0.1143721103668213
	model : 0.06504063606262207
			 train-loss:  2.1926802504630314 	 ± 0.22469306691772217
	data : 0.11437597274780273
	model : 0.06503782272338868
			 train-loss:  2.1927407234200933 	 ± 0.2241616995085687
	data : 0.11419444084167481
	model : 0.06495556831359864
			 train-loss:  2.193581576054951 	 ± 0.2239656903383411
	data : 0.11426653861999511
	model : 0.06490154266357422
			 train-loss:  2.1936049198320773 	 ± 0.22343958927795066
	data : 0.11434450149536132
	model : 0.0648921012878418
			 train-loss:  2.194472694508383 	 ± 0.22327639933725443
	data : 0.11450634002685547
	model : 0.064898681640625
			 train-loss:  2.194779384413431 	 ± 0.2228017227596583
	data : 0.11434040069580079
	model : 0.06494674682617188
			 train-loss:  2.1935944264685667 	 ± 0.22296339830290596
	data : 0.11445040702819824
	model : 0.06499276161193848
			 train-loss:  2.1926622698383946 	 ± 0.22287052779092564
	data : 0.11449384689331055
	model : 0.06500453948974609
			 train-loss:  2.1936764728038685 	 ± 0.22286011340456446
	data : 0.11456990242004395
	model : 0.06493911743164063
			 train-loss:  2.1939794113646904 	 ± 0.222395701683736
	data : 0.11439895629882812
	model : 0.06484904289245605
			 train-loss:  2.1938655387271533 	 ± 0.22189608019071233
	data : 0.11456608772277832
	model : 0.06476154327392578
			 train-loss:  2.1946632311894345 	 ± 0.22170941235879754
	data : 0.11453428268432617
	model : 0.06466798782348633
			 train-loss:  2.194570686366107 	 ± 0.2212137814144856
	data : 0.11456098556518554
	model : 0.06463303565979003
			 train-loss:  2.1957528120733696 	 ± 0.22141888519508143
	data : 0.11463932991027832
	model : 0.06463379859924316
			 train-loss:  2.1947907959776267 	 ± 0.22139068663079253
	data : 0.11476292610168456
	model : 0.06462922096252441
			 train-loss:  2.1968040100733437 	 ± 0.2229436605918692
	data : 0.11475229263305664
	model : 0.06455154418945312
			 train-loss:  2.196221358480707 	 ± 0.2226214966973524
	data : 0.11484622955322266
	model : 0.0644266128540039
			 train-loss:  2.1949264866665072 	 ± 0.22298191847049953
	data : 0.1148031234741211
	model : 0.06432428359985351
			 train-loss:  2.194313192576693 	 ± 0.2226841779576036
	data : 0.11475024223327637
	model : 0.06417241096496581
			 train-loss:  2.1959907665002816 	 ± 0.22363664847172532
	data : 0.11488785743713378
	model : 0.06402864456176757
			 train-loss:  2.1949997746426124 	 ± 0.22365328981010718
	data : 0.11503787040710449
	model : 0.06402549743652344
			 train-loss:  2.195771461957461 	 ± 0.22347532103270012
	data : 0.1150421142578125
	model : 0.06402425765991211
			 train-loss:  2.1961182643627297 	 ± 0.22305545957453884
	data : 0.11510934829711914
	model : 0.06395049095153808
			 train-loss:  2.197730213787423 	 ± 0.2239263876131399
	data : 0.11520113945007324
	model : 0.06392440795898438
			 train-loss:  2.1971014901104136 	 ± 0.22365340150700985
	data : 0.11500730514526367
	model : 0.06394987106323242
			 train-loss:  2.196616945875452 	 ± 0.22330008642938554
	data : 0.1149482250213623
	model : 0.06390209197998047
			 train-loss:  2.1969265038684265 	 ± 0.22287701574881913
	data : 0.11509017944335938
	model : 0.06386418342590332
			 train-loss:  2.1979143207083274 	 ± 0.2229234242908257
	data : 0.11514034271240234
	model : 0.06390738487243652
			 train-loss:  2.1987998365354136 	 ± 0.22287191829869737
	data : 0.11508655548095703
	model : 0.06395454406738281
			 train-loss:  2.198202574103447 	 ± 0.2225959561995466
	data : 0.11525635719299317
	model : 0.06394462585449219
			 train-loss:  2.1976799756288528 	 ± 0.2222786062763624
	data : 0.11527452468872071
	model : 0.06397457122802734
			 train-loss:  2.1970962013941087 	 ± 0.22200125549994992
	data : 0.11514487266540527
	model : 0.06400537490844727
			 train-loss:  2.1963072286164467 	 ± 0.22188041701800615
	data : 0.11503491401672364
	model : 0.06396098136901855
			 train-loss:  2.1959943467206915 	 ± 0.2214768919983507
	data : 0.11515436172485352
	model : 0.06396450996398925
			 train-loss:  2.195853235291653 	 ± 0.22103352589158018
	data : 0.11519560813903809
	model : 0.06401042938232422
			 train-loss:  2.1951459300761322 	 ± 0.2208584989141037
	data : 0.11516270637512208
	model : 0.06401576995849609
			 train-loss:  2.1956061716002178 	 ± 0.220526838517223
	data : 0.11514530181884766
	model : 0.06401505470275878
			 train-loss:  2.1955432225818092 	 ± 0.22008218978901467
	data : 0.11535062789916992
	model : 0.06403741836547852
			 train-loss:  2.194900256010794 	 ± 0.21987035818636363
	data : 0.11515116691589355
	model : 0.06398549079895019
			 train-loss:  2.193825182665783 	 ± 0.22008057621310306
	data : 0.11507954597473144
	model : 0.06397252082824707
			 train-loss:  2.194034782409668 	 ± 0.21966487490559988
	data : 0.11524419784545899
	model : 0.06391606330871583
			 train-loss:  2.194046383359993 	 ± 0.21922693550379585
	data : 0.11543064117431641
	model : 0.06393666267395019
			 train-loss:  2.1943447164126804 	 ± 0.2188425754672143
	data : 0.11537327766418456
	model : 0.06394824981689454
			 train-loss:  2.194970791518924 	 ± 0.21863566167549975
	data : 0.11541557312011719
	model : 0.06394991874694825
			 train-loss:  2.1942795346102377 	 ± 0.21848169260666286
	data : 0.11547651290893554
	model : 0.06388931274414063
			 train-loss:  2.194405716540767 	 ± 0.2180621494538129
	data : 0.11541733741760254
	model : 0.06392107009887696
			 train-loss:  2.1916746902279556 	 ± 0.2219623239199919
	data : 0.11493849754333496
	model : 0.05547432899475098
#epoch  45    val-loss:  2.4030406412325407  train-loss:  2.1916746902279556  lr:  1.220703125e-06
			 train-loss:  1.9887758493423462 	 ± 0.0
	data : 5.201900005340576
	model : 0.08296918869018555
			 train-loss:  2.1598721146583557 	 ± 0.17109626531600952
	data : 2.7130473852157593
	model : 0.0856088399887085
			 train-loss:  2.266650160153707 	 ± 0.20571596114466206
	data : 1.845666488011678
	model : 0.07863394419352214
			 train-loss:  2.2330335080623627 	 ± 0.1874287341784752
	data : 1.412856936454773
	model : 0.0751577615737915
			 train-loss:  2.208541464805603 	 ± 0.17465126695263436
	data : 1.1531492233276368
	model : 0.07305321693420411
			 train-loss:  2.1470937927563987 	 ± 0.2104716201172973
	data : 0.13565559387207032
	model : 0.06947951316833496
			 train-loss:  2.221752796854292 	 ± 0.26723355936337967
	data : 0.11365447044372559
	model : 0.06474800109863281
			 train-loss:  2.2208318561315536 	 ± 0.2499859801398309
	data : 0.11420555114746093
	model : 0.06467704772949219
			 train-loss:  2.2079070011774697 	 ± 0.2385073157913216
	data : 0.1140432357788086
	model : 0.0646742343902588
			 train-loss:  2.2215375542640685 	 ± 0.2299332370806847
	data : 0.11407179832458496
	model : 0.06471195220947265
			 train-loss:  2.200286182490262 	 ± 0.2293015528222335
	data : 0.11401576995849609
	model : 0.06467761993408203
			 train-loss:  2.1886933147907257 	 ± 0.22288100490161128
	data : 0.11409540176391601
	model : 0.06477813720703125
			 train-loss:  2.211805536196782 	 ± 0.22861498934129162
	data : 0.11422605514526367
	model : 0.06492462158203124
			 train-loss:  2.211552049432482 	 ± 0.22030081189104764
	data : 0.11435017585754395
	model : 0.0649406909942627
			 train-loss:  2.199739193916321 	 ± 0.21737194156890988
	data : 0.11432056427001953
	model : 0.0649482250213623
			 train-loss:  2.202164150774479 	 ± 0.21067891958275428
	data : 0.11436347961425782
	model : 0.0649336814880371
			 train-loss:  2.2110552437165203 	 ± 0.20745966747247097
	data : 0.11424612998962402
	model : 0.06489839553833007
			 train-loss:  2.1929178569051953 	 ± 0.21503681346038678
	data : 0.1143373966217041
	model : 0.06485857963562011
			 train-loss:  2.1835948354319523 	 ± 0.2130061978378142
	data : 0.11435523033142089
	model : 0.06490478515625
			 train-loss:  2.1790914475917815 	 ± 0.20853869626569757
	data : 0.1143707275390625
	model : 0.06497788429260254
			 train-loss:  2.20332666238149 	 ± 0.2305741202885202
	data : 0.11438374519348145
	model : 0.06498913764953614
			 train-loss:  2.197365094314922 	 ± 0.22692334905552047
	data : 0.11442241668701172
	model : 0.0649336338043213
			 train-loss:  2.202967317208 	 ± 0.22348556024792293
	data : 0.11435451507568359
	model : 0.06489558219909668
			 train-loss:  2.202115053931872 	 ± 0.218818250913709
	data : 0.11434407234191894
	model : 0.06486067771911622
			 train-loss:  2.195120816230774 	 ± 0.21711801979588102
	data : 0.11432623863220215
	model : 0.06484670639038086
			 train-loss:  2.1915829410919776 	 ± 0.2136353495783112
	data : 0.11434621810913086
	model : 0.06481895446777344
			 train-loss:  2.1951088772879706 	 ± 0.21041132996695486
	data : 0.1143610954284668
	model : 0.06490488052368164
			 train-loss:  2.195805332490376 	 ± 0.20665151374305646
	data : 0.11443676948547363
	model : 0.06497383117675781
			 train-loss:  2.1965996602485918 	 ± 0.20310079750376828
	data : 0.11441688537597657
	model : 0.06499257087707519
			 train-loss:  2.19417644739151 	 ± 0.2001130255740879
	data : 0.11449437141418457
	model : 0.06491084098815918
			 train-loss:  2.1867684587355583 	 ± 0.2009969912585201
	data : 0.11432738304138183
	model : 0.06490607261657715
			 train-loss:  2.190237309783697 	 ± 0.19877202468513763
	data : 0.11427621841430664
	model : 0.06484732627868653
			 train-loss:  2.1971177729693325 	 ± 0.1995693869226827
	data : 0.1140355110168457
	model : 0.06483263969421386
			 train-loss:  2.2035007021006416 	 ± 0.2000025226657294
	data : 0.11408762931823731
	model : 0.06478195190429688
			 train-loss:  2.2094681092670987 	 ± 0.2001720793243172
	data : 0.11409478187561035
	model : 0.06485247611999512
			 train-loss:  2.199443611833784 	 ± 0.20608980232546592
	data : 0.11428723335266114
	model : 0.06493182182312011
			 train-loss:  2.2004821687131315 	 ± 0.20338121137101597
	data : 0.11436305046081544
	model : 0.06495890617370606
			 train-loss:  2.198778597932113 	 ± 0.20095465329156134
	data : 0.11446146965026856
	model : 0.06491870880126953
			 train-loss:  2.213667808434902 	 ± 0.21856688533916133
	data : 0.11430573463439941
	model : 0.06486902236938477
			 train-loss:  2.2125587582588198 	 ± 0.2159286131621287
	data : 0.11422085762023926
	model : 0.06487789154052734
			 train-loss:  2.210577173930843 	 ± 0.213646984228142
	data : 0.11417179107666016
	model : 0.0648491382598877
			 train-loss:  2.2114231018793014 	 ± 0.21115772945105335
	data : 0.11421689987182618
	model : 0.0648885726928711
			 train-loss:  2.2091747993646664 	 ± 0.2091960094794357
	data : 0.11433687210083007
	model : 0.06492161750793457
			 train-loss:  2.207531300458041 	 ± 0.20708574125492454
	data : 0.11451840400695801
	model : 0.06503658294677735
			 train-loss:  2.207792705959744 	 ± 0.20477920278433132
	data : 0.1144495964050293
	model : 0.06496562957763671
			 train-loss:  2.215934800065082 	 ± 0.209776352792069
	data : 0.11441454887390137
	model : 0.06490435600280761
			 train-loss:  2.224387934867372 	 ± 0.2153062243667075
	data : 0.11439118385314942
	model : 0.0648582935333252
			 train-loss:  2.2272741993268332 	 ± 0.2139685462002716
	data : 0.11429104804992676
	model : 0.06486954689025878
			 train-loss:  2.2336096471669724 	 ± 0.21627487233926881
	data : 0.1142197608947754
	model : 0.06487069129943848
			 train-loss:  2.22807288646698 	 ± 0.2175809142150562
	data : 0.11434521675109863
	model : 0.06494107246398925
			 train-loss:  2.2217359706467272 	 ± 0.22004775737976467
	data : 0.11446256637573242
	model : 0.0649993896484375
			 train-loss:  2.221565753221512 	 ± 0.21792503257976611
	data : 0.11452751159667969
	model : 0.06498641967773437
			 train-loss:  2.215254560956415 	 ± 0.2206047979366076
	data : 0.11452479362487793
	model : 0.0649538516998291
			 train-loss:  2.215697564460613 	 ± 0.21857641039463752
	data : 0.114585542678833
	model : 0.06491661071777344
			 train-loss:  2.2086487206545744 	 ± 0.222688243507702
	data : 0.1145665168762207
	model : 0.06485371589660645
			 train-loss:  2.204263169850622 	 ± 0.22307473006501627
	data : 0.11452679634094239
	model : 0.06485219001770019
			 train-loss:  2.2080521813610146 	 ± 0.22291990003088724
	data : 0.11450881958007812
	model : 0.06491107940673828
			 train-loss:  2.2081407000278603 	 ± 0.2209908317898671
	data : 0.11453084945678711
	model : 0.06491971015930176
			 train-loss:  2.2099617760060197 	 ± 0.21954851237475323
	data : 0.11444339752197266
	model : 0.0649306297302246
			 train-loss:  2.207921105623245 	 ± 0.21827479454178372
	data : 0.11442222595214843
	model : 0.06496329307556152
			 train-loss:  2.2075861340663474 	 ± 0.2164938127950285
	data : 0.11419453620910644
	model : 0.06492972373962402
			 train-loss:  2.2124261067759607 	 ± 0.21804255465828792
	data : 0.11409955024719239
	model : 0.06487174034118652
			 train-loss:  2.2086353358768283 	 ± 0.21835486811648291
	data : 0.1142435073852539
	model : 0.06487970352172852
			 train-loss:  2.2097195219248533 	 ± 0.21681310007656232
	data : 0.11442327499389648
	model : 0.06492152214050292
			 train-loss:  2.215281422321613 	 ± 0.21969193133901996
	data : 0.11442050933837891
	model : 0.0649235725402832
			 train-loss:  2.2135820117863743 	 ± 0.21845132974213624
	data : 0.11469807624816894
	model : 0.0649505615234375
			 train-loss:  2.20781115631559 	 ± 0.22182586186152328
	data : 0.11467370986938477
	model : 0.06494536399841308
			 train-loss:  2.2073292276438545 	 ± 0.220224081244045
	data : 0.1144526481628418
	model : 0.06491165161132813
			 train-loss:  2.2030444939931235 	 ± 0.22145919515783138
	data : 0.11426405906677246
	model : 0.06484785079956054
			 train-loss:  2.196867707797459 	 ± 0.22577882872092245
	data : 0.11438455581665039
	model : 0.06485824584960938
			 train-loss:  2.1936276513086237 	 ± 0.22581621768922247
	data : 0.11430921554565429
	model : 0.06490068435668946
			 train-loss:  2.1958304196596146 	 ± 0.22500940854762208
	data : 0.11438674926757812
	model : 0.06495704650878906
			 train-loss:  2.1951780335543907 	 ± 0.22353148891048366
	data : 0.1145932674407959
	model : 0.06501541137695313
			 train-loss:  2.196872854554975 	 ± 0.22248773547332054
	data : 0.11474819183349609
	model : 0.06501827239990235
			 train-loss:  2.196725519498189 	 ± 0.22100314077866676
	data : 0.11459145545959473
	model : 0.06497282981872558
			 train-loss:  2.197480551506344 	 ± 0.21964170990719434
	data : 0.11454863548278808
	model : 0.06488494873046875
			 train-loss:  2.1942578229037197 	 ± 0.22001202117222218
	data : 0.11447229385375976
	model : 0.06489744186401367
			 train-loss:  2.1918443013460207 	 ± 0.21962067405417438
	data : 0.11444578170776368
	model : 0.06492052078247071
			 train-loss:  2.1917954638034485 	 ± 0.2182266692283438
	data : 0.11441235542297364
	model : 0.06497278213500976
			 train-loss:  2.186947685480118 	 ± 0.22109764492654785
	data : 0.11443758010864258
	model : 0.06505808830261231
			 train-loss:  2.195947967929605 	 ± 0.23401082682535446
	data : 0.11452088356018067
	model : 0.065116548538208
			 train-loss:  2.1993966655033392 	 ± 0.2346414794782286
	data : 0.11463327407836914
	model : 0.06510372161865234
			 train-loss:  2.2012650736843247 	 ± 0.23383658589383052
	data : 0.11447906494140625
	model : 0.06506924629211426
			 train-loss:  2.2007451937312172 	 ± 0.2324887840961093
	data : 0.11430630683898926
	model : 0.06501884460449218
			 train-loss:  2.2004293638117174 	 ± 0.23113528307344725
	data : 0.11429839134216309
	model : 0.06496038436889648
			 train-loss:  2.2011181615119755 	 ± 0.22987527697493515
	data : 0.11434741020202636
	model : 0.0649794101715088
			 train-loss:  2.199895973863273 	 ± 0.22883120008282795
	data : 0.1142819881439209
	model : 0.06500077247619629
			 train-loss:  2.197836694392291 	 ± 0.22833661742830583
	data : 0.11438074111938476
	model : 0.06506814956665039
			 train-loss:  2.1972694584492887 	 ± 0.22711254828472077
	data : 0.11447515487670898
	model : 0.06511473655700684
			 train-loss:  2.195861805809869 	 ± 0.2262373742542565
	data : 0.11452603340148926
	model : 0.06511459350585938
			 train-loss:  2.194235390359229 	 ± 0.22551932415842751
	data : 0.11429176330566407
	model : 0.06506075859069824
			 train-loss:  2.1939176968906238 	 ± 0.2243108006259918
	data : 0.1143418788909912
	model : 0.06497206687927246
			 train-loss:  2.190921019482356 	 ± 0.22494549809225184
	data : 0.11433877944946289
	model : 0.06489596366882325
			 train-loss:  2.188728340128635 	 ± 0.22474274969297123
	data : 0.11450810432434082
	model : 0.06487107276916504
			 train-loss:  2.1874825854050486 	 ± 0.22388279459203533
	data : 0.11448264122009277
	model : 0.064902925491333
			 train-loss:  2.1903022403518357 	 ± 0.22440293891780463
	data : 0.1147031307220459
	model : 0.06493625640869141
			 train-loss:  2.1867411345550694 	 ± 0.2259534454837429
	data : 0.11473479270935058
	model : 0.06498818397521973
			 train-loss:  2.188392616048151 	 ± 0.22538533113326853
	data : 0.11480040550231933
	model : 0.06498632431030274
			 train-loss:  2.185964598800197 	 ± 0.22552863917663116
	data : 0.1145167350769043
	model : 0.06495332717895508
			 train-loss:  2.183581008911133 	 ± 0.2256479647440677
	data : 0.11447968482971191
	model : 0.06492576599121094
			 train-loss:  2.1829209280486155 	 ± 0.22462512307228424
	data : 0.11445422172546386
	model : 0.0649376392364502
			 train-loss:  2.1828939353718475 	 ± 0.22352147204668876
	data : 0.11449322700500489
	model : 0.06492075920104981
			 train-loss:  2.180672949957616 	 ± 0.22356190451282548
	data : 0.11444535255432128
	model : 0.06494803428649902
			 train-loss:  2.184167111149201 	 ± 0.22529290418612435
	data : 0.11463885307312012
	model : 0.06496381759643555
			 train-loss:  2.1810774076552617 	 ± 0.22642063849727462
	data : 0.11461067199707031
	model : 0.06497416496276856
			 train-loss:  2.179398815586882 	 ± 0.22600556770910124
	data : 0.11453986167907715
	model : 0.06493477821350098
			 train-loss:  2.1798166716210194 	 ± 0.22498812259690662
	data : 0.11450757980346679
	model : 0.06493802070617676
			 train-loss:  2.1811026710051076 	 ± 0.2243388303438632
	data : 0.11452174186706543
	model : 0.06489276885986328
			 train-loss:  2.1839058136721268 	 ± 0.22519948623258063
	data : 0.11452159881591797
	model : 0.06491560935974121
			 train-loss:  2.1838883421637796 	 ± 0.22417358933987053
	data : 0.11465530395507813
	model : 0.06494483947753907
			 train-loss:  2.1821597095008367 	 ± 0.22389676200060604
	data : 0.11472387313842773
	model : 0.06501250267028809
			 train-loss:  2.180531575211457 	 ± 0.22355405174481732
	data : 0.11464099884033203
	model : 0.06504716873168945
			 train-loss:  2.1818955060655036 	 ± 0.22303026549458158
	data : 0.11453375816345215
	model : 0.06505331993103028
			 train-loss:  2.1832038467390493 	 ± 0.22248503312387694
	data : 0.11456208229064942
	model : 0.06504349708557129
			 train-loss:  2.186533340163853 	 ± 0.22434996938746252
	data : 0.1144017219543457
	model : 0.06494708061218261
			 train-loss:  2.1841003596782684 	 ± 0.22489938793431488
	data : 0.11432528495788574
	model : 0.06487183570861817
			 train-loss:  2.1853826341466007 	 ± 0.22436167162576784
	data : 0.1143315315246582
	model : 0.06486105918884277
			 train-loss:  2.1842592621253707 	 ± 0.22373916731475627
	data : 0.11446571350097656
	model : 0.06489033699035644
			 train-loss:  2.188181800000808 	 ± 0.22683503503173788
	data : 0.11446475982666016
	model : 0.06491913795471191
			 train-loss:  2.188035534818967 	 ± 0.22589354683591087
	data : 0.1144784927368164
	model : 0.06498188972473144
			 train-loss:  2.189100128560027 	 ± 0.22526024848105622
	data : 0.11445198059082032
	model : 0.06500792503356934
			 train-loss:  2.1894833914569167 	 ± 0.22437476191741773
	data : 0.11440181732177734
	model : 0.06497716903686523
			 train-loss:  2.190572724109743 	 ± 0.22378450228845048
	data : 0.1142496109008789
	model : 0.06496801376342773
			 train-loss:  2.1902238890047996 	 ± 0.22291389339947024
	data : 0.11412005424499512
	model : 0.06493353843688965
			 train-loss:  2.1873389892578126 	 ± 0.22433253712238654
	data : 0.11421422958374024
	model : 0.06493601799011231
			 train-loss:  2.1874942155111405 	 ± 0.2234472950304402
	data : 0.1143198013305664
	model : 0.06496362686157227
			 train-loss:  2.1872193550500345 	 ± 0.22258722653984273
	data : 0.11446828842163086
	model : 0.06504197120666504
			 train-loss:  2.1840410381555557 	 ± 0.22459055682373474
	data : 0.11439824104309082
	model : 0.06503920555114746
			 train-loss:  2.183281412420347 	 ± 0.2238833699618798
	data : 0.1144601821899414
	model : 0.06509671211242676
			 train-loss:  2.186691997601436 	 ± 0.22635974843140919
	data : 0.11435661315917969
	model : 0.0650515079498291
			 train-loss:  2.186009255984357 	 ± 0.22562845125974684
	data : 0.11428208351135254
	model : 0.06503596305847167
			 train-loss:  2.184096522403486 	 ± 0.22583578075341035
	data : 0.11421008110046386
	model : 0.0649658203125
			 train-loss:  2.1842396420643744 	 ± 0.2249911808857581
	data : 0.11431713104248047
	model : 0.06498408317565918
			 train-loss:  2.1830998712511205 	 ± 0.22453516404596927
	data : 0.11432886123657227
	model : 0.06496162414550781
			 train-loss:  2.1826433994151926 	 ± 0.22376440481397172
	data : 0.11445307731628418
	model : 0.0649998664855957
			 train-loss:  2.184430742965025 	 ± 0.22390536828246607
	data : 0.11439995765686035
	model : 0.06505136489868164
			 train-loss:  2.184725984169619 	 ± 0.223113267019133
	data : 0.11419215202331542
	model : 0.0650475025177002
			 train-loss:  2.18451528272767 	 ± 0.22231709519743087
	data : 0.11418886184692383
	model : 0.06504554748535156
			 train-loss:  2.185140074585839 	 ± 0.2216375112981938
	data : 0.11413106918334961
	model : 0.06505770683288574
			 train-loss:  2.185799818379538 	 ± 0.22098146508675498
	data : 0.11413230895996093
	model : 0.06504349708557129
			 train-loss:  2.186050658530377 	 ± 0.22021644999922377
	data : 0.11422872543334961
	model : 0.06501750946044922
			 train-loss:  2.1857617354728807 	 ± 0.2194664870793271
	data : 0.11439614295959473
	model : 0.0650439739227295
			 train-loss:  2.1882597149668874 	 ± 0.22071425649623647
	data : 0.11440873146057129
	model : 0.06501789093017578
			 train-loss:  2.1878999289539127 	 ± 0.21998862854888673
	data : 0.11447858810424805
	model : 0.06496734619140625
			 train-loss:  2.1862589745685974 	 ± 0.22011131592761662
	data : 0.11425690650939942
	model : 0.06490707397460938
			 train-loss:  2.189102771347516 	 ± 0.22201304040508862
	data : 0.11419115066528321
	model : 0.06490135192871094
			 train-loss:  2.18660805987663 	 ± 0.22330053486823534
	data : 0.11415071487426758
	model : 0.06489000320434571
			 train-loss:  2.1851537042372935 	 ± 0.22324234127376372
	data : 0.11424708366394043
	model : 0.06492691040039063
			 train-loss:  2.1862483688648915 	 ± 0.22289013545636424
	data : 0.11423101425170898
	model : 0.06495599746704102
			 train-loss:  2.1849738995234174 	 ± 0.222689983932333
	data : 0.11442837715148926
	model : 0.06502742767333984
			 train-loss:  2.1844647704370765 	 ± 0.22203894866001783
	data : 0.11439414024353027
	model : 0.06497001647949219
			 train-loss:  2.185641851864363 	 ± 0.22177952502999598
	data : 0.11426115036010742
	model : 0.06492376327514648
			 train-loss:  2.1860381575191723 	 ± 0.22110755846336533
	data : 0.11395273208618165
	model : 0.06487069129943848
			 train-loss:  2.1838697102162743 	 ± 0.2220146961893141
	data : 0.11398797035217285
	model : 0.06486868858337402
			 train-loss:  2.183025357031053 	 ± 0.22154528545934066
	data : 0.11403584480285645
	model : 0.06488475799560547
			 train-loss:  2.181839100825481 	 ± 0.2213273590886876
	data : 0.11408319473266601
	model : 0.06490635871887207
			 train-loss:  2.183158677095061 	 ± 0.22123613704780415
	data : 0.11420421600341797
	model : 0.06492700576782226
			 train-loss:  2.183578987664814 	 ± 0.22059778538534358
	data : 0.11436691284179687
	model : 0.0649604320526123
			 train-loss:  2.1817694362604394 	 ± 0.2210762111985455
	data : 0.11420278549194336
	model : 0.06495523452758789
			 train-loss:  2.1821094669401644 	 ± 0.22042596955760127
	data : 0.11412477493286133
	model : 0.06492242813110352
			 train-loss:  2.181826516708232 	 ± 0.21976949607115942
	data : 0.11405363082885742
	model : 0.06489424705505371
			 train-loss:  2.180100114257247 	 ± 0.22018253027071827
	data : 0.11409664154052734
	model : 0.06492905616760254
			 train-loss:  2.181522062219725 	 ± 0.22025093604846194
	data : 0.11415314674377441
	model : 0.06496458053588867
			 train-loss:  2.185109150118944 	 ± 0.22430343908700234
	data : 0.11437087059020996
	model : 0.0649289608001709
			 train-loss:  2.185647556998513 	 ± 0.22372897000204817
	data : 0.11441245079040527
	model : 0.06490049362182618
			 train-loss:  2.186580906431359 	 ± 0.22337604227378793
	data : 0.11451783180236816
	model : 0.06490788459777833
			 train-loss:  2.185606195541199 	 ± 0.22306004319575107
	data : 0.11452598571777343
	model : 0.0648798942565918
			 train-loss:  2.1844484288068045 	 ± 0.22289788754661505
	data : 0.1145634651184082
	model : 0.06485457420349121
			 train-loss:  2.1828439856422017 	 ± 0.2232083224597059
	data : 0.11445226669311523
	model : 0.06491975784301758
			 train-loss:  2.182034592067494 	 ± 0.2227994610081313
	data : 0.11458516120910645
	model : 0.06495375633239746
			 train-loss:  2.182419010073121 	 ± 0.22220358208762786
	data : 0.11463828086853027
	model : 0.06499056816101074
			 train-loss:  2.182831671348838 	 ± 0.22162240572018982
	data : 0.11468572616577148
	model : 0.06500287055969238
			 train-loss:  2.1823675480881177 	 ± 0.22106476642657072
	data : 0.11467700004577637
	model : 0.06500144004821777
			 train-loss:  2.1820108602786887 	 ± 0.22047852761362743
	data : 0.11471695899963379
	model : 0.06500229835510254
			 train-loss:  2.181354537691389 	 ± 0.22001808419166022
	data : 0.11454930305480956
	model : 0.06500406265258789
			 train-loss:  2.1809522116726097 	 ± 0.21945668991505785
	data : 0.11435408592224121
	model : 0.0649521827697754
			 train-loss:  2.181222018548998 	 ± 0.21886514890797676
	data : 0.11423859596252442
	model : 0.06503973007202149
			 train-loss:  2.181983066408822 	 ± 0.21848422968094583
	data : 0.1142737865447998
	model : 0.06505956649780273
			 train-loss:  2.1817415280049075 	 ± 0.21789691438995565
	data : 0.1142791748046875
	model : 0.06503758430480958
			 train-loss:  2.182704896397061 	 ± 0.21767273323873398
	data : 0.11427583694458007
	model : 0.06503834724426269
			 train-loss:  2.183053889985901 	 ± 0.217121086897794
	data : 0.11443986892700195
	model : 0.06506009101867676
			 train-loss:  2.1836976467908085 	 ± 0.2166969250158573
	data : 0.11454367637634277
	model : 0.06494112014770508
			 train-loss:  2.1879691556503214 	 ± 0.22365529737077755
	data : 0.11458301544189453
	model : 0.06486568450927735
			 train-loss:  2.1880847565505817 	 ± 0.22305219244161958
	data : 0.11444201469421386
	model : 0.06481375694274902
			 train-loss:  2.1878463371379957 	 ± 0.22247203997127277
	data : 0.11442046165466309
	model : 0.06479558944702149
			 train-loss:  2.1883527630118915 	 ± 0.22198008750707582
	data : 0.11442608833312988
	model : 0.06481046676635742
			 train-loss:  2.1879983024801164 	 ± 0.22143853591054763
	data : 0.11437287330627441
	model : 0.06484446525573731
			 train-loss:  2.1876645557423857 	 ± 0.22089597083613247
	data : 0.11428618431091309
	model : 0.06492381095886231
			 train-loss:  2.1866973178096547 	 ± 0.2207096246727244
	data : 0.114436674118042
	model : 0.06496057510375977
			 train-loss:  2.1871342784480046 	 ± 0.22020999575102834
	data : 0.11448850631713867
	model : 0.06493191719055176
			 train-loss:  2.1867492860524442 	 ± 0.21969687465701945
	data : 0.11452555656433105
	model : 0.06493000984191895
			 train-loss:  2.185396295040846 	 ± 0.21992037026569758
	data : 0.11445937156677247
	model : 0.0649336814880371
			 train-loss:  2.184340072419359 	 ± 0.21983759905344702
	data : 0.11441211700439453
	model : 0.06487116813659669
			 train-loss:  2.182678660781113 	 ± 0.22048172106438807
	data : 0.11447381973266602
	model : 0.06485128402709961
			 train-loss:  2.1814149679281774 	 ± 0.2206188986748084
	data : 0.11450591087341308
	model : 0.06490864753723144
			 train-loss:  2.1811151997167237 	 ± 0.22009518679325238
	data : 0.11444621086120606
	model : 0.0650334358215332
			 train-loss:  2.1812033259929136 	 ± 0.21953932565164905
	data : 0.11445765495300293
	model : 0.06502103805541992
			 train-loss:  2.18324271898077 	 ± 0.22084709890010976
	data : 0.11454763412475585
	model : 0.06503801345825196
			 train-loss:  2.1828493646640874 	 ± 0.22036103215460195
	data : 0.11441659927368164
	model : 0.06507129669189453
			 train-loss:  2.1825008195638658 	 ± 0.2198644237330613
	data : 0.1141425609588623
	model : 0.06502861976623535
			 train-loss:  2.1824891561299413 	 ± 0.21931687737866723
	data : 0.11404657363891602
	model : 0.06490731239318848
			 train-loss:  2.183972603613787 	 ± 0.21978193357581485
	data : 0.1142209529876709
	model : 0.06490387916564941
			 train-loss:  2.1827655249628526 	 ± 0.21991013836267084
	data : 0.1141885757446289
	model : 0.0649411678314209
			 train-loss:  2.1830370800167906 	 ± 0.2194045976917255
	data : 0.11418156623840332
	model : 0.06493854522705078
			 train-loss:  2.183594682740002 	 ± 0.2190136616223767
	data : 0.11442060470581054
	model : 0.06494827270507812
			 train-loss:  2.1852501931699733 	 ± 0.2197634661883673
	data : 0.1145200252532959
	model : 0.06494359970092774
			 train-loss:  2.184707968707246 	 ± 0.21937008153053042
	data : 0.11446785926818848
	model : 0.06499238014221191
			 train-loss:  2.183310867502139 	 ± 0.21976331034338423
	data : 0.11426405906677246
	model : 0.06496667861938477
			 train-loss:  2.182393451056412 	 ± 0.21963582463281708
	data : 0.11424055099487304
	model : 0.06496319770812989
			 train-loss:  2.183362817196619 	 ± 0.21955995275414553
	data : 0.11420168876647949
	model : 0.06500492095947266
			 train-loss:  2.181962014939548 	 ± 0.21997767510772223
	data : 0.11417627334594727
	model : 0.06504621505737304
			 train-loss:  2.1821950713418565 	 ± 0.21948435597286187
	data : 0.11420087814331055
	model : 0.06503958702087402
			 train-loss:  2.1813974867404347 	 ± 0.2192762603095958
	data : 0.11480813026428223
	model : 0.06507339477539062
			 train-loss:  2.180914944020387 	 ± 0.21887665987685567
	data : 0.11491217613220214
	model : 0.06509890556335449
			 train-loss:  2.1799620384393736 	 ± 0.21881153359694527
	data : 0.11485557556152344
	model : 0.06507868766784668
			 train-loss:  2.178630187003701 	 ± 0.21917618630901303
	data : 0.11484713554382324
	model : 0.06506671905517578
			 train-loss:  2.1788990569004816 	 ± 0.2187062900537246
	data : 0.11462483406066895
	model : 0.06498913764953614
			 train-loss:  2.17848662116112 	 ± 0.2182886594478949
	data : 0.11421666145324708
	model : 0.06494965553283691
			 train-loss:  2.1770340100815306 	 ± 0.21884322339272722
	data : 0.11466994285583496
	model : 0.0649256706237793
			 train-loss:  2.1777792941440235 	 ± 0.21862366541803843
	data : 0.11484737396240234
	model : 0.06486024856567382
			 train-loss:  2.1778044355401085 	 ± 0.2181287996983781
	data : 0.11491274833679199
	model : 0.06480793952941895
			 train-loss:  2.1796106916290148 	 ± 0.21928719558393026
	data : 0.11517581939697266
	model : 0.06480212211608886
			 train-loss:  2.1792422239021336 	 ± 0.21886383561366302
	data : 0.11520142555236816
	model : 0.06476016044616699
			 train-loss:  2.1789600072162494 	 ± 0.21841541661499864
	data : 0.1147526741027832
	model : 0.06471920013427734
			 train-loss:  2.177935659620497 	 ± 0.21846810109617687
	data : 0.11470527648925781
	model : 0.06467108726501465
			 train-loss:  2.179987260725646 	 ± 0.22014577914459613
	data : 0.11461801528930664
	model : 0.06444263458251953
			 train-loss:  2.1799738638201474 	 ± 0.21966043366260954
	data : 0.11461434364318848
	model : 0.0642728328704834
			 train-loss:  2.1795932096347475 	 ± 0.2192532142752672
	data : 0.11474909782409667
	model : 0.06408939361572266
			 train-loss:  2.1791124645799527 	 ± 0.2188943697293644
	data : 0.11486930847167968
	model : 0.06392955780029297
			 train-loss:  2.179226376699365 	 ± 0.21842479625232736
	data : 0.11500201225280762
	model : 0.06381764411926269
			 train-loss:  2.1781978044675028 	 ± 0.218509012279446
	data : 0.11514983177185059
	model : 0.06387972831726074
			 train-loss:  2.1777602459849983 	 ± 0.21813897537176707
	data : 0.11520543098449706
	model : 0.06387429237365723
			 train-loss:  2.177657258868729 	 ± 0.21767601485578375
	data : 0.11506133079528809
	model : 0.06384587287902832
			 train-loss:  2.177908795002179 	 ± 0.21724432938931512
	data : 0.11494898796081543
	model : 0.06381387710571289
			 train-loss:  2.178828450973998 	 ± 0.21723760712110962
	data : 0.1149439811706543
	model : 0.06383090019226074
			 train-loss:  2.1795222249071475 	 ± 0.21703760463458807
	data : 0.11504607200622559
	model : 0.06384105682373047
			 train-loss:  2.181298768470056 	 ± 0.21829202104666934
	data : 0.11510882377624512
	model : 0.06382551193237304
			 train-loss:  2.1800095699414483 	 ± 0.21873521034273785
	data : 0.11525797843933105
	model : 0.063897705078125
			 train-loss:  2.1794235461925364 	 ± 0.21846427269868854
	data : 0.11537227630615235
	model : 0.06393132209777833
			 train-loss:  2.1812287881970405 	 ± 0.21978774815786814
	data : 0.11513872146606445
	model : 0.0638657569885254
			 train-loss:  2.185345826307273 	 ± 0.22841675149571897
	data : 0.11496386528015137
	model : 0.06378231048583985
			 train-loss:  2.185596321732545 	 ± 0.22797749604488576
	data : 0.11494245529174804
	model : 0.06378154754638672
			 train-loss:  2.1873330613713207 	 ± 0.22910650586827677
	data : 0.11491317749023437
	model : 0.0637822151184082
			 train-loss:  2.187087774765296 	 ± 0.22866851370033606
	data : 0.11492094993591309
	model : 0.06375708580017089
			 train-loss:  2.187375523606125 	 ± 0.2282456276478603
	data : 0.11513810157775879
	model : 0.06379227638244629
			 train-loss:  2.18703938645076 	 ± 0.22784199792404608
	data : 0.11516790390014649
	model : 0.0638427734375
			 train-loss:  2.1874900708797007 	 ± 0.22749015961670235
	data : 0.11505050659179687
	model : 0.06386375427246094
			 train-loss:  2.1872721482669153 	 ± 0.2270568790303764
	data : 0.11504302024841309
	model : 0.06383252143859863
			 train-loss:  2.1878146737454887 	 ± 0.22676149061449882
	data : 0.1150467872619629
	model : 0.06386222839355468
			 train-loss:  2.188069239139557 	 ± 0.22634316117928333
	data : 0.11504788398742676
	model : 0.06390933990478516
			 train-loss:  2.1881683057997807 	 ± 0.2258972591486287
	data : 0.11513710021972656
	model : 0.06391310691833496
			 train-loss:  2.1880451180632154 	 ± 0.22545705216366735
	data : 0.11530728340148926
	model : 0.06392669677734375
			 train-loss:  2.1878405196864614 	 ± 0.22503448321696362
	data : 0.11543374061584473
	model : 0.06398196220397949
			 train-loss:  2.19040672657058 	 ± 0.22827014086921102
	data : 0.11540350914001465
	model : 0.0638859748840332
			 train-loss:  2.191005345419341 	 ± 0.22802178522441685
	data : 0.11526012420654297
	model : 0.06384377479553223
			 train-loss:  2.195209064055234 	 ± 0.23726989289505732
	data : 0.11501288414001465
	model : 0.05542154312133789
#epoch  46    val-loss:  2.4077507445686743  train-loss:  2.195209064055234  lr:  1.220703125e-06
			 train-loss:  2.2085680961608887 	 ± 0.0
	data : 5.242514610290527
	model : 0.07369828224182129
			 train-loss:  2.308820128440857 	 ± 0.10025203227996826
	data : 2.8390434980392456
	model : 0.06900334358215332
			 train-loss:  2.1110899448394775 	 ± 0.2913670611644007
	data : 1.9306259155273438
	model : 0.0676117738087972
			 train-loss:  2.1828407645225525 	 ± 0.28127497866725387
	data : 1.476691722869873
	model : 0.06687432527542114
			 train-loss:  2.1945728778839113 	 ± 0.2526718441676998
	data : 1.204249334335327
	model : 0.06641898155212403
			 train-loss:  2.3219303290049234 	 ± 0.3664725807971549
	data : 0.1786658763885498
	model : 0.06461048126220703
			 train-loss:  2.3222246510641917 	 ± 0.3392884472637858
	data : 0.11432352066040039
	model : 0.06463742256164551
			 train-loss:  2.337240159511566 	 ± 0.31985204016602614
	data : 0.11421489715576172
	model : 0.06459975242614746
			 train-loss:  2.2997622754838734 	 ± 0.31964793649556666
	data : 0.11403350830078125
	model : 0.0646395206451416
			 train-loss:  2.246349203586578 	 ± 0.34297803023056256
	data : 0.1140009880065918
	model : 0.06471424102783203
			 train-loss:  2.2838708379051904 	 ± 0.3478773721797703
	data : 0.11391887664794922
	model : 0.06474757194519043
			 train-loss:  2.276728004217148 	 ± 0.3339086672118428
	data : 0.11395187377929687
	model : 0.06547050476074219
			 train-loss:  2.298798845364497 	 ± 0.32979375991049426
	data : 0.11359615325927734
	model : 0.06548891067504883
			 train-loss:  2.3033002700124467 	 ± 0.31821139053526776
	data : 0.11353306770324707
	model : 0.06544613838195801
			 train-loss:  2.2920454104741412 	 ± 0.31029232864156864
	data : 0.11355667114257813
	model : 0.06547417640686035
			 train-loss:  2.28415796905756 	 ± 0.30198828142735
	data : 0.11348767280578613
	model : 0.06547865867614747
			 train-loss:  2.2614243872025432 	 ± 0.3067595951102893
	data : 0.11354584693908691
	model : 0.06484050750732422
			 train-loss:  2.2549533314175076 	 ± 0.29930830004247
	data : 0.11413273811340333
	model : 0.06486926078796387
			 train-loss:  2.264267268933748 	 ± 0.29399306815144804
	data : 0.1142423152923584
	model : 0.06493415832519531
			 train-loss:  2.2565544486045837 	 ± 0.2885144576381161
	data : 0.11413054466247559
	model : 0.06488728523254395
			 train-loss:  2.258646011352539 	 ± 0.2817166080191693
	data : 0.1141392707824707
	model : 0.06486077308654785
			 train-loss:  2.2643585963682695 	 ± 0.2764816225886839
	data : 0.11397199630737305
	model : 0.06479325294494628
			 train-loss:  2.2565972494042437 	 ± 0.2728438486556434
	data : 0.11394615173339843
	model : 0.0647933006286621
			 train-loss:  2.2520642280578613 	 ± 0.267982373813008
	data : 0.11397161483764648
	model : 0.06478824615478515
			 train-loss:  2.2460472488403322 	 ± 0.26421746252126654
	data : 0.11408019065856934
	model : 0.06478300094604492
			 train-loss:  2.270564702840952 	 ± 0.28662427130750745
	data : 0.11425471305847168
	model : 0.06480984687805176
			 train-loss:  2.267782935389766 	 ± 0.28162376616313956
	data : 0.11442375183105469
	model : 0.06489362716674804
			 train-loss:  2.2531174549034665 	 ± 0.2868561255453855
	data : 0.11433334350585937
	model : 0.06493353843688965
			 train-loss:  2.240058578293899 	 ± 0.2902135983353765
	data : 0.1143420696258545
	model : 0.06492228507995605
			 train-loss:  2.234720301628113 	 ± 0.2867802067295827
	data : 0.11423859596252442
	model : 0.06487808227539063
			 train-loss:  2.2286713815504506 	 ± 0.28405558195731617
	data : 0.11400065422058106
	model : 0.06489434242248535
			 train-loss:  2.2181630209088326 	 ± 0.28563837611467463
	data : 0.1138601303100586
	model : 0.0649038314819336
			 train-loss:  2.2164639415162983 	 ± 0.2814413958399737
	data : 0.11394691467285156
	model : 0.06488723754882812
			 train-loss:  2.225688520599814 	 ± 0.28228999194332777
	data : 0.11396670341491699
	model : 0.06494498252868652
			 train-loss:  2.228452512196132 	 ± 0.2786944526437692
	data : 0.11393380165100098
	model : 0.06501388549804688
			 train-loss:  2.2305971119138928 	 ± 0.27508918027314455
	data : 0.1141214370727539
	model : 0.06497211456298828
			 train-loss:  2.227437161110543 	 ± 0.2720078725398199
	data : 0.11424713134765625
	model : 0.06493840217590333
			 train-loss:  2.234501838684082 	 ± 0.27182325215667746
	data : 0.11430478096008301
	model : 0.06489095687866211
			 train-loss:  2.2255470905548487 	 ± 0.27393511076714017
	data : 0.11420884132385253
	model : 0.0648801326751709
			 train-loss:  2.222443976998329 	 ± 0.2711825529496169
	data : 0.11438088417053223
	model : 0.06486392021179199
			 train-loss:  2.2233220571424903 	 ± 0.2679125977583974
	data : 0.11439104080200195
	model : 0.06488156318664551
			 train-loss:  2.218964619295938 	 ± 0.26617035500747577
	data : 0.11452245712280273
	model : 0.06489691734313965
			 train-loss:  2.213479249976402 	 ± 0.2654483204049739
	data : 0.11446061134338378
	model : 0.06492390632629394
			 train-loss:  2.2110640108585358 	 ± 0.2628920284591297
	data : 0.1145538330078125
	model : 0.06485624313354492
			 train-loss:  2.2138287835650976 	 ± 0.2606007025009366
	data : 0.11444716453552246
	model : 0.06483354568481445
			 train-loss:  2.205726867136748 	 ± 0.26342021519559594
	data : 0.11437616348266602
	model : 0.06478877067565918
			 train-loss:  2.206591291630522 	 ± 0.26066874553377256
	data : 0.11429066658020019
	model : 0.0647580623626709
			 train-loss:  2.2033329755067825 	 ± 0.25890459407021627
	data : 0.11425328254699707
	model : 0.0647895336151123
			 train-loss:  2.201489589652237 	 ± 0.2565671542686124
	data : 0.11430072784423828
	model : 0.06484179496765137
			 train-loss:  2.2013821172714234 	 ± 0.2539896386111514
	data : 0.11443519592285156
	model : 0.06489253044128418
			 train-loss:  2.196965318100125 	 ± 0.2534190716292158
	data : 0.11451940536499024
	model : 0.06503033638000488
			 train-loss:  2.1928463601149044 	 ± 0.2526884620440027
	data : 0.11451501846313476
	model : 0.06510000228881836
			 train-loss:  2.199563608979279 	 ± 0.25493730371106055
	data : 0.1144942283630371
	model : 0.06509127616882324
			 train-loss:  2.197602759908747 	 ± 0.2529688428576258
	data : 0.11441020965576172
	model : 0.06503968238830567
			 train-loss:  2.19635733907873 	 ± 0.2508255968380282
	data : 0.11427826881408691
	model : 0.06502013206481934
			 train-loss:  2.1985212841204236 	 ± 0.24909349896479302
	data : 0.11420073509216308
	model : 0.0649444580078125
			 train-loss:  2.1993160728822674 	 ± 0.2469704272564018
	data : 0.11425924301147461
	model : 0.06490306854248047
			 train-loss:  2.203100091424482 	 ± 0.24649327756080425
	data : 0.11434273719787598
	model : 0.06485061645507813
			 train-loss:  2.2036691904067993 	 ± 0.2444338521602056
	data : 0.1144479751586914
	model : 0.06490521430969239
			 train-loss:  2.202348909775416 	 ± 0.2426004017507735
	data : 0.11449885368347168
	model : 0.0649099349975586
			 train-loss:  2.204032618491376 	 ± 0.24095686772809222
	data : 0.11456174850463867
	model : 0.06491322517395019
			 train-loss:  2.202978774424522 	 ± 0.23914745011755437
	data : 0.11449728012084961
	model : 0.06495437622070313
			 train-loss:  2.2023083009417097 	 ± 0.23730059505884618
	data : 0.11434373855590821
	model : 0.06498575210571289
			 train-loss:  2.205680249258876 	 ± 0.23695572651842176
	data : 0.11420068740844727
	model : 0.06494321823120117
			 train-loss:  2.2008754711884717 	 ± 0.23824713597775385
	data : 0.11418709754943848
	model : 0.06496176719665528
			 train-loss:  2.2021923155495613 	 ± 0.23667358791587267
	data : 0.11427888870239258
	model : 0.06499934196472168
			 train-loss:  2.2055023054578413 	 ± 0.23643487428380885
	data : 0.11433148384094238
	model : 0.06502952575683593
			 train-loss:  2.204698771238327 	 ± 0.2347820890507229
	data : 0.114495849609375
	model : 0.0650418758392334
			 train-loss:  2.2074964754823325 	 ± 0.23421357088661576
	data : 0.11450061798095704
	model : 0.06504530906677246
			 train-loss:  2.212274828978947 	 ± 0.23589784521707732
	data : 0.11458373069763184
	model : 0.06501269340515137
			 train-loss:  2.2132671131214625 	 ± 0.23437778390390399
	data : 0.114528226852417
	model : 0.06495747566223145
			 train-loss:  2.214134515987502 	 ± 0.23285920110459687
	data : 0.11454095840454101
	model : 0.06487855911254883
			 train-loss:  2.2112898679628765 	 ± 0.23251504627889097
	data : 0.11458382606506348
	model : 0.06485662460327149
			 train-loss:  2.214411998117292 	 ± 0.23247417961951933
	data : 0.11472821235656738
	model : 0.06485538482666016
			 train-loss:  2.208548485438029 	 ± 0.23636377060960398
	data : 0.1146665096282959
	model : 0.06485557556152344
			 train-loss:  2.2105310716127096 	 ± 0.23543051543696125
	data : 0.11467385292053223
	model : 0.06488327980041504
			 train-loss:  2.213410792412696 	 ± 0.23524017888420898
	data : 0.11455316543579101
	model : 0.06491336822509766
			 train-loss:  2.2159271209667892 	 ± 0.23476805147390897
	data : 0.11458673477172851
	model : 0.064918851852417
			 train-loss:  2.2132180944273743 	 ± 0.2345011619578367
	data : 0.11455006599426269
	model : 0.06497259140014648
			 train-loss:  2.21157466173172 	 ± 0.23348828414626582
	data : 0.11440668106079102
	model : 0.06500601768493652
			 train-loss:  2.2104495896233454 	 ± 0.23226061904943016
	data : 0.11427745819091797
	model : 0.0649653434753418
			 train-loss:  2.2113464169385955 	 ± 0.23098111958443523
	data : 0.11432867050170899
	model : 0.06496915817260743
			 train-loss:  2.208251035357096 	 ± 0.2312901918028542
	data : 0.11423215866088868
	model : 0.06499485969543457
			 train-loss:  2.2060544987519584 	 ± 0.23077859937732556
	data : 0.11417279243469239
	model : 0.06500229835510254
			 train-loss:  2.2055846088072832 	 ± 0.2294574800956329
	data : 0.1143620491027832
	model : 0.06498851776123046
			 train-loss:  2.2041490313618683 	 ± 0.2285031560879502
	data : 0.11439590454101563
	model : 0.06504230499267578
			 train-loss:  2.2075828697489595 	 ± 0.2294070228236615
	data : 0.11443686485290527
	model : 0.06504235267639161
			 train-loss:  2.2075549404729498 	 ± 0.2280999984190387
	data : 0.11426811218261719
	model : 0.06498408317565918
			 train-loss:  2.203906132933799 	 ± 0.22938313149750547
	data : 0.11429286003112793
	model : 0.06494827270507812
			 train-loss:  2.205659896797604 	 ± 0.22870445578872167
	data : 0.11424021720886231
	model : 0.06492719650268555
			 train-loss:  2.205445953777858 	 ± 0.227453422313614
	data : 0.11428046226501465
	model : 0.06492109298706054
			 train-loss:  2.2024446622185083 	 ± 0.2280184796547236
	data : 0.11429228782653808
	model : 0.06492352485656738
			 train-loss:  2.2021664727118706 	 ± 0.2268049570302018
	data : 0.11443605422973632
	model : 0.06497578620910645
			 train-loss:  2.2088852562802903 	 ± 0.2347157007726699
	data : 0.11441502571105958
	model : 0.06496248245239258
			 train-loss:  2.207626623856394 	 ± 0.23379576708385497
	data : 0.11429071426391602
	model : 0.06498117446899414
			 train-loss:  2.2063067133227983 	 ± 0.232930432683001
	data : 0.11422696113586425
	model : 0.06493334770202637
			 train-loss:  2.207807339343828 	 ± 0.23219263672498813
	data : 0.11409516334533691
	model : 0.06489396095275879
			 train-loss:  2.203938418505143 	 ± 0.23412653161304606
	data : 0.11420588493347168
	model : 0.06490168571472169
			 train-loss:  2.2047579481144144 	 ± 0.23308230993661197
	data : 0.11429829597473144
	model : 0.06491389274597167
			 train-loss:  2.2012592589855196 	 ± 0.23451211335403793
	data : 0.11453452110290527
	model : 0.06492481231689454
			 train-loss:  2.204956813613967 	 ± 0.23625962025159677
	data : 0.1146270751953125
	model : 0.06496539115905761
			 train-loss:  2.2084770167575165 	 ± 0.23774553992094427
	data : 0.11465754508972167
	model : 0.06497321128845215
			 train-loss:  2.2075828985103128 	 ± 0.2367608898581012
	data : 0.11456217765808105
	model : 0.06498160362243652
			 train-loss:  2.2059721018259344 	 ± 0.23618630837193283
	data : 0.1144289493560791
	model : 0.06495418548583984
			 train-loss:  2.206268009685335 	 ± 0.23507829063532013
	data : 0.11427960395812989
	model : 0.06493568420410156
			 train-loss:  2.20497657330531 	 ± 0.23434074506671332
	data : 0.11428227424621581
	model : 0.06496763229370117
			 train-loss:  2.203463827338174 	 ± 0.2337625407806524
	data : 0.11442227363586426
	model : 0.06501803398132325
			 train-loss:  2.2052741525349795 	 ± 0.23343012361972915
	data : 0.11448845863342286
	model : 0.06499624252319336
			 train-loss:  2.202809955001971 	 ± 0.23376381867716
	data : 0.11460385322570801
	model : 0.06504511833190918
			 train-loss:  2.2025554635308007 	 ± 0.2327139981296445
	data : 0.11467919349670411
	model : 0.0650707721710205
			 train-loss:  2.200928022195627 	 ± 0.23229131919630103
	data : 0.11460151672363281
	model : 0.06504645347595214
			 train-loss:  2.2015282724584853 	 ± 0.23133843441114144
	data : 0.11447515487670898
	model : 0.06500205993652344
			 train-loss:  2.2023701013716974 	 ± 0.23048478691358282
	data : 0.1143862247467041
	model : 0.06500124931335449
			 train-loss:  2.2025415102640786 	 ± 0.22947889607879454
	data : 0.11435484886169434
	model : 0.0649744987487793
			 train-loss:  2.2034878689309827 	 ± 0.22870230343995462
	data : 0.11435389518737793
	model : 0.06494669914245606
			 train-loss:  2.2018781242699457 	 ± 0.22836776828651323
	data : 0.11454110145568848
	model : 0.06495432853698731
			 train-loss:  2.2046443641695204 	 ± 0.229333244514684
	data : 0.11464276313781738
	model : 0.06495585441589355
			 train-loss:  2.203640905477233 	 ± 0.228617230622096
	data : 0.114677095413208
	model : 0.06493825912475586
			 train-loss:  2.2039306384174764 	 ± 0.22767638205206917
	data : 0.11462192535400391
	model : 0.0649195671081543
			 train-loss:  2.2060631215572357 	 ± 0.2279160250101018
	data : 0.11459259986877442
	model : 0.06488227844238281
			 train-loss:  2.2064647004624045 	 ± 0.22701489577821754
	data : 0.11438355445861817
	model : 0.06483731269836426
			 train-loss:  2.2089855573216424 	 ± 0.22777677593416676
	data : 0.1143221378326416
	model : 0.06480817794799805
			 train-loss:  2.2071050114747957 	 ± 0.22779793654270483
	data : 0.11438641548156739
	model : 0.06485180854797364
			 train-loss:  2.2074664129364874 	 ± 0.22691293921786643
	data : 0.11436152458190918
	model : 0.06488194465637206
			 train-loss:  2.209930506706238 	 ± 0.22766304955739824
	data : 0.11436734199523926
	model : 0.06490187644958496
			 train-loss:  2.2116320502190363 	 ± 0.22755442763120895
	data : 0.11432104110717774
	model : 0.06489377021789551
			 train-loss:  2.209628864536135 	 ± 0.22776940060059317
	data : 0.1143110752105713
	model : 0.0649080753326416
			 train-loss:  2.2113907607272267 	 ± 0.22774511910197978
	data : 0.11430945396423339
	model : 0.06486144065856933
			 train-loss:  2.2104314842889474 	 ± 0.22712012250307523
	data : 0.11433944702148438
	model : 0.06484990119934082
			 train-loss:  2.2108235826859106 	 ± 0.22628872290885862
	data : 0.11439509391784668
	model : 0.06489181518554688
			 train-loss:  2.2104020218812783 	 ± 0.22547460810905848
	data : 0.11456828117370606
	model : 0.06492142677307129
			 train-loss:  2.2083884450522335 	 ± 0.22579812862753473
	data : 0.11460084915161133
	model : 0.06496729850769042
			 train-loss:  2.206339276822886 	 ± 0.22617632614013528
	data : 0.11455988883972168
	model : 0.06500434875488281
			 train-loss:  2.2056168531303975 	 ± 0.22548477440889828
	data : 0.11454191207885742
	model : 0.06501631736755371
			 train-loss:  2.2061254695609764 	 ± 0.22472523308474168
	data : 0.11442103385925292
	model : 0.06495203971862792
			 train-loss:  2.2069500737330494 	 ± 0.2241024153451364
	data : 0.11437935829162597
	model : 0.06496329307556152
			 train-loss:  2.2056752069153056 	 ± 0.22377745244088354
	data : 0.1143561840057373
	model : 0.06493048667907715
			 train-loss:  2.202553946038951 	 ± 0.22593841428175987
	data : 0.11436142921447753
	model : 0.06498970985412597
			 train-loss:  2.201595575689412 	 ± 0.2254055532193441
	data : 0.11442198753356933
	model : 0.06497535705566407
			 train-loss:  2.2012550456183297 	 ± 0.2246349707456661
	data : 0.11448392868041993
	model : 0.06498966217041016
			 train-loss:  2.200612984650524 	 ± 0.2239658578685158
	data : 0.1145317554473877
	model : 0.06499266624450684
			 train-loss:  2.199803223072643 	 ± 0.22338289266089806
	data : 0.11450695991516113
	model : 0.06498770713806153
			 train-loss:  2.199653180329116 	 ± 0.22260764382207499
	data : 0.1144524097442627
	model : 0.06493110656738281
			 train-loss:  2.2007077982028327 	 ± 0.22219154856199536
	data : 0.1144099235534668
	model : 0.06495227813720703
			 train-loss:  2.2026002357745993 	 ± 0.22258552936777953
	data : 0.1144188404083252
	model : 0.0649911880493164
			 train-loss:  2.202900123922792 	 ± 0.2218513320032153
	data : 0.11435971260070801
	model : 0.06501564979553223
			 train-loss:  2.202425864278054 	 ± 0.2211696989551376
	data : 0.11441659927368164
	model : 0.06504535675048828
			 train-loss:  2.200492464207314 	 ± 0.22166419027092224
	data : 0.11442780494689941
	model : 0.0650665283203125
			 train-loss:  2.199227488280943 	 ± 0.2214544475616439
	data : 0.11446876525878906
	model : 0.06506471633911133
			 train-loss:  2.197691056728363 	 ± 0.22151040124860044
	data : 0.11430444717407226
	model : 0.06505403518676758
			 train-loss:  2.195606013796977 	 ± 0.22224766002507818
	data : 0.11427145004272461
	model : 0.0650510311126709
			 train-loss:  2.193263093107625 	 ± 0.22337847388204626
	data : 0.11422848701477051
	model : 0.0650303840637207
			 train-loss:  2.1922253658568938 	 ± 0.22301456831976607
	data : 0.1144012451171875
	model : 0.0650256633758545
			 train-loss:  2.1922606539416623 	 ± 0.22228974431758208
	data : 0.11440815925598144
	model : 0.06503133773803711
			 train-loss:  2.1913991112862865 	 ± 0.2218293172793016
	data : 0.11453299522399903
	model : 0.06507325172424316
			 train-loss:  2.188936338210717 	 ± 0.2232328884138347
	data : 0.11454477310180664
	model : 0.06504912376403808
			 train-loss:  2.1913494843586236 	 ± 0.2245527691117524
	data : 0.1144254207611084
	model : 0.06501893997192383
			 train-loss:  2.1908688311335407 	 ± 0.2239220367390535
	data : 0.11425247192382812
	model : 0.06494193077087403
			 train-loss:  2.190246509306086 	 ± 0.2233537928631544
	data : 0.11414728164672852
	model : 0.06494102478027344
			 train-loss:  2.1909946493804453 	 ± 0.2228544772137399
	data : 0.1142146110534668
	model : 0.06487803459167481
			 train-loss:  2.189316696261767 	 ± 0.22317286909136036
	data : 0.1142333984375
	model : 0.06493229866027832
			 train-loss:  2.188890149563919 	 ± 0.22254881920695166
	data : 0.11443214416503907
	model : 0.06494903564453125
			 train-loss:  2.188446141459459 	 ± 0.22193706655060866
	data : 0.11450772285461426
	model : 0.06499390602111817
			 train-loss:  2.1881586356860834 	 ± 0.22128983975882016
	data : 0.11467618942260742
	model : 0.06495013236999511
			 train-loss:  2.1877144596793436 	 ± 0.22069156334759535
	data : 0.1145601749420166
	model : 0.06495566368103027
			 train-loss:  2.188014220042401 	 ± 0.22005951516034097
	data : 0.11462907791137696
	model : 0.06489806175231934
			 train-loss:  2.185877763582561 	 ± 0.22111967595783544
	data : 0.11446847915649414
	model : 0.06495442390441894
			 train-loss:  2.1874501570349647 	 ± 0.22139505470053958
	data : 0.11458606719970703
	model : 0.06495180130004882
			 train-loss:  2.1858073769236457 	 ± 0.2217636628336679
	data : 0.11449313163757324
	model : 0.0650238037109375
			 train-loss:  2.185234681998982 	 ± 0.22123576051608265
	data : 0.11464028358459473
	model : 0.06506862640380859
			 train-loss:  2.1842172766289516 	 ± 0.22098642821935252
	data : 0.11452093124389648
	model : 0.06508040428161621
			 train-loss:  2.1847569907820503 	 ± 0.22045609053468448
	data : 0.11457705497741699
	model : 0.06501336097717285
			 train-loss:  2.1850227462073972 	 ± 0.2198456402318954
	data : 0.11425585746765136
	model : 0.0649644374847412
			 train-loss:  2.1876056941076256 	 ± 0.22182994622572308
	data : 0.11431255340576171
	model : 0.06491765975952149
			 train-loss:  2.1865296466009956 	 ± 0.22165018448896218
	data : 0.11418795585632324
	model : 0.06491203308105468
			 train-loss:  2.1845613582567736 	 ± 0.2225480648682648
	data : 0.11425189971923828
	model : 0.06494193077087403
			 train-loss:  2.1849963840118236 	 ± 0.22199353940723288
	data : 0.11424169540405274
	model : 0.0650146484375
			 train-loss:  2.184312927588988 	 ± 0.2215557496290883
	data : 0.11442594528198242
	model : 0.06511774063110351
			 train-loss:  2.1832045682981693 	 ± 0.2214303216774933
	data : 0.11442174911499023
	model : 0.06512627601623536
			 train-loss:  2.1839038014411924 	 ± 0.22101246275418454
	data : 0.11436495780944825
	model : 0.06510233879089355
			 train-loss:  2.184655072280715 	 ± 0.22063143904773683
	data : 0.11434082984924317
	model : 0.06505231857299805
			 train-loss:  2.185438791474143 	 ± 0.22027696688210283
	data : 0.1144317626953125
	model : 0.06498823165893555
			 train-loss:  2.183995863127578 	 ± 0.22053509153148704
	data : 0.1144179344177246
	model : 0.06495404243469238
			 train-loss:  2.185421319111534 	 ± 0.22077872231538406
	data : 0.1143369197845459
	model : 0.06498074531555176
			 train-loss:  2.1860306649594694 	 ± 0.2203363040129148
	data : 0.11444830894470215
	model : 0.06503124237060547
			 train-loss:  2.187022221985684 	 ± 0.2201566826911348
	data : 0.11440477371215821
	model : 0.0651137351989746
			 train-loss:  2.1867215480396456 	 ± 0.2196055279463082
	data : 0.11437225341796875
	model : 0.06514739990234375
			 train-loss:  2.186202069546314 	 ± 0.21913586408680422
	data : 0.11427936553955079
	model : 0.06514077186584473
			 train-loss:  2.186046799654683 	 ± 0.21856573956132788
	data : 0.11426014900207519
	model : 0.06510014533996582
			 train-loss:  2.1849874301960592 	 ± 0.21847577466638687
	data : 0.11425681114196777
	model : 0.06504402160644532
			 train-loss:  2.1861414441263487 	 ± 0.2184829335765554
	data : 0.11432399749755859
	model : 0.06495084762573242
			 train-loss:  2.186936454847455 	 ± 0.2181900400692542
	data : 0.11441879272460938
	model : 0.06497454643249512
			 train-loss:  2.1862575544594485 	 ± 0.21782727020390188
	data : 0.11448397636413574
	model : 0.0649658203125
			 train-loss:  2.185148966681097 	 ± 0.21781030558679776
	data : 0.11445679664611816
	model : 0.06499571800231933
			 train-loss:  2.1852950536287747 	 ± 0.21726062823921635
	data : 0.1144801139831543
	model : 0.06500225067138672
			 train-loss:  2.184903417314802 	 ± 0.21677468033978914
	data : 0.11434040069580079
	model : 0.06498088836669921
			 train-loss:  2.1854440403468716 	 ± 0.21635621847053407
	data : 0.11422562599182129
	model : 0.0649444580078125
			 train-loss:  2.1860829266634854 	 ± 0.21599539272948692
	data : 0.11431784629821777
	model : 0.06495432853698731
			 train-loss:  2.1871260973676363 	 ± 0.21595145758718273
	data : 0.11446256637573242
	model : 0.06494617462158203
			 train-loss:  2.186608806848526 	 ± 0.21553446866014214
	data : 0.11448912620544434
	model : 0.06495285034179688
			 train-loss:  2.187736416337502 	 ± 0.21558823686906847
	data : 0.11462550163269043
	model : 0.06496615409851074
			 train-loss:  2.1866997096798206 	 ± 0.21555561751386254
	data : 0.11466827392578124
	model : 0.06496000289916992
			 train-loss:  2.185439998293158 	 ± 0.2157681272540708
	data : 0.11443886756896973
	model : 0.06492338180541993
			 train-loss:  2.1841579062097214 	 ± 0.21601239066283676
	data : 0.1143331527709961
	model : 0.06489748954772949
			 train-loss:  2.1822713991490805 	 ± 0.21716296640471672
	data : 0.11439356803894044
	model : 0.06495838165283203
			 train-loss:  2.1842942492475785 	 ± 0.21856273069750012
	data : 0.11440544128417969
	model : 0.06500391960144043
			 train-loss:  2.1855979080937336 	 ± 0.2188355509197047
	data : 0.11435651779174805
	model : 0.06505289077758789
			 train-loss:  2.1859455292041483 	 ± 0.21836615293514242
	data : 0.11456742286682128
	model : 0.06507663726806641
			 train-loss:  2.185714720538929 	 ± 0.21786855077074946
	data : 0.11460838317871094
	model : 0.06509261131286621
			 train-loss:  2.1854399158841087 	 ± 0.2173855024201159
	data : 0.11451902389526367
	model : 0.06501183509826661
			 train-loss:  2.1852743410951154 	 ± 0.2168830319898694
	data : 0.1144287109375
	model : 0.06498427391052246
			 train-loss:  2.184706935342753 	 ± 0.21652783268938655
	data : 0.11453614234924317
	model : 0.06493396759033203
			 train-loss:  2.1871097490821088 	 ± 0.21883366553919636
	data : 0.11443943977355957
	model : 0.06491985321044921
			 train-loss:  2.1885931536416026 	 ± 0.2193925724328957
	data : 0.11457748413085937
	model : 0.0648998737335205
			 train-loss:  2.1896420756051707 	 ± 0.2194189525319808
	data : 0.11470375061035157
	model : 0.06494407653808594
			 train-loss:  2.18909525319382 	 ± 0.21905723653759268
	data : 0.11474456787109374
	model : 0.06499991416931153
			 train-loss:  2.188632583837905 	 ± 0.21865766964959343
	data : 0.11471858024597167
	model : 0.06500697135925293
			 train-loss:  2.189562838011925 	 ± 0.21858555550847653
	data : 0.11482472419738769
	model : 0.0649991512298584
			 train-loss:  2.189907751126921 	 ± 0.21814538171601072
	data : 0.11480493545532226
	model : 0.06498141288757324
			 train-loss:  2.1905932448127055 	 ± 0.21788531298581731
	data : 0.11463003158569336
	model : 0.06489925384521485
			 train-loss:  2.1893070386006284 	 ± 0.21822728267606684
	data : 0.11475505828857421
	model : 0.06481447219848632
			 train-loss:  2.1891169693018937 	 ± 0.21775355828367177
	data : 0.11478948593139648
	model : 0.06475591659545898
			 train-loss:  2.188094633042545 	 ± 0.217798093780221
	data : 0.11485686302185058
	model : 0.06474719047546387
			 train-loss:  2.187844341354711 	 ± 0.21734353405662266
	data : 0.1149096965789795
	model : 0.06475830078125
			 train-loss:  2.1883106348249646 	 ± 0.2169722756914556
	data : 0.11497282981872559
	model : 0.06468873023986817
			 train-loss:  2.1874022642068103 	 ± 0.216920075406169
	data : 0.11500797271728516
	model : 0.06450843811035156
			 train-loss:  2.188869453211713 	 ± 0.2175627008772791
	data : 0.11505317687988281
	model : 0.06433348655700684
			 train-loss:  2.191745352326778 	 ± 0.22136710741416685
	data : 0.1149557113647461
	model : 0.06411900520324706
			 train-loss:  2.1908320584151424 	 ± 0.22131331575475496
	data : 0.11480646133422852
	model : 0.06392722129821778
			 train-loss:  2.190354077194048 	 ± 0.22095010238381946
	data : 0.11489982604980468
	model : 0.0638127326965332
			 train-loss:  2.1912734637528786 	 ± 0.2209117986562738
	data : 0.11500334739685059
	model : 0.06384201049804687
			 train-loss:  2.191225130496354 	 ± 0.220436405520985
	data : 0.11509914398193359
	model : 0.0638657569885254
			 train-loss:  2.1906454998024545 	 ± 0.2201399647276627
	data : 0.11516127586364747
	model : 0.06388916969299316
			 train-loss:  2.1902640553621144 	 ± 0.21974622787622475
	data : 0.11532368659973144
	model : 0.06391854286193847
			 train-loss:  2.191591813209209 	 ± 0.22021682556858713
	data : 0.11521639823913574
	model : 0.06391992568969726
			 train-loss:  2.190260723485785 	 ± 0.2206951155613733
	data : 0.1150273323059082
	model : 0.06392297744750977
			 train-loss:  2.1914851766095382 	 ± 0.2210308879804806
	data : 0.11499872207641601
	model : 0.06393818855285645
			 train-loss:  2.1901895063264027 	 ± 0.22146613343737384
	data : 0.11516189575195312
	model : 0.0640073299407959
			 train-loss:  2.189647474548308 	 ± 0.22116047042468964
	data : 0.11511974334716797
	model : 0.06400628089904785
			 train-loss:  2.1898338442047436 	 ± 0.22071804460340155
	data : 0.11526684761047364
	model : 0.06402435302734374
			 train-loss:  2.1892678336978455 	 ± 0.22043411836736782
	data : 0.11537337303161621
	model : 0.0639887809753418
			 train-loss:  2.1893923701333606 	 ± 0.2199867000583213
	data : 0.1152268409729004
	model : 0.06394872665405274
			 train-loss:  2.1881852772991355 	 ± 0.2203352151200853
	data : 0.11506156921386719
	model : 0.06393027305603027
			 train-loss:  2.190004558348265 	 ± 0.22170457554738346
	data : 0.11513152122497558
	model : 0.063908052444458
			 train-loss:  2.1892683296787494 	 ± 0.22155033429840087
	data : 0.11510391235351562
	model : 0.06397290229797363
			 train-loss:  2.188966664356914 	 ± 0.22114998398347682
	data : 0.11512141227722168
	model : 0.06398496627807618
			 train-loss:  2.188942818023898 	 ± 0.22070217483665816
	data : 0.11519122123718262
	model : 0.06400375366210938
			 train-loss:  2.1897257782759203 	 ± 0.2206002232862276
	data : 0.11525192260742187
	model : 0.0639352798461914
			 train-loss:  2.19028333344134 	 ± 0.22033182731190815
	data : 0.11505908966064453
	model : 0.06392545700073242
			 train-loss:  2.190397412776947 	 ± 0.21989809044124603
	data : 0.1151237964630127
	model : 0.06384081840515136
			 train-loss:  2.1896799698293923 	 ± 0.21975259087223123
	data : 0.1151233196258545
	model : 0.06382164955139161
			 train-loss:  2.1894962035474324 	 ± 0.21933546390387246
	data : 0.11518673896789551
	model : 0.06384930610656739
			 train-loss:  2.189808434177293 	 ± 0.21895767242791891
	data : 0.11520752906799317
	model : 0.063895845413208
			 train-loss:  2.1903011859871273 	 ± 0.2186667372120866
	data : 0.1153191089630127
	model : 0.06388206481933593
			 train-loss:  2.189618379461999 	 ± 0.21850870163229882
	data : 0.11537671089172363
	model : 0.06389884948730469
			 train-loss:  2.1899688714183867 	 ± 0.21815331775301297
	data : 0.11496400833129883
	model : 0.05544428825378418
#epoch  47    val-loss:  2.3958340444062887  train-loss:  2.1899688714183867  lr:  6.103515625e-07
			 train-loss:  2.228886604309082 	 ± 0.0
	data : 5.519144296646118
	model : 0.0730292797088623
			 train-loss:  2.1141836643218994 	 ± 0.11470293998718262
	data : 2.824280619621277
	model : 0.0690457820892334
			 train-loss:  2.1902804374694824 	 ± 0.14266258816318395
	data : 1.9209974606831868
	model : 0.0675202210744222
			 train-loss:  2.1892303228378296 	 ± 0.1235628130477876
	data : 1.4693480134010315
	model : 0.06682193279266357
			 train-loss:  2.1276479721069337 	 ± 0.16548038766209835
	data : 1.1981502056121827
	model : 0.06638646125793457
			 train-loss:  2.1274179220199585 	 ± 0.15106311109117457
	data : 0.1171152114868164
	model : 0.06471567153930664
			 train-loss:  2.1182600430079868 	 ± 0.1416448196880289
	data : 0.1140942096710205
	model : 0.06465010643005371
			 train-loss:  2.1728851795196533 	 ± 0.19606806687684966
	data : 0.11401762962341308
	model : 0.06472220420837402
			 train-loss:  2.18391129705641 	 ± 0.1874670086773413
	data : 0.11397171020507812
	model : 0.06474165916442871
			 train-loss:  2.173970675468445 	 ± 0.18032979517035838
	data : 0.11412882804870605
	model : 0.06479315757751465
			 train-loss:  2.157398809086193 	 ± 0.17974661290395028
	data : 0.11417121887207031
	model : 0.06483983993530273
			 train-loss:  2.1535736123720803 	 ± 0.172561277643114
	data : 0.11416268348693848
	model : 0.06501936912536621
			 train-loss:  2.161113115457388 	 ± 0.16783609567728477
	data : 0.11424374580383301
	model : 0.06503453254699706
			 train-loss:  2.169844082423619 	 ± 0.1647661190633205
	data : 0.11432037353515626
	model : 0.06503934860229492
			 train-loss:  2.17447837193807 	 ± 0.1601208564658632
	data : 0.11448216438293457
	model : 0.06506705284118652
			 train-loss:  2.167616382241249 	 ± 0.1572977246248036
	data : 0.11466293334960938
	model : 0.06505336761474609
			 train-loss:  2.1784759409287395 	 ± 0.15866319318227431
	data : 0.11468467712402344
	model : 0.06489887237548828
			 train-loss:  2.1850962506400213 	 ± 0.156590348789011
	data : 0.11467223167419434
	model : 0.06488232612609864
			 train-loss:  2.1731152283517936 	 ± 0.160666707343566
	data : 0.11453013420104981
	model : 0.06487870216369629
			 train-loss:  2.1656198740005492 	 ± 0.1599703963436848
	data : 0.11442799568176269
	model : 0.06482157707214356
			 train-loss:  2.1561693123408725 	 ± 0.16173494817935027
	data : 0.1142693042755127
	model : 0.06487593650817872
			 train-loss:  2.1477301066572014 	 ± 0.16268008087292377
	data : 0.11433582305908203
	model : 0.06490740776062012
			 train-loss:  2.152762470038041 	 ± 0.16084560249528648
	data : 0.11438016891479492
	model : 0.06499624252319336
			 train-loss:  2.1357445269823074 	 ± 0.17735381460936675
	data : 0.11449141502380371
	model : 0.06502623558044433
			 train-loss:  2.1378863191604616 	 ± 0.17408703312958013
	data : 0.11443920135498047
	model : 0.06504511833190918
			 train-loss:  2.1426404943832984 	 ± 0.1723534824723064
	data : 0.11451573371887207
	model : 0.0649862289428711
			 train-loss:  2.158462882041931 	 ± 0.1873887875302061
	data : 0.11444997787475586
	model : 0.0649184226989746
			 train-loss:  2.157177231141499 	 ± 0.1841333609053952
	data : 0.11439700126647949
	model : 0.06487321853637695
			 train-loss:  2.1494577999772697 	 ± 0.18548440013987932
	data : 0.11442136764526367
	model : 0.06486115455627442
			 train-loss:  2.162040050824483 	 ± 0.19454748861694365
	data : 0.11450667381286621
	model : 0.06490864753723144
			 train-loss:  2.1616706386689217 	 ± 0.19139459935934286
	data : 0.11449799537658692
	model : 0.06494135856628418
			 train-loss:  2.167886085808277 	 ± 0.1915325854667595
	data : 0.11441717147827149
	model : 0.0649946689605713
			 train-loss:  2.17606190479163 	 ± 0.19419598678918812
	data : 0.11437854766845704
	model : 0.06497917175292969
			 train-loss:  2.168394684791565 	 ± 0.19632332792404517
	data : 0.11421880722045899
	model : 0.06494660377502441
			 train-loss:  2.1722877706800188 	 ± 0.19482539062074164
	data : 0.1141202449798584
	model : 0.06491975784301758
			 train-loss:  2.1708241038852267 	 ± 0.19229548825245069
	data : 0.11411967277526855
	model : 0.06493258476257324
			 train-loss:  2.1674521742640316 	 ± 0.19075502134031824
	data : 0.1142298698425293
	model : 0.06501331329345703
			 train-loss:  2.177013961892379 	 ± 0.19700949446580504
	data : 0.11442275047302246
	model : 0.06505031585693359
			 train-loss:  2.176831636673365 	 ± 0.19447057782414603
	data : 0.11451635360717774
	model : 0.06509175300598144
			 train-loss:  2.1722371131181717 	 ± 0.19415615292734248
	data : 0.11446642875671387
	model : 0.06511650085449219
			 train-loss:  2.1737839157988383 	 ± 0.19202313950856822
	data : 0.11440167427062989
	model : 0.06507148742675781
			 train-loss:  2.1692135277248563 	 ± 0.19196714974198528
	data : 0.11433734893798828
	model : 0.06498470306396484
			 train-loss:  2.1623470866402914 	 ± 0.19487071135210518
	data : 0.11408786773681641
	model : 0.06493520736694336
			 train-loss:  2.1594090922312303 	 ± 0.19360450166365675
	data : 0.11424221992492675
	model : 0.06496362686157227
			 train-loss:  2.152659135394626 	 ± 0.19660742221104036
	data : 0.11430540084838867
	model : 0.06498699188232422
			 train-loss:  2.1513137299081553 	 ± 0.19466797100625433
	data : 0.11450395584106446
	model : 0.0650101661682129
			 train-loss:  2.152344267419044 	 ± 0.19271269163945773
	data : 0.11455321311950684
	model : 0.06503510475158691
			 train-loss:  2.1721024264891944 	 ± 0.2339071553982285
	data : 0.11463403701782227
	model : 0.0650212287902832
			 train-loss:  2.178424056695432 	 ± 0.23561451345321135
	data : 0.11444182395935058
	model : 0.06493654251098632
			 train-loss:  2.185846004486084 	 ± 0.2389625391398171
	data : 0.11448345184326172
	model : 0.06483721733093262
			 train-loss:  2.185268883611642 	 ± 0.23664336036686875
	data : 0.11429510116577149
	model : 0.06484332084655761
			 train-loss:  2.17948210468659 	 ± 0.23797265119531563
	data : 0.11428017616271972
	model : 0.06482572555541992
			 train-loss:  2.1762324796532684 	 ± 0.2368788654093792
	data : 0.11437816619873047
	model : 0.06488008499145508
			 train-loss:  2.168722051161307 	 ± 0.24096065995675459
	data : 0.1145737648010254
	model : 0.06501379013061523
			 train-loss:  2.1666289112784645 	 ± 0.2392549954868341
	data : 0.11466197967529297
	model : 0.06511597633361817
			 train-loss:  2.1692372730800082 	 ± 0.2378969369272105
	data : 0.11464447975158691
	model : 0.06511621475219727
			 train-loss:  2.1672923690394352 	 ± 0.23624962897497123
	data : 0.11463384628295899
	model : 0.06511845588684081
			 train-loss:  2.1640254937369248 	 ± 0.23549927775424845
	data : 0.11436338424682617
	model : 0.06501469612121583
			 train-loss:  2.161451984260042 	 ± 0.23431611662623938
	data : 0.11426587104797363
	model : 0.0649306297302246
			 train-loss:  2.1649199108282726 	 ± 0.23387718893021003
	data : 0.11430821418762208
	model : 0.06488137245178223
			 train-loss:  2.1633662219907417 	 ± 0.2322642440696494
	data : 0.11441340446472167
	model : 0.06487693786621093
			 train-loss:  2.1676573388038145 	 ± 0.23280851797124463
	data : 0.11444368362426757
	model : 0.06488075256347656
			 train-loss:  2.1668928483175853 	 ± 0.23103187505623488
	data : 0.11463360786437989
	model : 0.0649406909942627
			 train-loss:  2.162232169881463 	 ± 0.2321857289598418
	data : 0.11457638740539551
	model : 0.06492533683776855
			 train-loss:  2.163051254932697 	 ± 0.230485926826288
	data : 0.1143913745880127
	model : 0.06489930152893067
			 train-loss:  2.164547723351103 	 ± 0.22905112775394287
	data : 0.11422929763793946
	model : 0.06486430168151855
			 train-loss:  2.1697437709836818 	 ± 0.23122131437139018
	data : 0.11422567367553711
	model : 0.06488056182861328
			 train-loss:  2.1761959002298465 	 ± 0.23551280089720059
	data : 0.1142805576324463
	model : 0.06485705375671387
			 train-loss:  2.17561882302381 	 ± 0.23384838142655429
	data : 0.11429643630981445
	model : 0.0649116039276123
			 train-loss:  2.178816074984414 	 ± 0.23368610840467596
	data : 0.11458430290222169
	model : 0.0649329662322998
			 train-loss:  2.17916906551576 	 ± 0.23205338997736802
	data : 0.11469454765319824
	model : 0.06494903564453125
			 train-loss:  2.1801457752784095 	 ± 0.23058318983675286
	data : 0.11465497016906738
	model : 0.0649216651916504
			 train-loss:  2.1760933056269605 	 ± 0.23156573815442347
	data : 0.11454305648803711
	model : 0.06498241424560547
			 train-loss:  2.1706596870680115 	 ± 0.23463445506445005
	data : 0.11444406509399414
	model : 0.06497812271118164
			 train-loss:  2.170862998962402 	 ± 0.2330715383319748
	data : 0.11412925720214843
	model : 0.06500015258789063
			 train-loss:  2.1680698865338375 	 ± 0.2327932239148965
	data : 0.11411433219909668
	model : 0.06503362655639648
			 train-loss:  2.168081014187305 	 ± 0.2312766598819743
	data : 0.11419286727905273
	model : 0.06505684852600098
			 train-loss:  2.165594755074917 	 ± 0.2308226866217586
	data : 0.1143453598022461
	model : 0.0650911808013916
			 train-loss:  2.161996628664717 	 ± 0.231548097762593
	data : 0.11447830200195312
	model : 0.06515674591064453
			 train-loss:  2.1610260620713233 	 ± 0.23025802487475128
	data : 0.11458754539489746
	model : 0.06514649391174317
			 train-loss:  2.158169656624029 	 ± 0.23025405450812333
	data : 0.1145756721496582
	model : 0.06514315605163574
			 train-loss:  2.156938638628983 	 ± 0.22911379165144183
	data : 0.11467876434326171
	model : 0.06512513160705566
			 train-loss:  2.1618501982056952 	 ± 0.23203189956173145
	data : 0.11452975273132324
	model : 0.06505675315856933
			 train-loss:  2.1662013488156453 	 ± 0.23402833750547328
	data : 0.11451201438903809
	model : 0.06493558883666992
			 train-loss:  2.1660026227726656 	 ± 0.23265475670881167
	data : 0.11447477340698242
	model : 0.06492199897766113
			 train-loss:  2.1656532190566837 	 ± 0.23132058874998146
	data : 0.11459898948669434
	model : 0.06495885848999024
			 train-loss:  2.1651806817657646 	 ± 0.23002906208161544
	data : 0.11452698707580566
	model : 0.06499662399291992
			 train-loss:  2.168911122462966 	 ± 0.23134992857505132
	data : 0.11462488174438476
	model : 0.06502361297607422
			 train-loss:  2.165878830331095 	 ± 0.231798515537407
	data : 0.11456212997436524
	model : 0.06509189605712891
			 train-loss:  2.1657317254278396 	 ± 0.23051132644851322
	data : 0.11454472541809083
	model : 0.0651160717010498
			 train-loss:  2.1654537601785346 	 ± 0.22925644846042367
	data : 0.11441683769226074
	model : 0.06504325866699219
			 train-loss:  2.164035236058028 	 ± 0.22840827936145852
	data : 0.1143064022064209
	model : 0.06499409675598145
			 train-loss:  2.163631150799413 	 ± 0.22721001930172646
	data : 0.11423850059509277
	model : 0.06499934196472168
			 train-loss:  2.1619718417208245 	 ± 0.22656401902718376
	data : 0.11442937850952148
	model : 0.0649339199066162
			 train-loss:  2.1645083264300697 	 ± 0.2267061945793789
	data : 0.11452007293701172
	model : 0.06494474411010742
			 train-loss:  2.1651957146823406 	 ± 0.22562183969532698
	data : 0.11455535888671875
	model : 0.06496872901916503
			 train-loss:  2.1663257326047445 	 ± 0.2247287360568659
	data : 0.11448907852172852
	model : 0.06497731208801269
			 train-loss:  2.1699695818278255 	 ± 0.22644115975351517
	data : 0.11456756591796875
	model : 0.06492476463317871
			 train-loss:  2.1693222582942306 	 ± 0.2253857320439669
	data : 0.11436452865600585
	model : 0.06494569778442383
			 train-loss:  2.1688333594799043 	 ± 0.22430872499731086
	data : 0.11411604881286622
	model : 0.06492147445678711
			 train-loss:  2.167217928584259 	 ± 0.2237793631925199
	data : 0.11404891014099121
	model : 0.06491494178771973
			 train-loss:  2.1671006550975873 	 ± 0.22268282258981706
	data : 0.11426939964294433
	model : 0.06489667892456055
			 train-loss:  2.1700942643637795 	 ± 0.22365218335695267
	data : 0.11431479454040527
	model : 0.06490974426269532
			 train-loss:  2.168289012633837 	 ± 0.22332712528482693
	data : 0.11455960273742676
	model : 0.06494154930114746
			 train-loss:  2.1678389322190057 	 ± 0.22230850718974587
	data : 0.11486001014709472
	model : 0.06494746208190919
			 train-loss:  2.167477607727051 	 ± 0.22128837328911338
	data : 0.1148350715637207
	model : 0.06494617462158203
			 train-loss:  2.1665550214107907 	 ± 0.22045661194835628
	data : 0.1147803783416748
	model : 0.06498236656188965
			 train-loss:  2.1648867814629167 	 ± 0.22011108792205117
	data : 0.11447811126708984
	model : 0.06503353118896485
			 train-loss:  2.163484505557139 	 ± 0.21958318373859156
	data : 0.11426491737365722
	model : 0.06504454612731933
			 train-loss:  2.162573289871216 	 ± 0.21878972666756788
	data : 0.11416072845458984
	model : 0.06505985260009765
			 train-loss:  2.165411457285151 	 ± 0.21982666833827705
	data : 0.11431879997253418
	model : 0.06506123542785644
			 train-loss:  2.1662290692329407 	 ± 0.2190125664091329
	data : 0.11427531242370606
	model : 0.06508893966674804
			 train-loss:  2.1628144515299166 	 ± 0.22101560732476985
	data : 0.1144554615020752
	model : 0.06508607864379883
			 train-loss:  2.162146535881779 	 ± 0.22015862218001075
	data : 0.1145434856414795
	model : 0.06503281593322754
			 train-loss:  2.1609615522882213 	 ± 0.2195641579385764
	data : 0.1145392894744873
	model : 0.06500706672668458
			 train-loss:  2.1585808324402778 	 ± 0.22010140783914917
	data : 0.11427350044250488
	model : 0.065024995803833
			 train-loss:  2.1577052448549843 	 ± 0.2193615848052186
	data : 0.11421098709106445
	model : 0.06496796607971192
			 train-loss:  2.1584241077051325 	 ± 0.21856846487596665
	data : 0.11425971984863281
	model : 0.06494002342224121
			 train-loss:  2.156006887179463 	 ± 0.21922635721437936
	data : 0.11428308486938477
	model : 0.06495976448059082
			 train-loss:  2.156209534406662 	 ± 0.21832219518092652
	data : 0.11428451538085938
	model : 0.06494417190551757
			 train-loss:  2.156746870230052 	 ± 0.21749783062550485
	data : 0.11438531875610351
	model : 0.06493492126464843
			 train-loss:  2.155795722711282 	 ± 0.21685715203373873
	data : 0.11442508697509765
	model : 0.06496715545654297
			 train-loss:  2.1546982943527095 	 ± 0.21631371069546415
	data : 0.11443705558776855
	model : 0.0649479866027832
			 train-loss:  2.15257992379127 	 ± 0.21671693840318929
	data : 0.11429347991943359
	model : 0.06494288444519043
			 train-loss:  2.1545242795944213 	 ± 0.21693152365756269
	data : 0.11428537368774414
	model : 0.06492934226989747
			 train-loss:  2.156536184606098 	 ± 0.21723666767356187
	data : 0.1144646167755127
	model : 0.06491599082946778
			 train-loss:  2.161390919384994 	 ± 0.22313629403332041
	data : 0.11445803642272949
	model : 0.06490645408630372
			 train-loss:  2.162088102661073 	 ± 0.22240178293321536
	data : 0.11444635391235351
	model : 0.0649301528930664
			 train-loss:  2.162316774213037 	 ± 0.2215531891328373
	data : 0.11452364921569824
	model : 0.0649043083190918
			 train-loss:  2.1597228921376743 	 ± 0.2226570756620758
	data : 0.1146127700805664
	model : 0.06494479179382324
			 train-loss:  2.160362936158217 	 ± 0.2219256285252823
	data : 0.1145108699798584
	model : 0.06491332054138184
			 train-loss:  2.160618221217936 	 ± 0.22110271008551005
	data : 0.11444916725158691
	model : 0.06487498283386231
			 train-loss:  2.15948956801479 	 ± 0.2206512875322605
	data : 0.11437735557556153
	model : 0.06487903594970704
			 train-loss:  2.161025329312282 	 ± 0.22053875801248893
	data : 0.11447277069091796
	model : 0.06492314338684083
			 train-loss:  2.161181335979038 	 ± 0.21972785091514022
	data : 0.11440887451171874
	model : 0.06492891311645507
			 train-loss:  2.1630740998422398 	 ± 0.22002038765357776
	data : 0.11443758010864258
	model : 0.06499195098876953
			 train-loss:  2.164571493211454 	 ± 0.2199103413865818
	data : 0.1144526481628418
	model : 0.06505637168884278
			 train-loss:  2.163261011890743 	 ± 0.2196483505921864
	data : 0.11454415321350098
	model : 0.06503047943115234
			 train-loss:  2.1643997379344144 	 ± 0.21926525696189123
	data : 0.11436476707458496
	model : 0.06497550010681152
			 train-loss:  2.165050595998764 	 ± 0.21861547661538772
	data : 0.11429991722106933
	model : 0.06490674018859863
			 train-loss:  2.165980870842088 	 ± 0.21811677763903653
	data : 0.11419944763183594
	model : 0.06487250328063965
			 train-loss:  2.167394459247589 	 ± 0.21799459790548145
	data : 0.11441531181335449
	model : 0.06482896804809571
			 train-loss:  2.1658662457566162 	 ± 0.21799302168065238
	data : 0.1144449234008789
	model : 0.06485252380371094
			 train-loss:  2.1683902367949486 	 ± 0.2193215373625034
	data : 0.11452775001525879
	model : 0.06490163803100586
			 train-loss:  2.166793648127852 	 ± 0.21940207152570598
	data : 0.11451177597045899
	model : 0.06496310234069824
			 train-loss:  2.1683375647623246 	 ± 0.21943836278368117
	data : 0.1145862102508545
	model : 0.06498632431030274
			 train-loss:  2.1697904504075343 	 ± 0.21939418931134083
	data : 0.11431231498718261
	model : 0.06497516632080078
			 train-loss:  2.170348752994795 	 ± 0.2187564902284908
	data : 0.11421728134155273
	model : 0.06491026878356934
			 train-loss:  2.1705862859751552 	 ± 0.2180403219072838
	data : 0.11423687934875489
	model : 0.06486759185791016
			 train-loss:  2.170465306440989 	 ± 0.2173173229834377
	data : 0.11437935829162597
	model : 0.06483759880065917
			 train-loss:  2.1689421885850413 	 ± 0.21739834792218402
	data : 0.11439166069030762
	model : 0.0648573875427246
			 train-loss:  2.167404939469538 	 ± 0.21750388630062029
	data : 0.11454153060913086
	model : 0.06490206718444824
			 train-loss:  2.169693911776823 	 ± 0.2186209648043673
	data : 0.1145096778869629
	model : 0.06494016647338867
			 train-loss:  2.1716754862240384 	 ± 0.21928416220663333
	data : 0.11460175514221191
	model : 0.0649531364440918
			 train-loss:  2.171785664558411 	 ± 0.21857992571763538
	data : 0.11441516876220703
	model : 0.064923095703125
			 train-loss:  2.1707520324450273 	 ± 0.21825792358079096
	data : 0.11433749198913574
	model : 0.06487116813659669
			 train-loss:  2.171247686550116 	 ± 0.21764978488391032
	data : 0.11431355476379394
	model : 0.06484694480895996
			 train-loss:  2.1709666410578956 	 ± 0.21698850333899686
	data : 0.11439733505249024
	model : 0.06487469673156739
			 train-loss:  2.1694210370381675 	 ± 0.2171758035112148
	data : 0.11435823440551758
	model : 0.06491026878356934
			 train-loss:  2.1749230682849885 	 ± 0.2273408226150067
	data : 0.11444277763366699
	model : 0.06497745513916016
			 train-loss:  2.175364747550917 	 ± 0.22670254700720036
	data : 0.11443657875061035
	model : 0.06499476432800293
			 train-loss:  2.1753054636496083 	 ± 0.22600301652155308
	data : 0.11441226005554199
	model : 0.06496391296386719
			 train-loss:  2.17558461756794 	 ± 0.22533670268764622
	data : 0.11430292129516602
	model : 0.0648874282836914
			 train-loss:  2.1746014400226312 	 ± 0.2249990622005004
	data : 0.11420741081237792
	model : 0.06483030319213867
			 train-loss:  2.1726943890253705 	 ± 0.22564176187471477
	data : 0.1143115520477295
	model : 0.06479663848876953
			 train-loss:  2.170750110982412 	 ± 0.22634316523430095
	data : 0.11450071334838867
	model : 0.06482520103454589
			 train-loss:  2.1694192822108014 	 ± 0.22631495330987353
	data : 0.11451234817504882
	model : 0.06490325927734375
			 train-loss:  2.169422719450224 	 ± 0.2256403959629921
	data : 0.11467704772949219
	model : 0.06500000953674316
			 train-loss:  2.1682661757666684 	 ± 0.22547070791424267
	data : 0.1146176815032959
	model : 0.06503558158874512
			 train-loss:  2.167542400780846 	 ± 0.22500339864927188
	data : 0.11458148956298828
	model : 0.06504459381103515
			 train-loss:  2.1704588152511777 	 ± 0.22754427766905438
	data : 0.11438350677490235
	model : 0.06496453285217285
			 train-loss:  2.1736450839874357 	 ± 0.23067600108165057
	data : 0.11433744430541992
	model : 0.06489834785461426
			 train-loss:  2.1740226463086345 	 ± 0.23006163564551144
	data : 0.11429758071899414
	model : 0.06484823226928711
			 train-loss:  2.1753013634133613 	 ± 0.230015315917892
	data : 0.11449966430664063
	model : 0.06483826637268067
			 train-loss:  2.1759247391564505 	 ± 0.22950454356149003
	data : 0.11451611518859864
	model : 0.06492266654968262
			 train-loss:  2.1757354228333994 	 ± 0.22886531628675896
	data : 0.11464242935180664
	model : 0.06500773429870606
			 train-loss:  2.1765387563382164 	 ± 0.2284665954460929
	data : 0.11458883285522461
	model : 0.06500687599182128
			 train-loss:  2.1758236583699 	 ± 0.22802248873325037
	data : 0.11456875801086426
	model : 0.06506495475769043
			 train-loss:  2.1758206057148937 	 ± 0.22738466598795504
	data : 0.1143918514251709
	model : 0.0650360107421875
			 train-loss:  2.1753092534012266 	 ± 0.2268553464255598
	data : 0.1142920970916748
	model : 0.06493501663208008
			 train-loss:  2.1752553717207515 	 ± 0.22622896121768116
	data : 0.11421656608581543
	model : 0.06489605903625488
			 train-loss:  2.174932598412692 	 ± 0.22564838502882167
	data : 0.11441597938537598
	model : 0.06491217613220215
			 train-loss:  2.1739028509848755 	 ± 0.2254594126498162
	data : 0.11442766189575196
	model : 0.06494259834289551
			 train-loss:  2.175392824022666 	 ± 0.22574753546558557
	data : 0.11449451446533203
	model : 0.06498847007751465
			 train-loss:  2.17585423379331 	 ± 0.22522356266985397
	data : 0.1145514965057373
	model : 0.06500892639160157
			 train-loss:  2.174838323106048 	 ± 0.22504192587880142
	data : 0.11447606086730958
	model : 0.06499238014221191
			 train-loss:  2.174521436665785 	 ± 0.22448100842283827
	data : 0.11425971984863281
	model : 0.06494932174682617
			 train-loss:  2.174415335376212 	 ± 0.22388788982754598
	data : 0.11420435905456543
	model : 0.06485786437988281
			 train-loss:  2.1737261846582725 	 ± 0.22349464871433208
	data : 0.11425929069519043
	model : 0.0648573875427246
			 train-loss:  2.175802464861619 	 ± 0.2247259032170055
	data : 0.11426138877868652
	model : 0.06486654281616211
			 train-loss:  2.177464393420993 	 ± 0.22530447365070552
	data : 0.11440458297729492
	model : 0.06494460105895997
			 train-loss:  2.1799569111317396 	 ± 0.22734189008165548
	data : 0.11449098587036133
	model : 0.0649561882019043
			 train-loss:  2.1830869678388605 	 ± 0.23086275893298058
	data : 0.11452255249023438
	model : 0.06495132446289062
			 train-loss:  2.1849902382830986 	 ± 0.23178009859334828
	data : 0.11426343917846679
	model : 0.06493768692016602
			 train-loss:  2.184533975674556 	 ± 0.23127235601320792
	data : 0.11419410705566406
	model : 0.06493802070617676
			 train-loss:  2.1879193825381145 	 ± 0.23547590391294232
	data : 0.11412391662597657
	model : 0.06486010551452637
			 train-loss:  2.1866793130254987 	 ± 0.23551823328370072
	data : 0.11414413452148438
	model : 0.06489801406860352
			 train-loss:  2.186850146211759 	 ± 0.23493497356742793
	data : 0.11442008018493652
	model : 0.06490750312805176
			 train-loss:  2.1883775630787987 	 ± 0.23532746812400443
	data : 0.11468386650085449
	model : 0.06492033004760742
			 train-loss:  2.1877738851308823 	 ± 0.23489283333742095
	data : 0.1146399974822998
	model : 0.0649141788482666
			 train-loss:  2.1889664349864373 	 ± 0.2349139788788075
	data : 0.11468148231506348
	model : 0.06496429443359375
			 train-loss:  2.1906831683498798 	 ± 0.2355923775438372
	data : 0.11460623741149903
	model : 0.06490902900695801
			 train-loss:  2.1898835885700922 	 ± 0.2352859855561809
	data : 0.11417312622070312
	model : 0.0649378776550293
			 train-loss:  2.1915206681279575 	 ± 0.2358647301098887
	data : 0.11414666175842285
	model : 0.06490821838378906
			 train-loss:  2.192409174035235 	 ± 0.23563072971662818
	data : 0.11425790786743165
	model : 0.0648848533630371
			 train-loss:  2.1900431328606835 	 ± 0.23748671164921753
	data : 0.11429400444030761
	model : 0.06496424674987793
			 train-loss:  2.190117258380577 	 ± 0.23691476658587862
	data : 0.11441154479980468
	model : 0.06502833366394042
			 train-loss:  2.1898431508586955 	 ± 0.23637747457692987
	data : 0.11454238891601562
	model : 0.06501755714416504
			 train-loss:  2.1893330287705197 	 ± 0.23592603924575753
	data : 0.11457347869873047
	model : 0.06502985954284668
			 train-loss:  2.1888027185485477 	 ± 0.2354884710613505
	data : 0.11446423530578613
	model : 0.06502676010131836
			 train-loss:  2.188090381464122 	 ± 0.23515645858928597
	data : 0.1144444465637207
	model : 0.06491475105285645
			 train-loss:  2.186307858746007 	 ± 0.236025732622087
	data : 0.11444463729858398
	model : 0.06493158340454101
			 train-loss:  2.1861416868200885 	 ± 0.23548345981232993
	data : 0.11447706222534179
	model : 0.06497774124145508
			 train-loss:  2.185256961350129 	 ± 0.23528718545966615
	data : 0.11446795463562012
	model : 0.06499099731445312
			 train-loss:  2.184878358175588 	 ± 0.23480469720574826
	data : 0.11465411186218262
	model : 0.06505818367004394
			 train-loss:  2.185605567914468 	 ± 0.23450308874797507
	data : 0.11508069038391114
	model : 0.06505589485168457
			 train-loss:  2.184119175106699 	 ± 0.23497979317242254
	data : 0.1149296760559082
	model : 0.06505317687988281
			 train-loss:  2.1830751135808613 	 ± 0.2349441743574827
	data : 0.11543197631835937
	model : 0.06495037078857421
			 train-loss:  2.1851731734733058 	 ± 0.2364451741251058
	data : 0.11540794372558594
	model : 0.06486515998840332
			 train-loss:  2.186239173737439 	 ± 0.23643405595714323
	data : 0.11521763801574707
	model : 0.06474127769470214
			 train-loss:  2.1866319983253653 	 ± 0.23597047545246974
	data : 0.11473746299743652
	model : 0.0646857738494873
			 train-loss:  2.186289562298371 	 ± 0.2354934397578158
	data : 0.11499485969543458
	model : 0.06459965705871581
			 train-loss:  2.1856338181303223 	 ± 0.23516788352995668
	data : 0.11460456848144532
	model : 0.06462216377258301
			 train-loss:  2.185016535754715 	 ± 0.23482336394278813
	data : 0.11461749076843261
	model : 0.0648238182067871
			 train-loss:  2.184602861934238 	 ± 0.2343827401368443
	data : 0.11465339660644532
	model : 0.06474566459655762
			 train-loss:  2.187112174730385 	 ± 0.23687325486979902
	data : 0.11481385231018067
	model : 0.06458392143249511
			 train-loss:  2.1894909416526427 	 ± 0.23904098221509773
	data : 0.11457138061523438
	model : 0.06436991691589355
			 train-loss:  2.1900152464707694 	 ± 0.23864696905363791
	data : 0.11458110809326172
	model : 0.06420164108276367
			 train-loss:  2.1894633546146243 	 ± 0.23827110777615487
	data : 0.11469020843505859
	model : 0.06386523246765137
			 train-loss:  2.189025945767112 	 ± 0.23784468653659877
	data : 0.11493091583251953
	model : 0.06389327049255371
			 train-loss:  2.189935424627164 	 ± 0.2377297786168191
	data : 0.1149223804473877
	model : 0.06393418312072754
			 train-loss:  2.188978995742469 	 ± 0.23766185065894432
	data : 0.11514401435852051
	model : 0.06403064727783203
			 train-loss:  2.1900050322896933 	 ± 0.2376656822395759
	data : 0.11529927253723145
	model : 0.0639963150024414
			 train-loss:  2.19027848325224 	 ± 0.2371940353398833
	data : 0.11517877578735351
	model : 0.06397547721862792
			 train-loss:  2.1902256204726847 	 ± 0.2366902105130478
	data : 0.11494483947753906
	model : 0.06393394470214844
			 train-loss:  2.1923770298392085 	 ± 0.2384797380171674
	data : 0.11501121520996094
	model : 0.06388444900512695
			 train-loss:  2.1912989933279494 	 ± 0.23855164457954897
	data : 0.11507601737976074
	model : 0.06391425132751465
			 train-loss:  2.1924113430896726 	 ± 0.23866509563827648
	data : 0.11487803459167481
	model : 0.06391215324401855
			 train-loss:  2.192607535976745 	 ± 0.23818450459235527
	data : 0.11499595642089844
	model : 0.06394305229187011
			 train-loss:  2.192851814130942 	 ± 0.23771776763519611
	data : 0.11512589454650879
	model : 0.06384878158569336
			 train-loss:  2.1942796593385117 	 ± 0.2382531319365363
	data : 0.1150942325592041
	model : 0.0638735294342041
			 train-loss:  2.194029191801371 	 ± 0.23779215607778345
	data : 0.1150437831878662
	model : 0.0638500690460205
			 train-loss:  2.1954775999602956 	 ± 0.23836967514228086
	data : 0.11510591506958008
	model : 0.06389570236206055
			 train-loss:  2.197070397803041 	 ± 0.23917300216374884
	data : 0.11511774063110351
	model : 0.06393957138061523
			 train-loss:  2.197951072089526 	 ± 0.23908049616992302
	data : 0.11516227722167968
	model : 0.06399884223937988
			 train-loss:  2.1991743445396423 	 ± 0.23936111822648845
	data : 0.11501078605651856
	model : 0.06401138305664063
			 train-loss:  2.198905751290109 	 ± 0.23891323414844973
	data : 0.11492180824279785
	model : 0.06394028663635254
			 train-loss:  2.198499804062228 	 ± 0.23851641023354103
	data : 0.1150467872619629
	model : 0.06394920349121094
			 train-loss:  2.1982696913332345 	 ± 0.23806456226270348
	data : 0.11511507034301757
	model : 0.06392264366149902
			 train-loss:  2.1968013005256655 	 ± 0.23871515155407713
	data : 0.11516270637512208
	model : 0.06395068168640136
			 train-loss:  2.1964489744004023 	 ± 0.2383042707088313
	data : 0.11523785591125488
	model : 0.06397037506103516
			 train-loss:  2.196450831871184 	 ± 0.23783097658914218
	data : 0.11537141799926758
	model : 0.0640068531036377
			 train-loss:  2.19609478763912 	 ± 0.23742777301248502
	data : 0.11503067016601562
	model : 0.06401891708374023
			 train-loss:  2.1956812091699733 	 ± 0.2370512297497648
	data : 0.11488490104675293
	model : 0.06399450302124024
			 train-loss:  2.1950410707324157 	 ± 0.23680583420593918
	data : 0.11493096351623536
	model : 0.06399898529052735
			 train-loss:  2.193829837720841 	 ± 0.23713299875740732
	data : 0.11473207473754883
	model : 0.05560345649719238
#epoch  48    val-loss:  2.439038163737247  train-loss:  2.193829837720841  lr:  6.103515625e-07
			 train-loss:  2.086786985397339 	 ± 0.0
	data : 5.599997043609619
	model : 0.07321023941040039
			 train-loss:  2.324111580848694 	 ± 0.23732459545135498
	data : 2.861193060874939
	model : 0.07014584541320801
			 train-loss:  2.2812508742014566 	 ± 0.20303379707009514
	data : 1.946630875269572
	model : 0.06832472483317058
			 train-loss:  2.29070782661438 	 ± 0.17659372557185124
	data : 1.4886256456375122
	model : 0.06736230850219727
			 train-loss:  2.2617024898529055 	 ± 0.16826619794736924
	data : 1.2137128353118896
	model : 0.06676826477050782
			 train-loss:  2.2451655069986978 	 ± 0.15799352355709959
	data : 0.11636853218078613
	model : 0.06505646705627441
			 train-loss:  2.2224742685045515 	 ± 0.15647783854986005
	data : 0.11474967002868652
	model : 0.06457681655883789
			 train-loss:  2.201477825641632 	 ± 0.156558628005142
	data : 0.11411547660827637
	model : 0.06461801528930664
			 train-loss:  2.2001334561242 	 ± 0.14765385939177567
	data : 0.11403942108154297
	model : 0.06471948623657227
			 train-loss:  2.198124074935913 	 ± 0.1402063996000047
	data : 0.11399040222167969
	model : 0.06483931541442871
			 train-loss:  2.200070684606379 	 ± 0.13382320993859723
	data : 0.11410961151123047
	model : 0.06485991477966309
			 train-loss:  2.1672924757003784 	 ± 0.16803209414182385
	data : 0.11409249305725097
	model : 0.06491131782531738
			 train-loss:  2.177612121288593 	 ± 0.1653505947072631
	data : 0.1140028953552246
	model : 0.06483888626098633
			 train-loss:  2.1741531235831126 	 ± 0.15982316528490823
	data : 0.11397857666015625
	model : 0.06479592323303222
			 train-loss:  2.2000880082448324 	 ± 0.1823655759129866
	data : 0.11407055854797363
	model : 0.06475062370300293
			 train-loss:  2.1919117271900177 	 ± 0.17939175469208557
	data : 0.11415581703186035
	model : 0.06479096412658691
			 train-loss:  2.2162921288434196 	 ± 0.19949647059896966
	data : 0.11414523124694824
	model : 0.06476802825927734
			 train-loss:  2.228113545311822 	 ± 0.19990866689625955
	data : 0.11424055099487304
	model : 0.0648129940032959
			 train-loss:  2.2177499093507467 	 ± 0.1994828900219181
	data : 0.11428451538085938
	model : 0.06486315727233886
			 train-loss:  2.23789359331131 	 ± 0.21333856732311637
	data : 0.11411209106445312
	model : 0.06484646797180176
			 train-loss:  2.227392548606509 	 ± 0.21342792927694412
	data : 0.1139681339263916
	model : 0.06484880447387695
			 train-loss:  2.2165454517711294 	 ± 0.21436373022713823
	data : 0.1140364646911621
	model : 0.0648836612701416
			 train-loss:  2.209201481031335 	 ± 0.2124628203753482
	data : 0.11400132179260254
	model : 0.06492171287536622
			 train-loss:  2.218496322631836 	 ± 0.2127126272826176
	data : 0.1140481948852539
	model : 0.06488666534423829
			 train-loss:  2.2254833221435546 	 ± 0.2112070813810395
	data : 0.114300537109375
	model : 0.06490330696105957
			 train-loss:  2.21389915392949 	 ± 0.21505242654409223
	data : 0.11447658538818359
	model : 0.06490855216979981
			 train-loss:  2.203706401365775 	 ± 0.2173381499135919
	data : 0.11443796157836914
	model : 0.06490416526794433
			 train-loss:  2.221922810588564 	 ± 0.23347053401807866
	data : 0.11445302963256836
	model : 0.06484456062316894
			 train-loss:  2.216870698435553 	 ± 0.2309622362730347
	data : 0.1143880844116211
	model : 0.06489238739013672
			 train-loss:  2.214963940779368 	 ± 0.227312278850314
	data : 0.11439123153686523
	model : 0.06491098403930665
			 train-loss:  2.2188058707021896 	 ± 0.22460383468621486
	data : 0.11430468559265136
	model : 0.06490116119384766
			 train-loss:  2.2197730503976345 	 ± 0.22113212342250724
	data : 0.11431937217712403
	model : 0.06490111351013184
			 train-loss:  2.211739659309387 	 ± 0.22244717898923927
	data : 0.11430363655090332
	model : 0.0649643898010254
			 train-loss:  2.223331027171191 	 ± 0.22904420143175325
	data : 0.11435699462890625
	model : 0.06492338180541993
			 train-loss:  2.2207393884658813 	 ± 0.22625365756324836
	data : 0.11427426338195801
	model : 0.0649332046508789
			 train-loss:  2.22073260611958 	 ± 0.22308911850215363
	data : 0.11416068077087402
	model : 0.06484131813049317
			 train-loss:  2.21696370678979 	 ± 0.22121261037933265
	data : 0.11412944793701171
	model : 0.06479954719543457
			 train-loss:  2.2215029409057214 	 ± 0.22002187902223852
	data : 0.11428837776184082
	model : 0.06477961540222169
			 train-loss:  2.222458371749291 	 ± 0.2172626129425078
	data : 0.11431145668029785
	model : 0.06478252410888671
			 train-loss:  2.22541905939579 	 ± 0.21532493606246805
	data : 0.11431012153625489
	model : 0.06478185653686523
			 train-loss:  2.222859536729208 	 ± 0.2132979720389265
	data : 0.11446614265441894
	model : 0.06489171981811523
			 train-loss:  2.2175919924463545 	 ± 0.21342542988604116
	data : 0.11448588371276855
	model : 0.06492700576782226
			 train-loss:  2.211940662805424 	 ± 0.2140852126783962
	data : 0.1142805576324463
	model : 0.06494255065917968
			 train-loss:  2.209440152753483 	 ± 0.21227268012181874
	data : 0.11416401863098144
	model : 0.06490693092346192
			 train-loss:  2.2050474034415353 	 ± 0.2119136595698712
	data : 0.11414513587951661
	model : 0.06490697860717773
			 train-loss:  2.2090596193852634 	 ± 0.21131861195968044
	data : 0.11423611640930176
	model : 0.0648460865020752
			 train-loss:  2.2160039582151048 	 ± 0.21429823675182696
	data : 0.11421051025390624
	model : 0.06487255096435547
			 train-loss:  2.2209967946012816 	 ± 0.21479903473782164
	data : 0.1142662525177002
	model : 0.06487674713134765
			 train-loss:  2.224750983471773 	 ± 0.2141810672339185
	data : 0.11434721946716309
	model : 0.06491851806640625
			 train-loss:  2.2219709658622744 	 ± 0.21291959836468363
	data : 0.11443228721618652
	model : 0.06489171981811523
			 train-loss:  2.222054628764882 	 ± 0.21082264719409205
	data : 0.11432280540466308
	model : 0.0648953914642334
			 train-loss:  2.219989939377858 	 ± 0.20930567175750028
	data : 0.11429004669189453
	model : 0.06483025550842285
			 train-loss:  2.223238038566877 	 ± 0.20864057515178494
	data : 0.11427721977233887
	model : 0.06480140686035156
			 train-loss:  2.2205024140852467 	 ± 0.20765691658027388
	data : 0.1142662525177002
	model : 0.06479992866516113
			 train-loss:  2.2266837965358386 	 ± 0.21071469347425845
	data : 0.11422224044799804
	model : 0.06484570503234863
			 train-loss:  2.2298797347715924 	 ± 0.21016561062665062
	data : 0.11427764892578125
	model : 0.06485128402709961
			 train-loss:  2.2316974275990535 	 ± 0.20875752277124188
	data : 0.11438326835632324
	model : 0.06487398147583008
			 train-loss:  2.226628615938384 	 ± 0.21045859845264148
	data : 0.11448040008544921
	model : 0.06489696502685546
			 train-loss:  2.226821770102291 	 ± 0.20867261383937713
	data : 0.1143707275390625
	model : 0.06485657691955567
			 train-loss:  2.225098923842112 	 ± 0.2073490917721086
	data : 0.11439762115478516
	model : 0.06484007835388184
			 train-loss:  2.2245524398616103 	 ± 0.20568604881381397
	data : 0.11441078186035156
	model : 0.06486468315124512
			 train-loss:  2.2216334304501935 	 ± 0.2052903828795242
	data : 0.11441073417663575
	model : 0.06489453315734864
			 train-loss:  2.22201346972632 	 ± 0.20367656058013664
	data : 0.11445612907409668
	model : 0.06489601135253906
			 train-loss:  2.2215653769671917 	 ± 0.2021103688831576
	data : 0.11448898315429687
	model : 0.064918851852417
			 train-loss:  2.2222499370574953 	 ± 0.2006244078684656
	data : 0.1144824504852295
	model : 0.06489171981811523
			 train-loss:  2.2203995206139306 	 ± 0.19965686788838133
	data : 0.11443758010864258
	model : 0.0648573875427246
			 train-loss:  2.2202077445699206 	 ± 0.19816741435868582
	data : 0.11432094573974609
	model : 0.06482539176940919
			 train-loss:  2.2206398248672485 	 ± 0.19673669663769952
	data : 0.11417441368103028
	model : 0.06479110717773437
			 train-loss:  2.2189234201458916 	 ± 0.19581805865531673
	data : 0.11428246498107911
	model : 0.06484651565551758
			 train-loss:  2.223369152205331 	 ± 0.19789058418411612
	data : 0.11428279876708984
	model : 0.06485910415649414
			 train-loss:  2.2243562248391164 	 ± 0.19666551872607269
	data : 0.11422557830810547
	model : 0.06488604545593261
			 train-loss:  2.2252402537398868 	 ± 0.1954370184300416
	data : 0.11434273719787598
	model : 0.06487140655517579
			 train-loss:  2.229589282649837 	 ± 0.19757077761330255
	data : 0.11417803764343262
	model : 0.06487970352172852
			 train-loss:  2.2247778193370715 	 ± 0.20049111176173653
	data : 0.11412653923034669
	model : 0.0648453712463379
			 train-loss:  2.2235925594965615 	 ± 0.19941085303799938
	data : 0.11406311988830567
	model : 0.06485915184020996
			 train-loss:  2.2216832088796714 	 ± 0.19878352600175728
	data : 0.11414732933044433
	model : 0.06491279602050781
			 train-loss:  2.218151722635542 	 ± 0.19987379979473568
	data : 0.1142129898071289
	model : 0.06500067710876464
			 train-loss:  2.216292138283069 	 ± 0.19925770364817647
	data : 0.11448264122009277
	model : 0.06506857872009278
			 train-loss:  2.2135518713842464 	 ± 0.1994661919369243
	data : 0.11448392868041993
	model : 0.06509294509887695
			 train-loss:  2.2163137823343275 	 ± 0.19972994572633151
	data : 0.11462793350219727
	model : 0.06508698463439941
			 train-loss:  2.213918259114395 	 ± 0.19964628541527651
	data : 0.11457371711730957
	model : 0.06504635810852051
			 train-loss:  2.212394810304409 	 ± 0.19889834315975527
	data : 0.11444835662841797
	model : 0.06499242782592773
			 train-loss:  2.211529108415167 	 ± 0.19785189366944766
	data : 0.11433324813842774
	model : 0.06494288444519043
			 train-loss:  2.208715712740308 	 ± 0.1983338516260626
	data : 0.11432695388793945
	model : 0.0649141788482666
			 train-loss:  2.204905926480013 	 ± 0.20023174597869287
	data : 0.11422591209411621
	model : 0.06493048667907715
			 train-loss:  2.2017556359601573 	 ± 0.20117188020794308
	data : 0.11432738304138183
	model : 0.0649637222290039
			 train-loss:  2.197695137440473 	 ± 0.20352613905594968
	data : 0.11431384086608887
	model : 0.06499900817871093
			 train-loss:  2.201618346300992 	 ± 0.2056483431091214
	data : 0.11444940567016601
	model : 0.06500244140625
			 train-loss:  2.20297299074323 	 ± 0.2048842208610181
	data : 0.11436080932617188
	model : 0.06497712135314941
			 train-loss:  2.201372159851922 	 ± 0.20430174620405925
	data : 0.11424932479858399
	model : 0.06495614051818847
			 train-loss:  2.2019723745492787 	 ± 0.2032558833774181
	data : 0.11406869888305664
	model : 0.06490058898925781
			 train-loss:  2.199646514395009 	 ± 0.2033621798538984
	data : 0.11415209770202636
	model : 0.06484031677246094
			 train-loss:  2.1977252473113356 	 ± 0.20310362554947986
	data : 0.11409320831298828
	model : 0.06490077972412109
			 train-loss:  2.19697175888305 	 ± 0.20215103678531704
	data : 0.11425275802612304
	model : 0.06494374275207519
			 train-loss:  2.1991019650509482 	 ± 0.2021421146170269
	data : 0.11442546844482422
	model : 0.06493058204650878
			 train-loss:  2.1965875836710134 	 ± 0.20257441932009376
	data : 0.11461544036865234
	model : 0.06492781639099121
			 train-loss:  2.2079173990131653 	 ± 0.2300790134856988
	data : 0.11449885368347168
	model : 0.06490035057067871
			 train-loss:  2.2075902892618764 	 ± 0.22892480132441642
	data : 0.1144704818725586
	model : 0.06479506492614746
			 train-loss:  2.2055616824313846 	 ± 0.22864929367558853
	data : 0.11436676979064941
	model : 0.06477837562561035
			 train-loss:  2.2047294533252715 	 ± 0.22765382146272703
	data : 0.11435222625732422
	model : 0.06478891372680665
			 train-loss:  2.2022225951204204 	 ± 0.2279069216179092
	data : 0.11430482864379883
	model : 0.06481285095214843
			 train-loss:  2.1995378019762972 	 ± 0.2283864125328188
	data : 0.11451630592346192
	model : 0.06490297317504883
			 train-loss:  2.199964188834996 	 ± 0.2273158296689845
	data : 0.11452221870422363
	model : 0.06495795249938965
			 train-loss:  2.1980108320713043 	 ± 0.22708730184605275
	data : 0.1145510196685791
	model : 0.06495919227600097
			 train-loss:  2.198174948919387 	 ± 0.22600954382717878
	data : 0.11462388038635254
	model : 0.06494460105895997
			 train-loss:  2.197302708085978 	 ± 0.22511843232994205
	data : 0.11445331573486328
	model : 0.0648808479309082
			 train-loss:  2.197428092778286 	 ± 0.22406772632983918
	data : 0.11433162689208984
	model : 0.06490840911865234
			 train-loss:  2.198365359394639 	 ± 0.2232385911098131
	data : 0.11429400444030761
	model : 0.06488885879516601
			 train-loss:  2.1970788491975277 	 ± 0.22261404749865896
	data : 0.11434974670410156
	model : 0.06489272117614746
			 train-loss:  2.2004428018223154 	 ± 0.2243656817672339
	data : 0.11427946090698242
	model : 0.06494050025939942
			 train-loss:  2.1994470261238717 	 ± 0.22359677686834734
	data : 0.11445670127868653
	model : 0.06505789756774902
			 train-loss:  2.201120306338583 	 ± 0.2232933390585001
	data : 0.11443381309509278
	model : 0.06494522094726562
			 train-loss:  2.200185581646134 	 ± 0.2225231058289412
	data : 0.11448850631713867
	model : 0.0649726390838623
			 train-loss:  2.197929473299729 	 ± 0.2228392916179351
	data : 0.11439990997314453
	model : 0.06492314338684083
			 train-loss:  2.19710359469704 	 ± 0.2220434719811399
	data : 0.11432557106018067
	model : 0.06488933563232421
			 train-loss:  2.1970615726092766 	 ± 0.22108477569667873
	data : 0.11430859565734863
	model : 0.06480026245117188
			 train-loss:  2.196101631873693 	 ± 0.22038059278501182
	data : 0.11430377960205078
	model : 0.06483521461486816
			 train-loss:  2.194450743117575 	 ± 0.22017014225097556
	data : 0.11434574127197265
	model : 0.06484899520874024
			 train-loss:  2.193345645896527 	 ± 0.21957150501259745
	data : 0.11433944702148438
	model : 0.06489758491516114
			 train-loss:  2.194451740384102 	 ± 0.21898737875782343
	data : 0.11442337036132813
	model : 0.06492195129394532
			 train-loss:  2.1931270085090446 	 ± 0.21856288702518792
	data : 0.11430964469909669
	model : 0.06492657661437988
			 train-loss:  2.1914659992593233 	 ± 0.2184307980439585
	data : 0.11436519622802735
	model : 0.06491961479187011
			 train-loss:  2.1893197065446435 	 ± 0.21882895790993345
	data : 0.11431503295898438
	model : 0.06487975120544434
			 train-loss:  2.1893977071008375 	 ± 0.21794651369872056
	data : 0.11450004577636719
	model : 0.06486167907714843
			 train-loss:  2.1858677673339844 	 ± 0.22060320995104066
	data : 0.11451230049133301
	model : 0.0649254322052002
			 train-loss:  2.1878480532812694 	 ± 0.2208387001307947
	data : 0.11468443870544434
	model : 0.06495943069458007
			 train-loss:  2.188227584042887 	 ± 0.2200087892227713
	data : 0.11460433006286622
	model : 0.06496949195861816
			 train-loss:  2.18590257037431 	 ± 0.22070848322779738
	data : 0.11457381248474122
	model : 0.06500582695007324
			 train-loss:  2.1850801632385846 	 ± 0.22004816198926588
	data : 0.11450839042663574
	model : 0.0650146484375
			 train-loss:  2.1850755682358374 	 ± 0.21920019525220835
	data : 0.11429481506347657
	model : 0.06487503051757812
			 train-loss:  2.185700015257333 	 ± 0.21847799171650337
	data : 0.114288330078125
	model : 0.06485013961791992
			 train-loss:  2.18388831615448 	 ± 0.21863439170493768
	data : 0.1143871784210205
	model : 0.0648508071899414
			 train-loss:  2.182895006093764 	 ± 0.21810967655209182
	data : 0.11439518928527832
	model : 0.06484050750732422
			 train-loss:  2.185055417801017 	 ± 0.21871803472622045
	data : 0.11432685852050781
	model : 0.06486444473266602
			 train-loss:  2.1851810508304172 	 ± 0.21791131519441614
	data : 0.1144981861114502
	model : 0.06494736671447754
			 train-loss:  2.1864111405961655 	 ± 0.21757861953970736
	data : 0.11450223922729492
	model : 0.06495084762573242
			 train-loss:  2.1849827470570586 	 ± 0.21742213965316515
	data : 0.1144017219543457
	model : 0.06498203277587891
			 train-loss:  2.1857621168744736 	 ± 0.2168249281229598
	data : 0.1143707275390625
	model : 0.06496129035949708
			 train-loss:  2.1859619686071823 	 ± 0.21605633037897312
	data : 0.11429495811462402
	model : 0.06490912437438964
			 train-loss:  2.188587243216378 	 ± 0.21749690616970532
	data : 0.11421327590942383
	model : 0.06487998962402344
			 train-loss:  2.1915464451972473 	 ± 0.21953444173949907
	data : 0.11415119171142578
	model : 0.06487889289855957
			 train-loss:  2.1894737087504965 	 ± 0.22014026532469538
	data : 0.11427502632141114
	model : 0.06489520072937012
			 train-loss:  2.188538255391421 	 ± 0.2196522331316555
	data : 0.1143312931060791
	model : 0.06497001647949219
			 train-loss:  2.1882936366730266 	 ± 0.21890776840740866
	data : 0.11444830894470215
	model : 0.06497025489807129
			 train-loss:  2.191674335249539 	 ± 0.2218916803055375
	data : 0.11440272331237793
	model : 0.06498007774353028
			 train-loss:  2.190992103047567 	 ± 0.22128301875181694
	data : 0.11429424285888672
	model : 0.0649327278137207
			 train-loss:  2.1908349901640496 	 ± 0.2205372419608096
	data : 0.11419920921325684
	model : 0.06488690376281739
			 train-loss:  2.1906351390722634 	 ± 0.21980427692855597
	data : 0.11418304443359376
	model : 0.06482090950012206
			 train-loss:  2.1928333412080803 	 ± 0.220691674894229
	data : 0.11416606903076172
	model : 0.06483545303344726
			 train-loss:  2.193011833031972 	 ± 0.21996559648920294
	data : 0.11436934471130371
	model : 0.06483497619628906
			 train-loss:  2.1951759958898784 	 ± 0.22083245732830978
	data : 0.11453022956848144
	model : 0.06490144729614258
			 train-loss:  2.1936271292598626 	 ± 0.22092620038486022
	data : 0.11447820663452149
	model : 0.0649219036102295
			 train-loss:  2.193783146883148 	 ± 0.22021143672701018
	data : 0.11442937850952148
	model : 0.06493124961853028
			 train-loss:  2.1942441595065127 	 ± 0.21956936090814347
	data : 0.11440434455871581
	model : 0.06491761207580567
			 train-loss:  2.192478359899213 	 ± 0.2199541926833487
	data : 0.11416354179382324
	model : 0.0648921012878418
			 train-loss:  2.1939762036005654 	 ± 0.22003969573061358
	data : 0.11412258148193359
	model : 0.06487908363342285
			 train-loss:  2.194694968545513 	 ± 0.2195214553942626
	data : 0.11413307189941406
	model : 0.06489763259887696
			 train-loss:  2.197666654103919 	 ± 0.22197100066240438
	data : 0.11420831680297852
	model : 0.06484794616699219
			 train-loss:  2.1968309999262012 	 ± 0.22152105559937163
	data : 0.11417412757873535
	model : 0.06490826606750488
			 train-loss:  2.1977954268455506 	 ± 0.2211623152276581
	data : 0.11432352066040039
	model : 0.06489276885986328
			 train-loss:  2.197678650388066 	 ± 0.22047935398259855
	data : 0.11430168151855469
	model : 0.06488142013549805
			 train-loss:  2.1979571639755626 	 ± 0.21982621652970738
	data : 0.11438136100769043
	model : 0.06480655670166016
			 train-loss:  2.198276461267764 	 ± 0.2191885441923299
	data : 0.1142873764038086
	model : 0.06478633880615234
			 train-loss:  2.1963765039676573 	 ± 0.21986148495693955
	data : 0.11428546905517578
	model : 0.06482009887695313
			 train-loss:  2.1976769866365373 	 ± 0.21982600924258605
	data : 0.11417427062988281
	model : 0.06485376358032227
			 train-loss:  2.1981710726956285 	 ± 0.21925475831322577
	data : 0.11425824165344238
	model : 0.06487622261047363
			 train-loss:  2.195996763463506 	 ± 0.220385058238542
	data : 0.11423683166503906
	model : 0.06489009857177734
			 train-loss:  2.194694389899572 	 ± 0.22037180135079565
	data : 0.11427168846130371
	model : 0.06498498916625976
			 train-loss:  2.19464920399457 	 ± 0.2197196270096442
	data : 0.11424565315246582
	model : 0.0649261474609375
			 train-loss:  2.19349900413962 	 ± 0.21958213499767448
	data : 0.11426367759704589
	model : 0.06491589546203613
			 train-loss:  2.194408525500381 	 ± 0.21926006524104086
	data : 0.11418037414550782
	model : 0.06489443778991699
			 train-loss:  2.193104624055153 	 ± 0.21928565324538543
	data : 0.11406621932983399
	model : 0.06492886543273926
			 train-loss:  2.1952896745218706 	 ± 0.2205208549148721
	data : 0.11408729553222656
	model : 0.06487474441528321
			 train-loss:  2.194797061640641 	 ± 0.21998170228068237
	data : 0.11418061256408692
	model : 0.06494388580322266
			 train-loss:  2.1964559629985265 	 ± 0.22044106663677787
	data : 0.11423296928405761
	model : 0.0649487018585205
			 train-loss:  2.1953892687504943 	 ± 0.2202663875772156
	data : 0.11413540840148925
	model : 0.06500205993652344
			 train-loss:  2.1956807698233654 	 ± 0.21967732652095454
	data : 0.11416449546813964
	model : 0.06493687629699707
			 train-loss:  2.1951757448442866 	 ± 0.21916240234254858
	data : 0.11412672996520996
	model : 0.06487598419189453
			 train-loss:  2.1959955179491524 	 ± 0.21882285870750764
	data : 0.11401138305664063
	model : 0.06480197906494141
			 train-loss:  2.1941415720515782 	 ± 0.2196193707700262
	data : 0.11399149894714355
	model : 0.06486859321594238
			 train-loss:  2.193084711527956 	 ± 0.21947036460202773
	data : 0.11412200927734376
	model : 0.06486382484436035
			 train-loss:  2.1932551782209795 	 ± 0.218878608886903
	data : 0.1142467975616455
	model : 0.0649223804473877
			 train-loss:  2.1966219782177867 	 ± 0.22295534576896936
	data : 0.11436581611633301
	model : 0.06499695777893066
			 train-loss:  2.195705911387568 	 ± 0.22269373090367434
	data : 0.11448521614074707
	model : 0.06503372192382813
			 train-loss:  2.1956357569307894 	 ± 0.22209307916143245
	data : 0.11445827484130859
	model : 0.06498899459838867
			 train-loss:  2.1949580741185013 	 ± 0.221686959072537
	data : 0.1143568992614746
	model : 0.0649561882019043
			 train-loss:  2.1943330994264327 	 ± 0.22125765551869636
	data : 0.11422119140625
	model : 0.06495389938354493
			 train-loss:  2.194501614316981 	 ± 0.22068045165788477
	data : 0.11411666870117188
	model : 0.06494450569152832
			 train-loss:  2.191844730780869 	 ± 0.22309031395640797
	data : 0.11411490440368652
	model : 0.06492171287536622
			 train-loss:  2.1931577506818267 	 ± 0.2232334754855585
	data : 0.11425595283508301
	model : 0.06492481231689454
			 train-loss:  2.1918718271854662 	 ± 0.22335277412008836
	data : 0.11433095932006836
	model : 0.06500520706176757
			 train-loss:  2.190171491354704 	 ± 0.22400634923096754
	data : 0.11441459655761718
	model : 0.06500558853149414
			 train-loss:  2.189720520701433 	 ± 0.22351263586794387
	data : 0.11446070671081543
	model : 0.06501984596252441
			 train-loss:  2.1908264332210896 	 ± 0.2234646071645048
	data : 0.11429948806762695
	model : 0.06494641304016113
			 train-loss:  2.191239007314046 	 ± 0.22296494919058246
	data : 0.11410861015319824
	model : 0.06490225791931152
			 train-loss:  2.191802626969863 	 ± 0.2225346580191785
	data : 0.11414060592651368
	model : 0.064825439453125
			 train-loss:  2.1927080541697856 	 ± 0.22233077936486795
	data : 0.11408753395080566
	model : 0.06480555534362793
			 train-loss:  2.1930050368260856 	 ± 0.22180779789120816
	data : 0.11411838531494141
	model : 0.06482105255126953
			 train-loss:  2.19328555629481 	 ± 0.22128499816576164
	data : 0.11432700157165528
	model : 0.06485958099365234
			 train-loss:  2.1916258561611177 	 ± 0.22196932550101856
	data : 0.11435480117797851
	model : 0.06488313674926757
			 train-loss:  2.190318864376391 	 ± 0.22218663481971324
	data : 0.11432299613952637
	model : 0.06488761901855469
			 train-loss:  2.1913562352114386 	 ± 0.2221234203841456
	data : 0.11427702903747558
	model : 0.06482400894165039
			 train-loss:  2.192121493992547 	 ± 0.22184242376361826
	data : 0.1141890048980713
	model : 0.06476225852966308
			 train-loss:  2.1921202458587348 	 ± 0.22129802507160035
	data : 0.11421294212341308
	model : 0.06479067802429199
			 train-loss:  2.194340957083353 	 ± 0.22302457716070578
	data : 0.11443037986755371
	model : 0.06480002403259277
			 train-loss:  2.194584223830584 	 ± 0.22250985939284856
	data : 0.11454505920410156
	model : 0.06479535102844239
			 train-loss:  2.196612831475078 	 ± 0.22387317166676066
	data : 0.11462812423706055
	model : 0.06488828659057617
			 train-loss:  2.1955981449438977 	 ± 0.22381100104082438
	data : 0.11477417945861816
	model : 0.06496667861938477
			 train-loss:  2.1954143458005912 	 ± 0.22329066095547342
	data : 0.11463389396667481
	model : 0.06497526168823242
			 train-loss:  2.195170021057129 	 ± 0.22278638415418084
	data : 0.11444830894470215
	model : 0.06499276161193848
			 train-loss:  2.1943578799188983 	 ± 0.222569207643306
	data : 0.11430363655090332
	model : 0.06500186920166015
			 train-loss:  2.194288563053563 	 ± 0.2220459426990631
	data : 0.11425151824951171
	model : 0.06502599716186523
			 train-loss:  2.1947961894559187 	 ± 0.22164736347920405
	data : 0.11415166854858398
	model : 0.06498208045959472
			 train-loss:  2.196790103600404 	 ± 0.22303544218191493
	data : 0.11421074867248535
	model : 0.06501688957214355
			 train-loss:  2.197326972872712 	 ± 0.2226547061951015
	data : 0.11428837776184082
	model : 0.06503996849060059
			 train-loss:  2.1962578258028738 	 ± 0.2226911874727
	data : 0.1141655445098877
	model : 0.0650169849395752
			 train-loss:  2.197397990710175 	 ± 0.2228085010980946
	data : 0.11422271728515625
	model : 0.06498141288757324
			 train-loss:  2.197008255971681 	 ± 0.22237100960719822
	data : 0.1142056941986084
	model : 0.06493067741394043
			 train-loss:  2.1964890635721215 	 ± 0.22199512663221813
	data : 0.11420817375183105
	model : 0.0648271083831787
			 train-loss:  2.1969426929950715 	 ± 0.22159172738610483
	data : 0.1140906810760498
	model : 0.06473922729492188
			 train-loss:  2.196199673333319 	 ± 0.22136432823456662
	data : 0.11426753997802734
	model : 0.06473307609558106
			 train-loss:  2.195815822562656 	 ± 0.22093890058254256
	data : 0.1144559383392334
	model : 0.06465559005737305
			 train-loss:  2.194306720532644 	 ± 0.2215867353510912
	data : 0.11454801559448242
	model : 0.06467461585998535
			 train-loss:  2.1939528547227383 	 ± 0.22115471077751284
	data : 0.11458711624145508
	model : 0.0646519660949707
			 train-loss:  2.1952735153834024 	 ± 0.22154620138230183
	data : 0.11484003067016602
	model : 0.0645409107208252
			 train-loss:  2.1940852071331665 	 ± 0.22177298389294678
	data : 0.1148366928100586
	model : 0.06440143585205078
			 train-loss:  2.19384772609509 	 ± 0.22131275569680503
	data : 0.11475653648376465
	model : 0.06427116394042968
			 train-loss:  2.194515070894308 	 ± 0.22105566816328254
	data : 0.11469697952270508
	model : 0.06410231590270996
			 train-loss:  2.195745050126288 	 ± 0.221353000979168
	data : 0.11485786437988281
	model : 0.06397795677185059
			 train-loss:  2.195428128346153 	 ± 0.2209233364245758
	data : 0.11485486030578614
	model : 0.06398096084594726
			 train-loss:  2.1957423877922486 	 ± 0.2204961426206324
	data : 0.11488170623779297
	model : 0.06393294334411621
			 train-loss:  2.196432630049771 	 ± 0.22027038507290747
	data : 0.11489429473876953
	model : 0.06392865180969239
			 train-loss:  2.1958908129147705 	 ± 0.2199520710825293
	data : 0.11484179496765137
	model : 0.06389856338500977
			 train-loss:  2.1958942693522854 	 ± 0.2194815911716332
	data : 0.11471271514892578
	model : 0.06388611793518066
			 train-loss:  2.1968629405853597 	 ± 0.21951480295584697
	data : 0.11473207473754883
	model : 0.06383328437805176
			 train-loss:  2.1963367942026104 	 ± 0.21919767934878512
	data : 0.1148904800415039
	model : 0.06385822296142578
			 train-loss:  2.195008318132489 	 ± 0.2196847612666958
	data : 0.11489224433898926
	model : 0.06390242576599121
			 train-loss:  2.1958636295895615 	 ± 0.219617837475052
	data : 0.11505188941955566
	model : 0.0639272689819336
			 train-loss:  2.196747377826579 	 ± 0.2195815739854896
	data : 0.11490082740783691
	model : 0.06390252113342285
			 train-loss:  2.1977218349774676 	 ± 0.2196408746174957
	data : 0.11482000350952148
	model : 0.06388316154479981
			 train-loss:  2.1971897059951084 	 ± 0.21933968577892382
	data : 0.11475191116333008
	model : 0.06387701034545898
			 train-loss:  2.1960400342941284 	 ± 0.21961247022203015
	data : 0.1147963523864746
	model : 0.06388320922851562
			 train-loss:  2.1955946521994507 	 ± 0.219269618472692
	data : 0.11484355926513672
	model : 0.06392478942871094
			 train-loss:  2.1957667235468254 	 ± 0.21883627373420456
	data : 0.11504178047180176
	model : 0.06398348808288574
			 train-loss:  2.1946092566665336 	 ± 0.21913635510458102
	data : 0.11501145362854004
	model : 0.06406421661376953
			 train-loss:  2.193898147683803 	 ± 0.2189735748352059
	data : 0.11506032943725586
	model : 0.0640333652496338
			 train-loss:  2.193795681965013 	 ± 0.2185357683593246
	data : 0.11499581336975098
	model : 0.06397500038146972
			 train-loss:  2.195290821213876 	 ± 0.2193569327981279
	data : 0.11494178771972656
	model : 0.063938570022583
			 train-loss:  2.1949420856184747 	 ± 0.2189848899702243
	data : 0.11493482589721679
	model : 0.06392855644226074
			 train-loss:  2.1952434539794923 	 ± 0.21859821455468637
	data : 0.11493010520935058
	model : 0.06385240554809571
			 train-loss:  2.1957849781826675 	 ± 0.21833028264839968
	data : 0.1148444652557373
	model : 0.06386566162109375
			 train-loss:  2.19647868379714 	 ± 0.2181736492861113
	data : 0.11473426818847657
	model : 0.06379890441894531
			 train-loss:  2.1964696593906567 	 ± 0.21774209628811456
	data : 0.11475486755371093
	model : 0.06382904052734376
			 train-loss:  2.1962884542510266 	 ± 0.2173321603083519
	data : 0.11478271484375
	model : 0.06380958557128906
			 train-loss:  2.1949526941075046 	 ± 0.21794779240361514
	data : 0.11483502388000488
	model : 0.06389145851135254
			 train-loss:  2.1924368124455214 	 ± 0.2212007050870004
	data : 0.11453680992126465
	model : 0.055485773086547854
#epoch  49    val-loss:  2.385978472860236  train-loss:  2.1924368124455214  lr:  6.103515625e-07
			 train-loss:  2.1855695247650146 	 ± 0.0
	data : 4.869929552078247
	model : 0.09412050247192383
			 train-loss:  2.08246648311615 	 ± 0.10310304164886475
	data : 2.6875985860824585
	model : 0.07944416999816895
			 train-loss:  2.2243812084198 	 ± 0.21763823954945474
	data : 1.8294885953267415
	model : 0.0748291810353597
			 train-loss:  2.1724055409431458 	 ± 0.20887606975087053
	data : 1.4004274010658264
	model : 0.07230734825134277
			 train-loss:  2.126872158050537 	 ± 0.2078377392155622
	data : 1.1431553363800049
	model : 0.07077512741088868
			 train-loss:  2.1054272452990213 	 ± 0.19569498277454803
	data : 0.19207749366760254
	model : 0.06489896774291992
			 train-loss:  2.096704397882734 	 ± 0.18243388489876575
	data : 0.11396303176879882
	model : 0.06487946510314942
			 train-loss:  2.1669817119836807 	 ± 0.2523770259057883
	data : 0.1140512466430664
	model : 0.06465229988098145
			 train-loss:  2.2191266881095038 	 ± 0.27994610595853375
	data : 0.1141472339630127
	model : 0.06460375785827636
			 train-loss:  2.2373626828193665 	 ± 0.2711564189628104
	data : 0.11405091285705567
	model : 0.06460309028625488
			 train-loss:  2.244758508422158 	 ± 0.2595931854837333
	data : 0.11387166976928711
	model : 0.06466317176818848
			 train-loss:  2.224326262871424 	 ± 0.2576143371291529
	data : 0.11368279457092285
	model : 0.06473808288574219
			 train-loss:  2.1955387867414036 	 ± 0.26684220204746356
	data : 0.11369528770446777
	model : 0.06481952667236328
			 train-loss:  2.182798836912428 	 ± 0.2612062185946697
	data : 0.11368217468261718
	model : 0.0648758888244629
			 train-loss:  2.1777161836624144 	 ± 0.2530647673762511
	data : 0.11357383728027344
	model : 0.06485590934753419
			 train-loss:  2.1912711039185524 	 ± 0.25058971128919116
	data : 0.11371421813964844
	model : 0.0648317813873291
			 train-loss:  2.1965828993741203 	 ± 0.24403444561175358
	data : 0.113765287399292
	model : 0.06478362083435059
			 train-loss:  2.1864127119382224 	 ± 0.24083744990540454
	data : 0.11385064125061035
	model : 0.06479787826538086
			 train-loss:  2.1727638746562756 	 ± 0.24146042833839526
	data : 0.11397686004638671
	model : 0.06483974456787109
			 train-loss:  2.1684600114822388 	 ± 0.2360930389843992
	data : 0.11417140960693359
	model : 0.06486835479736328
			 train-loss:  2.1808349859146845 	 ± 0.2369566222974886
	data : 0.11407012939453125
	model : 0.0648193359375
			 train-loss:  2.2019066485491665 	 ± 0.2508396963871502
	data : 0.11398978233337402
	model : 0.06475906372070313
			 train-loss:  2.1956828055174453 	 ± 0.24705682330126633
	data : 0.11393156051635742
	model : 0.06475472450256348
			 train-loss:  2.197275231281916 	 ± 0.24197559046588044
	data : 0.11388168334960938
	model : 0.06473803520202637
			 train-loss:  2.210112600326538 	 ± 0.24528606153176855
	data : 0.11384139060974122
	model : 0.06478066444396972
			 train-loss:  2.2001730524576626 	 ± 0.24560347946347033
	data : 0.11406798362731933
	model : 0.06479535102844239
			 train-loss:  2.20414756404029 	 ± 0.24186291727673878
	data : 0.1142117977142334
	model : 0.06489229202270508
			 train-loss:  2.2063244027750835 	 ± 0.23777386557901348
	data : 0.11425118446350098
	model : 0.0648641586303711
			 train-loss:  2.2121662148113908 	 ± 0.23567441361681649
	data : 0.11409950256347656
	model : 0.0647578239440918
			 train-loss:  2.2167450388272605 	 ± 0.23302149724333038
	data : 0.11402287483215331
	model : 0.0646742343902588
			 train-loss:  2.217444077614815 	 ± 0.22926425059914068
	data : 0.11387019157409668
	model : 0.06470246315002441
			 train-loss:  2.2186276353895664 	 ± 0.22574976448080722
	data : 0.11380572319030761
	model : 0.0646700382232666
			 train-loss:  2.2177153103279346 	 ± 0.22236289920482447
	data : 0.11379146575927734
	model : 0.06469407081604003
			 train-loss:  2.212132008636699 	 ± 0.22140393882565695
	data : 0.11396203041076661
	model : 0.06482996940612792
			 train-loss:  2.216795761244638 	 ± 0.21990602801568954
	data : 0.11417746543884277
	model : 0.06490874290466309
			 train-loss:  2.215718557437261 	 ± 0.21692389885824354
	data : 0.1141364574432373
	model : 0.06488056182861328
			 train-loss:  2.218878233754957 	 ± 0.21481062047548014
	data : 0.11417737007141113
	model : 0.0647965431213379
			 train-loss:  2.2125424617215206 	 ± 0.21544036670102562
	data : 0.11411933898925782
	model : 0.06477627754211426
			 train-loss:  2.2070300609637528 	 ± 0.21535813023150902
	data : 0.11409296989440917
	model : 0.06473569869995117
			 train-loss:  2.212315317988396 	 ± 0.21519542253783233
	data : 0.11402735710144044
	model : 0.06477398872375488
			 train-loss:  2.210350824565422 	 ± 0.21291770651661168
	data : 0.11411404609680176
	model : 0.06479988098144532
			 train-loss:  2.214210110051291 	 ± 0.21181413421984202
	data : 0.11407880783081055
	model : 0.06486740112304687
			 train-loss:  2.2051176692164223 	 ± 0.21747205394421346
	data : 0.11420860290527343
	model : 0.06490936279296874
			 train-loss:  2.201076702638106 	 ± 0.21661346480891813
	data : 0.11420936584472656
	model : 0.06487040519714356
			 train-loss:  2.2056341436174183 	 ± 0.21631594327645967
	data : 0.11422295570373535
	model : 0.06480269432067871
			 train-loss:  2.1981955403866977 	 ± 0.21969372764745462
	data : 0.11411290168762207
	model : 0.06483354568481445
			 train-loss:  2.1907658754511083 	 ± 0.223108960682574
	data : 0.11428313255310059
	model : 0.06487250328063965
			 train-loss:  2.1893591955304146 	 ± 0.2209832029920074
	data : 0.11422905921936036
	model : 0.06487889289855957
			 train-loss:  2.189287017802803 	 ± 0.21871722034317606
	data : 0.11427907943725586
	model : 0.06493196487426758
			 train-loss:  2.1976053166389464 	 ± 0.22421192810819537
	data : 0.11421728134155273
	model : 0.0649752140045166
			 train-loss:  2.1985295449986175 	 ± 0.22209906111493072
	data : 0.11419792175292968
	model : 0.06498236656188965
			 train-loss:  2.198593183205678 	 ± 0.2199535956514975
	data : 0.11397972106933593
	model : 0.06493840217590333
			 train-loss:  2.196968089859441 	 ± 0.2181836157273004
	data : 0.11384963989257812
	model : 0.06489953994750977
			 train-loss:  2.198628189387145 	 ± 0.21649156394615873
	data : 0.1139078140258789
	model : 0.06489787101745606
			 train-loss:  2.1985010255466806 	 ± 0.21451646594311488
	data : 0.11396880149841308
	model : 0.06489238739013672
			 train-loss:  2.1977005196469173 	 ± 0.21267538865865301
	data : 0.1140449047088623
	model : 0.0648810863494873
			 train-loss:  2.200167934099833 	 ± 0.21160867958408844
	data : 0.11409063339233398
	model : 0.06492962837219238
			 train-loss:  2.202695470431755 	 ± 0.2106426733722256
	data : 0.11424689292907715
	model : 0.06492023468017578
			 train-loss:  2.205123026492232 	 ± 0.2096666208616671
	data : 0.1142031192779541
	model : 0.06490049362182618
			 train-loss:  2.2104182223478954 	 ± 0.2118530871972315
	data : 0.11409683227539062
	model : 0.06482467651367188
			 train-loss:  2.2073171861836167 	 ± 0.21147801303647498
	data : 0.11395130157470704
	model : 0.06479272842407227
			 train-loss:  2.2070987743716084 	 ± 0.20977254830306405
	data : 0.11396284103393554
	model : 0.06477570533752441
			 train-loss:  2.2034440608251664 	 ± 0.21008133490063957
	data : 0.11400694847106933
	model : 0.06481337547302246
			 train-loss:  2.2003669887781143 	 ± 0.20985966351373314
	data : 0.11407442092895508
	model : 0.06485724449157715
			 train-loss:  2.2004516454843372 	 ± 0.20824020262849294
	data : 0.11415214538574218
	model : 0.0649561882019043
			 train-loss:  2.1985777508128774 	 ± 0.2072081037053693
	data : 0.114404296875
	model : 0.06493768692016602
			 train-loss:  2.1962245649366237 	 ± 0.20654260653804665
	data : 0.11440410614013671
	model : 0.06494169235229492
			 train-loss:  2.2015427841859707 	 ± 0.2095888574231563
	data : 0.11418385505676269
	model : 0.06485419273376465
			 train-loss:  2.2029528168664463 	 ± 0.20838919344630707
	data : 0.1141139030456543
	model : 0.06481733322143554
			 train-loss:  2.2005371843065533 	 ± 0.20786610614349416
	data : 0.11408419609069824
	model : 0.06480040550231933
			 train-loss:  2.197282950643083 	 ± 0.20818514066686747
	data : 0.11396160125732421
	model : 0.06484789848327636
			 train-loss:  2.1981440765990152 	 ± 0.2068616516410037
	data : 0.11397128105163574
	model : 0.06484465599060059
			 train-loss:  2.1973727843532824 	 ± 0.20554412388806045
	data : 0.11407146453857422
	model : 0.06491599082946778
			 train-loss:  2.1959964439675614 	 ± 0.20448899128479991
	data : 0.11405644416809083
	model : 0.06492123603820801
			 train-loss:  2.193511722882589 	 ± 0.20424267020460568
	data : 0.1139866828918457
	model : 0.06484007835388184
			 train-loss:  2.1937423960158697 	 ± 0.2029043533572702
	data : 0.11403393745422363
	model : 0.06477894783020019
			 train-loss:  2.197491799082075 	 ± 0.20421534900154018
	data : 0.11400346755981446
	model : 0.0647761344909668
			 train-loss:  2.1958168118427963 	 ± 0.2034337069027205
	data : 0.11420331001281739
	model : 0.06479640007019043
			 train-loss:  2.193719334240201 	 ± 0.20298907124906407
	data : 0.11417970657348633
	model : 0.06484708786010743
			 train-loss:  2.191572816669941 	 ± 0.20261663674326216
	data : 0.11434531211853027
	model : 0.06491127014160156
			 train-loss:  2.187517641503134 	 ± 0.20460259965152353
	data : 0.11428627967834473
	model : 0.06497635841369628
			 train-loss:  2.191977644838938 	 ± 0.2072750126526363
	data : 0.11439042091369629
	model : 0.06499261856079101
			 train-loss:  2.198653946439904 	 ± 0.21470978424029774
	data : 0.1142810344696045
	model : 0.06490721702575683
			 train-loss:  2.1981384654839835 	 ± 0.21347958512748144
	data : 0.11431403160095215
	model : 0.06486330032348633
			 train-loss:  2.1965637108858895 	 ± 0.21271032326386846
	data : 0.11427764892578125
	model : 0.06489853858947754
			 train-loss:  2.200511323851208 	 ± 0.21457907432120146
	data : 0.11460256576538086
	model : 0.06484203338623047
			 train-loss:  2.2012851087526344 	 ± 0.2134629419888348
	data : 0.11450281143188476
	model : 0.06482782363891601
			 train-loss:  2.1996207359162243 	 ± 0.21281360201481472
	data : 0.11447196006774903
	model : 0.06488289833068847
			 train-loss:  2.1993780738851996 	 ± 0.21162688589138168
	data : 0.11441011428833008
	model : 0.0648876667022705
			 train-loss:  2.1971651064025033 	 ± 0.21148089824279218
	data : 0.11434135437011719
	model : 0.06483316421508789
			 train-loss:  2.1967891716695096 	 ± 0.21034594207056245
	data : 0.11401576995849609
	model : 0.06485114097595215
			 train-loss:  2.1958171219929405 	 ± 0.20940504065110674
	data : 0.11400580406188965
	model : 0.06480841636657715
			 train-loss:  2.192723576740552 	 ± 0.21037918778653403
	data : 0.11401238441467285
	model : 0.0647958755493164
			 train-loss:  2.192872334033885 	 ± 0.20926207472149685
	data : 0.11409602165222169
	model : 0.06478428840637207
			 train-loss:  2.2047657816033612 	 ± 0.23796291098247122
	data : 0.11423463821411133
	model : 0.06479473114013672
			 train-loss:  2.2043875604867935 	 ± 0.23674897906264422
	data : 0.11420302391052246
	model : 0.06483559608459473
			 train-loss:  2.2007234182554423 	 ± 0.23824595126951661
	data : 0.11417007446289062
	model : 0.06493926048278809
			 train-loss:  2.199634419412029 	 ± 0.2372698298377862
	data : 0.11420021057128907
	model : 0.06495542526245117
			 train-loss:  2.202598462201128 	 ± 0.237885055716297
	data : 0.11403441429138184
	model : 0.06487245559692383
			 train-loss:  2.2000902700424194 	 ± 0.23800466066531142
	data : 0.11393980979919434
	model : 0.06486315727233886
			 train-loss:  2.1989054042514007 	 ± 0.2371197072033102
	data : 0.11398501396179199
	model : 0.06482167243957519
			 train-loss:  2.1981288451774446 	 ± 0.23608352354669154
	data : 0.11412553787231446
	model : 0.06476902961730957
			 train-loss:  2.197684822730648 	 ± 0.2349774868257088
	data : 0.11405529975891113
	model : 0.06477103233337403
			 train-loss:  2.2002629889891696 	 ± 0.23530436967112603
	data : 0.11416683197021485
	model : 0.06484441757202149
			 train-loss:  2.1989208562033515 	 ± 0.23458083474485295
	data : 0.11423263549804688
	model : 0.06489081382751465
			 train-loss:  2.196355660006685 	 ± 0.23494671455313382
	data : 0.11427574157714844
	model : 0.06489510536193847
			 train-loss:  2.195443422994881 	 ± 0.23403478781963988
	data : 0.11415033340454102
	model : 0.06486444473266602
			 train-loss:  2.1961588197284274 	 ± 0.2330662844501568
	data : 0.11423521041870117
	model : 0.06485605239868164
			 train-loss:  2.19679507421791 	 ± 0.2320889179634045
	data : 0.11435918807983399
	model : 0.06486902236938477
			 train-loss:  2.1946316719055177 	 ± 0.23213301219153593
	data : 0.11430940628051758
	model : 0.06489744186401367
			 train-loss:  2.193468648034173 	 ± 0.23140671332457793
	data : 0.11427550315856934
	model : 0.06496219635009766
			 train-loss:  2.1896835022739003 	 ± 0.23379752727014927
	data : 0.11441402435302735
	model : 0.0650247573852539
			 train-loss:  2.189559910149701 	 ± 0.2327644010244355
	data : 0.11430120468139648
	model : 0.0650444507598877
			 train-loss:  2.1878979111972607 	 ± 0.2324137320386196
	data : 0.11416749954223633
	model : 0.06506457328796386
			 train-loss:  2.188920919791512 	 ± 0.23165867901024373
	data : 0.11404933929443359
	model : 0.06501588821411133
			 train-loss:  2.190295682898883 	 ± 0.23112865422655715
	data : 0.1140824317932129
	model : 0.0649794578552246
			 train-loss:  2.1910724324038906 	 ± 0.23029080992698983
	data : 0.11399235725402831
	model : 0.06493186950683594
			 train-loss:  2.1908587975017095 	 ± 0.2293245681005962
	data : 0.11415162086486816
	model : 0.06495370864868164
			 train-loss:  2.187999363706893 	 ± 0.23046179178102238
	data : 0.11421113014221192
	model : 0.06499838829040527
			 train-loss:  2.189181078473727 	 ± 0.22986128356835092
	data : 0.11438755989074707
	model : 0.06501655578613282
			 train-loss:  2.1878483522036842 	 ± 0.229374553756613
	data : 0.11437849998474121
	model : 0.06503448486328126
			 train-loss:  2.1888132789095893 	 ± 0.22867902286056835
	data : 0.11448197364807129
	model : 0.06503911018371582
			 train-loss:  2.186979480875217 	 ± 0.22864646007907402
	data : 0.11436400413513184
	model : 0.06499848365783692
			 train-loss:  2.1885246124959763 	 ± 0.2283664846850755
	data : 0.11427831649780273
	model : 0.06483726501464844
			 train-loss:  2.190528395652771 	 ± 0.22854303651017654
	data : 0.11424527168273926
	model : 0.06481790542602539
			 train-loss:  2.1898816776654075 	 ± 0.22774911857731203
	data : 0.11430473327636718
	model : 0.0648116111755371
			 train-loss:  2.189520680998254 	 ± 0.2268868850945088
	data : 0.1142397403717041
	model : 0.06488561630249023
			 train-loss:  2.1914888927713037 	 ± 0.2270847184047663
	data : 0.11430540084838867
	model : 0.06492381095886231
			 train-loss:  2.188084611596987 	 ± 0.22945834085701153
	data : 0.11428241729736328
	model : 0.0650442123413086
			 train-loss:  2.1877638541735136 	 ± 0.22860313580959496
	data : 0.11440296173095703
	model : 0.06503920555114746
			 train-loss:  2.188750037710175 	 ± 0.228006358613827
	data : 0.11427135467529297
	model : 0.0650052547454834
			 train-loss:  2.1901855685494165 	 ± 0.22773453298003815
	data : 0.114276123046875
	model : 0.06487441062927246
			 train-loss:  2.1894941903594742 	 ± 0.22701578472289158
	data : 0.11419353485107422
	model : 0.06489968299865723
			 train-loss:  2.1896561437578344 	 ± 0.22617483657542212
	data : 0.11437525749206542
	model : 0.06488504409790039
			 train-loss:  2.1900331020355224 	 ± 0.22537784146799203
	data : 0.11431097984313965
	model : 0.06494083404541015
			 train-loss:  2.1896297020070694 	 ± 0.22459663016380124
	data : 0.11439504623413085
	model : 0.06496086120605468
			 train-loss:  2.1889634811095076 	 ± 0.22391026801362984
	data : 0.11439189910888672
	model : 0.06504960060119629
			 train-loss:  2.1895315526188286 	 ± 0.22319658550786425
	data : 0.11451268196105957
	model : 0.06499776840209961
			 train-loss:  2.1906947146216744 	 ± 0.22281164392876457
	data : 0.11436586380004883
	model : 0.06495800018310546
			 train-loss:  2.1894922528948104 	 ± 0.22246663434553418
	data : 0.11427226066589355
	model : 0.0648777961730957
			 train-loss:  2.187309087590968 	 ± 0.2231763205185614
	data : 0.11418046951293945
	model : 0.06486144065856933
			 train-loss:  2.1863978328839155 	 ± 0.2226521863925497
	data : 0.11424283981323242
	model : 0.06483292579650879
			 train-loss:  2.1835280715168772 	 ± 0.22449224954790353
	data : 0.1142333984375
	model : 0.06489510536193847
			 train-loss:  2.1906381564007864 	 ± 0.23932387145028916
	data : 0.11432480812072754
	model : 0.06492886543273926
			 train-loss:  2.1920815007439973 	 ± 0.23912527283651464
	data : 0.1143078327178955
	model : 0.06498923301696777
			 train-loss:  2.1923680109520483 	 ± 0.238329916104102
	data : 0.11444587707519531
	model : 0.06498398780822753
			 train-loss:  2.193191672668976 	 ± 0.23772630408884873
	data : 0.11429662704467773
	model : 0.06492476463317871
			 train-loss:  2.19236471846297 	 ± 0.2371338696056408
	data : 0.11417236328125
	model : 0.06484389305114746
			 train-loss:  2.191652608397823 	 ± 0.23649550499466607
	data : 0.11404938697814941
	model : 0.06485233306884766
			 train-loss:  2.1898084314664206 	 ± 0.2367783851721356
	data : 0.11412191390991211
	model : 0.06483287811279297
			 train-loss:  2.193848257033241 	 ± 0.24112392557284787
	data : 0.11411662101745605
	model : 0.06484732627868653
			 train-loss:  2.193038188313183 	 ± 0.24053550772602833
	data : 0.11423382759094239
	model : 0.06488337516784667
			 train-loss:  2.191439516404096 	 ± 0.24055696264070558
	data : 0.11434803009033204
	model : 0.06495170593261719
			 train-loss:  2.1901287498412194 	 ± 0.24032219825353404
	data : 0.114375638961792
	model : 0.06501140594482421
			 train-loss:  2.190011618983361 	 ± 0.23955012095473094
	data : 0.11425528526306153
	model : 0.06496849060058593
			 train-loss:  2.190386508519833 	 ± 0.23882670843425086
	data : 0.11409869194030761
	model : 0.06493349075317383
			 train-loss:  2.1903472941392548 	 ± 0.23806540264728213
	data : 0.11410207748413086
	model : 0.06493821144104003
			 train-loss:  2.191186045544057 	 ± 0.23754343320937002
	data : 0.11407012939453125
	model : 0.0648580551147461
			 train-loss:  2.1919585016538514 	 ± 0.23699424756771972
	data : 0.11408648490905762
	model : 0.06483397483825684
			 train-loss:  2.1907218687236307 	 ± 0.23676652354422106
	data : 0.1141282081604004
	model : 0.06485133171081543
			 train-loss:  2.1906795583156327 	 ± 0.23603068521153744
	data : 0.11419010162353516
	model : 0.06484742164611816
			 train-loss:  2.1895390682750278 	 ± 0.2357456426011334
	data : 0.11405696868896484
	model : 0.06488137245178223
			 train-loss:  2.1925885435993684 	 ± 0.23820482249778918
	data : 0.11404294967651367
	model : 0.06490087509155273
			 train-loss:  2.1908149937304056 	 ± 0.23855453375722852
	data : 0.11410861015319824
	model : 0.06485447883605958
			 train-loss:  2.1897208444999925 	 ± 0.23824294670184712
	data : 0.11421570777893067
	model : 0.06490397453308105
			 train-loss:  2.189323913620179 	 ± 0.23757898090941987
	data : 0.11435813903808593
	model : 0.06495766639709473
			 train-loss:  2.1932837220723043 	 ± 0.24229873975018337
	data : 0.11441831588745117
	model : 0.06493139266967773
			 train-loss:  2.1932745433989025 	 ± 0.24157656535690297
	data : 0.11441106796264648
	model : 0.0649383544921875
			 train-loss:  2.1962345224865794 	 ± 0.24389719966119688
	data : 0.11415863037109375
	model : 0.06487116813659669
			 train-loss:  2.1945677357561446 	 ± 0.24414225229556255
	data : 0.11412668228149414
	model : 0.06482896804809571
			 train-loss:  2.19349217484569 	 ± 0.24383094727989685
	data : 0.11398200988769532
	model : 0.06480441093444825
			 train-loss:  2.191462060739828 	 ± 0.2445661953534964
	data : 0.11405959129333496
	model : 0.06481132507324219
			 train-loss:  2.1923686369306092 	 ± 0.2441480076229395
	data : 0.11410889625549317
	model : 0.06483302116394044
			 train-loss:  2.191654287535569 	 ± 0.24362667038738028
	data : 0.11443562507629394
	model : 0.06490507125854492
			 train-loss:  2.191962954657418 	 ± 0.2429637153729862
	data : 0.11434597969055176
	model : 0.06491799354553222
			 train-loss:  2.1927709010514347 	 ± 0.24250813910944724
	data : 0.11436619758605956
	model : 0.06491250991821289
			 train-loss:  2.1927856361798646 	 ± 0.24182219655179285
	data : 0.1142770767211914
	model : 0.06489496231079102
			 train-loss:  2.1928088665008545 	 ± 0.24114216204303743
	data : 0.11429166793823242
	model : 0.06485357284545898
			 train-loss:  2.192606430479934 	 ± 0.2404828040369464
	data : 0.11415205001831055
	model : 0.06483955383300781
			 train-loss:  2.1905793766180675 	 ± 0.24134247832134026
	data : 0.11424269676208496
	model : 0.0648080825805664
			 train-loss:  2.1900740321828516 	 ± 0.24077034040157178
	data : 0.1142575740814209
	model : 0.0648533821105957
			 train-loss:  2.1889026106058895 	 ± 0.24062462827716688
	data : 0.1143991470336914
	model : 0.06485490798950196
			 train-loss:  2.189775183552601 	 ± 0.24025484165673458
	data : 0.11438755989074707
	model : 0.0649043083190918
			 train-loss:  2.188822665292284 	 ± 0.2399473162250277
	data : 0.11440043449401856
	model : 0.06490569114685059
			 train-loss:  2.1885201499268816 	 ± 0.23933311252477527
	data : 0.11419687271118165
	model : 0.06485867500305176
			 train-loss:  2.188115025720289 	 ± 0.23875247261655863
	data : 0.11417150497436523
	model : 0.06480584144592286
			 train-loss:  2.1877865874193567 	 ± 0.23815536906889156
	data : 0.11412949562072754
	model : 0.06485791206359863
			 train-loss:  2.187775963798482 	 ± 0.23752117703355438
	data : 0.11413040161132812
	model : 0.06484613418579102
			 train-loss:  2.1891820437062983 	 ± 0.23767519424643366
	data : 0.11421818733215332
	model : 0.06488580703735351
			 train-loss:  2.1890530266259844 	 ± 0.23705554369144596
	data : 0.11433844566345215
	model : 0.06495532989501954
			 train-loss:  2.189940706597573 	 ± 0.2367505645026495
	data : 0.11435480117797851
	model : 0.06496858596801758
			 train-loss:  2.191409275556604 	 ± 0.23700385459218587
	data : 0.11425104141235351
	model : 0.06489806175231934
			 train-loss:  2.192716114879272 	 ± 0.23708160980474127
	data : 0.11414604187011719
	model : 0.06483917236328125
			 train-loss:  2.1917340134836962 	 ± 0.23686306659433015
	data : 0.11403126716613769
	model : 0.06481699943542481
			 train-loss:  2.1940212573760594 	 ± 0.23839317720284472
	data : 0.11412162780761718
	model : 0.06480655670166016
			 train-loss:  2.1948216514928 	 ± 0.23804679011476262
	data : 0.11411657333374023
	model : 0.06482300758361817
			 train-loss:  2.195217031512769 	 ± 0.2375063535827708
	data : 0.11427783966064453
	model : 0.06486272811889648
			 train-loss:  2.1943653912255257 	 ± 0.23720719858707665
	data : 0.11433444023132325
	model : 0.06491460800170898
			 train-loss:  2.193477849265439 	 ± 0.23693981455385493
	data : 0.11435699462890625
	model : 0.06491279602050781
			 train-loss:  2.194166752696037 	 ± 0.23654643590976482
	data : 0.11417560577392578
	model : 0.06490120887756348
			 train-loss:  2.1953089504099603 	 ± 0.23650953528213517
	data : 0.11416678428649903
	model : 0.06488623619079589
			 train-loss:  2.195028287939506 	 ± 0.23595694242893087
	data : 0.11407809257507324
	model : 0.0649219036102295
			 train-loss:  2.194929940947171 	 ± 0.2353792004911538
	data : 0.11411862373352051
	model : 0.06490960121154785
			 train-loss:  2.194367630224602 	 ± 0.23493822621334381
	data : 0.11420316696166992
	model : 0.06494860649108887
			 train-loss:  2.1973793337984784 	 ± 0.23827940452021612
	data : 0.11440520286560059
	model : 0.06496629714965821
			 train-loss:  2.195770236473639 	 ± 0.23881424279676797
	data : 0.11441621780395508
	model : 0.0649984359741211
			 train-loss:  2.196048877665386 	 ± 0.23827026358908318
	data : 0.1144228458404541
	model : 0.06496267318725586
			 train-loss:  2.1947981176468043 	 ± 0.23837702002903963
	data : 0.11425461769104003
	model : 0.06493949890136719
			 train-loss:  2.194074447075146 	 ± 0.23803497604084597
	data : 0.11421170234680175
	model : 0.06487975120544434
			 train-loss:  2.1935097876049223 	 ± 0.23760781726807706
	data : 0.1140871524810791
	model : 0.06486549377441406
			 train-loss:  2.194509533344287 	 ± 0.2374864145172637
	data : 0.11417202949523926
	model : 0.06493120193481446
			 train-loss:  2.1920731028295912 	 ± 0.23955437203765537
	data : 0.11413311958312988
	model : 0.06499805450439453
			 train-loss:  2.191393887492972 	 ± 0.2391959042777134
	data : 0.11429262161254883
	model : 0.0650627613067627
			 train-loss:  2.190083611791379 	 ± 0.23940134810979102
	data : 0.11469783782958984
	model : 0.06507248878479004
			 train-loss:  2.189733259067979 	 ± 0.23889893516480676
	data : 0.11470179557800293
	model : 0.06513619422912598
			 train-loss:  2.1894055207570395 	 ± 0.23839372772863063
	data : 0.11471176147460938
	model : 0.06503539085388184
			 train-loss:  2.1886635057387815 	 ± 0.23809367783530117
	data : 0.11457724571228027
	model : 0.06491189002990723
			 train-loss:  2.18741186277582 	 ± 0.23826144121824738
	data : 0.11453495025634766
	model : 0.06500892639160157
			 train-loss:  2.185953511495024 	 ± 0.2386900443979259
	data : 0.11415858268737793
	model : 0.06503000259399414
			 train-loss:  2.186403808810494 	 ± 0.23824016352858682
	data : 0.11434330940246581
	model : 0.06493105888366699
			 train-loss:  2.1849041821190673 	 ± 0.2387389870476535
	data : 0.11429371833801269
	model : 0.06496868133544922
			 train-loss:  2.1858157569223695 	 ± 0.23858585015499728
	data : 0.11458253860473633
	model : 0.06494407653808594
			 train-loss:  2.184141593129111 	 ± 0.23935365861108937
	data : 0.11462392807006835
	model : 0.06473565101623535
			 train-loss:  2.185680215912206 	 ± 0.23992151942032294
	data : 0.11468667984008789
	model : 0.06467967033386231
			 train-loss:  2.187042735417684 	 ± 0.24025476017146438
	data : 0.1144627571105957
	model : 0.06453113555908203
			 train-loss:  2.1867514120793974 	 ± 0.23976245900338988
	data : 0.11451225280761719
	model : 0.06434082984924316
			 train-loss:  2.1863524199582405 	 ± 0.23930894738768363
	data : 0.11456465721130371
	model : 0.06425886154174805
			 train-loss:  2.185696923941897 	 ± 0.23898771873289712
	data : 0.11467161178588867
	model : 0.06414518356323243
			 train-loss:  2.186984568183599 	 ± 0.2392566592019
	data : 0.11469755172729493
	model : 0.06398887634277343
			 train-loss:  2.1867204707601795 	 ± 0.2387694186305912
	data : 0.11486082077026367
	model : 0.06401467323303223
			 train-loss:  2.1873612641256095 	 ± 0.23845015586500212
	data : 0.11488575935363769
	model : 0.06397924423217774
			 train-loss:  2.187830971232776 	 ± 0.23804277244141056
	data : 0.11484642028808593
	model : 0.06388735771179199
			 train-loss:  2.189655017443481 	 ± 0.2391507147580217
	data : 0.11479153633117675
	model : 0.0638432502746582
			 train-loss:  2.1907586429873085 	 ± 0.239233026734868
	data : 0.11481375694274902
	model : 0.0638012409210205
			 train-loss:  2.1895252653893005 	 ± 0.23946787744556378
	data : 0.11491479873657226
	model : 0.06383614540100098
			 train-loss:  2.1906024480270125 	 ± 0.23952985907241398
	data : 0.11503810882568359
	model : 0.06382737159729004
			 train-loss:  2.189939804720979 	 ± 0.2392406599132782
	data : 0.11496953964233399
	model : 0.06389269828796387
			 train-loss:  2.1908699975294224 	 ± 0.23916661947512732
	data : 0.11498427391052246
	model : 0.06388936042785645
			 train-loss:  2.1908334608357323 	 ± 0.2386664120260484
	data : 0.11488518714904786
	model : 0.0639009952545166
			 train-loss:  2.190749109784762 	 ± 0.23817224125399364
	data : 0.11485133171081544
	model : 0.06385903358459473
			 train-loss:  2.189613920029763 	 ± 0.23832733000911507
	data : 0.11478128433227539
	model : 0.06384363174438476
			 train-loss:  2.1899316369994613 	 ± 0.23788554675134238
	data : 0.11495013236999511
	model : 0.06384239196777344
			 train-loss:  2.1897395805076316 	 ± 0.23741436551639244
	data : 0.11503129005432129
	model : 0.06386680603027343
			 train-loss:  2.189997218671392 	 ± 0.23696139809708638
	data : 0.11516728401184081
	model : 0.06386733055114746
			 train-loss:  2.1907822404588972 	 ± 0.236795027158284
	data : 0.11510095596313477
	model : 0.0638047218322754
			 train-loss:  2.1912940730893515 	 ± 0.23644900865233878
	data : 0.11494383811950684
	model : 0.06378536224365235
			 train-loss:  2.190724841013611 	 ± 0.23613872016604767
	data : 0.11485939025878907
	model : 0.06377878189086914
			 train-loss:  2.1912849276296553 	 ± 0.2358264904255234
	data : 0.11478509902954101
	model : 0.06382489204406738
			 train-loss:  2.192544966816423 	 ± 0.23618749531438116
	data : 0.11472125053405761
	model : 0.06383242607116699
			 train-loss:  2.1923034553527834 	 ± 0.23574545261805083
	data : 0.11474652290344238
	model : 0.0638768196105957
			 train-loss:  2.19538927458197 	 ± 0.24028123211599534
	data : 0.11484856605529785
	model : 0.06386356353759766
			 train-loss:  2.1955806196682035 	 ± 0.23982317013370866
	data : 0.1147756576538086
	model : 0.06384644508361817
			 train-loss:  2.195185877117715 	 ± 0.23943075695563268
	data : 0.11490283012390137
	model : 0.0638197422027588
			 train-loss:  2.1950122333887054 	 ± 0.238974933151036
	data : 0.1150364875793457
	model : 0.06384420394897461
			 train-loss:  2.193941321559981 	 ± 0.23911579241733233
	data : 0.11507863998413086
	model : 0.06395449638366699
			 train-loss:  2.194355101790279 	 ± 0.23873976754770315
	data : 0.11481013298034667
	model : 0.055578756332397464
#epoch  50    val-loss:  2.420444864975779  train-loss:  2.194355101790279  lr:  3.0517578125e-07
			 train-loss:  2.1372947692871094 	 ± 0.0
	data : 5.096081495285034
	model : 0.07128429412841797
			 train-loss:  2.0240054726600647 	 ± 0.11328929662704468
	data : 2.7256617546081543
	model : 0.06919574737548828
			 train-loss:  1.954843004544576 	 ± 0.13462244913764937
	data : 1.8548506100972493
	model : 0.06782746315002441
			 train-loss:  1.9781760275363922 	 ± 0.12339243390368031
	data : 1.419733464717865
	model : 0.0670589804649353
			 train-loss:  2.0225691556930543 	 ± 0.141645873592825
	data : 1.1587316036224364
	model : 0.06660957336425781
			 train-loss:  2.0542641282081604 	 ± 0.14745332934603006
	data : 0.16230006217956544
	model : 0.0653036117553711
			 train-loss:  2.0536664043154036 	 ± 0.1365231071562381
	data : 0.11402363777160644
	model : 0.0647822380065918
			 train-loss:  2.0613561123609543 	 ± 0.12931612508857943
	data : 0.11406412124633789
	model : 0.06473832130432129
			 train-loss:  2.073807703124152 	 ± 0.12690517622131203
	data : 0.11401472091674805
	model : 0.0647653579711914
			 train-loss:  2.096134531497955 	 ± 0.13777088503629983
	data : 0.11393728256225585
	model : 0.06479086875915527
			 train-loss:  2.1369480328126387 	 ± 0.18415403026440477
	data : 0.11390213966369629
	model : 0.06484227180480957
			 train-loss:  2.139110972483953 	 ± 0.17645993694219628
	data : 0.11397228240966797
	model : 0.06494169235229492
			 train-loss:  2.170584045923673 	 ± 0.2015676620544925
	data : 0.11408648490905762
	model : 0.0649177074432373
			 train-loss:  2.1920967527798245 	 ± 0.20915011086746246
	data : 0.11404852867126465
	model : 0.06489486694335937
			 train-loss:  2.2068048238754274 	 ± 0.2094184819743155
	data : 0.11394729614257812
	model : 0.06485919952392578
			 train-loss:  2.1939966455101967 	 ± 0.2087482593961766
	data : 0.11408653259277343
	model : 0.06486649513244629
			 train-loss:  2.1741847781574024 	 ± 0.21746887765869027
	data : 0.11404638290405274
	model : 0.06484308242797851
			 train-loss:  2.161537680361006 	 ± 0.21767975152729546
	data : 0.11410551071166992
	model : 0.06490392684936523
			 train-loss:  2.1701017241728935 	 ± 0.21496680154002362
	data : 0.11413593292236328
	model : 0.06492948532104492
			 train-loss:  2.1665577471256254 	 ± 0.2100924205416515
	data : 0.11425943374633789
	model : 0.06499519348144531
			 train-loss:  2.159835730280195 	 ± 0.2072213467789489
	data : 0.11424689292907715
	model : 0.0650052547454834
			 train-loss:  2.1441204764626245 	 ± 0.21488413255825686
	data : 0.11422457695007324
	model : 0.0649782657623291
			 train-loss:  2.144079249838124 	 ± 0.2101609165431606
	data : 0.11422314643859863
	model : 0.06494326591491699
			 train-loss:  2.140709638595581 	 ± 0.20636967355241137
	data : 0.11423196792602539
	model : 0.06497421264648437
			 train-loss:  2.149742422103882 	 ± 0.20698573077734517
	data : 0.1142575740814209
	model : 0.06494574546813965
			 train-loss:  2.1371621672923746 	 ± 0.21248965095753033
	data : 0.11427865028381348
	model : 0.06495842933654786
			 train-loss:  2.1431764982364796 	 ± 0.2107606167013008
	data : 0.11436271667480469
	model : 0.06501660346984864
			 train-loss:  2.149895655257361 	 ± 0.2098870614369885
	data : 0.11434130668640137
	model : 0.06501884460449218
			 train-loss:  2.1509537820158333 	 ± 0.2063125634280011
	data : 0.11427388191223145
	model : 0.06499190330505371
			 train-loss:  2.1466280659039816 	 ± 0.20417807785645087
	data : 0.11411604881286622
	model : 0.06491098403930665
			 train-loss:  2.1517257190519765 	 ± 0.20278923328283963
	data : 0.11393013000488281
	model : 0.06485791206359863
			 train-loss:  2.1609368808567524 	 ± 0.20607905074404362
	data : 0.11398878097534179
	model : 0.06485118865966796
			 train-loss:  2.1665909470933857 	 ± 0.2054376779365558
	data : 0.11402449607849122
	model : 0.06486625671386718
			 train-loss:  2.173672953072716 	 ± 0.2064423291765425
	data : 0.11408720016479493
	model : 0.06487178802490234
			 train-loss:  2.1721134424209594 	 ± 0.20367487865371714
	data : 0.11415143013000488
	model : 0.06496920585632324
			 train-loss:  2.1703150305483074 	 ± 0.20110777695357226
	data : 0.11418852806091309
	model : 0.06492762565612793
			 train-loss:  2.1742925805014535 	 ± 0.1998019025609665
	data : 0.11404929161071778
	model : 0.06488957405090331
			 train-loss:  2.169816704172837 	 ± 0.1990263581568294
	data : 0.11407923698425293
	model : 0.064892578125
			 train-loss:  2.1721139534925804 	 ± 0.196967894457916
	data : 0.11414608955383301
	model : 0.0649216651916504
			 train-loss:  2.179180982708931 	 ± 0.19943474079721038
	data : 0.11416573524475097
	model : 0.06490936279296874
			 train-loss:  2.1736129638625354 	 ± 0.20011053537111456
	data : 0.11423401832580567
	model : 0.06498856544494629
			 train-loss:  2.168580046721867 	 ± 0.2003230712078485
	data : 0.11438727378845215
	model : 0.06501593589782714
			 train-loss:  2.1667152754096097 	 ± 0.19834853713877723
	data : 0.11434741020202636
	model : 0.06505184173583985
			 train-loss:  2.170301152901216 	 ± 0.19748650292494394
	data : 0.11426267623901368
	model : 0.06499357223510742
			 train-loss:  2.1854212204615275 	 ± 0.21952985046570436
	data : 0.11422686576843262
	model : 0.06497626304626465
			 train-loss:  2.1876693160637566 	 ± 0.21765362536005953
	data : 0.11434273719787598
	model : 0.06491355895996094
			 train-loss:  2.1929291841831615 	 ± 0.21826087167010122
	data : 0.11427760124206543
	model : 0.06490693092346192
			 train-loss:  2.1973358864585557 	 ± 0.21807807762151732
	data : 0.11424942016601562
	model : 0.0648660659790039
			 train-loss:  2.191497289404577 	 ± 0.219599084422033
	data : 0.11431312561035156
	model : 0.06488962173461914
			 train-loss:  2.1943355631828307 	 ± 0.21829800004899513
	data : 0.11426081657409667
	model : 0.06484589576721192
			 train-loss:  2.191856381939907 	 ± 0.21685696050710948
	data : 0.11416039466857911
	model : 0.06487889289855957
			 train-loss:  2.1902218025464277 	 ± 0.215078686539819
	data : 0.11406655311584472
	model : 0.06488733291625977
			 train-loss:  2.189665027384488 	 ± 0.21307780973515683
	data : 0.11414713859558105
	model : 0.06494269371032715
			 train-loss:  2.194979965686798 	 ± 0.2146125498560893
	data : 0.114170503616333
	model : 0.06500210762023925
			 train-loss:  2.1940953146327624 	 ± 0.21275191947631686
	data : 0.11437325477600098
	model : 0.06512103080749512
			 train-loss:  2.2121681549719403 	 ± 0.2498391888744708
	data : 0.1143528938293457
	model : 0.06511859893798828
			 train-loss:  2.219806623040584 	 ± 0.25414941693421655
	data : 0.11442599296569825
	model : 0.06508212089538574
			 train-loss:  2.22630703243716 	 ± 0.25668429169246004
	data : 0.11430878639221191
	model : 0.06495323181152343
			 train-loss:  2.2345595501236994 	 ± 0.26214525465411387
	data : 0.1141817569732666
	model : 0.06483674049377441
			 train-loss:  2.230341438452403 	 ± 0.261962887562843
	data : 0.11406087875366211
	model : 0.06479325294494628
			 train-loss:  2.225894118918747 	 ± 0.262080676178651
	data : 0.11423382759094239
	model : 0.06480464935302735
			 train-loss:  2.225285103244166 	 ± 0.26000204324722276
	data : 0.11436758041381836
	model : 0.06486072540283203
			 train-loss:  2.226676861445109 	 ± 0.25816297796145
	data : 0.11449227333068848
	model : 0.0649064064025879
			 train-loss:  2.22731439396739 	 ± 0.2561881193015211
	data : 0.11459188461303711
	model : 0.06496243476867676
			 train-loss:  2.2284160503974326 	 ± 0.2543625313851146
	data : 0.11471438407897949
	model : 0.06490659713745117
			 train-loss:  2.2271107001738115 	 ± 0.2526474735379404
	data : 0.11471586227416992
	model : 0.06489157676696777
			 train-loss:  2.2253724603510614 	 ± 0.25115227594489087
	data : 0.11462154388427734
	model : 0.06487579345703125
			 train-loss:  2.22232366660062 	 ± 0.2505446677255472
	data : 0.11451945304870606
	model : 0.06494064331054687
			 train-loss:  2.2181192532829614 	 ± 0.2511273072939782
	data : 0.11452240943908691
	model : 0.06495318412780762
			 train-loss:  2.2182679227420263 	 ± 0.24933014670566792
	data : 0.1145092487335205
	model : 0.06502876281738282
			 train-loss:  2.2203045848389746 	 ± 0.24815380472175563
	data : 0.11438884735107421
	model : 0.06504654884338379
			 train-loss:  2.2210572279161878 	 ± 0.24650608144057468
	data : 0.11437025070190429
	model : 0.06502952575683593
			 train-loss:  2.2195652821292615 	 ± 0.2451389656658645
	data : 0.11445221900939942
	model : 0.06497030258178711
			 train-loss:  2.2180133687483297 	 ± 0.24383777222862268
	data : 0.11444001197814942
	model : 0.06496253013610839
			 train-loss:  2.216452123324076 	 ± 0.24257880183879174
	data : 0.11433696746826172
	model : 0.06489429473876954
			 train-loss:  2.215330548976597 	 ± 0.24117327872005684
	data : 0.11433258056640624
	model : 0.0648810863494873
			 train-loss:  2.215475340942284 	 ± 0.23960542547232636
	data : 0.11428713798522949
	model : 0.06488037109375
			 train-loss:  2.2116689544457655 	 ± 0.240396225117256
	data : 0.11426682472229004
	model : 0.06494035720825195
			 train-loss:  2.2130729986142508 	 ± 0.23919152622462544
	data : 0.11429820060729981
	model : 0.06493840217590333
			 train-loss:  2.21394355148077 	 ± 0.23781778741533158
	data : 0.11443529129028321
	model : 0.06499147415161133
			 train-loss:  2.2137850758470137 	 ± 0.23634946774109972
	data : 0.11447114944458008
	model : 0.06499700546264649
			 train-loss:  2.2113740894852616 	 ± 0.23590396267060143
	data : 0.1143557071685791
	model : 0.06499457359313965
			 train-loss:  2.21070316469813 	 ± 0.23455724455604515
	data : 0.11428484916687012
	model : 0.0649611473083496
			 train-loss:  2.207489937543869 	 ± 0.23498743854037563
	data : 0.1143312931060791
	model : 0.064990234375
			 train-loss:  2.2053976942511166 	 ± 0.2343867921464642
	data : 0.1142688274383545
	model : 0.0649838924407959
			 train-loss:  2.20322896020357 	 ± 0.233876364081827
	data : 0.11420783996582032
	model : 0.06503462791442871
			 train-loss:  2.202474036435971 	 ± 0.23263372824469142
	data : 0.11428546905517578
	model : 0.06507568359375
			 train-loss:  2.2018315101211723 	 ± 0.23138579515410101
	data : 0.11430497169494629
	model : 0.06505413055419922
			 train-loss:  2.203475996349635 	 ± 0.2305987891080961
	data : 0.11417427062988281
	model : 0.06504263877868652
			 train-loss:  2.2009922398461237 	 ± 0.2305081439464234
	data : 0.11397500038146972
	model : 0.06503472328186036
			 train-loss:  2.1992120690398163 	 ± 0.22985935916570435
	data : 0.1139451026916504
	model : 0.06501960754394531
			 train-loss:  2.2007357218991155 	 ± 0.22906830013444815
	data : 0.1140228271484375
	model : 0.0650336742401123
			 train-loss:  2.1975024425855247 	 ± 0.22993443575746803
	data : 0.11410346031188964
	model : 0.06505622863769531
			 train-loss:  2.1997772822988795 	 ± 0.22975783967542104
	data : 0.11421341896057129
	model : 0.06504645347595214
			 train-loss:  2.1993622942974693 	 ± 0.22858080179542375
	data : 0.11427388191223145
	model : 0.06508703231811523
			 train-loss:  2.197146948426962 	 ± 0.22841006451777984
	data : 0.11431679725646973
	model : 0.0650245189666748
			 train-loss:  2.19743953414799 	 ± 0.22724772562673953
	data : 0.11436042785644532
	model : 0.06496338844299317
			 train-loss:  2.1921304099413814 	 ± 0.23205321374974805
	data : 0.11419854164123536
	model : 0.06493611335754394
			 train-loss:  2.191313967560277 	 ± 0.2310196798311608
	data : 0.11413044929504394
	model : 0.0649216651916504
			 train-loss:  2.1923926854133606 	 ± 0.2301121273054698
	data : 0.11427497863769531
	model : 0.06485061645507813
			 train-loss:  2.1931908272280554 	 ± 0.22910919010190312
	data : 0.1142451286315918
	model : 0.0648869514465332
			 train-loss:  2.193615518364252 	 ± 0.22802328774709119
	data : 0.11422677040100097
	model : 0.06492524147033692
			 train-loss:  2.194079568085161 	 ± 0.22696207286699618
	data : 0.11434831619262695
	model : 0.06491022109985352
			 train-loss:  2.1903804254073362 	 ± 0.2289670140952018
	data : 0.11438102722167968
	model : 0.06489086151123047
			 train-loss:  2.1921548173541114 	 ± 0.22859142561489057
	data : 0.11429295539855958
	model : 0.06494898796081543
			 train-loss:  2.190368756933032 	 ± 0.22824554424200438
	data : 0.11435694694519043
	model : 0.06490554809570312
			 train-loss:  2.191122087362771 	 ± 0.22730883266971966
	data : 0.11423540115356445
	model : 0.06487669944763183
			 train-loss:  2.1933156329172627 	 ± 0.22738894348625946
	data : 0.11423096656799317
	model : 0.06488480567932128
			 train-loss:  2.1923769428095685 	 ± 0.2265535921902332
	data : 0.11427583694458007
	model : 0.06498689651489258
			 train-loss:  2.1915179393508217 	 ± 0.22569970091160715
	data : 0.11444058418273925
	model : 0.06499342918395996
			 train-loss:  2.1915274987349638 	 ± 0.22468075780214994
	data : 0.11451277732849122
	model : 0.06506972312927246
			 train-loss:  2.1902813240885735 	 ± 0.22406046805769717
	data : 0.11456680297851562
	model : 0.06504712104797364
			 train-loss:  2.191645608539075 	 ± 0.22353362309685954
	data : 0.11470766067504883
	model : 0.06507062911987305
			 train-loss:  2.191220039861244 	 ± 0.22259702706556964
	data : 0.11457614898681641
	model : 0.06498851776123046
			 train-loss:  2.1913543110308438 	 ± 0.22163173750780907
	data : 0.11448836326599121
	model : 0.06490702629089355
			 train-loss:  2.1914501570422074 	 ± 0.22067675417167076
	data : 0.11435122489929199
	model : 0.06485490798950196
			 train-loss:  2.1924472415549126 	 ± 0.21999393227564967
	data : 0.11452159881591797
	model : 0.06489672660827636
			 train-loss:  2.189381978269351 	 ± 0.22155472582598304
	data : 0.11445999145507812
	model : 0.06494722366333008
			 train-loss:  2.188291751036123 	 ± 0.22093949223230097
	data : 0.11456766128540039
	model : 0.06501951217651367
			 train-loss:  2.1860021203756332 	 ± 0.22143017037683588
	data : 0.114601469039917
	model : 0.06506752967834473
			 train-loss:  2.186982151890589 	 ± 0.22077445106255372
	data : 0.11463966369628906
	model : 0.06507244110107421
			 train-loss:  2.1856292241909463 	 ± 0.22037086678763923
	data : 0.11451268196105957
	model : 0.06504960060119629
			 train-loss:  2.186757414321589 	 ± 0.21982670078963298
	data : 0.11444063186645508
	model : 0.06498923301696777
			 train-loss:  2.185121951564666 	 ± 0.2196885598220695
	data : 0.11439704895019531
	model : 0.06491785049438477
			 train-loss:  2.183919313430786 	 ± 0.21921748283796405
	data : 0.11431126594543457
	model : 0.06489119529724122
			 train-loss:  2.183151614098322 	 ± 0.2185144750826504
	data : 0.11445107460021972
	model : 0.06487913131713867
			 train-loss:  2.1874285701691636 	 ± 0.2228843630448511
	data : 0.11449394226074219
	model : 0.06490855216979981
			 train-loss:  2.18548440374434 	 ± 0.2230904900428474
	data : 0.1145294189453125
	model : 0.06497793197631836
			 train-loss:  2.193467461785605 	 ± 0.23987683779670996
	data : 0.11456007957458496
	model : 0.06495318412780762
			 train-loss:  2.1951839153583235 	 ± 0.2397464003571695
	data : 0.11455760002136231
	model : 0.0649498462677002
			 train-loss:  2.1940344708566446 	 ± 0.23918889920516134
	data : 0.11439394950866699
	model : 0.06496272087097169
			 train-loss:  2.19708201379487 	 ± 0.24082062866971835
	data : 0.11438612937927246
	model : 0.06492719650268555
			 train-loss:  2.196408987045288 	 ± 0.2400381576668262
	data : 0.1143639087677002
	model : 0.06490592956542969
			 train-loss:  2.196892021307305 	 ± 0.23920568875727338
	data : 0.11449370384216309
	model : 0.06494655609130859
			 train-loss:  2.1956575517301204 	 ± 0.23874613931899658
	data : 0.1145057201385498
	model : 0.06500983238220215
			 train-loss:  2.192907204522806 	 ± 0.24000374742127878
	data : 0.11458883285522461
	model : 0.06507658958435059
			 train-loss:  2.1906376241767496 	 ± 0.2405865395302053
	data : 0.11458659172058105
	model : 0.06515240669250488
			 train-loss:  2.1907506706058113 	 ± 0.23971691608884377
	data : 0.11466655731201172
	model : 0.0650944709777832
			 train-loss:  2.192730794707648 	 ± 0.23998306475804668
	data : 0.11437487602233887
	model : 0.0650674819946289
			 train-loss:  2.1911737663405284 	 ± 0.23982802953487448
	data : 0.11434946060180665
	model : 0.06498432159423828
			 train-loss:  2.1907611197613654 	 ± 0.2390259340360604
	data : 0.11424951553344727
	model : 0.06496205329895019
			 train-loss:  2.1903081427157765 	 ± 0.2382435325698378
	data : 0.11425390243530273
	model : 0.06491293907165527
			 train-loss:  2.1896452837057048 	 ± 0.23754041787065572
	data : 0.11426262855529785
	model : 0.06496682167053222
			 train-loss:  2.1889665606949062 	 ± 0.2368532917290151
	data : 0.11448826789855956
	model : 0.06503162384033204
			 train-loss:  2.1887578635380187 	 ± 0.23604842866114
	data : 0.11453266143798828
	model : 0.06505775451660156
			 train-loss:  2.192065500233271 	 ± 0.2385866538912272
	data : 0.1145514965057373
	model : 0.06498098373413086
			 train-loss:  2.1913251536233083 	 ± 0.2379419690014455
	data : 0.11436953544616699
	model : 0.06494784355163574
			 train-loss:  2.190778406890663 	 ± 0.23722938372494987
	data : 0.11422014236450195
	model : 0.06491413116455078
			 train-loss:  2.1913113818072634 	 ± 0.23652086270011297
	data : 0.1141016960144043
	model : 0.06489405632019044
			 train-loss:  2.1898357343673704 	 ± 0.23641832375146404
	data : 0.11413273811340333
	model : 0.06491785049438477
			 train-loss:  2.187293340828245 	 ± 0.23768262892474376
	data : 0.11426544189453125
	model : 0.06500611305236817
			 train-loss:  2.1865899147171723 	 ± 0.23705713121547814
	data : 0.11448898315429687
	model : 0.06505413055419922
			 train-loss:  2.1873825959909974 	 ± 0.23648318550048997
	data : 0.11453347206115723
	model : 0.0650637149810791
			 train-loss:  2.1875189627919878 	 ± 0.23572016759700712
	data : 0.1145474910736084
	model : 0.06503100395202636
			 train-loss:  2.1869985541989725 	 ± 0.2350472868919585
	data : 0.11453132629394532
	model : 0.06501622200012207
			 train-loss:  2.1879950830569634 	 ± 0.2346209794483601
	data : 0.1144373893737793
	model : 0.06491246223449706
			 train-loss:  2.1879751219111645 	 ± 0.23387271815078736
	data : 0.11424503326416016
	model : 0.06487479209899902
			 train-loss:  2.187753163561036 	 ± 0.23314802793006675
	data : 0.11429553031921387
	model : 0.06486124992370605
			 train-loss:  2.1892625423347427 	 ± 0.23318680951560486
	data : 0.11445622444152832
	model : 0.0648686408996582
			 train-loss:  2.18695672377944 	 ± 0.23426824081729056
	data : 0.11449298858642579
	model : 0.06494927406311035
			 train-loss:  2.1869165771496224 	 ± 0.23354011856437534
	data : 0.11457319259643554
	model : 0.06504788398742675
			 train-loss:  2.1919836269484625 	 ± 0.24153258206673686
	data : 0.11470503807067871
	model : 0.06503548622131347
			 train-loss:  2.1901823201793835 	 ± 0.24187957502875973
	data : 0.11465630531311036
	model : 0.06508412361145019
			 train-loss:  2.191501152224657 	 ± 0.24172814357308126
	data : 0.11446375846862793
	model : 0.06504945755004883
			 train-loss:  2.190679136912028 	 ± 0.24122432581339595
	data : 0.1143636703491211
	model : 0.06493611335754394
			 train-loss:  2.1909716158028107 	 ± 0.24052599216486986
	data : 0.1142953872680664
	model : 0.0648728847503662
			 train-loss:  2.190327136102551 	 ± 0.23994849033645893
	data : 0.11432785987854004
	model : 0.06488480567932128
			 train-loss:  2.189451644817988 	 ± 0.23950067018650145
	data : 0.11430797576904297
	model : 0.06488757133483887
			 train-loss:  2.1922598929094845 	 ± 0.2415492715588985
	data : 0.11434011459350586
	model : 0.06493129730224609
			 train-loss:  2.192270083988414 	 ± 0.2408378211169371
	data : 0.11448335647583008
	model : 0.06494812965393067
			 train-loss:  2.1911989094918236 	 ± 0.24053839320978398
	data : 0.11445865631103516
	model : 0.06497817039489746
			 train-loss:  2.1911584485408873 	 ± 0.23983871799858217
	data : 0.1144012451171875
	model : 0.06497230529785156
			 train-loss:  2.192091050175573 	 ± 0.23945710741331594
	data : 0.1143028736114502
	model : 0.06490287780761719
			 train-loss:  2.1931270278733352 	 ± 0.2391565177087309
	data : 0.11436028480529785
	model : 0.06491169929504395
			 train-loss:  2.1926291384015766 	 ± 0.23856265452599912
	data : 0.1142493724822998
	model : 0.06496243476867676
			 train-loss:  2.190932057797909 	 ± 0.23894097629455321
	data : 0.11436185836791993
	model : 0.06499214172363281
			 train-loss:  2.192181239020353 	 ± 0.23884068303369965
	data : 0.11433672904968262
	model : 0.06503491401672364
			 train-loss:  2.1923642553640215 	 ± 0.23818128321838666
	data : 0.11447806358337402
	model : 0.06504058837890625
			 train-loss:  2.190697159181094 	 ± 0.23855417375642313
	data : 0.11444487571716308
	model : 0.06501173973083496
			 train-loss:  2.1893046034706964 	 ± 0.2386190616476924
	data : 0.11432304382324218
	model : 0.06500830650329589
			 train-loss:  2.1894438622406174 	 ± 0.2379663146317611
	data : 0.11428155899047851
	model : 0.06498990058898926
			 train-loss:  2.189039713733799 	 ± 0.23737394137303233
	data : 0.11432456970214844
	model : 0.06497478485107422
			 train-loss:  2.190541870607053 	 ± 0.23759032552410358
	data : 0.11440048217773438
	model : 0.06497316360473633
			 train-loss:  2.189486514614976 	 ± 0.2373735345650296
	data : 0.11448888778686524
	model : 0.06503515243530274
			 train-loss:  2.1886714142722052 	 ± 0.2369891735544072
	data : 0.11465249061584473
	model : 0.06505579948425293
			 train-loss:  2.1905621745253123 	 ± 0.23774625623193768
	data : 0.11467337608337402
	model : 0.06501169204711914
			 train-loss:  2.1894773098236735 	 ± 0.23757089058761266
	data : 0.1147263526916504
	model : 0.06503195762634277
			 train-loss:  2.190431440130193 	 ± 0.23729718468482192
	data : 0.11468648910522461
	model : 0.06500210762023925
			 train-loss:  2.1908212490182706 	 ± 0.23672892594220624
	data : 0.11453824043273926
	model : 0.06486854553222657
			 train-loss:  2.1893967910816796 	 ± 0.23691587142219128
	data : 0.11454529762268066
	model : 0.06483545303344726
			 train-loss:  2.190039471806032 	 ± 0.2364608586172919
	data : 0.11451740264892578
	model : 0.06484794616699219
			 train-loss:  2.189683238044381 	 ± 0.23589565203823254
	data : 0.11452383995056152
	model : 0.06481122970581055
			 train-loss:  2.1884322388802167 	 ± 0.23592141332796773
	data : 0.11452269554138184
	model : 0.06489577293395996
			 train-loss:  2.1898784588292703 	 ± 0.23616875579786142
	data : 0.11462903022766113
	model : 0.06499552726745605
			 train-loss:  2.19112004500169 	 ± 0.23619633804960036
	data : 0.11461496353149414
	model : 0.06500282287597656
			 train-loss:  2.1925938749799925 	 ± 0.2364902702415965
	data : 0.11451172828674316
	model : 0.06496939659118653
			 train-loss:  2.1915072287399755 	 ± 0.23637933082015472
	data : 0.11429500579833984
	model : 0.06497201919555665
			 train-loss:  2.1918143196539446 	 ± 0.235821051161671
	data : 0.11422982215881347
	model : 0.06490592956542969
			 train-loss:  2.1908416131033968 	 ± 0.2356256614742694
	data : 0.11414866447448731
	model : 0.06490082740783691
			 train-loss:  2.1892206364870073 	 ± 0.2361455918632543
	data : 0.11409058570861816
	model : 0.06487126350402832
			 train-loss:  2.188164778609774 	 ± 0.23603023367282502
	data : 0.11417627334594727
	model : 0.06493792533874512
			 train-loss:  2.189908234199675 	 ± 0.23673919173219654
	data : 0.11437888145446777
	model : 0.06496376991271972
			 train-loss:  2.1889706621029106 	 ± 0.23653102404702706
	data : 0.11443371772766113
	model : 0.0650169849395752
			 train-loss:  2.1907761634564866 	 ± 0.2373487354836717
	data : 0.11450691223144531
	model : 0.06499114036560058
			 train-loss:  2.189875179383813 	 ± 0.23711858156848623
	data : 0.11461148262023926
	model : 0.06505756378173828
			 train-loss:  2.192288997103867 	 ± 0.23905380049806674
	data : 0.1144866943359375
	model : 0.06498842239379883
			 train-loss:  2.191353310709414 	 ± 0.2388535184661132
	data : 0.11439557075500488
	model : 0.06498608589172364
			 train-loss:  2.1903819172428203 	 ± 0.23868817713549578
	data : 0.1144552230834961
	model : 0.0649709701538086
			 train-loss:  2.190478881010028 	 ± 0.23812057448570081
	data : 0.11453990936279297
	model : 0.06498384475708008
			 train-loss:  2.190566382521675 	 ± 0.23755631227186674
	data : 0.11448912620544434
	model : 0.06494441032409667
			 train-loss:  2.1908547217247047 	 ± 0.2370295462826442
	data : 0.11462988853454589
	model : 0.0650087833404541
			 train-loss:  2.1908860763288893 	 ± 0.2364702921490701
	data : 0.11468496322631835
	model : 0.0650186538696289
			 train-loss:  2.191537021471301 	 ± 0.23610485590834596
	data : 0.11452183723449708
	model : 0.06500663757324218
			 train-loss:  2.1920494047280785 	 ± 0.23567123344840768
	data : 0.11428599357604981
	model : 0.06500344276428223
			 train-loss:  2.1904975486356157 	 ± 0.23621593391603415
	data : 0.11418318748474121
	model : 0.06495828628540039
			 train-loss:  2.192166296972169 	 ± 0.23693534672948463
	data : 0.11425461769104003
	model : 0.06489253044128418
			 train-loss:  2.1920490204463907 	 ± 0.23639506601030436
	data : 0.11428613662719726
	model : 0.06484146118164062
			 train-loss:  2.194209229508671 	 ± 0.23798931929833186
	data : 0.11446809768676758
	model : 0.06482419967651368
			 train-loss:  2.1949024086129176 	 ± 0.23766581436419215
	data : 0.11465210914611816
	model : 0.06482410430908203
			 train-loss:  2.194569150426171 	 ± 0.2371763299780304
	data : 0.11474838256835937
	model : 0.06478767395019532
			 train-loss:  2.1939712493128365 	 ± 0.23680523993634753
	data : 0.11464529037475586
	model : 0.0647613525390625
			 train-loss:  2.1934536571975225 	 ± 0.23639655257450665
	data : 0.11462044715881348
	model : 0.0647252082824707
			 train-loss:  2.1928714716915594 	 ± 0.2360253733145855
	data : 0.11447701454162598
	model : 0.06465554237365723
			 train-loss:  2.193041500768491 	 ± 0.23551162907142267
	data : 0.11439862251281738
	model : 0.06500563621520997
			 train-loss:  2.191788846651713 	 ± 0.23573438647570807
	data : 0.1144627571105957
	model : 0.06490674018859863
			 train-loss:  2.1908690359740133 	 ± 0.23561658375066577
	data : 0.11465520858764648
	model : 0.06480522155761718
			 train-loss:  2.1898687389978755 	 ± 0.2355774788710156
	data : 0.11480703353881835
	model : 0.0646817684173584
			 train-loss:  2.1900712398060582 	 ± 0.23508009327117194
	data : 0.11501922607421874
	model : 0.06454339027404785
			 train-loss:  2.190250475854332 	 ± 0.23458186899986214
	data : 0.11515769958496094
	model : 0.06398067474365235
			 train-loss:  2.1891674959141274 	 ± 0.23464436895104124
	data : 0.11513171195983887
	model : 0.0639434814453125
			 train-loss:  2.1893428398933246 	 ± 0.23415103056515274
	data : 0.11509613990783692
	model : 0.06389417648315429
			 train-loss:  2.1896872309775186 	 ± 0.23370447346056042
	data : 0.11500177383422852
	model : 0.06385555267333984
			 train-loss:  2.189583510799981 	 ± 0.23320777359607964
	data : 0.114984130859375
	model : 0.06389007568359376
			 train-loss:  2.188781418861487 	 ± 0.23303078837489846
	data : 0.1150507926940918
	model : 0.06394443511962891
			 train-loss:  2.188010266993908 	 ± 0.23283346892166448
	data : 0.11526455879211425
	model : 0.06395235061645507
			 train-loss:  2.1895539048364605 	 ± 0.23354159587900566
	data : 0.11535115242004394
	model : 0.06392483711242676
			 train-loss:  2.189280431984849 	 ± 0.23308623548805926
	data : 0.11528410911560058
	model : 0.06393885612487793
			 train-loss:  2.1890947523237276 	 ± 0.23261360726977953
	data : 0.11516551971435547
	model : 0.06387839317321778
			 train-loss:  2.188669192242323 	 ± 0.23221928117379917
	data : 0.11510372161865234
	model : 0.06382918357849121
			 train-loss:  2.188007402916749 	 ± 0.23196072383033006
	data : 0.11501469612121581
	model : 0.0638230323791504
			 train-loss:  2.186792482973629 	 ± 0.23224289919203153
	data : 0.11499233245849609
	model : 0.06394820213317871
			 train-loss:  2.187280000241335 	 ± 0.2318861018860515
	data : 0.11514959335327149
	model : 0.06395435333251953
			 train-loss:  2.189125578589891 	 ± 0.23318270587940124
	data : 0.11526255607604981
	model : 0.06400275230407715
			 train-loss:  2.189683389956834 	 ± 0.23286678520162138
	data : 0.11526122093200683
	model : 0.06396288871765136
			 train-loss:  2.189380991702177 	 ± 0.23243906235928768
	data : 0.11509261131286622
	model : 0.06392827033996581
			 train-loss:  2.1887336072882984 	 ± 0.23218736645358534
	data : 0.11510090827941895
	model : 0.06380467414855957
			 train-loss:  2.1896994070485536 	 ± 0.2322114797098081
	data : 0.11514344215393066
	model : 0.06381196975708008
			 train-loss:  2.190032840255768 	 ± 0.2318020795672656
	data : 0.11506552696228027
	model : 0.06383056640625
			 train-loss:  2.1901620999876275 	 ± 0.2313451009008369
	data : 0.11512904167175293
	model : 0.06383981704711914
			 train-loss:  2.190807366847992 	 ± 0.23110635960294382
	data : 0.11534290313720703
	model : 0.06381092071533204
			 train-loss:  2.191479291099001 	 ± 0.23089008328350358
	data : 0.11519217491149902
	model : 0.06380023956298828
			 train-loss:  2.1913439617270516 	 ± 0.23044148682142124
	data : 0.11507043838500977
	model : 0.06373114585876465
			 train-loss:  2.1929394543877705 	 ± 0.23137604644156606
	data : 0.115150785446167
	model : 0.06367182731628418
			 train-loss:  2.1935934302375073 	 ± 0.23115430300838025
	data : 0.11510004997253417
	model : 0.06370344161987304
			 train-loss:  2.194827586996789 	 ± 0.23153758144174344
	data : 0.11499409675598145
	model : 0.06371951103210449
			 train-loss:  2.193738947622478 	 ± 0.23173788753355234
	data : 0.11493358612060547
	model : 0.055351543426513675
#epoch  51    val-loss:  2.479200501190989  train-loss:  2.193738947622478  lr:  3.0517578125e-07
			 train-loss:  2.058262825012207 	 ± 0.0
	data : 5.42166805267334
	model : 0.09292459487915039
			 train-loss:  2.055034041404724 	 ± 0.00322878360748291
	data : 2.7659037113189697
	model : 0.08164167404174805
			 train-loss:  2.0814197063446045 	 ± 0.03740797585669708
	data : 1.882872184117635
	model : 0.07599091529846191
			 train-loss:  2.045562744140625 	 ± 0.07004771737012039
	data : 1.4407227635383606
	model : 0.07315480709075928
			 train-loss:  2.0580631256103517 	 ± 0.06745653648510383
	data : 1.175438404083252
	model : 0.07145357131958008
			 train-loss:  2.0708769162495932 	 ± 0.06791872408648951
	data : 0.11391758918762207
	model : 0.0658104419708252
			 train-loss:  2.101843629564558 	 ± 0.09852707133251025
	data : 0.11478314399719239
	model : 0.06468591690063477
			 train-loss:  2.117938995361328 	 ± 0.10152616122514574
	data : 0.11417546272277831
	model : 0.06466002464294433
			 train-loss:  2.1367098490397134 	 ± 0.10945792065529103
	data : 0.11411261558532715
	model : 0.06469144821166992
			 train-loss:  2.127984046936035 	 ± 0.1070896323456898
	data : 0.11412711143493652
	model : 0.06473789215087891
			 train-loss:  2.115686774253845 	 ± 0.10926049533758757
	data : 0.11417183876037598
	model : 0.06482019424438476
			 train-loss:  2.0823872089385986 	 ± 0.1521200383688
	data : 0.11412181854248046
	model : 0.0651400089263916
			 train-loss:  2.096732781483577 	 ± 0.15436972683893257
	data : 0.11406378746032715
	model : 0.0651817798614502
			 train-loss:  2.1172569819859097 	 ± 0.16614459151928876
	data : 0.11417102813720703
	model : 0.06517395973205567
			 train-loss:  2.137692022323608 	 ± 0.17779209534964666
	data : 0.11405820846557617
	model : 0.06516199111938477
			 train-loss:  2.1315916180610657 	 ± 0.17376025531825012
	data : 0.11411056518554688
	model : 0.06512198448181153
			 train-loss:  2.1593746297499714 	 ± 0.20190820018394076
	data : 0.11417126655578613
	model : 0.06481046676635742
			 train-loss:  2.187786102294922 	 ± 0.22852722478351462
	data : 0.11444687843322754
	model : 0.06483840942382812
			 train-loss:  2.207551956176758 	 ± 0.23771501020522084
	data : 0.11443858146667481
	model : 0.06490058898925781
			 train-loss:  2.210339879989624 	 ± 0.23201440178912477
	data : 0.1145359992980957
	model : 0.06494779586791992
			 train-loss:  2.2453751223427907 	 ± 0.27534829022999957
	data : 0.11458086967468262
	model : 0.06492328643798828
			 train-loss:  2.2555113488977607 	 ± 0.2729983022422819
	data : 0.1144935131072998
	model : 0.0649001121520996
			 train-loss:  2.2451506386632505 	 ± 0.27138403967511365
	data : 0.11425251960754394
	model : 0.06482892036437989
			 train-loss:  2.241681923468908 	 ± 0.2661903678568004
	data : 0.11424388885498046
	model : 0.06478772163391114
			 train-loss:  2.242288589477539 	 ± 0.2608291634569529
	data : 0.11428570747375488
	model : 0.06474223136901855
			 train-loss:  2.231121737223405 	 ± 0.2617875236320201
	data : 0.11424317359924316
	model : 0.06474795341491699
			 train-loss:  2.2217614032604076 	 ± 0.2612900136854472
	data : 0.11434874534606934
	model : 0.06481337547302246
			 train-loss:  2.2348129749298096 	 ± 0.26539299883221784
	data : 0.11460795402526855
	model : 0.06489400863647461
			 train-loss:  2.2182220467205704 	 ± 0.27515805195270643
	data : 0.11458268165588378
	model : 0.0649144172668457
			 train-loss:  2.2077879309654236 	 ± 0.2763068478076932
	data : 0.11460771560668945
	model : 0.06495485305786133
			 train-loss:  2.2156503238985614 	 ± 0.2752039854253167
	data : 0.1144477367401123
	model : 0.06496176719665528
			 train-loss:  2.2038149423897266 	 ± 0.2787701722986444
	data : 0.11435084342956543
	model : 0.06497187614440918
			 train-loss:  2.205945777170586 	 ± 0.27477840158587813
	data : 0.11426301002502441
	model : 0.06497788429260254
			 train-loss:  2.220128637902877 	 ± 0.2827022378410682
	data : 0.1143655776977539
	model : 0.06496143341064453
			 train-loss:  2.2170542546680996 	 ± 0.2792104459741649
	data : 0.11435379981994628
	model : 0.06495537757873535
			 train-loss:  2.20492861005995 	 ± 0.284497898057251
	data : 0.11454806327819825
	model : 0.06502346992492676
			 train-loss:  2.1959297624794214 	 ± 0.2857739758603194
	data : 0.11463642120361328
	model : 0.06503548622131347
			 train-loss:  2.1896587296536096 	 ± 0.28455701788380516
	data : 0.11464085578918456
	model : 0.0650364875793457
			 train-loss:  2.201583746152046 	 ± 0.29034514313857956
	data : 0.1145942211151123
	model : 0.06501564979553223
			 train-loss:  2.197047674655914 	 ± 0.2880889731153573
	data : 0.11454992294311524
	model : 0.06498026847839355
			 train-loss:  2.2037304378137357 	 ± 0.28767578073329897
	data : 0.11448049545288086
	model : 0.06489806175231934
			 train-loss:  2.204268801779974 	 ± 0.2842513410287923
	data : 0.11443128585815429
	model : 0.06485090255737305
			 train-loss:  2.210222238718077 	 ± 0.28356375952414176
	data : 0.11442418098449707
	model : 0.06484041213989258
			 train-loss:  2.2098133780739526 	 ± 0.2803357453685424
	data : 0.11443133354187011
	model : 0.0648890495300293
			 train-loss:  2.217472971810235 	 ± 0.28182118407341966
	data : 0.11447763442993164
	model : 0.06486787796020507
			 train-loss:  2.213395144628442 	 ± 0.280080130107608
	data : 0.11456766128540039
	model : 0.06487026214599609
			 train-loss:  2.2228672200060906 	 ± 0.2844344723245478
	data : 0.11465163230895996
	model : 0.06486220359802246
			 train-loss:  2.223467767238617 	 ± 0.28148613003615336
	data : 0.11455225944519043
	model : 0.0648083209991455
			 train-loss:  2.2188688686915805 	 ± 0.28041505958659346
	data : 0.11445364952087403
	model : 0.06475777626037597
			 train-loss:  2.210940999984741 	 ± 0.2830894868049766
	data : 0.11441564559936523
	model : 0.06481490135192872
			 train-loss:  2.230476220448812 	 ± 0.3124892536302477
	data : 0.11438603401184082
	model : 0.06484551429748535
			 train-loss:  2.2327202696066637 	 ± 0.30988462623263474
	data : 0.11432456970214844
	model : 0.06490006446838378
			 train-loss:  2.2294492721557617 	 ± 0.3078522267212741
	data : 0.11443381309509278
	model : 0.0649256706237793
			 train-loss:  2.232106164649681 	 ± 0.3056011603078389
	data : 0.11449961662292481
	model : 0.06494545936584473
			 train-loss:  2.229395294189453 	 ± 0.3034647718718451
	data : 0.1144343376159668
	model : 0.0649421215057373
			 train-loss:  2.224253875868661 	 ± 0.30315056987393074
	data : 0.11436996459960938
	model : 0.06493639945983887
			 train-loss:  2.219308662832829 	 ± 0.3027498472360073
	data : 0.11437788009643554
	model : 0.06494736671447754
			 train-loss:  2.2167200487235497 	 ± 0.30076422893337884
	data : 0.1143463134765625
	model : 0.06497344970703126
			 train-loss:  2.2129336211640958 	 ± 0.2995955004848806
	data : 0.11437840461730957
	model : 0.06500649452209473
			 train-loss:  2.2101364016532896 	 ± 0.297864311325553
	data : 0.11442251205444336
	model : 0.0650324821472168
			 train-loss:  2.206284800513846 	 ± 0.29691540883937695
	data : 0.11437811851501464
	model : 0.0650491714477539
			 train-loss:  2.212865633349265 	 ± 0.2989625286412226
	data : 0.11444964408874511
	model : 0.06497383117675781
			 train-loss:  2.2063065540222895 	 ± 0.30104355172965935
	data : 0.11429123878479004
	model : 0.06496844291687012
			 train-loss:  2.2032371256500483 	 ± 0.29967435213653226
	data : 0.1141749382019043
	model : 0.06492595672607422
			 train-loss:  2.2065552069590644 	 ± 0.29854266824243975
	data : 0.1143186092376709
	model : 0.06489548683166504
			 train-loss:  2.2072927536386433 	 ± 0.29633201492746364
	data : 0.11436161994934083
	model : 0.0648921012878418
			 train-loss:  2.200579883447334 	 ± 0.299125658609006
	data : 0.11425895690917968
	model : 0.0649838924407959
			 train-loss:  2.2050069377702823 	 ± 0.2991211360465636
	data : 0.11444416046142578
	model : 0.06497101783752442
			 train-loss:  2.2023934540541275 	 ± 0.29772671615068524
	data : 0.1145514965057373
	model : 0.06500663757324218
			 train-loss:  2.2001128077507017 	 ± 0.296198898833255
	data : 0.1143409252166748
	model : 0.06502385139465332
			 train-loss:  2.2014800007914155 	 ± 0.2943279560226839
	data : 0.11424646377563477
	model : 0.06498351097106933
			 train-loss:  2.201346978545189 	 ± 0.29227901435841774
	data : 0.11426138877868652
	model : 0.06529555320739747
			 train-loss:  2.1995661405667866 	 ± 0.29066325652586195
	data : 0.11424598693847657
	model : 0.0653238296508789
			 train-loss:  2.1996011717899426 	 ± 0.2886927906106079
	data : 0.11430811882019043
	model : 0.065330171585083
			 train-loss:  2.202305496533712 	 ± 0.2877037884193778
	data : 0.11445503234863282
	model : 0.06533622741699219
			 train-loss:  2.2022343419100108 	 ± 0.2858053970638653
	data : 0.11452236175537109
	model : 0.06540346145629883
			 train-loss:  2.2014245414114617 	 ± 0.2840312015376892
	data : 0.11454882621765136
	model : 0.06502804756164551
			 train-loss:  2.1989979178477554 	 ± 0.2830068189797699
	data : 0.11459274291992187
	model : 0.06499500274658203
			 train-loss:  2.1986048085780086 	 ± 0.28123136319657716
	data : 0.1144974708557129
	model : 0.06496901512145996
			 train-loss:  2.203527621924877 	 ± 0.2828726468897942
	data : 0.1144230842590332
	model : 0.06494030952453614
			 train-loss:  2.2018430865841148 	 ± 0.28152457036010053
	data : 0.11450400352478027
	model : 0.06490631103515625
			 train-loss:  2.2024692718575642 	 ± 0.2798594412519839
	data : 0.11460394859313965
	model : 0.0649406909942627
			 train-loss:  2.205198128539396 	 ± 0.2792638585201168
	data : 0.11457157135009766
	model : 0.06500673294067383
			 train-loss:  2.2021614696298326 	 ± 0.2789717515797546
	data : 0.11447076797485352
	model : 0.06500935554504395
			 train-loss:  2.203683372104869 	 ± 0.2776764427569011
	data : 0.114569091796875
	model : 0.06500868797302246
			 train-loss:  2.204821144425592 	 ± 0.27625654927040433
	data : 0.11453776359558106
	model : 0.06502556800842285
			 train-loss:  2.208536100113529 	 ± 0.2768164433810433
	data : 0.11433653831481934
	model : 0.06496381759643555
			 train-loss:  2.2056670270182868 	 ± 0.2765370238469143
	data : 0.11425666809082032
	model : 0.06491003036499024
			 train-loss:  2.2023168145940546 	 ± 0.27676919391437466
	data : 0.1144334316253662
	model : 0.06496281623840332
			 train-loss:  2.203907510969374 	 ± 0.2756361014719378
	data : 0.11451268196105957
	model : 0.0650014877319336
			 train-loss:  2.205088075700697 	 ± 0.2743461380090396
	data : 0.11453285217285156
	model : 0.06499037742614747
			 train-loss:  2.2010919399883435 	 ± 0.2755011525181158
	data : 0.11456308364868165
	model : 0.06504631042480469
			 train-loss:  2.1974299876920638 	 ± 0.2762579533607435
	data : 0.114691162109375
	model : 0.06509232521057129
			 train-loss:  2.1974859313761934 	 ± 0.27478509672147855
	data : 0.11467165946960449
	model : 0.06502728462219239
			 train-loss:  2.1981286601016397 	 ± 0.2734060567353109
	data : 0.1144552230834961
	model : 0.06496958732604981
			 train-loss:  2.19647628813982 	 ± 0.2724547642983474
	data : 0.11450047492980957
	model : 0.06500077247619629
			 train-loss:  2.1955456684545145 	 ± 0.2712000469477925
	data : 0.11457061767578125
	model : 0.06497001647949219
			 train-loss:  2.1952314498473187 	 ± 0.26983057248606707
	data : 0.1145960807800293
	model : 0.06499018669128417
			 train-loss:  2.1957082339007443 	 ± 0.26850582066168993
	data : 0.1145716667175293
	model : 0.06501202583312989
			 train-loss:  2.192677317857742 	 ± 0.2688566174112674
	data : 0.11456046104431153
	model : 0.06501193046569824
			 train-loss:  2.1945775322394794 	 ± 0.268196346373511
	data : 0.11461210250854492
	model : 0.06494264602661133
			 train-loss:  2.1931197187479805 	 ± 0.26728026194283017
	data : 0.11448931694030762
	model : 0.06491823196411133
			 train-loss:  2.1937871152914843 	 ± 0.2660650129018232
	data : 0.11432695388793945
	model : 0.06486496925354004
			 train-loss:  2.1995140463113785 	 ± 0.27108684851417025
	data : 0.11423544883728028
	model : 0.06479916572570801
			 train-loss:  2.1976747410637993 	 ± 0.27044413382631394
	data : 0.114296293258667
	model : 0.06486349105834961
			 train-loss:  2.197384351829313 	 ± 0.26918187803574867
	data : 0.11414837837219238
	model : 0.06491069793701172
			 train-loss:  2.1972234639051917 	 ± 0.2679261865478151
	data : 0.11424756050109863
	model : 0.06497430801391602
			 train-loss:  2.1980824768543243 	 ± 0.26683089463441767
	data : 0.11440420150756836
	model : 0.06497974395751953
			 train-loss:  2.1990211524000953 	 ± 0.26578315800578317
	data : 0.11438846588134766
	model : 0.06499290466308594
			 train-loss:  2.1960994579575277 	 ± 0.2663249059747932
	data : 0.11450915336608887
	model : 0.06494736671447754
			 train-loss:  2.195870802209184 	 ± 0.2651333761008564
	data : 0.11451988220214844
	model : 0.0649341106414795
			 train-loss:  2.196802463914667 	 ± 0.26412954049611
	data : 0.11460914611816406
	model : 0.06493997573852539
			 train-loss:  2.1949511238958985 	 ± 0.2636871355820127
	data : 0.1145556926727295
	model : 0.06499199867248535
			 train-loss:  2.1931421380294 	 ± 0.2632313997660228
	data : 0.11472220420837402
	model : 0.06508030891418456
			 train-loss:  2.1954128576361613 	 ± 0.2632034279568692
	data : 0.11462993621826172
	model : 0.06506257057189942
			 train-loss:  2.193548102831018 	 ± 0.2628283240325435
	data : 0.11449470520019531
	model : 0.06504697799682617
			 train-loss:  2.1922505543782163 	 ± 0.2620755859411465
	data : 0.11443710327148438
	model : 0.065020751953125
			 train-loss:  2.1912714919801486 	 ± 0.26117752606909833
	data : 0.11447858810424805
	model : 0.06500811576843261
			 train-loss:  2.1905289567819164 	 ± 0.2602028748077558
	data : 0.11448860168457031
	model : 0.06496810913085938
			 train-loss:  2.1887222399314243 	 ± 0.2598648997561997
	data : 0.11447081565856934
	model : 0.06499905586242676
			 train-loss:  2.188688320561874 	 ± 0.258789116748426
	data : 0.11460256576538086
	model : 0.06506352424621582
			 train-loss:  2.191029827125737 	 ± 0.259010151773591
	data : 0.11472949981689454
	model : 0.06502652168273926
			 train-loss:  2.191423077893451 	 ± 0.2579916836188576
	data : 0.11476492881774902
	model : 0.06496834754943848
			 train-loss:  2.1927332003270426 	 ± 0.25735978071270466
	data : 0.114555025100708
	model : 0.06493639945983887
			 train-loss:  2.1949075956344606 	 ± 0.25746932953038437
	data : 0.11452369689941407
	model : 0.06489825248718262
			 train-loss:  2.1947490743228366 	 ± 0.2564517148455707
	data : 0.1145434856414795
	model : 0.06486196517944336
			 train-loss:  2.194201164358244 	 ± 0.2555140969288728
	data : 0.11449766159057617
	model : 0.06489553451538085
			 train-loss:  2.191520852036774 	 ± 0.25630016174980436
	data : 0.11442084312438965
	model : 0.06494464874267578
			 train-loss:  2.1911817493364794 	 ± 0.2553336418673248
	data : 0.11460237503051758
	model : 0.06497311592102051
			 train-loss:  2.1953535015766437 	 ± 0.2587253761495436
	data : 0.11457591056823731
	model : 0.06498484611511231
			 train-loss:  2.1940645871271616 	 ± 0.25815461638809023
	data : 0.11456995010375977
	model : 0.06495504379272461
			 train-loss:  2.1920779636411956 	 ± 0.2581781222164424
	data : 0.11431550979614258
	model : 0.06492452621459961
			 train-loss:  2.1888400396906342 	 ± 0.2598820468756707
	data : 0.11422009468078613
	model : 0.06493721008300782
			 train-loss:  2.187997552885938 	 ± 0.2590927624909694
	data : 0.11424946784973145
	model : 0.06492047309875489
			 train-loss:  2.186522203904611 	 ± 0.2586957266765154
	data : 0.11438164710998536
	model : 0.06497774124145508
			 train-loss:  2.1860144024386123 	 ± 0.25781040710310943
	data : 0.11437735557556153
	model : 0.06503782272338868
			 train-loss:  2.184259417283274 	 ± 0.25768183392714034
	data : 0.11460628509521484
	model : 0.06505227088928223
			 train-loss:  2.1852327801179197 	 ± 0.25699915810045687
	data : 0.11478285789489746
	model : 0.06498374938964843
			 train-loss:  2.18370370899173 	 ± 0.25670225902767957
	data : 0.11481952667236328
	model : 0.06502480506896972
			 train-loss:  2.1839394961084637 	 ± 0.2557989278778158
	data : 0.11463170051574707
	model : 0.06495418548583984
			 train-loss:  2.1844259001684527 	 ± 0.2549551911062658
	data : 0.11454877853393555
	model : 0.06486048698425292
			 train-loss:  2.184008265884829 	 ± 0.2541042716085411
	data : 0.11452751159667969
	model : 0.0648423194885254
			 train-loss:  2.1870238580903805 	 ± 0.25575138043971496
	data : 0.11443662643432617
	model : 0.06483492851257325
			 train-loss:  2.1865356034702725 	 ± 0.2549286786048401
	data : 0.11434111595153809
	model : 0.06483235359191894
			 train-loss:  2.188244400353267 	 ± 0.25487430591608284
	data : 0.11447482109069824
	model : 0.06483197212219238
			 train-loss:  2.1896662859067524 	 ± 0.2545763717442855
	data : 0.11462478637695313
	model : 0.06485648155212402
			 train-loss:  2.1874933794242186 	 ± 0.2550638996771558
	data : 0.11460447311401367
	model : 0.06486687660217286
			 train-loss:  2.190324892868867 	 ± 0.25650844516489063
	data : 0.11449608802795411
	model : 0.06485872268676758
			 train-loss:  2.190856970396618 	 ± 0.2557281650515493
	data : 0.11438493728637696
	model : 0.0648277759552002
			 train-loss:  2.192508223851522 	 ± 0.2556700689609671
	data : 0.11437411308288574
	model : 0.06485428810119628
			 train-loss:  2.1921196378619467 	 ± 0.2548665116562894
	data : 0.11438169479370117
	model : 0.0649045467376709
			 train-loss:  2.1915613177575564 	 ± 0.25411938219012004
	data : 0.11442375183105469
	model : 0.065040922164917
			 train-loss:  2.193448820924447 	 ± 0.25435431251701673
	data : 0.11450114250183105
	model : 0.06505403518676758
			 train-loss:  2.192520632372274 	 ± 0.2537869704002836
	data : 0.11463375091552734
	model : 0.06504764556884765
			 train-loss:  2.1931692677159464 	 ± 0.253095009954827
	data : 0.11470909118652343
	model : 0.06505317687988281
			 train-loss:  2.1948291017458987 	 ± 0.25312742705280555
	data : 0.1145395278930664
	model : 0.06501665115356445
			 train-loss:  2.1937283239546854 	 ± 0.25269430134029747
	data : 0.1143773078918457
	model : 0.06484107971191407
			 train-loss:  2.1931058998349346 	 ± 0.25201407052996666
	data : 0.11436905860900878
	model : 0.06487045288085938
			 train-loss:  2.193302437944232 	 ± 0.2512324700722138
	data : 0.1144528865814209
	model : 0.06487717628479003
			 train-loss:  2.195096546411514 	 ± 0.25146582523586963
	data : 0.11435589790344239
	model : 0.06488518714904785
			 train-loss:  2.197400984556779 	 ± 0.2523726739018132
	data : 0.114404296875
	model : 0.0648838996887207
			 train-loss:  2.196142827287132 	 ± 0.25209851796322724
	data : 0.11438326835632324
	model : 0.06493353843688965
			 train-loss:  2.199435612175362 	 ± 0.25479449959446054
	data : 0.11437420845031739
	model : 0.06490654945373535
			 train-loss:  2.19873692131624 	 ± 0.2541730777505722
	data : 0.11411733627319336
	model : 0.06485204696655274
			 train-loss:  2.1992262919743855 	 ± 0.2534791700682596
	data : 0.11408329010009766
	model : 0.0648460865020752
			 train-loss:  2.198139258896012 	 ± 0.25309998427060587
	data : 0.11414422988891601
	model : 0.06483993530273438
			 train-loss:  2.198846916238705 	 ± 0.2525057245019442
	data : 0.11424088478088379
	model : 0.06482448577880859
			 train-loss:  2.196619817898387 	 ± 0.2533928505350827
	data : 0.11425924301147461
	model : 0.06488723754882812
			 train-loss:  2.1976779459496223 	 ± 0.2530140445868307
	data : 0.1144791603088379
	model : 0.06495227813720703
			 train-loss:  2.1979413306011875 	 ± 0.2522920236621364
	data : 0.11451072692871093
	model : 0.06493735313415527
			 train-loss:  2.197520196786401 	 ± 0.25161316680522766
	data : 0.11457619667053223
	model : 0.06496520042419433
			 train-loss:  2.197919819937196 	 ± 0.2509350862672764
	data : 0.11446356773376465
	model : 0.06492390632629394
			 train-loss:  2.1985582981495497 	 ± 0.2503488661637965
	data : 0.11442251205444336
	model : 0.06486353874206544
			 train-loss:  2.1974949857284285 	 ± 0.2500199107281336
	data : 0.11434431076049804
	model : 0.06492576599121094
			 train-loss:  2.1964633539744787 	 ± 0.24967566553928078
	data : 0.11449613571166992
	model : 0.0649721622467041
			 train-loss:  2.195858259770003 	 ± 0.2490939972329747
	data : 0.11448259353637695
	model : 0.0649653434753418
			 train-loss:  2.194706103896017 	 ± 0.24885919778260163
	data : 0.11456527709960937
	model : 0.06508235931396485
			 train-loss:  2.1954318599754505 	 ± 0.24834694217701164
	data : 0.11466178894042969
	model : 0.06511306762695312
			 train-loss:  2.195663279661253 	 ± 0.24767150965373969
	data : 0.11464600563049317
	model : 0.06504392623901367
			 train-loss:  2.19388275080257 	 ± 0.24812874287185474
	data : 0.11444635391235351
	model : 0.06499090194702148
			 train-loss:  2.1950404163223602 	 ± 0.24792933095239778
	data : 0.11439166069030762
	model : 0.06492776870727539
			 train-loss:  2.19728097483352 	 ± 0.24907800020814327
	data : 0.11449146270751953
	model : 0.06484155654907227
			 train-loss:  2.1973263611559006 	 ± 0.248397281623642
	data : 0.11441583633422851
	model : 0.06482276916503907
			 train-loss:  2.1996595762346103 	 ± 0.24972406612906672
	data : 0.11447062492370605
	model : 0.06485576629638672
			 train-loss:  2.2001990840241716 	 ± 0.24915572109500123
	data : 0.11454410552978515
	model : 0.06489095687866211
			 train-loss:  2.19977254316371 	 ± 0.2485527629059707
	data : 0.11466393470764161
	model : 0.06494941711425781
			 train-loss:  2.200534701347351 	 ± 0.2481051277073337
	data : 0.114556884765625
	model : 0.06497502326965332
			 train-loss:  2.2012752884245934 	 ± 0.24765155316585064
	data : 0.11472721099853515
	model : 0.06500377655029296
			 train-loss:  2.200260499797801 	 ± 0.2473871242325391
	data : 0.1145789623260498
	model : 0.06499314308166504
			 train-loss:  2.2000724008208827 	 ± 0.24674879731383562
	data : 0.1146242618560791
	model : 0.06501779556274415
			 train-loss:  2.2016712651826946 	 ± 0.24708684562672814
	data : 0.11449766159057617
	model : 0.06501407623291015
			 train-loss:  2.201209871098399 	 ± 0.24652503225288525
	data : 0.11458625793457031
	model : 0.06502823829650879
			 train-loss:  2.203478833554322 	 ± 0.2478873733592226
	data : 0.11451339721679688
	model : 0.06501975059509277
			 train-loss:  2.203334223978298 	 ± 0.2472558246690096
	data : 0.11465864181518555
	model : 0.06504912376403808
			 train-loss:  2.2028356863902165 	 ± 0.24671875576917074
	data : 0.11459589004516602
	model : 0.06499743461608887
			 train-loss:  2.2024756420631797 	 ± 0.24613992100168544
	data : 0.11455869674682617
	model : 0.0649862289428711
			 train-loss:  2.2032016769883596 	 ± 0.24572472440185586
	data : 0.11454811096191406
	model : 0.06497859954833984
			 train-loss:  2.2018541735832136 	 ± 0.2458320427512849
	data : 0.1142347812652588
	model : 0.0649643898010254
			 train-loss:  2.204333283793387 	 ± 0.24768248497230502
	data : 0.11415824890136719
	model : 0.06489377021789551
			 train-loss:  2.2028808861970903 	 ± 0.24791059394873374
	data : 0.11420955657958984
	model : 0.06493072509765625
			 train-loss:  2.204028149149311 	 ± 0.24782480824705538
	data : 0.11437544822692872
	model : 0.0649177074432373
			 train-loss:  2.2046315970987376 	 ± 0.24735861495406622
	data : 0.11439356803894044
	model : 0.06492233276367188
			 train-loss:  2.203074545108626 	 ± 0.24773898588198642
	data : 0.1146777629852295
	model : 0.0649754524230957
			 train-loss:  2.20479214016129 	 ± 0.2483397394088134
	data : 0.11462135314941406
	model : 0.0649641990661621
			 train-loss:  2.2050880158819806 	 ± 0.2477693337540953
	data : 0.11460528373718262
	model : 0.06493721008300782
			 train-loss:  2.204254330361931 	 ± 0.24745528129631517
	data : 0.11436100006103515
	model : 0.06494274139404296
			 train-loss:  2.205089925567885 	 ± 0.2471479970392169
	data : 0.11430568695068359
	model : 0.0648848533630371
			 train-loss:  2.2046561567829204 	 ± 0.24663214815084164
	data : 0.11421284675598145
	model : 0.06480674743652344
			 train-loss:  2.2052688490260732 	 ± 0.2462000359918719
	data : 0.11436405181884765
	model : 0.06483979225158691
			 train-loss:  2.207206365608034 	 ± 0.24720517244417287
	data : 0.114384126663208
	model : 0.06483407020568847
			 train-loss:  2.2059915376500494 	 ± 0.24724622142793015
	data : 0.11460466384887695
	model : 0.06488418579101562
			 train-loss:  2.2051566383748686 	 ± 0.24696036253797424
	data : 0.11472902297973633
	model : 0.06492171287536622
			 train-loss:  2.2045672404374317 	 ± 0.24652937354700083
	data : 0.1146697998046875
	model : 0.0649909496307373
			 train-loss:  2.2040684384720346 	 ± 0.24606040665893641
	data : 0.11460881233215332
	model : 0.06498651504516602
			 train-loss:  2.2057781768399614 	 ± 0.2467583466454614
	data : 0.11482472419738769
	model : 0.06495170593261719
			 train-loss:  2.205754004694797 	 ± 0.2461867392607805
	data : 0.11457452774047852
	model : 0.06483721733093262
			 train-loss:  2.2061509245551676 	 ± 0.24568809751765083
	data : 0.11447596549987793
	model : 0.06480517387390136
			 train-loss:  2.2060737910620665 	 ± 0.24512657846499886
	data : 0.11466078758239746
	model : 0.06471834182739258
			 train-loss:  2.2075811417679807 	 ± 0.2455768485984597
	data : 0.11471490859985352
	model : 0.06466841697692871
			 train-loss:  2.2058735977519643 	 ± 0.24631768325656622
	data : 0.11459369659423828
	model : 0.06469817161560058
			 train-loss:  2.2041581103165226 	 ± 0.24707347632722867
	data : 0.11478462219238281
	model : 0.06477642059326172
			 train-loss:  2.204321061705684 	 ± 0.24652827852138273
	data : 0.11480398178100586
	model : 0.0647200107574463
			 train-loss:  2.2036223748339667 	 ± 0.24619509620454896
	data : 0.1148231029510498
	model : 0.06470122337341308
			 train-loss:  2.2040849067270756 	 ± 0.24574202666375447
	data : 0.11471805572509766
	model : 0.0646249771118164
			 train-loss:  2.203967890739441 	 ± 0.24520157962132083
	data : 0.11455364227294922
	model : 0.06445193290710449
			 train-loss:  2.204071957980637 	 ± 0.24466347667480784
	data : 0.11463494300842285
	model : 0.06425371170043945
			 train-loss:  2.203567529564912 	 ± 0.244241725912951
	data : 0.11474237442016602
	model : 0.06417632102966309
			 train-loss:  2.2048174315377285 	 ± 0.24443201939829043
	data : 0.11484756469726562
	model : 0.06419892311096191
			 train-loss:  2.2036819077995666 	 ± 0.24449968211142833
	data : 0.1149531364440918
	model : 0.06415553092956543
			 train-loss:  2.2037574120189833 	 ± 0.2439702576159035
	data : 0.11509819030761718
	model : 0.06413860321044922
			 train-loss:  2.204067491865777 	 ± 0.24348702679121179
	data : 0.11523089408874512
	model : 0.0641317367553711
			 train-loss:  2.2022802136067687 	 ± 0.24447553536131988
	data : 0.11500186920166015
	model : 0.06414303779602051
			 train-loss:  2.2023504839434644 	 ± 0.24395269363710922
	data : 0.11484198570251465
	model : 0.06398777961730957
			 train-loss:  2.2011266636033344 	 ± 0.24414659647273476
	data : 0.11505494117736817
	model : 0.06399788856506347
			 train-loss:  2.201848712373287 	 ± 0.2438768302054079
	data : 0.11509532928466797
	model : 0.06405320167541503
			 train-loss:  2.20131453871727 	 ± 0.24349732442405053
	data : 0.11499719619750977
	model : 0.0641097068786621
			 train-loss:  2.2014056413988525 	 ± 0.24298710451046565
	data : 0.11517786979675293
	model : 0.06406073570251465
			 train-loss:  2.200284702938144 	 ± 0.2430893775285535
	data : 0.11526961326599121
	model : 0.0640453815460205
			 train-loss:  2.200776623382728 	 ± 0.24269896856739973
	data : 0.11495637893676758
	model : 0.06396064758300782
			 train-loss:  2.2013758167624475 	 ± 0.24236990298210148
	data : 0.11489028930664062
	model : 0.06388688087463379
			 train-loss:  2.2016944494484867 	 ± 0.24191690455193962
	data : 0.11489191055297851
	model : 0.06382670402526855
			 train-loss:  2.2022322184783367 	 ± 0.2415608639514217
	data : 0.11491055488586426
	model : 0.06388645172119141
			 train-loss:  2.2028317652612066 	 ± 0.24124367178728448
	data : 0.11495647430419922
	model : 0.06391081809997559
			 train-loss:  2.2014349396111537 	 ± 0.24173149012501613
	data : 0.11512484550476074
	model : 0.06394791603088379
			 train-loss:  2.2014202049800327 	 ± 0.24123776592352106
	data : 0.11521949768066406
	model : 0.06395101547241211
			 train-loss:  2.200512076781048 	 ± 0.24116621386027745
	data : 0.11509184837341309
	model : 0.06394729614257813
			 train-loss:  2.2000232814294605 	 ± 0.24079959932412678
	data : 0.11498560905456542
	model : 0.06389865875244141
			 train-loss:  2.1992591109968003 	 ± 0.2406135408152993
	data : 0.1150242805480957
	model : 0.06385421752929688
			 train-loss:  2.1996910801853042 	 ± 0.24022623218875686
	data : 0.11499161720275879
	model : 0.06391305923461914
			 train-loss:  2.2000911741256712 	 ± 0.23982841111135814
	data : 0.11497917175292968
	model : 0.06395902633666992
			 train-loss:  2.198967529957988 	 ± 0.2400086598004116
	data : 0.11506872177124024
	model : 0.06398682594299317
			 train-loss:  2.200101551555452 	 ± 0.24020482115545683
	data : 0.11521048545837402
	model : 0.06398725509643555
			 train-loss:  2.199280507008549 	 ± 0.24008368567671892
	data : 0.11496467590332031
	model : 0.06401925086975098
			 train-loss:  2.2005687811243253 	 ± 0.2404852132684639
	data : 0.11505532264709473
	model : 0.06393613815307617
			 train-loss:  2.2001710031546797 	 ± 0.2400969198458996
	data : 0.11503300666809083
	model : 0.0638885498046875
			 train-loss:  2.200044957920909 	 ± 0.2396359748562973
	data : 0.11487922668457032
	model : 0.055491161346435544
#epoch  52    val-loss:  2.416187963987652  train-loss:  2.200044957920909  lr:  3.0517578125e-07
			 train-loss:  2.49175763130188 	 ± 0.0
	data : 5.573062419891357
	model : 0.07665491104125977
			 train-loss:  2.207559883594513 	 ± 0.28419774770736694
	data : 2.8493868112564087
	model : 0.07080543041229248
			 train-loss:  2.3861555655797324 	 ± 0.3429845600735179
	data : 1.9377174377441406
	model : 0.06863792737325032
			 train-loss:  2.338009625673294 	 ± 0.30851726195634754
	data : 1.4817454814910889
	model : 0.06761181354522705
			 train-loss:  2.2814639806747437 	 ± 0.2982213282333133
	data : 1.2082039833068847
	model : 0.06709237098693847
			 train-loss:  2.2119110027949014 	 ± 0.31353051553366174
	data : 0.11641945838928222
	model : 0.06475324630737304
			 train-loss:  2.3103665624346053 	 ± 0.3773848336023372
	data : 0.11410980224609375
	model : 0.06470394134521484
			 train-loss:  2.3058704137802124 	 ± 0.3532115598660208
	data : 0.11403264999389648
	model : 0.06479420661926269
			 train-loss:  2.2844780815972223 	 ± 0.3384633154702906
	data : 0.11396760940551758
	model : 0.06480650901794434
			 train-loss:  2.261773467063904 	 ± 0.3282395010006969
	data : 0.11396489143371583
	model : 0.06475915908813476
			 train-loss:  2.241873957894065 	 ± 0.3192278541253053
	data : 0.11390805244445801
	model : 0.06472964286804199
			 train-loss:  2.2455835541089377 	 ± 0.30588493439556314
	data : 0.11393251419067382
	model : 0.06473774909973144
			 train-loss:  2.2410219816061168 	 ± 0.2943092507889122
	data : 0.11400408744812011
	model : 0.06475887298583985
			 train-loss:  2.221492281981877 	 ± 0.29221440572295443
	data : 0.11418895721435547
	model : 0.06484017372131348
			 train-loss:  2.238954695065816 	 ± 0.2897684314089718
	data : 0.11423640251159668
	model : 0.06485095024108886
			 train-loss:  2.2324656024575233 	 ± 0.2816904501297923
	data : 0.11422810554504395
	model : 0.06484622955322265
			 train-loss:  2.2164487137514004 	 ± 0.2806894067438263
	data : 0.11409668922424317
	model : 0.06480555534362793
			 train-loss:  2.2341359853744507 	 ± 0.2823610909903305
	data : 0.11413826942443847
	model : 0.06477956771850586
			 train-loss:  2.3051083966305383 	 ± 0.40767521794764855
	data : 0.11407709121704102
	model : 0.06471705436706543
			 train-loss:  2.3157518029212953 	 ± 0.4000518579061495
	data : 0.11399474143981933
	model : 0.06471991539001465
			 train-loss:  2.3017223676045737 	 ± 0.39541998790386773
	data : 0.11402659416198731
	model : 0.06474461555480956
			 train-loss:  2.2883604656566274 	 ± 0.39115109724351516
	data : 0.11412696838378907
	model : 0.06484088897705079
			 train-loss:  2.291513847268146 	 ± 0.38283913922016155
	data : 0.11419262886047363
	model : 0.06490507125854492
			 train-loss:  2.2865981459617615 	 ± 0.375519204114191
	data : 0.11418619155883789
	model : 0.0649404525756836
			 train-loss:  2.282599983215332 	 ± 0.3684531626686622
	data : 0.11418218612670898
	model : 0.06487126350402832
			 train-loss:  2.27299637060899 	 ± 0.36447498705059106
	data : 0.11412086486816406
	model : 0.06482076644897461
			 train-loss:  2.2613802574299 	 ± 0.3625330712253926
	data : 0.11417465209960938
	model : 0.06482172012329102
			 train-loss:  2.2578359842300415 	 ± 0.35647645305869574
	data : 0.1140702247619629
	model : 0.06479148864746094
			 train-loss:  2.2556116827603043 	 ± 0.35047407834765937
	data : 0.11413884162902832
	model : 0.06480240821838379
			 train-loss:  2.2495918035507203 	 ± 0.34610490659012827
	data : 0.11435728073120117
	model : 0.06487789154052734
			 train-loss:  2.2570409390234176 	 ± 0.3429127386843999
	data : 0.11439695358276367
	model : 0.06489334106445313
			 train-loss:  2.2532523050904274 	 ± 0.33817074339445574
	data : 0.11423811912536622
	model : 0.06477322578430175
			 train-loss:  2.252331950447776 	 ± 0.33304822390069083
	data : 0.1142040729522705
	model : 0.06472887992858886
			 train-loss:  2.2433236451709972 	 ± 0.3321696391726529
	data : 0.1140665054321289
	model : 0.06471705436706543
			 train-loss:  2.2607156787599836 	 ± 0.3427369388114276
	data : 0.11388888359069824
	model : 0.06470837593078613
			 train-loss:  2.259617133273019 	 ± 0.33800566619426836
	data : 0.1138697624206543
	model : 0.0647343635559082
			 train-loss:  2.252054736420915 	 ± 0.3364801415259757
	data : 0.11405296325683593
	model : 0.06484551429748535
			 train-loss:  2.25076236850337 	 ± 0.33211630377207785
	data : 0.11403155326843262
	model : 0.06489872932434082
			 train-loss:  2.2525280255537767 	 ± 0.32801138552039477
	data : 0.11401305198669434
	model : 0.0648618221282959
			 train-loss:  2.266625779867172 	 ± 0.3356379041404404
	data : 0.1139939308166504
	model : 0.06484284400939941
			 train-loss:  2.267199934982672 	 ± 0.3315393785444047
	data : 0.11401600837707519
	model : 0.0648388385772705
			 train-loss:  2.262688954671224 	 ± 0.32883972145341933
	data : 0.11401662826538086
	model : 0.064841890335083
			 train-loss:  2.2576895259147465 	 ± 0.32660456514863456
	data : 0.11401786804199218
	model : 0.06489882469177247
			 train-loss:  2.2531991546804253 	 ± 0.3242117196074543
	data : 0.11422052383422851
	model : 0.06499180793762208
			 train-loss:  2.25657606654697 	 ± 0.3213707280658178
	data : 0.11422152519226074
	model : 0.06500916481018067
			 train-loss:  2.2497923711071843 	 ± 0.32109932984699957
	data : 0.1143831729888916
	model : 0.06502938270568848
			 train-loss:  2.2423429945681956 	 ± 0.32165781291677326
	data : 0.1141805648803711
	model : 0.06499872207641602
			 train-loss:  2.2421947022279105 	 ± 0.31829119907207615
	data : 0.11432504653930664
	model : 0.06489009857177734
			 train-loss:  2.2442740323592205 	 ± 0.31535580602108826
	data : 0.11435432434082031
	model : 0.06482982635498047
			 train-loss:  2.2388868737220764 	 ± 0.3144556425543865
	data : 0.11439094543457032
	model : 0.06487555503845215
			 train-loss:  2.231791477577359 	 ± 0.31537392699259625
	data : 0.11430673599243164
	model : 0.06487627029418945
			 train-loss:  2.2259214107806864 	 ± 0.31512751337461664
	data : 0.11447019577026367
	model : 0.0648758888244629
			 train-loss:  2.2355501111948266 	 ± 0.319769731477413
	data : 0.11446151733398438
	model : 0.06490855216979981
			 train-loss:  2.2279230797732317 	 ± 0.3216243278104995
	data : 0.11440234184265137
	model : 0.06490178108215332
			 train-loss:  2.2229257757013494 	 ± 0.32079586251743997
	data : 0.11438798904418945
	model : 0.06487827301025391
			 train-loss:  2.223185496670859 	 ± 0.3179245462707398
	data : 0.11422128677368164
	model : 0.06481270790100098
			 train-loss:  2.2177288155806694 	 ± 0.3177580436437553
	data : 0.11415014266967774
	model : 0.06485171318054199
			 train-loss:  2.22386319472872 	 ± 0.3183932377665858
	data : 0.11408123970031739
	model : 0.06486115455627442
			 train-loss:  2.2239256228430797 	 ± 0.31568381682413266
	data : 0.11401185989379883
	model : 0.06488509178161621
			 train-loss:  2.217896177371343 	 ± 0.31644941599515886
	data : 0.11406707763671875
	model : 0.06491632461547851
			 train-loss:  2.2155189690042714 	 ± 0.31438456848083063
	data : 0.11423964500427246
	model : 0.06498932838439941
			 train-loss:  2.2206737706738133 	 ± 0.3144270852484493
	data : 0.11426701545715331
	model : 0.06491451263427735
			 train-loss:  2.2202691710184492 	 ± 0.31193791903191215
	data : 0.11409645080566407
	model : 0.06484675407409668
			 train-loss:  2.2204782720655203 	 ± 0.3094957594601192
	data : 0.1140326976776123
	model : 0.06486291885375976
			 train-loss:  2.2192572685388416 	 ± 0.30726110025162473
	data : 0.11406311988830567
	model : 0.06485905647277831
			 train-loss:  2.2319652474287786 	 ± 0.3216768033371403
	data : 0.11415762901306152
	model : 0.06480932235717773
			 train-loss:  2.2334805833759592 	 ± 0.31950446075961075
	data : 0.1142852783203125
	model : 0.06487884521484374
			 train-loss:  2.229893205796971 	 ± 0.3185029356204287
	data : 0.11432723999023438
	model : 0.06501379013061523
			 train-loss:  2.241160245909207 	 ± 0.32955463352490866
	data : 0.1143369197845459
	model : 0.06497054100036621
			 train-loss:  2.239524185657501 	 ± 0.3274743201461948
	data : 0.11429948806762695
	model : 0.06494593620300293
			 train-loss:  2.242644634045346 	 ± 0.3262064066094523
	data : 0.11414833068847656
	model : 0.06495599746704102
			 train-loss:  2.2438140130705304 	 ± 0.32408298813402936
	data : 0.11408123970031739
	model : 0.0649343490600586
			 train-loss:  2.2409379139338452 	 ± 0.32277949001455575
	data : 0.11419029235839843
	model : 0.06488704681396484
			 train-loss:  2.234953741769533 	 ± 0.32464261159329766
	data : 0.11430940628051758
	model : 0.06494946479797363
			 train-loss:  2.236458756128947 	 ± 0.32273085170213633
	data : 0.11433525085449218
	model : 0.06496000289916992
			 train-loss:  2.2353309549783407 	 ± 0.32074933297732333
	data : 0.11440138816833496
	model : 0.06497058868408204
			 train-loss:  2.231446263077971 	 ± 0.3204542590283242
	data : 0.11442022323608399
	model : 0.06498908996582031
			 train-loss:  2.232450225414374 	 ± 0.3185152949642945
	data : 0.1144218921661377
	model : 0.06496806144714355
			 train-loss:  2.2339884329445754 	 ± 0.31678438267885256
	data : 0.11417036056518555
	model : 0.06491303443908691
			 train-loss:  2.2377431482076644 	 ± 0.3165622748294551
	data : 0.11415038108825684
	model : 0.06487522125244141
			 train-loss:  2.2330003211527694 	 ± 0.3174492799872576
	data : 0.11406798362731933
	model : 0.06492748260498046
			 train-loss:  2.234147640263162 	 ± 0.3156766022753637
	data : 0.11421570777893067
	model : 0.06491589546203613
			 train-loss:  2.233685899929828 	 ± 0.31379703131823516
	data : 0.11426177024841308
	model : 0.06491999626159668
			 train-loss:  2.232938242810113 	 ± 0.3119979618017848
	data : 0.11447739601135254
	model : 0.06496820449829102
			 train-loss:  2.2344667252372292 	 ± 0.3104734534840304
	data : 0.11432194709777832
	model : 0.06496739387512207
			 train-loss:  2.233633064946463 	 ± 0.30875877588803063
	data : 0.11427350044250488
	model : 0.06487789154052734
			 train-loss:  2.233596246818016 	 ± 0.3069793614607192
	data : 0.11404047012329102
	model : 0.06491303443908691
			 train-loss:  2.235835323279554 	 ± 0.3059438384915191
	data : 0.11407504081726075
	model : 0.0649186611175537
			 train-loss:  2.2392800590965187 	 ± 0.30593161752878323
	data : 0.11410408020019532
	model : 0.06489949226379395
			 train-loss:  2.2366964168018764 	 ± 0.3052020855445088
	data : 0.11437220573425293
	model : 0.06491446495056152
			 train-loss:  2.2371977384273825 	 ± 0.30355777751331475
	data : 0.11447954177856445
	model : 0.0649524211883545
			 train-loss:  2.2372274930062503 	 ± 0.30190363280911237
	data : 0.1145096778869629
	model : 0.0648949146270752
			 train-loss:  2.235269042753404 	 ± 0.3008631077753168
	data : 0.11426801681518554
	model : 0.06487822532653809
			 train-loss:  2.233572167284945 	 ± 0.29970556988342645
	data : 0.1142153263092041
	model : 0.06484742164611816
			 train-loss:  2.234516007021854 	 ± 0.29826440818242794
	data : 0.11417975425720214
	model : 0.0648536205291748
			 train-loss:  2.2338539573053517 	 ± 0.2967770420584187
	data : 0.11418876647949219
	model : 0.06487007141113281
			 train-loss:  2.233342852789102 	 ± 0.2952857670932842
	data : 0.11419596672058105
	model : 0.0648838996887207
			 train-loss:  2.235828091903609 	 ± 0.29479325786254124
	data : 0.11449556350708008
	model : 0.06490592956542969
			 train-loss:  2.2335248246337427 	 ± 0.29418557081619
	data : 0.11448435783386231
	model : 0.06492795944213867
			 train-loss:  2.23209232211113 	 ± 0.2930577636916229
	data : 0.11442971229553223
	model : 0.06491084098815918
			 train-loss:  2.23675274258793 	 ± 0.2953040458343772
	data : 0.11433939933776856
	model : 0.06487388610839843
			 train-loss:  2.2374000607752333 	 ± 0.2939249133932571
	data : 0.11425461769104003
	model : 0.06478691101074219
			 train-loss:  2.2378886220524614 	 ± 0.29253622927534173
	data : 0.1140406608581543
	model : 0.06476926803588867
			 train-loss:  2.237863179582816 	 ± 0.29112652242663273
	data : 0.11415891647338867
	model : 0.0647468090057373
			 train-loss:  2.2345391194025677 	 ± 0.29171321535073674
	data : 0.11415042877197265
	model : 0.06479263305664062
			 train-loss:  2.2352412028132744 	 ± 0.2904230683229172
	data : 0.11436758041381836
	model : 0.06482958793640137
			 train-loss:  2.232722894053593 	 ± 0.29022322708620585
	data : 0.11451692581176758
	model : 0.06496477127075195
			 train-loss:  2.2294721172915564 	 ± 0.29082700418919466
	data : 0.11453070640563964
	model : 0.06496448516845703
			 train-loss:  2.226736872568043 	 ± 0.29088208739036137
	data : 0.11438574790954589
	model : 0.06497859954833984
			 train-loss:  2.226918189092116 	 ± 0.28956306511652646
	data : 0.11431446075439453
	model : 0.06491241455078126
			 train-loss:  2.2267757353481947 	 ± 0.2882596479267841
	data : 0.11419887542724609
	model : 0.06491098403930665
			 train-loss:  2.2289492370826856 	 ± 0.2878820825387386
	data : 0.11416778564453126
	model : 0.064874267578125
			 train-loss:  2.2266239518612885 	 ± 0.2876599646163666
	data : 0.11426215171813965
	model : 0.06489334106445313
			 train-loss:  2.2255773973046686 	 ± 0.28661151336383883
	data : 0.11439108848571777
	model : 0.06487603187561035
			 train-loss:  2.2250017383824225 	 ± 0.28542884024861936
	data : 0.11439681053161621
	model : 0.06487717628479003
			 train-loss:  2.2251456698466994 	 ± 0.2842000719236932
	data : 0.11433882713317871
	model : 0.06482501029968261
			 train-loss:  2.2223860860889793 	 ± 0.2845394820063477
	data : 0.1142852783203125
	model : 0.0648228645324707
			 train-loss:  2.2278286572230064 	 ± 0.2893826481077431
	data : 0.11421961784362793
	model : 0.06481051445007324
			 train-loss:  2.2284932967995394 	 ± 0.2882546202171316
	data : 0.1141242504119873
	model : 0.06481719017028809
			 train-loss:  2.228818393747012 	 ± 0.2870729528085567
	data : 0.11421360969543456
	model : 0.06489253044128418
			 train-loss:  2.2280487629992902 	 ± 0.28600852847063174
	data : 0.1143404483795166
	model : 0.06494345664978027
			 train-loss:  2.228384943282018 	 ± 0.28485795483729104
	data : 0.11441636085510254
	model : 0.06497626304626465
			 train-loss:  2.228308742608481 	 ± 0.2836988809859151
	data : 0.11447610855102539
	model : 0.06499438285827637
			 train-loss:  2.2274612082589056 	 ± 0.2827089222648399
	data : 0.11437439918518066
	model : 0.064945650100708
			 train-loss:  2.2276955404281615 	 ± 0.2815879064953787
	data : 0.11436386108398437
	model : 0.06488761901855469
			 train-loss:  2.226298393711211 	 ± 0.2809029224787941
	data : 0.11424832344055176
	model : 0.06486706733703614
			 train-loss:  2.225786581752807 	 ± 0.27985379593487664
	data : 0.11424007415771484
	model : 0.06480798721313477
			 train-loss:  2.2257462115958333 	 ± 0.27875884479804997
	data : 0.1142350196838379
	model : 0.06480903625488281
			 train-loss:  2.2242903016334354 	 ± 0.2781644044138441
	data : 0.11437773704528809
	model : 0.0648794174194336
			 train-loss:  2.223542920442728 	 ± 0.27722246818642915
	data : 0.11438651084899902
	model : 0.0648956298828125
			 train-loss:  2.223958646068136 	 ± 0.2762030154181769
	data : 0.11437578201293945
	model : 0.06490879058837891
			 train-loss:  2.2236801304600458 	 ± 0.2751732678238503
	data : 0.11437320709228516
	model : 0.0649287223815918
			 train-loss:  2.223180856023516 	 ± 0.27419683778375153
	data : 0.11432418823242188
	model : 0.06489024162292481
			 train-loss:  2.2243173540528143 	 ± 0.273486047943657
	data : 0.11438298225402832
	model : 0.06486301422119141
			 train-loss:  2.2241025774567214 	 ± 0.27248259668000385
	data : 0.11434922218322754
	model : 0.06488418579101562
			 train-loss:  2.223841792520355 	 ± 0.2714958831731537
	data : 0.11438183784484864
	model : 0.06487827301025391
			 train-loss:  2.2220039263258884 	 ± 0.2713509896217423
	data : 0.11437602043151855
	model : 0.06489601135253906
			 train-loss:  2.220620198526244 	 ± 0.2708507213420979
	data : 0.11441245079040527
	model : 0.06488304138183594
			 train-loss:  2.2215912307766703 	 ± 0.27011564810971533
	data : 0.11427197456359864
	model : 0.0648719310760498
			 train-loss:  2.2192123906952994 	 ± 0.27060651736359786
	data : 0.11423554420471191
	model : 0.06484003067016601
			 train-loss:  2.2178184343567975 	 ± 0.270149174568338
	data : 0.11423788070678711
	model : 0.06486268043518066
			 train-loss:  2.2188003214312273 	 ± 0.2694486355726824
	data : 0.11425108909606933
	model : 0.06487593650817872
			 train-loss:  2.217583084439898 	 ± 0.26889636202182265
	data : 0.1142810344696045
	model : 0.06494488716125488
			 train-loss:  2.2168642861975565 	 ± 0.2680988952478656
	data : 0.11432151794433594
	model : 0.06494240760803223
			 train-loss:  2.2140097766086972 	 ± 0.2693597194500661
	data : 0.11441588401794434
	model : 0.06493186950683594
			 train-loss:  2.2123766182220144 	 ± 0.2691550736332915
	data : 0.11438570022583008
	model : 0.06491403579711914
			 train-loss:  2.209628164362745 	 ± 0.2702859930663924
	data : 0.11429071426391602
	model : 0.06487064361572266
			 train-loss:  2.2135060582612014 	 ± 0.2734437810946585
	data : 0.11428370475769042
	model : 0.06483125686645508
			 train-loss:  2.213722418618682 	 ± 0.27253735048466576
	data : 0.11427288055419922
	model : 0.0648719310760498
			 train-loss:  2.211818299293518 	 ± 0.27261998260717907
	data : 0.11425533294677734
	model : 0.06495537757873535
			 train-loss:  2.212334198667514 	 ± 0.2717892222866369
	data : 0.11438684463500977
	model : 0.06497550010681152
			 train-loss:  2.2103850708196036 	 ± 0.2719504781823703
	data : 0.11441106796264648
	model : 0.06501812934875488
			 train-loss:  2.209066980804493 	 ± 0.27154697937074396
	data : 0.11436452865600585
	model : 0.06505055427551269
			 train-loss:  2.208518911491741 	 ± 0.27074878301596955
	data : 0.11432886123657227
	model : 0.06501941680908203
			 train-loss:  2.207939157947417 	 ± 0.26996986910731424
	data : 0.11425929069519043
	model : 0.06492547988891602
			 train-loss:  2.2065865160563054 	 ± 0.2696295997107949
	data : 0.11413707733154296
	model : 0.06495366096496583
			 train-loss:  2.207165790211623 	 ± 0.26886690030150784
	data : 0.11422772407531738
	model : 0.06491322517395019
			 train-loss:  2.210252928582928 	 ± 0.27079172555242365
	data : 0.11430611610412597
	model : 0.06490459442138671
			 train-loss:  2.2089483745442995 	 ± 0.27043644228531144
	data : 0.11439743041992187
	model : 0.0649341106414795
			 train-loss:  2.211501205712557 	 ± 0.27150499940428063
	data : 0.11443653106689453
	model : 0.06496553421020508
			 train-loss:  2.2125173821952773 	 ± 0.2709655441582047
	data : 0.11440825462341309
	model : 0.06491718292236329
			 train-loss:  2.2115886910462086 	 ± 0.2703848352126006
	data : 0.11428923606872558
	model : 0.06496119499206543
			 train-loss:  2.2106086015701294 	 ± 0.26984265274063757
	data : 0.11414861679077148
	model : 0.06500744819641113
			 train-loss:  2.2155677014734687 	 ± 0.2763687183789336
	data : 0.11412167549133301
	model : 0.06496100425720215
			 train-loss:  2.2138942219994284 	 ± 0.2763621698402502
	data : 0.11408629417419433
	model : 0.0649559497833252
			 train-loss:  2.214409939495914 	 ± 0.27560812104698107
	data : 0.11420221328735351
	model : 0.06498780250549316
			 train-loss:  2.21399480068755 	 ± 0.2748337598359767
	data : 0.11424646377563477
	model : 0.06495833396911621
			 train-loss:  2.2140346588123414 	 ± 0.27401506549476
	data : 0.11429600715637207
	model : 0.06487712860107422
			 train-loss:  2.2110326134946923 	 ± 0.2759602017717876
	data : 0.11428594589233398
	model : 0.06489334106445313
			 train-loss:  2.210292002734016 	 ± 0.2753157556128089
	data : 0.11435790061950683
	model : 0.06488256454467774
			 train-loss:  2.2091288162253755 	 ± 0.27492818724267826
	data : 0.11468925476074218
	model : 0.06488957405090331
			 train-loss:  2.2082803942436398 	 ± 0.27435223109667956
	data : 0.1146632194519043
	model : 0.06486091613769532
			 train-loss:  2.208944137385815 	 ± 0.2736966210747948
	data : 0.11471428871154785
	model : 0.06486916542053223
			 train-loss:  2.206928795781629 	 ± 0.2741933280951647
	data : 0.11474723815917968
	model : 0.06495513916015624
			 train-loss:  2.2069491686139786 	 ± 0.27340892829243924
	data : 0.11472930908203124
	model : 0.06498241424560547
			 train-loss:  2.2070487927306783 	 ± 0.27263427729165196
	data : 0.11434354782104492
	model : 0.06495051383972168
			 train-loss:  2.2060593306008034 	 ± 0.2721797549002778
	data : 0.11433401107788085
	model : 0.06499781608581542
			 train-loss:  2.206334527958645 	 ± 0.2714388215196922
	data : 0.11432313919067383
	model : 0.06495695114135742
			 train-loss:  2.2071085642169974 	 ± 0.2708764748605568
	data : 0.11426711082458496
	model : 0.06491546630859375
			 train-loss:  2.2043609903918373 	 ± 0.272612785696121
	data : 0.11415939331054688
	model : 0.06493215560913086
			 train-loss:  2.2019233433581187 	 ± 0.27381876830761215
	data : 0.11420435905456543
	model : 0.06494617462158203
			 train-loss:  2.2035999658343557 	 ± 0.27399554791838443
	data : 0.11443505287170411
	model : 0.06493330001831055
			 train-loss:  2.202785607244148 	 ± 0.2734666725380458
	data : 0.11447210311889648
	model : 0.06502947807312012
			 train-loss:  2.2011655497810114 	 ± 0.2736016910839518
	data : 0.11446785926818848
	model : 0.06500720977783203
			 train-loss:  2.2003861652838217 	 ± 0.27306595749304785
	data : 0.1145327091217041
	model : 0.06497950553894043
			 train-loss:  2.1994689811942396 	 ± 0.27261650148823613
	data : 0.11438927650451661
	model : 0.06495156288146972
			 train-loss:  2.1981455317155563 	 ± 0.2724850591755468
	data : 0.1142423152923584
	model : 0.06495914459228516
			 train-loss:  2.199638374942414 	 ± 0.2725250729021355
	data : 0.11409821510314941
	model : 0.06489319801330566
			 train-loss:  2.1993684409156677 	 ± 0.27182834905210657
	data : 0.11416478157043457
	model : 0.06493663787841797
			 train-loss:  2.198683010276995 	 ± 0.27127577895699395
	data : 0.11465067863464355
	model : 0.06494145393371582
			 train-loss:  2.1986602906781343 	 ± 0.27056488222685005
	data : 0.11473307609558106
	model : 0.06497983932495117
			 train-loss:  2.1974401958286762 	 ± 0.2703856619841163
	data : 0.11468791961669922
	model : 0.06499409675598145
			 train-loss:  2.1963291483221896 	 ± 0.27012333493104784
	data : 0.11488838195800781
	model : 0.06500363349914551
			 train-loss:  2.1963172001937 	 ± 0.2694262924230281
	data : 0.1147397518157959
	model : 0.06488852500915528
			 train-loss:  2.196764156757257 	 ± 0.2688066653724778
	data : 0.11428380012512207
	model : 0.06486039161682129
			 train-loss:  2.1984453973721485 	 ± 0.26914595708887484
	data : 0.11421761512756348
	model : 0.06482920646667481
			 train-loss:  2.1984104958887634 	 ± 0.2684624210852717
	data : 0.1141965389251709
	model : 0.06481199264526367
			 train-loss:  2.1982001409386145 	 ± 0.26779990341124715
	data : 0.11411385536193848
	model : 0.06488347053527832
			 train-loss:  2.1991877909281743 	 ± 0.26748746137297047
	data : 0.11415948867797851
	model : 0.06496291160583496
			 train-loss:  2.198452188372612 	 ± 0.26701961612328756
	data : 0.11409487724304199
	model : 0.06502180099487305
			 train-loss:  2.1976210009399337 	 ± 0.2666138146197493
	data : 0.11421990394592285
	model : 0.06501421928405762
			 train-loss:  2.198919133974774 	 ± 0.2665890948570634
	data : 0.11421661376953125
	model : 0.0649897575378418
			 train-loss:  2.200224995613098 	 ± 0.2665785319585541
	data : 0.1141279697418213
	model : 0.06490674018859863
			 train-loss:  2.2014187369860854 	 ± 0.2664677071450793
	data : 0.1142354965209961
	model : 0.06490416526794433
			 train-loss:  2.2004877340502853 	 ± 0.2661493814883872
	data : 0.11454186439514161
	model : 0.06492252349853515
			 train-loss:  2.1996298802709116 	 ± 0.2657865569131421
	data : 0.11454629898071289
	model : 0.06497673988342285
			 train-loss:  2.2001442995624267 	 ± 0.265246562568342
	data : 0.11447720527648926
	model : 0.06496448516845703
			 train-loss:  2.2002100732464056 	 ± 0.2646098745853828
	data : 0.11452202796936035
	model : 0.06500635147094727
			 train-loss:  2.200726778883683 	 ± 0.26408124194891175
	data : 0.11439456939697265
	model : 0.06500201225280762
			 train-loss:  2.2023488572665624 	 ± 0.2644933273991148
	data : 0.11411108970642089
	model : 0.06492762565612793
			 train-loss:  2.2039508847828726 	 ± 0.2648851357772618
	data : 0.11406817436218261
	model : 0.0648838996887207
			 train-loss:  2.2050167485228123 	 ± 0.26471282945319724
	data : 0.11419267654418945
	model : 0.06490130424499511
			 train-loss:  2.2045054989801325 	 ± 0.2641955963717138
	data : 0.114208984375
	model : 0.06488533020019531
			 train-loss:  2.207392970535243 	 ± 0.26692514666025124
	data : 0.1143406867980957
	model : 0.06486601829528808
			 train-loss:  2.207785026971684 	 ± 0.2663654193575409
	data : 0.11429338455200196
	model : 0.06489253044128418
			 train-loss:  2.20791516315054 	 ± 0.26575496804252907
	data : 0.11416959762573242
	model : 0.06488776206970215
			 train-loss:  2.20688476210915 	 ± 0.2655740421277978
	data : 0.11404561996459961
	model : 0.06482062339782715
			 train-loss:  2.206412482699123 	 ± 0.26505554717363145
	data : 0.11416015625
	model : 0.06482839584350586
			 train-loss:  2.206969810947436 	 ± 0.2645777023200985
	data : 0.11420121192932128
	model : 0.06484394073486328
			 train-loss:  2.2044949558648197 	 ± 0.2665042726567881
	data : 0.11440806388854981
	model : 0.06486315727233886
			 train-loss:  2.2032553642583648 	 ± 0.2665355484915921
	data : 0.11463804244995117
	model : 0.06544990539550781
			 train-loss:  2.2018664280573526 	 ± 0.26673495081043946
	data : 0.11441731452941895
	model : 0.06546721458435059
			 train-loss:  2.2012741640544258 	 ± 0.26628248001392435
	data : 0.11436042785644532
	model : 0.06537013053894043
			 train-loss:  2.2018714589732036 	 ± 0.26583711303759844
	data : 0.1143345832824707
	model : 0.06523847579956055
			 train-loss:  2.2020215235816107 	 ± 0.26525521468752766
	data : 0.11415543556213378
	model : 0.06502165794372558
			 train-loss:  2.205810260983695 	 ± 0.27070051630509256
	data : 0.1140674114227295
	model : 0.06425437927246094
			 train-loss:  2.2050043874900247 	 ± 0.27037516013311735
	data : 0.11444525718688965
	model : 0.06410627365112305
			 train-loss:  2.2067746990605404 	 ± 0.2710968828930248
	data : 0.11447310447692871
	model : 0.06400799751281738
			 train-loss:  2.2072937821717242 	 ± 0.27061785139185096
	data : 0.11452598571777343
	model : 0.06395998001098632
			 train-loss:  2.2058124438576074 	 ± 0.2709577861509901
	data : 0.11461033821105956
	model : 0.06398963928222656
			 train-loss:  2.205185794211053 	 ± 0.2705376372294785
	data : 0.11463365554809571
	model : 0.06397099494934082
			 train-loss:  2.203977942980569 	 ± 0.27057742577365473
	data : 0.11449990272521973
	model : 0.06396980285644531
			 train-loss:  2.2026798233965437 	 ± 0.27071918048831517
	data : 0.11469826698303223
	model : 0.06395845413208008
			 train-loss:  2.2021419655563483 	 ± 0.2702648315656513
	data : 0.11482400894165039
	model : 0.06396775245666504
			 train-loss:  2.2035150893191076 	 ± 0.2705059283689554
	data : 0.11495108604431152
	model : 0.06396069526672363
			 train-loss:  2.20432677814516 	 ± 0.27021885087220576
	data : 0.11513295173645019
	model : 0.06398711204528809
			 train-loss:  2.205753327422001 	 ± 0.2705372500976578
	data : 0.11532449722290039
	model : 0.06400032043457031
			 train-loss:  2.2054111857374177 	 ± 0.2700196741476133
	data : 0.11507797241210938
	model : 0.06397438049316406
			 train-loss:  2.2043178246111053 	 ± 0.26998161683365596
	data : 0.11487655639648438
	model : 0.06397013664245606
			 train-loss:  2.204794483880202 	 ± 0.26951932501935405
	data : 0.1149059772491455
	model : 0.06393222808837891
			 train-loss:  2.204471088049323 	 ± 0.2690062329611066
	data : 0.11487507820129395
	model : 0.0639388084411621
			 train-loss:  2.2031082939510505 	 ± 0.26928222033232185
	data : 0.1149055004119873
	model : 0.06398200988769531
			 train-loss:  2.2037772994963722 	 ± 0.2689290216588712
	data : 0.11507158279418946
	model : 0.06399235725402833
			 train-loss:  2.2037282332045134 	 ± 0.2683784617261733
	data : 0.11516366004943848
	model : 0.06398653984069824
			 train-loss:  2.2039043650335195 	 ± 0.267844321275262
	data : 0.11498966217041015
	model : 0.06397690773010253
			 train-loss:  2.2029391837313894 	 ± 0.2677259567643776
	data : 0.11482844352722169
	model : 0.06397762298583984
			 train-loss:  2.2024462290620996 	 ± 0.26729529726584916
	data : 0.11490030288696289
	model : 0.06394233703613281
			 train-loss:  2.2019717635646945 	 ± 0.2668600534484349
	data : 0.11491260528564454
	model : 0.06398706436157227
			 train-loss:  2.201462332982136 	 ± 0.26644445539350503
	data : 0.11491966247558594
	model : 0.06398382186889648
			 train-loss:  2.202166630744934 	 ± 0.26614317578708196
	data : 0.1150136947631836
	model : 0.06398167610168456
			 train-loss:  2.2023919284106253 	 ± 0.2656363676482267
	data : 0.11523957252502441
	model : 0.06393685340881347
			 train-loss:  2.202968391161116 	 ± 0.26526605289430977
	data : 0.11504497528076171
	model : 0.06387557983398437
			 train-loss:  2.2024250303803696 	 ± 0.26488177125043677
	data : 0.11484861373901367
	model : 0.06383371353149414
			 train-loss:  2.201683500620324 	 ± 0.2646228244142639
	data : 0.11489548683166503
	model : 0.06385622024536133
			 train-loss:  2.2022011382907043 	 ± 0.2642322641243542
	data : 0.11496648788452149
	model : 0.06395621299743652
			 train-loss:  2.201934321783483 	 ± 0.2637500973766097
	data : 0.11448078155517578
	model : 0.05559072494506836
#epoch  53    val-loss:  2.392331700575979  train-loss:  2.201934321783483  lr:  1.52587890625e-07
			 train-loss:  2.2925591468811035 	 ± 0.0
	data : 5.5018370151519775
	model : 0.07419943809509277
			 train-loss:  2.2445486783981323 	 ± 0.04801046848297119
	data : 2.815606117248535
	model : 0.07088899612426758
			 train-loss:  2.2582682768503823 	 ± 0.043739282336951416
	data : 1.914380629857381
	model : 0.06880680720011394
			 train-loss:  2.2420190572738647 	 ± 0.0471906243341972
	data : 1.4643846154212952
	model : 0.06774502992630005
			 train-loss:  2.2498050689697267 	 ± 0.044989464684006275
	data : 1.1943925857543944
	model : 0.06712102890014648
			 train-loss:  2.200203458468119 	 ± 0.11827218086075998
	data : 0.11686711311340332
	model : 0.06524109840393066
			 train-loss:  2.1939468724387035 	 ± 0.11056603526567166
	data : 0.11382336616516113
	model : 0.06469316482543945
			 train-loss:  2.199393481016159 	 ± 0.10442413969023462
	data : 0.11414222717285157
	model : 0.06466012001037598
			 train-loss:  2.18652171558804 	 ± 0.10496789798017873
	data : 0.11411061286926269
	model : 0.06468348503112793
			 train-loss:  2.1849180698394775 	 ± 0.099697436090009
	data : 0.1141282558441162
	model : 0.0647592544555664
			 train-loss:  2.1706949797543613 	 ± 0.10516151201489199
	data : 0.11419620513916015
	model : 0.06480846405029297
			 train-loss:  2.1557222505410514 	 ± 0.1122647486800714
	data : 0.11427097320556641
	model : 0.06481723785400391
			 train-loss:  2.173336001542898 	 ± 0.12392261374117751
	data : 0.11438674926757812
	model : 0.06488065719604492
			 train-loss:  2.206199978079115 	 ± 0.16822732030127477
	data : 0.11435146331787109
	model : 0.06490387916564941
			 train-loss:  2.199606982866923 	 ± 0.16438455547338662
	data : 0.11427760124206543
	model : 0.06485948562622071
			 train-loss:  2.2040103897452354 	 ± 0.16007572998532923
	data : 0.11417293548583984
	model : 0.06475038528442383
			 train-loss:  2.202341255019693 	 ± 0.1554397224204486
	data : 0.11389455795288086
	model : 0.06470751762390137
			 train-loss:  2.1855114565955267 	 ± 0.16623572810819154
	data : 0.11383452415466308
	model : 0.06474165916442871
			 train-loss:  2.1827467868202612 	 ± 0.16222657220208928
	data : 0.11386017799377442
	model : 0.06477713584899902
			 train-loss:  2.216874670982361 	 ± 0.21709703996662405
	data : 0.11390156745910644
	model : 0.06479597091674805
			 train-loss:  2.2127692699432373 	 ± 0.2126590506931621
	data : 0.11393332481384277
	model : 0.06485824584960938
			 train-loss:  2.216056216846813 	 ± 0.20831496759076734
	data : 0.1140408992767334
	model : 0.06490240097045899
			 train-loss:  2.211792925129766 	 ± 0.20471503502073546
	data : 0.11400871276855469
	model : 0.06483597755432129
			 train-loss:  2.21178670724233 	 ± 0.20040476430037515
	data : 0.11379647254943848
	model : 0.06473493576049805
			 train-loss:  2.2156611919403075 	 ± 0.19727104686779073
	data : 0.1137420654296875
	model : 0.06472625732421874
			 train-loss:  2.209056624999413 	 ± 0.1962386543724585
	data : 0.11364789009094238
	model : 0.06475157737731933
			 train-loss:  2.203920567477191 	 ± 0.19434295432650797
	data : 0.11377339363098145
	model : 0.06476550102233887
			 train-loss:  2.212403876440866 	 ± 0.19586572136992417
	data : 0.11400132179260254
	model : 0.06481647491455078
			 train-loss:  2.2095107538946746 	 ± 0.1930670091791371
	data : 0.11410422325134277
	model : 0.06493525505065918
			 train-loss:  2.20161919593811 	 ± 0.1945209456281514
	data : 0.11423230171203613
	model : 0.06495656967163085
			 train-loss:  2.2051194329415598 	 ± 0.19231576785931045
	data : 0.11428022384643555
	model : 0.06491146087646485
			 train-loss:  2.1997660472989082 	 ± 0.19161937039374424
	data : 0.11430273056030274
	model : 0.06489033699035644
			 train-loss:  2.1966760230786875 	 ± 0.18950161198536875
	data : 0.11424875259399414
	model : 0.06487693786621093
			 train-loss:  2.190655561054454 	 ± 0.18987041878887922
	data : 0.11430253982543945
	model : 0.06500825881958008
			 train-loss:  2.1884062358311245 	 ± 0.18759737614437963
	data : 0.11399960517883301
	model : 0.06498961448669434
			 train-loss:  2.191134293874105 	 ± 0.1856762746341922
	data : 0.1140284538269043
	model : 0.0650390625
			 train-loss:  2.1896433701386324 	 ± 0.18336828129207586
	data : 0.11391220092773438
	model : 0.06503348350524903
			 train-loss:  2.1850127734636007 	 ± 0.1831186941759868
	data : 0.11378469467163085
	model : 0.0650144100189209
			 train-loss:  2.182796741143251 	 ± 0.181271233128429
	data : 0.11381926536560058
	model : 0.06483731269836426
			 train-loss:  2.1916486978530885 	 ± 0.18733314396289902
	data : 0.11391286849975586
	model : 0.06483254432678223
			 train-loss:  2.1939682553454145 	 ± 0.18561513040827424
	data : 0.11385908126831054
	model : 0.06482205390930176
			 train-loss:  2.187055914174943 	 ± 0.1886575377441417
	data : 0.11399016380310059
	model : 0.06484494209289551
			 train-loss:  2.187488647394402 	 ± 0.18647203063677445
	data : 0.11404824256896973
	model : 0.06494207382202148
			 train-loss:  2.1898163963447916 	 ± 0.18497173153402266
	data : 0.11415810585021972
	model : 0.06494178771972656
			 train-loss:  2.1946524805492826 	 ± 0.18569673551799165
	data : 0.11431183815002441
	model : 0.06494717597961426
			 train-loss:  2.1875782038854514 	 ± 0.1896989290352035
	data : 0.11438570022583008
	model : 0.06497335433959961
			 train-loss:  2.191422632400026 	 ± 0.18947267307102186
	data : 0.11428108215332031
	model : 0.06494321823120117
			 train-loss:  2.1872502267360687 	 ± 0.18965811590812917
	data : 0.1142648696899414
	model : 0.0649035930633545
			 train-loss:  2.183565329532234 	 ± 0.189440971365477
	data : 0.11410202980041503
	model : 0.06491117477416992
			 train-loss:  2.1805738592147828 	 ± 0.18870246378255634
	data : 0.11408123970031739
	model : 0.06495156288146972
			 train-loss:  2.1890274075900806 	 ± 0.19617220907195854
	data : 0.11405048370361329
	model : 0.06497368812561036
			 train-loss:  2.1854066527806797 	 ± 0.19598997615824723
	data : 0.11410479545593262
	model : 0.06500768661499023
			 train-loss:  2.184777561223732 	 ± 0.19418520547215645
	data : 0.11414008140563965
	model : 0.06500201225280762
			 train-loss:  2.187433573934767 	 ± 0.1933480878547898
	data : 0.11413154602050782
	model : 0.06498279571533203
			 train-loss:  2.1905898744409735 	 ± 0.1929812010549432
	data : 0.1139195442199707
	model : 0.06491403579711914
			 train-loss:  2.1956962943077087 	 ± 0.19496375722297998
	data : 0.11389808654785157
	model : 0.06486048698425292
			 train-loss:  2.1913316333503055 	 ± 0.19598679592382387
	data : 0.1138850212097168
	model : 0.06490755081176758
			 train-loss:  2.191049756674931 	 ± 0.1943015632759131
	data : 0.11382575035095215
	model : 0.06491856575012207
			 train-loss:  2.1971296213440974 	 ± 0.19813422278361162
	data : 0.11396045684814453
	model : 0.0649573802947998
			 train-loss:  2.1959262251853944 	 ± 0.19669348144766655
	data : 0.11417288780212402
	model : 0.06499681472778321
			 train-loss:  2.1941266138045514 	 ± 0.19557199918368706
	data : 0.1141897201538086
	model : 0.06501150131225586
			 train-loss:  2.1966926513179654 	 ± 0.19502090794655552
	data : 0.11409478187561035
	model : 0.06488914489746093
			 train-loss:  2.198599588303339 	 ± 0.19404873359016486
	data : 0.11420583724975586
	model : 0.06483755111694336
			 train-loss:  2.198233377188444 	 ± 0.19254870030729881
	data : 0.11420760154724122
	model : 0.06482701301574707
			 train-loss:  2.19539627295274 	 ± 0.19240520682263654
	data : 0.1142115592956543
	model : 0.06484804153442383
			 train-loss:  2.1949486154498477 	 ± 0.1909761344351044
	data : 0.11425952911376953
	model : 0.06483936309814453
			 train-loss:  2.193252360642846 	 ± 0.19004585749691844
	data : 0.11434917449951172
	model : 0.0649646282196045
			 train-loss:  2.1902146830278286 	 ± 0.19027488226249434
	data : 0.11421618461608887
	model : 0.06504054069519043
			 train-loss:  2.189679011054661 	 ± 0.18894268892513816
	data : 0.11411681175231933
	model : 0.06503496170043946
			 train-loss:  2.1905169112341745 	 ± 0.1877173205226164
	data : 0.11391592025756836
	model : 0.06495327949523926
			 train-loss:  2.1904522432407862 	 ± 0.18639146500066606
	data : 0.11397576332092285
	model : 0.06494312286376953
			 train-loss:  2.186470556590292 	 ± 0.18810867964263925
	data : 0.1140472412109375
	model : 0.06487317085266113
			 train-loss:  2.1848347791253704 	 ± 0.18733074047470508
	data : 0.11406340599060058
	model : 0.0648430347442627
			 train-loss:  2.1899995948817277 	 ± 0.1912220638563098
	data : 0.11415152549743653
	model : 0.06487083435058594
			 train-loss:  2.190731825828552 	 ± 0.1900473853258597
	data : 0.11433272361755371
	model : 0.06491870880126953
			 train-loss:  2.187514890181391 	 ± 0.19083742383209457
	data : 0.1142737865447998
	model : 0.06493844985961914
			 train-loss:  2.1891081379605577 	 ± 0.19010226364369487
	data : 0.11419687271118165
	model : 0.06492791175842286
			 train-loss:  2.1883877103145304 	 ± 0.18898549155831973
	data : 0.11431989669799805
	model : 0.06488690376281739
			 train-loss:  2.182502538343019 	 ± 0.19484601692753306
	data : 0.11418919563293457
	model : 0.06491074562072754
			 train-loss:  2.180439993739128 	 ± 0.1944903116168466
	data : 0.11415762901306152
	model : 0.0649336338043213
			 train-loss:  2.1895573286362637 	 ± 0.20978453557515994
	data : 0.11423344612121582
	model : 0.06493339538574219
			 train-loss:  2.18964570033841 	 ± 0.20850295464268548
	data : 0.11439623832702636
	model : 0.06499128341674805
			 train-loss:  2.1887682776853263 	 ± 0.20739535815446317
	data : 0.11430459022521973
	model : 0.06501169204711914
			 train-loss:  2.1949860680670965 	 ± 0.21379811308725832
	data : 0.11439681053161621
	model : 0.06495156288146972
			 train-loss:  2.1930081591886634 	 ± 0.21330844133834614
	data : 0.11443290710449219
	model : 0.06494412422180176
			 train-loss:  2.1939588452494423 	 ± 0.2122457043300855
	data : 0.11423869132995605
	model : 0.06488704681396484
			 train-loss:  2.1937130566301017 	 ± 0.2110346857552757
	data : 0.1140988826751709
	model : 0.06497774124145508
			 train-loss:  2.194515797224912 	 ± 0.2099657446677683
	data : 0.11414494514465331
	model : 0.06499433517456055
			 train-loss:  2.194064065311732 	 ± 0.20882583027151377
	data : 0.11418447494506836
	model : 0.06502571105957031
			 train-loss:  2.191653413242764 	 ± 0.20890402626828913
	data : 0.11423215866088868
	model : 0.06508107185363769
			 train-loss:  2.1902932229932848 	 ± 0.2081533870096132
	data : 0.11445269584655762
	model : 0.0651665210723877
			 train-loss:  2.1893993434698684 	 ± 0.207194567247907
	data : 0.11434516906738282
	model : 0.06503605842590332
			 train-loss:  2.1904542087226786 	 ± 0.20632584026481013
	data : 0.1143730640411377
	model : 0.06502742767333984
			 train-loss:  2.1890177422381463 	 ± 0.20569243013884478
	data : 0.11426115036010742
	model : 0.06499509811401367
			 train-loss:  2.1884524094431024 	 ± 0.20468037638138306
	data : 0.11419157981872559
	model : 0.06497230529785156
			 train-loss:  2.1880296543240547 	 ± 0.2036532313360587
	data : 0.11414084434509278
	model : 0.06497130393981934
			 train-loss:  2.1891811557651795 	 ± 0.20291465391654892
	data : 0.1142110824584961
	model : 0.06496477127075195
			 train-loss:  2.1911839660333126 	 ± 0.20283811630579143
	data : 0.11419034004211426
	model : 0.06495609283447265
			 train-loss:  2.187707361548838 	 ± 0.20472473431378055
	data : 0.11429495811462402
	model : 0.06503477096557617
			 train-loss:  2.188852186203003 	 ± 0.2040167796943162
	data : 0.11426949501037598
	model : 0.06500101089477539
			 train-loss:  2.191626723450009 	 ± 0.2048915437500385
	data : 0.11425323486328125
	model : 0.0649646282196045
			 train-loss:  2.1907810486999213 	 ± 0.20406176142702673
	data : 0.11437654495239258
	model : 0.06497044563293457
			 train-loss:  2.1885584291902562 	 ± 0.20430566017845275
	data : 0.11447219848632813
	model : 0.06496930122375488
			 train-loss:  2.184727360422795 	 ± 0.20700528727997394
	data : 0.1143913745880127
	model : 0.06494350433349609
			 train-loss:  2.186231496220543 	 ± 0.2065874501225138
	data : 0.1144132137298584
	model : 0.06493825912475586
			 train-loss:  2.1900896272569335 	 ± 0.20937691314470463
	data : 0.11451239585876465
	model : 0.06494994163513183
			 train-loss:  2.1894757758791203 	 ± 0.20849203004164507
	data : 0.11445245742797852
	model : 0.06502056121826172
			 train-loss:  2.1914874304223946 	 ± 0.20856519174593202
	data : 0.11438708305358887
	model : 0.06505169868469238
			 train-loss:  2.1897687529205183 	 ± 0.20837316967037717
	data : 0.11445260047912598
	model : 0.06501631736755371
			 train-loss:  2.189400448582389 	 ± 0.20745949451178217
	data : 0.11444015502929687
	model : 0.06502413749694824
			 train-loss:  2.188880384505332 	 ± 0.20659489459922264
	data : 0.11422953605651856
	model : 0.06499724388122559
			 train-loss:  2.1907805470483646 	 ± 0.20664255287527616
	data : 0.11420216560363769
	model : 0.06491246223449706
			 train-loss:  2.1924191270254356 	 ± 0.20645573905588954
	data : 0.11423697471618652
	model : 0.06487545967102051
			 train-loss:  2.192473968915772 	 ± 0.20554906362897477
	data : 0.11433467864990235
	model : 0.06487522125244141
			 train-loss:  2.1917800229528677 	 ± 0.20478750102823667
	data : 0.11442985534667968
	model : 0.06491918563842773
			 train-loss:  2.191320798520384 	 ± 0.2039623465089547
	data : 0.11449127197265625
	model : 0.06502995491027833
			 train-loss:  2.191311600880745 	 ± 0.20308886624163588
	data : 0.11455631256103516
	model : 0.06507477760314942
			 train-loss:  2.1898104548454285 	 ± 0.2028773160379748
	data : 0.11453356742858886
	model : 0.06512870788574218
			 train-loss:  2.186588396545218 	 ± 0.20503259626427628
	data : 0.11444668769836426
	model : 0.06515545845031738
			 train-loss:  2.184367142120997 	 ± 0.2056093100833466
	data : 0.11430897712707519
	model : 0.06509671211242676
			 train-loss:  2.1844915240264138 	 ± 0.20476245547878497
	data : 0.11428322792053222
	model : 0.06499109268188477
			 train-loss:  2.1849366856403036 	 ± 0.2039803231583512
	data : 0.1142500400543213
	model : 0.0649348258972168
			 train-loss:  2.1862027780796454 	 ± 0.20363020607628762
	data : 0.11415700912475586
	model : 0.06485724449157715
			 train-loss:  2.1856504621044284 	 ± 0.20289993857349295
	data : 0.1141977310180664
	model : 0.06490740776062012
			 train-loss:  2.185454887390137 	 ± 0.2020984436523005
	data : 0.11433582305908203
	model : 0.0649953842163086
			 train-loss:  2.186550087398953 	 ± 0.20166694527763926
	data : 0.1143712043762207
	model : 0.06501927375793456
			 train-loss:  2.1842821789538767 	 ± 0.20247813112750787
	data : 0.11443510055541992
	model : 0.06508083343505859
			 train-loss:  2.185198374092579 	 ± 0.20194976347124893
	data : 0.11445717811584473
	model : 0.06510863304138184
			 train-loss:  2.1829064844190613 	 ± 0.20282974944072732
	data : 0.1143270492553711
	model : 0.0650299072265625
			 train-loss:  2.1809520703095657 	 ± 0.20326384869202704
	data : 0.11430506706237793
	model : 0.0649606704711914
			 train-loss:  2.1794636440641098 	 ± 0.20319646909953284
	data : 0.11439647674560546
	model : 0.06498937606811524
			 train-loss:  2.1812511167742987 	 ± 0.20345654023692278
	data : 0.11442046165466309
	model : 0.06496658325195312
			 train-loss:  2.182075341841332 	 ± 0.2029113112729413
	data : 0.11450276374816895
	model : 0.06502017974853516
			 train-loss:  2.1829813069372035 	 ± 0.2024225822952359
	data : 0.11466860771179199
	model : 0.06503944396972657
			 train-loss:  2.185114932060242 	 ± 0.20317824627156317
	data : 0.11473503112792968
	model : 0.06503062248229981
			 train-loss:  2.184238177011995 	 ± 0.20268604903855544
	data : 0.11472253799438477
	model : 0.06502151489257812
			 train-loss:  2.182709076108724 	 ± 0.2027307496943471
	data : 0.11453742980957031
	model : 0.06496381759643555
			 train-loss:  2.183268341465273 	 ± 0.20210092304767732
	data : 0.11458649635314941
	model : 0.06488204002380371
			 train-loss:  2.1840408482997535 	 ± 0.2015770065161234
	data : 0.11443867683410644
	model : 0.06485462188720703
			 train-loss:  2.1844914623669216 	 ± 0.20092604645907655
	data : 0.11440873146057129
	model : 0.06487474441528321
			 train-loss:  2.185415134362295 	 ± 0.20051034594744507
	data : 0.11435899734497071
	model : 0.06487889289855957
			 train-loss:  2.1863892044819577 	 ± 0.20013758227540784
	data : 0.11451125144958496
	model : 0.06493310928344727
			 train-loss:  2.185605757719987 	 ± 0.1996549638629743
	data : 0.11429672241210938
	model : 0.06498913764953614
			 train-loss:  2.1842330818374953 	 ± 0.19963649619379126
	data : 0.11443729400634765
	model : 0.0650184154510498
			 train-loss:  2.186228262967077 	 ± 0.2003823796543735
	data : 0.11429300308227539
	model : 0.0649899959564209
			 train-loss:  2.18596372130799 	 ± 0.19972036523152342
	data : 0.11434531211853027
	model : 0.06493582725524902
			 train-loss:  2.1853969283655386 	 ± 0.1991576735720992
	data : 0.11428155899047851
	model : 0.06492247581481933
			 train-loss:  2.185550664727752 	 ± 0.1984924551765995
	data : 0.11450490951538086
	model : 0.06485285758972167
			 train-loss:  2.1858085569919354 	 ± 0.19785012883387035
	data : 0.11451826095581055
	model : 0.06486496925354004
			 train-loss:  2.185867474079132 	 ± 0.19719083701320056
	data : 0.1146728515625
	model : 0.06492295265197753
			 train-loss:  2.190038940764421 	 ± 0.20306867044459778
	data : 0.11466679573059083
	model : 0.06493258476257324
			 train-loss:  2.1903308951540996 	 ± 0.20243137222982344
	data : 0.11466774940490723
	model : 0.06494307518005371
			 train-loss:  2.1905587225957635 	 ± 0.20178829748053118
	data : 0.11453108787536621
	model : 0.06498522758483886
			 train-loss:  2.191118250419567 	 ± 0.2012511142019932
	data : 0.1144484519958496
	model : 0.06490821838378906
			 train-loss:  2.192900044687333 	 ± 0.2018158206289234
	data : 0.11436467170715332
	model : 0.06482019424438476
			 train-loss:  2.1919330901060348 	 ± 0.20152782262906946
	data : 0.11434330940246581
	model : 0.06483578681945801
			 train-loss:  2.1917700319533138 	 ± 0.20089531237930583
	data : 0.11447291374206543
	model : 0.06484127044677734
			 train-loss:  2.190904002400893 	 ± 0.20055234084953916
	data : 0.11455941200256348
	model : 0.06487550735473632
			 train-loss:  2.191712677103918 	 ± 0.20017892619407263
	data : 0.11461005210876465
	model : 0.06492886543273926
			 train-loss:  2.191946402937174 	 ± 0.19957414860470152
	data : 0.11461071968078614
	model : 0.06501116752624511
			 train-loss:  2.190454402325316 	 ± 0.19984649360932752
	data : 0.1146458625793457
	model : 0.06502184867858887
			 train-loss:  2.1888025366229775 	 ± 0.20032823220512805
	data : 0.1145620346069336
	model : 0.06499681472778321
			 train-loss:  2.190997653212284 	 ± 0.2016576273659076
	data : 0.11439871788024902
	model : 0.0649221420288086
			 train-loss:  2.191164890440499 	 ± 0.20105321514255162
	data : 0.11425743103027344
	model : 0.06492385864257813
			 train-loss:  2.191513911160556 	 ± 0.20049286481328557
	data : 0.11433525085449218
	model : 0.0648843765258789
			 train-loss:  2.191327521599919 	 ± 0.19990239661516004
	data : 0.11437039375305176
	model : 0.06488304138183594
			 train-loss:  2.18953453495117 	 ± 0.2006373293389573
	data : 0.11436452865600585
	model : 0.06494178771972656
			 train-loss:  2.189450355512755 	 ± 0.20004226105820339
	data : 0.11449103355407715
	model : 0.06499133110046387
			 train-loss:  2.188751918324352 	 ± 0.19965488378458676
	data : 0.11463074684143067
	model : 0.06498265266418457
			 train-loss:  2.1895798760301926 	 ± 0.19935757246368788
	data : 0.11446528434753418
	model : 0.06493806838989258
			 train-loss:  2.1879677117219445 	 ± 0.19988213139137156
	data : 0.11429944038391113
	model : 0.06493091583251953
			 train-loss:  2.18834867172463 	 ± 0.19936248308504823
	data : 0.11429924964904785
	model : 0.06487545967102051
			 train-loss:  2.188079209686015 	 ± 0.19881686636496126
	data : 0.11431183815002441
	model : 0.06488509178161621
			 train-loss:  2.186742429760681 	 ± 0.19902291509946587
	data : 0.1142648696899414
	model : 0.0649594783782959
			 train-loss:  2.186361760411944 	 ± 0.19851698008046476
	data : 0.11442055702209472
	model : 0.06502375602722169
			 train-loss:  2.184466427700086 	 ± 0.19953377338693243
	data : 0.11447610855102539
	model : 0.06501150131225586
			 train-loss:  2.183015112823012 	 ± 0.19989872893645153
	data : 0.11442065238952637
	model : 0.06503772735595703
			 train-loss:  2.182878575298224 	 ± 0.19934470138136745
	data : 0.11427111625671386
	model : 0.06498675346374512
			 train-loss:  2.1834244268566536 	 ± 0.19892044649960133
	data : 0.11425309181213379
	model : 0.0649038314819336
			 train-loss:  2.183754116296768 	 ± 0.198416155527299
	data : 0.11420621871948242
	model : 0.064886474609375
			 train-loss:  2.1867364394730626 	 ± 0.20187230598431305
	data : 0.11420516967773438
	model : 0.06491470336914062
			 train-loss:  2.1897309252193997 	 ± 0.20530837905081276
	data : 0.11457953453063965
	model : 0.06489391326904297
			 train-loss:  2.192195805695539 	 ± 0.20742940964291043
	data : 0.1148153305053711
	model : 0.06494364738464356
			 train-loss:  2.192803437943044 	 ± 0.20702822153633527
	data : 0.11477293968200683
	model : 0.06496191024780273
			 train-loss:  2.194213165463628 	 ± 0.2073515714102414
	data : 0.1147653579711914
	model : 0.06495628356933594
			 train-loss:  2.1955125222923937 	 ± 0.20754724935608534
	data : 0.11483297348022461
	model : 0.06490235328674317
			 train-loss:  2.1957601971804777 	 ± 0.20701912541857953
	data : 0.11447882652282715
	model : 0.06491651535034179
			 train-loss:  2.194250465073484 	 ± 0.20749742981847427
	data : 0.11445508003234864
	model : 0.06492977142333985
			 train-loss:  2.1943154013346113 	 ± 0.20694968209973857
	data : 0.1145540714263916
	model : 0.06494779586791992
			 train-loss:  2.1954989176047475 	 ± 0.20704466626609572
	data : 0.11463756561279297
	model : 0.06498141288757324
			 train-loss:  2.1941971760145655 	 ± 0.20728004633977076
	data : 0.11456880569458008
	model : 0.06505918502807617
			 train-loss:  2.192835266391436 	 ± 0.2075945757026161
	data : 0.11459832191467285
	model : 0.0650744915008545
			 train-loss:  2.192351194243357 	 ± 0.207164682406188
	data : 0.1144604206085205
	model : 0.06505837440490722
			 train-loss:  2.193514361823957 	 ± 0.20726095623478707
	data : 0.11438274383544922
	model : 0.06500015258789063
			 train-loss:  2.192621454825768 	 ± 0.2071025929882989
	data : 0.1143233299255371
	model : 0.06496806144714355
			 train-loss:  2.1916398369536108 	 ± 0.20702788889778723
	data : 0.11438016891479492
	model : 0.06500802040100098
			 train-loss:  2.1910645356638176 	 ± 0.20665877910862815
	data : 0.11444511413574218
	model : 0.06499500274658203
			 train-loss:  2.192208975252479 	 ± 0.20676115189775698
	data : 0.11457467079162598
	model : 0.06497950553894043
			 train-loss:  2.1913910882556857 	 ± 0.20656185245361575
	data : 0.11462087631225586
	model : 0.06502647399902343
			 train-loss:  2.191238515377045 	 ± 0.20605604167992755
	data : 0.11449432373046875
	model : 0.06501469612121583
			 train-loss:  2.1924103006201596 	 ± 0.20620976984313488
	data : 0.11445870399475097
	model : 0.06492652893066406
			 train-loss:  2.1918614182141747 	 ± 0.20584585873077954
	data : 0.11440348625183105
	model : 0.06493639945983887
			 train-loss:  2.192823291412128 	 ± 0.2057927995652878
	data : 0.11431498527526855
	model : 0.06498246192932129
			 train-loss:  2.190712913578632 	 ± 0.20747813138273222
	data : 0.11442766189575196
	model : 0.06504545211791993
			 train-loss:  2.1903895459524017 	 ± 0.20702299479375125
	data : 0.11454696655273437
	model : 0.06514430046081543
			 train-loss:  2.1911498493361243 	 ± 0.20680660496424286
	data : 0.1146249771118164
	model : 0.06516923904418945
			 train-loss:  2.191483852368046 	 ± 0.2063621559769191
	data : 0.11470475196838378
	model : 0.06521658897399903
			 train-loss:  2.1911024978527656 	 ± 0.20593859881727902
	data : 0.11479601860046387
	model : 0.06521167755126953
			 train-loss:  2.190802109868903 	 ± 0.2054910044112227
	data : 0.11500377655029297
	model : 0.06512336730957032
			 train-loss:  2.189717011792319 	 ± 0.2056004829233436
	data : 0.11490035057067871
	model : 0.06505265235900878
			 train-loss:  2.189570906602941 	 ± 0.20512362674900914
	data : 0.11475715637207032
	model : 0.06502056121826172
			 train-loss:  2.188347969415053 	 ± 0.2054088565742874
	data : 0.11467561721801758
	model : 0.06500377655029296
			 train-loss:  2.1875444230899004 	 ± 0.20525982481635813
	data : 0.11458353996276856
	model : 0.06500678062438965
			 train-loss:  2.187024987746622 	 ± 0.20491995816282343
	data : 0.11439642906188965
	model : 0.06509542465209961
			 train-loss:  2.1863553989765254 	 ± 0.2046773643009898
	data : 0.11449756622314453
	model : 0.065116548538208
			 train-loss:  2.1855443815390267 	 ± 0.2045489948557511
	data : 0.11455903053283692
	model : 0.06512584686279296
			 train-loss:  2.186159381119337 	 ± 0.20427720230781235
	data : 0.11466450691223144
	model : 0.06506943702697754
			 train-loss:  2.187698881560509 	 ± 0.20506599063721587
	data : 0.1145561695098877
	model : 0.06494331359863281
			 train-loss:  2.18695111035212 	 ± 0.20489494684573023
	data : 0.11442294120788574
	model : 0.06482625007629395
			 train-loss:  2.188713225451383 	 ± 0.20608522051278658
	data : 0.1144474983215332
	model : 0.06471757888793946
			 train-loss:  2.1886077520534464 	 ± 0.20562438699982044
	data : 0.11450858116149902
	model : 0.06469140052795411
			 train-loss:  2.1893851080456295 	 ± 0.20548595626171873
	data : 0.11445112228393554
	model : 0.0646519660949707
			 train-loss:  2.1905447998389 	 ± 0.20575153770292803
	data : 0.11465935707092285
	model : 0.06467194557189941
			 train-loss:  2.190372695880277 	 ± 0.20530784389640216
	data : 0.11507034301757812
	model : 0.0645902156829834
			 train-loss:  2.1916370889875623 	 ± 0.20572330483126222
	data : 0.11518268585205078
	model : 0.06446266174316406
			 train-loss:  2.1938620225518153 	 ± 0.2079630650509179
	data : 0.11512489318847656
	model : 0.0642812728881836
			 train-loss:  2.1944387316178644 	 ± 0.20768553125901415
	data : 0.11511101722717285
	model : 0.06412544250488281
			 train-loss:  2.193214847853309 	 ± 0.20804836025709514
	data : 0.11525192260742187
	model : 0.06401848793029785
			 train-loss:  2.192442156341919 	 ± 0.20792122180161965
	data : 0.11500005722045899
	model : 0.063999605178833
			 train-loss:  2.1922556270723756 	 ± 0.20748792789465434
	data : 0.11511735916137696
	model : 0.06398997306823731
			 train-loss:  2.1917642127899897 	 ± 0.20717242445852135
	data : 0.1152801513671875
	model : 0.06398549079895019
			 train-loss:  2.190058484159667 	 ± 0.2083446868074961
	data : 0.11527385711669921
	model : 0.06396222114562988
			 train-loss:  2.190172029667146 	 ± 0.20790430798847756
	data : 0.11506171226501465
	model : 0.06389093399047852
			 train-loss:  2.1914205377937384 	 ± 0.20833309020317167
	data : 0.11512384414672852
	model : 0.06380677223205566
			 train-loss:  2.190453754080103 	 ± 0.2084147240205071
	data : 0.11495580673217773
	model : 0.06381387710571289
			 train-loss:  2.1894918341757887 	 ± 0.20849481171948117
	data : 0.11499624252319336
	model : 0.06381869316101074
			 train-loss:  2.191197793191998 	 ± 0.20969858922349013
	data : 0.11518850326538085
	model : 0.06381754875183106
			 train-loss:  2.189586955960057 	 ± 0.21072186055471182
	data : 0.11524848937988282
	model : 0.06387243270874024
			 train-loss:  2.1905969087050052 	 ± 0.21085699904806882
	data : 0.11513147354125977
	model : 0.06386537551879883
			 train-loss:  2.1916986385981243 	 ± 0.2111054748983137
	data : 0.11522493362426758
	model : 0.06383509635925293
			 train-loss:  2.192424927509672 	 ± 0.21096729911408066
	data : 0.11525678634643555
	model : 0.06384396553039551
			 train-loss:  2.1921515011590373 	 ± 0.21057375162904865
	data : 0.11521453857421875
	model : 0.06389236450195312
			 train-loss:  2.1917366863768777 	 ± 0.21023908205878336
	data : 0.11527824401855469
	model : 0.06390409469604492
			 train-loss:  2.1922764260260785 	 ± 0.20997645734951956
	data : 0.11536917686462403
	model : 0.06388592720031738
			 train-loss:  2.1904677546754177 	 ± 0.21144348715193143
	data : 0.115195894241333
	model : 0.06394047737121582
			 train-loss:  2.1894038755719256 	 ± 0.21166933528232493
	data : 0.11511015892028809
	model : 0.06397428512573242
			 train-loss:  2.1889084799569627 	 ± 0.21138327203773477
	data : 0.11506609916687012
	model : 0.06394877433776855
			 train-loss:  2.188483216589497 	 ± 0.21106251313117477
	data : 0.11506304740905762
	model : 0.06392893791198731
			 train-loss:  2.1873859818201944 	 ± 0.2113458125378409
	data : 0.1148941993713379
	model : 0.06395492553710938
			 train-loss:  2.1865306692123414 	 ± 0.21135406932917597
	data : 0.11507763862609863
	model : 0.06390981674194336
			 train-loss:  2.1875212638976564 	 ± 0.2115133371452379
	data : 0.11484832763671875
	model : 0.06382908821105956
			 train-loss:  2.187270227878813 	 ± 0.2111307136904975
	data : 0.11474466323852539
	model : 0.06379170417785644
			 train-loss:  2.1867977404311714 	 ± 0.21084649729450355
	data : 0.11481380462646484
	model : 0.0637725830078125
			 train-loss:  2.1883621206433754 	 ± 0.21189710722781727
	data : 0.11495003700256348
	model : 0.06378979682922363
			 train-loss:  2.1887499715767653 	 ± 0.21157153143577512
	data : 0.11487865447998047
	model : 0.06380624771118164
			 train-loss:  2.1875011934898794 	 ± 0.21209742633783116
	data : 0.11491498947143555
	model : 0.055408573150634764
#epoch  54    val-loss:  2.3946432753613123  train-loss:  2.1875011934898794  lr:  1.52587890625e-07
			 train-loss:  2.8605093955993652 	 ± 0.0
	data : 5.61121129989624
	model : 0.07070136070251465
			 train-loss:  2.4482256174087524 	 ± 0.4122837781906128
	data : 2.8705644607543945
	model : 0.06773722171783447
			 train-loss:  2.464487632115682 	 ± 0.33741297458368097
	data : 1.9519041379292805
	model : 0.06664586067199707
			 train-loss:  2.351886510848999 	 ± 0.3513156331485955
	data : 1.4925284385681152
	model : 0.06616228818893433
			 train-loss:  2.2953539371490477 	 ± 0.33394889853425247
	data : 1.2169837951660156
	model : 0.06587486267089844
			 train-loss:  2.260812282562256 	 ± 0.31448452829166856
	data : 0.11757950782775879
	model : 0.06473269462585449
			 train-loss:  2.317497866494315 	 ± 0.32257000106049577
	data : 0.11426305770874023
	model : 0.06469731330871582
			 train-loss:  2.2818691432476044 	 ± 0.31611836600786436
	data : 0.11411738395690918
	model : 0.06477317810058594
			 train-loss:  2.2602899339463978 	 ± 0.3042247441087953
	data : 0.11405582427978515
	model : 0.06480093002319336
			 train-loss:  2.2315855264663695 	 ± 0.3011858438145311
	data : 0.11393132209777831
	model : 0.06486778259277344
			 train-loss:  2.246146960691972 	 ± 0.29083782039299555
	data : 0.1140063762664795
	model : 0.0648488998413086
			 train-loss:  2.227971374988556 	 ± 0.28490633963167705
	data : 0.1141392707824707
	model : 0.06496319770812989
			 train-loss:  2.2130364821507382 	 ± 0.27857541833460114
	data : 0.114265775680542
	model : 0.06515898704528808
			 train-loss:  2.208172627857753 	 ± 0.26901420785713837
	data : 0.11409354209899902
	model : 0.0651512622833252
			 train-loss:  2.209498087565104 	 ± 0.25993973064636494
	data : 0.11398549079895019
	model : 0.0650402545928955
			 train-loss:  2.197627067565918 	 ± 0.2558504227723483
	data : 0.11395916938781739
	model : 0.06504111289978028
			 train-loss:  2.194526938831105 	 ± 0.2485209375044481
	data : 0.11398725509643555
	model : 0.06499676704406739
			 train-loss:  2.19147084818946 	 ± 0.24184741557558
	data : 0.11410207748413086
	model : 0.06480221748352051
			 train-loss:  2.1759127943139327 	 ± 0.24447637313302176
	data : 0.11430559158325196
	model : 0.06486473083496094
			 train-loss:  2.1641272842884063 	 ± 0.2437608025379317
	data : 0.11437745094299316
	model : 0.06497797966003419
			 train-loss:  2.1597376494180587 	 ± 0.23869481538074613
	data : 0.11435513496398926
	model : 0.06499099731445312
			 train-loss:  2.158578498796983 	 ± 0.23326733296386273
	data : 0.11433825492858887
	model : 0.06497254371643066
			 train-loss:  2.1595099127810933 	 ± 0.22818177736702128
	data : 0.11423144340515137
	model : 0.06495532989501954
			 train-loss:  2.170624469717344 	 ± 0.22964915854637424
	data : 0.11424341201782226
	model : 0.06482644081115722
			 train-loss:  2.1790138483047485 	 ± 0.22873204133702563
	data : 0.11423282623291016
	model : 0.06479034423828126
			 train-loss:  2.177677966081179 	 ± 0.2243896547569036
	data : 0.1142347812652588
	model : 0.06477742195129395
			 train-loss:  2.175561953473974 	 ± 0.2204592729479869
	data : 0.11434712409973144
	model : 0.06479783058166504
			 train-loss:  2.165838748216629 	 ± 0.2223040583827693
	data : 0.11434817314147949
	model : 0.06484622955322265
			 train-loss:  2.1502405158404647 	 ± 0.23351129215140753
	data : 0.1143336296081543
	model : 0.06498990058898926
			 train-loss:  2.1435470859209698 	 ± 0.2323987899323227
	data : 0.11444768905639649
	model : 0.06503548622131347
			 train-loss:  2.1424514209070513 	 ± 0.22869844738709966
	data : 0.11442203521728515
	model : 0.0650374412536621
			 train-loss:  2.1420001573860645 	 ± 0.22511069392430444
	data : 0.11429157257080078
	model : 0.06502418518066407
			 train-loss:  2.138884591333794 	 ± 0.22237319945198739
	data : 0.11412830352783203
	model : 0.06499571800231933
			 train-loss:  2.137652940609876 	 ± 0.21919282053992123
	data : 0.11408863067626954
	model : 0.0649383544921875
			 train-loss:  2.1369842972074236 	 ± 0.2160739805550697
	data : 0.11401200294494629
	model : 0.06488080024719238
			 train-loss:  2.1359267135461173 	 ± 0.21314367001213952
	data : 0.11404094696044922
	model : 0.06487360000610351
			 train-loss:  2.1283315549025663 	 ± 0.21512575816509477
	data : 0.11414537429809571
	model : 0.0648916244506836
			 train-loss:  2.1247571204838 	 ± 0.21338686581160057
	data : 0.11430401802062988
	model : 0.06493430137634278
			 train-loss:  2.118366015263093 	 ± 0.21428619598986054
	data : 0.11434483528137207
	model : 0.06499004364013672
			 train-loss:  2.1153957813978197 	 ± 0.2124021627158538
	data : 0.11433877944946289
	model : 0.06498832702636718
			 train-loss:  2.1199934569800774 	 ± 0.21180147671591718
	data : 0.11431999206542968
	model : 0.06499991416931153
			 train-loss:  2.1224285392534163 	 ± 0.20984491453260068
	data : 0.11427664756774902
	model : 0.0649951457977295
			 train-loss:  2.11921091412389 	 ± 0.2084362052780403
	data : 0.11414713859558105
	model : 0.06498537063598633
			 train-loss:  2.1178253455595537 	 ± 0.20625421670107394
	data : 0.11418623924255371
	model : 0.0649564266204834
			 train-loss:  2.1159178998735215 	 ± 0.20434171896965242
	data : 0.11419649124145508
	model : 0.06499733924865722
			 train-loss:  2.114885838135429 	 ± 0.20222695378336228
	data : 0.11419672966003418
	model : 0.06500945091247559
			 train-loss:  2.1225702255330186 	 ± 0.206741166525072
	data : 0.11410889625549317
	model : 0.0649801254272461
			 train-loss:  2.123817726969719 	 ± 0.20475496988197678
	data : 0.11419596672058105
	model : 0.06492724418640136
			 train-loss:  2.1314906879347197 	 ± 0.20951124431741594
	data : 0.11418190002441406
	model : 0.06487832069396973
			 train-loss:  2.1300839805603027 	 ± 0.207639169526725
	data : 0.11426706314086914
	model : 0.0648305892944336
			 train-loss:  2.132303523082359 	 ± 0.20619158600279766
	data : 0.1143183708190918
	model : 0.06483330726623535
			 train-loss:  2.1280259696336894 	 ± 0.20647165712897028
	data : 0.11443662643432617
	model : 0.06488623619079589
			 train-loss:  2.1326243585010745 	 ± 0.20718529063939528
	data : 0.11445069313049316
	model : 0.06493330001831055
			 train-loss:  2.1378750469949512 	 ± 0.20878701859056828
	data : 0.11443767547607422
	model : 0.06492352485656738
			 train-loss:  2.141783361001448 	 ± 0.20886426826545912
	data : 0.11429023742675781
	model : 0.0649648666381836
			 train-loss:  2.1485385107142583 	 ± 0.21296722783734207
	data : 0.11416091918945312
	model : 0.06494197845458985
			 train-loss:  2.1568977602741173 	 ± 0.22016460354397954
	data : 0.11399569511413574
	model : 0.06490583419799804
			 train-loss:  2.1514186715257577 	 ± 0.2221438369200904
	data : 0.11402206420898438
	model : 0.06494078636169434
			 train-loss:  2.146665167000334 	 ± 0.223208503801031
	data : 0.1141127586364746
	model : 0.06506500244140626
			 train-loss:  2.1484015762805937 	 ± 0.2217421042872456
	data : 0.11422624588012695
	model : 0.06511030197143555
			 train-loss:  2.1521521650376867 	 ± 0.22182767501200093
	data : 0.1142646312713623
	model : 0.06510896682739258
			 train-loss:  2.155733537289404 	 ± 0.2218022714609881
	data : 0.11441516876220703
	model : 0.06509361267089844
			 train-loss:  2.155988066915482 	 ± 0.22004402172360776
	data : 0.11429867744445801
	model : 0.0650637149810791
			 train-loss:  2.1585617680102587 	 ± 0.21927181047312255
	data : 0.11423935890197753
	model : 0.06495394706726074
			 train-loss:  2.1576585604594305 	 ± 0.21769851349277217
	data : 0.11425995826721191
	model : 0.06487474441528321
			 train-loss:  2.154567747405081 	 ± 0.2174753447606373
	data : 0.11432256698608398
	model : 0.06490106582641601
			 train-loss:  2.1539641814445383 	 ± 0.21590198117489992
	data : 0.11443572044372559
	model : 0.06493301391601562
			 train-loss:  2.1519954379867103 	 ± 0.214913608453518
	data : 0.11450209617614746
	model : 0.06493778228759765
			 train-loss:  2.152215487715127 	 ± 0.21335829612240795
	data : 0.1145662784576416
	model : 0.0649756908416748
			 train-loss:  2.155021016938346 	 ± 0.21310689860348164
	data : 0.11459202766418457
	model : 0.0650014877319336
			 train-loss:  2.155392609851461 	 ± 0.21162366212297978
	data : 0.11458187103271485
	model : 0.06502246856689453
			 train-loss:  2.152604747149679 	 ± 0.21145777607479516
	data : 0.11444568634033203
	model : 0.06503958702087402
			 train-loss:  2.149525719146206 	 ± 0.21162337991265756
	data : 0.11438150405883789
	model : 0.06503615379333497
			 train-loss:  2.147406080284634 	 ± 0.21096738851959615
	data : 0.11426310539245606
	model : 0.06506175994873047
			 train-loss:  2.144980867703756 	 ± 0.2105921454228906
	data : 0.11423006057739257
	model : 0.06509056091308593
			 train-loss:  2.144971300112574 	 ± 0.20920209947583418
	data : 0.11423287391662598
	model : 0.06506361961364746
			 train-loss:  2.1456740444356743 	 ± 0.207929477557378
	data : 0.11433334350585937
	model : 0.06503496170043946
			 train-loss:  2.143053595836346 	 ± 0.20786802746671362
	data : 0.1144247055053711
	model : 0.06501297950744629
			 train-loss:  2.138772703424285 	 ± 0.20997999215304752
	data : 0.11441941261291504
	model : 0.06501536369323731
			 train-loss:  2.136939841508865 	 ± 0.20929845527873603
	data : 0.11439356803894044
	model : 0.06498069763183593
			 train-loss:  2.135752077455874 	 ± 0.2082736017272744
	data : 0.11441636085510254
	model : 0.0649838924407959
			 train-loss:  2.134265728113128 	 ± 0.20743153680923423
	data : 0.11428027153015137
	model : 0.06499519348144531
			 train-loss:  2.1333222590297103 	 ± 0.20635509614074007
	data : 0.11419901847839356
	model : 0.06498055458068848
			 train-loss:  2.1342560989516124 	 ± 0.20529947100493923
	data : 0.11428332328796387
	model : 0.06492414474487304
			 train-loss:  2.134517722971299 	 ± 0.20410233959127916
	data : 0.11429414749145508
	model : 0.06494717597961426
			 train-loss:  2.1377669212430024 	 ± 0.20511153553566014
	data : 0.11429820060729981
	model : 0.06495428085327148
			 train-loss:  2.140040567551536 	 ± 0.2050164492685403
	data : 0.11437172889709472
	model : 0.06495165824890137
			 train-loss:  2.1415214132178915 	 ± 0.20431567156976443
	data : 0.11445002555847168
	model : 0.06498584747314454
			 train-loss:  2.143792345282737 	 ± 0.20427843144868535
	data : 0.11434435844421387
	model : 0.06501550674438476
			 train-loss:  2.1441787587271794 	 ± 0.20317308763058015
	data : 0.11437525749206542
	model : 0.06498017311096191
			 train-loss:  2.1447658722217264 	 ± 0.20213042329185865
	data : 0.1142697811126709
	model : 0.06495962142944336
			 train-loss:  2.146591041399085 	 ± 0.20178145712036363
	data : 0.11428475379943848
	model : 0.0649724006652832
			 train-loss:  2.146103351346908 	 ± 0.20074818562307828
	data : 0.11428437232971192
	model : 0.06498641967773437
			 train-loss:  2.1476531130202274 	 ± 0.20023605116969065
	data : 0.11440005302429199
	model : 0.06499767303466797
			 train-loss:  2.147504515396921 	 ± 0.19918459957563373
	data : 0.11447834968566895
	model : 0.06500115394592285
			 train-loss:  2.145360338191191 	 ± 0.19924354626161778
	data : 0.11456861495971679
	model : 0.06504979133605956
			 train-loss:  2.148520813774817 	 ± 0.2006181428942957
	data : 0.11453442573547364
	model : 0.0650411605834961
			 train-loss:  2.1565678289958408 	 ± 0.21475132385314744
	data : 0.11438884735107421
	model : 0.06498064994812011
			 train-loss:  2.1563562913374468 	 ± 0.2136742303375917
	data : 0.11424593925476074
	model : 0.0649794578552246
			 train-loss:  2.1583402729034424 	 ± 0.21351766214022067
	data : 0.11420207023620606
	model : 0.06496992111206054
			 train-loss:  2.159175742970835 	 ± 0.21262222129757433
	data : 0.11421360969543456
	model : 0.0649454116821289
			 train-loss:  2.160777449607849 	 ± 0.2121888383812399
	data : 0.11427931785583496
	model : 0.06493954658508301
			 train-loss:  2.158268192439403 	 ± 0.2126715914632353
	data : 0.11445384025573731
	model : 0.06501646041870117
			 train-loss:  2.1579714188208947 	 ± 0.21166809227549385
	data : 0.11455340385437011
	model : 0.06497130393981934
			 train-loss:  2.1598280725024996 	 ± 0.21150694399725303
	data : 0.11454977989196777
	model : 0.06501145362854004
			 train-loss:  2.1595249873287274 	 ± 0.21052981417129715
	data : 0.11441783905029297
	model : 0.06496753692626953
			 train-loss:  2.159756350740094 	 ± 0.2095572593146502
	data : 0.11430387496948242
	model : 0.06495141983032227
			 train-loss:  2.1586755492069103 	 ± 0.20888423030847647
	data : 0.1143143653869629
	model : 0.0649289608001709
			 train-loss:  2.159636558742698 	 ± 0.2081635526156888
	data : 0.11429710388183593
	model : 0.06496353149414062
			 train-loss:  2.1635515689849854 	 ± 0.21120798527456114
	data : 0.11427836418151856
	model : 0.06497812271118164
			 train-loss:  2.1732390163181066 	 ± 0.23351672699544762
	data : 0.11444106101989746
	model : 0.06499242782592773
			 train-loss:  2.1737403933491026 	 ± 0.23253191027452583
	data : 0.11450271606445313
	model : 0.06500062942504883
			 train-loss:  2.173727181105487 	 ± 0.23150076378278037
	data : 0.11439542770385742
	model : 0.0649953842163086
			 train-loss:  2.1753499068711935 	 ± 0.23112777650196606
	data : 0.11438302993774414
	model : 0.06494355201721191
			 train-loss:  2.1751620831696883 	 ± 0.23012941699746858
	data : 0.1143038272857666
	model : 0.06488623619079589
			 train-loss:  2.1748441457748413 	 ± 0.22916069780096535
	data : 0.11419754028320313
	model : 0.06488995552062989
			 train-loss:  2.17448274905865 	 ± 0.2282124732445128
	data : 0.11421666145324708
	model : 0.0648841381072998
			 train-loss:  2.1765451997013416 	 ± 0.22833583230918833
	data : 0.11436009407043457
	model : 0.064872407913208
			 train-loss:  2.174873666603024 	 ± 0.2280982657708278
	data : 0.11434578895568848
	model : 0.0649646282196045
			 train-loss:  2.1749487996101378 	 ± 0.2271473466905221
	data : 0.11439933776855468
	model : 0.06498827934265136
			 train-loss:  2.1739604965714383 	 ± 0.22646570097481303
	data : 0.11442689895629883
	model : 0.06499853134155273
			 train-loss:  2.1731503263848726 	 ± 0.22571165739093016
	data : 0.11442298889160156
	model : 0.06501646041870117
			 train-loss:  2.1787027246583768 	 ± 0.23300796685492148
	data : 0.11428031921386719
	model : 0.06498422622680664
			 train-loss:  2.1770211487047133 	 ± 0.23281467947035686
	data : 0.11422410011291503
	model : 0.06493339538574219
			 train-loss:  2.1765972909927367 	 ± 0.23192958164432192
	data : 0.11419782638549805
	model : 0.06494135856628418
			 train-loss:  2.177907791402605 	 ± 0.23147157910443922
	data : 0.11426839828491211
	model : 0.06495990753173828
			 train-loss:  2.176767736908019 	 ± 0.23091334862894988
	data : 0.11427764892578125
	model : 0.06499185562133789
			 train-loss:  2.1747723016887903 	 ± 0.23110622708802783
	data : 0.11427440643310546
	model : 0.06504659652709961
			 train-loss:  2.1756226036899773 	 ± 0.23040964006423761
	data : 0.11452841758728027
	model : 0.06505489349365234
			 train-loss:  2.17502707701463 	 ± 0.22962138071596147
	data : 0.11447105407714844
	model : 0.06505303382873535
			 train-loss:  2.1734334139423517 	 ± 0.2294638515212778
	data : 0.11441602706909179
	model : 0.06503691673278808
			 train-loss:  2.1761817941159913 	 ± 0.23074723954269627
	data : 0.11436514854431153
	model : 0.06498785018920898
			 train-loss:  2.176082952578265 	 ± 0.22988093690789818
	data : 0.11436147689819336
	model : 0.06496629714965821
			 train-loss:  2.1758952558930242 	 ± 0.22903179527868592
	data : 0.11421751976013184
	model : 0.06495471000671386
			 train-loss:  2.174814866207264 	 ± 0.2285244274291166
	data : 0.11435427665710449
	model : 0.06501035690307617
			 train-loss:  2.1756651094731163 	 ± 0.22789693192018334
	data : 0.11433424949645996
	model : 0.06498527526855469
			 train-loss:  2.177310920109714 	 ± 0.22787341114080273
	data : 0.114406156539917
	model : 0.06498966217041016
			 train-loss:  2.1765497050423552 	 ± 0.22722103436557942
	data : 0.11438684463500977
	model : 0.06496057510375977
			 train-loss:  2.175452924460816 	 ± 0.22676853386605442
	data : 0.11429238319396973
	model : 0.06495766639709473
			 train-loss:  2.1739494800567627 	 ± 0.22665136633282523
	data : 0.11416082382202149
	model : 0.06489806175231934
			 train-loss:  2.174410696570755 	 ± 0.2259121301368051
	data : 0.11423048973083497
	model : 0.06492381095886231
			 train-loss:  2.1738161285158615 	 ± 0.2252259423809906
	data : 0.11421175003051758
	model : 0.06491074562072754
			 train-loss:  2.175023125601815 	 ± 0.22489745293577715
	data : 0.1142974853515625
	model : 0.06498637199401855
			 train-loss:  2.1743643283843994 	 ± 0.22425362024543596
	data : 0.11427717208862305
	model : 0.06501774787902832
			 train-loss:  2.1749741324063003 	 ± 0.22359876732052997
	data : 0.11443204879760742
	model : 0.06500568389892578
			 train-loss:  2.1738450380220806 	 ± 0.22324610061563568
	data : 0.11432380676269531
	model : 0.06497883796691895
			 train-loss:  2.174584375757749 	 ± 0.22266474440380396
	data : 0.11419930458068847
	model : 0.06496515274047851
			 train-loss:  2.1754226797335856 	 ± 0.22214386319276833
	data : 0.1141507625579834
	model : 0.06492295265197753
			 train-loss:  2.174474308154727 	 ± 0.22169757441628374
	data : 0.11434707641601563
	model : 0.06489310264587403
			 train-loss:  2.174752125740051 	 ± 0.22098336878834682
	data : 0.11425724029541015
	model : 0.06492414474487304
			 train-loss:  2.17686748978318 	 ± 0.2217689391809023
	data : 0.11434011459350586
	model : 0.06495342254638672
			 train-loss:  2.177144285879637 	 ± 0.22106440048279638
	data : 0.11442193984985352
	model : 0.06500234603881835
			 train-loss:  2.1770679031322206 	 ± 0.2203427958498146
	data : 0.11450366973876953
	model : 0.06500773429870606
			 train-loss:  2.1797738199110155 	 ± 0.22216198051821137
	data : 0.11437568664550782
	model : 0.06499247550964356
			 train-loss:  2.178531696719508 	 ± 0.2219800038649715
	data : 0.11425275802612304
	model : 0.06496624946594239
			 train-loss:  2.1773969401151705 	 ± 0.22171794062254355
	data : 0.11416420936584473
	model : 0.06496410369873047
			 train-loss:  2.177249465778375 	 ± 0.22101838005995875
	data : 0.1142282485961914
	model : 0.06495914459228516
			 train-loss:  2.1758209987531734 	 ± 0.22104369247729622
	data : 0.114265775680542
	model : 0.0649949550628662
			 train-loss:  2.178192661243415 	 ± 0.22235496873390226
	data : 0.11432919502258301
	model : 0.06503887176513672
			 train-loss:  2.1805244527757166 	 ± 0.2236006357843576
	data : 0.11438417434692383
	model : 0.06502985954284668
			 train-loss:  2.1812558936776583 	 ± 0.2230970716075139
	data : 0.11458864212036132
	model : 0.06501302719116211
			 train-loss:  2.181778809906524 	 ± 0.22250638413745227
	data : 0.11452312469482422
	model : 0.0649829387664795
			 train-loss:  2.179763203018282 	 ± 0.2233013814336075
	data : 0.11434988975524903
	model : 0.06488709449768067
			 train-loss:  2.179553697748882 	 ± 0.22263561181156508
	data : 0.11426887512207032
	model : 0.06483497619628906
			 train-loss:  2.1779583938194045 	 ± 0.22289816292291964
	data : 0.11436152458190918
	model : 0.06486115455627442
			 train-loss:  2.1774317047682152 	 ± 0.22232872850148552
	data : 0.1143178939819336
	model : 0.06488475799560547
			 train-loss:  2.1770881585732194 	 ± 0.2217062630933491
	data : 0.11439399719238282
	model : 0.0649263858795166
			 train-loss:  2.178149226875532 	 ± 0.22147032612836742
	data : 0.11459527015686036
	model : 0.06494030952453614
			 train-loss:  2.18024410086976 	 ± 0.22247728303683978
	data : 0.11455636024475098
	model : 0.06496925354003906
			 train-loss:  2.1796811068759245 	 ± 0.2219426824929923
	data : 0.11451892852783203
	model : 0.06493372917175293
			 train-loss:  2.1800739841851575 	 ± 0.22135205562560292
	data : 0.11432723999023438
	model : 0.06488533020019531
			 train-loss:  2.180239108412765 	 ± 0.22071821468440952
	data : 0.1141939640045166
	model : 0.06486148834228515
			 train-loss:  2.181670082097798 	 ± 0.2208780971275299
	data : 0.1141059398651123
	model : 0.06487579345703125
			 train-loss:  2.1819371211117713 	 ± 0.22027048046832676
	data : 0.11427373886108398
	model : 0.06490063667297363
			 train-loss:  2.1835255588803975 	 ± 0.22063739045034522
	data : 0.11425657272338867
	model : 0.06494097709655762
			 train-loss:  2.1820081932978197 	 ± 0.220923475404353
	data : 0.11438393592834473
	model : 0.0649935245513916
			 train-loss:  2.1815463823113737 	 ± 0.22038368951900936
	data : 0.11443071365356446
	model : 0.0651010513305664
			 train-loss:  2.1814020296161094 	 ± 0.2197721535900206
	data : 0.11427946090698242
	model : 0.06509051322937012
			 train-loss:  2.1801850809065324 	 ± 0.2197580028856707
	data : 0.11412353515625
	model : 0.06504955291748046
			 train-loss:  2.1795492980215285 	 ± 0.21931173594799272
	data : 0.11414251327514649
	model : 0.06508970260620117
			 train-loss:  2.178292562289791 	 ± 0.21935403795329533
	data : 0.1141441822052002
	model : 0.06514101028442383
			 train-loss:  2.1777429574138516 	 ± 0.21887551972148556
	data : 0.11424508094787597
	model : 0.0650719165802002
			 train-loss:  2.178858005935377 	 ± 0.21879441414543124
	data : 0.11443805694580078
	model : 0.0651334285736084
			 train-loss:  2.1784319741570433 	 ± 0.2182751526617204
	data : 0.11452937126159668
	model : 0.06514549255371094
			 train-loss:  2.179209320609634 	 ± 0.2179396524894721
	data : 0.11453690528869628
	model : 0.06509251594543457
			 train-loss:  2.179154436434469 	 ± 0.21735428558349248
	data : 0.1146820068359375
	model : 0.06499691009521484
			 train-loss:  2.178320822868755 	 ± 0.21707027263927814
	data : 0.11443867683410644
	model : 0.06494927406311035
			 train-loss:  2.1784509515508694 	 ± 0.21649950154135125
	data : 0.11437664031982422
	model : 0.06489381790161133
			 train-loss:  2.177464310454313 	 ± 0.2163493577289151
	data : 0.11439552307128906
	model : 0.06487245559692383
			 train-loss:  2.1767400195724087 	 ± 0.21600889011722807
	data : 0.11444358825683594
	model : 0.0648676872253418
			 train-loss:  2.1811967900910303 	 ± 0.22403008928944848
	data : 0.11433954238891601
	model : 0.06490192413330079
			 train-loss:  2.1824399971713624 	 ± 0.22410551098748924
	data : 0.11455283164978028
	model : 0.06487846374511719
			 train-loss:  2.1826323060791726 	 ± 0.2235400556666538
	data : 0.11465673446655274
	model : 0.0648691177368164
			 train-loss:  2.1814215060361883 	 ± 0.22359678766215837
	data : 0.1147500991821289
	model : 0.0648923397064209
			 train-loss:  2.18023206392924 	 ± 0.22363721072996734
	data : 0.11460666656494141
	model : 0.06489486694335937
			 train-loss:  2.18303103592931 	 ± 0.22646436411571697
	data : 0.11455941200256348
	model : 0.06492695808410645
			 train-loss:  2.184500251324649 	 ± 0.2268234049645504
	data : 0.11442742347717286
	model : 0.06496405601501465
			 train-loss:  2.1878902057204583 	 ± 0.2311988259401578
	data : 0.11433448791503906
	model : 0.06497683525085449
			 train-loss:  2.1877417324775426 	 ± 0.23062665579909777
	data : 0.11417951583862304
	model : 0.06499309539794922
			 train-loss:  2.189720869064331 	 ± 0.23173733069459007
	data : 0.1142538070678711
	model : 0.0649998664855957
			 train-loss:  2.1895440300898765 	 ± 0.23117367878983922
	data : 0.11435751914978028
	model : 0.06494307518005371
			 train-loss:  2.1895370353566537 	 ± 0.23060077809495919
	data : 0.11440668106079102
	model : 0.06492533683776855
			 train-loss:  2.192700093602899 	 ± 0.2343837919880095
	data : 0.11432228088378907
	model : 0.06490912437438964
			 train-loss:  2.1926988374953176 	 ± 0.23380861685629586
	data : 0.11425423622131348
	model : 0.06487178802490234
			 train-loss:  2.193158731227968 	 ± 0.23333013108109277
	data : 0.11410508155822754
	model : 0.06486921310424805
			 train-loss:  2.193152445033916 	 ± 0.23276312423955464
	data : 0.1139756202697754
	model : 0.06486697196960449
			 train-loss:  2.1931262534597646 	 ± 0.23220051810118192
	data : 0.11400465965270996
	model : 0.06492304801940918
			 train-loss:  2.1937190133791704 	 ± 0.23179861148729428
	data : 0.11412167549133301
	model : 0.0649942398071289
			 train-loss:  2.194409081810399 	 ± 0.23145747025781793
	data : 0.11424951553344727
	model : 0.06501932144165039
			 train-loss:  2.1939792837415424 	 ± 0.23098930910419466
	data : 0.11444063186645508
	model : 0.06499099731445312
			 train-loss:  2.195458616690613 	 ± 0.2314362937146623
	data : 0.1143728256225586
	model : 0.06501293182373047
			 train-loss:  2.1947505440352097 	 ± 0.23111878318753737
	data : 0.11436352729797364
	model : 0.06495833396911621
			 train-loss:  2.1932723259142306 	 ± 0.23157797912382672
	data : 0.11442561149597168
	model : 0.06496286392211914
			 train-loss:  2.1919303569838267 	 ± 0.2318649351886094
	data : 0.11440849304199219
	model : 0.06495556831359864
			 train-loss:  2.191980786101763 	 ± 0.23132626225822733
	data : 0.11449975967407226
	model : 0.06507501602172852
			 train-loss:  2.1921809122518257 	 ± 0.23080881799552838
	data : 0.11462182998657226
	model : 0.06508569717407227
			 train-loss:  2.192875729727855 	 ± 0.2305026956655919
	data : 0.11465096473693848
	model : 0.06509571075439453
			 train-loss:  2.191741268569176 	 ± 0.23057981205136796
	data : 0.11459493637084961
	model : 0.06503882408142089
			 train-loss:  2.191353947060293 	 ± 0.23012383996982969
	data : 0.11459689140319824
	model : 0.06499719619750977
			 train-loss:  2.190724796598608 	 ± 0.22978893577722362
	data : 0.11436653137207031
	model : 0.06494350433349609
			 train-loss:  2.192787695794084 	 ± 0.23130120900541484
	data : 0.1142357349395752
	model : 0.06507568359375
			 train-loss:  2.192443884170807 	 ± 0.23083626403794943
	data : 0.11431884765625
	model : 0.06498203277587891
			 train-loss:  2.191398790598985 	 ± 0.23084389934857302
	data : 0.1142920970916748
	model : 0.06489005088806152
			 train-loss:  2.1918195134827068 	 ± 0.23041371854204878
	data : 0.1143277645111084
	model : 0.06482343673706055
			 train-loss:  2.194112065633138 	 ± 0.23244746106841419
	data : 0.11456360816955566
	model : 0.06460933685302735
			 train-loss:  2.1943071110058674 	 ± 0.23195107868895481
	data : 0.11469230651855469
	model : 0.0642460823059082
			 train-loss:  2.1968183485946993 	 ± 0.23449844301283254
	data : 0.11466441154479981
	model : 0.06409096717834473
			 train-loss:  2.196080676296301 	 ± 0.23424743773724493
	data : 0.11476073265075684
	model : 0.06399831771850586
			 train-loss:  2.197176808353074 	 ± 0.23432069951023304
	data : 0.1147911548614502
	model : 0.06389832496643066
			 train-loss:  2.197899708540543 	 ± 0.23406652803157119
	data : 0.11486859321594238
	model : 0.06385807991027832
			 train-loss:  2.197296862994438 	 ± 0.23373821470516318
	data : 0.11509013175964355
	model : 0.06390490531921386
			 train-loss:  2.196350794413994 	 ± 0.23367674067227737
	data : 0.11527500152587891
	model : 0.06397800445556641
			 train-loss:  2.1979577899490814 	 ± 0.2344559415392477
	data : 0.11537008285522461
	model : 0.06395950317382812
			 train-loss:  2.197567839907785 	 ± 0.23403013902631406
	data : 0.11545615196228028
	model : 0.06394305229187011
			 train-loss:  2.1963094777249275 	 ± 0.234323654229829
	data : 0.1153475284576416
	model : 0.06395606994628907
			 train-loss:  2.195348341081102 	 ± 0.2342904284709749
	data : 0.11520471572875976
	model : 0.06389064788818359
			 train-loss:  2.1950229768511615 	 ± 0.23384904627213426
	data : 0.11509861946105956
	model : 0.06385130882263183
			 train-loss:  2.1948995414902184 	 ± 0.23336498656282517
	data : 0.11507668495178222
	model : 0.06388096809387207
			 train-loss:  2.1939374138620606 	 ± 0.23334881238045305
	data : 0.11506609916687012
	model : 0.06388020515441895
			 train-loss:  2.1932902360955873 	 ± 0.23307700192007208
	data : 0.11508517265319824
	model : 0.06385622024536133
			 train-loss:  2.195454986758252 	 ± 0.23499818988911908
	data : 0.11489839553833008
	model : 0.06379408836364746
			 train-loss:  2.194074941075538 	 ± 0.235488728125428
	data : 0.11495146751403809
	model : 0.06380915641784668
			 train-loss:  2.1939635404343467 	 ± 0.23501007358190712
	data : 0.11491589546203614
	model : 0.06378316879272461
			 train-loss:  2.195858052519501 	 ± 0.2363801029935201
	data : 0.11490793228149414
	model : 0.06378579139709473
			 train-loss:  2.194542944188021 	 ± 0.23678997029131826
	data : 0.11493506431579589
	model : 0.06383533477783203
			 train-loss:  2.1945590226630856 	 ± 0.23630833377410726
	data : 0.11518783569335937
	model : 0.06393508911132813
			 train-loss:  2.1946973472471663 	 ± 0.23583947089262178
	data : 0.11512551307678223
	model : 0.06390132904052734
			 train-loss:  2.1957577476578374 	 ± 0.23595279128593877
	data : 0.11506619453430175
	model : 0.06386690139770508
			 train-loss:  2.194850404099767 	 ± 0.23591164027713485
	data : 0.11494832038879395
	model : 0.06385445594787598
			 train-loss:  2.195709321498871 	 ± 0.23582913697356747
	data : 0.11503548622131347
	model : 0.06381773948669434
			 train-loss:  2.1960616686429635 	 ± 0.23542481548619365
	data : 0.11502480506896973
	model : 0.06376914978027344
			 train-loss:  2.196470901606575 	 ± 0.23504667465594067
	data : 0.1149181842803955
	model : 0.06377696990966797
			 train-loss:  2.196871103976555 	 ± 0.23466770703708517
	data : 0.11489996910095215
	model : 0.06377191543579101
			 train-loss:  2.1968292331132364 	 ± 0.23420625409261278
	data : 0.1149622917175293
	model : 0.06377658843994141
			 train-loss:  2.1954408500708786 	 ± 0.2347915535928524
	data : 0.11496667861938477
	model : 0.06377668380737304
			 train-loss:  2.1946030212566257 	 ± 0.23471415077334554
	data : 0.11465277671813964
	model : 0.05539264678955078
#epoch  55    val-loss:  2.4056231222654643  train-loss:  2.1946030212566257  lr:  1.52587890625e-07
			 train-loss:  2.2227213382720947 	 ± 0.0
	data : 5.184550762176514
	model : 0.07278060913085938
			 train-loss:  2.2531495094299316 	 ± 0.030428171157836914
	data : 2.6732062101364136
	model : 0.07372140884399414
			 train-loss:  2.2510834534962973 	 ± 0.025015719876021112
	data : 1.8793394565582275
	model : 0.07060639063517253
			 train-loss:  2.2446600794792175 	 ± 0.02435403215027341
	data : 1.4378852844238281
	model : 0.06912797689437866
			 train-loss:  2.202194833755493 	 ± 0.08767943591907057
	data : 1.1731696128845215
	model : 0.06829090118408203
			 train-loss:  2.2324270804723105 	 ± 0.1047680610219762
	data : 0.15918440818786622
	model : 0.06667237281799317
			 train-loss:  2.1936948810304915 	 ± 0.13568123207043703
	data : 0.14951176643371583
	model : 0.06467924118041993
			 train-loss:  2.139822229743004 	 ± 0.19085088430849742
	data : 0.11388716697692872
	model : 0.06480741500854492
			 train-loss:  2.1316725810368857 	 ± 0.1814063848138481
	data : 0.11392903327941895
	model : 0.0648758888244629
			 train-loss:  2.1231579422950744 	 ± 0.17398258666383512
	data : 0.11399116516113281
	model : 0.06488642692565919
			 train-loss:  2.1281746409156104 	 ± 0.1666427326451332
	data : 0.11399731636047364
	model : 0.06495127677917481
			 train-loss:  2.186952441930771 	 ± 0.25191026007047257
	data : 0.1141268253326416
	model : 0.06497564315795898
			 train-loss:  2.179252505302429 	 ± 0.24349292160013955
	data : 0.1141469955444336
	model : 0.06505517959594727
			 train-loss:  2.1693487082208907 	 ± 0.23733730413679624
	data : 0.11402697563171386
	model : 0.06495862007141114
			 train-loss:  2.152796252568563 	 ± 0.23750684742333747
	data : 0.11395807266235351
	model : 0.06494274139404296
			 train-loss:  2.1453159153461456 	 ± 0.2317827438283056
	data : 0.11390800476074218
	model : 0.06499271392822266
			 train-loss:  2.126852463273441 	 ± 0.23668002604275853
	data : 0.11396865844726563
	model : 0.06502213478088378
			 train-loss:  2.1510329842567444 	 ± 0.250689477513356
	data : 0.11417994499206544
	model : 0.06490597724914551
			 train-loss:  2.194484277775413 	 ± 0.3058134065711959
	data : 0.11433558464050293
	model : 0.06495614051818847
			 train-loss:  2.212238663434982 	 ± 0.30795274580618887
	data : 0.11426005363464356
	model : 0.06492438316345214
			 train-loss:  2.2323974143891108 	 ± 0.31376177449386095
	data : 0.11420774459838867
	model : 0.06477127075195313
			 train-loss:  2.2195719968188894 	 ± 0.3121312712602021
	data : 0.11395683288574218
	model : 0.06475629806518554
			 train-loss:  2.2181837092275205 	 ± 0.3053398467452867
	data : 0.1138232707977295
	model : 0.06475238800048828
			 train-loss:  2.2266945292552314 	 ± 0.3016848041972189
	data : 0.11381673812866211
	model : 0.0647420883178711
			 train-loss:  2.232909712791443 	 ± 0.29715359057818036
	data : 0.11390094757080078
	model : 0.06478090286254883
			 train-loss:  2.2488705699260416 	 ± 0.3021139134463014
	data : 0.11381688117980956
	model : 0.06490669250488282
			 train-loss:  2.277286578107763 	 ± 0.3299796199023537
	data : 0.11405501365661622
	model : 0.0648500919342041
			 train-loss:  2.2667462485177174 	 ± 0.32862958631596384
	data : 0.11403326988220215
	model : 0.06483182907104493
			 train-loss:  2.2706808632817763 	 ± 0.3235843475315409
	data : 0.11401834487915039
	model : 0.06486244201660156
			 train-loss:  2.2647727012634276 	 ± 0.3197325245833605
	data : 0.11384954452514648
	model : 0.06481733322143554
			 train-loss:  2.265059248093636 	 ± 0.3145371909772364
	data : 0.11391363143920899
	model : 0.06479535102844239
			 train-loss:  2.2596299424767494 	 ± 0.31105588928825695
	data : 0.1139366626739502
	model : 0.06487040519714356
			 train-loss:  2.263418501073664 	 ± 0.30705549170087043
	data : 0.113960599899292
	model : 0.06493334770202637
			 train-loss:  2.2490729514290306 	 ± 0.31353033599400854
	data : 0.11405048370361329
	model : 0.06503815650939941
			 train-loss:  2.2480359077453613 	 ± 0.3090780313372442
	data : 0.11421942710876465
	model : 0.06511592864990234
			 train-loss:  2.2537121838993497 	 ± 0.3065996471498075
	data : 0.11435556411743164
	model : 0.06511850357055664
			 train-loss:  2.2479316286138586 	 ± 0.3044103254196764
	data : 0.11422677040100097
	model : 0.0650815486907959
			 train-loss:  2.237143585556432 	 ± 0.30746252761274556
	data : 0.11420111656188965
	model : 0.06505212783813477
			 train-loss:  2.241062879562378 	 ± 0.3044552354930989
	data : 0.11420884132385253
	model : 0.06494264602661133
			 train-loss:  2.2301066875457765 	 ± 0.3083134010310944
	data : 0.11412272453308106
	model : 0.0649446964263916
			 train-loss:  2.237632361853995 	 ± 0.3082273788204512
	data : 0.11402873992919922
	model : 0.06492452621459961
			 train-loss:  2.2342386018662226 	 ± 0.3053102293545898
	data : 0.11412739753723145
	model : 0.06495442390441894
			 train-loss:  2.233396796293037 	 ± 0.30178854124610416
	data : 0.11416859626770019
	model : 0.06496219635009766
			 train-loss:  2.226869146932255 	 ± 0.3013945023503209
	data : 0.11414637565612792
	model : 0.0650111198425293
			 train-loss:  2.2271029498842028 	 ± 0.2980308955062719
	data : 0.11417250633239746
	model : 0.0649451732635498
			 train-loss:  2.2312055022820183 	 ± 0.2960555433462571
	data : 0.11412239074707031
	model : 0.06486897468566895
			 train-loss:  2.227351394105465 	 ± 0.2940532362520709
	data : 0.1140721321105957
	model : 0.06484494209289551
			 train-loss:  2.2262987767656646 	 ± 0.2910635320691944
	data : 0.11414880752563476
	model : 0.06481771469116211
			 train-loss:  2.22203098267925 	 ± 0.2895916375257266
	data : 0.11420001983642578
	model : 0.06479253768920898
			 train-loss:  2.2179085087776182 	 ± 0.28812982309498497
	data : 0.1142303466796875
	model : 0.06483039855957032
			 train-loss:  2.219973150421591 	 ± 0.28566433572235245
	data : 0.11422319412231445
	model : 0.06493840217590333
			 train-loss:  2.2281227088891544 	 ± 0.288828640041174
	data : 0.11419930458068847
	model : 0.06497139930725097
			 train-loss:  2.225167434170561 	 ± 0.28688348393279695
	data : 0.11416330337524414
	model : 0.06499199867248535
			 train-loss:  2.2292504685896413 	 ± 0.2857649239958823
	data : 0.11412677764892579
	model : 0.06491594314575196
			 train-loss:  2.2318647839806296 	 ± 0.2838061070521684
	data : 0.11414422988891601
	model : 0.06488876342773438
			 train-loss:  2.2280160124812807 	 ± 0.2827053306111208
	data : 0.11415705680847169
	model : 0.06486964225769043
			 train-loss:  2.2290477940910742 	 ± 0.28032084185226536
	data : 0.11421947479248047
	model : 0.0648730754852295
			 train-loss:  2.2303264572702606 	 ± 0.27806140421421793
	data : 0.11426434516906739
	model : 0.06492280960083008
			 train-loss:  2.2321276846578564 	 ± 0.2760359466696949
	data : 0.11417565345764161
	model : 0.06498155593872071
			 train-loss:  2.228661912679672 	 ± 0.27501744684986573
	data : 0.11407814025878907
	model : 0.06498928070068359
			 train-loss:  2.231358350300398 	 ± 0.2735524286336226
	data : 0.11414132118225098
	model : 0.06491894721984863
			 train-loss:  2.2301160000985667 	 ± 0.2715108288510933
	data : 0.11417865753173828
	model : 0.06486124992370605
			 train-loss:  2.2304319635270136 	 ± 0.26935885129681664
	data : 0.11411361694335938
	model : 0.06478962898254395
			 train-loss:  2.2313902992755175 	 ± 0.26735442993853914
	data : 0.11429262161254883
	model : 0.06475858688354492
			 train-loss:  2.232652137829707 	 ± 0.2654818765736684
	data : 0.11442956924438477
	model : 0.06479320526123047
			 train-loss:  2.2318761294538323 	 ± 0.26353724744209556
	data : 0.11444835662841797
	model : 0.06482248306274414
			 train-loss:  2.228547777702559 	 ± 0.2629570851465837
	data : 0.11439929008483887
	model : 0.06490240097045899
			 train-loss:  2.2290305512792923 	 ± 0.26104632749018886
	data : 0.1145258903503418
	model : 0.06490077972412109
			 train-loss:  2.226991243984388 	 ± 0.2596928384966319
	data : 0.11440420150756836
	model : 0.06487345695495605
			 train-loss:  2.2215085659708294 	 ± 0.261822573167183
	data : 0.11448707580566406
	model : 0.06486926078796387
			 train-loss:  2.220298130747298 	 ± 0.26016939227207986
	data : 0.1145554542541504
	model : 0.06491150856018066
			 train-loss:  2.220466439922651 	 ± 0.2583602354962014
	data : 0.11467304229736328
	model : 0.0648733139038086
			 train-loss:  2.220645631829353 	 ± 0.25658904783846415
	data : 0.11449880599975586
	model : 0.0648961067199707
			 train-loss:  2.2183364355886304 	 ± 0.2556120143053088
	data : 0.11467041969299316
	model : 0.06497244834899903
			 train-loss:  2.219595405260722 	 ± 0.25413308621428266
	data : 0.11466293334960938
	model : 0.06498408317565918
			 train-loss:  2.2224315451948264 	 ± 0.25364762504715954
	data : 0.11446099281311035
	model : 0.06495966911315917
			 train-loss:  2.217470361040784 	 ± 0.2556798489772612
	data : 0.11428813934326172
	model : 0.06496720314025879
			 train-loss:  2.220991776539729 	 ± 0.2559080078912488
	data : 0.11425271034240722
	model : 0.06491823196411133
			 train-loss:  2.221915963329846 	 ± 0.2544141435394479
	data : 0.11407065391540527
	model : 0.06489443778991699
			 train-loss:  2.2219609677791596 	 ± 0.25281937125967147
	data : 0.11401882171630859
	model : 0.06487793922424316
			 train-loss:  2.2259504324124184 	 ± 0.2537750857164033
	data : 0.11410880088806152
	model : 0.06488780975341797
			 train-loss:  2.224899553671116 	 ± 0.2524001949949986
	data : 0.11421089172363282
	model : 0.06488227844238281
			 train-loss:  2.222250340932823 	 ± 0.2520194851144311
	data : 0.11431560516357422
	model : 0.0649691104888916
			 train-loss:  2.2237046531268527 	 ± 0.25086500543245543
	data : 0.11437773704528809
	model : 0.06496849060058593
			 train-loss:  2.223114344652961 	 ± 0.24944364242629385
	data : 0.1142812728881836
	model : 0.0649458885192871
			 train-loss:  2.221285343170166 	 ± 0.24856179019455651
	data : 0.1142310619354248
	model : 0.06499452590942383
			 train-loss:  2.2216432478236054 	 ± 0.2471514328750363
	data : 0.11419825553894043
	model : 0.06499924659729003
			 train-loss:  2.2242117388681932 	 ± 0.24690817737572726
	data : 0.11424345970153808
	model : 0.0649573802947998
			 train-loss:  2.222172683544373 	 ± 0.2462611316212802
	data : 0.11418142318725585
	model : 0.06502366065979004
			 train-loss:  2.220422469245063 	 ± 0.24544519944077667
	data : 0.11426944732666015
	model : 0.06508746147155761
			 train-loss:  2.225990408069485 	 ± 0.24974287975000356
	data : 0.11425981521606446
	model : 0.06504907608032226
			 train-loss:  2.2251357617585557 	 ± 0.2485156395131549
	data : 0.11424069404602051
	model : 0.0650421142578125
			 train-loss:  2.2235875052790486 	 ± 0.24762162635874396
	data : 0.11414928436279297
	model : 0.06505384445190429
			 train-loss:  2.2218293433493757 	 ± 0.24688386414142946
	data : 0.11420350074768067
	model : 0.06501588821411133
			 train-loss:  2.2215326760944567 	 ± 0.24559788112918962
	data : 0.11410727500915527
	model : 0.06494393348693847
			 train-loss:  2.224421667555968 	 ± 0.24593271274504097
	data : 0.11417069435119628
	model : 0.06494274139404296
			 train-loss:  2.226403875449269 	 ± 0.24543138017692961
	data : 0.11412606239318848
	model : 0.06492819786071777
			 train-loss:  2.2280850824044673 	 ± 0.2447367358774407
	data : 0.11427626609802247
	model : 0.0649186611175537
			 train-loss:  2.226377829156741 	 ± 0.24408338947761038
	data : 0.11422886848449706
	model : 0.06492605209350585
			 train-loss:  2.22686585187912 	 ± 0.24290844460468708
	data : 0.11426405906677246
	model : 0.06494135856628418
			 train-loss:  2.22664731799966 	 ± 0.24171281528344638
	data : 0.11411848068237304
	model : 0.06488194465637206
			 train-loss:  2.223106921887865 	 ± 0.24314247878153783
	data : 0.11414923667907714
	model : 0.0648651123046875
			 train-loss:  2.2207217853046157 	 ± 0.24315543723182437
	data : 0.1139894962310791
	model : 0.06491832733154297
			 train-loss:  2.222395577109777 	 ± 0.2425791084255065
	data : 0.11405649185180664
	model : 0.06486296653747559
			 train-loss:  2.2227872950690135 	 ± 0.24145425430775797
	data : 0.11409201622009277
	model : 0.0648895263671875
			 train-loss:  2.221633293718662 	 ± 0.24060337892504996
	data : 0.11427845954895019
	model : 0.06493802070617676
			 train-loss:  2.219247861443279 	 ± 0.24073248322767224
	data : 0.11434497833251953
	model : 0.0649909496307373
			 train-loss:  2.2159211900499134 	 ± 0.2420736998366205
	data : 0.11441855430603028
	model : 0.0649573802947998
			 train-loss:  2.213814122961202 	 ± 0.24195362256046515
	data : 0.11430821418762208
	model : 0.0649878978729248
			 train-loss:  2.213455067981373 	 ± 0.24088049286622085
	data : 0.11424202919006347
	model : 0.06496109962463378
			 train-loss:  2.2100455116581275 	 ± 0.24244470829200354
	data : 0.1142432689666748
	model : 0.06496429443359375
			 train-loss:  2.207833454012871 	 ± 0.2424825050857253
	data : 0.11422963142395019
	model : 0.06497979164123535
			 train-loss:  2.207821139192159 	 ± 0.241407224523283
	data : 0.11429052352905274
	model : 0.06501183509826661
			 train-loss:  2.2076129808760525 	 ± 0.24035627440182122
	data : 0.11440463066101074
	model : 0.0650167465209961
			 train-loss:  2.206607432987379 	 ± 0.23954968032625934
	data : 0.1144139289855957
	model : 0.06501083374023438
			 train-loss:  2.205905468299471 	 ± 0.2386336648977063
	data : 0.11438751220703125
	model : 0.06498780250549316
			 train-loss:  2.2040302641371374 	 ± 0.2384684657702741
	data : 0.11429519653320312
	model : 0.06491169929504395
			 train-loss:  2.2011345008672296 	 ± 0.23951279865700395
	data : 0.11416897773742676
	model : 0.06481304168701171
			 train-loss:  2.199972177753929 	 ± 0.2388382877735764
	data : 0.11418194770812988
	model : 0.06479873657226562
			 train-loss:  2.2010837803284327 	 ± 0.2381499672114444
	data : 0.11422233581542969
	model : 0.0647890567779541
			 train-loss:  2.2019920161932953 	 ± 0.23737243226252377
	data : 0.11427130699157714
	model : 0.0648238182067871
			 train-loss:  2.2015702831940573 	 ± 0.23644310676454086
	data : 0.11440925598144532
	model : 0.06490211486816407
			 train-loss:  2.201003001957405 	 ± 0.23556334250270908
	data : 0.11436648368835449
	model : 0.06494145393371582
			 train-loss:  2.2027534986695936 	 ± 0.23541344265705302
	data : 0.11430897712707519
	model : 0.06493330001831055
			 train-loss:  2.202070439338684 	 ± 0.23459323894332446
	data : 0.11417865753173828
	model : 0.06492938995361328
			 train-loss:  2.2027737573971824 	 ± 0.23379273323550398
	data : 0.11410417556762695
	model : 0.06488618850708008
			 train-loss:  2.199889011270418 	 ± 0.23511103279632142
	data : 0.11402173042297363
	model : 0.06490540504455566
			 train-loss:  2.198859808035195 	 ± 0.23447786796670642
	data : 0.11422166824340821
	model : 0.06491551399230958
			 train-loss:  2.198542749234872 	 ± 0.23359481454731323
	data : 0.11432456970214844
	model : 0.06492962837219238
			 train-loss:  2.197711400802319 	 ± 0.23288613500172278
	data : 0.11448516845703124
	model : 0.06498475074768066
			 train-loss:  2.196249702504573 	 ± 0.23259340064798883
	data : 0.11441407203674317
	model : 0.06503157615661621
			 train-loss:  2.1946799980871603 	 ± 0.23240616217077087
	data : 0.1144601821899414
	model : 0.06496977806091309
			 train-loss:  2.198137103166795 	 ± 0.23491300991775518
	data : 0.11439776420593262
	model : 0.06495051383972168
			 train-loss:  2.1981409850405225 	 ± 0.23403483164929229
	data : 0.1143369197845459
	model : 0.06491165161132813
			 train-loss:  2.196809806647124 	 ± 0.23367506173964048
	data : 0.11427783966064453
	model : 0.06487274169921875
			 train-loss:  2.1964737360968307 	 ± 0.23284712063175073
	data : 0.11444387435913086
	model : 0.06483807563781738
			 train-loss:  2.1975619940862168 	 ± 0.2323426282397132
	data : 0.11447706222534179
	model : 0.06482791900634766
			 train-loss:  2.196271054122759 	 ± 0.23199187291000148
	data : 0.11446070671081543
	model : 0.06489205360412598
			 train-loss:  2.1966873304449397 	 ± 0.23120758350074844
	data : 0.11442928314208985
	model : 0.06496596336364746
			 train-loss:  2.197574975660869 	 ± 0.2306179337887499
	data : 0.11434545516967773
	model : 0.06493487358093261
			 train-loss:  2.19836669864384 	 ± 0.22998954532145538
	data : 0.11422076225280761
	model : 0.06493973731994629
			 train-loss:  2.19766354141101 	 ± 0.22933033871615494
	data : 0.11413650512695313
	model : 0.0649303913116455
			 train-loss:  2.1980533424790925 	 ± 0.22857427977978553
	data : 0.1140636920928955
	model : 0.06488490104675293
			 train-loss:  2.198572492433919 	 ± 0.2278638220924138
	data : 0.1141383171081543
	model : 0.06484732627868653
			 train-loss:  2.1976421939915625 	 ± 0.22735097261615128
	data : 0.11429057121276856
	model : 0.06495194435119629
			 train-loss:  2.201326153866232 	 ± 0.2308729313301358
	data : 0.11443891525268554
	model : 0.06498956680297852
			 train-loss:  2.2022189939914107 	 ± 0.23033908817050422
	data : 0.114432954788208
	model : 0.06502923965454102
			 train-loss:  2.1999302764196655 	 ± 0.2312306812283142
	data : 0.11437358856201171
	model : 0.06507067680358887
			 train-loss:  2.203604019728283 	 ± 0.23474720553312495
	data : 0.11422257423400879
	model : 0.06508278846740723
			 train-loss:  2.207615917523702 	 ± 0.23903363521992632
	data : 0.11415233612060546
	model : 0.06496481895446778
			 train-loss:  2.2084462595301746 	 ± 0.23845776920348147
	data : 0.11400470733642579
	model : 0.06492390632629394
			 train-loss:  2.2086465923409713 	 ± 0.23768482281842299
	data : 0.11408309936523438
	model : 0.06489362716674804
			 train-loss:  2.208452417959575 	 ± 0.23691889675354028
	data : 0.11421489715576172
	model : 0.06487364768981933
			 train-loss:  2.210226173524733 	 ± 0.23716544732864292
	data : 0.11429820060729981
	model : 0.06489424705505371
			 train-loss:  2.2099800355972783 	 ± 0.23641889212507372
	data : 0.11430888175964356
	model : 0.06491141319274903
			 train-loss:  2.2105761705300746 	 ± 0.23577676260313815
	data : 0.11441316604614257
	model : 0.06493792533874512
			 train-loss:  2.210908438749374 	 ± 0.23506131911987466
	data : 0.11436152458190918
	model : 0.06491694450378419
			 train-loss:  2.2101615078841585 	 ± 0.23450310701306068
	data : 0.11430220603942871
	model : 0.06487655639648438
			 train-loss:  2.2105690638224282 	 ± 0.23382063958564778
	data : 0.11424469947814941
	model : 0.06482429504394531
			 train-loss:  2.211403049528599 	 ± 0.2333259105575873
	data : 0.1143178939819336
	model : 0.06481614112854003
			 train-loss:  2.2113821728629355 	 ± 0.2326003171427253
	data : 0.11433463096618653
	model : 0.06481819152832032
			 train-loss:  2.211837864216463 	 ± 0.23195338187235945
	data : 0.11437878608703614
	model : 0.06485471725463868
			 train-loss:  2.21108242924228 	 ± 0.23144058837665762
	data : 0.11443681716918945
	model : 0.06486515998840332
			 train-loss:  2.209760747304777 	 ± 0.23135009633892772
	data : 0.11458134651184082
	model : 0.06488337516784667
			 train-loss:  2.2117580153725362 	 ± 0.2320618347335594
	data : 0.11454839706420898
	model : 0.06489295959472656
			 train-loss:  2.211446094225688 	 ± 0.23139648864247542
	data : 0.11433777809143067
	model : 0.06482362747192383
			 train-loss:  2.2110135312565786 	 ± 0.2307699516309347
	data : 0.1143651008605957
	model : 0.06484088897705079
			 train-loss:  2.2118181784947715 	 ± 0.2303169625591219
	data : 0.11425113677978516
	model : 0.06487030982971191
			 train-loss:  2.21088815440793 	 ± 0.22995071838317588
	data : 0.11419095993041992
	model : 0.06487350463867188
			 train-loss:  2.2124541955835677 	 ± 0.2301754976299658
	data : 0.11418228149414063
	model : 0.0649271011352539
			 train-loss:  2.212536439561007 	 ± 0.22950398819468315
	data : 0.11437668800354003
	model : 0.0649881362915039
			 train-loss:  2.213580223016961 	 ± 0.2292425556434421
	data : 0.11434450149536132
	model : 0.06495523452758789
			 train-loss:  2.2137333175350475 	 ± 0.2285878627419062
	data : 0.11440958976745605
	model : 0.06492676734924316
			 train-loss:  2.211108531760073 	 ± 0.23052981009702286
	data : 0.11437339782714843
	model : 0.064915132522583
			 train-loss:  2.21079233782632 	 ± 0.22990804609334245
	data : 0.11434478759765625
	model : 0.06491546630859375
			 train-loss:  2.209205583415248 	 ± 0.23021293390219547
	data : 0.11428680419921874
	model : 0.06493906974792481
			 train-loss:  2.2068662407708035 	 ± 0.23165002797557102
	data : 0.11428217887878418
	model : 0.06496009826660157
			 train-loss:  2.207482560966792 	 ± 0.2311438915275703
	data : 0.11435437202453613
	model : 0.06498904228210449
			 train-loss:  2.2064545827204953 	 ± 0.23090500327120878
	data : 0.11441712379455567
	model : 0.0650101661682129
			 train-loss:  2.206856526931127 	 ± 0.2303254944595867
	data : 0.11436090469360352
	model : 0.0649651050567627
			 train-loss:  2.2081012047456774 	 ± 0.23029459505385708
	data : 0.11478147506713868
	model : 0.06491875648498535
			 train-loss:  2.2066297347729025 	 ± 0.23051269300198793
	data : 0.11475000381469727
	model : 0.06491551399230958
			 train-loss:  2.206337543133178 	 ± 0.22991580817246007
	data : 0.1146045207977295
	model : 0.06486096382141113
			 train-loss:  2.2060126416061236 	 ± 0.22933230682147426
	data : 0.11458144187927247
	model : 0.06484751701354981
			 train-loss:  2.206351540539716 	 ± 0.2287578450327222
	data : 0.11469106674194336
	model : 0.06486291885375976
			 train-loss:  2.206436969900644 	 ± 0.22814503484648005
	data : 0.11431350708007812
	model : 0.06489319801330566
			 train-loss:  2.2051851532676 	 ± 0.22817380258946218
	data : 0.11431307792663574
	model : 0.06496953964233398
			 train-loss:  2.2043828228686717 	 ± 0.22783048544113535
	data : 0.11447820663452149
	model : 0.06502227783203125
			 train-loss:  2.2050098893503662 	 ± 0.22738956712828365
	data : 0.11437349319458008
	model : 0.06503968238830567
			 train-loss:  2.2053413102501316 	 ± 0.22683614800357163
	data : 0.11432876586914062
	model : 0.06506767272949218
			 train-loss:  2.2063022970529125 	 ± 0.22662900564818383
	data : 0.11432809829711914
	model : 0.06504635810852051
			 train-loss:  2.205215614909927 	 ± 0.22653642143419608
	data : 0.11425118446350098
	model : 0.0649186611175537
			 train-loss:  2.2037999067899476 	 ± 0.22679872582593527
	data : 0.11415495872497558
	model : 0.06496062278747558
			 train-loss:  2.203103474120504 	 ± 0.22642024647063738
	data : 0.11438198089599609
	model : 0.06497001647949219
			 train-loss:  2.2057052752910518 	 ± 0.2287279660240022
	data : 0.1149569034576416
	model : 0.06493277549743652
			 train-loss:  2.207212352631043 	 ± 0.22911233407194345
	data : 0.11490874290466309
	model : 0.06496195793151856
			 train-loss:  2.206833357133236 	 ± 0.22859167831519078
	data : 0.11494688987731934
	model : 0.06506943702697754
			 train-loss:  2.205375596128329 	 ± 0.22892986643788277
	data : 0.11497740745544434
	model : 0.06502623558044433
			 train-loss:  2.2074441149007136 	 ± 0.23020147472360897
	data : 0.11522889137268066
	model : 0.06498475074768066
			 train-loss:  2.2060895216464997 	 ± 0.23041897713532147
	data : 0.11461992263793945
	model : 0.06497035026550294
			 train-loss:  2.2062938984353746 	 ± 0.22986325324497597
	data : 0.1145411491394043
	model : 0.06493186950683594
			 train-loss:  2.205327272415161 	 ± 0.22970274890307182
	data : 0.11459498405456543
	model : 0.06487293243408203
			 train-loss:  2.2044125343191214 	 ± 0.2295048093346038
	data : 0.11459102630615234
	model : 0.0648737907409668
			 train-loss:  2.2042937267060374 	 ± 0.22894786433187952
	data : 0.11417341232299805
	model : 0.06487851142883301
			 train-loss:  2.2052120592535998 	 ± 0.22876510128019586
	data : 0.11420598030090331
	model : 0.064935302734375
			 train-loss:  2.2051016127021565 	 ± 0.22821464954209725
	data : 0.11441936492919921
	model : 0.06503167152404785
			 train-loss:  2.2049892513072433 	 ± 0.22766845087583934
	data : 0.11471972465515137
	model : 0.06508302688598633
			 train-loss:  2.2048541135512867 	 ± 0.22712883360463187
	data : 0.11475286483764649
	model : 0.06506314277648925
			 train-loss:  2.2074412809034283 	 ± 0.22963647649745766
	data : 0.11480188369750977
	model : 0.06507959365844726
			 train-loss:  2.206420733815148 	 ± 0.2295636712454113
	data : 0.1148078441619873
	model : 0.06505155563354492
			 train-loss:  2.2069095039819655 	 ± 0.22912853774444242
	data : 0.1146385669708252
	model : 0.06490836143493653
			 train-loss:  2.205793837893684 	 ± 0.22916125286908096
	data : 0.1143178939819336
	model : 0.06489777565002441
			 train-loss:  2.205307268200906 	 ± 0.2287324246777686
	data : 0.1143078327178955
	model : 0.06491961479187011
			 train-loss:  2.2042572247647794 	 ± 0.2287113791509365
	data : 0.11432075500488281
	model : 0.0649571418762207
			 train-loss:  2.2047088950179345 	 ± 0.2282745168761905
	data : 0.11433258056640624
	model : 0.06500606536865235
			 train-loss:  2.203220757069411 	 ± 0.22878841258453608
	data : 0.11445064544677734
	model : 0.06512584686279296
			 train-loss:  2.2043096052336804 	 ± 0.22882090817611023
	data : 0.11440515518188477
	model : 0.06507148742675781
			 train-loss:  2.2031617722380052 	 ± 0.22892079645551738
	data : 0.1144284725189209
	model : 0.06502881050109863
			 train-loss:  2.204767318621074 	 ± 0.22962446712729168
	data : 0.11430168151855469
	model : 0.06492099761962891
			 train-loss:  2.2041044516996906 	 ± 0.22931191179648694
	data : 0.11421046257019044
	model : 0.06484627723693848
			 train-loss:  2.203672145826245 	 ± 0.22888235382908612
	data : 0.11423678398132324
	model : 0.06475977897644043
			 train-loss:  2.2050441127639635 	 ± 0.2292752508170636
	data : 0.11432380676269531
	model : 0.06468963623046875
			 train-loss:  2.203151626437234 	 ± 0.2304918817387507
	data : 0.1143465518951416
	model : 0.06470584869384766
			 train-loss:  2.2021656866584505 	 ± 0.23044762716275408
	data : 0.11448655128479004
	model : 0.0646669864654541
			 train-loss:  2.2027256457010904 	 ± 0.2300876307831999
	data : 0.11465158462524414
	model : 0.06457366943359374
			 train-loss:  2.202143870623766 	 ± 0.2297438196112803
	data : 0.11462597846984864
	model : 0.06445527076721191
			 train-loss:  2.2018833675048426 	 ± 0.2292706667428255
	data : 0.11458497047424317
	model : 0.06433682441711426
			 train-loss:  2.200934417937931 	 ± 0.22921366650429986
	data : 0.11459031105041503
	model : 0.06415610313415528
			 train-loss:  2.199811801639707 	 ± 0.2293399623625787
	data : 0.11472039222717285
	model : 0.06408815383911133
			 train-loss:  2.2004741933034815 	 ± 0.22906028285067953
	data : 0.11483540534973144
	model : 0.06405820846557617
			 train-loss:  2.199467124877038 	 ± 0.22907365473492555
	data : 0.11480350494384765
	model : 0.06401491165161133
			 train-loss:  2.1986710804289786 	 ± 0.22889940304605827
	data : 0.11485209465026855
	model : 0.06400985717773437
			 train-loss:  2.2005495170666936 	 ± 0.23019270917355306
	data : 0.11481685638427734
	model : 0.06397895812988282
			 train-loss:  2.1995859894997034 	 ± 0.23017069705687013
	data : 0.11474881172180176
	model : 0.06393108367919922
			 train-loss:  2.199814262288682 	 ± 0.22970699276982467
	data : 0.11463513374328613
	model : 0.06386899948120117
			 train-loss:  2.1996626161922843 	 ± 0.22923159669545123
	data : 0.1146613597869873
	model : 0.0639035701751709
			 train-loss:  2.1993450791523927 	 ± 0.2287994819115338
	data : 0.11476044654846192
	model : 0.06396999359130859
			 train-loss:  2.1992953069069805 	 ± 0.22831959049403985
	data : 0.11495814323425294
	model : 0.06399517059326172
			 train-loss:  2.199686589600152 	 ± 0.22792138391995023
	data : 0.11487994194030762
	model : 0.06400303840637207
			 train-loss:  2.198687608540058 	 ± 0.2279697782331011
	data : 0.11483950614929199
	model : 0.06400585174560547
			 train-loss:  2.1991067046446426 	 ± 0.227588948969213
	data : 0.11484794616699219
	model : 0.0639678955078125
			 train-loss:  2.198453137204667 	 ± 0.2273447533282032
	data : 0.11477670669555665
	model : 0.06393861770629883
			 train-loss:  2.1986582637323764 	 ± 0.22689892326698455
	data : 0.11463861465454102
	model : 0.0639723777770996
			 train-loss:  2.197900457948935 	 ± 0.22674142209736667
	data : 0.11480145454406739
	model : 0.06402206420898438
			 train-loss:  2.1978870961130883 	 ± 0.22627830761468398
	data : 0.11474723815917968
	model : 0.06403326988220215
			 train-loss:  2.1984846848782484 	 ± 0.22601156458307323
	data : 0.11480255126953125
	model : 0.06400203704833984
			 train-loss:  2.198619288954175 	 ± 0.22556346740598793
	data : 0.11485524177551269
	model : 0.06397790908813476
			 train-loss:  2.1984343418190555 	 ± 0.2251270081690437
	data : 0.11484780311584472
	model : 0.06397185325622559
			 train-loss:  2.19783030791455 	 ± 0.22487576899711997
	data : 0.11474800109863281
	model : 0.06395444869995118
			 train-loss:  2.1974443650245665 	 ± 0.22450818261877611
	data : 0.11495661735534668
	model : 0.06399860382080078
			 train-loss:  2.1980740563328047 	 ± 0.22428160736502248
	data : 0.1149057388305664
	model : 0.06402478218078614
			 train-loss:  2.1981739917444805 	 ± 0.22384176129541003
	data : 0.11488919258117676
	model : 0.06400270462036133
			 train-loss:  2.1960062467062427 	 ± 0.22603377616262213
	data : 0.11486711502075195
	model : 0.06396322250366211
			 train-loss:  2.1951225418744125 	 ± 0.22602587816928582
	data : 0.11476731300354004
	model : 0.06392407417297363
			 train-loss:  2.196188298861186 	 ± 0.2262208136546343
	data : 0.11457347869873047
	model : 0.06388826370239258
			 train-loss:  2.192662785295397 	 ± 0.23269165981217513
	data : 0.11445698738098145
	model : 0.05547418594360352
#epoch  56    val-loss:  2.3945805336299695  train-loss:  2.192662785295397  lr:  7.62939453125e-08
			 train-loss:  2.151970863342285 	 ± 0.0
	data : 5.673970699310303
	model : 0.07149696350097656
			 train-loss:  2.031802475452423 	 ± 0.12016838788986206
	data : 2.9017035961151123
	model : 0.06804919242858887
			 train-loss:  2.0171220302581787 	 ± 0.10028954036971148
	data : 1.9725571473439534
	model : 0.06689039866129558
			 train-loss:  2.0639514327049255 	 ± 0.11883801046480931
	data : 1.5078395009040833
	model : 0.06629717350006104
			 train-loss:  2.0885881900787355 	 ± 0.11715740454862932
	data : 1.229093647003174
	model : 0.06602301597595214
			 train-loss:  2.0835744937260947 	 ± 0.10753557685432176
	data : 0.11711678504943848
	model : 0.06466474533081054
			 train-loss:  2.0992065838405063 	 ± 0.10666812073827357
	data : 0.11411037445068359
	model : 0.06472091674804688
			 train-loss:  2.1572033762931824 	 ± 0.18303339183792924
	data : 0.11403560638427734
	model : 0.06477580070495606
			 train-loss:  2.1580643918779163 	 ± 0.17258272002654607
	data : 0.11415677070617676
	model : 0.06482658386230469
			 train-loss:  2.144230008125305 	 ± 0.16890478764947905
	data : 0.11413493156433105
	model : 0.06478695869445801
			 train-loss:  2.1377815550023858 	 ± 0.16233028988448484
	data : 0.11420440673828125
	model : 0.06480197906494141
			 train-loss:  2.16798065106074 	 ± 0.1848973648441424
	data : 0.11424813270568848
	model : 0.06497087478637695
			 train-loss:  2.181091602032001 	 ± 0.18335765047359082
	data : 0.11410660743713379
	model : 0.06500091552734374
			 train-loss:  2.1687889950616017 	 ± 0.18217080230971994
	data : 0.11405749320983886
	model : 0.0650087833404541
			 train-loss:  2.1648595174153646 	 ± 0.17660679183967176
	data : 0.11403417587280273
	model : 0.06505880355834961
			 train-loss:  2.1910537779331207 	 ± 0.19882825692601772
	data : 0.11399927139282226
	model : 0.06505093574523926
			 train-loss:  2.1766704320907593 	 ± 0.20128913144861194
	data : 0.11379427909851074
	model : 0.06481842994689942
			 train-loss:  2.1909118824534946 	 ± 0.20424072554077558
	data : 0.11397595405578613
	model : 0.06476268768310547
			 train-loss:  2.206377863883972 	 ± 0.2093426003820302
	data : 0.11394524574279785
	model : 0.064762544631958
			 train-loss:  2.1920374810695646 	 ± 0.2134019520154071
	data : 0.11401071548461914
	model : 0.06480517387390136
			 train-loss:  2.1995685952050343 	 ± 0.21096482308113435
	data : 0.11401786804199218
	model : 0.06484951972961425
			 train-loss:  2.185279363935644 	 ± 0.21626598020992785
	data : 0.11418690681457519
	model : 0.06488614082336426
			 train-loss:  2.1904877009599106 	 ± 0.2129183942716555
	data : 0.11410489082336425
	model : 0.06488246917724609
			 train-loss:  2.1765349706014 	 ± 0.2189130548549752
	data : 0.11406826972961426
	model : 0.06485586166381836
			 train-loss:  2.1779088020324706 	 ± 0.21459568138647114
	data : 0.11401762962341308
	model : 0.06479077339172364
			 train-loss:  2.2159801629873424 	 ± 0.28375308896756946
	data : 0.1139650821685791
	model : 0.06474461555480956
			 train-loss:  2.2151579591963024 	 ± 0.2784803848637658
	data : 0.11395511627197266
	model : 0.06474976539611817
			 train-loss:  2.2132751856531416 	 ± 0.2736372510494329
	data : 0.11398062705993653
	model : 0.06478819847106934
			 train-loss:  2.2165695305528312 	 ± 0.26944246802503485
	data : 0.11407699584960937
	model : 0.0648108959197998
			 train-loss:  2.2082897663116454 	 ± 0.26863981307761037
	data : 0.11414542198181152
	model : 0.064837646484375
			 train-loss:  2.201585223597865 	 ± 0.266810601279081
	data : 0.11400742530822754
	model : 0.0648078441619873
			 train-loss:  2.197106219828129 	 ± 0.26379003183007615
	data : 0.11383223533630371
	model : 0.06480011940002442
			 train-loss:  2.1994360215736157 	 ± 0.26009658645437395
	data : 0.11396937370300293
	model : 0.06479830741882324
			 train-loss:  2.2193331437952377 	 ± 0.2805798854360277
	data : 0.11400346755981446
	model : 0.06479353904724121
			 train-loss:  2.2382953235081264 	 ± 0.29782707929007735
	data : 0.11394548416137695
	model : 0.06476769447326661
			 train-loss:  2.235846519470215 	 ± 0.294018597695901
	data : 0.11408672332763672
	model : 0.0648430347442627
			 train-loss:  2.2391887935432226 	 ± 0.2907106478550157
	data : 0.11421718597412109
	model : 0.06488876342773438
			 train-loss:  2.2402762802023637 	 ± 0.28693626481985257
	data : 0.11404061317443848
	model : 0.06483826637268067
			 train-loss:  2.2543777319101186 	 ± 0.29627296215620347
	data : 0.11398649215698242
	model : 0.0648529052734375
			 train-loss:  2.245488113164902 	 ± 0.29776704642751867
	data : 0.11398968696594239
	model : 0.06488137245178223
			 train-loss:  2.2532695502769657 	 ± 0.29820241160278105
	data : 0.11408324241638183
	model : 0.06487026214599609
			 train-loss:  2.256837169329325 	 ± 0.2955152587847701
	data : 0.11408448219299316
	model : 0.06483678817749024
			 train-loss:  2.259391108224558 	 ± 0.2925274420020294
	data : 0.11421985626220703
	model : 0.06490130424499511
			 train-loss:  2.2577957023273814 	 ± 0.2893733366863166
	data : 0.11422538757324219
	model : 0.0649075984954834
			 train-loss:  2.257170820236206 	 ± 0.28617003403345787
	data : 0.11428141593933105
	model : 0.06489367485046386
			 train-loss:  2.2493139531301414 	 ± 0.2879077382312231
	data : 0.1141214370727539
	model : 0.06484594345092773
			 train-loss:  2.2477175352421215 	 ± 0.2850341446136738
	data : 0.1140988826751709
	model : 0.06488800048828125
			 train-loss:  2.2488915448387465 	 ± 0.2821642263560748
	data : 0.1140716552734375
	model : 0.06489024162292481
			 train-loss:  2.254979768577887 	 ± 0.28243762410242007
	data : 0.11409759521484375
	model : 0.064886474609375
			 train-loss:  2.250432102680206 	 ± 0.28140535401654504
	data : 0.11407537460327148
	model : 0.0648880958557129
			 train-loss:  2.245619224567039 	 ± 0.28070347003506174
	data : 0.11405348777770996
	model : 0.06498808860778808
			 train-loss:  2.2390339855964365 	 ± 0.28194111871355393
	data : 0.11405444145202637
	model : 0.06495494842529297
			 train-loss:  2.240967460398404 	 ± 0.27961645338163393
	data : 0.11397080421447754
	model : 0.06490211486816407
			 train-loss:  2.2492088675498962 	 ± 0.28343832823646004
	data : 0.1139451026916504
	model : 0.06486272811889648
			 train-loss:  2.244052704897794 	 ± 0.2833941649111819
	data : 0.11397848129272461
	model : 0.06489291191101074
			 train-loss:  2.239251937185015 	 ± 0.28310017788889624
	data : 0.11418347358703614
	model : 0.06485514640808106
			 train-loss:  2.2341960480338647 	 ± 0.28314504845829336
	data : 0.1142199993133545
	model : 0.06492824554443359
			 train-loss:  2.2318484454319396 	 ± 0.28125255264245863
	data : 0.1143803596496582
	model : 0.06498680114746094
			 train-loss:  2.2390054767414673 	 ± 0.2841358992060051
	data : 0.1143683910369873
	model : 0.06502561569213867
			 train-loss:  2.2379842042922973 	 ± 0.28186733151991133
	data : 0.11422686576843262
	model : 0.06496701240539551
			 train-loss:  2.2390407187039734 	 ± 0.2796671596028225
	data : 0.11415309906005859
	model : 0.06496796607971192
			 train-loss:  2.2419594295563234 	 ± 0.2783376722620187
	data : 0.11411352157592773
	model : 0.06490426063537598
			 train-loss:  2.2563126919761536 	 ± 0.2983540657501637
	data : 0.11398072242736816
	model : 0.06491374969482422
			 train-loss:  2.2586477026343346 	 ± 0.2965936279900754
	data : 0.11395454406738281
	model : 0.06493101119995118
			 train-loss:  2.256910672554603 	 ± 0.29463118505024877
	data : 0.11399574279785156
	model : 0.06495275497436523
			 train-loss:  2.2587346308159106 	 ± 0.29276016303834373
	data : 0.11399655342102051
	model : 0.06489982604980468
			 train-loss:  2.259399207670297 	 ± 0.290617327842114
	data : 0.11388611793518066
	model : 0.06484694480895996
			 train-loss:  2.2585246317526875 	 ± 0.2885613325654011
	data : 0.11389460563659667
	model : 0.06484031677246094
			 train-loss:  2.258130595303964 	 ± 0.28648110447870867
	data : 0.11395959854125977
	model : 0.06483945846557618
			 train-loss:  2.256377070290702 	 ± 0.2848001737503466
	data : 0.11407032012939453
	model : 0.06483573913574218
			 train-loss:  2.255270246049048 	 ± 0.282939008893009
	data : 0.11406307220458985
	model : 0.06491785049438477
			 train-loss:  2.2543891767660775 	 ± 0.28106535011651357
	data : 0.11423397064208984
	model : 0.06497440338134766
			 train-loss:  2.2510288708830535 	 ± 0.28058611787145243
	data : 0.11439824104309082
	model : 0.06500344276428223
			 train-loss:  2.254622897586307 	 ± 0.2803704923733149
	data : 0.11450433731079102
	model : 0.06496996879577636
			 train-loss:  2.2517965698242186 	 ± 0.2795543471399801
	data : 0.11436166763305664
	model : 0.06492958068847657
			 train-loss:  2.2509726599643103 	 ± 0.27780073301101826
	data : 0.11427497863769531
	model : 0.06488456726074218
			 train-loss:  2.2500297342027937 	 ± 0.2761133274226294
	data : 0.11410903930664062
	model : 0.06490230560302734
			 train-loss:  2.2458975391510205 	 ± 0.2767235620425324
	data : 0.11405763626098633
	model : 0.06491074562072754
			 train-loss:  2.2473174061956285 	 ± 0.2752523641527023
	data : 0.11396675109863282
	model : 0.06497359275817871
			 train-loss:  2.2495438560843466 	 ± 0.27424154484953067
	data : 0.11393003463745117
	model : 0.06502332687377929
			 train-loss:  2.2509998289155373 	 ± 0.2728543831408356
	data : 0.11396446228027343
	model : 0.0650186538696289
			 train-loss:  2.2499257834946236 	 ± 0.271357758228358
	data : 0.11400580406188965
	model : 0.06496572494506836
			 train-loss:  2.248576369630285 	 ± 0.2699947761286515
	data : 0.11397080421447754
	model : 0.06490511894226074
			 train-loss:  2.2469701837925684 	 ± 0.2687814757569188
	data : 0.11395840644836426
	model : 0.0648763656616211
			 train-loss:  2.245423632509568 	 ± 0.2675714322363111
	data : 0.11410961151123047
	model : 0.06488170623779296
			 train-loss:  2.2441101587095926 	 ± 0.266286726362899
	data : 0.11410598754882813
	model : 0.06491780281066895
			 train-loss:  2.2438209947498367 	 ± 0.2647654999787074
	data : 0.11419515609741211
	model : 0.06495361328125
			 train-loss:  2.2450569583611055 	 ± 0.26350914983417945
	data : 0.11404008865356445
	model : 0.0649744987487793
			 train-loss:  2.244108279099625 	 ± 0.2621756654451535
	data : 0.11398181915283204
	model : 0.06495060920715331
			 train-loss:  2.242571779092153 	 ± 0.26111771178275633
	data : 0.11379027366638184
	model : 0.06499791145324707
			 train-loss:  2.2438588627092133 	 ± 0.25996594814002544
	data : 0.1138420581817627
	model : 0.06496281623840332
			 train-loss:  2.240816318470499 	 ± 0.2601732075051741
	data : 0.11393370628356933
	model : 0.0650017261505127
			 train-loss:  2.238073838654385 	 ± 0.26010420484544955
	data : 0.11413860321044922
	model : 0.06507067680358887
			 train-loss:  2.234356350087105 	 ± 0.2611890234108431
	data : 0.11431808471679687
	model : 0.06511220932006836
			 train-loss:  2.2326786041259767 	 ± 0.2603194152274465
	data : 0.11448917388916016
	model : 0.06508293151855468
			 train-loss:  2.2301427498459816 	 ± 0.260136892726495
	data : 0.11446723937988282
	model : 0.06510090827941895
			 train-loss:  2.2281124272297337 	 ± 0.25955595512855123
	data : 0.1143411636352539
	model : 0.06505360603332519
			 train-loss:  2.2273764756261087 	 ± 0.2583300014918683
	data : 0.11428813934326172
	model : 0.06494898796081543
			 train-loss:  2.2295220837448584 	 ± 0.257898160284569
	data : 0.11414995193481445
	model : 0.06491942405700683
			 train-loss:  2.2351471757888794 	 ± 0.26263828901599795
	data : 0.11417431831359863
	model : 0.0648775577545166
			 train-loss:  2.2379164908191944 	 ± 0.26279806375535814
	data : 0.11422414779663086
	model : 0.06489906311035157
			 train-loss:  2.2355770246655333 	 ± 0.26256145777815865
	data : 0.11438579559326172
	model : 0.06494402885437012
			 train-loss:  2.23659217704847 	 ± 0.26148485157765594
	data : 0.1143531322479248
	model : 0.06495246887207032
			 train-loss:  2.236736141718351 	 ± 0.2602287780145544
	data : 0.11448955535888672
	model : 0.06495943069458007
			 train-loss:  2.235492347535633 	 ± 0.2592970581814591
	data : 0.11437001228332519
	model : 0.064918851852417
			 train-loss:  2.2338349549275525 	 ± 0.2586292752991633
	data : 0.11431078910827637
	model : 0.06487298011779785
			 train-loss:  2.232948407948574 	 ± 0.25757966261888127
	data : 0.11422638893127442
	model : 0.06486744880676269
			 train-loss:  2.2296129056700953 	 ± 0.25869555944639916
	data : 0.11430215835571289
	model : 0.06488986015319824
			 train-loss:  2.227747458930409 	 ± 0.25823486342026236
	data : 0.11423730850219727
	model : 0.06493892669677734
			 train-loss:  2.2263678258115593 	 ± 0.2574616173931279
	data : 0.1143763542175293
	model : 0.06503033638000488
			 train-loss:  2.2250690363548897 	 ± 0.2566609881557324
	data : 0.11444082260131835
	model : 0.06506648063659667
			 train-loss:  2.2219224348664284 	 ± 0.25765425792284297
	data : 0.11437468528747559
	model : 0.06507463455200195
			 train-loss:  2.220080807145718 	 ± 0.2572510267928058
	data : 0.11420245170593261
	model : 0.06510753631591797
			 train-loss:  2.2179269069119503 	 ± 0.25714163521331734
	data : 0.11411194801330567
	model : 0.06505331993103028
			 train-loss:  2.2187621396520862 	 ± 0.2561764551490316
	data : 0.11405911445617675
	model : 0.06500782966613769
			 train-loss:  2.2168082389338264 	 ± 0.2559290336645671
	data : 0.11405296325683593
	model : 0.0649949073791504
			 train-loss:  2.216292509665856 	 ± 0.25489350192867527
	data : 0.11415348052978516
	model : 0.06498255729675292
			 train-loss:  2.217037657559928 	 ± 0.2539390909362312
	data : 0.11430830955505371
	model : 0.06495990753173828
			 train-loss:  2.21736127789281 	 ± 0.25289430357986586
	data : 0.11436042785644532
	model : 0.06498351097106933
			 train-loss:  2.2183819393316906 	 ± 0.2520843792587139
	data : 0.11436581611633301
	model : 0.06497631072998047
			 train-loss:  2.2196874717050346 	 ± 0.25144758115848026
	data : 0.11428813934326172
	model : 0.06495585441589355
			 train-loss:  2.2184824963084986 	 ± 0.2507654859686976
	data : 0.11420073509216308
	model : 0.06489248275756836
			 train-loss:  2.2174147183332984 	 ± 0.2500223603048131
	data : 0.11426992416381836
	model : 0.06485605239868164
			 train-loss:  2.2226137288155092 	 ± 0.2556006896646657
	data : 0.11433219909667969
	model : 0.06480617523193359
			 train-loss:  2.2208938884735105 	 ± 0.2552955784106731
	data : 0.11435475349426269
	model : 0.06481928825378418
			 train-loss:  2.221440186576238 	 ± 0.25435382648469884
	data : 0.11445679664611816
	model : 0.06480755805969238
			 train-loss:  2.220297708286075 	 ± 0.25367482155857823
	data : 0.11447830200195312
	model : 0.06486287117004394
			 train-loss:  2.2210982143878937 	 ± 0.2528429481734331
	data : 0.11443324089050293
	model : 0.06486449241638184
			 train-loss:  2.2205942416375923 	 ± 0.2519255624512052
	data : 0.11431784629821777
	model : 0.06487026214599609
			 train-loss:  2.21876974105835 	 ± 0.2518088575876621
	data : 0.11428894996643066
	model : 0.06480603218078614
			 train-loss:  2.2171439250916927 	 ± 0.25152991589867996
	data : 0.11421380043029786
	model : 0.06482577323913574
			 train-loss:  2.2155326225540857 	 ± 0.2512530907950378
	data : 0.11420550346374511
	model : 0.06478338241577149
			 train-loss:  2.2130444489027323 	 ± 0.25193388263557387
	data : 0.11421055793762207
	model : 0.06483492851257325
			 train-loss:  2.2148340611315485 	 ± 0.25183919493659895
	data : 0.11432828903198242
	model : 0.06483731269836426
			 train-loss:  2.2157509565353393 	 ± 0.25112911753915546
	data : 0.11437835693359374
	model : 0.06492972373962402
			 train-loss:  2.2188070179785 	 ± 0.2527111955755435
	data : 0.1144554615020752
	model : 0.06489949226379395
			 train-loss:  2.2196790658644514 	 ± 0.2519924979905344
	data : 0.11441316604614257
	model : 0.0649141788482666
			 train-loss:  2.216983671637549 	 ± 0.25305216474862463
	data : 0.11436314582824707
	model : 0.06490569114685059
			 train-loss:  2.2184880028525704 	 ± 0.25275879307876414
	data : 0.11425786018371582
	model : 0.06488375663757324
			 train-loss:  2.219267566714968 	 ± 0.25202211165945604
	data : 0.11419658660888672
	model : 0.0648575782775879
			 train-loss:  2.217320073580911 	 ± 0.25218180917085836
	data : 0.1141974925994873
	model : 0.06493191719055176
			 train-loss:  2.2141843203087928 	 ± 0.2540359301989949
	data : 0.11422100067138671
	model : 0.06497998237609863
			 train-loss:  2.2140684369560724 	 ± 0.25314990076007654
	data : 0.11429557800292969
	model : 0.06498546600341797
			 train-loss:  2.2129289168450565 	 ± 0.25263714055866965
	data : 0.11427445411682129
	model : 0.06504554748535156
			 train-loss:  2.213286331604267 	 ± 0.2518010009536965
	data : 0.11425414085388183
	model : 0.06503729820251465
			 train-loss:  2.211963463319491 	 ± 0.25144227773762357
	data : 0.11421356201171876
	model : 0.06497654914855958
			 train-loss:  2.213752667920119 	 ± 0.25151642503325367
	data : 0.1141472339630127
	model : 0.06491246223449706
			 train-loss:  2.2115859405414477 	 ± 0.2520380887418069
	data : 0.11398487091064453
	model : 0.06487646102905273
			 train-loss:  2.2099829560158235 	 ± 0.25194674591498073
	data : 0.1141136646270752
	model : 0.0648648738861084
			 train-loss:  2.207964243094126 	 ± 0.25231168575621066
	data : 0.11422386169433593
	model : 0.06490435600280761
			 train-loss:  2.207996449723149 	 ± 0.2514751381266386
	data : 0.11420035362243652
	model : 0.06495041847229004
			 train-loss:  2.2075304930147372 	 ± 0.25071194348296505
	data : 0.11433987617492676
	model : 0.06497464179992676
			 train-loss:  2.206747227244907 	 ± 0.25007779739096575
	data : 0.11442732810974121
	model : 0.06497802734375
			 train-loss:  2.207558810710907 	 ± 0.2494665992179095
	data : 0.11438703536987305
	model : 0.0649526596069336
			 train-loss:  2.2056501111676616 	 ± 0.24978614805215943
	data : 0.11419939994812012
	model : 0.06491098403930665
			 train-loss:  2.2041283945242562 	 ± 0.24970399516624184
	data : 0.11428742408752442
	model : 0.06489033699035644
			 train-loss:  2.203461031245578 	 ± 0.24904701657882414
	data : 0.11426835060119629
	model : 0.06491236686706543
			 train-loss:  2.2006235733816895 	 ± 0.25079053034002013
	data : 0.11426892280578613
	model : 0.06492056846618652
			 train-loss:  2.2000678177899533 	 ± 0.25009821876257254
	data : 0.1143256664276123
	model : 0.0650092601776123
			 train-loss:  2.1987437099218368 	 ± 0.2498738794593468
	data : 0.1144446849822998
	model : 0.06509995460510254
			 train-loss:  2.1968280210258055 	 ± 0.2502725048614424
	data : 0.11440496444702149
	model : 0.06511659622192383
			 train-loss:  2.1957664555973477 	 ± 0.24986219602871493
	data : 0.11435451507568359
	model : 0.06511926651000977
			 train-loss:  2.194957752900621 	 ± 0.24930714395080994
	data : 0.11422147750854492
	model : 0.0651024341583252
			 train-loss:  2.1938523221306685 	 ± 0.24894627168189015
	data : 0.11402826309204102
	model : 0.06501665115356445
			 train-loss:  2.195285933667963 	 ± 0.2488688497719735
	data : 0.11394400596618652
	model : 0.06494073867797852
			 train-loss:  2.1967505381768007 	 ± 0.2488303311184112
	data : 0.11397590637207031
	model : 0.06490883827209473
			 train-loss:  2.196213126182556 	 ± 0.248180818622075
	data : 0.11400008201599121
	model : 0.06493849754333496
			 train-loss:  2.198610490986279 	 ± 0.24937301024792696
	data : 0.11421422958374024
	model : 0.0649423599243164
			 train-loss:  2.1986311490719137 	 ± 0.24863426980835399
	data : 0.11428937911987305
	model : 0.06495795249938965
			 train-loss:  2.1989839546820695 	 ± 0.2479443378661725
	data : 0.1144096851348877
	model : 0.06493401527404785
			 train-loss:  2.1987341977002326 	 ± 0.2472397378120906
	data : 0.11433176994323731
	model : 0.06492547988891602
			 train-loss:  2.2003376504709555 	 ± 0.24741007856655045
	data : 0.11425862312316895
	model : 0.064815092086792
			 train-loss:  2.1997166444800493 	 ± 0.24682838852345065
	data : 0.11416702270507813
	model : 0.06487512588500977
			 train-loss:  2.200794845476918 	 ± 0.2465263261849433
	data : 0.11426405906677246
	model : 0.06489696502685546
			 train-loss:  2.20015985420772 	 ± 0.2459636186455195
	data : 0.1142390251159668
	model : 0.0649336814880371
			 train-loss:  2.1990091516212984 	 ± 0.245735798898796
	data : 0.11436042785644532
	model : 0.06507110595703125
			 train-loss:  2.2002412491599044 	 ± 0.24558521509883302
	data : 0.11474728584289551
	model : 0.06514387130737305
			 train-loss:  2.2013929586731984 	 ± 0.24537327650895224
	data : 0.11475167274475098
	model : 0.06507248878479004
			 train-loss:  2.2006273829071215 	 ± 0.2449000080554579
	data : 0.11465883255004883
	model : 0.06510787010192871
			 train-loss:  2.200073598490821 	 ± 0.24433114636341594
	data : 0.11469154357910157
	model : 0.06510610580444336
			 train-loss:  2.2000725717175733 	 ± 0.24365526400199788
	data : 0.11474323272705078
	model : 0.06494460105895997
			 train-loss:  2.2022070426207323 	 ± 0.24467594966290676
	data : 0.11440606117248535
	model : 0.0649019718170166
			 train-loss:  2.2017327488445844 	 ± 0.24409040070347227
	data : 0.11436443328857422
	model : 0.06490302085876465
			 train-loss:  2.202064545258232 	 ± 0.24346758502270532
	data : 0.11429367065429688
	model : 0.06482272148132324
			 train-loss:  2.2005101493887 	 ± 0.24372242939917033
	data : 0.11427006721496583
	model : 0.06484274864196778
			 train-loss:  2.199968324553582 	 ± 0.24317807389722595
	data : 0.11417427062988281
	model : 0.06490912437438964
			 train-loss:  2.2007485653627366 	 ± 0.24276032394855154
	data : 0.1142146110534668
	model : 0.06495370864868164
			 train-loss:  2.1999835644630674 	 ± 0.24233972244147722
	data : 0.11427807807922363
	model : 0.06501846313476563
			 train-loss:  2.1990005370801087 	 ± 0.24207329572789205
	data : 0.11425719261169434
	model : 0.06506857872009278
			 train-loss:  2.198472743912747 	 ± 0.24154442881117857
	data : 0.11428532600402833
	model : 0.06506586074829102
			 train-loss:  2.1988657912658773 	 ± 0.24097219552449178
	data : 0.11421971321105957
	model : 0.06504316329956054
			 train-loss:  2.1988531295210123 	 ± 0.24034390823449495
	data : 0.11423821449279785
	model : 0.06500344276428223
			 train-loss:  2.1980788207424737 	 ± 0.2399604281650944
	data : 0.1147092342376709
	model : 0.06496572494506836
			 train-loss:  2.197459126870657 	 ± 0.2394959576397763
	data : 0.11490578651428222
	model : 0.06495304107666015
			 train-loss:  2.1973160529747986 	 ± 0.23888938807384655
	data : 0.11515398025512695
	model : 0.06505827903747559
			 train-loss:  2.1973401642575556 	 ± 0.23827943494998075
	data : 0.11536035537719727
	model : 0.06510987281799316
			 train-loss:  2.197822005010498 	 ± 0.23776960690143512
	data : 0.11574382781982422
	model : 0.06515913009643555
			 train-loss:  2.1977107049238804 	 ± 0.23717356333061784
	data : 0.11532044410705566
	model : 0.0651695728302002
			 train-loss:  2.197604622074108 	 ± 0.23658160855202653
	data : 0.11521711349487304
	model : 0.06516199111938477
			 train-loss:  2.198410649895668 	 ± 0.2362631789054647
	data : 0.11498298645019531
	model : 0.06499013900756836
			 train-loss:  2.1978819506678415 	 ± 0.23579330227363215
	data : 0.11463665962219238
	model : 0.0648625373840332
			 train-loss:  2.1980370395254383 	 ± 0.23521920830045315
	data : 0.11421704292297363
	model : 0.06479153633117676
			 train-loss:  2.198082775317977 	 ± 0.23464003578091308
	data : 0.11413969993591308
	model : 0.06475887298583985
			 train-loss:  2.198911729396558 	 ± 0.2343620250138395
	data : 0.11431121826171875
	model : 0.0647428035736084
			 train-loss:  2.1980495249352803 	 ± 0.23411382297156327
	data : 0.1142268180847168
	model : 0.06479592323303222
			 train-loss:  2.198274254220203 	 ± 0.23356705845789535
	data : 0.1144193172454834
	model : 0.06487607955932617
			 train-loss:  2.1961934595292316 	 ± 0.23490837900098643
	data : 0.11473665237426758
	model : 0.06493287086486817
			 train-loss:  2.1960153470818815 	 ± 0.23435702588599194
	data : 0.114697265625
	model : 0.06494297981262206
			 train-loss:  2.195171624849858 	 ± 0.23411213863349856
	data : 0.11458191871643067
	model : 0.06494545936584473
			 train-loss:  2.1929919237182256 	 ± 0.23567027794660236
	data : 0.11455636024475098
	model : 0.06485309600830078
			 train-loss:  2.1927620726174086 	 ± 0.23513474754949693
	data : 0.11453676223754883
	model : 0.06486053466796875
			 train-loss:  2.1924618769366786 	 ± 0.2346200549505221
	data : 0.11419377326965333
	model : 0.06483893394470215
			 train-loss:  2.1908937384824796 	 ± 0.23517962480628385
	data : 0.11430482864379883
	model : 0.06488866806030273
			 train-loss:  2.1901594968599696 	 ± 0.23487407501561045
	data : 0.11419920921325684
	model : 0.0650421142578125
			 train-loss:  2.192432103046151 	 ± 0.2366738261262113
	data : 0.11411819458007813
	model : 0.0651280403137207
			 train-loss:  2.1921824404486903 	 ± 0.23615371019350492
	data : 0.11413121223449707
	model : 0.0650754451751709
			 train-loss:  2.19279304187968 	 ± 0.23577978913523293
	data : 0.11406030654907226
	model : 0.06504335403442382
			 train-loss:  2.1924331680350346 	 ± 0.23529811470440246
	data : 0.11393733024597168
	model : 0.06492037773132324
			 train-loss:  2.192712389715186 	 ± 0.23479648624519392
	data : 0.11398005485534668
	model : 0.06477179527282714
			 train-loss:  2.1948303721167823 	 ± 0.23634974687367352
	data : 0.11420202255249023
	model : 0.06470112800598145
			 train-loss:  2.1939210406255936 	 ± 0.2361998129781164
	data : 0.11423754692077637
	model : 0.06530237197875977
			 train-loss:  2.1954306224444964 	 ± 0.23673332422115256
	data : 0.11394586563110351
	model : 0.06526002883911133
			 train-loss:  2.1945602091973138 	 ± 0.23655770090333184
	data : 0.11407513618469238
	model : 0.0652742862701416
			 train-loss:  2.1933214478194714 	 ± 0.2367528808484694
	data : 0.11411151885986329
	model : 0.06516427993774414
			 train-loss:  2.1923644341362847 	 ± 0.23666001554726698
	data : 0.11403570175170899
	model : 0.06502285003662109
			 train-loss:  2.190569653975225 	 ± 0.23766555950505747
	data : 0.11397352218627929
	model : 0.06428794860839844
			 train-loss:  2.1894371246976476 	 ± 0.23775188461038377
	data : 0.11453990936279297
	model : 0.06424474716186523
			 train-loss:  2.189422184960884 	 ± 0.23723003273196963
	data : 0.11459136009216309
	model : 0.06411910057067871
			 train-loss:  2.1891693111069856 	 ± 0.23674229045048856
	data : 0.1147378921508789
	model : 0.06405839920043946
			 train-loss:  2.1893689145212587 	 ± 0.23624638318932975
	data : 0.11485118865966797
	model : 0.06405377388000488
			 train-loss:  2.188629116330828 	 ± 0.23600131559636034
	data : 0.11495537757873535
	model : 0.06401820182800293
			 train-loss:  2.1896116754104353 	 ± 0.2359651709104677
	data : 0.1148003101348877
	model : 0.06385860443115235
			 train-loss:  2.1887842176298213 	 ± 0.23579533720138302
	data : 0.11476702690124511
	model : 0.06386852264404297
			 train-loss:  2.190004151091616 	 ± 0.23602668486789993
	data : 0.11485438346862793
	model : 0.06384062767028809
			 train-loss:  2.1903255838028928 	 ± 0.2355752846919731
	data : 0.1148571491241455
	model : 0.06386618614196778
			 train-loss:  2.1910012475514815 	 ± 0.23530373132773946
	data : 0.11488280296325684
	model : 0.063861083984375
			 train-loss:  2.1935745804621702 	 ± 0.23811137803351262
	data : 0.11495542526245117
	model : 0.06393017768859863
			 train-loss:  2.193808386305801 	 ± 0.23763787826589322
	data : 0.11500606536865235
	model : 0.0638923168182373
			 train-loss:  2.193729199126176 	 ± 0.23714335342182172
	data : 0.11481103897094727
	model : 0.06388940811157226
			 train-loss:  2.193213857213656 	 ± 0.23678285903979232
	data : 0.114764404296875
	model : 0.06383213996887208
			 train-loss:  2.192105920482968 	 ± 0.236913673170428
	data : 0.1148099422454834
	model : 0.06385126113891601
			 train-loss:  2.1943117291474143 	 ± 0.23889069032555235
	data : 0.11479182243347168
	model : 0.0638887882232666
			 train-loss:  2.193355740833675 	 ± 0.2388620483811868
	data : 0.11479802131652832
	model : 0.06387367248535156
			 train-loss:  2.194022759062345 	 ± 0.23859874225454583
	data : 0.11479463577270507
	model : 0.06388945579528808
			 train-loss:  2.193917234576478 	 ± 0.23811701348985553
	data : 0.11481714248657227
	model : 0.06392292976379395
			 train-loss:  2.1932355998977413 	 ± 0.2378719378328578
	data : 0.11464090347290039
	model : 0.06396398544311524
			 train-loss:  2.192636758209723 	 ± 0.2375756637262464
	data : 0.11478495597839355
	model : 0.06397347450256348
			 train-loss:  2.1924626356171024 	 ± 0.23711198874850303
	data : 0.11493048667907715
	model : 0.06405611038208008
			 train-loss:  2.192975410974648 	 ± 0.23677312437424355
	data : 0.11507210731506348
	model : 0.0640486240386963
			 train-loss:  2.1934149389266966 	 ± 0.23640086589641468
	data : 0.11503915786743164
	model : 0.06402049064636231
			 train-loss:  2.193099125447976 	 ± 0.2359823151442076
	data : 0.11516337394714356
	model : 0.06394996643066406
			 train-loss:  2.193415953999474 	 ± 0.23556711522126944
	data : 0.11499428749084473
	model : 0.06389069557189941
			 train-loss:  2.1936729576276695 	 ± 0.23513650323543236
	data : 0.11477646827697754
	model : 0.06382560729980469
			 train-loss:  2.1934067395728403 	 ± 0.23471137992265226
	data : 0.1147697925567627
	model : 0.06380705833435059
			 train-loss:  2.195708023333082 	 ± 0.237104523874087
	data : 0.11487979888916015
	model : 0.0638505458831787
			 train-loss:  2.1966536780819297 	 ± 0.2371223071553366
	data : 0.11463837623596192
	model : 0.05550503730773926
#epoch  57    val-loss:  2.449944282832899  train-loss:  2.1966536780819297  lr:  7.62939453125e-08
			 train-loss:  2.187098741531372 	 ± 0.0
	data : 5.658714294433594
	model : 0.07171058654785156
			 train-loss:  2.197383403778076 	 ± 0.010284662246704102
	data : 2.894569158554077
	model : 0.0681999921798706
			 train-loss:  2.073787212371826 	 ± 0.17499300913259352
	data : 1.9678470293680828
	model : 0.0670028527577718
			 train-loss:  2.0949785709381104 	 ± 0.15592990722633823
	data : 1.5044474601745605
	model : 0.06642693281173706
			 train-loss:  2.1731899261474608 	 ± 0.20956949470002642
	data : 1.2264217376708983
	model : 0.06604990959167481
			 train-loss:  2.1778628826141357 	 ± 0.19159504221504323
	data : 0.11743412017822266
	model : 0.06464142799377441
			 train-loss:  2.212728909083775 	 ± 0.19687154364607976
	data : 0.11417083740234375
	model : 0.0645869255065918
			 train-loss:  2.1967553794384003 	 ± 0.18894358855413218
	data : 0.11393990516662597
	model : 0.06464977264404297
			 train-loss:  2.1816174189249673 	 ± 0.18321111148319083
	data : 0.1139601230621338
	model : 0.06468453407287597
			 train-loss:  2.1625698089599608 	 ± 0.18296169883947488
	data : 0.1139444351196289
	model : 0.06473393440246582
			 train-loss:  2.1617267565293745 	 ± 0.17446750505892492
	data : 0.11407175064086914
	model : 0.06479134559631347
			 train-loss:  2.1910980145136514 	 ± 0.1933693698927797
	data : 0.11410069465637207
	model : 0.06483616828918456
			 train-loss:  2.1919237833756666 	 ± 0.18580530216208535
	data : 0.1142697811126709
	model : 0.06487092971801758
			 train-loss:  2.176647501332419 	 ± 0.18732693467912329
	data : 0.11417760848999023
	model : 0.06484179496765137
			 train-loss:  2.1693341811498006 	 ± 0.183032074298014
	data : 0.11408038139343261
	model : 0.06487874984741211
			 train-loss:  2.164143390953541 	 ± 0.17835668897154633
	data : 0.11398444175720215
	model : 0.06484866142272949
			 train-loss:  2.167228004511665 	 ± 0.17347075914175622
	data : 0.11389784812927246
	model : 0.0649024486541748
			 train-loss:  2.1796301007270813 	 ± 0.1761678873841095
	data : 0.11394209861755371
	model : 0.06488184928894043
			 train-loss:  2.1892789727763127 	 ± 0.17628814512193536
	data : 0.11394729614257812
	model : 0.06492719650268555
			 train-loss:  2.190980690717697 	 ± 0.17198446336833595
	data : 0.11406340599060058
	model : 0.06487884521484374
			 train-loss:  2.1923680248714628 	 ± 0.1679542858927557
	data : 0.11398906707763672
	model : 0.06484026908874511
			 train-loss:  2.190851769664071 	 ± 0.16423979654721846
	data : 0.11409015655517578
	model : 0.06483330726623535
			 train-loss:  2.1968033780222354 	 ± 0.16303733738998435
	data : 0.11409196853637696
	model : 0.0648200511932373
			 train-loss:  2.188644344607989 	 ± 0.16433116106237555
	data : 0.11414408683776855
	model : 0.06484556198120117
			 train-loss:  2.1805632972717284 	 ± 0.1658065773770307
	data : 0.11416583061218262
	model : 0.06488580703735351
			 train-loss:  2.190765674297626 	 ± 0.1704014547804005
	data : 0.1143117904663086
	model : 0.06492671966552735
			 train-loss:  2.1861251725090876 	 ± 0.16888195514438148
	data : 0.11422414779663086
	model : 0.06495108604431152
			 train-loss:  2.201594122818538 	 ± 0.18429132093167322
	data : 0.11416001319885254
	model : 0.06490631103515625
			 train-loss:  2.1908801876265427 	 ± 0.18975304585324693
	data : 0.1141890525817871
	model : 0.06488227844238281
			 train-loss:  2.1885979930559794 	 ± 0.1869680589831081
	data : 0.11412105560302735
	model : 0.0648193359375
			 train-loss:  2.1909211566371303 	 ± 0.1843673528925143
	data : 0.11401219367980957
	model : 0.06484498977661132
			 train-loss:  2.1973401568830013 	 ± 0.1849497297268927
	data : 0.11408157348632812
	model : 0.06488542556762696
			 train-loss:  2.1853291880000723 	 ± 0.1943869152642746
	data : 0.1141021728515625
	model : 0.06498875617980956
			 train-loss:  2.1836772631196415 	 ± 0.1917419205657443
	data : 0.11408839225769044
	model : 0.06500735282897949
			 train-loss:  2.179619710786002 	 ± 0.19045814083002457
	data : 0.11403460502624511
	model : 0.06506476402282715
			 train-loss:  2.1845639248689017 	 ± 0.1900585902313295
	data : 0.11400742530822754
	model : 0.06499009132385254
			 train-loss:  2.1853829880018494 	 ± 0.18753703989804826
	data : 0.11377248764038086
	model : 0.0648625373840332
			 train-loss:  2.188935414740914 	 ± 0.18631033659159058
	data : 0.11382565498352051
	model : 0.06479873657226562
			 train-loss:  2.185238229922759 	 ± 0.1853130612415652
	data : 0.11381311416625976
	model : 0.06477551460266114
			 train-loss:  2.1879464954137804 	 ± 0.18376196771061318
	data : 0.11384773254394531
	model : 0.06480779647827148
			 train-loss:  2.191587253314693 	 ± 0.1829618662619093
	data : 0.11395635604858398
	model : 0.0648681640625
			 train-loss:  2.1988819241523743 	 ± 0.18670758089773504
	data : 0.11410589218139648
	model : 0.0649139404296875
			 train-loss:  2.194346597028333 	 ± 0.18685003519731244
	data : 0.11402435302734375
	model : 0.06485686302185059
			 train-loss:  2.197914781895551 	 ± 0.18619058354960288
	data : 0.1138237476348877
	model : 0.06478333473205566
			 train-loss:  2.1937763664457535 	 ± 0.1861454332822935
	data : 0.11388616561889649
	model : 0.06482071876525879
			 train-loss:  2.193640245043713 	 ± 0.184113260276178
	data : 0.11393837928771973
	model : 0.06486024856567382
			 train-loss:  2.195598432358275 	 ± 0.18262763206837923
	data : 0.11401972770690919
	model : 0.06491351127624512
			 train-loss:  2.1978152419130006 	 ± 0.18135316486890027
	data : 0.11399688720703124
	model : 0.06500244140625
			 train-loss:  2.1916211259608365 	 ± 0.18455185289462125
	data : 0.11415529251098633
	model : 0.06511197090148926
			 train-loss:  2.1933183455467224 	 ± 0.1830828933584844
	data : 0.11419758796691895
	model : 0.0650221824645996
			 train-loss:  2.191611558783288 	 ± 0.18168037818653313
	data : 0.11406221389770507
	model : 0.0649759292602539
			 train-loss:  2.189485951111867 	 ± 0.18056418197618845
	data : 0.11400232315063477
	model : 0.06493496894836426
			 train-loss:  2.188100979013263 	 ± 0.17913126068330631
	data : 0.11415643692016601
	model : 0.06491985321044921
			 train-loss:  2.1840745365178145 	 ± 0.17986949393748347
	data : 0.11428656578063964
	model : 0.06493334770202637
			 train-loss:  2.179509676586498 	 ± 0.18135613291659866
	data : 0.11424994468688965
	model : 0.06497254371643066
			 train-loss:  2.1783583951847896 	 ± 0.17993227716890448
	data : 0.1143606185913086
	model : 0.06497869491577149
			 train-loss:  2.1759022725255868 	 ± 0.1792915310721384
	data : 0.1143561840057373
	model : 0.06498641967773437
			 train-loss:  2.1817686989389617 	 ± 0.18317443549570164
	data : 0.11418614387512208
	model : 0.06494417190551757
			 train-loss:  2.1797033750404746 	 ± 0.18229531997640883
	data : 0.1140129566192627
	model : 0.06485953330993652
			 train-loss:  2.1764670888582867 	 ± 0.18247099304859712
	data : 0.11394786834716797
	model : 0.06484122276306152
			 train-loss:  2.17560860368072 	 ± 0.18109128241353234
	data : 0.11396222114562989
	model : 0.06486678123474121
			 train-loss:  2.177318496088828 	 ± 0.18012069256293337
	data : 0.11414308547973633
	model : 0.06487898826599121
			 train-loss:  2.1756724100264293 	 ± 0.17915491461357821
	data : 0.11425728797912597
	model : 0.06492099761962891
			 train-loss:  2.1727044600993395 	 ± 0.17930400347759523
	data : 0.11441564559936523
	model : 0.06498093605041504
			 train-loss:  2.172952954585736 	 ± 0.1779305015750477
	data : 0.1143420696258545
	model : 0.06499156951904297
			 train-loss:  2.1727041566010676 	 ± 0.17658879100954764
	data : 0.11434359550476074
	model : 0.06497030258178711
			 train-loss:  2.175018639706854 	 ± 0.17627173665415447
	data : 0.1140552043914795
	model : 0.06493177413940429
			 train-loss:  2.1676937008605286 	 ± 0.18495852199473198
	data : 0.11402621269226074
	model : 0.06491341590881347
			 train-loss:  2.16703060226164 	 ± 0.18369475317004347
	data : 0.11391563415527343
	model : 0.06489815711975097
			 train-loss:  2.169797360897064 	 ± 0.1838202936984362
	data : 0.11404328346252442
	model : 0.06495904922485352
			 train-loss:  2.1742693252966436 	 ± 0.18631660923536245
	data : 0.11394639015197754
	model : 0.06500716209411621
			 train-loss:  2.1756009310483932 	 ± 0.18535813177652286
	data : 0.11412954330444336
	model : 0.06504635810852051
			 train-loss:  2.178747720914344 	 ± 0.18601061550077372
	data : 0.11410584449768066
	model : 0.0650632381439209
			 train-loss:  2.1827040894611462 	 ± 0.18781650726502735
	data : 0.11413202285766602
	model : 0.06502408981323242
			 train-loss:  2.1870310513178506 	 ± 0.19023716474051927
	data : 0.11385993957519532
	model : 0.06487746238708496
			 train-loss:  2.188187177243986 	 ± 0.18924650411961738
	data : 0.11398906707763672
	model : 0.06485285758972167
			 train-loss:  2.1907085211246047 	 ± 0.18929412201016035
	data : 0.1139094352722168
	model : 0.06485142707824706
			 train-loss:  2.1950975152162404 	 ± 0.19197955188783883
	data : 0.11393175125122071
	model : 0.06482501029968261
			 train-loss:  2.1933093538767174 	 ± 0.1914132217373357
	data : 0.11397838592529297
	model : 0.06484503746032715
			 train-loss:  2.1956939205527304 	 ± 0.1913902823761539
	data : 0.1142615795135498
	model : 0.06491918563842773
			 train-loss:  2.197238543887197 	 ± 0.1907062765095784
	data : 0.1141042709350586
	model : 0.06488270759582519
			 train-loss:  2.1950660202561356 	 ± 0.19054571498431808
	data : 0.1140446662902832
	model : 0.06478519439697265
			 train-loss:  2.1959331911730477 	 ± 0.1895570903459508
	data : 0.11400880813598632
	model : 0.06478900909423828
			 train-loss:  2.1966175933678946 	 ± 0.18852853266141242
	data : 0.11391634941101074
	model : 0.06483497619628906
			 train-loss:  2.1956403606078205 	 ± 0.1876301501396092
	data : 0.11384878158569336
	model : 0.06487641334533692
			 train-loss:  2.1950245721395625 	 ± 0.18662246257310786
	data : 0.11395044326782226
	model : 0.0649287223815918
			 train-loss:  2.1934922960982925 	 ± 0.18609013717901388
	data : 0.11397409439086914
	model : 0.06503615379333497
			 train-loss:  2.194192107428204 	 ± 0.18514488566520843
	data : 0.11403059959411621
	model : 0.0650336742401123
			 train-loss:  2.1934540660193798 	 ± 0.18423194513253588
	data : 0.11393861770629883
	model : 0.0649498462677002
			 train-loss:  2.189183247089386 	 ± 0.1875836704767278
	data : 0.11402134895324707
	model : 0.06495728492736816
			 train-loss:  2.187841005377717 	 ± 0.18698422738887965
	data : 0.11406540870666504
	model : 0.06494388580322266
			 train-loss:  2.1859121128268866 	 ± 0.18687333836459766
	data : 0.11417341232299805
	model : 0.06492390632629394
			 train-loss:  2.1840397683523034 	 ± 0.186731531963083
	data : 0.11413779258728027
	model : 0.06499629020690918
			 train-loss:  2.1846308974509543 	 ± 0.18582308551451793
	data : 0.11433701515197754
	model : 0.06503000259399414
			 train-loss:  2.1831793145129557 	 ± 0.18537747938715865
	data : 0.11437344551086426
	model : 0.06499533653259278
			 train-loss:  2.1837730295956135 	 ± 0.18450021779515893
	data : 0.1142697811126709
	model : 0.06495060920715331
			 train-loss:  2.186095698592589 	 ± 0.18495215350853003
	data : 0.11411781311035156
	model : 0.06486177444458008
			 train-loss:  2.182169666095656 	 ± 0.1880249370118727
	data : 0.11420412063598633
	model : 0.06483769416809082
			 train-loss:  2.179430502833742 	 ± 0.18902795461928604
	data : 0.11415181159973145
	model : 0.06489300727844238
			 train-loss:  2.177875369787216 	 ± 0.18871586417607464
	data : 0.11408653259277343
	model : 0.06493511199951171
			 train-loss:  2.178228233120229 	 ± 0.18781245426663368
	data : 0.11416811943054199
	model : 0.06497750282287598
			 train-loss:  2.176285848898046 	 ± 0.18790624704112133
	data : 0.11438322067260742
	model : 0.06508111953735352
			 train-loss:  2.1747434555905536 	 ± 0.1876395743712282
	data : 0.11420164108276368
	model : 0.06501874923706055
			 train-loss:  2.1762770528976736 	 ± 0.18738279904164692
	data : 0.11428632736206054
	model : 0.06493229866027832
			 train-loss:  2.176946428843907 	 ± 0.18661326068774317
	data : 0.11427006721496583
	model : 0.06489319801330566
			 train-loss:  2.176673652990809 	 ± 0.18575195466014055
	data : 0.11427626609802247
	model : 0.064898681640625
			 train-loss:  2.1768472551185396 	 ± 0.184890556658664
	data : 0.11424355506896973
	model : 0.06492133140563965
			 train-loss:  2.1807538491708263 	 ± 0.18841701801015412
	data : 0.11438260078430176
	model : 0.06494283676147461
			 train-loss:  2.1775538877609675 	 ± 0.19047616387744673
	data : 0.11425743103027344
	model : 0.06506342887878418
			 train-loss:  2.178893035108393 	 ± 0.19012314850828158
	data : 0.11435480117797851
	model : 0.0650259017944336
			 train-loss:  2.179773627100764 	 ± 0.1894900084322008
	data : 0.11443800926208496
	model : 0.06493005752563477
			 train-loss:  2.1784348360129764 	 ± 0.1891687659429303
	data : 0.11442456245422364
	model : 0.0648503303527832
			 train-loss:  2.1796781278289523 	 ± 0.18878895337395263
	data : 0.11441173553466796
	model : 0.06485962867736816
			 train-loss:  2.1809636417188143 	 ± 0.1884552039606198
	data : 0.11457061767578125
	model : 0.06477174758911133
			 train-loss:  2.1794381307519 	 ± 0.18833967609480448
	data : 0.11447672843933106
	model : 0.06485180854797364
			 train-loss:  2.1812488231165656 	 ± 0.1885287291346901
	data : 0.11437206268310547
	model : 0.06494851112365722
			 train-loss:  2.1805822706630087 	 ± 0.1878585440646606
	data : 0.11430516242980956
	model : 0.06501898765563965
			 train-loss:  2.1798284498311706 	 ± 0.18723846423686388
	data : 0.11439933776855468
	model : 0.06497268676757813
			 train-loss:  2.1818615729067505 	 ± 0.18775355912473968
	data : 0.11415262222290039
	model : 0.06492085456848144
			 train-loss:  2.1847784479459125 	 ± 0.18965787182521598
	data : 0.1141282081604004
	model : 0.06481399536132812
			 train-loss:  2.18505024712933 	 ± 0.18889600233492138
	data : 0.1141127586364746
	model : 0.06481246948242188
			 train-loss:  2.1837598632593624 	 ± 0.18865498324753044
	data : 0.11414289474487305
	model : 0.06479597091674805
			 train-loss:  2.1843867515160786 	 ± 0.18801407405226409
	data : 0.11414632797241211
	model : 0.0648615837097168
			 train-loss:  2.1839030308108174 	 ± 0.18733125032827733
	data : 0.11429715156555176
	model : 0.06493988037109374
			 train-loss:  2.183744367599487 	 ± 0.1865887856988553
	data : 0.11445965766906738
	model : 0.06500782966613769
			 train-loss:  2.1851479874716864 	 ± 0.18650825998380213
	data : 0.11447091102600097
	model : 0.06494131088256835
			 train-loss:  2.1862986425715167 	 ± 0.18622098619587255
	data : 0.11435074806213379
	model : 0.06485948562622071
			 train-loss:  2.186348346993327 	 ± 0.18549297987892016
	data : 0.1142336368560791
	model : 0.06484789848327636
			 train-loss:  2.185720911321714 	 ± 0.18490892384202723
	data : 0.1143251895904541
	model : 0.06486520767211915
			 train-loss:  2.183880739028637 	 ± 0.18537832730229487
	data : 0.11419925689697266
	model : 0.06489372253417969
			 train-loss:  2.182646155357361 	 ± 0.1852051312198318
	data : 0.11421723365783691
	model : 0.0650141716003418
			 train-loss:  2.1832833046262916 	 ± 0.18464632591342928
	data : 0.11431589126586914
	model : 0.06509618759155274
			 train-loss:  2.184140778125677 	 ± 0.18421447347319495
	data : 0.1144101619720459
	model : 0.0651026725769043
			 train-loss:  2.18212263708684 	 ± 0.1849957323241192
	data : 0.11438150405883789
	model : 0.06501717567443847
			 train-loss:  2.182541842813845 	 ± 0.18437316083759345
	data : 0.11427450180053711
	model : 0.06491475105285645
			 train-loss:  2.181443469489322 	 ± 0.18413684515099826
	data : 0.11430673599243164
	model : 0.06477174758911133
			 train-loss:  2.1795711691362145 	 ± 0.1847583153882155
	data : 0.1143887996673584
	model : 0.06483426094055175
			 train-loss:  2.1805793841679892 	 ± 0.18446554039930418
	data : 0.1144744873046875
	model : 0.0648348331451416
			 train-loss:  2.179120644391012 	 ± 0.18459790459307307
	data : 0.11442861557006836
	model : 0.06491951942443848
			 train-loss:  2.1787406367915017 	 ± 0.18399199988889559
	data : 0.11447968482971191
	model : 0.06501550674438476
			 train-loss:  2.1776575830811304 	 ± 0.18378570176905382
	data : 0.11444721221923829
	model : 0.06506767272949218
			 train-loss:  2.1776159472868475 	 ± 0.18313809300526693
	data : 0.11441655158996582
	model : 0.06497397422790527
			 train-loss:  2.1760228955662333 	 ± 0.18348130226709838
	data : 0.11431522369384765
	model : 0.06492242813110352
			 train-loss:  2.176894667247931 	 ± 0.18314005253276613
	data : 0.11433615684509277
	model : 0.06481771469116211
			 train-loss:  2.1766396497857983 	 ± 0.18253309687915087
	data : 0.11437435150146484
	model : 0.06481351852416992
			 train-loss:  2.1778100312572635 	 ± 0.18245203112314623
	data : 0.1143801212310791
	model : 0.06482715606689453
			 train-loss:  2.176870259297948 	 ± 0.1821846114177507
	data : 0.11437182426452637
	model : 0.06493582725524902
			 train-loss:  2.178640313245155 	 ± 0.18283197730573228
	data : 0.1142949104309082
	model : 0.06495494842529297
			 train-loss:  2.178026702580036 	 ± 0.18237025729221884
	data : 0.11426253318786621
	model : 0.06500716209411621
			 train-loss:  2.17801641702652 	 ± 0.1817613832371908
	data : 0.11418161392211915
	model : 0.06486721038818359
			 train-loss:  2.177993446785883 	 ± 0.18115874302452895
	data : 0.11412987709045411
	model : 0.06485252380371094
			 train-loss:  2.177859663179046 	 ± 0.18056932646696172
	data : 0.11409306526184082
	model : 0.06478328704833984
			 train-loss:  2.1776677136327707 	 ± 0.17999382108326087
	data : 0.11408767700195313
	model : 0.06481404304504394
			 train-loss:  2.178147113942481 	 ± 0.17950644454795966
	data : 0.11422109603881836
	model : 0.06486301422119141
			 train-loss:  2.1779887084038028 	 ± 0.1789372525311409
	data : 0.11437325477600098
	model : 0.06500353813171386
			 train-loss:  2.17740773772582 	 ± 0.17850941141534998
	data : 0.1143491268157959
	model : 0.06508240699768067
			 train-loss:  2.1757375571378477 	 ± 0.17915860792588167
	data : 0.11444144248962403
	model : 0.06504111289978028
			 train-loss:  2.177635188344159 	 ± 0.1801666278106735
	data : 0.11439957618713378
	model : 0.0649960994720459
			 train-loss:  2.1764290955081678 	 ± 0.18023789561414214
	data : 0.11407151222229003
	model : 0.06494512557983398
			 train-loss:  2.177524331957102 	 ± 0.18020374778106263
	data : 0.1141359806060791
	model : 0.06490554809570312
			 train-loss:  2.179231490407671 	 ± 0.18093643920290317
	data : 0.1140927791595459
	model : 0.06488356590270997
			 train-loss:  2.183104970572907 	 ± 0.18695327829543337
	data : 0.11413130760192872
	model : 0.06490635871887207
			 train-loss:  2.183405152859132 	 ± 0.18641807700767182
	data : 0.11424465179443359
	model : 0.0649306297302246
			 train-loss:  2.184706771519126 	 ± 0.18659034070417763
	data : 0.11447062492370605
	model : 0.06495981216430664
			 train-loss:  2.184138615203626 	 ± 0.18616629371868856
	data : 0.11424880027770996
	model : 0.06499705314636231
			 train-loss:  2.1836455047848715 	 ± 0.18571275536332138
	data : 0.11441359519958497
	model : 0.06490755081176758
			 train-loss:  2.1828001023766523 	 ± 0.18547599912854582
	data : 0.11416292190551758
	model : 0.0648737907409668
			 train-loss:  2.183484980747813 	 ± 0.1851348402774846
	data : 0.11411447525024414
	model : 0.0648266315460205
			 train-loss:  2.1831335845078237 	 ± 0.18464247479109386
	data : 0.11410026550292969
	model : 0.06487922668457032
			 train-loss:  2.1827259140856126 	 ± 0.18417487435321855
	data : 0.11434826850891114
	model : 0.06486520767211915
			 train-loss:  2.1811088177195765 	 ± 0.18484201263241623
	data : 0.11428823471069335
	model : 0.06498746871948242
			 train-loss:  2.1846340548160463 	 ± 0.1899815635885621
	data : 0.11438226699829102
	model : 0.06503715515136718
			 train-loss:  2.1840231032729838 	 ± 0.18960106936685167
	data : 0.11441640853881836
	model : 0.06512608528137206
			 train-loss:  2.1825529672633643 	 ± 0.19004175708061685
	data : 0.11445412635803223
	model : 0.06502900123596192
			 train-loss:  2.183227150099618 	 ± 0.18970656275402417
	data : 0.11410541534423828
	model : 0.06496009826660157
			 train-loss:  2.184757084331729 	 ± 0.19024647609375098
	data : 0.11410212516784668
	model : 0.06489000320434571
			 train-loss:  2.1835190138574374 	 ± 0.19041799657272687
	data : 0.11425371170043945
	model : 0.06488795280456543
			 train-loss:  2.1837990531760654 	 ± 0.18991890846813994
	data : 0.11426753997802734
	model : 0.06487946510314942
			 train-loss:  2.18384867740077 	 ± 0.18938882296823872
	data : 0.11421818733215332
	model : 0.06494693756103516
			 train-loss:  2.1841243816746605 	 ± 0.18889802856754
	data : 0.11441164016723633
	model : 0.06499581336975098
			 train-loss:  2.183662627283381 	 ± 0.1884773291041248
	data : 0.11433472633361816
	model : 0.06500554084777832
			 train-loss:  2.183294251069918 	 ± 0.18802414815378285
	data : 0.11426239013671875
	model : 0.0649491786956787
			 train-loss:  2.1822689315660404 	 ± 0.1880192198509084
	data : 0.11411566734313965
	model : 0.06484670639038086
			 train-loss:  2.182239870014398 	 ± 0.1875080141126501
	data : 0.11417665481567382
	model : 0.06473922729492188
			 train-loss:  2.1815133874480788 	 ± 0.18726002317804613
	data : 0.11418385505676269
	model : 0.06479878425598144
			 train-loss:  2.1816928162369678 	 ± 0.1867719029088438
	data : 0.11426753997802734
	model : 0.06477489471435546
			 train-loss:  2.180248654462437 	 ± 0.18731022885340703
	data : 0.11429810523986816
	model : 0.06487627029418945
			 train-loss:  2.180618579083301 	 ± 0.18687987776841933
	data : 0.1142960548400879
	model : 0.06497731208801269
			 train-loss:  2.181504901754793 	 ± 0.18678059839723823
	data : 0.11424708366394043
	model : 0.06509952545166016
			 train-loss:  2.1810068431653473 	 ± 0.1864142161326507
	data : 0.11429057121276856
	model : 0.06502790451049804
			 train-loss:  2.1795759825182213 	 ± 0.18696876813742727
	data : 0.11427545547485352
	model : 0.0650256633758545
			 train-loss:  2.181473508477211 	 ± 0.18831613418027077
	data : 0.1141702651977539
	model : 0.06498169898986816
			 train-loss:  2.1808999227118617 	 ± 0.1879957142430229
	data : 0.11415252685546876
	model : 0.06498942375183106
			 train-loss:  2.1805308098645555 	 ± 0.18758066654590463
	data : 0.11425132751464843
	model : 0.0649658203125
			 train-loss:  2.180186093159211 	 ± 0.18716066836456816
	data : 0.11434078216552734
	model : 0.06500153541564942
			 train-loss:  2.1797478685573655 	 ± 0.18678287865999132
	data : 0.11438417434692383
	model : 0.06503429412841796
			 train-loss:  2.180085776421019 	 ± 0.18636825836196708
	data : 0.11444668769836426
	model : 0.06505804061889649
			 train-loss:  2.180097424622738 	 ± 0.18589710760783085
	data : 0.11459274291992187
	model : 0.0650167465209961
			 train-loss:  2.181086122091092 	 ± 0.18595060406059824
	data : 0.11470241546630859
	model : 0.06495103836059571
			 train-loss:  2.1816348469257356 	 ± 0.18564659357132196
	data : 0.1143280029296875
	model : 0.06485476493835449
			 train-loss:  2.182467750055873 	 ± 0.18555844704767455
	data : 0.11435647010803222
	model : 0.06477723121643067
			 train-loss:  2.1822088437505287 	 ± 0.18513496607395724
	data : 0.1142812728881836
	model : 0.06475963592529296
			 train-loss:  2.1831219278532883 	 ± 0.18513380425514137
	data : 0.11428227424621581
	model : 0.06480283737182617
			 train-loss:  2.1830032023729062 	 ± 0.18468723435404094
	data : 0.11414051055908203
	model : 0.06494526863098145
			 train-loss:  2.1839999326845496 	 ± 0.18478543094244504
	data : 0.1148076057434082
	model : 0.06505355834960938
			 train-loss:  2.1838063209959606 	 ± 0.1843572195603923
	data : 0.11472806930541993
	model : 0.06509962081909179
			 train-loss:  2.183013269866722 	 ± 0.184263270280087
	data : 0.11476011276245117
	model : 0.06508326530456543
			 train-loss:  2.1842321925438366 	 ± 0.18465446739909158
	data : 0.11481008529663086
	model : 0.0650296688079834
			 train-loss:  2.1840679999173545 	 ± 0.18422740025366158
	data : 0.11449480056762695
	model : 0.06485676765441895
			 train-loss:  2.1839784667605446 	 ± 0.1837927980128645
	data : 0.11407661437988281
	model : 0.06477165222167969
			 train-loss:  2.1848721153928206 	 ± 0.1838135099821908
	data : 0.11412620544433594
	model : 0.06479010581970215
			 train-loss:  2.184790662999423 	 ± 0.1833832919628491
	data : 0.11416106224060059
	model : 0.06477947235107422
			 train-loss:  2.1860675039425703 	 ± 0.18389446850647145
	data : 0.11416134834289551
	model : 0.06479582786560059
			 train-loss:  2.185772851248768 	 ± 0.18351469714221744
	data : 0.11443743705749512
	model : 0.06487774848937988
			 train-loss:  2.1842261358749036 	 ± 0.1844802486719222
	data : 0.1144803524017334
	model : 0.06494808197021484
			 train-loss:  2.1839293583675667 	 ± 0.18410415169464656
	data : 0.11441421508789062
	model : 0.06491856575012207
			 train-loss:  2.182904639002365 	 ± 0.18429583368607158
	data : 0.1142723560333252
	model : 0.06485452651977539
			 train-loss:  2.182020754989134 	 ± 0.18433307686690026
	data : 0.11423630714416504
	model : 0.06488051414489746
			 train-loss:  2.1821623066244604 	 ± 0.18392361854639314
	data : 0.11421923637390137
	model : 0.06483678817749024
			 train-loss:  2.1824833794073624 	 ± 0.18356663777010415
	data : 0.11421875953674317
	model : 0.06481084823608399
			 train-loss:  2.184327858066127 	 ± 0.18518287997010874
	data : 0.11428728103637695
	model : 0.06527113914489746
			 train-loss:  2.185557846550469 	 ± 0.18566790798675947
	data : 0.11402978897094726
	model : 0.06529021263122559
			 train-loss:  2.185589626765572 	 ± 0.18525174964702537
	data : 0.11414036750793458
	model : 0.06523499488830567
			 train-loss:  2.183545439371041 	 ± 0.1873415464292109
	data : 0.11424474716186524
	model : 0.06518588066101075
			 train-loss:  2.1837922498914932 	 ± 0.18696126350978992
	data : 0.11422972679138184
	model : 0.06497249603271485
			 train-loss:  2.184777919170076 	 ± 0.18713216037150934
	data : 0.11419906616210937
	model : 0.06423640251159668
			 train-loss:  2.185053531293827 	 ± 0.18676548553594421
	data : 0.11453533172607422
	model : 0.06410927772521972
			 train-loss:  2.18627760598534 	 ± 0.18726581557853209
	data : 0.11461911201477051
	model : 0.0639369010925293
			 train-loss:  2.186725873613982 	 ± 0.1869790453336206
	data : 0.11459026336669922
	model : 0.06382460594177246
			 train-loss:  2.18736269266709 	 ± 0.18682084105796087
	data : 0.11471819877624512
	model : 0.06383781433105469
			 train-loss:  2.1865440761888184 	 ± 0.1868289758174147
	data : 0.11488237380981445
	model : 0.06392054557800293
			 train-loss:  2.1863522082567215 	 ± 0.1864486985915662
	data : 0.11502742767333984
	model : 0.06387009620666503
			 train-loss:  2.1848899622331874 	 ± 0.18737655455018673
	data : 0.1149266242980957
	model : 0.06383018493652344
			 train-loss:  2.1856917285511637 	 ± 0.18737585165885348
	data : 0.11486382484436035
	model : 0.06382365226745605
			 train-loss:  2.18478430687113 	 ± 0.18749129490507516
	data : 0.1148439884185791
	model : 0.06381492614746094
			 train-loss:  2.1854129787218772 	 ± 0.18734169580110493
	data : 0.11490821838378906
	model : 0.06382088661193848
			 train-loss:  2.185756924786145 	 ± 0.18702069749863634
	data : 0.11495447158813477
	model : 0.06385235786437989
			 train-loss:  2.18487373720698 	 ± 0.1871220056703787
	data : 0.1150475025177002
	model : 0.06387209892272949
			 train-loss:  2.185070785019687 	 ± 0.1867548693911941
	data : 0.11509137153625489
	model : 0.06381368637084961
			 train-loss:  2.1856664031744004 	 ± 0.18659272948105637
	data : 0.11492156982421875
	model : 0.06372556686401368
			 train-loss:  2.186633790677019 	 ± 0.1868073331293387
	data : 0.11468496322631835
	model : 0.06370587348937988
			 train-loss:  2.1863451703520846 	 ± 0.18647480533080013
	data : 0.11466174125671387
	model : 0.0637291431427002
			 train-loss:  2.185445819372012 	 ± 0.18661589495921044
	data : 0.11458549499511719
	model : 0.06378569602966308
			 train-loss:  2.1854676951150425 	 ± 0.18623340492877066
	data : 0.11460304260253906
	model : 0.0638498306274414
			 train-loss:  2.1897994299324193 	 ± 0.19778704812036318
	data : 0.11487789154052734
	model : 0.06389198303222657
			 train-loss:  2.1884770122000843 	 ± 0.19846699067642473
	data : 0.11486129760742188
	model : 0.06386303901672363
			 train-loss:  2.188286157754751 	 ± 0.1980874473888049
	data : 0.11499009132385254
	model : 0.06382966041564941
			 train-loss:  2.188581169612946 	 ± 0.19774203752973749
	data : 0.11503968238830567
	model : 0.06378674507141113
			 train-loss:  2.1896183605653694 	 ± 0.19801936104180026
	data : 0.11515293121337891
	model : 0.06378655433654785
			 train-loss:  2.1889478397369384 	 ± 0.19790596409702205
	data : 0.11514592170715332
	model : 0.06382651329040527
			 train-loss:  2.1891397954933196 	 ± 0.19753465383931118
	data : 0.11529121398925782
	model : 0.06388883590698242
			 train-loss:  2.188713210915762 	 ± 0.19725814054846547
	data : 0.1152188777923584
	model : 0.06388692855834961
			 train-loss:  2.1889716862689834 	 ± 0.19691067132831752
	data : 0.1150700569152832
	model : 0.06382894515991211
			 train-loss:  2.1880130767822266 	 ± 0.1971132910571661
	data : 0.11500186920166015
	model : 0.06377120018005371
			 train-loss:  2.187440414989696 	 ± 0.19693800904516087
	data : 0.11484651565551758
	model : 0.06379146575927734
			 train-loss:  2.186774988658726 	 ± 0.19684000902982407
	data : 0.1146697998046875
	model : 0.05536952018737793
#epoch  58    val-loss:  2.389674450221815  train-loss:  2.186774988658726  lr:  7.62939453125e-08
			 train-loss:  2.4733448028564453 	 ± 0.0
	data : 4.9535300731658936
	model : 0.09005880355834961
			 train-loss:  2.6063555479049683 	 ± 0.13301074504852295
	data : 2.785426378250122
	model : 0.07728862762451172
			 train-loss:  2.436853806177775 	 ± 0.26316582813126216
	data : 1.8949069182078044
	model : 0.07305796941121419
			 train-loss:  2.2999438047409058 	 ± 0.32890003846509414
	data : 1.4497633576393127
	model : 0.07101607322692871
			 train-loss:  2.303717517852783 	 ± 0.2942739402061884
	data : 1.1826692581176759
	model : 0.06973609924316407
			 train-loss:  2.3143612146377563 	 ± 0.2696863634730444
	data : 0.2148585319519043
	model : 0.06466159820556641
			 train-loss:  2.292996713093349 	 ± 0.25510641162408276
	data : 0.11430649757385254
	model : 0.06470108032226562
			 train-loss:  2.3167880475521088 	 ± 0.24679255309379064
	data : 0.11441802978515625
	model : 0.0647082805633545
			 train-loss:  2.2794543637169733 	 ± 0.25551829645446916
	data : 0.11431856155395508
	model : 0.06465387344360352
			 train-loss:  2.2797926664352417 	 ± 0.24240806479830096
	data : 0.11423764228820801
	model : 0.06468687057495118
			 train-loss:  2.268280441110784 	 ± 0.23397652277406952
	data : 0.11420392990112305
	model : 0.06474347114562988
			 train-loss:  2.2405635515848794 	 ± 0.24214337568933741
	data : 0.11406230926513672
	model : 0.06484971046447754
			 train-loss:  2.2317179349752574 	 ± 0.23465312898676333
	data : 0.11397752761840821
	model : 0.06489348411560059
			 train-loss:  2.2547259671347484 	 ± 0.24085449656879626
	data : 0.11400809288024902
	model : 0.06490693092346192
			 train-loss:  2.267781162261963 	 ± 0.2377596046996129
	data : 0.11404356956481934
	model : 0.06491684913635254
			 train-loss:  2.243720956146717 	 ± 0.2483544453998269
	data : 0.11391158103942871
	model : 0.0648613452911377
			 train-loss:  2.2507168896058025 	 ± 0.2425588355440053
	data : 0.11400136947631836
	model : 0.06473751068115234
			 train-loss:  2.2517037987709045 	 ± 0.23575993542822024
	data : 0.11398134231567383
	model : 0.06474733352661133
			 train-loss:  2.2408982515335083 	 ± 0.23400644591124073
	data : 0.11400284767150878
	model : 0.06481394767761231
			 train-loss:  2.255035787820816 	 ± 0.2362595915615572
	data : 0.11406679153442383
	model : 0.0648036003112793
			 train-loss:  2.2330314091273715 	 ± 0.2506878934532103
	data : 0.11412558555603028
	model : 0.06488847732543945
			 train-loss:  2.2308141751722856 	 ± 0.24513484867854074
	data : 0.11406970024108887
	model : 0.06499595642089843
			 train-loss:  2.2172934646191806 	 ± 0.24799243794746376
	data : 0.11401829719543458
	model : 0.06497817039489746
			 train-loss:  2.2150887697935104 	 ± 0.24300109982427265
	data : 0.11389541625976562
	model : 0.06492295265197753
			 train-loss:  2.2033177280426024 	 ± 0.24497536877982787
	data : 0.11371850967407227
	model : 0.06496763229370117
			 train-loss:  2.2136087784400353 	 ± 0.2456672232923058
	data : 0.11373906135559082
	model : 0.06491250991821289
			 train-loss:  2.202013452847799 	 ± 0.2482193627857862
	data : 0.11375408172607422
	model : 0.06486721038818359
			 train-loss:  2.1925546654633115 	 ± 0.2486524619760853
	data : 0.11389179229736328
	model : 0.06489176750183105
			 train-loss:  2.1877994167393653 	 ± 0.24562001580349127
	data : 0.11405463218688965
	model : 0.06488509178161621
			 train-loss:  2.185149276256561 	 ± 0.24191298680305764
	data : 0.1141624927520752
	model : 0.06483979225158691
			 train-loss:  2.1940474317919825 	 ± 0.24291851583610904
	data : 0.1141517162322998
	model : 0.0647970199584961
			 train-loss:  2.188740346580744 	 ± 0.2409117705216728
	data : 0.11410999298095703
	model : 0.0648158073425293
			 train-loss:  2.196371912956238 	 ± 0.2411295193315548
	data : 0.1140897274017334
	model : 0.06480956077575684
			 train-loss:  2.211438470027026 	 ± 0.25283271436212823
	data : 0.11408605575561523
	model : 0.0648421287536621
			 train-loss:  2.207766366004944 	 ± 0.25011285129590327
	data : 0.1140787124633789
	model : 0.06489367485046386
			 train-loss:  2.2197917799154916 	 ± 0.2566712485448734
	data : 0.11415348052978516
	model : 0.0649341106414795
			 train-loss:  2.2154732040456824 	 ± 0.25450145243533434
	data : 0.11419138908386231
	model : 0.06493330001831055
			 train-loss:  2.2060845274674263 	 ± 0.2575421086841191
	data : 0.11414246559143067
	model : 0.06492376327514648
			 train-loss:  2.203772239195995 	 ± 0.25461813588376303
	data : 0.11415648460388184
	model : 0.06487388610839843
			 train-loss:  2.204135024547577 	 ± 0.25142547237330776
	data : 0.11416583061218262
	model : 0.06483945846557618
			 train-loss:  2.2082905769348145 	 ± 0.24972722953304627
	data : 0.1141395092010498
	model : 0.06486096382141113
			 train-loss:  2.2080327783312117 	 ± 0.2467418981304609
	data : 0.1141268253326416
	model : 0.06485538482666016
			 train-loss:  2.2095328153565874 	 ± 0.24404962328567886
	data : 0.11414198875427246
	model : 0.0649001121520996
			 train-loss:  2.211081466891549 	 ± 0.2414740257693882
	data : 0.11417369842529297
	model : 0.06496176719665528
			 train-loss:  2.2122156778971354 	 ± 0.23889440558327532
	data : 0.11411147117614746
	model : 0.06501197814941406
			 train-loss:  2.218160489331121 	 ± 0.23962514142948704
	data : 0.11410379409790039
	model : 0.06500978469848633
			 train-loss:  2.2143245707166956 	 ± 0.23848555163290253
	data : 0.1140021800994873
	model : 0.0649609088897705
			 train-loss:  2.216449479262034 	 ± 0.23643745715574183
	data : 0.11393837928771973
	model : 0.06491289138793946
			 train-loss:  2.218916031779075 	 ± 0.2346355186451549
	data : 0.11379895210266114
	model : 0.0648946762084961
			 train-loss:  2.224464864730835 	 ± 0.23550252203932645
	data : 0.11380000114440918
	model : 0.06487569808959961
			 train-loss:  2.219507177670797 	 ± 0.23580265427236893
	data : 0.11384930610656738
	model : 0.0648923397064209
			 train-loss:  2.2165796825518975 	 ± 0.2344582841212655
	data : 0.1140284538269043
	model : 0.06525201797485351
			 train-loss:  2.2238403333807892 	 ± 0.2380646778577325
	data : 0.11401453018188476
	model : 0.06524882316589356
			 train-loss:  2.2164927191204495 	 ± 0.2418400127183897
	data : 0.11410508155822754
	model : 0.0652696132659912
			 train-loss:  2.2186303615570067 	 ± 0.24014569118433782
	data : 0.11421542167663574
	model : 0.06521239280700683
			 train-loss:  2.216510002102171 	 ± 0.23851081366930052
	data : 0.11404228210449219
	model : 0.06516036987304688
			 train-loss:  2.2115843275137115 	 ± 0.2392656905194602
	data : 0.1138498306274414
	model : 0.06484708786010743
			 train-loss:  2.2210429886291765 	 ± 0.24771074331620144
	data : 0.11389298439025879
	model : 0.06484446525573731
			 train-loss:  2.2181083772142056 	 ± 0.24661730421135944
	data : 0.11381678581237793
	model : 0.06486725807189941
			 train-loss:  2.2196859776973725 	 ± 0.24485356249124712
	data : 0.11384339332580566
	model : 0.06490039825439453
			 train-loss:  2.222166923225903 	 ± 0.24359747959267636
	data : 0.11398792266845703
	model : 0.06494622230529785
			 train-loss:  2.216445932465215 	 ± 0.24572169921188775
	data : 0.11408352851867676
	model : 0.06494131088256835
			 train-loss:  2.212019786002144 	 ± 0.2462425269892048
	data : 0.11401920318603516
	model : 0.06493158340454101
			 train-loss:  2.213089259341359 	 ± 0.24445860992367383
	data : 0.11409187316894531
	model : 0.06486029624938965
			 train-loss:  2.2102582913178663 	 ± 0.24362583500758558
	data : 0.11396250724792481
	model : 0.06483697891235352
			 train-loss:  2.205669776959853 	 ± 0.24458698139935134
	data : 0.1138956069946289
	model : 0.06485328674316407
			 train-loss:  2.2061422351580946 	 ± 0.2427851854847382
	data : 0.11387133598327637
	model : 0.06488223075866699
			 train-loss:  2.2048042714595795 	 ± 0.24124210473441848
	data : 0.11405563354492188
	model : 0.06493306159973145
			 train-loss:  2.204538046449855 	 ± 0.23949765541865586
	data : 0.11410555839538575
	model : 0.06499924659729003
			 train-loss:  2.199869896684374 	 ± 0.2409218395783304
	data : 0.11415038108825684
	model : 0.06503119468688964
			 train-loss:  2.202612149883324 	 ± 0.24031691105481898
	data : 0.11411585807800292
	model : 0.06494903564453125
			 train-loss:  2.1992256111568875 	 ± 0.2403422088739477
	data : 0.11418118476867675
	model : 0.06486048698425292
			 train-loss:  2.1967033294782246 	 ± 0.23964795362398128
	data : 0.11404423713684082
	model : 0.06486248970031738
			 train-loss:  2.199503083486815 	 ± 0.23922220760237445
	data : 0.1139146327972412
	model : 0.06482858657836914
			 train-loss:  2.1994307041168213 	 ± 0.23762285682267256
	data : 0.11402482986450195
	model : 0.06480073928833008
			 train-loss:  2.19874612281197 	 ± 0.23612881102916145
	data : 0.11406660079956055
	model : 0.06487917900085449
			 train-loss:  2.203364861475957 	 ± 0.2380209857820699
	data : 0.1140657901763916
	model : 0.06489877700805664
			 train-loss:  2.207616998599126 	 ± 0.23941568050261966
	data : 0.11412181854248046
	model : 0.06484460830688477
			 train-loss:  2.2071701454210886 	 ± 0.23792829799282988
	data : 0.11405782699584961
	model : 0.06481008529663086
			 train-loss:  2.2029433369636537 	 ± 0.23940271180079487
	data : 0.11387224197387695
	model : 0.0647953987121582
			 train-loss:  2.2069825419673212 	 ± 0.24064766060084652
	data : 0.11387734413146973
	model : 0.06477122306823731
			 train-loss:  2.207033517884045 	 ± 0.2391762356734428
	data : 0.11384091377258301
	model : 0.06481103897094727
			 train-loss:  2.2046213006398765 	 ± 0.2387324705952966
	data : 0.1137955665588379
	model : 0.0648543357849121
			 train-loss:  2.2041139120147344 	 ± 0.23735220647054708
	data : 0.11399669647216797
	model : 0.06490707397460938
			 train-loss:  2.2055240378660312 	 ± 0.23630557089558513
	data : 0.11406517028808594
	model : 0.06494641304016113
			 train-loss:  2.204700555912284 	 ± 0.23505032901846623
	data : 0.11401200294494629
	model : 0.06492595672607422
			 train-loss:  2.203486634396959 	 ± 0.23396654718898344
	data : 0.11399521827697753
	model : 0.06494255065917968
			 train-loss:  2.2042436491359365 	 ± 0.23274052726293612
	data : 0.11414284706115722
	model : 0.06492338180541993
			 train-loss:  2.2022145678488054 	 ± 0.23221075154747273
	data : 0.11410026550292969
	model : 0.06490726470947265
			 train-loss:  2.206324887275696 	 ± 0.2341502343590073
	data : 0.11423873901367188
	model : 0.06490473747253418
			 train-loss:  2.204860034879747 	 ± 0.2332744444030763
	data : 0.11432185173034667
	model : 0.06489758491516114
			 train-loss:  2.2096890573916226 	 ± 0.23653234104971568
	data : 0.1144528865814209
	model : 0.06486101150512695
			 train-loss:  2.209474112397881 	 ± 0.23526625840769855
	data : 0.1143028736114502
	model : 0.06480989456176758
			 train-loss:  2.210766924188492 	 ± 0.23434337412277234
	data : 0.11407876014709473
	model : 0.06473011970520019
			 train-loss:  2.210697593187031 	 ± 0.23310769415755186
	data : 0.11393985748291016
	model : 0.0647697925567627
			 train-loss:  2.2083279515306153 	 ± 0.2330377821784792
	data : 0.11393589973449707
	model : 0.06479988098144532
			 train-loss:  2.2070939565442274 	 ± 0.2321485071531899
	data : 0.11391410827636719
	model : 0.06480622291564941
			 train-loss:  2.2074233901743985 	 ± 0.23098382756120367
	data : 0.11396775245666504
	model : 0.06484751701354981
			 train-loss:  2.2069235257428104 	 ± 0.22986755051476887
	data : 0.11418137550354004
	model : 0.06494703292846679
			 train-loss:  2.206967532634735 	 ± 0.22871574409277198
	data : 0.11416902542114257
	model : 0.06486654281616211
			 train-loss:  2.2074615152755586 	 ± 0.22763427655142882
	data : 0.11410274505615234
	model : 0.06479263305664062
			 train-loss:  2.207042796939027 	 ± 0.22655475788313367
	data : 0.11396098136901855
	model : 0.06475505828857422
			 train-loss:  2.204642662724245 	 ± 0.22675167712350425
	data : 0.11394848823547363
	model : 0.06477761268615723
			 train-loss:  2.20620912198837 	 ± 0.22621820484414987
	data : 0.11401557922363281
	model : 0.06477646827697754
			 train-loss:  2.204094852719988 	 ± 0.22616850427588242
	data : 0.1140601634979248
	model : 0.06484990119934082
			 train-loss:  2.20202840274235 	 ± 0.22609289485256415
	data : 0.11407389640808105
	model : 0.06491456031799317
			 train-loss:  2.200714862235239 	 ± 0.2254399033234739
	data : 0.11419882774353027
	model : 0.06496939659118653
			 train-loss:  2.1973899084108846 	 ± 0.2270142821655171
	data : 0.11403079032897949
	model : 0.06489372253417969
			 train-loss:  2.1944964949144135 	 ± 0.2279623666271505
	data : 0.11400032043457031
	model : 0.06489644050598145
			 train-loss:  2.195634272965518 	 ± 0.22723450335288117
	data : 0.11395039558410644
	model : 0.06487746238708496
			 train-loss:  2.1973754854889602 	 ± 0.22694456364769128
	data : 0.11405549049377442
	model : 0.06488709449768067
			 train-loss:  2.196853081030505 	 ± 0.225996176764773
	data : 0.1140775203704834
	model : 0.06487693786621093
			 train-loss:  2.195920787026397 	 ± 0.225210200660265
	data : 0.11420211791992188
	model : 0.06494250297546386
			 train-loss:  2.200065262484969 	 ± 0.2285075269842459
	data : 0.11415905952453613
	model : 0.06483225822448731
			 train-loss:  2.2049714679303376 	 ± 0.2334645958696779
	data : 0.11414756774902343
	model : 0.06476621627807617
			 train-loss:  2.2039693645362197 	 ± 0.23270437241018455
	data : 0.1139291763305664
	model : 0.06470398902893067
			 train-loss:  2.202644603884118 	 ± 0.23214666061099232
	data : 0.11379466056823731
	model : 0.0647122859954834
			 train-loss:  2.204824335494284 	 ± 0.2323601804427478
	data : 0.11386919021606445
	model : 0.06472702026367187
			 train-loss:  2.2056767850362955 	 ± 0.23156703679348212
	data : 0.11385712623596192
	model : 0.06483263969421386
			 train-loss:  2.2035490810871123 	 ± 0.2317653113077649
	data : 0.11391115188598633
	model : 0.06491627693176269
			 train-loss:  2.2018200346261017 	 ± 0.23158148627875513
	data : 0.11409487724304199
	model : 0.0649709701538086
			 train-loss:  2.201680214678655 	 ± 0.230635557214366
	data : 0.11409626007080079
	model : 0.06495981216430664
			 train-loss:  2.202227836701928 	 ± 0.22977572837775254
	data : 0.11392087936401367
	model : 0.0649186611175537
			 train-loss:  2.201706349849701 	 ± 0.22892040906853037
	data : 0.1138829231262207
	model : 0.06489696502685546
			 train-loss:  2.200257932662964 	 ± 0.22857265450968983
	data : 0.11386590003967285
	model : 0.06490902900695801
			 train-loss:  2.2066231984940785 	 ± 0.23852754145390645
	data : 0.1139444351196289
	model : 0.06491475105285645
			 train-loss:  2.206821257673849 	 ± 0.237597002312667
	data : 0.11409082412719726
	model : 0.06494898796081543
			 train-loss:  2.2071821577847004 	 ± 0.23670201360749146
	data : 0.1142183780670166
	model : 0.06498355865478515
			 train-loss:  2.204830295355745 	 ± 0.23727941284662665
	data : 0.11423425674438477
	model : 0.06497044563293457
			 train-loss:  2.205783801812392 	 ± 0.23661300612160963
	data : 0.11422924995422364
	model : 0.06485972404479981
			 train-loss:  2.2043661889229114 	 ± 0.23626170704919808
	data : 0.11400179862976074
	model : 0.06479926109313965
			 train-loss:  2.202614811333743 	 ± 0.23621714137053854
	data : 0.11387767791748046
	model : 0.06474447250366211
			 train-loss:  2.2035612289170574 	 ± 0.23557850755626542
	data : 0.11387434005737304
	model : 0.06472373008728027
			 train-loss:  2.2015715881959714 	 ± 0.23581683125260774
	data : 0.11387772560119629
	model : 0.0647705078125
			 train-loss:  2.2008212592866685 	 ± 0.2351023101290433
	data : 0.11393837928771973
	model : 0.06482892036437989
			 train-loss:  2.2042127628536785 	 ± 0.23752786451885488
	data : 0.11406483650207519
	model : 0.06484556198120117
			 train-loss:  2.2054869597845705 	 ± 0.237125435028488
	data : 0.11409101486206055
	model : 0.06478390693664551
			 train-loss:  2.205229354077491 	 ± 0.2362839619148845
	data : 0.11395645141601562
	model : 0.06469283103942872
			 train-loss:  2.2028072291998555 	 ± 0.23714564777197247
	data : 0.11392898559570312
	model : 0.06471772193908691
			 train-loss:  2.208049702644348 	 ± 0.24424694104024927
	data : 0.11399335861206054
	model : 0.0647036075592041
			 train-loss:  2.209186481245866 	 ± 0.24375067007402984
	data : 0.11394715309143066
	model : 0.06472344398498535
			 train-loss:  2.209086298942566 	 ± 0.24289378978454604
	data : 0.113922119140625
	model : 0.06479172706604004
			 train-loss:  2.20848411613411 	 ± 0.24214936828414843
	data : 0.11410894393920898
	model : 0.0648892879486084
			 train-loss:  2.2103601578209133 	 ± 0.24234771105242733
	data : 0.11414694786071777
	model : 0.06484622955322265
			 train-loss:  2.210364218415885 	 ± 0.24151058838524236
	data : 0.1138986587524414
	model : 0.06482324600219727
			 train-loss:  2.211273247248506 	 ± 0.24093086176879297
	data : 0.11378798484802247
	model : 0.06482496261596679
			 train-loss:  2.2100204415872793 	 ± 0.24058667500583852
	data : 0.11379733085632324
	model : 0.06483597755432129
			 train-loss:  2.2090315093865267 	 ± 0.24007211004153586
	data : 0.11376438140869141
	model : 0.0648465633392334
			 train-loss:  2.2111183076896923 	 ± 0.24060820460300616
	data : 0.1137925148010254
	model : 0.06485571861267089
			 train-loss:  2.213577318191528 	 ± 0.24167606949393988
	data : 0.11398701667785645
	model : 0.0648918628692627
			 train-loss:  2.2134784025861727 	 ± 0.2408775347554953
	data : 0.11413726806640626
	model : 0.06484289169311523
			 train-loss:  2.2134959948690316 	 ± 0.24008396424084005
	data : 0.1139744758605957
	model : 0.06479148864746094
			 train-loss:  2.2142440197514555 	 ± 0.2394757317563
	data : 0.11397852897644042
	model : 0.06478309631347656
			 train-loss:  2.211875013716809 	 ± 0.24048887174221792
	data : 0.11399569511413574
	model : 0.06480045318603515
			 train-loss:  2.2125904167852095 	 ± 0.23987618993985108
	data : 0.11398801803588868
	model : 0.06483306884765624
			 train-loss:  2.2115268852466192 	 ± 0.2394724555837307
	data : 0.11402339935302734
	model : 0.06493382453918457
			 train-loss:  2.212302515461187 	 ± 0.23890508377233904
	data : 0.11423764228820801
	model : 0.06497230529785156
			 train-loss:  2.2124021875707407 	 ± 0.23815112964573135
	data : 0.11429662704467773
	model : 0.06495485305786133
			 train-loss:  2.212462638908962 	 ± 0.2374022615450072
	data : 0.11418709754943848
	model : 0.06490535736083984
			 train-loss:  2.209644342958927 	 ± 0.23931253365290445
	data : 0.11403765678405761
	model : 0.0648496150970459
			 train-loss:  2.208055471781618 	 ± 0.23941322707977988
	data : 0.1139636516571045
	model : 0.06484975814819335
			 train-loss:  2.2105605315279075 	 ± 0.24078040082995658
	data : 0.1139974594116211
	model : 0.06488776206970215
			 train-loss:  2.2096268494436346 	 ± 0.24033466421527203
	data : 0.11405382156372071
	model : 0.06498332023620605
			 train-loss:  2.208085923660092 	 ± 0.24040712727246114
	data : 0.1141824722290039
	model : 0.0650115966796875
			 train-loss:  2.2062502528681898 	 ± 0.24082761317905763
	data : 0.11431756019592285
	model : 0.06506590843200684
			 train-loss:  2.2057442550199577 	 ± 0.24018909149646656
	data : 0.11452775001525879
	model : 0.06500334739685058
			 train-loss:  2.208740073049854 	 ± 0.2425596492505802
	data : 0.11439337730407714
	model : 0.0649381160736084
			 train-loss:  2.209290835119429 	 ± 0.24194138052112024
	data : 0.11420817375183105
	model : 0.064874267578125
			 train-loss:  2.210751333180264 	 ± 0.24196615485391695
	data : 0.11406569480895996
	model : 0.06484894752502442
			 train-loss:  2.21058731359594 	 ± 0.24126286253625798
	data : 0.11403374671936035
	model : 0.06483759880065917
			 train-loss:  2.2101233131007145 	 ± 0.24063244363201536
	data : 0.1138765811920166
	model : 0.06485514640808106
			 train-loss:  2.210449404494707 	 ± 0.2399698008292819
	data : 0.11395306587219238
	model : 0.06488332748413086
			 train-loss:  2.211931715121848 	 ± 0.24006367326669664
	data : 0.11405386924743652
	model : 0.06485228538513184
			 train-loss:  2.2111111728624366 	 ± 0.23961601817607964
	data : 0.11402502059936523
	model : 0.06480655670166016
			 train-loss:  2.210005396434239 	 ± 0.2393752340923605
	data : 0.11397581100463867
	model : 0.06481471061706542
			 train-loss:  2.2112689993598242 	 ± 0.2392788181611433
	data : 0.11419377326965333
	model : 0.06481447219848632
			 train-loss:  2.2095042877951583 	 ± 0.2397477472599923
	data : 0.11431684494018554
	model : 0.06485214233398437
			 train-loss:  2.208151199174731 	 ± 0.23975013435962506
	data : 0.11470980644226074
	model : 0.06487812995910644
			 train-loss:  2.2094850959724552 	 ± 0.23974094530693774
	data : 0.11486105918884278
	model : 0.0649810791015625
			 train-loss:  2.208230400747723 	 ± 0.23966268765808388
	data : 0.11486048698425293
	model : 0.06499338150024414
			 train-loss:  2.209207456414871 	 ± 0.23935893656275967
	data : 0.11459083557128906
	model : 0.06499028205871582
			 train-loss:  2.2083526044101505 	 ± 0.23897735285511845
	data : 0.11437335014343261
	model : 0.06488685607910157
			 train-loss:  2.2077063738974068 	 ± 0.23848292065263738
	data : 0.11390228271484375
	model : 0.06490573883056641
			 train-loss:  2.2072676672883658 	 ± 0.2379080197748455
	data : 0.1138547420501709
	model : 0.06490774154663086
			 train-loss:  2.2076548093074075 	 ± 0.23732226282378407
	data : 0.11432347297668458
	model : 0.06493396759033203
			 train-loss:  2.2102704964658266 	 ± 0.23934240603065532
	data : 0.11439976692199708
	model : 0.06494083404541015
			 train-loss:  2.2100662069524675 	 ± 0.23871785464357126
	data : 0.1145261287689209
	model : 0.06502442359924317
			 train-loss:  2.2085892691257154 	 ± 0.23893724603906263
	data : 0.11464071273803711
	model : 0.06503329277038575
			 train-loss:  2.207393932594824 	 ± 0.23886724064445328
	data : 0.11459355354309082
	model : 0.06500611305236817
			 train-loss:  2.2080453094683197 	 ± 0.2384060545978983
	data : 0.11402015686035157
	model : 0.06491942405700683
			 train-loss:  2.2072163863955994 	 ± 0.23805549808935417
	data : 0.11424908638000489
	model : 0.0648585319519043
			 train-loss:  2.2076175274948278 	 ± 0.23749946598012972
	data : 0.114265775680542
	model : 0.06483578681945801
			 train-loss:  2.2065779569853157 	 ± 0.23732094936297599
	data : 0.114192533493042
	model : 0.06482167243957519
			 train-loss:  2.2054225132637417 	 ± 0.2372521486939131
	data : 0.11422338485717773
	model : 0.06481428146362304
			 train-loss:  2.203879759250543 	 ± 0.2376166242376077
	data : 0.1144172191619873
	model : 0.06487665176391602
			 train-loss:  2.2050069491474 	 ± 0.23753178534043415
	data : 0.11414480209350586
	model : 0.0649557113647461
			 train-loss:  2.204410916657617 	 ± 0.2370750433656675
	data : 0.11451711654663085
	model : 0.0649454116821289
			 train-loss:  2.203168206744724 	 ± 0.23711800288606924
	data : 0.11455063819885254
	model : 0.06495380401611328
			 train-loss:  2.2026590582114367 	 ± 0.2366299597481866
	data : 0.11459417343139648
	model : 0.06494369506835937
			 train-loss:  2.2029509365558626 	 ± 0.2360735532875236
	data : 0.1144178867340088
	model : 0.06489038467407227
			 train-loss:  2.202472366503815 	 ± 0.2355828116487145
	data : 0.11465544700622558
	model : 0.06490364074707031
			 train-loss:  2.2025325971074623 	 ± 0.23500051382840922
	data : 0.11431655883789063
	model : 0.06496667861938477
			 train-loss:  2.2019478006315936 	 ± 0.23456827861512256
	data : 0.11432275772094727
	model : 0.06494574546813965
			 train-loss:  2.201117153261222 	 ± 0.23429175235730587
	data : 0.11427650451660157
	model : 0.06501107215881348
			 train-loss:  2.202612512867625 	 ± 0.2346934613299532
	data : 0.11435956954956054
	model : 0.06504721641540527
			 train-loss:  2.2029401029197917 	 ± 0.23417010231801222
	data : 0.11406087875366211
	model : 0.06499724388122559
			 train-loss:  2.2051727771759033 	 ± 0.23579144648257874
	data : 0.11395659446716308
	model : 0.06493220329284669
			 train-loss:  2.2048386449997244 	 ± 0.23527307628145122
	data : 0.11379776000976563
	model : 0.0648676872253418
			 train-loss:  2.205587317498677 	 ± 0.23495777856375727
	data : 0.11382741928100586
	model : 0.06481857299804687
			 train-loss:  2.2051859696706138 	 ± 0.23446948999982084
	data : 0.11421961784362793
	model : 0.06482124328613281
			 train-loss:  2.2042144022846673 	 ± 0.2343365535352822
	data : 0.11422910690307617
	model : 0.0648848533630371
			 train-loss:  2.2038015205905124 	 ± 0.2338601360957834
	data : 0.11439480781555175
	model : 0.06494803428649902
			 train-loss:  2.203856399921184 	 ± 0.23331189111583625
	data : 0.11456289291381835
	model : 0.06504974365234376
			 train-loss:  2.203400503809207 	 ± 0.23286120799793988
	data : 0.11447653770446778
	model : 0.06504168510437011
			 train-loss:  2.203339808486229 	 ± 0.23232073585212307
	data : 0.11413969993591308
	model : 0.06502771377563477
			 train-loss:  2.2018712472032616 	 ± 0.2327804420248756
	data : 0.11414709091186523
	model : 0.06497750282287598
			 train-loss:  2.20420900687644 	 ± 0.2347711475444792
	data : 0.11403365135192871
	model : 0.06491785049438477
			 train-loss:  2.203490841279336 	 ± 0.23447085001023976
	data : 0.11411762237548828
	model : 0.06485161781311036
			 train-loss:  2.203525289552941 	 ± 0.23393546886252523
	data : 0.11420917510986328
	model : 0.06484084129333496
			 train-loss:  2.2029742750254546 	 ± 0.23354558883507034
	data : 0.114262056350708
	model : 0.06479148864746094
			 train-loss:  2.202923515803134 	 ± 0.23301782235757362
	data : 0.1143484115600586
	model : 0.06535983085632324
			 train-loss:  2.2020147072302327 	 ± 0.2328846372708959
	data : 0.1139150619506836
	model : 0.0653160572052002
			 train-loss:  2.2028235320018545 	 ± 0.23267419002974765
	data : 0.11384735107421876
	model : 0.06522068977355958
			 train-loss:  2.204841791519097 	 ± 0.2341024482269783
	data : 0.11384086608886719
	model : 0.0650254249572754
			 train-loss:  2.204617313808865 	 ± 0.2336058016000485
	data : 0.11384372711181641
	model : 0.06486515998840332
			 train-loss:  2.2040316246252143 	 ± 0.2332539070024338
	data : 0.11391377449035645
	model : 0.06410379409790039
			 train-loss:  2.203292275315339 	 ± 0.23300481770564685
	data : 0.11442995071411133
	model : 0.06395416259765625
			 train-loss:  2.2018548328625527 	 ± 0.2334998120788419
	data : 0.11455268859863281
	model : 0.06389493942260742
			 train-loss:  2.203475763183494 	 ± 0.2342714767959459
	data : 0.11460661888122559
	model : 0.06385970115661621
			 train-loss:  2.20293587135232 	 ± 0.23390436551848492
	data : 0.11468148231506348
	model : 0.06383066177368164
			 train-loss:  2.2025114308187974 	 ± 0.2334862767000302
	data : 0.1146726131439209
	model : 0.06375384330749512
			 train-loss:  2.206999580921798 	 ± 0.2427632711787019
	data : 0.11483526229858398
	model : 0.06377983093261719
			 train-loss:  2.207745047086298 	 ± 0.24250772602164783
	data : 0.11463913917541504
	model : 0.06373915672302247
			 train-loss:  2.206278159068181 	 ± 0.24302269974749158
	data : 0.1147878646850586
	model : 0.06381211280822754
			 train-loss:  2.207023572921753 	 ± 0.24277300815617092
	data : 0.11472187042236329
	model : 0.06385540962219238
			 train-loss:  2.2067962410086293 	 ± 0.24228317697635512
	data : 0.11482267379760742
	model : 0.06390495300292968
			 train-loss:  2.20643910476427 	 ± 0.24183373347389847
	data : 0.11460599899291993
	model : 0.0638916015625
			 train-loss:  2.206680406041506 	 ± 0.24135373429748078
	data : 0.11460957527160645
	model : 0.06391654014587403
			 train-loss:  2.205877996388838 	 ± 0.24116619453858013
	data : 0.1145235538482666
	model : 0.063881254196167
			 train-loss:  2.204826495051384 	 ± 0.24121162263127743
	data : 0.11457333564758301
	model : 0.06389245986938477
			 train-loss:  2.204329848784134 	 ± 0.24083359662819162
	data : 0.11445393562316894
	model : 0.06394505500793457
			 train-loss:  2.204396216337346 	 ± 0.24033769985353354
	data : 0.11456112861633301
	model : 0.06396045684814453
			 train-loss:  2.202254368444529 	 ± 0.24214599604975698
	data : 0.11468739509582519
	model : 0.06396141052246093
			 train-loss:  2.2022402135075114 	 ± 0.24164938653229295
	data : 0.11459240913391114
	model : 0.06388258934020996
			 train-loss:  2.20118131394289 	 ± 0.2417223011592638
	data : 0.11455154418945312
	model : 0.06384830474853516
			 train-loss:  2.201862158329506 	 ± 0.24146577690166532
	data : 0.11474151611328125
	model : 0.06383862495422363
			 train-loss:  2.2011635443459636 	 ± 0.24122547334588593
	data : 0.11474571228027344
	model : 0.06383266448974609
			 train-loss:  2.199778844752619 	 ± 0.24172027024850778
	data : 0.11473450660705567
	model : 0.06384692192077637
			 train-loss:  2.1988208562016007 	 ± 0.24170567993156425
	data : 0.11477780342102051
	model : 0.06393909454345703
			 train-loss:  2.198459057807922 	 ± 0.2412893341813402
	data : 0.11479701995849609
	model : 0.06394515037536622
			 train-loss:  2.1974476927305124 	 ± 0.24133856564146958
	data : 0.11459865570068359
	model : 0.06393446922302246
			 train-loss:  2.1970218509908706 	 ± 0.24095371280872882
	data : 0.11447653770446778
	model : 0.06390213966369629
			 train-loss:  2.1970050160592725 	 ± 0.24047719671924314
	data : 0.1144749641418457
	model : 0.06384224891662597
			 train-loss:  2.198841779720126 	 ± 0.24177500716885483
	data : 0.11454300880432129
	model : 0.06376609802246094
			 train-loss:  2.197112108211891 	 ± 0.2428699778251878
	data : 0.11457939147949218
	model : 0.06376838684082031
			 train-loss:  2.1965120914392173 	 ± 0.2425844551450103
	data : 0.11452732086181641
	model : 0.05532350540161133
#epoch  59    val-loss:  2.4019998061029533  train-loss:  2.1965120914392173  lr:  3.814697265625e-08
			 train-loss:  2.117556571960449 	 ± 0.0
	data : 5.366840839385986
	model : 0.08360433578491211
			 train-loss:  2.0047020316123962 	 ± 0.11285454034805298
	data : 2.9519808292388916
	model : 0.07413971424102783
			 train-loss:  2.1372350454330444 	 ± 0.20885584587512776
	data : 2.0058650970458984
	model : 0.07088271776835124
			 train-loss:  2.0915963649749756 	 ± 0.19739361876469688
	data : 1.5328218340873718
	model : 0.06936138868331909
			 train-loss:  2.0407252073287965 	 ± 0.20377166462555008
	data : 1.249041700363159
	model : 0.06840672492980956
			 train-loss:  2.0298941930135093 	 ± 0.1875872164506762
	data : 0.19857320785522461
	model : 0.06467275619506836
			 train-loss:  2.0383077689579556 	 ± 0.17489053293888954
	data : 0.11399164199829101
	model : 0.06468944549560547
			 train-loss:  2.0345637500286102 	 ± 0.1638947373831624
	data : 0.11405243873596191
	model : 0.06476359367370606
			 train-loss:  2.0424557526906333 	 ± 0.15612541481785203
	data : 0.11398844718933106
	model : 0.06471762657165528
			 train-loss:  2.0989490032196043 	 ± 0.22508002318020992
	data : 0.11382684707641602
	model : 0.06479382514953613
			 train-loss:  2.091449347409335 	 ± 0.21591182974201933
	data : 0.11357989311218261
	model : 0.06480445861816406
			 train-loss:  2.09503702322642 	 ± 0.20706201317348058
	data : 0.1134732723236084
	model : 0.06485381126403808
			 train-loss:  2.0880505305070143 	 ± 0.20040548437035427
	data : 0.11356968879699707
	model : 0.06515884399414062
			 train-loss:  2.081279465130397 	 ± 0.1946525970519892
	data : 0.11338024139404297
	model : 0.06521453857421874
			 train-loss:  2.0679172595342 	 ± 0.19458503926906207
	data : 0.11352272033691406
	model : 0.06518945693969727
			 train-loss:  2.086912401020527 	 ± 0.20226000572139108
	data : 0.11356759071350098
	model : 0.06510233879089355
			 train-loss:  2.0912541291292976 	 ± 0.19698807408491803
	data : 0.11350374221801758
	model : 0.06507883071899415
			 train-loss:  2.1237843102878995 	 ± 0.2337479892509301
	data : 0.11341280937194824
	model : 0.06480813026428223
			 train-loss:  2.132375848920722 	 ± 0.2304150461048458
	data : 0.11359891891479493
	model : 0.0648313045501709
			 train-loss:  2.1289523780345916 	 ± 0.22507603538177653
	data : 0.11366395950317383
	model : 0.06487245559692383
			 train-loss:  2.129488859857832 	 ± 0.2196648220983153
	data : 0.11372599601745606
	model : 0.06498360633850098
			 train-loss:  2.1484524499286306 	 ± 0.2315411523569876
	data : 0.11371321678161621
	model : 0.06494550704956055
			 train-loss:  2.1419402671896894 	 ± 0.22850244298213343
	data : 0.11360859870910645
	model : 0.06489534378051758
			 train-loss:  2.1482197294632592 	 ± 0.22570940915627713
	data : 0.11361212730407715
	model : 0.06486616134643555
			 train-loss:  2.1555711221694946 	 ± 0.22406244504539272
	data : 0.11367383003234863
	model : 0.0648615837097168
			 train-loss:  2.1443269894673276 	 ± 0.22679025553433518
	data : 0.11370272636413574
	model : 0.06485228538513184
			 train-loss:  2.1391172144148083 	 ± 0.22413065244936814
	data : 0.11383023262023925
	model : 0.06493473052978516
			 train-loss:  2.1496587736266 	 ± 0.2268056856103227
	data : 0.11423020362854004
	model : 0.0650050163269043
			 train-loss:  2.148015967730818 	 ± 0.22303041136443058
	data : 0.11431255340576171
	model : 0.06502838134765625
			 train-loss:  2.142868415514628 	 ± 0.2210269224197388
	data : 0.11424770355224609
	model : 0.06494030952453614
			 train-loss:  2.1431943447359147 	 ± 0.21744007752953198
	data : 0.11415228843688965
	model : 0.06495909690856934
			 train-loss:  2.1402705013751984 	 ± 0.21463386527377384
	data : 0.11410613059997558
	model : 0.06495108604431152
			 train-loss:  2.143444790984645 	 ± 0.21211822410052922
	data : 0.11377410888671875
	model : 0.06500205993652344
			 train-loss:  2.1433411205516144 	 ± 0.2089764067470157
	data : 0.11396327018737792
	model : 0.0649899959564209
			 train-loss:  2.147366973331996 	 ± 0.20730278927483836
	data : 0.1138756275177002
	model : 0.06505165100097657
			 train-loss:  2.160498241583506 	 ± 0.21866817103552652
	data : 0.11396279335021972
	model : 0.06497130393981934
			 train-loss:  2.1639284894273088 	 ± 0.21667267606232396
	data : 0.1139007568359375
	model : 0.06489291191101074
			 train-loss:  2.161464346082587 	 ± 0.21432746906443592
	data : 0.11380739212036133
	model : 0.06483383178710937
			 train-loss:  2.166935144326626 	 ± 0.21423290396862155
	data : 0.11367502212524414
	model : 0.06482768058776855
			 train-loss:  2.1712605118751527 	 ± 0.21325568477070436
	data : 0.11383285522460937
	model : 0.06483654975891114
			 train-loss:  2.1856005133652108 	 ± 0.22933423991888383
	data : 0.11378993988037109
	model : 0.06486177444458008
			 train-loss:  2.1818526642663136 	 ± 0.22785489282001853
	data : 0.11388392448425293
	model : 0.06488261222839356
			 train-loss:  2.182295050731925 	 ± 0.225208081368721
	data : 0.11395158767700195
	model : 0.0648810863494873
			 train-loss:  2.1824567914009094 	 ± 0.22263671647979558
	data : 0.11382436752319336
	model : 0.06484441757202149
			 train-loss:  2.179533412721422 	 ± 0.22100146274252497
	data : 0.11363658905029297
	model : 0.064837646484375
			 train-loss:  2.1844676992167598 	 ± 0.2210780288823144
	data : 0.11373782157897949
	model : 0.06485171318054199
			 train-loss:  2.1877480669224516 	 ± 0.21984218902143957
	data : 0.11369919776916504
	model : 0.06537027359008789
			 train-loss:  2.192752093076706 	 ± 0.22022850377861589
	data : 0.11328191757202148
	model : 0.065350341796875
			 train-loss:  2.1868179866245816 	 ± 0.22181307348476442
	data : 0.11343135833740234
	model : 0.06545028686523438
			 train-loss:  2.1934031677246093 	 ± 0.22436997275319712
	data : 0.11356511116027831
	model : 0.06543054580688476
			 train-loss:  2.194169834548352 	 ± 0.22222551112843575
	data : 0.1134429931640625
	model : 0.06553502082824707
			 train-loss:  2.1927898205243626 	 ± 0.22029890704102345
	data : 0.11311054229736328
	model : 0.06497020721435547
			 train-loss:  2.1913455927147054 	 ± 0.21845910144547123
	data : 0.11359877586364746
	model : 0.06524014472961426
			 train-loss:  2.1884921524259777 	 ± 0.21742154135801794
	data : 0.11358180046081542
	model : 0.0651970386505127
			 train-loss:  2.1877789670770817 	 ± 0.21549965112547945
	data : 0.11353182792663574
	model : 0.06520519256591797
			 train-loss:  2.1920006275177 	 ± 0.21584958899634504
	data : 0.11361784934997558
	model : 0.06509976387023926
			 train-loss:  2.1862758126175192 	 ± 0.21819480889836385
	data : 0.11408057212829589
	model : 0.06515631675720215
			 train-loss:  2.184263591108651 	 ± 0.21683847734629727
	data : 0.11411771774291993
	model : 0.06487860679626464
			 train-loss:  2.1835847426269015 	 ± 0.2150551621478425
	data : 0.11401162147521973
	model : 0.06480197906494141
			 train-loss:  2.188968284924825 	 ± 0.21722771428403428
	data : 0.11387662887573242
	model : 0.06506624221801757
			 train-loss:  2.1868558164502754 	 ± 0.21606031434660575
	data : 0.11360015869140624
	model : 0.06509671211242676
			 train-loss:  2.187607842106973 	 ± 0.21439128035191488
	data : 0.11359152793884278
	model : 0.06512303352355957
			 train-loss:  2.1865813391549245 	 ± 0.21283648587361761
	data : 0.11365809440612792
	model : 0.06515474319458008
			 train-loss:  2.1835780702531338 	 ± 0.21250836074199064
	data : 0.11370482444763183
	model : 0.06524028778076171
			 train-loss:  2.183621413891132 	 ± 0.21086763007693302
	data : 0.11386098861694335
	model : 0.0649658203125
			 train-loss:  2.181915203730265 	 ± 0.20971568339142754
	data : 0.11406874656677246
	model : 0.06486239433288574
			 train-loss:  2.1825961853141216 	 ± 0.2082182666155384
	data : 0.11402473449707032
	model : 0.06484050750732422
			 train-loss:  2.181135594844818 	 ± 0.2070270707475828
	data : 0.11387314796447753
	model : 0.06489086151123047
			 train-loss:  2.184448826140252 	 ± 0.20732948583637897
	data : 0.11389365196228027
	model : 0.06489462852478027
			 train-loss:  2.182354119845799 	 ± 0.20657733350792543
	data : 0.11390409469604493
	model : 0.0649388313293457
			 train-loss:  2.1818152212760817 	 ± 0.20516695284570077
	data : 0.11399221420288086
	model : 0.06501026153564453
			 train-loss:  2.184615519311693 	 ± 0.20509901364674293
	data : 0.11403236389160157
	model : 0.06500635147094727
			 train-loss:  2.18206055523598 	 ± 0.20483986138407848
	data : 0.11423344612121582
	model : 0.06493425369262695
			 train-loss:  2.1773919028204842 	 ± 0.2073245796737366
	data : 0.11422309875488282
	model : 0.06485276222229004
			 train-loss:  2.1769249566396076 	 ± 0.20597694802781077
	data : 0.11426515579223633
	model : 0.06481618881225586
			 train-loss:  2.1753825670794438 	 ± 0.20505287628660332
	data : 0.11432738304138183
	model : 0.06481971740722656
			 train-loss:  2.175164674783682 	 ± 0.20372586856669636
	data : 0.11442012786865234
	model : 0.06483273506164551
			 train-loss:  2.174587592100486 	 ± 0.202479052983602
	data : 0.1143002986907959
	model : 0.06485748291015625
			 train-loss:  2.169633812542203 	 ± 0.20589541644152654
	data : 0.11434316635131836
	model : 0.06495494842529297
			 train-loss:  2.16685788333416 	 ± 0.20608679684713332
	data : 0.11433663368225097
	model : 0.06497054100036621
			 train-loss:  2.1640806050948154 	 ± 0.2063116258853006
	data : 0.11434597969055176
	model : 0.06494865417480469
			 train-loss:  2.163099163916053 	 ± 0.20523993089980508
	data : 0.11419939994812012
	model : 0.06493601799011231
			 train-loss:  2.1675470220037254 	 ± 0.20793787593588448
	data : 0.11433849334716797
	model : 0.06485042572021485
			 train-loss:  2.1639490099180314 	 ± 0.2092795083045082
	data : 0.11425213813781739
	model : 0.06483206748962403
			 train-loss:  2.165168784646427 	 ± 0.20834496026738072
	data : 0.11426744461059571
	model : 0.06486353874206544
			 train-loss:  2.1651376513547675 	 ± 0.20713030937461718
	data : 0.11417732238769532
	model : 0.06486225128173828
			 train-loss:  2.1653656795107086 	 ± 0.20594732156649243
	data : 0.11423277854919434
	model : 0.0649226188659668
			 train-loss:  2.1672332964160224 	 ± 0.2055134398761666
	data : 0.11406803131103516
	model : 0.06496109962463378
			 train-loss:  2.166328213188086 	 ± 0.20453190990357273
	data : 0.11422858238220215
	model : 0.06498022079467773
			 train-loss:  2.1643597880999246 	 ± 0.20423842871121575
	data : 0.11414957046508789
	model : 0.06493668556213379
			 train-loss:  2.165899838720049 	 ± 0.20363792743365333
	data : 0.11415724754333496
	model : 0.06492505073547364
			 train-loss:  2.1651414956735526 	 ± 0.20265733298249097
	data : 0.11403951644897461
	model : 0.06481099128723145
			 train-loss:  2.1642864814368625 	 ± 0.20173159967150703
	data : 0.11401662826538086
	model : 0.06480989456176758
			 train-loss:  2.1676249694316945 	 ± 0.20322213875004563
	data : 0.11398773193359375
	model : 0.06474900245666504
			 train-loss:  2.1672581986377115 	 ± 0.20218099271968698
	data : 0.11391215324401856
	model : 0.0647974967956543
			 train-loss:  2.1687422208487988 	 ± 0.20164466448237153
	data : 0.11387772560119629
	model : 0.06482515335083008
			 train-loss:  2.1657372872854017 	 ± 0.2027516550906205
	data : 0.11397752761840821
	model : 0.06492147445678711
			 train-loss:  2.1694261005946567 	 ± 0.204960177113241
	data : 0.11399688720703124
	model : 0.06495695114135742
			 train-loss:  2.1696329044573233 	 ± 0.20393267371348295
	data : 0.11383938789367676
	model : 0.06492671966552735
			 train-loss:  2.1673567152023314 	 ± 0.204170450283042
	data : 0.11390852928161621
	model : 0.06490964889526367
			 train-loss:  2.173362908977093 	 ± 0.21184966530897162
	data : 0.11402583122253418
	model : 0.06490511894226074
			 train-loss:  2.1726866189171288 	 ± 0.21091816453878862
	data : 0.11404929161071778
	model : 0.0649073600769043
			 train-loss:  2.171394956922068 	 ± 0.2102967914361692
	data : 0.1140587329864502
	model : 0.0649306297302246
			 train-loss:  2.1792690753936768 	 ± 0.2240216105527414
	data : 0.11415143013000488
	model : 0.06498236656188965
			 train-loss:  2.1798447291056315 	 ± 0.22302956384053158
	data : 0.11411285400390625
	model : 0.0649759292602539
			 train-loss:  2.1790256635198055 	 ± 0.2221336571155363
	data : 0.11404256820678711
	model : 0.06498026847839355
			 train-loss:  2.1787615891928986 	 ± 0.22110992887626552
	data : 0.11394205093383789
	model : 0.06489243507385253
			 train-loss:  2.177952457357336 	 ± 0.22024298275589194
	data : 0.1136927604675293
	model : 0.06482095718383789
			 train-loss:  2.176391463760936 	 ± 0.219829745656382
	data : 0.11378145217895508
	model : 0.06483540534973145
			 train-loss:  2.1761903654445303 	 ± 0.2188383098255238
	data : 0.11387743949890136
	model : 0.06481294631958008
			 train-loss:  2.1778378508112453 	 ± 0.21853449484019638
	data : 0.11390771865844726
	model : 0.06478662490844726
			 train-loss:  2.177083100591387 	 ± 0.21770197899870156
	data : 0.1139646053314209
	model : 0.06483597755432129
			 train-loss:  2.1780854431928787 	 ± 0.21699599016108514
	data : 0.11419534683227539
	model : 0.06489710807800293
			 train-loss:  2.1768551311994853 	 ± 0.21643765394863693
	data : 0.11412053108215332
	model : 0.0648879051208496
			 train-loss:  2.177292073291281 	 ± 0.21554505949675176
	data : 0.11395974159240722
	model : 0.06484861373901367
			 train-loss:  2.17821381626458 	 ± 0.21484148439343478
	data : 0.11390571594238282
	model : 0.06483621597290039
			 train-loss:  2.183945105626033 	 ± 0.22264925539654776
	data : 0.11389722824096679
	model : 0.06484065055847169
			 train-loss:  2.1822215652061723 	 ± 0.22248627506685678
	data : 0.11383357048034667
	model : 0.06486310958862304
			 train-loss:  2.1838133365166286 	 ± 0.22222321176032542
	data : 0.1138831615447998
	model : 0.06487293243408203
			 train-loss:  2.185141395529111 	 ± 0.2217690571168601
	data : 0.11408729553222656
	model : 0.06488995552062989
			 train-loss:  2.184728117028544 	 ± 0.220897152063255
	data : 0.11414451599121093
	model : 0.06491508483886718
			 train-loss:  2.1836614481738357 	 ± 0.22030265528657492
	data : 0.11414041519165039
	model : 0.06493992805480957
			 train-loss:  2.1819802474200243 	 ± 0.22018970404227403
	data : 0.11415014266967774
	model : 0.06485319137573242
			 train-loss:  2.1823702742976527 	 ± 0.21934270133904393
	data : 0.11412491798400878
	model : 0.06482629776000977
			 train-loss:  2.1815434856414795 	 ± 0.21865748265593032
	data : 0.11408638954162598
	model : 0.06492156982421875
			 train-loss:  2.1789511432723394 	 ± 0.21970815064210433
	data : 0.1141584873199463
	model : 0.06493649482727051
			 train-loss:  2.177151509157316 	 ± 0.21977182168960485
	data : 0.11428513526916503
	model : 0.06495046615600586
			 train-loss:  2.1805479414761066 	 ± 0.22223265541309845
	data : 0.11432480812072754
	model : 0.06498479843139648
			 train-loss:  2.182914600815884 	 ± 0.22298305589521963
	data : 0.11440453529357911
	model : 0.06501727104187012
			 train-loss:  2.1822310741131123 	 ± 0.22225939913855963
	data : 0.11438755989074707
	model : 0.06494698524475098
			 train-loss:  2.180829008116977 	 ± 0.22198580955533678
	data : 0.11429047584533691
	model : 0.06493048667907715
			 train-loss:  2.1804799672329063 	 ± 0.22117943712542854
	data : 0.11409058570861816
	model : 0.06484184265136719
			 train-loss:  2.181006705850587 	 ± 0.22042945617058474
	data : 0.11405744552612304
	model : 0.06485471725463868
			 train-loss:  2.180413931163389 	 ± 0.21971179615627684
	data : 0.11396665573120117
	model : 0.06481313705444336
			 train-loss:  2.182506472976119 	 ± 0.2202327031877196
	data : 0.11396455764770508
	model : 0.0648282527923584
			 train-loss:  2.182150786413866 	 ± 0.219460445603835
	data : 0.11394920349121093
	model : 0.06485285758972167
			 train-loss:  2.1804984477314635 	 ± 0.2195054523424953
	data : 0.11400895118713379
	model : 0.06490440368652343
			 train-loss:  2.1787393982859626 	 ± 0.2196756858305121
	data : 0.11403131484985352
	model : 0.06490168571472169
			 train-loss:  2.1788988585094753 	 ± 0.21889207468350824
	data : 0.11394000053405762
	model : 0.06487979888916015
			 train-loss:  2.178220867259162 	 ± 0.21825534079036696
	data : 0.11386771202087402
	model : 0.06484618186950683
			 train-loss:  2.178956871337079 	 ± 0.21765429559665295
	data : 0.11391239166259766
	model : 0.06484966278076172
			 train-loss:  2.178970107730006 	 ± 0.21688661013486507
	data : 0.11390047073364258
	model : 0.06485767364501953
			 train-loss:  2.178506992079995 	 ± 0.21619738111460743
	data : 0.1140824317932129
	model : 0.06488261222839356
			 train-loss:  2.1794058920608625 	 ± 0.21571337977749933
	data : 0.11409578323364258
	model : 0.06491174697875976
			 train-loss:  2.1828708048524526 	 ± 0.21895241363342702
	data : 0.11421418190002441
	model : 0.06493206024169922
			 train-loss:  2.1820145673947793 	 ± 0.21844474782919798
	data : 0.11412396430969238
	model : 0.06497011184692383
			 train-loss:  2.1839227262808354 	 ± 0.2189180009601532
	data : 0.11399154663085938
	model : 0.0649040699005127
			 train-loss:  2.1838495320564992 	 ± 0.2181789643800923
	data : 0.11379842758178711
	model : 0.06485800743103028
			 train-loss:  2.182870955275209 	 ± 0.2177712337164256
	data : 0.11392626762390137
	model : 0.0648998737335205
			 train-loss:  2.181408782800039 	 ± 0.21777672618962224
	data : 0.11401829719543458
	model : 0.06491870880126953
			 train-loss:  2.182679191330411 	 ± 0.21761137153802193
	data : 0.11403026580810546
	model : 0.0648641586303711
			 train-loss:  2.1828663404050626 	 ± 0.21690655505435893
	data : 0.11420092582702637
	model : 0.0649104118347168
			 train-loss:  2.183036474620595 	 ± 0.21620672314176476
	data : 0.11426053047180176
	model : 0.06491107940673828
			 train-loss:  2.1831888231364163 	 ± 0.21551184899515521
	data : 0.11418404579162597
	model : 0.06481418609619141
			 train-loss:  2.185596803695925 	 ± 0.21688397504234885
	data : 0.11387190818786622
	model : 0.06471815109252929
			 train-loss:  2.185925550185717 	 ± 0.21622645600529933
	data : 0.11388740539550782
	model : 0.06471085548400879
			 train-loss:  2.1861467809434147 	 ± 0.21555444782316302
	data : 0.11392383575439453
	model : 0.06472220420837402
			 train-loss:  2.185404269755641 	 ± 0.21507255401126008
	data : 0.11406059265136718
	model : 0.06474142074584961
			 train-loss:  2.183597947066685 	 ± 0.21559407837850483
	data : 0.11412243843078614
	model : 0.06482806205749511
			 train-loss:  2.185273540019989 	 ± 0.21595534690743007
	data : 0.11422886848449706
	model : 0.06492271423339843
			 train-loss:  2.185110570481105 	 ± 0.21529350266892436
	data : 0.11425547599792481
	model : 0.06492123603820801
			 train-loss:  2.1859651642081177 	 ± 0.21490173662349385
	data : 0.11413655281066895
	model : 0.06484451293945312
			 train-loss:  2.1850565255053938 	 ± 0.21455343797111137
	data : 0.11396045684814453
	model : 0.064823579788208
			 train-loss:  2.1838954338213292 	 ± 0.214411365049817
	data : 0.11390109062194824
	model : 0.06481881141662597
			 train-loss:  2.184135814146562 	 ± 0.21378281088223108
	data : 0.11385688781738282
	model : 0.06481342315673828
			 train-loss:  2.185975461121065 	 ± 0.2144438856007198
	data : 0.11392726898193359
	model : 0.0648122787475586
			 train-loss:  2.185520569007554 	 ± 0.21388119033332512
	data : 0.11403980255126953
	model : 0.06491751670837402
			 train-loss:  2.1852823879037584 	 ± 0.21326590184047867
	data : 0.1140784740447998
	model : 0.06493372917175293
			 train-loss:  2.184203272035136 	 ± 0.2130935313059751
	data : 0.11402392387390137
	model : 0.06489415168762207
			 train-loss:  2.1860705866533165 	 ± 0.21384812877419965
	data : 0.1140404224395752
	model : 0.06479935646057129
			 train-loss:  2.1861240891685263 	 ± 0.21322306613083042
	data : 0.11393909454345703
	model : 0.06482372283935547
			 train-loss:  2.18646108965541 	 ± 0.2126479961403297
	data : 0.11401486396789551
	model : 0.06480464935302735
			 train-loss:  2.186416018215907 	 ± 0.21203333978927122
	data : 0.11408095359802246
	model : 0.06481924057006835
			 train-loss:  2.1875420562152206 	 ± 0.21194129950548715
	data : 0.11418094635009765
	model : 0.06484260559082031
			 train-loss:  2.19116270337786 	 ± 0.21666429037715337
	data : 0.11418261528015136
	model : 0.0649451732635498
			 train-loss:  2.189939911392602 	 ± 0.21665261207564124
	data : 0.11464228630065917
	model : 0.06491217613220215
			 train-loss:  2.1893230878700645 	 ± 0.21619465492494758
	data : 0.11442584991455078
	model : 0.06490306854248047
			 train-loss:  2.1912457574619335 	 ± 0.21709871569032405
	data : 0.11474003791809081
	model : 0.06482257843017578
			 train-loss:  2.1898105057924155 	 ± 0.21733664335730024
	data : 0.11448397636413574
	model : 0.06481781005859374
			 train-loss:  2.190508633852005 	 ± 0.21693326207637526
	data : 0.11455588340759278
	model : 0.06477751731872558
			 train-loss:  2.191133282461219 	 ± 0.21649543513045189
	data : 0.11401820182800293
	model : 0.0648284912109375
			 train-loss:  2.1904367398429705 	 ± 0.2161031247355449
	data : 0.11416373252868653
	model : 0.0648458480834961
			 train-loss:  2.189992193967267 	 ± 0.21559529964533697
	data : 0.11378769874572754
	model : 0.06494007110595704
			 train-loss:  2.1901379804248395 	 ± 0.21501768930297693
	data : 0.11402297019958496
	model : 0.06496782302856445
			 train-loss:  2.1904524435868136 	 ± 0.21447819458798592
	data : 0.11392860412597657
	model : 0.0650026798248291
			 train-loss:  2.1896782543069575 	 ± 0.21415989956432704
	data : 0.11389122009277344
	model : 0.06493511199951171
			 train-loss:  2.187378046984341 	 ± 0.21587801061906461
	data : 0.11392498016357422
	model : 0.0649001121520996
			 train-loss:  2.1875279583829514 	 ± 0.21531286079534795
	data : 0.11397132873535157
	model : 0.06488103866577148
			 train-loss:  2.187624039473357 	 ± 0.2147465355319825
	data : 0.11394925117492676
	model : 0.06486301422119141
			 train-loss:  2.1864285230636598 	 ± 0.21481035446205984
	data : 0.11406002044677735
	model : 0.06487164497375489
			 train-loss:  2.1857836608487275 	 ± 0.21443159846949642
	data : 0.11423273086547851
	model : 0.06492862701416016
			 train-loss:  2.186810739338398 	 ± 0.21434297410110684
	data : 0.11416044235229492
	model : 0.0649442195892334
			 train-loss:  2.186727080320447 	 ± 0.2137901030297699
	data : 0.11405973434448242
	model : 0.06495037078857421
			 train-loss:  2.1858517447697747 	 ± 0.2135848499340474
	data : 0.11402678489685059
	model : 0.06488428115844727
			 train-loss:  2.1851780634659987 	 ± 0.21324303828599378
	data : 0.11401352882385254
	model : 0.06484026908874511
			 train-loss:  2.1850988463479646 	 ± 0.21270123188078827
	data : 0.11397705078125
	model : 0.0648118495941162
			 train-loss:  2.183749359271248 	 ± 0.2130002318725851
	data : 0.11400232315063477
	model : 0.06488947868347168
			 train-loss:  2.184419571149229 	 ± 0.21266981721827422
	data : 0.11417083740234375
	model : 0.06488480567932128
			 train-loss:  2.184313935251092 	 ± 0.2121400056032333
	data : 0.11421537399291992
	model : 0.06496834754943848
			 train-loss:  2.18444945037365 	 ± 0.21161762585567645
	data : 0.1142512321472168
	model : 0.06500091552734374
			 train-loss:  2.1846501762001074 	 ± 0.21110964361439513
	data : 0.114223051071167
	model : 0.06497983932495117
			 train-loss:  2.18684324474618 	 ± 0.21286937543746093
	data : 0.11411457061767578
	model : 0.06481623649597168
			 train-loss:  2.1870939620022702 	 ± 0.21237431570123913
	data : 0.11387953758239747
	model : 0.06478981971740723
			 train-loss:  2.1877069303802417 	 ± 0.2120330889388109
	data : 0.11382098197937011
	model : 0.06482172012329102
			 train-loss:  2.1877119581873825 	 ± 0.21151531503528734
	data : 0.11376967430114746
	model : 0.06481361389160156
			 train-loss:  2.188450562722475 	 ± 0.21126614782298278
	data : 0.1137763500213623
	model : 0.06485247611999512
			 train-loss:  2.187581059437443 	 ± 0.21112439083592402
	data : 0.11385927200317383
	model : 0.06499929428100586
			 train-loss:  2.1864135528986273 	 ± 0.21128504087278227
	data : 0.11389617919921875
	model : 0.06501560211181641
			 train-loss:  2.1883414966638006 	 ± 0.21260503898464883
	data : 0.11395492553710937
	model : 0.06494197845458985
			 train-loss:  2.187069504601615 	 ± 0.21289390466575603
	data : 0.11397480964660645
	model : 0.06498823165893555
			 train-loss:  2.1865896737971013 	 ± 0.2125026111164169
	data : 0.11403403282165528
	model : 0.06499290466308594
			 train-loss:  2.187546544479874 	 ± 0.21245598422900985
	data : 0.11402506828308105
	model : 0.06495051383972168
			 train-loss:  2.1872338319608304 	 ± 0.2120055734412564
	data : 0.1141024112701416
	model : 0.06495246887207032
			 train-loss:  2.1861191453220687 	 ± 0.21213437142098607
	data : 0.11421961784362793
	model : 0.06495852470397949
			 train-loss:  2.1858190558677495 	 ± 0.21168598462920743
	data : 0.11419339179992676
	model : 0.0649034023284912
			 train-loss:  2.1844935985626996 	 ± 0.21208775996721882
	data : 0.1140854835510254
	model : 0.0648681640625
			 train-loss:  2.1841530783385177 	 ± 0.2116576889185353
	data : 0.11403846740722656
	model : 0.06483039855957032
			 train-loss:  2.1862825893480844 	 ± 0.21348895217765773
	data : 0.11405439376831054
	model : 0.0648158073425293
			 train-loss:  2.186753743859731 	 ± 0.213114544757819
	data : 0.11393871307373046
	model : 0.06479692459106445
			 train-loss:  2.187018502300436 	 ± 0.21266573736760877
	data : 0.11404647827148437
	model : 0.06476807594299316
			 train-loss:  2.189303952104905 	 ± 0.21487482834188837
	data : 0.11418728828430176
	model : 0.06515746116638184
			 train-loss:  2.1880974667566315 	 ± 0.21513926335560937
	data : 0.11425576210021973
	model : 0.06557073593139648
			 train-loss:  2.187422022691222 	 ± 0.21489213317422173
	data : 0.11384506225585937
	model : 0.06550469398498535
			 train-loss:  2.1864150650799274 	 ± 0.21493856842494247
	data : 0.1138007640838623
	model : 0.06539416313171387
			 train-loss:  2.1865692875120373 	 ± 0.2144728161773213
	data : 0.11380267143249512
	model : 0.0652358055114746
			 train-loss:  2.187573658154074 	 ± 0.21452744902902732
	data : 0.11380343437194824
	model : 0.0646857738494873
			 train-loss:  2.186996637987145 	 ± 0.21423009451583114
	data : 0.11393499374389648
	model : 0.06414141654968261
			 train-loss:  2.186415080961428 	 ± 0.2139392786036612
	data : 0.11441078186035156
	model : 0.06402764320373536
			 train-loss:  2.1867915881252706 	 ± 0.2135473404459921
	data : 0.11445999145507812
	model : 0.06401629447937011
			 train-loss:  2.1876892323079318 	 ± 0.21351514049501863
	data : 0.11443204879760742
	model : 0.06398382186889648
			 train-loss:  2.1874349050191575 	 ± 0.21308739620193703
	data : 0.11439990997314453
	model : 0.06389732360839843
			 train-loss:  2.186223581947129 	 ± 0.21342321570837158
	data : 0.11432685852050781
	model : 0.06386442184448242
			 train-loss:  2.187974269809641 	 ± 0.21462766917188833
	data : 0.11444311141967774
	model : 0.06388320922851562
			 train-loss:  2.188132270788535 	 ± 0.2141821512834991
	data : 0.11453785896301269
	model : 0.06382832527160645
			 train-loss:  2.188655682827564 	 ± 0.21387588017743928
	data : 0.11457080841064453
	model : 0.06385345458984375
			 train-loss:  2.189438435990932 	 ± 0.2137593305532382
	data : 0.11461291313171387
	model : 0.06390395164489746
			 train-loss:  2.1882778114407375 	 ± 0.2140517632702887
	data : 0.11453824043273926
	model : 0.06382613182067871
			 train-loss:  2.1890923641309015 	 ± 0.21396937319406875
	data : 0.11453990936279297
	model : 0.06378154754638672
			 train-loss:  2.1884957691615594 	 ± 0.2137195422215257
	data : 0.11456270217895508
	model : 0.06382870674133301
			 train-loss:  2.1883378649751344 	 ± 0.21328779863051414
	data : 0.1145857334136963
	model : 0.0638664722442627
			 train-loss:  2.1881251033410987 	 ± 0.21287035272471003
	data : 0.11453218460083008
	model : 0.0638707160949707
			 train-loss:  2.1882696442367617 	 ± 0.2124419332526722
	data : 0.11465888023376465
	model : 0.06398935317993164
			 train-loss:  2.187166737921444 	 ± 0.2126974793559958
	data : 0.11465258598327636
	model : 0.06405353546142578
			 train-loss:  2.18683225991296 	 ± 0.2123252051730749
	data : 0.11475238800048829
	model : 0.06404070854187012
			 train-loss:  2.187036163952886 	 ± 0.2119153825736516
	data : 0.1148371696472168
	model : 0.06402144432067872
			 train-loss:  2.1869485891931424 	 ± 0.2114886639700867
	data : 0.11488475799560546
	model : 0.06405577659606934
			 train-loss:  2.1867900821361466 	 ± 0.21107475642048518
	data : 0.11505861282348633
	model : 0.0640641689300537
			 train-loss:  2.186425959871661 	 ± 0.21072649076102354
	data : 0.11501407623291016
	model : 0.063970947265625
			 train-loss:  2.188162495333507 	 ± 0.21207351482616912
	data : 0.11492776870727539
	model : 0.06386361122131348
			 train-loss:  2.1891347942352293 	 ± 0.21220431407344414
	data : 0.11489715576171874
	model : 0.06380343437194824
			 train-loss:  2.1883921955686167 	 ± 0.21210640971668623
	data : 0.11489109992980957
	model : 0.06387667655944824
			 train-loss:  2.1884284643899825 	 ± 0.2116859251942089
	data : 0.11471667289733886
	model : 0.06386098861694336
			 train-loss:  2.1874820983457 	 ± 0.21180062797616508
	data : 0.11473641395568848
	model : 0.06399149894714355
			 train-loss:  2.18821146779173 	 ± 0.2117014045047285
	data : 0.11481094360351562
	model : 0.0640645980834961
			 train-loss:  2.188177524360956 	 ± 0.21128658848867699
	data : 0.11462054252624512
	model : 0.06405415534973144
			 train-loss:  2.186612084042281 	 ± 0.21235004930463247
	data : 0.11446771621704102
	model : 0.055510950088500974
#epoch  60    val-loss:  2.395737371946636  train-loss:  2.186612084042281  lr:  3.814697265625e-08
			 train-loss:  1.869726538658142 	 ± 0.0
	data : 5.666948556900024
	model : 0.07090878486633301
			 train-loss:  1.9035284519195557 	 ± 0.033801913261413574
	data : 2.8951497077941895
	model : 0.06779360771179199
			 train-loss:  2.0156731605529785 	 ± 0.16098007405381584
	data : 1.968168020248413
	model : 0.06678660710652669
			 train-loss:  2.003009110689163 	 ± 0.1411278592576201
	data : 1.5046332478523254
	model : 0.06621646881103516
			 train-loss:  2.026320958137512 	 ± 0.13456384006385697
	data : 1.226440191268921
	model : 0.06583709716796875
			 train-loss:  2.114017148812612 	 ± 0.23139280852818211
	data : 0.11568126678466797
	model : 0.06457548141479492
			 train-loss:  2.155149851526533 	 ± 0.23673842068486092
	data : 0.11382546424865722
	model : 0.06458473205566406
			 train-loss:  2.1427045315504074 	 ± 0.22388310962413832
	data : 0.11384387016296386
	model : 0.06458082199096679
			 train-loss:  2.138480544090271 	 ± 0.2114168613175705
	data : 0.11389493942260742
	model : 0.06466102600097656
			 train-loss:  2.15642112493515 	 ± 0.2076635747884071
	data : 0.11406922340393066
	model : 0.06475014686584472
			 train-loss:  2.135554151101546 	 ± 0.20870574514154427
	data : 0.11430802345275878
	model : 0.06481695175170898
			 train-loss:  2.131369243065516 	 ± 0.20030201045371523
	data : 0.11425304412841797
	model : 0.06484012603759766
			 train-loss:  2.1450555599652805 	 ± 0.19819802196975464
	data : 0.11408720016479493
	model : 0.0648320198059082
			 train-loss:  2.1567272543907166 	 ± 0.19556977288057967
	data : 0.11413102149963379
	model : 0.06483807563781738
			 train-loss:  2.168593668937683 	 ± 0.1940852015114252
	data : 0.11428971290588379
	model : 0.0648996353149414
			 train-loss:  2.17644951492548 	 ± 0.19036928218106014
	data : 0.11425542831420898
	model : 0.06490812301635743
			 train-loss:  2.1848118655821858 	 ± 0.1876899948262099
	data : 0.11429300308227539
	model : 0.06492052078247071
			 train-loss:  2.1747641099823847 	 ± 0.18704738597734266
	data : 0.11453580856323242
	model : 0.06499080657958985
			 train-loss:  2.173836438279403 	 ± 0.1821010938715227
	data : 0.114589262008667
	model : 0.06498045921325683
			 train-loss:  2.174873250722885 	 ± 0.17754771919278803
	data : 0.11428632736206054
	model : 0.06490058898925781
			 train-loss:  2.161977546555655 	 ± 0.18261454460382098
	data : 0.11403570175170899
	model : 0.06488261222839356
			 train-loss:  2.1775531497868625 	 ± 0.19216357143630938
	data : 0.1139723777770996
	model : 0.06488690376281739
			 train-loss:  2.182237920553788 	 ± 0.18921986994081297
	data : 0.1138267993927002
	model : 0.06484618186950683
			 train-loss:  2.1814724852641425 	 ± 0.18527221786271145
	data : 0.11366300582885742
	model : 0.06483712196350097
			 train-loss:  2.1787583589553834 	 ± 0.18201526991484765
	data : 0.1137160301208496
	model : 0.06489949226379395
			 train-loss:  2.1858209875913768 	 ± 0.18194054736547244
	data : 0.11398673057556152
	model : 0.06491231918334961
			 train-loss:  2.1835497706024736 	 ± 0.17891469596260215
	data : 0.11405043601989746
	model : 0.0649139404296875
			 train-loss:  2.1959441857678548 	 ± 0.1871230040718222
	data : 0.11399312019348144
	model : 0.06485247611999512
			 train-loss:  2.1957814570130973 	 ± 0.18387045895056406
	data : 0.11389145851135254
	model : 0.06485428810119628
			 train-loss:  2.189738031228383 	 ± 0.18368605739171667
	data : 0.11388926506042481
	model : 0.06531457901000977
			 train-loss:  2.184574007987976 	 ± 0.18289936126706713
	data : 0.113895845413208
	model : 0.06561298370361328
			 train-loss:  2.182056497782469 	 ± 0.1805637544680107
	data : 0.11369643211364747
	model : 0.06559357643127442
			 train-loss:  2.1717371073636142 	 ± 0.18714423785379572
	data : 0.11382713317871093
	model : 0.06569900512695312
			 train-loss:  2.167994127554052 	 ± 0.18562113339486858
	data : 0.11411385536193848
	model : 0.06621785163879394
			 train-loss:  2.169971629551479 	 ± 0.18331319704447627
	data : 0.11410226821899414
	model : 0.06575660705566407
			 train-loss:  2.1688754161198935 	 ± 0.18086555850131286
	data : 0.11408934593200684
	model : 0.06543793678283691
			 train-loss:  2.1816813494708085 	 ± 0.19424711612063827
	data : 0.11426539421081543
	model : 0.06542396545410156
			 train-loss:  2.1753580946671334 	 ± 0.19549523696101098
	data : 0.11410446166992187
	model : 0.06534147262573242
			 train-loss:  2.176528050349309 	 ± 0.19310733615131373
	data : 0.11387243270874023
	model : 0.06487650871276855
			 train-loss:  2.1858157098293303 	 ± 0.1993046570445021
	data : 0.11390256881713867
	model : 0.06486029624938965
			 train-loss:  2.1795456612982402 	 ± 0.20081346804386124
	data : 0.11377620697021484
	model : 0.06491403579711914
			 train-loss:  2.183350145816803 	 ± 0.19989833197344972
	data : 0.11383953094482421
	model : 0.06493916511535644
			 train-loss:  2.185756708300391 	 ± 0.19817492495722663
	data : 0.11387677192687988
	model : 0.0650212287902832
			 train-loss:  2.196314825253053 	 ± 0.20778378354906205
	data : 0.11400303840637208
	model : 0.06494474411010742
			 train-loss:  2.195051328341166 	 ± 0.2056329713870571
	data : 0.11381616592407226
	model : 0.06489720344543456
			 train-loss:  2.190104948437732 	 ± 0.2060744571515876
	data : 0.11375327110290527
	model : 0.06486310958862304
			 train-loss:  2.1957745273062526 	 ± 0.20746509392246254
	data : 0.11366806030273438
	model : 0.06486926078796387
			 train-loss:  2.1978149389227233 	 ± 0.2057686465285834
	data : 0.11370892524719238
	model : 0.06500701904296875
			 train-loss:  2.196114216532026 	 ± 0.20399871853154725
	data : 0.11363430023193359
	model : 0.06505537033081055
			 train-loss:  2.1999402594566346 	 ± 0.20371661502648616
	data : 0.11383605003356934
	model : 0.06509542465209961
			 train-loss:  2.1996334510691025 	 ± 0.20172117219648206
	data : 0.11395096778869629
	model : 0.06507010459899902
			 train-loss:  2.2073619617865634 	 ± 0.20725618823207953
	data : 0.11391873359680176
	model : 0.06497149467468262
			 train-loss:  2.2089190325647032 	 ± 0.20559845783662706
	data : 0.11386275291442871
	model : 0.06477417945861816
			 train-loss:  2.2008772735242492 	 ± 0.21193262781768912
	data : 0.11395149230957032
	model : 0.06477131843566894
			 train-loss:  2.2031785748221657 	 ± 0.21067695110510556
	data : 0.11400222778320312
	model : 0.0647573471069336
			 train-loss:  2.199668283973421 	 ± 0.21040415920732963
	data : 0.11407303810119629
	model : 0.06474218368530274
			 train-loss:  2.204299905843902 	 ± 0.21141086231499945
	data : 0.11415181159973145
	model : 0.06479392051696778
			 train-loss:  2.2040552928529937 	 ± 0.2095885674192555
	data : 0.11426191329956055
	model : 0.06478157043457031
			 train-loss:  2.213986392748558 	 ± 0.2211406501677356
	data : 0.11405735015869141
	model : 0.06470913887023926
			 train-loss:  2.2138824661572776 	 ± 0.21929152122409598
	data : 0.11397247314453125
	model : 0.06467938423156738
			 train-loss:  2.2119523579957057 	 ± 0.21799988490570466
	data : 0.11397514343261719
	model : 0.06473031044006347
			 train-loss:  2.2063757181167603 	 ± 0.2205775785340541
	data : 0.11414918899536133
	model : 0.06476917266845703
			 train-loss:  2.207498747204977 	 ± 0.21899855948230043
	data : 0.11425290107727051
	model : 0.06481418609619141
			 train-loss:  2.201845284551382 	 ± 0.2218661118947548
	data : 0.11440958976745605
	model : 0.06488499641418458
			 train-loss:  2.219629639845628 	 ± 0.26212478187399885
	data : 0.1144650936126709
	model : 0.06490774154663086
			 train-loss:  2.2161045255083027 	 ± 0.26167932237566816
	data : 0.11439700126647949
	model : 0.06488828659057617
			 train-loss:  2.212627039026858 	 ± 0.2612511609620212
	data : 0.11406903266906739
	model : 0.06485419273376465
			 train-loss:  2.2096499818212845 	 ± 0.2604654929129378
	data : 0.11394743919372559
	model : 0.06479368209838868
			 train-loss:  2.206174021181853 	 ± 0.2601550464848314
	data : 0.11404342651367187
	model : 0.06477055549621583
			 train-loss:  2.210618829727173 	 ± 0.26091563405786333
	data : 0.11411538124084472
	model : 0.06477384567260742
			 train-loss:  2.2076158859360384 	 ± 0.26028710025305885
	data : 0.11414508819580078
	model : 0.0648148536682129
			 train-loss:  2.2047471668985157 	 ± 0.25960105738550004
	data : 0.11433539390563965
	model : 0.06486349105834961
			 train-loss:  2.202983212797609 	 ± 0.2582509470539381
	data : 0.11426610946655273
	model : 0.06490979194641114
			 train-loss:  2.203633846463384 	 ± 0.25656030501015903
	data : 0.11417036056518555
	model : 0.06489067077636719
			 train-loss:  2.200216186841329 	 ± 0.2565343972487349
	data : 0.11400074958801269
	model : 0.06488256454467774
			 train-loss:  2.201835285676153 	 ± 0.25522654319749816
	data : 0.11402382850646972
	model : 0.06478748321533204
			 train-loss:  2.197618794131589 	 ± 0.25621435470612763
	data : 0.1139291763305664
	model : 0.06476058959960937
			 train-loss:  2.1963799458283644 	 ± 0.2547986620216772
	data : 0.11406264305114747
	model : 0.06475539207458496
			 train-loss:  2.1937336499177955 	 ± 0.2542573125114019
	data : 0.11406326293945312
	model : 0.06480355262756347
			 train-loss:  2.1929533064365385 	 ± 0.25275838689033087
	data : 0.11427145004272461
	model : 0.06482582092285157
			 train-loss:  2.190140669728503 	 ± 0.252449896437146
	data : 0.11420211791992188
	model : 0.0648643970489502
			 train-loss:  2.191729782558069 	 ± 0.25131313365525965
	data : 0.11449122428894043
	model : 0.06483688354492187
			 train-loss:  2.1908553494028298 	 ± 0.24992008285632433
	data : 0.11433401107788085
	model : 0.06478710174560547
			 train-loss:  2.192214396737871 	 ± 0.24873636167396723
	data : 0.1142470359802246
	model : 0.06473383903503419
			 train-loss:  2.1882287488264196 	 ± 0.24995253665672496
	data : 0.11411828994750976
	model : 0.06479568481445312
			 train-loss:  2.187466886154441 	 ± 0.24859432670656675
	data : 0.11426582336425781
	model : 0.06479763984680176
			 train-loss:  2.186701934913109 	 ± 0.24726327521709382
	data : 0.11407980918884278
	model : 0.06485152244567871
			 train-loss:  2.1871702657504515 	 ± 0.245893160741519
	data : 0.114143705368042
	model : 0.06492247581481933
			 train-loss:  2.1850764443365374 	 ± 0.24529549870721434
	data : 0.11416192054748535
	model : 0.06499137878417968
			 train-loss:  2.183755920992957 	 ± 0.24424685061275392
	data : 0.11411643028259277
	model : 0.06490941047668457
			 train-loss:  2.1840257159956207 	 ± 0.24291461231671918
	data : 0.11392521858215332
	model : 0.06490097045898438
			 train-loss:  2.1826882634473885 	 ± 0.24192747224930083
	data : 0.11393284797668457
	model : 0.06488327980041504
			 train-loss:  2.1828255768745177 	 ± 0.2406268760060405
	data : 0.11409344673156738
	model : 0.0648618221282959
			 train-loss:  2.183135675623062 	 ± 0.23936220512894482
	data : 0.11425957679748536
	model : 0.06482667922973633
			 train-loss:  2.1806254800997285 	 ± 0.23933965275826952
	data : 0.11428756713867187
	model : 0.0648651123046875
			 train-loss:  2.1797238054374852 	 ± 0.23825197419969535
	data : 0.11435418128967285
	model : 0.06493511199951171
			 train-loss:  2.1777506906961657 	 ± 0.2378078063306206
	data : 0.11434350013732911
	model : 0.0649557113647461
			 train-loss:  2.1800678992757994 	 ± 0.23768955189556812
	data : 0.11435403823852539
	model : 0.06495046615600586
			 train-loss:  2.1848533972345217 	 ± 0.24118447346394348
	data : 0.11420602798461914
	model : 0.06492915153503417
			 train-loss:  2.186882450580597 	 ± 0.2408232535041405
	data : 0.11409754753112793
	model : 0.06485996246337891
			 train-loss:  2.1889843067320265 	 ± 0.24054813057782481
	data : 0.1140683650970459
	model : 0.06478404998779297
			 train-loss:  2.1843523137709675 	 ± 0.2438505814971158
	data : 0.11406807899475098
	model : 0.06482472419738769
			 train-loss:  2.1849616754402237 	 ± 0.2427419807006829
	data : 0.11392741203308106
	model : 0.06485481262207031
			 train-loss:  2.183785392687871 	 ± 0.24186692722843411
	data : 0.11393551826477051
	model : 0.0648763656616211
			 train-loss:  2.1858552365075976 	 ± 0.24163615974640132
	data : 0.11396842002868653
	model : 0.06491451263427735
			 train-loss:  2.1914160161648155 	 ± 0.2471518593537319
	data : 0.11407790184020997
	model : 0.06487975120544434
			 train-loss:  2.1901727034666827 	 ± 0.24632705960580736
	data : 0.11392254829406738
	model : 0.06476416587829589
			 train-loss:  2.192438962282958 	 ± 0.2463021324257099
	data : 0.11388149261474609
	model : 0.06472930908203126
			 train-loss:  2.193085556730218 	 ± 0.24526177096867088
	data : 0.11397414207458496
	model : 0.06475720405578614
			 train-loss:  2.1901638193563984 	 ± 0.24604262163002205
	data : 0.11416268348693848
	model : 0.06478891372680665
			 train-loss:  2.189844199129053 	 ± 0.24495475258788932
	data : 0.11415066719055175
	model : 0.06489048004150391
			 train-loss:  2.195820279419422 	 ± 0.2518557001742117
	data : 0.11432528495788574
	model : 0.06495628356933594
			 train-loss:  2.195820085770261 	 ± 0.25073881796014535
	data : 0.11443171501159669
	model : 0.06495852470397949
			 train-loss:  2.197234234266114 	 ± 0.2500888706132508
	data : 0.11437692642211914
	model : 0.06489205360412598
			 train-loss:  2.1989175951999167 	 ± 0.24964699225287426
	data : 0.11416888236999512
	model : 0.06484193801879883
			 train-loss:  2.2001298203550537 	 ± 0.24890829531855405
	data : 0.1141136646270752
	model : 0.06479411125183106
			 train-loss:  2.2053677149308033 	 ± 0.25418168552117887
	data : 0.11416549682617187
	model : 0.06480484008789063
			 train-loss:  2.2058209190934392 	 ± 0.2531498215284612
	data : 0.11413779258728027
	model : 0.06484737396240234
			 train-loss:  2.205328549657549 	 ± 0.2521406563859967
	data : 0.11414961814880371
	model : 0.0649531364440918
			 train-loss:  2.2053583870331446 	 ± 0.2510880833952111
	data : 0.11430840492248535
	model : 0.06499161720275878
			 train-loss:  2.2033434722049177 	 ± 0.2510206694923283
	data : 0.11436033248901367
	model : 0.06505327224731446
			 train-loss:  2.2010822931274037 	 ± 0.2512241119975216
	data : 0.11429152488708497
	model : 0.06504416465759277
			 train-loss:  2.2009399605960382 	 ± 0.25020573067603236
	data : 0.1142348289489746
	model : 0.06497230529785156
			 train-loss:  2.2051391515039627 	 ± 0.25350922987366953
	data : 0.11436657905578614
	model : 0.06486387252807617
			 train-loss:  2.203253927230835 	 ± 0.25336436004996976
	data : 0.11420660018920899
	model : 0.06490921974182129
			 train-loss:  2.2042703098720975 	 ± 0.25261265989430465
	data : 0.11427540779113769
	model : 0.0648460865020752
			 train-loss:  2.2048341777380998 	 ± 0.2516957517765057
	data : 0.11418595314025878
	model : 0.0648615837097168
			 train-loss:  2.203573288396001 	 ± 0.25111298979151625
	data : 0.11426405906677246
	model : 0.06497173309326172
			 train-loss:  2.2016780256300934 	 ± 0.25105515864692596
	data : 0.11415996551513671
	model : 0.06506729125976562
			 train-loss:  2.200408267057859 	 ± 0.25050317680015
	data : 0.11437158584594727
	model : 0.06501975059509277
			 train-loss:  2.2012521847513797 	 ± 0.2497306659483582
	data : 0.11420698165893554
	model : 0.06501626968383789
			 train-loss:  2.1999450515617025 	 ± 0.24923235454433423
	data : 0.11428499221801758
	model : 0.06497793197631836
			 train-loss:  2.200742530643492 	 ± 0.24846261613192186
	data : 0.11410846710205078
	model : 0.06491813659667969
			 train-loss:  2.1998887373440303 	 ± 0.24772953932055872
	data : 0.11407332420349121
	model : 0.06488032341003418
			 train-loss:  2.1978912265212447 	 ± 0.24789110360961836
	data : 0.11400270462036133
	model : 0.06492719650268555
			 train-loss:  2.1971001888022705 	 ± 0.24714901579787335
	data : 0.11418290138244629
	model : 0.0650320053100586
			 train-loss:  2.197323078656719 	 ± 0.2462590786499303
	data : 0.11427135467529297
	model : 0.06505613327026367
			 train-loss:  2.1981115306633106 	 ± 0.2455387034052309
	data : 0.11446628570556641
	model : 0.06507563591003418
			 train-loss:  2.1976827408770006 	 ± 0.24470572530558451
	data : 0.11444640159606934
	model : 0.06503057479858398
			 train-loss:  2.196889417512076 	 ± 0.24400953393839744
	data : 0.1143068790435791
	model : 0.06491560935974121
			 train-loss:  2.197983548996296 	 ± 0.24348711637212134
	data : 0.11423821449279785
	model : 0.06478004455566407
			 train-loss:  2.1973817197369856 	 ± 0.24273347307199294
	data : 0.11402707099914551
	model : 0.06479992866516113
			 train-loss:  2.196894620681976 	 ± 0.2419529001710586
	data : 0.1141007423400879
	model : 0.06483750343322754
			 train-loss:  2.1973706202374563 	 ± 0.2411785024760283
	data : 0.11422696113586425
	model : 0.06488542556762696
			 train-loss:  2.2007842754495557 	 ± 0.2438113120507618
	data : 0.11444973945617676
	model : 0.06497015953063964
			 train-loss:  2.19954681233184 	 ± 0.24343139931295812
	data : 0.11445097923278809
	model : 0.06497797966003419
			 train-loss:  2.1984383021893144 	 ± 0.24297145676067758
	data : 0.11441092491149903
	model : 0.06492514610290527
			 train-loss:  2.198345316422952 	 ± 0.2421518403994719
	data : 0.11428995132446289
	model : 0.06484460830688477
			 train-loss:  2.1969998962927186 	 ± 0.2418922827129674
	data : 0.1141848087310791
	model : 0.06483755111694336
			 train-loss:  2.1948812500635784 	 ± 0.24246774605015564
	data : 0.11413521766662597
	model : 0.06488327980041504
			 train-loss:  2.19664420828914 	 ± 0.24262619264292123
	data : 0.11417794227600098
	model : 0.06491880416870117
			 train-loss:  2.1956377703892556 	 ± 0.24214279646106943
	data : 0.11439518928527832
	model : 0.0649681568145752
			 train-loss:  2.198390067792406 	 ± 0.24372388518986876
	data : 0.11444530487060547
	model : 0.06499552726745605
			 train-loss:  2.197788302000467 	 ± 0.24304529197674202
	data : 0.11446628570556641
	model : 0.06499161720275878
			 train-loss:  2.1982039190107776 	 ± 0.2423149029901505
	data : 0.1143378734588623
	model : 0.06494851112365722
			 train-loss:  2.1990295159511075 	 ± 0.2417556078151209
	data : 0.11426811218261719
	model : 0.0648775577545166
			 train-loss:  2.1991954016837343 	 ± 0.2409933622614934
	data : 0.11423087120056152
	model : 0.06482791900634766
			 train-loss:  2.1984892374352563 	 ± 0.24039240941593057
	data : 0.11410870552062988
	model : 0.0648261547088623
			 train-loss:  2.199432725426536 	 ± 0.23992854675583047
	data : 0.11423368453979492
	model : 0.06482839584350586
			 train-loss:  2.2001490607857703 	 ± 0.23934809494822157
	data : 0.11443700790405273
	model : 0.06483249664306641
			 train-loss:  2.1990979621129005 	 ± 0.23897375767149137
	data : 0.11442666053771973
	model : 0.0649263858795166
			 train-loss:  2.1980931376233515 	 ± 0.2385759685090687
	data : 0.11447563171386718
	model : 0.06501483917236328
			 train-loss:  2.1982214377701648 	 ± 0.23784862037284896
	data : 0.1145350456237793
	model : 0.06501750946044922
			 train-loss:  2.201219701185459 	 ± 0.2401922507315614
	data : 0.11441459655761718
	model : 0.06497006416320801
			 train-loss:  2.201721338792281 	 ± 0.23954944356201613
	data : 0.1142080307006836
	model : 0.0648763656616211
			 train-loss:  2.200479134019599 	 ± 0.23935926250097947
	data : 0.11421117782592774
	model : 0.06482906341552734
			 train-loss:  2.2006975148252383 	 ± 0.23865812811844142
	data : 0.11415328979492187
	model : 0.06478004455566407
			 train-loss:  2.2012622228690555 	 ± 0.23805865594430575
	data : 0.11428389549255372
	model : 0.06477718353271485
			 train-loss:  2.2017390022616414 	 ± 0.23743373047346134
	data : 0.11421823501586914
	model : 0.06486525535583496
			 train-loss:  2.204446420950048 	 ± 0.23933647785337434
	data : 0.11430935859680176
	model : 0.06495075225830078
			 train-loss:  2.202775369610703 	 ± 0.23962820703760263
	data : 0.11418871879577637
	model : 0.06501960754394531
			 train-loss:  2.2013830681179845 	 ± 0.23962327669613315
	data : 0.11416506767272949
	model : 0.0650263786315918
			 train-loss:  2.2017581132106008 	 ± 0.23898034333080556
	data : 0.11404237747192383
	model : 0.06501612663269044
			 train-loss:  2.201088013320134 	 ± 0.2384555718467984
	data : 0.11389589309692383
	model : 0.06501207351684571
			 train-loss:  2.1998872709274293 	 ± 0.23830025078139933
	data : 0.11388483047485351
	model : 0.06500592231750488
			 train-loss:  2.2010392715985123 	 ± 0.23811047757262335
	data : 0.11407246589660644
	model : 0.06495985984802247
			 train-loss:  2.1985439257433184 	 ± 0.23973357696289752
	data : 0.11409382820129395
	model : 0.06499147415161133
			 train-loss:  2.197395925441485 	 ± 0.23954661257017262
	data : 0.11424307823181153
	model : 0.06508989334106445
			 train-loss:  2.19648787096226 	 ± 0.23918356700725996
	data : 0.11443800926208496
	model : 0.06511330604553223
			 train-loss:  2.1962979442543453 	 ± 0.2385317779531079
	data : 0.11439542770385742
	model : 0.06513161659240722
			 train-loss:  2.1958367119836546 	 ± 0.2379524135291389
	data : 0.11433334350585937
	model : 0.06518359184265136
			 train-loss:  2.1948724074678108 	 ± 0.2376521694381565
	data : 0.1142646312713623
	model : 0.06510992050170898
			 train-loss:  2.196780594971662 	 ± 0.23839593608232548
	data : 0.1140561580657959
	model : 0.06500711441040039
			 train-loss:  2.1950237653825595 	 ± 0.2389321447202258
	data : 0.1139760971069336
	model : 0.06495985984802247
			 train-loss:  2.1952612187411336 	 ± 0.2383072755726682
	data : 0.11413216590881348
	model : 0.06496133804321289
			 train-loss:  2.194820370084496 	 ± 0.23774142967146436
	data : 0.11424846649169922
	model : 0.06494221687316895
			 train-loss:  2.19548507233992 	 ± 0.2372781412672565
	data : 0.1144373893737793
	model : 0.06502265930175781
			 train-loss:  2.1944440076959895 	 ± 0.23707407466852923
	data : 0.11482219696044922
	model : 0.06504588127136231
			 train-loss:  2.1930360396703086 	 ± 0.23723285376372985
	data : 0.11491103172302246
	model : 0.06508584022521972
			 train-loss:  2.1916018918940896 	 ± 0.2374277801466793
	data : 0.11470990180969239
	model : 0.06508212089538574
			 train-loss:  2.1922955906203905 	 ± 0.23699839891260255
	data : 0.11444578170776368
	model : 0.06504607200622559
			 train-loss:  2.1928497198969126 	 ± 0.23650443226826035
	data : 0.11421394348144531
	model : 0.06493206024169922
			 train-loss:  2.1945066927628196 	 ± 0.23700565090062073
	data : 0.11383099555969238
	model : 0.06503329277038575
			 train-loss:  2.194205090557177 	 ± 0.23643115246270768
	data : 0.11380457878112793
	model : 0.06498966217041016
			 train-loss:  2.1933085288756935 	 ± 0.2361545392562129
	data : 0.11392326354980468
	model : 0.06501951217651367
			 train-loss:  2.194024396794183 	 ± 0.23576335998063874
	data : 0.11405472755432129
	model : 0.06504063606262207
			 train-loss:  2.193935788222376 	 ± 0.2351674864386582
	data : 0.11413154602050782
	model : 0.06513638496398926
			 train-loss:  2.195992564312135 	 ± 0.23634256793688066
	data : 0.11420097351074218
	model : 0.06502933502197265
			 train-loss:  2.196746602729337 	 ± 0.23598664061427269
	data : 0.11414761543273926
	model : 0.06495037078857421
			 train-loss:  2.1984057503938677 	 ± 0.23655664754818945
	data : 0.11402230262756348
	model : 0.06482210159301757
			 train-loss:  2.1989434672825374 	 ± 0.23608996625930806
	data : 0.1140279769897461
	model : 0.064823579788208
			 train-loss:  2.200246551839432 	 ± 0.23622837052232082
	data : 0.11415209770202636
	model : 0.06482033729553223
			 train-loss:  2.199997295299774 	 ± 0.23567243640966604
	data : 0.11458873748779297
	model : 0.06484994888305665
			 train-loss:  2.199652547345442 	 ± 0.23514540570792095
	data : 0.11465091705322265
	model : 0.06496925354003906
			 train-loss:  2.2003387817522375 	 ± 0.23477586186015562
	data : 0.11476540565490723
	model : 0.06506743431091308
			 train-loss:  2.201759226692533 	 ± 0.2350866967992715
	data : 0.11489129066467285
	model : 0.0650390625
			 train-loss:  2.2031792678694795 	 ± 0.2354021520266397
	data : 0.11481280326843261
	model : 0.06503124237060547
			 train-loss:  2.202724613249302 	 ± 0.234926686489934
	data : 0.11427350044250488
	model : 0.06498632431030274
			 train-loss:  2.2012184635874186 	 ± 0.2353684848755119
	data : 0.11427526473999024
	model : 0.06494626998901368
			 train-loss:  2.2006238789785475 	 ± 0.23496469964748162
	data : 0.11427135467529297
	model : 0.06495089530944824
			 train-loss:  2.2005629562088664 	 ± 0.23440891254479712
	data : 0.11413488388061524
	model : 0.0650181770324707
			 train-loss:  2.2013988348672973 	 ± 0.23417039955955896
	data : 0.11422343254089355
	model : 0.06502785682678222
			 train-loss:  2.2018117020387606 	 ± 0.23369738647204924
	data : 0.11435647010803222
	model : 0.06506357192993165
			 train-loss:  2.200950774076943 	 ± 0.23348904813482835
	data : 0.11437816619873047
	model : 0.06505522727966309
			 train-loss:  2.2025863214980723 	 ± 0.23417092222315386
	data : 0.1144169807434082
	model : 0.06502795219421387
			 train-loss:  2.2019817884321564 	 ± 0.2337963308032889
	data : 0.11452240943908691
	model : 0.06496624946594239
			 train-loss:  2.2015486620538245 	 ± 0.23334385105603234
	data : 0.11429729461669921
	model : 0.06490964889526367
			 train-loss:  2.2010513511272745 	 ± 0.23292327748072136
	data : 0.11430387496948242
	model : 0.06484174728393555
			 train-loss:  2.202452387439606 	 ± 0.23330973884900513
	data : 0.11422843933105468
	model : 0.06478414535522461
			 train-loss:  2.2026968717575075 	 ± 0.23280700104611152
	data : 0.11415624618530273
	model : 0.0647385597229004
			 train-loss:  2.2019504745621488 	 ± 0.23254336987110794
	data : 0.11415572166442871
	model : 0.06473164558410645
			 train-loss:  2.2012277712693087 	 ± 0.2322676467129128
	data : 0.11445860862731934
	model : 0.0646909236907959
			 train-loss:  2.201250686773805 	 ± 0.23174653360761044
	data : 0.11450753211975098
	model : 0.06465291976928711
			 train-loss:  2.2000083742397174 	 ± 0.23197167885754938
	data : 0.11456432342529296
	model : 0.06460127830505372
			 train-loss:  2.200856925116645 	 ± 0.23180377274639283
	data : 0.11462159156799316
	model : 0.06445341110229492
			 train-loss:  2.2015361701492715 	 ± 0.23151466817230157
	data : 0.11462292671203614
	model : 0.06431736946105956
			 train-loss:  2.2012896054641793 	 ± 0.23103389779329303
	data : 0.11465344429016114
	model : 0.06419219970703124
			 train-loss:  2.2004429792103015 	 ± 0.23087932328020716
	data : 0.11475892066955566
	model : 0.06411895751953126
			 train-loss:  2.2006055517488172 	 ± 0.2303877467447385
	data : 0.11488780975341797
	model : 0.06403779983520508
			 train-loss:  2.201266839193261 	 ± 0.2301040626569214
	data : 0.11511521339416504
	model : 0.06404776573181152
			 train-loss:  2.2014923921395173 	 ± 0.22963094107993942
	data : 0.11519427299499511
	model : 0.06397790908813476
			 train-loss:  2.2024044456153082 	 ± 0.2295544345550972
	data : 0.11503424644470214
	model : 0.06391496658325195
			 train-loss:  2.2040783386885354 	 ± 0.2304758633417549
	data : 0.11486644744873047
	model : 0.06386113166809082
			 train-loss:  2.2033713220531106 	 ± 0.23023594219690108
	data : 0.114951753616333
	model : 0.06389904022216797
			 train-loss:  2.2047262364245475 	 ± 0.23067855593085893
	data : 0.1148460865020752
	model : 0.06391410827636719
			 train-loss:  2.2032336309804754 	 ± 0.23132373149266364
	data : 0.11503067016601562
	model : 0.06394963264465332
			 train-loss:  2.202910699924839 	 ± 0.23088849359241662
	data : 0.11520619392395019
	model : 0.06403203010559082
			 train-loss:  2.2019581409061657 	 ± 0.23086912606608145
	data : 0.11545014381408691
	model : 0.06407217979431153
			 train-loss:  2.2021642225058007 	 ± 0.23040756557408376
	data : 0.11530351638793945
	model : 0.06396818161010742
			 train-loss:  2.2013265574971834 	 ± 0.23029144557487843
	data : 0.11528959274291992
	model : 0.06391992568969726
			 train-loss:  2.2017743552868794 	 ± 0.22991784767211548
	data : 0.1150115966796875
	model : 0.06392626762390137
			 train-loss:  2.200667864527584 	 ± 0.23008441781264216
	data : 0.11501364707946778
	model : 0.06390495300292968
			 train-loss:  2.1998279869801713 	 ± 0.2299819328417842
	data : 0.11486878395080566
	model : 0.06389737129211426
			 train-loss:  2.199016580816175 	 ± 0.2298584489324997
	data : 0.11496996879577637
	model : 0.0639920711517334
			 train-loss:  2.1979862037970097 	 ± 0.2299528277831625
	data : 0.11498031616210938
	model : 0.06402769088745117
			 train-loss:  2.198801097831106 	 ± 0.229839168473301
	data : 0.11486082077026367
	model : 0.06393942832946778
			 train-loss:  2.198530549462508 	 ± 0.22941268287269628
	data : 0.11473240852355956
	model : 0.0639376163482666
			 train-loss:  2.197501422897462 	 ± 0.2295202794224189
	data : 0.11472973823547364
	model : 0.06397509574890137
			 train-loss:  2.1970684633676307 	 ± 0.22916038652921475
	data : 0.11463217735290528
	model : 0.06396846771240235
			 train-loss:  2.1986274251937865 	 ± 0.23002083591509637
	data : 0.11464171409606934
	model : 0.06399002075195312
			 train-loss:  2.196859586286355 	 ± 0.23125765586581887
	data : 0.11482057571411133
	model : 0.0640779972076416
			 train-loss:  2.197398610058285 	 ± 0.23095629016619282
	data : 0.11477985382080078
	model : 0.06402401924133301
			 train-loss:  2.1968374143947256 	 ± 0.2306714974993731
	data : 0.11478128433227539
	model : 0.06395597457885742
			 train-loss:  2.1958628653541323 	 ± 0.23073824967835357
	data : 0.11491026878356933
	model : 0.06394877433776855
			 train-loss:  2.1970832081402047 	 ± 0.2311052164650505
	data : 0.11484966278076172
	model : 0.06394281387329101
			 train-loss:  2.1994853573851287 	 ± 0.2338213454728222
	data : 0.11473073959350585
	model : 0.05552144050598144
#epoch  61    val-loss:  2.454923466632241  train-loss:  2.1994853573851287  lr:  3.814697265625e-08
			 train-loss:  2.22947359085083 	 ± 0.0
	data : 5.42954683303833
	model : 0.07170939445495605
			 train-loss:  2.2713645696640015 	 ± 0.04189097881317139
	data : 2.8159834146499634
	model : 0.0685347318649292
			 train-loss:  2.1851731141408286 	 ± 0.12660109299563704
	data : 1.9152912298838298
	model : 0.06729340553283691
			 train-loss:  2.1401678323745728 	 ± 0.13452621945683174
	data : 1.4651715159416199
	model : 0.06662070751190186
			 train-loss:  2.1322532653808595 	 ± 0.12136063801690412
	data : 1.1950042724609375
	model : 0.06623625755310059
			 train-loss:  2.1193318367004395 	 ± 0.11449230095923883
	data : 0.13198256492614746
	model : 0.06488661766052246
			 train-loss:  2.116359370095389 	 ± 0.10624904389338378
	data : 0.11427793502807618
	model : 0.06478562355041503
			 train-loss:  2.09748212993145 	 ± 0.11123040630005462
	data : 0.1142387866973877
	model : 0.06475086212158203
			 train-loss:  2.177227324909634 	 ± 0.24874059380925378
	data : 0.11412835121154785
	model : 0.06476974487304688
			 train-loss:  2.1506615042686463 	 ± 0.2490710343349551
	data : 0.11411967277526855
	model : 0.0648120403289795
			 train-loss:  2.1687798608433115 	 ± 0.24429379376502294
	data : 0.11412210464477539
	model : 0.06479034423828126
			 train-loss:  2.1648448208967843 	 ± 0.23425733599175436
	data : 0.11417140960693359
	model : 0.06478424072265625
			 train-loss:  2.1630701376841617 	 ± 0.22515111432619409
	data : 0.11426253318786621
	model : 0.06480050086975098
			 train-loss:  2.1884540915489197 	 ± 0.23547522259454903
	data : 0.11423697471618652
	model : 0.06480545997619629
			 train-loss:  2.209030763308207 	 ± 0.24016577673600384
	data : 0.11415128707885742
	model : 0.06474838256835938
			 train-loss:  2.2151217237114906 	 ± 0.23373301494445117
	data : 0.1139599323272705
	model : 0.0647590160369873
			 train-loss:  2.1944994435590854 	 ± 0.2412923162183764
	data : 0.11397638320922851
	model : 0.06476917266845703
			 train-loss:  2.184332549571991 	 ± 0.23821134478604958
	data : 0.11398868560791016
	model : 0.06482982635498047
			 train-loss:  2.2161529001436735 	 ± 0.26829779969659423
	data : 0.11412630081176758
	model : 0.06485834121704101
			 train-loss:  2.2246021687984467 	 ± 0.2640850933527718
	data : 0.1140749454498291
	model : 0.06490635871887207
			 train-loss:  2.2215363127844676 	 ± 0.2580851197094436
	data : 0.11416707038879395
	model : 0.06488080024719238
			 train-loss:  2.2500134218822825 	 ± 0.28391927809206385
	data : 0.11409139633178711
	model : 0.06481847763061524
			 train-loss:  2.244278259899305 	 ± 0.2789784810443997
	data : 0.11387748718261718
	model : 0.064793062210083
			 train-loss:  2.2414973924557366 	 ± 0.2734300318696794
	data : 0.11377348899841308
	model : 0.06478872299194335
			 train-loss:  2.2541907739639284 	 ± 0.27502790668189225
	data : 0.11385111808776856
	model : 0.06481657028198243
			 train-loss:  2.246445431159093 	 ± 0.27245341551019614
	data : 0.11399250030517578
	model : 0.06487970352172852
			 train-loss:  2.238435882109183 	 ± 0.27046172955415465
	data : 0.11400079727172852
	model : 0.06494965553283691
			 train-loss:  2.2376926158155714 	 ± 0.26561622566338744
	data : 0.11420860290527343
	model : 0.06493253707885742
			 train-loss:  2.234083631943012 	 ± 0.26169418386851284
	data : 0.11413054466247559
	model : 0.06489968299865723
			 train-loss:  2.232983140150706 	 ± 0.25736389100185847
	data : 0.11399617195129394
	model : 0.06483902931213378
			 train-loss:  2.2273872629288705 	 ± 0.25502732584129495
	data : 0.11384439468383789
	model : 0.06481266021728516
			 train-loss:  2.2287488095462322 	 ± 0.25112534366678463
	data : 0.11401000022888183
	model : 0.06479907035827637
			 train-loss:  2.235278089841207 	 ± 0.2500342333320504
	data : 0.11399641036987304
	model : 0.06480889320373535
			 train-loss:  2.234041063224568 	 ± 0.24643229664162453
	data : 0.11410994529724121
	model : 0.06484360694885254
			 train-loss:  2.233260403360639 	 ± 0.24292897454922202
	data : 0.11423921585083008
	model : 0.06491298675537109
			 train-loss:  2.229787829849455 	 ± 0.24041059089953612
	data : 0.11422290802001953
	model : 0.06492271423339843
			 train-loss:  2.2309627178552986 	 ± 0.2372442988665109
	data : 0.11410160064697265
	model : 0.0649332046508789
			 train-loss:  2.224712224383103 	 ± 0.23716917512215702
	data : 0.11395611763000488
	model : 0.06489143371582032
			 train-loss:  2.2261234521865845 	 ± 0.2342703767558761
	data : 0.11385273933410645
	model : 0.06490941047668457
			 train-loss:  2.228689906001091 	 ± 0.23187803887346561
	data : 0.11395778656005859
	model : 0.06540818214416504
			 train-loss:  2.223701386916928 	 ± 0.23119566866687127
	data : 0.11402816772460937
	model : 0.06541070938110352
			 train-loss:  2.2264980446724665 	 ± 0.22912759986975686
	data : 0.11407527923583985
	model : 0.06539525985717773
			 train-loss:  2.223869853241499 	 ± 0.22708731757898526
	data : 0.11420588493347168
	model : 0.06566729545593261
			 train-loss:  2.2214430543509396 	 ± 0.22505527589837332
	data : 0.1142333984375
	model : 0.06567902565002441
			 train-loss:  2.2202599975797863 	 ± 0.22267893436303493
	data : 0.11417818069458008
	model : 0.06517071723937988
			 train-loss:  2.2276329864626345 	 ± 0.2257303628656669
	data : 0.11410622596740723
	model : 0.06512594223022461
			 train-loss:  2.221118168627962 	 ± 0.22764541734821517
	data : 0.11387429237365723
	model : 0.06516809463500976
			 train-loss:  2.230971244474252 	 ± 0.23517165608598048
	data : 0.11392436027526856
	model : 0.06491847038269043
			 train-loss:  2.2261329816312205 	 ± 0.23516088472285995
	data : 0.11387009620666504
	model : 0.0649327278137207
			 train-loss:  2.2271634244918825 	 ± 0.23290911893167862
	data : 0.11385016441345215
	model : 0.06491355895996094
			 train-loss:  2.2221599864024744 	 ± 0.23331248865570398
	data : 0.11397438049316407
	model : 0.06496410369873047
			 train-loss:  2.217630670620845 	 ± 0.23331126344201417
	data : 0.11418204307556153
	model : 0.06493430137634278
			 train-loss:  2.217544807578033 	 ± 0.23110056180172522
	data : 0.11420912742614746
	model : 0.0649256706237793
			 train-loss:  2.2268496266117803 	 ± 0.2387617311952334
	data : 0.11429009437561036
	model : 0.06478986740112305
			 train-loss:  2.2323148987509986 	 ± 0.239965845826636
	data : 0.11429448127746582
	model : 0.06473817825317382
			 train-loss:  2.229598560503551 	 ± 0.2386653422787826
	data : 0.11432452201843261
	model : 0.0647005558013916
			 train-loss:  2.2406650384267173 	 ± 0.2506391341032761
	data : 0.11432733535766601
	model : 0.06466441154479981
			 train-loss:  2.2351876312288743 	 ± 0.25188685143382217
	data : 0.11426796913146972
	model : 0.06472454071044922
			 train-loss:  2.2310000415575706 	 ± 0.25177111808792896
	data : 0.11418910026550293
	model : 0.06484265327453613
			 train-loss:  2.2307094236214957 	 ± 0.2496741891075109
	data : 0.11409173011779786
	model : 0.06488547325134278
			 train-loss:  2.2277604576017036 	 ± 0.24867059218691123
	data : 0.11391558647155761
	model : 0.06490216255187989
			 train-loss:  2.232149552914404 	 ± 0.2490277194319316
	data : 0.11384902000427247
	model : 0.06486992835998535
			 train-loss:  2.233582816426716 	 ± 0.24730104368351358
	data : 0.11377458572387696
	model : 0.06478915214538575
			 train-loss:  2.240905987098813 	 ± 0.25215239775304343
	data : 0.11384339332580566
	model : 0.06479411125183106
			 train-loss:  2.2374387062512913 	 ± 0.2517381087273482
	data : 0.1137631893157959
	model : 0.06484441757202149
			 train-loss:  2.235343078772227 	 ± 0.2503943888543622
	data : 0.11394228935241699
	model : 0.06480660438537597
			 train-loss:  2.2353611757506187 	 ± 0.24851879253279588
	data : 0.11400938034057617
	model : 0.06486015319824219
			 train-loss:  2.2338071903761696 	 ± 0.24701240432087235
	data : 0.11410222053527833
	model : 0.0649024486541748
			 train-loss:  2.2327936991401343 	 ± 0.2453583050512453
	data : 0.11406512260437011
	model : 0.06482119560241699
			 train-loss:  2.2347972546304975 	 ± 0.24416729935810405
	data : 0.1140212059020996
	model : 0.06469826698303223
			 train-loss:  2.2315060571885446 	 ± 0.24400045742567472
	data : 0.11382999420166015
	model : 0.064691162109375
			 train-loss:  2.231475689344936 	 ± 0.24230022020488926
	data : 0.11376113891601562
	model : 0.06469740867614746
			 train-loss:  2.229092001914978 	 ± 0.24148345685618477
	data : 0.11378622055053711
	model : 0.06471757888793946
			 train-loss:  2.2276170817581384 	 ± 0.24017708638999813
	data : 0.11380467414855958
	model : 0.06475129127502441
			 train-loss:  2.227880295117696 	 ± 0.23858127728290174
	data : 0.11407146453857422
	model : 0.06484227180480957
			 train-loss:  2.2255903153043044 	 ± 0.23783474424305986
	data : 0.11410021781921387
	model : 0.06487803459167481
			 train-loss:  2.223105803712622 	 ± 0.23727596395007952
	data : 0.11407203674316406
	model : 0.06484355926513671
			 train-loss:  2.227225030079866 	 ± 0.23850498461738973
	data : 0.11397080421447754
	model : 0.06478681564331054
			 train-loss:  2.2261367734474473 	 ± 0.23718546478202415
	data : 0.11398096084594726
	model : 0.0648228645324707
			 train-loss:  2.22398334890604 	 ± 0.23647425787652593
	data : 0.11399636268615723
	model : 0.06482024192810058
			 train-loss:  2.2210604158448586 	 ± 0.2364596928883131
	data : 0.11410207748413086
	model : 0.06482820510864258
			 train-loss:  2.2189560794248813 	 ± 0.23577532739395674
	data : 0.11424307823181153
	model : 0.06492671966552735
			 train-loss:  2.2204939270594033 	 ± 0.23476408134487534
	data : 0.11426801681518554
	model : 0.06493711471557617
			 train-loss:  2.2184421093690965 	 ± 0.2341099722485397
	data : 0.11432514190673829
	model : 0.06493573188781739
			 train-loss:  2.217107367515564 	 ± 0.23305006810314857
	data : 0.11417670249938965
	model : 0.0648651123046875
			 train-loss:  2.2201868087746375 	 ± 0.23342417814443167
	data : 0.11420807838439942
	model : 0.06482052803039551
			 train-loss:  2.2186904159085503 	 ± 0.23249329414801717
	data : 0.11409230232238769
	model : 0.06479511260986329
			 train-loss:  2.2187411365183918 	 ± 0.23116901934281328
	data : 0.11408538818359375
	model : 0.06487846374511719
			 train-loss:  2.216873360483834 	 ± 0.23053345025747907
	data : 0.11397905349731445
	model : 0.06488409042358398
			 train-loss:  2.2179320454597473 	 ± 0.22946659159795751
	data : 0.11416916847229004
	model : 0.06494665145874023
			 train-loss:  2.2167088186347876 	 ± 0.2284971701130306
	data : 0.11408886909484864
	model : 0.06498007774353028
			 train-loss:  2.217309338890988 	 ± 0.22732413683288913
	data : 0.11419677734375
	model : 0.06491279602050781
			 train-loss:  2.2145760187538723 	 ± 0.22761357610716984
	data : 0.11410212516784668
	model : 0.06480326652526855
			 train-loss:  2.212750016374791 	 ± 0.22708342124898326
	data : 0.11412134170532226
	model : 0.06479406356811523
			 train-loss:  2.212403510746203 	 ± 0.22591006431537264
	data : 0.11409850120544433
	model : 0.06479482650756836
			 train-loss:  2.2159652337431908 	 ± 0.22739590271688556
	data : 0.11416845321655274
	model : 0.06480741500854492
			 train-loss:  2.216072483161061 	 ± 0.22622316282484636
	data : 0.11415858268737793
	model : 0.06485137939453126
			 train-loss:  2.218978536372282 	 ± 0.22687856889991312
	data : 0.11427831649780273
	model : 0.06491045951843262
			 train-loss:  2.2167711546926787 	 ± 0.2267850416444435
	data : 0.11421985626220703
	model : 0.06488986015319824
			 train-loss:  2.215549476146698 	 ± 0.2259754365213983
	data : 0.11402306556701661
	model : 0.06483578681945801
			 train-loss:  2.213811144970431 	 ± 0.22552490858051685
	data : 0.1139000415802002
	model : 0.0647878646850586
			 train-loss:  2.212964048572615 	 ± 0.22457808706427929
	data : 0.1139101505279541
	model : 0.06482253074645997
			 train-loss:  2.21325663686956 	 ± 0.22350477829957227
	data : 0.1140174388885498
	model : 0.0648390769958496
			 train-loss:  2.2115656550113973 	 ± 0.22308871668523714
	data : 0.11413726806640626
	model : 0.06488428115844727
			 train-loss:  2.2121777738843647 	 ± 0.2221115863442038
	data : 0.1142033576965332
	model : 0.06493730545043945
			 train-loss:  2.21361495188947 	 ± 0.22155139661073872
	data : 0.11423296928405761
	model : 0.06496129035949708
			 train-loss:  2.217538829161742 	 ± 0.22418373041959014
	data : 0.11408472061157227
	model : 0.0648923397064209
			 train-loss:  2.2128957918396703 	 ± 0.22825353023583603
	data : 0.11385116577148438
	model : 0.06483869552612305
			 train-loss:  2.212039611755161 	 ± 0.22737824036697463
	data : 0.113702392578125
	model : 0.06476659774780273
			 train-loss:  2.2125335509126836 	 ± 0.2264010815085536
	data : 0.11376304626464843
	model : 0.06474599838256836
			 train-loss:  2.211999202633763 	 ± 0.22544861714189487
	data : 0.11386547088623047
	model : 0.06476631164550781
			 train-loss:  2.209750914147922 	 ± 0.22568639456094966
	data : 0.11392240524291992
	model : 0.0647843360900879
			 train-loss:  2.2120018195261997 	 ± 0.2259448132704019
	data : 0.11399869918823242
	model : 0.06483478546142578
			 train-loss:  2.2094881241781668 	 ± 0.22653311326627584
	data : 0.11421136856079102
	model : 0.06490659713745117
			 train-loss:  2.2105349208997644 	 ± 0.2258227927873228
	data : 0.11406421661376953
	model : 0.06490755081176758
			 train-loss:  2.208159461103637 	 ± 0.22628574123604325
	data : 0.11382384300231933
	model : 0.0648615837097168
			 train-loss:  2.2093371672508044 	 ± 0.2256733837570244
	data : 0.11390299797058105
	model : 0.06487073898315429
			 train-loss:  2.2108701871613325 	 ± 0.22532608911607035
	data : 0.11397590637207031
	model : 0.06487207412719727
			 train-loss:  2.210836334388797 	 ± 0.22437764481742356
	data : 0.11393423080444336
	model : 0.06487059593200684
			 train-loss:  2.2106985251108804 	 ± 0.22344583924137992
	data : 0.11410861015319824
	model : 0.06490511894226074
			 train-loss:  2.2079260142381525 	 ± 0.2245836873057626
	data : 0.11426434516906739
	model : 0.06498255729675292
			 train-loss:  2.2062608619205286 	 ± 0.22441013423549172
	data : 0.11424088478088379
	model : 0.06502728462219239
			 train-loss:  2.2109841379692883 	 ± 0.22950428594608863
	data : 0.11422052383422851
	model : 0.06499910354614258
			 train-loss:  2.210414070275522 	 ± 0.22866441243556224
	data : 0.11404461860656738
	model : 0.0649172306060791
			 train-loss:  2.2102894220352174 	 ± 0.22775214777730962
	data : 0.11387214660644532
	model : 0.06486520767211915
			 train-loss:  2.2094167007340326 	 ± 0.22705631724034836
	data : 0.11405472755432129
	model : 0.06489367485046386
			 train-loss:  2.2085763591481005 	 ± 0.2263572565986335
	data : 0.11398935317993164
	model : 0.06488103866577148
			 train-loss:  2.2055119117721915 	 ± 0.22810074571507316
	data : 0.1140218734741211
	model : 0.06494417190551757
			 train-loss:  2.206599755804668 	 ± 0.22754800162231653
	data : 0.11425261497497559
	model : 0.06499099731445312
			 train-loss:  2.207322083069728 	 ± 0.2268195464027514
	data : 0.11445331573486328
	model : 0.0650327205657959
			 train-loss:  2.2080618798277762 	 ± 0.22610955204757813
	data : 0.1143460750579834
	model : 0.06497130393981934
			 train-loss:  2.206561643968929 	 ± 0.2259049747334946
	data : 0.11422300338745117
	model : 0.06491284370422364
			 train-loss:  2.2059306300672374 	 ± 0.2251708461970499
	data : 0.11420168876647949
	model : 0.06488142013549805
			 train-loss:  2.204702452937169 	 ± 0.22477579329174965
	data : 0.11410422325134277
	model : 0.06491045951843262
			 train-loss:  2.2021590912783586 	 ± 0.22586878601514396
	data : 0.11408286094665528
	model : 0.06494526863098145
			 train-loss:  2.2017815384794686 	 ± 0.22507960672842006
	data : 0.11401877403259278
	model : 0.06494450569152832
			 train-loss:  2.198823849650195 	 ± 0.22689371927409777
	data : 0.11417431831359863
	model : 0.06502714157104492
			 train-loss:  2.1965649119321853 	 ± 0.22761105950445099
	data : 0.11414074897766113
	model : 0.06507019996643067
			 train-loss:  2.1944884713605153 	 ± 0.22809884964500812
	data : 0.114117431640625
	model : 0.06502447128295899
			 train-loss:  2.191702221121107 	 ± 0.2296443605917199
	data : 0.1139676570892334
	model : 0.06494088172912597
			 train-loss:  2.1910186388813857 	 ± 0.22897147037704127
	data : 0.11384296417236328
	model : 0.06497645378112793
			 train-loss:  2.1913152479789626 	 ± 0.2281909906043061
	data : 0.11391849517822265
	model : 0.06492280960083008
			 train-loss:  2.190628160129894 	 ± 0.2275390761353734
	data : 0.11398816108703613
	model : 0.0648716926574707
			 train-loss:  2.191424763864941 	 ± 0.22694764565743622
	data : 0.11397781372070312
	model : 0.06489129066467285
			 train-loss:  2.1917858288205903 	 ± 0.2262052128884859
	data : 0.11403374671936035
	model : 0.06490082740783691
			 train-loss:  2.1912182847114456 	 ± 0.22553277485152165
	data : 0.11426749229431152
	model : 0.06483469009399415
			 train-loss:  2.1888517173780064 	 ± 0.22657604789680827
	data : 0.1141744613647461
	model : 0.06480507850646973
			 train-loss:  2.189231528623684 	 ± 0.2258562409288615
	data : 0.11407432556152344
	model : 0.06470417976379395
			 train-loss:  2.190385740875398 	 ± 0.22553459171334436
	data : 0.11396698951721192
	model : 0.06466560363769532
			 train-loss:  2.1902833787600198 	 ± 0.22478502530486014
	data : 0.11408052444458008
	model : 0.06469311714172363
			 train-loss:  2.1915409904442087 	 ± 0.2245682994545017
	data : 0.11398873329162598
	model : 0.06470708847045899
			 train-loss:  2.1920776986762096 	 ± 0.2239255125231803
	data : 0.11402630805969238
	model : 0.06472392082214355
			 train-loss:  2.191483822523379 	 ± 0.22331259307910373
	data : 0.11400895118713379
	model : 0.06483325958251954
			 train-loss:  2.191275417804718 	 ± 0.22260129802709708
	data : 0.11419763565063476
	model : 0.06484880447387695
			 train-loss:  2.192051523731601 	 ± 0.22209099994279047
	data : 0.11411733627319336
	model : 0.0648007869720459
			 train-loss:  2.192329471692061 	 ± 0.2214050691863157
	data : 0.11412782669067383
	model : 0.06480531692504883
			 train-loss:  2.1923033242013044 	 ± 0.22069907275359577
	data : 0.11405043601989746
	model : 0.06483840942382812
			 train-loss:  2.194130184529703 	 ± 0.2211871977578541
	data : 0.11418218612670898
	model : 0.06485552787780761
			 train-loss:  2.1934276804234245 	 ± 0.22066729400566437
	data : 0.1140859603881836
	model : 0.06493539810180664
			 train-loss:  2.193551031500101 	 ± 0.2199821266941233
	data : 0.11414923667907714
	model : 0.06497688293457031
			 train-loss:  2.197965305784474 	 ± 0.22629470737161084
	data : 0.11418361663818359
	model : 0.06495480537414551
			 train-loss:  2.1982822602177845 	 ± 0.2256310304587317
	data : 0.1141754150390625
	model : 0.06487827301025391
			 train-loss:  2.2013536004201035 	 ± 0.22830944285252702
	data : 0.11414098739624023
	model : 0.06480679512023926
			 train-loss:  2.199709704736384 	 ± 0.22857789736108644
	data : 0.11414036750793458
	model : 0.06477947235107422
			 train-loss:  2.1973544164137406 	 ± 0.22987164314012867
	data : 0.11416606903076172
	model : 0.06481761932373047
			 train-loss:  2.196949306740818 	 ± 0.22923728316823222
	data : 0.11418075561523437
	model : 0.06486587524414063
			 train-loss:  2.196578769626732 	 ± 0.2285997689576214
	data : 0.11433634757995606
	model : 0.0649643898010254
			 train-loss:  2.196534785486403 	 ± 0.22791910580181055
	data : 0.11448006629943848
	model : 0.06501235961914062
			 train-loss:  2.1962437657914924 	 ± 0.2272750926127418
	data : 0.11464629173278809
	model : 0.06502318382263184
			 train-loss:  2.1960603138979744 	 ± 0.22661819979893363
	data : 0.11467342376708985
	model : 0.06500091552734374
			 train-loss:  2.1961824656927096 	 ± 0.22596021485007348
	data : 0.11451959609985352
	model : 0.06495018005371093
			 train-loss:  2.195973666601403 	 ± 0.225318940274759
	data : 0.11443142890930176
	model : 0.06486358642578124
			 train-loss:  2.1952541715147866 	 ± 0.22486485802413453
	data : 0.11423449516296387
	model : 0.06484994888305665
			 train-loss:  2.198955507114016 	 ± 0.2294421182809187
	data : 0.11414055824279785
	model : 0.06482586860656739
			 train-loss:  2.1995515918731687 	 ± 0.22892070623996535
	data : 0.11400389671325684
	model : 0.0648308277130127
			 train-loss:  2.1984473805535925 	 ± 0.22873633355480655
	data : 0.11414189338684082
	model : 0.0648808479309082
			 train-loss:  2.197833447806579 	 ± 0.22823464258578086
	data : 0.11428465843200683
	model : 0.06499595642089843
			 train-loss:  2.198503487565544 	 ± 0.22776714084035754
	data : 0.11453409194946289
	model : 0.06502103805541992
			 train-loss:  2.1982637583876454 	 ± 0.2271525470148157
	data : 0.11443204879760742
	model : 0.06498022079467773
			 train-loss:  2.19902992910809 	 ± 0.22675250528249322
	data : 0.1142923355102539
	model : 0.06489009857177734
			 train-loss:  2.1986097024949216 	 ± 0.22619552331278367
	data : 0.1142603874206543
	model : 0.06487789154052734
			 train-loss:  2.196755430855594 	 ± 0.22694851426751442
	data : 0.1142038345336914
	model : 0.06483001708984375
			 train-loss:  2.198583855003607 	 ± 0.22766779999341738
	data : 0.11394367218017579
	model : 0.06487855911254883
			 train-loss:  2.198080593477125 	 ± 0.22715033986160538
	data : 0.11396732330322265
	model : 0.06490278244018555
			 train-loss:  2.200264325657406 	 ± 0.2284640255424556
	data : 0.11424922943115234
	model : 0.0649984359741211
			 train-loss:  2.1998878954559244 	 ± 0.22790656588024108
	data : 0.11431961059570313
	model : 0.06498122215270996
			 train-loss:  2.1985386960646687 	 ± 0.2280399620958758
	data : 0.11414313316345215
	model : 0.06494507789611817
			 train-loss:  2.198688287684258 	 ± 0.22744186368741126
	data : 0.11399879455566406
	model : 0.06481223106384278
			 train-loss:  2.197835990360805 	 ± 0.22714018561606286
	data : 0.11403613090515137
	model : 0.06484088897705079
			 train-loss:  2.1970241659565977 	 ± 0.22681641411156048
	data : 0.1139669418334961
	model : 0.0648430347442627
			 train-loss:  2.1990128936567856 	 ± 0.22787670737709412
	data : 0.1139113426208496
	model : 0.06486616134643555
			 train-loss:  2.201134577393532 	 ± 0.22916616461571865
	data : 0.11414885520935059
	model : 0.06488456726074218
			 train-loss:  2.20149485187827 	 ± 0.228626207327252
	data : 0.11466464996337891
	model : 0.06497321128845215
			 train-loss:  2.2010490562497953 	 ± 0.22812028755660888
	data : 0.1146470546722412
	model : 0.06500344276428223
			 train-loss:  2.200636750001174 	 ± 0.2276070713474753
	data : 0.11473598480224609
	model : 0.06500740051269531
			 train-loss:  2.2037446985439377 	 ± 0.23113684240849583
	data : 0.11460223197937011
	model : 0.06493124961853028
			 train-loss:  2.2032466927155627 	 ± 0.23065485211729012
	data : 0.1143765926361084
	model : 0.06492009162902831
			 train-loss:  2.2046594619750977 	 ± 0.23092457901629315
	data : 0.11454334259033203
	model : 0.06490554809570312
			 train-loss:  2.2034909940844205 	 ± 0.23092969350534429
	data : 0.11487369537353516
	model : 0.06489200592041015
			 train-loss:  2.202219288945198 	 ± 0.23104915122982753
	data : 0.11475825309753418
	model : 0.0649099349975586
			 train-loss:  2.2012615091172023 	 ± 0.23087136705399142
	data : 0.11493682861328125
	model : 0.06501727104187012
			 train-loss:  2.1994441960117603 	 ± 0.23173594216003437
	data : 0.11511101722717285
	model : 0.06507034301757812
			 train-loss:  2.200066669821152 	 ± 0.23133369172023976
	data : 0.11473417282104492
	model : 0.06509218215942383
			 train-loss:  2.199535982281554 	 ± 0.2308898392671238
	data : 0.11450529098510742
	model : 0.06508064270019531
			 train-loss:  2.199700955646794 	 ± 0.23033805727320525
	data : 0.11447052955627442
	model : 0.0650263786315918
			 train-loss:  2.1986050640495076 	 ± 0.23031341611489195
	data : 0.11424441337585449
	model : 0.06494135856628418
			 train-loss:  2.1981405785694212 	 ± 0.2298531291172051
	data : 0.1142033576965332
	model : 0.06485934257507324
			 train-loss:  2.1976497012835283 	 ± 0.22940866935301862
	data : 0.11406426429748535
	model : 0.06483488082885742
			 train-loss:  2.1997113296289763 	 ± 0.23078256460383514
	data : 0.11400666236877441
	model : 0.06481757164001464
			 train-loss:  2.20017816112155 	 ± 0.2303313221668618
	data : 0.1140702247619629
	model : 0.06486654281616211
			 train-loss:  2.1994683821619403 	 ± 0.2300149545848928
	data : 0.11422348022460938
	model : 0.06491904258728028
			 train-loss:  2.199879226819524 	 ± 0.22954941510340063
	data : 0.11415157318115235
	model : 0.06495246887207032
			 train-loss:  2.200464219554489 	 ± 0.22916827704933154
	data : 0.11417407989501953
	model : 0.0649538516998291
			 train-loss:  2.1996297435225727 	 ± 0.22895634983554866
	data : 0.11411762237548828
	model : 0.06492786407470703
			 train-loss:  2.1991638582806257 	 ± 0.22852492200371666
	data : 0.11403031349182129
	model : 0.06492290496826172
			 train-loss:  2.2003203950546406 	 ± 0.22862511451582843
	data : 0.11420636177062989
	model : 0.06492514610290527
			 train-loss:  2.19930978139974 	 ± 0.2285807940520443
	data : 0.11419816017150879
	model : 0.06495466232299804
			 train-loss:  2.1997967515516716 	 ± 0.2281687174858171
	data : 0.11429324150085449
	model : 0.0649324893951416
			 train-loss:  2.199838451598877 	 ± 0.22764802090693662
	data : 0.11441602706909179
	model : 0.06496376991271972
			 train-loss:  2.2004635339433496 	 ± 0.2273183426949329
	data : 0.11459617614746094
	model : 0.06490683555603027
			 train-loss:  2.2004450710650483 	 ± 0.22680363002661227
	data : 0.11437973976135254
	model : 0.06495141983032227
			 train-loss:  2.202531896732949 	 ± 0.22840883374938042
	data : 0.11444001197814942
	model : 0.06480364799499512
			 train-loss:  2.2009545794516936 	 ± 0.2291047072707685
	data : 0.11434097290039062
	model : 0.06475539207458496
			 train-loss:  2.2018049040011 	 ± 0.22894515000045906
	data : 0.11442375183105469
	model : 0.06462931632995605
			 train-loss:  2.2013465563456216 	 ± 0.22853879480492334
	data : 0.11440486907958984
	model : 0.06450724601745605
			 train-loss:  2.2000836223627616 	 ± 0.22881815952104295
	data : 0.11452713012695312
	model : 0.0642622470855713
			 train-loss:  2.1997854063689446 	 ± 0.22835761001777297
	data : 0.11468095779418945
	model : 0.06422843933105468
			 train-loss:  2.2005955122019114 	 ± 0.2281829439148613
	data : 0.11481900215148926
	model : 0.06405868530273437
			 train-loss:  2.2007493800992006 	 ± 0.22769603656151283
	data : 0.11458544731140137
	model : 0.06396026611328125
			 train-loss:  2.200092265398606 	 ± 0.22741801171295042
	data : 0.11470561027526856
	model : 0.06396245956420898
			 train-loss:  2.202269942729504 	 ± 0.2293159055808732
	data : 0.11477451324462891
	model : 0.0639418125152588
			 train-loss:  2.202074318610389 	 ± 0.22884047233782173
	data : 0.11472783088684083
	model : 0.06393556594848633
			 train-loss:  2.203241447010777 	 ± 0.22903980954873346
	data : 0.11486167907714843
	model : 0.06396031379699707
			 train-loss:  2.2031428197510223 	 ± 0.22855484257674208
	data : 0.11518287658691406
	model : 0.06394157409667969
			 train-loss:  2.2022426316078674 	 ± 0.2284833667209009
	data : 0.11503658294677735
	model : 0.06385159492492676
			 train-loss:  2.200975028640133 	 ± 0.2288253587138916
	data : 0.11488242149353027
	model : 0.06377573013305664
			 train-loss:  2.2005261937274208 	 ± 0.22844617520963922
	data : 0.1148993968963623
	model : 0.06372809410095215
			 train-loss:  2.201300541392895 	 ± 0.2282772167726525
	data : 0.11477108001708984
	model : 0.06372485160827637
			 train-loss:  2.2009434206216407 	 ± 0.2278657621769061
	data : 0.11467423439025878
	model : 0.06375617980957031
			 train-loss:  2.200783913830916 	 ± 0.2274039166037744
	data : 0.11478581428527831
	model : 0.06380367279052734
			 train-loss:  2.199740054696427 	 ± 0.22750709984715373
	data : 0.11495442390441894
	model : 0.06394901275634765
			 train-loss:  2.1992935582625965 	 ± 0.2271423427388265
	data : 0.1148538589477539
	model : 0.06392555236816407
			 train-loss:  2.1981895941275136 	 ± 0.22732412685678494
	data : 0.11485872268676758
	model : 0.06388888359069825
			 train-loss:  2.1970146317951014 	 ± 0.2275960040755915
	data : 0.11488814353942871
	model : 0.06387028694152833
			 train-loss:  2.1959396527737987 	 ± 0.22775090391688174
	data : 0.1149165153503418
	model : 0.06385216712951661
			 train-loss:  2.1961403600568694 	 ± 0.2273092344911173
	data : 0.1148681640625
	model : 0.06378359794616699
			 train-loss:  2.1973100490415627 	 ± 0.2275892577979289
	data : 0.11499090194702148
	model : 0.06379680633544922
			 train-loss:  2.196183347413617 	 ± 0.22781915566321043
	data : 0.1149404525756836
	model : 0.06383242607116699
			 train-loss:  2.1953284328721137 	 ± 0.22775949072826393
	data : 0.1148989200592041
	model : 0.06382851600646973
			 train-loss:  2.194850068092346 	 ± 0.2274288184401855
	data : 0.11494522094726563
	model : 0.06383657455444336
			 train-loss:  2.1959501348168726 	 ± 0.22764079812637586
	data : 0.11505355834960937
	model : 0.06383452415466309
			 train-loss:  2.197998983519418 	 ± 0.22949583822402714
	data : 0.1149815559387207
	model : 0.06384692192077637
			 train-loss:  2.19775685962481 	 ± 0.22907408790436704
	data : 0.11503267288208008
	model : 0.06388554573059083
			 train-loss:  2.1977448228775986 	 ± 0.2286227901185296
	data : 0.11509180068969727
	model : 0.06389226913452148
			 train-loss:  2.1978022266836725 	 ± 0.22817590385875025
	data : 0.11494636535644531
	model : 0.06386957168579102
			 train-loss:  2.1952760284766555 	 ± 0.23127514973366978
	data : 0.11448497772216797
	model : 0.05540895462036133
#epoch  62    val-loss:  2.4327696499071623  train-loss:  2.1952760284766555  lr:  1.9073486328125e-08
			 train-loss:  2.351142406463623 	 ± 0.0
	data : 5.422877311706543
	model : 0.07628536224365234
			 train-loss:  2.2330785989761353 	 ± 0.11806380748748779
	data : 2.7751165628433228
	model : 0.07258462905883789
			 train-loss:  2.119452118873596 	 ± 0.18738906688121854
	data : 1.8883214791615803
	model : 0.06995820999145508
			 train-loss:  2.197174996137619 	 ± 0.2108519237297953
	data : 1.4445445537567139
	model : 0.06853353977203369
			 train-loss:  2.221481680870056 	 ± 0.1947564804651509
	data : 1.1783047199249268
	model : 0.06769824028015137
			 train-loss:  2.1955451369285583 	 ± 0.18700782638022617
	data : 0.11656675338745118
	model : 0.0653918743133545
			 train-loss:  2.1947707618985857 	 ± 0.17314599469217415
	data : 0.11384387016296386
	model : 0.06458673477172852
			 train-loss:  2.166487216949463 	 ± 0.1784147021616295
	data : 0.11372489929199218
	model : 0.06464800834655762
			 train-loss:  2.155782275729709 	 ± 0.1709143199152095
	data : 0.11389679908752441
	model : 0.06475830078125
			 train-loss:  2.1543526887893676 	 ± 0.16220027048760022
	data : 0.11405658721923828
	model : 0.06485457420349121
			 train-loss:  2.161870176141912 	 ± 0.15646831373477108
	data : 0.11390514373779297
	model : 0.06481809616088867
			 train-loss:  2.1636751691500344 	 ± 0.14992657124996978
	data : 0.11423234939575196
	model : 0.0648221492767334
			 train-loss:  2.1718785212590146 	 ± 0.1468211206221828
	data : 0.11414899826049804
	model : 0.06481499671936035
			 train-loss:  2.16885278906141 	 ± 0.14190035715754684
	data : 0.11412491798400878
	model : 0.06484127044677734
			 train-loss:  2.1568653265635174 	 ± 0.14423980103961417
	data : 0.11414456367492676
	model : 0.06487507820129394
			 train-loss:  2.1387891992926598 	 ± 0.15622418458063506
	data : 0.11424894332885742
	model : 0.06496505737304688
			 train-loss:  2.125512543846579 	 ± 0.16059471445426204
	data : 0.11395392417907715
	model : 0.06501288414001465
			 train-loss:  2.1115043560663858 	 ± 0.1664143811720608
	data : 0.11403827667236328
	model : 0.06500425338745117
			 train-loss:  2.118120507190102 	 ± 0.16439009359038445
	data : 0.11395530700683594
	model : 0.06499881744384765
			 train-loss:  2.127316784858704 	 ± 0.16516584593469977
	data : 0.113822603225708
	model : 0.06497282981872558
			 train-loss:  2.1240916706266857 	 ± 0.16182937887766038
	data : 0.11388921737670898
	model : 0.06501898765563965
			 train-loss:  2.1226891495964746 	 ± 0.15823924484025073
	data : 0.11398916244506836
	model : 0.06501588821411133
			 train-loss:  2.1370430407316787 	 ± 0.16877123568444594
	data : 0.11393537521362304
	model : 0.06501836776733398
			 train-loss:  2.1374293764432273 	 ± 0.16522814767114866
	data : 0.11403017044067383
	model : 0.06502604484558105
			 train-loss:  2.1255300092697142 	 ± 0.17206570166842408
	data : 0.11410555839538575
	model : 0.0650597095489502
			 train-loss:  2.120953243512374 	 ± 0.17026908030261936
	data : 0.11399631500244141
	model : 0.06494841575622559
			 train-loss:  2.1337134264133595 	 ± 0.1793074934178668
	data : 0.11378283500671386
	model : 0.06492204666137695
			 train-loss:  2.140465544802802 	 ± 0.17953796434596572
	data : 0.1138685703277588
	model : 0.06493577957153321
			 train-loss:  2.139189304976628 	 ± 0.17654453710764914
	data : 0.11378254890441894
	model : 0.06496958732604981
			 train-loss:  2.1417778213818868 	 ± 0.17413601932980685
	data : 0.11379489898681641
	model : 0.06493501663208008
			 train-loss:  2.1510945250911098 	 ± 0.17874341829398677
	data : 0.11384587287902832
	model : 0.0649827003479004
			 train-loss:  2.1516942642629147 	 ± 0.17596007241200315
	data : 0.1139286994934082
	model : 0.06498885154724121
			 train-loss:  2.145013892289364 	 ± 0.17734650290629717
	data : 0.11371502876281739
	model : 0.06493821144104003
			 train-loss:  2.149075034786673 	 ± 0.17626966461999183
	data : 0.11383543014526368
	model : 0.06487183570861817
			 train-loss:  2.1497529336384367 	 ± 0.1737782395857798
	data : 0.11386442184448242
	model : 0.06492280960083008
			 train-loss:  2.143281857172648 	 ± 0.17557231542416615
	data : 0.1139214038848877
	model : 0.06491918563842773
			 train-loss:  2.1474782582875847 	 ± 0.1750041849699858
	data : 0.11400899887084961
	model : 0.06491203308105468
			 train-loss:  2.1411394225923637 	 ± 0.17693839651999632
	data : 0.11411867141723633
	model : 0.06497759819030761
			 train-loss:  2.1467455197603273 	 ± 0.1780413544962358
	data : 0.11412663459777832
	model : 0.06508808135986328
			 train-loss:  2.1475918024778364 	 ± 0.17588117408114823
	data : 0.11392073631286621
	model : 0.06499762535095215
			 train-loss:  2.158777356147766 	 ± 0.1875749435739271
	data : 0.11394481658935547
	model : 0.06496057510375977
			 train-loss:  2.1654795550164723 	 ± 0.19023231815140249
	data : 0.1137969970703125
	model : 0.06494650840759278
			 train-loss:  2.163131910701131 	 ± 0.18862191258485156
	data : 0.11383352279663086
	model : 0.0649198055267334
			 train-loss:  2.1652394831180573 	 ± 0.1869776187355161
	data : 0.11386656761169434
	model : 0.06484179496765137
			 train-loss:  2.1553628974490695 	 ± 0.19615247278110215
	data : 0.11408061981201172
	model : 0.06491608619689941
			 train-loss:  2.154698263043943 	 ± 0.19405988909319902
	data : 0.11401886940002441
	model : 0.06498045921325683
			 train-loss:  2.147613872873022 	 ± 0.19790567193954686
	data : 0.11409392356872558
	model : 0.06496410369873047
			 train-loss:  2.1517993981639543 	 ± 0.19792437505052368
	data : 0.11400389671325684
	model : 0.06489667892456055
			 train-loss:  2.157548040759807 	 ± 0.19990207218581818
	data : 0.11380424499511718
	model : 0.06485576629638672
			 train-loss:  2.1567009234428407 	 ± 0.19798177813775408
	data : 0.11377573013305664
	model : 0.06484532356262207
			 train-loss:  2.1651299911386825 	 ± 0.2048918719329266
	data : 0.11387925148010254
	model : 0.06480083465576172
			 train-loss:  2.1686839759349823 	 ± 0.20449334863311738
	data : 0.11392607688903808
	model : 0.0648223876953125
			 train-loss:  2.1710279730131044 	 ± 0.20325900816885248
	data : 0.11400256156921387
	model : 0.06485548019409179
			 train-loss:  2.1747601054332875 	 ± 0.20319294581608255
	data : 0.11412057876586915
	model : 0.06485834121704101
			 train-loss:  2.172019696235657 	 ± 0.20234185235625404
	data : 0.11399869918823242
	model : 0.0647740364074707
			 train-loss:  2.1693950103861943 	 ± 0.20146961984345793
	data : 0.11381120681762695
	model : 0.06477999687194824
			 train-loss:  2.168224495753907 	 ± 0.19988653824987718
	data : 0.11385226249694824
	model : 0.06477785110473633
			 train-loss:  2.164766321922171 	 ± 0.19986849764346334
	data : 0.11397171020507812
	model : 0.06479549407958984
			 train-loss:  2.164403218334004 	 ± 0.19818675147792764
	data : 0.11398248672485352
	model : 0.06482539176940919
			 train-loss:  2.1624160587787626 	 ± 0.197120102229915
	data : 0.11412239074707031
	model : 0.06488261222839356
			 train-loss:  2.164823389444195 	 ± 0.196384979226014
	data : 0.11425108909606933
	model : 0.06487188339233399
			 train-loss:  2.166242785992161 	 ± 0.19510998587947315
	data : 0.11412277221679687
	model : 0.06478924751281738
			 train-loss:  2.170235794688028 	 ± 0.19609229828818156
	data : 0.11396026611328125
	model : 0.06476840972900391
			 train-loss:  2.171813739463687 	 ± 0.1949570165443243
	data : 0.11400456428527832
	model : 0.06476130485534667
			 train-loss:  2.1705587808902447 	 ± 0.19371187655257824
	data : 0.11398663520812988
	model : 0.06476211547851562
			 train-loss:  2.1707385940985247 	 ± 0.19224422717846693
	data : 0.1139533519744873
	model : 0.06480116844177246
			 train-loss:  2.170482188908022 	 ± 0.19081554540237017
	data : 0.11409511566162109
	model : 0.0648895263671875
			 train-loss:  2.172514103791293 	 ± 0.1901361202028436
	data : 0.11420969963073731
	model : 0.06488361358642578
			 train-loss:  2.1764011573100435 	 ± 0.19145555742230638
	data : 0.11398725509643555
	model : 0.06481714248657226
			 train-loss:  2.173844567367009 	 ± 0.19126572828341692
	data : 0.11386318206787109
	model : 0.06475839614868165
			 train-loss:  2.170961228894516 	 ± 0.1914400325927631
	data : 0.11389875411987305
	model : 0.06473002433776856
			 train-loss:  2.1725131968657174 	 ± 0.19055518660573478
	data : 0.11383638381958008
	model : 0.06472735404968262
			 train-loss:  2.174954754032501 	 ± 0.1903761299645489
	data : 0.11381330490112304
	model : 0.0647552490234375
			 train-loss:  2.1808706876393913 	 ± 0.195724738790578
	data : 0.11403336524963378
	model : 0.06480121612548828
			 train-loss:  2.178294569651286 	 ± 0.19567444937294223
	data : 0.11416120529174804
	model : 0.06486310958862304
			 train-loss:  2.176258187544973 	 ± 0.19518121866809265
	data : 0.11411290168762207
	model : 0.06483969688415528
			 train-loss:  2.177621804274522 	 ± 0.19427371646498134
	data : 0.11403179168701172
	model : 0.06476516723632812
			 train-loss:  2.173675841245896 	 ± 0.19610542988229612
	data : 0.11405434608459472
	model : 0.06475887298583985
			 train-loss:  2.1746540959877305 	 ± 0.1950517430949396
	data : 0.11401314735412597
	model : 0.06481800079345704
			 train-loss:  2.174929554760456 	 ± 0.19384429841659492
	data : 0.11406035423278808
	model : 0.0648263931274414
			 train-loss:  2.1713850144986755 	 ± 0.19523528584048244
	data : 0.11406850814819336
	model : 0.06484260559082031
			 train-loss:  2.171928667440647 	 ± 0.19410285378561346
	data : 0.11426153182983398
	model : 0.0649024486541748
			 train-loss:  2.1731406522084433 	 ± 0.19324192481707148
	data : 0.11421990394592285
	model : 0.06490411758422851
			 train-loss:  2.172502108982631 	 ± 0.1921763014535752
	data : 0.11416373252868653
	model : 0.06480679512023926
			 train-loss:  2.174413002238554 	 ± 0.19184360032145156
	data : 0.11398653984069824
	model : 0.06480493545532226
			 train-loss:  2.1755645219669786 	 ± 0.19102021799550195
	data : 0.11393780708312988
	model : 0.0648162841796875
			 train-loss:  2.1756325891648216 	 ± 0.18992027673825895
	data : 0.11388072967529297
	model : 0.06482062339782715
			 train-loss:  2.1793678890575063 	 ± 0.19202524029669943
	data : 0.1139561653137207
	model : 0.06480846405029297
			 train-loss:  2.180605226688171 	 ± 0.1912958706898308
	data : 0.1142077922821045
	model : 0.06488227844238281
			 train-loss:  2.179845248328315 	 ± 0.19036520793394088
	data : 0.11444931030273438
	model : 0.06481800079345704
			 train-loss:  2.174851683469919 	 ± 0.1951535156864475
	data : 0.11442313194274903
	model : 0.06486635208129883
			 train-loss:  2.1747022595094596 	 ± 0.19409523498149847
	data : 0.11426849365234375
	model : 0.06482949256896972
			 train-loss:  2.1734648686583324 	 ± 0.19341338932818283
	data : 0.11415839195251465
	model : 0.06483006477355957
			 train-loss:  2.1777477708268673 	 ± 0.19676558693711768
	data : 0.11405625343322753
	model : 0.0648153305053711
			 train-loss:  2.1794854151575187 	 ± 0.19645095155590128
	data : 0.11390018463134766
	model : 0.06488537788391113
			 train-loss:  2.1790647643307843 	 ± 0.1954680950256084
	data : 0.11398887634277344
	model : 0.06487040519714356
			 train-loss:  2.1801310433554897 	 ± 0.19473836002502878
	data : 0.11411685943603515
	model : 0.0649254322052002
			 train-loss:  2.18250126376444 	 ± 0.1951435364732299
	data : 0.1141552448272705
	model : 0.06495270729064942
			 train-loss:  2.1812301296176333 	 ± 0.1945628173656167
	data : 0.11406679153442383
	model : 0.06498918533325196
			 train-loss:  2.180176101922989 	 ± 0.19387142508879565
	data : 0.11409163475036621
	model : 0.06492242813110352
			 train-loss:  2.1806570362336566 	 ± 0.1929692186629144
	data : 0.11408796310424804
	model : 0.0648566722869873
			 train-loss:  2.182306896237766 	 ± 0.19273550631671982
	data : 0.1141127109527588
	model : 0.06480631828308106
			 train-loss:  2.180827047060994 	 ± 0.19237905399075694
	data : 0.11417417526245117
	model : 0.06482348442077637
			 train-loss:  2.180661282860316 	 ± 0.19145931191960214
	data : 0.11415925025939941
	model : 0.06479582786560059
			 train-loss:  2.18506171249208 	 ± 0.19575849972339762
	data : 0.11421890258789062
	model : 0.06484541893005372
			 train-loss:  2.184692417675594 	 ± 0.19486966778907114
	data : 0.114201021194458
	model : 0.06489391326904297
			 train-loss:  2.184983448447468 	 ± 0.19398006743171278
	data : 0.1142458438873291
	model : 0.06493072509765625
			 train-loss:  2.1827642398851888 	 ± 0.19443975816699208
	data : 0.1141502857208252
	model : 0.06490945816040039
			 train-loss:  2.182903406816885 	 ± 0.19355118104378793
	data : 0.11420674324035644
	model : 0.06490874290466309
			 train-loss:  2.180852795730938 	 ± 0.1938552066350504
	data : 0.11413674354553223
	model : 0.06488494873046875
			 train-loss:  2.178300961717829 	 ± 0.19482707271702626
	data : 0.1141160011291504
	model : 0.06487746238708496
			 train-loss:  2.1798250004649162 	 ± 0.19461885898034081
	data : 0.11404423713684082
	model : 0.06486859321594238
			 train-loss:  2.1809463342734143 	 ± 0.1941188749917317
	data : 0.11408500671386719
	model : 0.06486992835998535
			 train-loss:  2.182387053966522 	 ± 0.19387146163527294
	data : 0.11409244537353516
	model : 0.06487483978271484
			 train-loss:  2.1838436634644216 	 ± 0.1936522209661507
	data : 0.11411118507385254
	model : 0.0649040699005127
			 train-loss:  2.183345154441636 	 ± 0.19288980123572977
	data : 0.11407904624938965
	model : 0.06487030982971191
			 train-loss:  2.1826806200875177 	 ± 0.19219702818368262
	data : 0.11406631469726562
	model : 0.06487822532653809
			 train-loss:  2.1820808073221625 	 ± 0.19149084331604382
	data : 0.11411728858947753
	model : 0.06489458084106445
			 train-loss:  2.181336534123461 	 ± 0.19085588175877063
	data : 0.11414780616760253
	model : 0.06489510536193847
			 train-loss:  2.179828426241875 	 ± 0.1907696784303512
	data : 0.11424641609191895
	model : 0.06492400169372559
			 train-loss:  2.1781962499145635 	 ± 0.19081923640124876
	data : 0.11440377235412598
	model : 0.0649728775024414
			 train-loss:  2.178846577151877 	 ± 0.19017017670403605
	data : 0.11447000503540039
	model : 0.06496944427490234
			 train-loss:  2.1769670974917528 	 ± 0.19052987254179865
	data : 0.11435079574584961
	model : 0.06497416496276856
			 train-loss:  2.1779197492907123 	 ± 0.1900539537123857
	data : 0.11425323486328125
	model : 0.06493282318115234
			 train-loss:  2.1805174407958985 	 ± 0.19148966923177424
	data : 0.11403937339782715
	model : 0.06484756469726563
			 train-loss:  2.1859302520751953 	 ± 0.20009895293051616
	data : 0.11382508277893066
	model : 0.06480212211608886
			 train-loss:  2.1840031100070383 	 ± 0.20048009137942588
	data : 0.11370768547058105
	model : 0.064825439453125
			 train-loss:  2.182561098597944 	 ± 0.20035555485201698
	data : 0.11377768516540528
	model : 0.06483125686645508
			 train-loss:  2.1819186182909234 	 ± 0.19970979777158837
	data : 0.11380529403686523
	model : 0.06492276191711426
			 train-loss:  2.180812840278332 	 ± 0.19933624208125866
	data : 0.11400489807128907
	model : 0.06499814987182617
			 train-loss:  2.1819071778814303 	 ± 0.1989655799299009
	data : 0.11414680480957032
	model : 0.06502275466918946
			 train-loss:  2.1849130328857536 	 ± 0.2011740657193533
	data : 0.11410460472106934
	model : 0.06493916511535644
			 train-loss:  2.1850192089726153 	 ± 0.2004200577118106
	data : 0.11404080390930176
	model : 0.06489100456237792
			 train-loss:  2.1830696766056232 	 ± 0.20093264248987835
	data : 0.11390900611877441
	model : 0.06486954689025878
			 train-loss:  2.1866675014849064 	 ± 0.20447347693746046
	data : 0.11384010314941406
	model : 0.06487784385681153
			 train-loss:  2.1891927429858375 	 ± 0.2058223863379053
	data : 0.11372780799865723
	model : 0.06486659049987793
			 train-loss:  2.18930856241797 	 ± 0.20507428196470284
	data : 0.11380748748779297
	model : 0.06493558883666992
			 train-loss:  2.1896713669749275 	 ± 0.20437403045428887
	data : 0.11383004188537597
	model : 0.06492815017700196
			 train-loss:  2.187881666121723 	 ± 0.20471997406827258
	data : 0.11378703117370606
	model : 0.06484599113464355
			 train-loss:  2.185791948011943 	 ± 0.20546997555388535
	data : 0.11363372802734376
	model : 0.06484041213989258
			 train-loss:  2.1878704339899917 	 ± 0.2062118027688287
	data : 0.1137270450592041
	model : 0.06483850479125977
			 train-loss:  2.188566203688232 	 ± 0.20565044407969937
	data : 0.11376008987426758
	model : 0.06483240127563476
			 train-loss:  2.190193158763272 	 ± 0.20584515472318876
	data : 0.11377501487731934
	model : 0.06485390663146973
			 train-loss:  2.1903472584154873 	 ± 0.2051374464111951
	data : 0.113948392868042
	model : 0.06488714218139649
			 train-loss:  2.189876517756232 	 ± 0.20450688373907608
	data : 0.11412677764892579
	model : 0.06485109329223633
			 train-loss:  2.1882812715556526 	 ± 0.20470858227689986
	data : 0.11404876708984375
	model : 0.06481060981750489
			 train-loss:  2.187838233247095 	 ± 0.20408132896898631
	data : 0.11397662162780761
	model : 0.06476211547851562
			 train-loss:  2.1869407006212183 	 ± 0.2036815981462617
	data : 0.11401510238647461
	model : 0.06475977897644043
			 train-loss:  2.1852461171630244 	 ± 0.20404107756379403
	data : 0.11397480964660645
	model : 0.0647815227508545
			 train-loss:  2.1836289978027343 	 ± 0.20431557887239027
	data : 0.114007568359375
	model : 0.06481866836547852
			 train-loss:  2.183780987531144 	 ± 0.20364642128727597
	data : 0.11403303146362305
	model : 0.06489677429199218
			 train-loss:  2.1868099303621995 	 ± 0.206359812905495
	data : 0.11412992477416992
	model : 0.06495542526245117
			 train-loss:  2.1844069014966876 	 ± 0.20780706052938336
	data : 0.11404571533203126
	model : 0.06495695114135742
			 train-loss:  2.1832767624359626 	 ± 0.20760244235971895
	data : 0.11402254104614258
	model : 0.06489081382751465
			 train-loss:  2.180247925173852 	 ± 0.21031759593882607
	data : 0.11397719383239746
	model : 0.06487851142883301
			 train-loss:  2.1815962363512087 	 ± 0.21031339546263852
	data : 0.1139312744140625
	model : 0.06482062339782715
			 train-loss:  2.1821054971901472 	 ± 0.20973900841963666
	data : 0.1138007640838623
	model : 0.06482181549072266
			 train-loss:  2.180558711667604 	 ± 0.20997061867997815
	data : 0.11407995223999023
	model : 0.06482696533203125
			 train-loss:  2.1807917408973165 	 ± 0.20932978679144476
	data : 0.1140909194946289
	model : 0.06490607261657715
			 train-loss:  2.1817223906517027 	 ± 0.20900431207490003
	data : 0.11407456398010254
	model : 0.06490960121154785
			 train-loss:  2.18155310613028 	 ± 0.20836522231727286
	data : 0.11412005424499512
	model : 0.0649374008178711
			 train-loss:  2.1819028942673295 	 ± 0.20776853490118638
	data : 0.11409544944763184
	model : 0.06487488746643066
			 train-loss:  2.18499567903624 	 ± 0.21083764522013054
	data : 0.1139073371887207
	model : 0.06485748291015625
			 train-loss:  2.183343209144546 	 ± 0.21124998883956878
	data : 0.1138718605041504
	model : 0.06485753059387207
			 train-loss:  2.182355440746654 	 ± 0.21098840339941696
	data : 0.11395382881164551
	model : 0.06485671997070312
			 train-loss:  2.183343817670661 	 ± 0.21073472359652576
	data : 0.11401629447937012
	model : 0.06486859321594238
			 train-loss:  2.1813885266195516 	 ± 0.21160776428670286
	data : 0.1141127109527588
	model : 0.06493625640869141
			 train-loss:  2.1816701108501073 	 ± 0.21100841804470905
	data : 0.11398863792419434
	model : 0.06494379043579102
			 train-loss:  2.180722792473065 	 ± 0.21074121293505804
	data : 0.11397042274475097
	model : 0.06487421989440918
			 train-loss:  2.1829979139215805 	 ± 0.21219186274751517
	data : 0.11366109848022461
	model : 0.06476917266845703
			 train-loss:  2.1832035284990456 	 ± 0.21158749351518513
	data : 0.11356277465820312
	model : 0.06477794647216797
			 train-loss:  2.181776513887006 	 ± 0.21179518494008537
	data : 0.11357522010803223
	model : 0.06475586891174316
			 train-loss:  2.181875661618448 	 ± 0.21118617614604285
	data : 0.11372718811035157
	model : 0.0648338794708252
			 train-loss:  2.182255316054684 	 ± 0.21063764439238564
	data : 0.11376433372497559
	model : 0.06488323211669922
			 train-loss:  2.1807071052278793 	 ± 0.21102548434308793
	data : 0.11429634094238281
	model : 0.06498932838439941
			 train-loss:  2.1809294758872553 	 ± 0.21044568712863684
	data : 0.11470427513122558
	model : 0.06497159004211425
			 train-loss:  2.179774582722766 	 ± 0.21040893747822015
	data : 0.11472644805908203
	model : 0.06497378349304199
			 train-loss:  2.1807553667700694 	 ± 0.21022241846214287
	data : 0.11464624404907227
	model : 0.06489005088806152
			 train-loss:  2.1805634998076453 	 ± 0.2096500108720835
	data : 0.11464390754699708
	model : 0.06481976509094238
			 train-loss:  2.179783477385839 	 ± 0.20932714320079276
	data : 0.11432600021362305
	model : 0.0647778034210205
			 train-loss:  2.180308550760891 	 ± 0.20886692333752474
	data : 0.11396517753601074
	model : 0.06475982666015626
			 train-loss:  2.179722527226249 	 ± 0.20844148190519537
	data : 0.11399612426757813
	model : 0.06475086212158203
			 train-loss:  2.1806214643957835 	 ± 0.20822464659178178
	data : 0.114082670211792
	model : 0.06477227210998535
			 train-loss:  2.1795173559499825 	 ± 0.20819450519111138
	data : 0.11405248641967773
	model : 0.06484990119934082
			 train-loss:  2.1800926955970557 	 ± 0.20777767407622347
	data : 0.11411824226379394
	model : 0.0648686408996582
			 train-loss:  2.1801442523156442 	 ± 0.20721956572905
	data : 0.11415882110595703
	model : 0.06481008529663086
			 train-loss:  2.179928089845627 	 ± 0.20668578592639442
	data : 0.11393518447875976
	model : 0.06476740837097168
			 train-loss:  2.180348661351711 	 ± 0.2062155715236832
	data : 0.11380929946899414
	model : 0.06474652290344238
			 train-loss:  2.178906274851037 	 ± 0.2066179873268034
	data : 0.11394791603088379
	model : 0.06475539207458496
			 train-loss:  2.180244418821837 	 ± 0.2068930452841504
	data : 0.11393966674804687
	model : 0.06475601196289063
			 train-loss:  2.1819978028691875 	 ± 0.20776128419124176
	data : 0.11392025947570801
	model : 0.06484408378601074
			 train-loss:  2.1822383161634207 	 ± 0.20724619056905813
	data : 0.11413564682006835
	model : 0.0649792194366455
			 train-loss:  2.1832871999147643 	 ± 0.2072188932437768
	data : 0.11456451416015626
	model : 0.06499204635620118
			 train-loss:  2.183953837635591 	 ± 0.20689152119574392
	data : 0.11463189125061035
	model : 0.06494827270507812
			 train-loss:  2.184513199023711 	 ± 0.20650736803121197
	data : 0.11450929641723633
	model : 0.06492304801940918
			 train-loss:  2.1864518942881603 	 ± 0.20775136628779245
	data : 0.11489992141723633
	model : 0.06488776206970215
			 train-loss:  2.1852819278155486 	 ± 0.20786974220480153
	data : 0.11464891433715821
	model : 0.06473293304443359
			 train-loss:  2.1852008092283 	 ± 0.20734728010128445
	data : 0.11455264091491699
	model : 0.0647280216217041
			 train-loss:  2.1843523499953688 	 ± 0.20716994646169434
	data : 0.11435604095458984
	model : 0.06479501724243164
			 train-loss:  2.1840927243232726 	 ± 0.20668382490119838
	data : 0.1144707202911377
	model : 0.06485724449157715
			 train-loss:  2.1822708619767752 	 ± 0.20777274022884898
	data : 0.11405277252197266
	model : 0.06494345664978027
			 train-loss:  2.1813106554569583 	 ± 0.20770441121345495
	data : 0.11435699462890625
	model : 0.06502876281738282
			 train-loss:  2.1801596580467786 	 ± 0.20783698706133297
	data : 0.1141617774963379
	model : 0.0650644302368164
			 train-loss:  2.1791830577102362 	 ± 0.2077933539258652
	data : 0.11426491737365722
	model : 0.0650106430053711
			 train-loss:  2.1797844305271057 	 ± 0.2074638031449701
	data : 0.11417264938354492
	model : 0.06493072509765625
			 train-loss:  2.1792573408015725 	 ± 0.20709718839515814
	data : 0.11406331062316895
	model : 0.06488723754882812
			 train-loss:  2.1794253282500926 	 ± 0.20661041671229383
	data : 0.11398539543151856
	model : 0.0648923397064209
			 train-loss:  2.1810336777797112 	 ± 0.20740805031767776
	data : 0.11391687393188477
	model : 0.06485867500305176
			 train-loss:  2.1827395669581215 	 ± 0.2083688152075672
	data : 0.11397252082824708
	model : 0.06489310264587403
			 train-loss:  2.1834480228878204 	 ± 0.20812427056782723
	data : 0.11399850845336915
	model : 0.06491847038269043
			 train-loss:  2.184073230101599 	 ± 0.20782807778890147
	data : 0.11414432525634766
	model : 0.06488804817199707
			 train-loss:  2.184142914583098 	 ± 0.20733980868055976
	data : 0.11409401893615723
	model : 0.064837646484375
			 train-loss:  2.1853742789774433 	 ± 0.20762806406619563
	data : 0.11410694122314453
	model : 0.06482625007629395
			 train-loss:  2.1840710378138817 	 ± 0.20801378398262016
	data : 0.11400327682495118
	model : 0.06482057571411133
			 train-loss:  2.1840762143911316 	 ± 0.20752948099740395
	data : 0.1140634536743164
	model : 0.06487350463867188
			 train-loss:  2.1836613565683365 	 ± 0.20713787040128645
	data : 0.11404781341552735
	model : 0.06490421295166016
			 train-loss:  2.1838774532766387 	 ± 0.20668444577544662
	data : 0.11412534713745118
	model : 0.0649446964263916
			 train-loss:  2.182446601193979 	 ± 0.20728428826972753
	data : 0.11410822868347167
	model : 0.06493244171142579
			 train-loss:  2.182985809840024 	 ± 0.2069636766649775
	data : 0.11415162086486816
	model : 0.06488599777221679
			 train-loss:  2.184965000369332 	 ± 0.20855965420693837
	data : 0.11402254104614258
	model : 0.06475162506103516
			 train-loss:  2.1861373946677505 	 ± 0.20881259928235266
	data : 0.11411404609680176
	model : 0.06517620086669922
			 train-loss:  2.1875067530451595 	 ± 0.20933394257988389
	data : 0.1135711669921875
	model : 0.0650609016418457
			 train-loss:  2.1868077030096353 	 ± 0.20912359763647753
	data : 0.11363034248352051
	model : 0.06504583358764648
			 train-loss:  2.1876881229025975 	 ± 0.20907008383034736
	data : 0.11372532844543456
	model : 0.06494512557983398
			 train-loss:  2.1874083434210885 	 ± 0.20864698872040166
	data : 0.11399235725402831
	model : 0.06489944458007812
			 train-loss:  2.1862469858827844 	 ± 0.20891244231482242
	data : 0.11411538124084472
	model : 0.06426510810852051
			 train-loss:  2.185135072023333 	 ± 0.2091209173811558
	data : 0.11472034454345703
	model : 0.06414799690246582
			 train-loss:  2.185600186126274 	 ± 0.20877945342690565
	data : 0.11466255187988281
	model : 0.0638920783996582
			 train-loss:  2.1857088419027204 	 ± 0.20832956485193582
	data : 0.11464767456054688
	model : 0.06381659507751465
			 train-loss:  2.1861855532812036 	 ± 0.2080013168105279
	data : 0.11470847129821778
	model : 0.06373744010925293
			 train-loss:  2.186076573479227 	 ± 0.20755718970342107
	data : 0.1146977424621582
	model : 0.06375293731689453
			 train-loss:  2.1853271471015336 	 ± 0.2074223618955632
	data : 0.11480445861816406
	model : 0.0637575626373291
			 train-loss:  2.1853682416703055 	 ± 0.20697771737929616
	data : 0.11498966217041015
	model : 0.06385278701782227
			 train-loss:  2.1851802926797133 	 ± 0.20655490846668056
	data : 0.11503376960754394
	model : 0.06376638412475585
			 train-loss:  2.185238521149818 	 ± 0.20611688603064038
	data : 0.11462492942810058
	model : 0.06372990608215331
			 train-loss:  2.1862962685399134 	 ± 0.20631790512865233
	data : 0.11448326110839843
	model : 0.06367464065551758
			 train-loss:  2.1874668311469163 	 ± 0.20666601370693438
	data : 0.11439118385314942
	model : 0.06367402076721192
			 train-loss:  2.187906626392813 	 ± 0.20634249298648674
	data : 0.11439728736877441
	model : 0.06365046501159669
			 train-loss:  2.187309809309668 	 ± 0.2061161090247837
	data : 0.1143716812133789
	model : 0.06375575065612793
			 train-loss:  2.1864163984855014 	 ± 0.20614946051217686
	data : 0.11463255882263183
	model : 0.06379499435424804
			 train-loss:  2.185956554294127 	 ± 0.2058446285022845
	data : 0.11457381248474122
	model : 0.06375346183776856
			 train-loss:  2.185655568256851 	 ± 0.2054720247227373
	data : 0.11462197303771973
	model : 0.06374759674072265
			 train-loss:  2.1852803730670316 	 ± 0.2051318598018976
	data : 0.11465539932250976
	model : 0.0637772560119629
			 train-loss:  2.186015094889969 	 ± 0.20503121710228733
	data : 0.114646577835083
	model : 0.0637272834777832
			 train-loss:  2.186592179901746 	 ± 0.20481082916778884
	data : 0.11466302871704101
	model : 0.06374688148498535
			 train-loss:  2.1884768900832507 	 ± 0.2065120577715504
	data : 0.11481747627258301
	model : 0.06376008987426758
			 train-loss:  2.1887146760577614 	 ± 0.20612733571401232
	data : 0.11476035118103027
	model : 0.06375002861022949
			 train-loss:  2.189297681854617 	 ± 0.2059152939217378
	data : 0.11457414627075195
	model : 0.06369686126708984
			 train-loss:  2.1896567832992737 	 ± 0.20557918972912634
	data : 0.11454362869262695
	model : 0.06374564170837402
			 train-loss:  2.1904251976013183 	 ± 0.20552561083972593
	data : 0.11453146934509277
	model : 0.06372933387756348
			 train-loss:  2.1901407783249933 	 ± 0.20516508071592696
	data : 0.11444473266601562
	model : 0.0637892246246338
			 train-loss:  2.1912017663319907 	 ± 0.20544640508806652
	data : 0.11438207626342774
	model : 0.06375565528869628
			 train-loss:  2.1904628606653027 	 ± 0.2053752219537533
	data : 0.11444973945617676
	model : 0.06371331214904785
			 train-loss:  2.189716025600283 	 ± 0.20531448256405785
	data : 0.11441421508789062
	model : 0.06368756294250488
			 train-loss:  2.1895171277663286 	 ± 0.20493602692300544
	data : 0.11447420120239257
	model : 0.06371307373046875
			 train-loss:  2.187788238748908 	 ± 0.2063902299297223
	data : 0.11432337760925293
	model : 0.05530104637145996
#epoch  63    val-loss:  2.4365877665971456  train-loss:  2.187788238748908  lr:  1.9073486328125e-08
			 train-loss:  2.054635763168335 	 ± 0.0
	data : 4.443681478500366
	model : 0.0723564624786377
			 train-loss:  2.212025284767151 	 ± 0.15738952159881592
	data : 2.815522789955139
	model : 0.07008707523345947
			 train-loss:  2.161672751108805 	 ± 0.14691855895143313
	data : 1.9138788382212322
	model : 0.068144957224528
			 train-loss:  2.1435402035713196 	 ± 0.13105405397724956
	data : 1.4637559056282043
	model : 0.06730020046234131
			 train-loss:  2.169952726364136 	 ± 0.12857144897768558
	data : 1.1938611030578614
	model : 0.06678209304809571
			 train-loss:  2.2093492348988852 	 ± 0.1467512827899444
	data : 0.3279129981994629
	model : 0.065299654006958
			 train-loss:  2.2017320905412947 	 ± 0.13714044485535312
	data : 0.11324710845947265
	model : 0.06469578742980957
			 train-loss:  2.1754648983478546 	 ± 0.14589832628691612
	data : 0.11401424407958985
	model : 0.06546592712402344
			 train-loss:  2.1879594326019287 	 ± 0.14202141337561905
	data : 0.11374950408935547
	model : 0.06538023948669433
			 train-loss:  2.1576945185661316 	 ± 0.16247079388565602
	data : 0.1135869026184082
	model : 0.06540989875793457
			 train-loss:  2.15823833508925 	 ± 0.1549193689585131
	data : 0.11355485916137695
	model : 0.0653531551361084
			 train-loss:  2.1686677237351737 	 ± 0.15230398417233484
	data : 0.11346373558044434
	model : 0.06538200378417969
			 train-loss:  2.1686623188165517 	 ± 0.14632893601389266
	data : 0.1134211540222168
	model : 0.06473336219787598
			 train-loss:  2.1628064683505466 	 ± 0.142578050531292
	data : 0.11382431983947754
	model : 0.06489758491516114
			 train-loss:  2.1394206364949544 	 ± 0.16318647912488302
	data : 0.11397995948791503
	model : 0.06494407653808594
			 train-loss:  2.158138245344162 	 ± 0.17384100781772233
	data : 0.11397128105163574
	model : 0.06502642631530761
			 train-loss:  2.15170620469486 	 ± 0.17060171848420452
	data : 0.11405110359191895
	model : 0.06497316360473633
			 train-loss:  2.1365353729989796 	 ± 0.17720221547641155
	data : 0.1139399528503418
	model : 0.06502389907836914
			 train-loss:  2.1433040280091133 	 ± 0.17485029344774378
	data : 0.11384968757629395
	model : 0.06493444442749023
			 train-loss:  2.1453541576862336 	 ± 0.17065711665497912
	data : 0.1137618064880371
	model : 0.06485066413879395
			 train-loss:  2.168049375216166 	 ± 0.19503450815137544
	data : 0.11387691497802735
	model : 0.0648195743560791
			 train-loss:  2.18189476294951 	 ± 0.20083582469425235
	data : 0.11385645866394042
	model : 0.06485924720764161
			 train-loss:  2.180687805880671 	 ± 0.19650287526703306
	data : 0.11390056610107421
	model : 0.06483597755432129
			 train-loss:  2.17754565179348 	 ± 0.1929548423814365
	data : 0.11393070220947266
	model : 0.06488208770751953
			 train-loss:  2.1842260122299195 	 ± 0.19186808365115107
	data : 0.11407337188720704
	model : 0.06492671966552735
			 train-loss:  2.1693627742620616 	 ± 0.20228781247529956
	data : 0.11393013000488281
	model : 0.06490583419799804
			 train-loss:  2.1781045154288963 	 ± 0.2034494021266714
	data : 0.11397647857666016
	model : 0.06489629745483398
			 train-loss:  2.1748048748288835 	 ± 0.200517709044201
	data : 0.11392393112182617
	model : 0.06485004425048828
			 train-loss:  2.1876744237439385 	 ± 0.20846678233881724
	data : 0.11381087303161622
	model : 0.06483559608459473
			 train-loss:  2.1802717288335165 	 ± 0.20880369463583903
	data : 0.11374063491821289
	model : 0.0648867130279541
			 train-loss:  2.1852928361585064 	 ± 0.2072411871555221
	data : 0.11383333206176757
	model : 0.06491689682006836
			 train-loss:  2.1776962354779243 	 ± 0.20831638155906262
	data : 0.11383605003356934
	model : 0.06494150161743165
			 train-loss:  2.1735050678253174 	 ± 0.20650133540929833
	data : 0.11390938758850097
	model : 0.06495466232299804
			 train-loss:  2.172559043940376 	 ± 0.20351445943776428
	data : 0.11398196220397949
	model : 0.06489219665527343
			 train-loss:  2.1698838370186944 	 ± 0.20119167179413633
	data : 0.11377019882202148
	model : 0.0648571491241455
			 train-loss:  2.17248217927085 	 ± 0.19897234941031308
	data : 0.11380276679992676
	model : 0.06483335494995117
			 train-loss:  2.1598468922279976 	 ± 0.21039822521627002
	data : 0.11382584571838379
	model : 0.06486034393310547
			 train-loss:  2.1579212075785588 	 ± 0.20794154688079228
	data : 0.11378259658813476
	model : 0.06484966278076172
			 train-loss:  2.1547541924012013 	 ± 0.20618466585381526
	data : 0.11390047073364258
	model : 0.06490349769592285
			 train-loss:  2.1528711557388305 	 ± 0.20393038218524395
	data : 0.11409673690795899
	model : 0.06492156982421875
			 train-loss:  2.16366492248163 	 ± 0.2126816519397949
	data : 0.11403579711914062
	model : 0.0648641586303711
			 train-loss:  2.1635020119803294 	 ± 0.21013706352074862
	data : 0.11378746032714844
	model : 0.0647822380065918
			 train-loss:  2.1654902613440226 	 ± 0.2080785826683383
	data : 0.11385912895202636
	model : 0.0648836612701416
			 train-loss:  2.162210155617107 	 ± 0.2068219566636114
	data : 0.11384429931640624
	model : 0.06489357948303223
			 train-loss:  2.170166720284356 	 ± 0.21121141917474254
	data : 0.11385750770568848
	model : 0.06485934257507324
			 train-loss:  2.183820190637008 	 ± 0.22809921442452083
	data : 0.11386699676513672
	model : 0.06491875648498535
			 train-loss:  2.177320160764329 	 ± 0.22992556215294235
	data : 0.11410293579101563
	model : 0.06501102447509766
			 train-loss:  2.176007946332296 	 ± 0.22769568213058355
	data : 0.11402130126953125
	model : 0.06546549797058106
			 train-loss:  2.1756911277770996 	 ± 0.22537096925502453
	data : 0.11332058906555176
	model : 0.06550545692443847
			 train-loss:  2.1721722173690794 	 ± 0.22446154704049898
	data : 0.11340169906616211
	model : 0.0654942512512207
			 train-loss:  2.1664440211127785 	 ± 0.2259108143252941
	data : 0.113468599319458
	model : 0.0655275821685791
			 train-loss:  2.172702885591067 	 ± 0.22814925585827875
	data : 0.11344428062438965
	model : 0.06546921730041504
			 train-loss:  2.1738071666573577 	 ± 0.22612690879920683
	data : 0.11347322463989258
	model : 0.06491703987121582
			 train-loss:  2.1693380452968456 	 ± 0.2263736642882256
	data : 0.11416187286376953
	model : 0.06484642028808593
			 train-loss:  2.1672446554357356 	 ± 0.22483316289662444
	data : 0.11390032768249511
	model : 0.06480064392089843
			 train-loss:  2.1659250131675174 	 ± 0.2230315078718456
	data : 0.11377434730529785
	model : 0.06474642753601074
			 train-loss:  2.163504638169941 	 ± 0.2218071874435573
	data : 0.11379575729370117
	model : 0.06475710868835449
			 train-loss:  2.1648736000061035 	 ± 0.2201295092827895
	data : 0.11390056610107421
	model : 0.06480779647827148
			 train-loss:  2.160067281480563 	 ± 0.22130417128560528
	data : 0.11392641067504883
	model : 0.06484403610229492
			 train-loss:  2.159531154235204 	 ± 0.21949085585563272
	data : 0.11415190696716308
	model : 0.06489577293395996
			 train-loss:  2.154691391303891 	 ± 0.22088879500932204
	data : 0.11419477462768554
	model : 0.06490087509155273
			 train-loss:  2.156700268868477 	 ± 0.2196612510523848
	data : 0.11404900550842285
	model : 0.0648573398590088
			 train-loss:  2.1663445896572537 	 ± 0.23076389783809775
	data : 0.11379842758178711
	model : 0.06483240127563476
			 train-loss:  2.1705693677067757 	 ± 0.23139659814845198
	data : 0.1137195110321045
	model : 0.06480493545532226
			 train-loss:  2.172333057110126 	 ± 0.2300428313936702
	data : 0.11363296508789063
	model : 0.06480498313903808
			 train-loss:  2.1720850576053965 	 ± 0.22830218640762812
	data : 0.11364130973815918
	model : 0.06482720375061035
			 train-loss:  2.172707621730975 	 ± 0.2266484731227063
	data : 0.11384391784667969
	model : 0.06486215591430664
			 train-loss:  2.169642713140039 	 ± 0.2263702092406755
	data : 0.11399450302124023
	model : 0.06483044624328613
			 train-loss:  2.1675657068473706 	 ± 0.2253756011018449
	data : 0.11390557289123535
	model : 0.06481456756591797
			 train-loss:  2.1692774619374955 	 ± 0.22421130205930506
	data : 0.11386251449584961
	model : 0.06477980613708496
			 train-loss:  2.17109129630344 	 ± 0.2231433832498427
	data : 0.11384105682373047
	model : 0.06478796005249024
			 train-loss:  2.171795247329606 	 ± 0.22166773402520937
	data : 0.11376895904541015
	model : 0.06477770805358887
			 train-loss:  2.1697011140927875 	 ± 0.2208602018975306
	data : 0.11377487182617188
	model : 0.06482067108154296
			 train-loss:  2.167419886266863 	 ± 0.22022702387226717
	data : 0.11391229629516601
	model : 0.06488509178161621
			 train-loss:  2.1662544361750284 	 ± 0.21898353458547107
	data : 0.11397624015808105
	model : 0.06495141983032227
			 train-loss:  2.166129198513533 	 ± 0.21754078664627072
	data : 0.11394195556640625
	model : 0.06488099098205566
			 train-loss:  2.1747856558143317 	 ± 0.22892007286372337
	data : 0.1138218879699707
	model : 0.06486210823059083
			 train-loss:  2.178479416248126 	 ± 0.22974578597639858
	data : 0.11384639739990235
	model : 0.0648345947265625
			 train-loss:  2.177398465856721 	 ± 0.22848659638732766
	data : 0.11387734413146973
	model : 0.06484675407409668
			 train-loss:  2.1880195900797843 	 ± 0.24589713758141798
	data : 0.11389880180358887
	model : 0.06479649543762207
			 train-loss:  2.1898905009399225 	 ± 0.24494681140744534
	data : 0.11397109031677247
	model : 0.06479549407958984
			 train-loss:  2.1964562011928095 	 ± 0.250517520401261
	data : 0.11398091316223144
	model : 0.06480331420898437
			 train-loss:  2.1955615052257675 	 ± 0.2491355747214981
	data : 0.11384549140930175
	model : 0.06475563049316406
			 train-loss:  2.196719046149935 	 ± 0.24787261899609114
	data : 0.1137204647064209
	model : 0.06473212242126465
			 train-loss:  2.19723083411946 	 ± 0.24645487163755928
	data : 0.1136538028717041
	model : 0.0647538185119629
			 train-loss:  2.198526353337044 	 ± 0.24530875666012106
	data : 0.11380186080932617
	model : 0.06483607292175293
			 train-loss:  2.1958570302217857 	 ± 0.24514786983003836
	data : 0.11400871276855469
	model : 0.06489901542663574
			 train-loss:  2.1955590668049725 	 ± 0.243766848127699
	data : 0.11409587860107422
	model : 0.06501536369323731
			 train-loss:  2.200097522039092 	 ± 0.2461040367428799
	data : 0.11411581039428711
	model : 0.06493148803710938
			 train-loss:  2.2004349059528776 	 ± 0.24475366949292854
	data : 0.11398348808288575
	model : 0.06487340927124023
			 train-loss:  2.19924844621302 	 ± 0.24366526399584432
	data : 0.11374249458312988
	model : 0.0648622989654541
			 train-loss:  2.1977374592553014 	 ± 0.24276565903361355
	data : 0.11375527381896973
	model : 0.0648266315460205
			 train-loss:  2.203140752289885 	 ± 0.24695636600220217
	data : 0.11382994651794434
	model : 0.0647770881652832
			 train-loss:  2.2042059555966804 	 ± 0.2458539551976677
	data : 0.11397452354431152
	model : 0.06486334800720214
			 train-loss:  2.204215337100782 	 ± 0.24455658070686886
	data : 0.11416072845458984
	model : 0.06492819786071777
			 train-loss:  2.2051880372067294 	 ± 0.24346417767372355
	data : 0.11420998573303223
	model : 0.06489362716674804
			 train-loss:  2.2063374949484755 	 ± 0.24246765894790315
	data : 0.11403360366821289
	model : 0.06483678817749024
			 train-loss:  2.2037953223500932 	 ± 0.24252327338244692
	data : 0.11396918296813965
	model : 0.06481633186340333
			 train-loss:  2.201375709639655 	 ± 0.2424812675256836
	data : 0.11394987106323243
	model : 0.0648531436920166
			 train-loss:  2.1992556977272035 	 ± 0.2421861764226818
	data : 0.11407370567321777
	model : 0.06489782333374024
			 train-loss:  2.200736175669302 	 ± 0.24143858731452825
	data : 0.11417946815490723
	model : 0.06490821838378906
			 train-loss:  2.200319488843282 	 ± 0.2402886428212757
	data : 0.11419534683227539
	model : 0.06498923301696777
			 train-loss:  2.2004811740615993 	 ± 0.2391249236751513
	data : 0.1142850399017334
	model : 0.06498889923095703
			 train-loss:  2.201939594287139 	 ± 0.23843236848975352
	data : 0.11416358947753906
	model : 0.06487455368041992
			 train-loss:  2.2013052213759647 	 ± 0.23738243077591067
	data : 0.11408123970031739
	model : 0.06482925415039062
			 train-loss:  2.1991203168653093 	 ± 0.2373184791174289
	data : 0.11397356986999511
	model : 0.06487040519714356
			 train-loss:  2.19510890724503 	 ± 0.23979031252686786
	data : 0.11413955688476562
	model : 0.06493310928344727
			 train-loss:  2.194935682747099 	 ± 0.23868431642100182
	data : 0.11414074897766113
	model : 0.06495466232299804
			 train-loss:  2.1943718063721964 	 ± 0.23765916730168832
	data : 0.11419668197631835
	model : 0.06501011848449707
			 train-loss:  2.1952607729218223 	 ± 0.2367584141574191
	data : 0.11410813331604004
	model : 0.06503157615661621
			 train-loss:  2.193232688817892 	 ± 0.23664740548533084
	data : 0.11411070823669434
	model : 0.06503820419311523
			 train-loss:  2.19403484250818 	 ± 0.23574011055599678
	data : 0.1138676643371582
	model : 0.06487479209899902
			 train-loss:  2.192730857207712 	 ± 0.23510006811087342
	data : 0.11379132270812989
	model : 0.0648472785949707
			 train-loss:  2.1943989239240946 	 ± 0.2347373340869998
	data : 0.11383376121520997
	model : 0.06486983299255371
			 train-loss:  2.1936209388401195 	 ± 0.2338620774246058
	data : 0.11381244659423828
	model : 0.06487412452697754
			 train-loss:  2.193617646036477 	 ± 0.23285187202114527
	data : 0.11388850212097168
	model : 0.06485872268676758
			 train-loss:  2.1934243939880633 	 ± 0.231863985401157
	data : 0.11401047706604003
	model : 0.0649503231048584
			 train-loss:  2.193134180570053 	 ± 0.2309007600703169
	data : 0.1140291690826416
	model : 0.06490144729614258
			 train-loss:  2.1907130970674404 	 ± 0.23142776105419485
	data : 0.11377091407775879
	model : 0.06481380462646484
			 train-loss:  2.191639228661855 	 ± 0.23068279916274675
	data : 0.11379361152648926
	model : 0.064776611328125
			 train-loss:  2.1928842146534566 	 ± 0.2301320554438331
	data : 0.11372346878051758
	model : 0.0647542953491211
			 train-loss:  2.191000232931043 	 ± 0.23012199845146278
	data : 0.1136899471282959
	model : 0.06477398872375488
			 train-loss:  2.18883730531708 	 ± 0.23042643882684438
	data : 0.11373381614685059
	model : 0.0648622989654541
			 train-loss:  2.188442249451914 	 ± 0.22953723857792263
	data : 0.1139462947845459
	model : 0.06496496200561523
			 train-loss:  2.187567102432251 	 ± 0.22882485561838226
	data : 0.1138838768005371
	model : 0.06498026847839355
			 train-loss:  2.186897113209679 	 ± 0.22803807406514476
	data : 0.11379504203796387
	model : 0.06495962142944336
			 train-loss:  2.190458872186856 	 ± 0.23063034421742923
	data : 0.11386690139770508
	model : 0.06491127014160156
			 train-loss:  2.1880134362727404 	 ± 0.23137477376547308
	data : 0.11381316184997559
	model : 0.06489992141723633
			 train-loss:  2.186870519504991 	 ± 0.23083867221259893
	data : 0.11393723487854004
	model : 0.06493334770202637
			 train-loss:  2.186692577141982 	 ± 0.22995799851613238
	data : 0.11406068801879883
	model : 0.06492810249328614
			 train-loss:  2.1885258368863405 	 ± 0.23003026133108986
	data : 0.1142313003540039
	model : 0.06495018005371093
			 train-loss:  2.1864852661436256 	 ± 0.23034437866472854
	data : 0.1141294002532959
	model : 0.0649655818939209
			 train-loss:  2.1858605755899188 	 ± 0.2295889977758763
	data : 0.11404118537902833
	model : 0.0648984432220459
			 train-loss:  2.186114767594124 	 ± 0.2287495027936518
	data : 0.1139369010925293
	model : 0.0648280143737793
			 train-loss:  2.1862108875204016 	 ± 0.22790342379622555
	data : 0.11389098167419434
	model : 0.06483697891235352
			 train-loss:  2.1871656240785824 	 ± 0.2273348070260473
	data : 0.11383905410766601
	model : 0.06484522819519042
			 train-loss:  2.18726168148709 	 ± 0.22650636845079192
	data : 0.11391825675964355
	model : 0.06484446525573731
			 train-loss:  2.188182268453681 	 ± 0.2259412831208502
	data : 0.1140350341796875
	model : 0.0648867130279541
			 train-loss:  2.190746095540712 	 ± 0.22713278930922498
	data : 0.11394915580749512
	model : 0.06486902236938477
			 train-loss:  2.1907042137214114 	 ± 0.22632068568521993
	data : 0.11390066146850586
	model : 0.06486401557922364
			 train-loss:  2.1929497287628497 	 ± 0.22707644127786386
	data : 0.114039945602417
	model : 0.06485171318054199
			 train-loss:  2.1929518765127156 	 ± 0.2262754651176892
	data : 0.11408939361572265
	model : 0.0648611068725586
			 train-loss:  2.191703697184583 	 ± 0.22597293906501065
	data : 0.11409506797790528
	model : 0.06488685607910157
			 train-loss:  2.192065455019474 	 ± 0.22522849266903466
	data : 0.11424922943115234
	model : 0.06489415168762207
			 train-loss:  2.1920039070063626 	 ± 0.22445171418019952
	data : 0.11437978744506835
	model : 0.064854097366333
			 train-loss:  2.1908039156704735 	 ± 0.2241479651279758
	data : 0.11422686576843262
	model : 0.06479606628417969
			 train-loss:  2.1906830980664207 	 ± 0.22338902612671796
	data : 0.11421771049499511
	model : 0.06475682258605957
			 train-loss:  2.192688393431741 	 ± 0.22395668035546476
	data : 0.11420760154724122
	model : 0.0647308349609375
			 train-loss:  2.1901573174751845 	 ± 0.22531780304437837
	data : 0.11422929763793946
	model : 0.06477932929992676
			 train-loss:  2.189863216082255 	 ± 0.22459418099108472
	data : 0.11425638198852539
	model : 0.06483888626098633
			 train-loss:  2.1932406536001245 	 ± 0.22763908312546394
	data : 0.11431298255920411
	model : 0.06492471694946289
			 train-loss:  2.195239559600228 	 ± 0.22821475295131546
	data : 0.11426434516906739
	model : 0.06496500968933105
			 train-loss:  2.197604900871227 	 ± 0.22932942241937904
	data : 0.11439285278320313
	model : 0.06497607231140137
			 train-loss:  2.1969527935052846 	 ± 0.2287259056867767
	data : 0.11431298255920411
	model : 0.06491374969482422
			 train-loss:  2.195754195797828 	 ± 0.2284715788348205
	data : 0.11403942108154297
	model : 0.06482734680175781
			 train-loss:  2.197803856470646 	 ± 0.2291633119735864
	data : 0.11404805183410645
	model : 0.06483950614929199
			 train-loss:  2.2005828201391133 	 ± 0.23105423510140508
	data : 0.11414632797241211
	model : 0.0648338794708252
			 train-loss:  2.2019335997255545 	 ± 0.23094292837734726
	data : 0.114131498336792
	model : 0.06481881141662597
			 train-loss:  2.204581247185761 	 ± 0.2326086500150116
	data : 0.11417765617370605
	model : 0.06483330726623535
			 train-loss:  2.2053330942988394 	 ± 0.23207433128282315
	data : 0.11441388130187988
	model : 0.06489977836608887
			 train-loss:  2.2074889707269136 	 ± 0.232954113363366
	data : 0.11435818672180176
	model : 0.0648188591003418
			 train-loss:  2.205235250202226 	 ± 0.23398802080363856
	data : 0.11431608200073243
	model : 0.06481280326843261
			 train-loss:  2.205287927498847 	 ± 0.233270125446917
	data : 0.11406517028808594
	model : 0.06477718353271485
			 train-loss:  2.2043051414373442 	 ± 0.23289609163288408
	data : 0.11407470703125
	model : 0.06476168632507324
			 train-loss:  2.2035008531628235 	 ± 0.23241761316972162
	data : 0.11402873992919922
	model : 0.06478705406188964
			 train-loss:  2.2034631737743515 	 ± 0.2317170081139026
	data : 0.11416163444519042
	model : 0.0648763656616211
			 train-loss:  2.2014901181181035 	 ± 0.23241662530191554
	data : 0.11419272422790527
	model : 0.06493086814880371
			 train-loss:  2.2021310230096183 	 ± 0.23187184352159929
	data : 0.11437797546386719
	model : 0.06500806808471679
			 train-loss:  2.2028655512093085 	 ± 0.2313807677768098
	data : 0.1143038272857666
	model : 0.06504759788513184
			 train-loss:  2.2012378692626955 	 ± 0.23166759746710897
	data : 0.11428265571594239
	model : 0.06500811576843261
			 train-loss:  2.19956312123795 	 ± 0.2320190284324792
	data : 0.11418242454528808
	model : 0.06491827964782715
			 train-loss:  2.198455755100694 	 ± 0.23179632958022617
	data : 0.11408462524414062
	model : 0.06484875679016114
			 train-loss:  2.1962278550759904 	 ± 0.2329650009460728
	data : 0.11413178443908692
	model : 0.06491317749023437
			 train-loss:  2.1962010764527595 	 ± 0.23229486370153007
	data : 0.1142165184020996
	model : 0.06496009826660157
			 train-loss:  2.1971175166538783 	 ± 0.23194545001078368
	data : 0.11427931785583496
	model : 0.06498522758483886
			 train-loss:  2.197419894012538 	 ± 0.23132016342575398
	data : 0.11431431770324707
	model : 0.06504793167114258
			 train-loss:  2.195200383999927 	 ± 0.23253757270090386
	data : 0.114323091506958
	model : 0.06511478424072266
			 train-loss:  2.1951330926980863 	 ± 0.23188518545965167
	data : 0.11430187225341797
	model : 0.06504979133605956
			 train-loss:  2.193687066685554 	 ± 0.23203995449737583
	data : 0.11414976119995117
	model : 0.06498537063598633
			 train-loss:  2.19321439464887 	 ± 0.2314809003636175
	data : 0.11406469345092773
	model : 0.06495742797851563
			 train-loss:  2.192803222830124 	 ± 0.2309064691965698
	data : 0.11404275894165039
	model : 0.06495466232299804
			 train-loss:  2.1932488804335124 	 ± 0.23034928070653293
	data : 0.11458768844604492
	model : 0.06491169929504395
			 train-loss:  2.1937436777385857 	 ± 0.22981601223513642
	data : 0.11449728012084961
	model : 0.06492848396301269
			 train-loss:  2.1934190554463346 	 ± 0.2292327283305498
	data : 0.11465325355529785
	model : 0.06495194435119629
			 train-loss:  2.1920709204029394 	 ± 0.2293425746560983
	data : 0.11466422080993652
	model : 0.06497707366943359
			 train-loss:  2.1919220794913588 	 ± 0.2287341905658426
	data : 0.11516890525817872
	model : 0.0649609088897705
			 train-loss:  2.194140024363676 	 ± 0.23011852033187297
	data : 0.11455163955688477
	model : 0.06491198539733886
			 train-loss:  2.192192018032074 	 ± 0.2310464771656144
	data : 0.11442384719848633
	model : 0.06483349800109864
			 train-loss:  2.192167873735781 	 ± 0.23043467024098846
	data : 0.1143381118774414
	model : 0.06480050086975098
			 train-loss:  2.19078089124278 	 ± 0.2306170982492027
	data : 0.114422607421875
	model : 0.06484637260437012
			 train-loss:  2.1896602832834136 	 ± 0.23053066883356738
	data : 0.114039945602417
	model : 0.06487140655517579
			 train-loss:  2.1900691588719687 	 ± 0.22999897133440295
	data : 0.11411404609680176
	model : 0.06493649482727051
			 train-loss:  2.1891258380573646 	 ± 0.229774428518384
	data : 0.11431999206542968
	model : 0.06501626968383789
			 train-loss:  2.1895526315748075 	 ± 0.22925814633044694
	data : 0.1144841194152832
	model : 0.06502385139465332
			 train-loss:  2.1891860692928997 	 ± 0.22872654015865304
	data : 0.11444048881530762
	model : 0.06497406959533691
			 train-loss:  2.1885894536972046 	 ± 0.22829437784366433
	data : 0.11475019454956055
	model : 0.0649454116821289
			 train-loss:  2.1891254592062857 	 ± 0.22783782415032774
	data : 0.11467494964599609
	model : 0.0649106502532959
			 train-loss:  2.1886950791484177 	 ± 0.2273420149025088
	data : 0.11467342376708985
	model : 0.06484313011169433
			 train-loss:  2.187788363078132 	 ± 0.22712871618706448
	data : 0.11442713737487793
	model : 0.06489739418029786
			 train-loss:  2.190663285255432 	 ± 0.23016143633498062
	data : 0.11445889472961426
	model : 0.06486783027648926
			 train-loss:  2.1911980714370953 	 ± 0.2297127170233503
	data : 0.11409196853637696
	model : 0.06490969657897949
			 train-loss:  2.190752074269965 	 ± 0.2292306405836895
	data : 0.1141930103302002
	model : 0.06493282318115234
			 train-loss:  2.190704433788807 	 ± 0.22866633851948734
	data : 0.11411824226379394
	model : 0.06495852470397949
			 train-loss:  2.1909487703267265 	 ± 0.22813175660833113
	data : 0.11423606872558593
	model : 0.06494150161743165
			 train-loss:  2.190651192316195 	 ± 0.22761434372148354
	data : 0.11420297622680664
	model : 0.06493334770202637
			 train-loss:  2.1912209200627597 	 ± 0.22720768870466165
	data : 0.11418805122375489
	model : 0.06487522125244141
			 train-loss:  2.1911078605099 	 ± 0.226664022085037
	data : 0.11412420272827148
	model : 0.06486263275146484
			 train-loss:  2.1918065949128223 	 ± 0.22634186463418998
	data : 0.114387845993042
	model : 0.06487069129943848
			 train-loss:  2.1925496667195734 	 ± 0.22605389931570677
	data : 0.11460070610046387
	model : 0.06484169960021972
			 train-loss:  2.193138953617641 	 ± 0.22567589030200608
	data : 0.11463770866394044
	model : 0.0648848533630371
			 train-loss:  2.19234804745534 	 ± 0.225432022466798
	data : 0.1145775318145752
	model : 0.06494989395141601
			 train-loss:  2.1912023874948607 	 ± 0.22551458251662942
	data : 0.11466665267944336
	model : 0.06497726440429688
			 train-loss:  2.1913692548241412 	 ± 0.22499770123173074
	data : 0.1144059181213379
	model : 0.06497635841369628
			 train-loss:  2.1906408661993866 	 ± 0.22472296749647874
	data : 0.11411633491516113
	model : 0.06494970321655273
			 train-loss:  2.190544328024221 	 ± 0.22420419461102103
	data : 0.11408319473266601
	model : 0.06491832733154297
			 train-loss:  2.190595468989125 	 ± 0.22368585826495535
	data : 0.11409835815429688
	model : 0.06489553451538085
			 train-loss:  2.192256434172529 	 ± 0.2245009742311402
	data : 0.11410012245178222
	model : 0.06486701965332031
			 train-loss:  2.1930939855925535 	 ± 0.22432502192922138
	data : 0.1141331672668457
	model : 0.06483521461486816
			 train-loss:  2.193910067484259 	 ± 0.2241363908102531
	data : 0.11422967910766602
	model : 0.06481003761291504
			 train-loss:  2.1934902570464394 	 ± 0.22371269044991984
	data : 0.11429414749145508
	model : 0.06477675437927247
			 train-loss:  2.192485070875867 	 ± 0.22370336837530322
	data : 0.11427559852600097
	model : 0.06474142074584961
			 train-loss:  2.1920721085221917 	 ± 0.2232833762132245
	data : 0.11415567398071289
	model : 0.06466813087463379
			 train-loss:  2.192983616093349 	 ± 0.22319575892981564
	data : 0.11414737701416015
	model : 0.06457304954528809
			 train-loss:  2.198559822248561 	 ± 0.23775603282131427
	data : 0.11414690017700195
	model : 0.06448378562927246
			 train-loss:  2.197677214940389 	 ± 0.23759459362818844
	data : 0.11402349472045899
	model : 0.0644139289855957
			 train-loss:  2.1990572277423555 	 ± 0.23797038647860502
	data : 0.11426014900207519
	model : 0.06428284645080566
			 train-loss:  2.198318143248033 	 ± 0.2377054594775319
	data : 0.11445908546447754
	model : 0.0641845703125
			 train-loss:  2.197305373455349 	 ± 0.23767392770051718
	data : 0.1146082878112793
	model : 0.06412625312805176
			 train-loss:  2.1983253242667584 	 ± 0.23765396657577098
	data : 0.11458325386047363
	model : 0.06402859687805176
			 train-loss:  2.197348649605461 	 ± 0.23759689975606346
	data : 0.11483173370361328
	model : 0.06394944190979004
			 train-loss:  2.1982474708969977 	 ± 0.23747361309020185
	data : 0.11485476493835449
	model : 0.06392593383789062
			 train-loss:  2.1978989089357444 	 ± 0.23702047589805006
	data : 0.11491584777832031
	model : 0.06392459869384766
			 train-loss:  2.1974387445163317 	 ± 0.23661513469922169
	data : 0.11490750312805176
	model : 0.06395378112792968
			 train-loss:  2.197140616229457 	 ± 0.2361528564805311
	data : 0.11516313552856446
	model : 0.06402087211608887
			 train-loss:  2.196188144988202 	 ± 0.2360998631897307
	data : 0.11524205207824707
	model : 0.06403436660766601
			 train-loss:  2.197480051699331 	 ± 0.23643004467047823
	data : 0.11524000167846679
	model : 0.06396236419677734
			 train-loss:  2.19704333086054 	 ± 0.23602609110934378
	data : 0.11510577201843261
	model : 0.06389713287353516
			 train-loss:  2.19638417698756 	 ± 0.23574821271509033
	data : 0.11514325141906738
	model : 0.06380767822265625
			 train-loss:  2.197887424145782 	 ± 0.23639479713013703
	data : 0.11511077880859374
	model : 0.06380314826965332
			 train-loss:  2.198995042343934 	 ± 0.23652244265430125
	data : 0.1150083065032959
	model : 0.0638272762298584
			 train-loss:  2.198473451048507 	 ± 0.23616949760720932
	data : 0.11491446495056153
	model : 0.06391692161560059
			 train-loss:  2.197654504913929 	 ± 0.2360236942946891
	data : 0.11503305435180664
	model : 0.06400299072265625
			 train-loss:  2.1983067739157027 	 ± 0.23575601088941364
	data : 0.11498322486877441
	model : 0.06403918266296386
			 train-loss:  2.1991983412719165 	 ± 0.23568255122426032
	data : 0.11474418640136719
	model : 0.06400165557861329
			 train-loss:  2.1994898723096266 	 ± 0.23524515541707872
	data : 0.11467351913452148
	model : 0.06394219398498535
			 train-loss:  2.199460275289489 	 ± 0.23476698503535237
	data : 0.11475572586059571
	model : 0.06390480995178223
			 train-loss:  2.19917904558452 	 ± 0.23433278390307807
	data : 0.11472396850585938
	model : 0.06390161514282226
			 train-loss:  2.198498477378199 	 ± 0.23410433272115352
	data : 0.11479973793029785
	model : 0.06390981674194336
			 train-loss:  2.1982485927252404 	 ± 0.23366690939385093
	data : 0.1148876667022705
	model : 0.0639838695526123
			 train-loss:  2.1990130705833435 	 ± 0.23351091156513076
	data : 0.11505494117736817
	model : 0.0640101432800293
			 train-loss:  2.1988233774306765 	 ± 0.23306458605165004
	data : 0.11480016708374023
	model : 0.06397733688354493
			 train-loss:  2.1976642632295214 	 ± 0.23332547923622862
	data : 0.11469779014587403
	model : 0.06391820907592774
			 train-loss:  2.1964563541261577 	 ± 0.23365204365556536
	data : 0.11467471122741699
	model : 0.06396722793579102
			 train-loss:  2.197361510569655 	 ± 0.23363567431448024
	data : 0.11479167938232422
	model : 0.06395411491394043
			 train-loss:  2.196775275585698 	 ± 0.23336422085403696
	data : 0.11467323303222657
	model : 0.06398000717163085
			 train-loss:  2.1980188079178333 	 ± 0.2337529779129441
	data : 0.11464900970458984
	model : 0.05560345649719238
#epoch  64    val-loss:  2.4051735401153564  train-loss:  2.1980188079178333  lr:  1.9073486328125e-08
			 train-loss:  2.2492504119873047 	 ± 0.0
	data : 5.547390460968018
	model : 0.07120180130004883
			 train-loss:  2.076365113258362 	 ± 0.17288529872894287
	data : 2.8340156078338623
	model : 0.06852865219116211
			 train-loss:  1.9961307048797607 	 ± 0.18111139735935672
	data : 1.926935116449992
	model : 0.06720852851867676
			 train-loss:  2.0206336975097656 	 ± 0.16248751839317482
	data : 1.4738520383834839
	model : 0.06655210256576538
			 train-loss:  2.039397048950195 	 ± 0.1500999944569158
	data : 1.2018530368804932
	model : 0.06620588302612304
			 train-loss:  2.0397456884384155 	 ± 0.1370241390927241
	data : 0.1152228832244873
	model : 0.06493172645568848
			 train-loss:  2.108375004359654 	 ± 0.2106021890826853
	data : 0.11398587226867676
	model : 0.06470675468444824
			 train-loss:  2.1160332560539246 	 ± 0.198039550402707
	data : 0.1142587661743164
	model : 0.0647817611694336
			 train-loss:  2.136158890194363 	 ± 0.19519798238870623
	data : 0.11410355567932129
	model : 0.06481900215148925
			 train-loss:  2.138923978805542 	 ± 0.18536676760144613
	data : 0.11418743133544922
	model : 0.06480326652526855
			 train-loss:  2.1450997482646597 	 ± 0.17781599259178626
	data : 0.11407098770141602
	model : 0.06478328704833984
			 train-loss:  2.1648150086402893 	 ± 0.18237120485816738
	data : 0.11390633583068847
	model : 0.06482329368591308
			 train-loss:  2.1602552120502176 	 ± 0.17592712401798583
	data : 0.11393976211547852
	model : 0.06483802795410157
			 train-loss:  2.1760468993868147 	 ± 0.17883377623900204
	data : 0.11396822929382325
	model : 0.06485772132873535
			 train-loss:  2.183772261937459 	 ± 0.17517121736704538
	data : 0.11446962356567383
	model : 0.06486434936523437
			 train-loss:  2.19507759809494 	 ± 0.17516936109446063
	data : 0.11453628540039062
	model : 0.06490850448608398
			 train-loss:  2.251329506144804 	 ± 0.2819712405418387
	data : 0.11460413932800292
	model : 0.06499543190002441
			 train-loss:  2.243306212955051 	 ± 0.276016351957254
	data : 0.11457772254943847
	model : 0.06495747566223145
			 train-loss:  2.223872699235615 	 ± 0.28102169757357964
	data : 0.11442389488220214
	model : 0.06490402221679688
			 train-loss:  2.2262461304664614 	 ± 0.274101377453896
	data : 0.11362814903259277
	model : 0.06494760513305664
			 train-loss:  2.2343428361983526 	 ± 0.26993518558916346
	data : 0.1137247085571289
	model : 0.06498918533325196
			 train-loss:  2.230160182172602 	 ± 0.2644245549345336
	data : 0.1137578010559082
	model : 0.06493654251098632
			 train-loss:  2.2219853504844336 	 ± 0.261439368592821
	data : 0.11376667022705078
	model : 0.0649794101715088
			 train-loss:  2.225682189067205 	 ± 0.25654811692243995
	data : 0.11386294364929199
	model : 0.06504101753234863
			 train-loss:  2.209041042327881 	 ± 0.2642546597503722
	data : 0.11412506103515625
	model : 0.06502580642700195
			 train-loss:  2.2206105910814724 	 ± 0.26550159552828945
	data : 0.11412124633789063
	model : 0.06497402191162109
			 train-loss:  2.2194761346887657 	 ± 0.2606027199075844
	data : 0.11418843269348145
	model : 0.06494393348693847
			 train-loss:  2.2207287464823042 	 ± 0.25598954968025894
	data : 0.11414074897766113
	model : 0.06492938995361328
			 train-loss:  2.215818059855494 	 ± 0.2528758343200783
	data : 0.11426124572753907
	model : 0.06496906280517578
			 train-loss:  2.210641026496887 	 ± 0.25018372559873053
	data : 0.1143463134765625
	model : 0.06501169204711914
			 train-loss:  2.2130890661670315 	 ± 0.24648040503053836
	data : 0.11432514190673829
	model : 0.06500940322875977
			 train-loss:  2.220369502902031 	 ± 0.24596182458897933
	data : 0.11431822776794434
	model : 0.06499080657958985
			 train-loss:  2.213506080887534 	 ± 0.24529855292235825
	data : 0.11418290138244629
	model : 0.06491546630859375
			 train-loss:  2.2068169537712548 	 ± 0.2447002237541787
	data : 0.11422138214111328
	model : 0.06488447189331055
			 train-loss:  2.2041657311575755 	 ± 0.2416741177101927
	data : 0.11418099403381347
	model : 0.06486992835998535
			 train-loss:  2.2002585331598916 	 ± 0.2394123986270176
	data : 0.11422119140625
	model : 0.06488480567932128
			 train-loss:  2.200069537033906 	 ± 0.23615765786404622
	data : 0.11418395042419434
	model : 0.06493782997131348
			 train-loss:  2.1988121584842077 	 ± 0.23315508413245264
	data : 0.11442184448242188
	model : 0.06498517990112304
			 train-loss:  2.2013543263459816 	 ± 0.23067941695165697
	data : 0.11442251205444336
	model : 0.06500511169433594
			 train-loss:  2.208519810438156 	 ± 0.23213162408159835
	data : 0.11442298889160156
	model : 0.06497321128845215
			 train-loss:  2.204655653092919 	 ± 0.23058206536629106
	data : 0.11422209739685059
	model : 0.06491060256958008
			 train-loss:  2.2094143402008783 	 ± 0.22984914882028767
	data : 0.11426262855529785
	model : 0.06484990119934082
			 train-loss:  2.2122696776722752 	 ± 0.22791322004227832
	data : 0.11423716545104981
	model : 0.06491460800170898
			 train-loss:  2.2223597277294505 	 ± 0.2348226399776612
	data : 0.11426420211791992
	model : 0.06489443778991699
			 train-loss:  2.221615028381348 	 ± 0.23225137921002717
	data : 0.1141998291015625
	model : 0.06492123603820801
			 train-loss:  2.220276220985081 	 ± 0.22988853255336836
	data : 0.11439542770385742
	model : 0.06499266624450684
			 train-loss:  2.215214282908338 	 ± 0.23000644374569607
	data : 0.11458964347839355
	model : 0.06505317687988281
			 train-loss:  2.218939612309138 	 ± 0.2290263932335541
	data : 0.11457881927490235
	model : 0.06500945091247559
			 train-loss:  2.214605234107193 	 ± 0.2286577915766267
	data : 0.11433291435241699
	model : 0.06495175361633301
			 train-loss:  2.209890079498291 	 ± 0.22875335932403817
	data : 0.11445407867431641
	model : 0.0649378776550293
			 train-loss:  2.2059853544422223 	 ± 0.228176252325831
	data : 0.11452016830444336
	model : 0.06491327285766602
			 train-loss:  2.207006041820233 	 ± 0.22608913189703228
	data : 0.11431097984313965
	model : 0.06487946510314942
			 train-loss:  2.2032244205474854 	 ± 0.22560024700019746
	data : 0.11435637474060059
	model : 0.06490702629089355
			 train-loss:  2.203523291481866 	 ± 0.2235121850802223
	data : 0.11461424827575684
	model : 0.06495776176452636
			 train-loss:  2.206542747670954 	 ± 0.2225796472377821
	data : 0.1145897388458252
	model : 0.06495804786682129
			 train-loss:  2.208367237022945 	 ± 0.22099798197791795
	data : 0.11448302268981933
	model : 0.06490764617919922
			 train-loss:  2.202778192988613 	 ± 0.22300798007656839
	data : 0.1143960952758789
	model : 0.06485514640808106
			 train-loss:  2.202100556472252 	 ± 0.22113632709739348
	data : 0.11446833610534668
	model : 0.0648078441619873
			 train-loss:  2.2086238901493913 	 ± 0.22481228872363107
	data : 0.11444497108459473
	model : 0.06481375694274902
			 train-loss:  2.215881152947744 	 ± 0.22979474829640462
	data : 0.11446914672851563
	model : 0.06476435661315919
			 train-loss:  2.216209904092257 	 ± 0.22791762761546067
	data : 0.11444940567016601
	model : 0.06483163833618164
			 train-loss:  2.2185123120584795 	 ± 0.22678616606869675
	data : 0.11465964317321778
	model : 0.06487455368041992
			 train-loss:  2.2182361209203325 	 ± 0.2249895868194419
	data : 0.11448264122009277
	model : 0.06491074562072754
			 train-loss:  2.223779823631048 	 ± 0.22752039045344213
	data : 0.11443800926208496
	model : 0.06489510536193847
			 train-loss:  2.2223020186791054 	 ± 0.22607278786041315
	data : 0.1143333911895752
	model : 0.06487417221069336
			 train-loss:  2.2271016655546245 	 ± 0.22766621996839345
	data : 0.11434526443481445
	model : 0.06485538482666016
			 train-loss:  2.2251905932355283 	 ± 0.22649358088204805
	data : 0.11418004035949707
	model : 0.06486339569091797
			 train-loss:  2.234024233677808 	 ± 0.2361634279941165
	data : 0.11436228752136231
	model : 0.06482481956481934
			 train-loss:  2.229370922282122 	 ± 0.23756532335150826
	data : 0.11428799629211425
	model : 0.06482172012329102
			 train-loss:  2.233581277302333 	 ± 0.23844119847345674
	data : 0.11433501243591308
	model : 0.06487908363342285
			 train-loss:  2.231983550837342 	 ± 0.2371331540862984
	data : 0.11434388160705566
	model : 0.06491851806640625
			 train-loss:  2.23207144273652 	 ± 0.23548180268745394
	data : 0.11437492370605469
	model : 0.06491799354553222
			 train-loss:  2.2253454678679168 	 ± 0.24072651802736958
	data : 0.11425909996032715
	model : 0.06493101119995118
			 train-loss:  2.2264487550065324 	 ± 0.23928020629435012
	data : 0.11416010856628418
	model : 0.06482014656066895
			 train-loss:  2.2251835028330484 	 ± 0.23792873042692556
	data : 0.11415333747863769
	model : 0.0647857666015625
			 train-loss:  2.2230429837578223 	 ± 0.2370840524417579
	data : 0.11419734954833985
	model : 0.06478633880615234
			 train-loss:  2.2210206551985308 	 ± 0.23619840951451523
	data : 0.1142582893371582
	model : 0.06482319831848145
			 train-loss:  2.2162533769240746 	 ± 0.23837871294172755
	data : 0.11438698768615722
	model : 0.06494126319885254
			 train-loss:  2.2128229427941237 	 ± 0.238794909815252
	data : 0.11463212966918945
	model : 0.06508564949035645
			 train-loss:  2.208133728802204 	 ± 0.24093013685060777
	data : 0.11470041275024415
	model : 0.06511883735656739
			 train-loss:  2.2053124448399486 	 ± 0.24076434242719116
	data : 0.11463475227355957
	model : 0.06506199836730957
			 train-loss:  2.2064529875429666 	 ± 0.23951182869757723
	data : 0.11434950828552246
	model : 0.06494841575622559
			 train-loss:  2.2055783027625946 	 ± 0.23819634022103292
	data : 0.11423277854919434
	model : 0.06479973793029785
			 train-loss:  2.2028950310888744 	 ± 0.23803286684505678
	data : 0.11421809196472169
	model : 0.06477527618408203
			 train-loss:  2.2039999849656047 	 ± 0.23684513752511488
	data : 0.11427798271179199
	model : 0.06479821205139161
			 train-loss:  2.2035650652508405 	 ± 0.2354982435523499
	data : 0.11428031921386719
	model : 0.06483983993530273
			 train-loss:  2.200277903984333 	 ± 0.23611697312525817
	data : 0.11457152366638183
	model : 0.06491656303405761
			 train-loss:  2.2030897357247095 	 ± 0.23623197275809127
	data : 0.11459269523620605
	model : 0.06494722366333008
			 train-loss:  2.2008232722121677 	 ± 0.23586131467734173
	data : 0.11450080871582032
	model : 0.0649388313293457
			 train-loss:  2.200630643632677 	 ± 0.2345543537096259
	data : 0.11427006721496583
	model : 0.06485295295715332
			 train-loss:  2.2012946081685496 	 ± 0.23334706473602818
	data : 0.11427569389343262
	model : 0.06480016708374023
			 train-loss:  2.1980432505192966 	 ± 0.2341388183204143
	data : 0.11418557167053223
	model : 0.0648435115814209
			 train-loss:  2.1979335559311735 	 ± 0.23287898209403582
	data : 0.11413073539733887
	model : 0.0648472785949707
			 train-loss:  2.19810903325994 	 ± 0.2316431333219998
	data : 0.11420392990112305
	model : 0.06487851142883301
			 train-loss:  2.2001572157207288 	 ± 0.23127483498321225
	data : 0.11438417434692383
	model : 0.06494250297546386
			 train-loss:  2.2008786077300706 	 ± 0.23017454404711782
	data : 0.11440544128417969
	model : 0.06496481895446778
			 train-loss:  2.2037068052390185 	 ± 0.23065560293256426
	data : 0.11439275741577148
	model : 0.06491999626159668
			 train-loss:  2.2055282544116586 	 ± 0.2301758984291055
	data : 0.11432676315307617
	model : 0.06483469009399415
			 train-loss:  2.206756748334326 	 ± 0.22933312997059413
	data : 0.11425566673278809
	model : 0.06478147506713867
			 train-loss:  2.2084386706352235 	 ± 0.22879642710734732
	data : 0.11407523155212403
	model : 0.06478004455566407
			 train-loss:  2.2062254483156867 	 ± 0.2287342240703874
	data : 0.1140939712524414
	model : 0.06479997634887695
			 train-loss:  2.208107365112679 	 ± 0.22839464353949177
	data : 0.11421666145324708
	model : 0.06479592323303222
			 train-loss:  2.208503409496789 	 ± 0.22731842053452575
	data : 0.11427793502807618
	model : 0.06490030288696289
			 train-loss:  2.2080508092275033 	 ± 0.22626953263493108
	data : 0.11429381370544434
	model : 0.06494660377502441
			 train-loss:  2.2069018057414462 	 ± 0.22549413312773764
	data : 0.11436309814453124
	model : 0.0649289608001709
			 train-loss:  2.205338047360474 	 ± 0.22499926761156336
	data : 0.11420388221740722
	model : 0.06484861373901367
			 train-loss:  2.2048999383070758 	 ± 0.22399082181257934
	data : 0.11406402587890625
	model : 0.06484870910644532
			 train-loss:  2.2016582224104138 	 ± 0.22545901397933277
	data : 0.11418585777282715
	model : 0.06481947898864746
			 train-loss:  2.200006080329965 	 ± 0.2250782406885821
	data : 0.11411781311035156
	model : 0.06482534408569336
			 train-loss:  2.203330076824535 	 ± 0.2267245087576538
	data : 0.11422219276428222
	model : 0.06486968994140625
			 train-loss:  2.20217498143514 	 ± 0.22602581873732494
	data : 0.11437397003173828
	model : 0.06496195793151856
			 train-loss:  2.2021632535117015 	 ± 0.22501454638755766
	data : 0.11440906524658204
	model : 0.06498198509216309
			 train-loss:  2.1998066881061655 	 ± 0.2254006658456444
	data : 0.11424174308776855
	model : 0.06499223709106446
			 train-loss:  2.198648316818371 	 ± 0.22474746794783548
	data : 0.11425242424011231
	model : 0.06498184204101562
			 train-loss:  2.2004469892253047 	 ± 0.2245907610048702
	data : 0.11412572860717773
	model : 0.06502790451049804
			 train-loss:  2.200146531236583 	 ± 0.22364381337654077
	data : 0.11406455039978028
	model : 0.06503167152404785
			 train-loss:  2.1978759775813828 	 ± 0.2240247561070442
	data : 0.11416082382202149
	model : 0.06503410339355468
			 train-loss:  2.1984270073599737 	 ± 0.22315309133402703
	data : 0.11426901817321777
	model : 0.06503157615661621
			 train-loss:  2.1975182515232503 	 ± 0.22243265550262753
	data : 0.11433696746826172
	model : 0.06503686904907227
			 train-loss:  2.199326033393542 	 ± 0.22238004588596463
	data : 0.11443133354187011
	model : 0.06496667861938477
			 train-loss:  2.197314060423985 	 ± 0.22255324645695515
	data : 0.11435203552246094
	model : 0.06490592956542969
			 train-loss:  2.1960729233554153 	 ± 0.22205935085064787
	data : 0.11425218582153321
	model : 0.06484637260437012
			 train-loss:  2.194183884597406 	 ± 0.22213691977926184
	data : 0.11427979469299317
	model : 0.06486568450927735
			 train-loss:  2.1945568323135376 	 ± 0.22127805398108782
	data : 0.11429638862609863
	model : 0.06487140655517579
			 train-loss:  2.1960239944458007 	 ± 0.22099588996594255
	data : 0.11429834365844727
	model : 0.06489481925964355
			 train-loss:  2.1969580158354742 	 ± 0.22036474379959797
	data : 0.1144561767578125
	model : 0.06492853164672852
			 train-loss:  2.200378667651199 	 ± 0.22282854717294484
	data : 0.1144096851348877
	model : 0.06496343612670899
			 train-loss:  2.2033789809793234 	 ± 0.22451701398221854
	data : 0.11423511505126953
	model : 0.06489810943603516
			 train-loss:  2.201210252074308 	 ± 0.22498703078568022
	data : 0.11402726173400879
	model : 0.06485524177551269
			 train-loss:  2.199301327191866 	 ± 0.2251662965224853
	data : 0.11401081085205078
	model : 0.06486473083496094
			 train-loss:  2.200090770503037 	 ± 0.22448576334742143
	data : 0.1139671802520752
	model : 0.06488738059997559
			 train-loss:  2.1993695443326775 	 ± 0.22378612154533875
	data : 0.114097261428833
	model : 0.0649301528930664
			 train-loss:  2.1989102058840873 	 ± 0.22300568612958419
	data : 0.11423692703247071
	model : 0.0650057315826416
			 train-loss:  2.1986917737704603 	 ± 0.22218629785463775
	data : 0.11445736885070801
	model : 0.06502981185913086
			 train-loss:  2.197099796930949 	 ± 0.22212762010196171
	data : 0.11440153121948242
	model : 0.06501445770263672
			 train-loss:  2.1973345016731933 	 ± 0.22132626854946627
	data : 0.11435952186584472
	model : 0.0649592399597168
			 train-loss:  2.1970133224542994 	 ± 0.22054883658471092
	data : 0.11411080360412598
	model : 0.06486172676086426
			 train-loss:  2.1945265928904214 	 ± 0.22166753587804458
	data : 0.11404995918273926
	model : 0.06485152244567871
			 train-loss:  2.1926262541640575 	 ± 0.2219940407570311
	data : 0.11406140327453614
	model : 0.06488313674926757
			 train-loss:  2.1938345832484107 	 ± 0.22165805350752005
	data : 0.11412253379821777
	model : 0.06488847732543945
			 train-loss:  2.194621389639293 	 ± 0.22106674492061826
	data : 0.11417889595031738
	model : 0.06494641304016113
			 train-loss:  2.194476088167916 	 ± 0.22029372229789715
	data : 0.11442723274230956
	model : 0.06501140594482421
			 train-loss:  2.192180201723859 	 ± 0.2212203734045923
	data : 0.11448912620544434
	model : 0.06504020690917969
			 train-loss:  2.192128254307641 	 ± 0.22045178413283495
	data : 0.11432933807373047
	model : 0.06498274803161622
			 train-loss:  2.1912849228957603 	 ± 0.2199232539282978
	data : 0.11421594619750977
	model : 0.06493992805480957
			 train-loss:  2.189452734712052 	 ± 0.22027645012951322
	data : 0.11419520378112794
	model : 0.06491451263427735
			 train-loss:  2.187850991884867 	 ± 0.22037742507715344
	data : 0.11417908668518066
	model : 0.06492023468017578
			 train-loss:  2.189043066791586 	 ± 0.22010668476739254
	data : 0.11418638229370118
	model : 0.06495299339294433
			 train-loss:  2.1890641418879464 	 ± 0.21936697810178235
	data : 0.114251708984375
	model : 0.06498937606811524
			 train-loss:  2.1949726589520773 	 ± 0.23022322671334178
	data : 0.11435317993164062
	model : 0.06498327255249023
			 train-loss:  2.194588150409673 	 ± 0.22950795118460024
	data : 0.11428923606872558
	model : 0.06495060920715331
			 train-loss:  2.194734384354792 	 ± 0.2287588028531202
	data : 0.11424899101257324
	model : 0.06490192413330079
			 train-loss:  2.1969643878001794 	 ± 0.22966158368185444
	data : 0.1141211986541748
	model : 0.06481542587280273
			 train-loss:  2.1969062407295423 	 ± 0.22891584467121212
	data : 0.11421647071838378
	model : 0.06482524871826172
			 train-loss:  2.1969203987429218 	 ± 0.22817627921895667
	data : 0.1142655849456787
	model : 0.06489753723144531
			 train-loss:  2.1979940632979074 	 ± 0.22783622465807093
	data : 0.1142961025238037
	model : 0.06493797302246093
			 train-loss:  2.1984624035039526 	 ± 0.2271847924743788
	data : 0.11430768966674805
	model : 0.06503849029541016
			 train-loss:  2.1961372638050514 	 ± 0.22833101176446005
	data : 0.11437048912048339
	model : 0.06505050659179687
			 train-loss:  2.193661450589978 	 ± 0.2297294983263637
	data : 0.11415343284606934
	model : 0.06507401466369629
			 train-loss:  2.193236745148897 	 ± 0.22907307614696318
	data : 0.11408305168151855
	model : 0.06499710083007812
			 train-loss:  2.192566580653931 	 ± 0.22851784410895906
	data : 0.11397171020507812
	model : 0.06502985954284668
			 train-loss:  2.1945402291085987 	 ± 0.2291837653512224
	data : 0.11391744613647461
	model : 0.06497516632080078
			 train-loss:  2.1947740872213446 	 ± 0.2284990536919791
	data : 0.11389732360839844
	model : 0.06500215530395508
			 train-loss:  2.192801256005357 	 ± 0.22918957015350444
	data : 0.11398653984069824
	model : 0.06500916481018067
			 train-loss:  2.1904990897034153 	 ± 0.23038815818949887
	data : 0.11404080390930176
	model : 0.06511163711547852
			 train-loss:  2.1914335539542047 	 ± 0.23000659572148016
	data : 0.11429448127746582
	model : 0.06507954597473145
			 train-loss:  2.193020655723389 	 ± 0.2302268132005405
	data : 0.11420598030090331
	model : 0.06503133773803711
			 train-loss:  2.1923235037497113 	 ± 0.229717323482059
	data : 0.11411514282226562
	model : 0.06500387191772461
			 train-loss:  2.191910954622122 	 ± 0.2290990898243129
	data : 0.11422367095947265
	model : 0.06497745513916016
			 train-loss:  2.192390184542712 	 ± 0.22850921695388074
	data : 0.1141667366027832
	model : 0.06491756439208984
			 train-loss:  2.1921253587767393 	 ± 0.22786624445338147
	data : 0.11405792236328124
	model : 0.06494503021240235
			 train-loss:  2.1923384285250376 	 ± 0.2272199605835247
	data : 0.11411852836608886
	model : 0.06499428749084472
			 train-loss:  2.1918551777139563 	 ± 0.22665093176636714
	data : 0.11422481536865234
	model : 0.0650327205657959
			 train-loss:  2.193339038854358 	 ± 0.22683987804458705
	data : 0.11454143524169921
	model : 0.06495251655578613
			 train-loss:  2.1922073371069772 	 ± 0.22668291598031515
	data : 0.11444640159606934
	model : 0.06492400169372559
			 train-loss:  2.191488066180186 	 ± 0.22623819243487323
	data : 0.1144556999206543
	model : 0.06487760543823243
			 train-loss:  2.189834535458667 	 ± 0.2266622134268374
	data : 0.11458382606506348
	model : 0.06488265991210937
			 train-loss:  2.1890118965941867 	 ± 0.22628944557395328
	data : 0.11476721763610839
	model : 0.06490497589111328
			 train-loss:  2.187586184986477 	 ± 0.22645673621297943
	data : 0.11448841094970703
	model : 0.06505355834960938
			 train-loss:  2.187216862042745 	 ± 0.22588086516026146
	data : 0.11463308334350586
	model : 0.06510329246520996
			 train-loss:  2.1857110268503264 	 ± 0.2261601909325285
	data : 0.11491751670837402
	model : 0.06512155532836914
			 train-loss:  2.1863322991591234 	 ± 0.2256928418717958
	data : 0.11516728401184081
	model : 0.06513280868530273
			 train-loss:  2.186489795726505 	 ± 0.2250853787698803
	data : 0.1150402545928955
	model : 0.06510419845581054
			 train-loss:  2.186705805685209 	 ± 0.22449191935421087
	data : 0.11507740020751953
	model : 0.06503329277038575
			 train-loss:  2.1862816204895843 	 ± 0.22395828935295284
	data : 0.11489477157592773
	model : 0.06494154930114746
			 train-loss:  2.1865996378724293 	 ± 0.22339731952143627
	data : 0.1150430679321289
	model : 0.06492919921875
			 train-loss:  2.1881293281514376 	 ± 0.2237738022272801
	data : 0.1145566463470459
	model : 0.06487507820129394
			 train-loss:  2.190491242611662 	 ± 0.2255029165290607
	data : 0.11448707580566406
	model : 0.06485486030578613
			 train-loss:  2.191112924505163 	 ± 0.2250670328514182
	data : 0.11437902450561524
	model : 0.06486873626708985
			 train-loss:  2.1892728146753813 	 ± 0.225894926710504
	data : 0.11460247039794921
	model : 0.06497387886047364
			 train-loss:  2.1879599581214144 	 ± 0.22602839521006107
	data : 0.1141280174255371
	model : 0.0650181770324707
			 train-loss:  2.1877582743763924 	 ± 0.2254562417327362
	data : 0.1142723560333252
	model : 0.0650848388671875
			 train-loss:  2.187975738950344 	 ± 0.22489158771202078
	data : 0.1142735481262207
	model : 0.06507830619812012
			 train-loss:  2.1871657715630284 	 ± 0.22459327935209414
	data : 0.11431632041931153
	model : 0.06504254341125489
			 train-loss:  2.187372106160873 	 ± 0.2240350928599414
	data : 0.11419687271118165
	model : 0.0650299072265625
			 train-loss:  2.1881361628065306 	 ± 0.22371741151057917
	data : 0.11438021659851075
	model : 0.06501455307006836
			 train-loss:  2.1884894709901763 	 ± 0.22320369164876427
	data : 0.11436624526977539
	model : 0.06500492095947266
			 train-loss:  2.188241236137621 	 ± 0.2226665929515003
	data : 0.11461863517761231
	model : 0.06503877639770508
			 train-loss:  2.187765957722113 	 ± 0.22220708799420824
	data : 0.11456079483032226
	model : 0.06509923934936523
			 train-loss:  2.1872307419776917 	 ± 0.22177942809876133
	data : 0.11466960906982422
	model : 0.0650914192199707
			 train-loss:  2.1861714385635223 	 ± 0.22173369710989113
	data : 0.11449713706970215
	model : 0.06507902145385742
			 train-loss:  2.184022302674775 	 ± 0.22327295631732966
	data : 0.11447653770446778
	model : 0.06503458023071289
			 train-loss:  2.183565519126178 	 ± 0.22281694293669327
	data : 0.11415629386901856
	model : 0.06495742797851563
			 train-loss:  2.184619491006814 	 ± 0.22277684892229574
	data : 0.1141125202178955
	model : 0.06489224433898926
			 train-loss:  2.185703496235173 	 ± 0.222771503812892
	data : 0.11398353576660156
	model : 0.06489334106445313
			 train-loss:  2.1859144215444917 	 ± 0.2222506575805275
	data : 0.11406593322753907
	model : 0.06492443084716797
			 train-loss:  2.18508629867996 	 ± 0.22203153405138265
	data : 0.11417593955993652
	model : 0.06501364707946777
			 train-loss:  2.185117414364448 	 ± 0.22149761376170848
	data : 0.11425795555114746
	model : 0.06506638526916504
			 train-loss:  2.1857743730955717 	 ± 0.22117012019130244
	data : 0.11464643478393555
	model : 0.06510071754455567
			 train-loss:  2.185459516161964 	 ± 0.22068984314275392
	data : 0.11468720436096191
	model : 0.06506285667419434
			 train-loss:  2.1852892676800915 	 ± 0.2201800829862985
	data : 0.11452603340148926
	model : 0.06499323844909669
			 train-loss:  2.185636520385742 	 ± 0.2197180840625598
	data : 0.11431736946105957
	model : 0.06483235359191894
			 train-loss:  2.184329136996202 	 ± 0.22002670221357482
	data : 0.11439933776855468
	model : 0.06485857963562011
			 train-loss:  2.183907219182665 	 ± 0.2195983690199374
	data : 0.11423420906066895
	model : 0.06482467651367188
			 train-loss:  2.1856129413427308 	 ± 0.22050346718371824
	data : 0.11435923576354981
	model : 0.06482620239257812
			 train-loss:  2.1866810763323747 	 ± 0.22054925610328843
	data : 0.11449904441833496
	model : 0.06488161087036133
			 train-loss:  2.187605832578949 	 ± 0.2204598272084333
	data : 0.11469087600708008
	model : 0.06495318412780762
			 train-loss:  2.1871091094585733 	 ± 0.22007528086969805
	data : 0.1146397590637207
	model : 0.06492328643798828
			 train-loss:  2.1863894549678995 	 ± 0.2198291977247093
	data : 0.11454219818115234
	model : 0.0649172306060791
			 train-loss:  2.1860406626354565 	 ± 0.21938974526672828
	data : 0.11444110870361328
	model : 0.06491870880126953
			 train-loss:  2.185769947405854 	 ± 0.21892965122800043
	data : 0.11446657180786132
	model : 0.06483926773071289
			 train-loss:  2.1852776510221466 	 ± 0.21855857575580645
	data : 0.11450920104980469
	model : 0.06480307579040527
			 train-loss:  2.184426896240679 	 ± 0.21843608998372022
	data : 0.11459550857543946
	model : 0.06472864151000976
			 train-loss:  2.1826645407293523 	 ± 0.2195311596233316
	data : 0.11467018127441406
	model : 0.06464681625366211
			 train-loss:  2.1848497109942966 	 ± 0.22147082849973745
	data : 0.11494050025939942
	model : 0.06447687149047851
			 train-loss:  2.185554552394732 	 ± 0.22123308018258966
	data : 0.11500916481018067
	model : 0.06435999870300294
			 train-loss:  2.187150456831844 	 ± 0.22204518679372373
	data : 0.11505765914916992
	model : 0.06424241065979004
			 train-loss:  2.1876685499099264 	 ± 0.22169517477508988
	data : 0.11505813598632812
	model : 0.06406850814819336
			 train-loss:  2.1878840876458514 	 ± 0.221234534463782
	data : 0.11482172012329102
	model : 0.06389966011047363
			 train-loss:  2.1883629130280537 	 ± 0.22087195340345794
	data : 0.11474061012268066
	model : 0.0638967514038086
			 train-loss:  2.1902321897027814 	 ± 0.22220912841666204
	data : 0.1147982120513916
	model : 0.06391067504882812
			 train-loss:  2.1918141127660355 	 ± 0.22302945615704595
	data : 0.11477665901184082
	model : 0.0638895034790039
			 train-loss:  2.1904144875481406 	 ± 0.22356907057032205
	data : 0.1148073673248291
	model : 0.0639336109161377
			 train-loss:  2.190227022028377 	 ± 0.22310919880802838
	data : 0.11505203247070313
	model : 0.06401472091674805
			 train-loss:  2.190948993094424 	 ± 0.2229077504489075
	data : 0.11496934890747071
	model : 0.0639915943145752
			 train-loss:  2.191877860639055 	 ± 0.22289028739177816
	data : 0.11484289169311523
	model : 0.06385436058044433
			 train-loss:  2.1917958546288405 	 ± 0.22242312541325104
	data : 0.11493039131164551
	model : 0.06381115913391114
			 train-loss:  2.191399341370879 	 ± 0.22203928204506937
	data : 0.1148756980895996
	model : 0.06382913589477539
			 train-loss:  2.1903972620744585 	 ± 0.2221129247879241
	data : 0.11492047309875489
	model : 0.06382198333740234
			 train-loss:  2.1905331884821257 	 ± 0.22165966740717405
	data : 0.1149216651916504
	model : 0.06383614540100098
			 train-loss:  2.1912341884557636 	 ± 0.22146573771817557
	data : 0.11499223709106446
	model : 0.06392111778259277
			 train-loss:  2.1934989045474156 	 ± 0.2237866694441794
	data : 0.11490530967712402
	model : 0.06389274597167968
			 train-loss:  2.1949251419232216 	 ± 0.22442514559719934
	data : 0.11497235298156738
	model : 0.06380972862243653
			 train-loss:  2.195126716719299 	 ± 0.22398682766829797
	data : 0.11475434303283691
	model : 0.06378331184387206
			 train-loss:  2.1940315543388835 	 ± 0.22418289839053646
	data : 0.11478052139282227
	model : 0.0638089656829834
			 train-loss:  2.195094141049114 	 ± 0.22434415171446234
	data : 0.11478495597839355
	model : 0.06380891799926758
			 train-loss:  2.193390789784883 	 ± 0.22547788755970163
	data : 0.11478724479675292
	model : 0.06386275291442871
			 train-loss:  2.1933474776244934 	 ± 0.22502386543718206
	data : 0.11489896774291992
	model : 0.06393656730651856
			 train-loss:  2.196261545261705 	 ± 0.2292124507937032
	data : 0.11504673957824707
	model : 0.06396770477294922
			 train-loss:  2.1954498763084414 	 ± 0.22911184391993727
	data : 0.11495928764343262
	model : 0.06389179229736328
			 train-loss:  2.194949767979018 	 ± 0.22879167749561932
	data : 0.11504058837890625
	model : 0.06395330429077148
			 train-loss:  2.195064529066994 	 ± 0.22834451302257208
	data : 0.11518950462341308
	model : 0.06396822929382324
			 train-loss:  2.1941341510403296 	 ± 0.22837087574132428
	data : 0.1150580883026123
	model : 0.06397371292114258
			 train-loss:  2.1932515985383763 	 ± 0.22835277648908164
	data : 0.11511335372924805
	model : 0.0639866828918457
			 train-loss:  2.1925629353990743 	 ± 0.22816871308062
	data : 0.11521706581115723
	model : 0.06405882835388184
			 train-loss:  2.1941709546372294 	 ± 0.22916578844675808
	data : 0.1148714542388916
	model : 0.055599308013916014
#epoch  65    val-loss:  2.4043659410978617  train-loss:  2.1941709546372294  lr:  1.9073486328125e-08
			 train-loss:  1.9019113779067993 	 ± 0.0
	data : 5.460065603256226
	model : 0.11191987991333008
			 train-loss:  2.0006638169288635 	 ± 0.09875243902206421
	data : 2.7835487127304077
	model : 0.09150338172912598
			 train-loss:  2.0817895332972207 	 ± 0.1402288362680658
	data : 1.8929530779520671
	model : 0.0825815995534261
			 train-loss:  2.145571619272232 	 ± 0.1641723434832833
	data : 1.4481229186058044
	model : 0.07802635431289673
			 train-loss:  2.1750137567520142 	 ± 0.15820684096886609
	data : 1.1812247276306151
	model : 0.07527780532836914
			 train-loss:  2.1578659415245056 	 ± 0.1494258175255332
	data : 0.11198759078979492
	model : 0.06584258079528808
			 train-loss:  2.1715650047574724 	 ± 0.14235285719667481
	data : 0.113427734375
	model : 0.06456460952758789
			 train-loss:  2.153694227337837 	 ± 0.14130409320725565
	data : 0.11387033462524414
	model : 0.06459708213806152
			 train-loss:  2.1636291477415295 	 ± 0.1361540648529342
	data : 0.11396522521972656
	model : 0.06468744277954101
			 train-loss:  2.212678873538971 	 ± 0.19579840869020587
	data : 0.11402316093444824
	model : 0.06478681564331054
			 train-loss:  2.199844327839938 	 ± 0.19104734787225125
	data : 0.11396555900573731
	model : 0.06501941680908203
			 train-loss:  2.2007218102614083 	 ± 0.18293706048165614
	data : 0.1137314796447754
	model : 0.06504669189453124
			 train-loss:  2.1842487041766825 	 ± 0.18479183301307026
	data : 0.1136704921722412
	model : 0.06504545211791993
			 train-loss:  2.1860803876604353 	 ± 0.1781922921706256
	data : 0.11367311477661132
	model : 0.06504325866699219
			 train-loss:  2.1868177096048993 	 ± 0.17217221359494644
	data : 0.11373515129089355
	model : 0.06510019302368164
			 train-loss:  2.1910085529088974 	 ± 0.16749332584397916
	data : 0.11381769180297852
	model : 0.06490077972412109
			 train-loss:  2.205319222281961 	 ± 0.1722803012512279
	data : 0.11395678520202637
	model : 0.06490602493286132
			 train-loss:  2.201663229200575 	 ± 0.16810357599515252
	data : 0.11399216651916504
	model : 0.0648538589477539
			 train-loss:  2.178139335230777 	 ± 0.19165654418478437
	data : 0.11385993957519532
	model : 0.0648120403289795
			 train-loss:  2.1753715872764587 	 ± 0.18719286185227665
	data : 0.11377968788146972
	model : 0.06473479270935059
			 train-loss:  2.159725024586632 	 ± 0.19562422902957324
	data : 0.11374645233154297
	model : 0.06479225158691407
			 train-loss:  2.167260402982885 	 ± 0.1942209226149945
	data : 0.11384401321411133
	model : 0.06478724479675294
			 train-loss:  2.167735094609468 	 ± 0.18996485799200824
	data : 0.11390972137451172
	model : 0.06547198295593262
			 train-loss:  2.180882766842842 	 ± 0.1963640662838304
	data : 0.113551664352417
	model : 0.06552882194519043
			 train-loss:  2.171900300979614 	 ± 0.19736495458423467
	data : 0.11365184783935547
	model : 0.06573982238769531
			 train-loss:  2.1857528411425076 	 ± 0.2055530529756528
	data : 0.1137300968170166
	model : 0.06566052436828614
			 train-loss:  2.1766176135451705 	 ± 0.20701915015399433
	data : 0.11359248161315919
	model : 0.06613726615905761
			 train-loss:  2.171845180647714 	 ± 0.2047957007987407
	data : 0.11361813545227051
	model : 0.06546831130981445
			 train-loss:  2.1748792303019555 	 ± 0.20187318015998798
	data : 0.11395339965820313
	model : 0.06540961265563965
			 train-loss:  2.170324524243673 	 ± 0.1999899245212622
	data : 0.11393890380859376
	model : 0.06573982238769531
			 train-loss:  2.1728570692000853 	 ± 0.19722624280497023
	data : 0.11342225074768067
	model : 0.06576800346374512
			 train-loss:  2.169652670621872 	 ± 0.19493828681037004
	data : 0.11355266571044922
	model : 0.06533679962158204
			 train-loss:  2.1712057951724892 	 ± 0.19216290596107954
	data : 0.11354379653930664
	model : 0.06597375869750977
			 train-loss:  2.166168584543116 	 ± 0.1915145730073133
	data : 0.11322007179260254
	model : 0.06605768203735352
			 train-loss:  2.1660528932298933 	 ± 0.18876002939885997
	data : 0.11318531036376953
	model : 0.06556644439697265
			 train-loss:  2.1782001389397516 	 ± 0.19951205986634202
	data : 0.11353592872619629
	model : 0.06548032760620118
			 train-loss:  2.187825499354182 	 ± 0.20509646953722344
	data : 0.11343774795532227
	model : 0.0654233455657959
			 train-loss:  2.1840103739186336 	 ± 0.20370601709536154
	data : 0.11329097747802734
	model : 0.0647928237915039
			 train-loss:  2.1762131299728003 	 ± 0.20674241044092664
	data : 0.11374197006225586
	model : 0.06478633880615234
			 train-loss:  2.1692881017923353 	 ± 0.20867234666998988
	data : 0.11386656761169434
	model : 0.0652766227722168
			 train-loss:  2.1678034299757423 	 ± 0.20632563070103146
	data : 0.11401877403259278
	model : 0.06539130210876465
			 train-loss:  2.164488148121607 	 ± 0.20495688075714386
	data : 0.11409964561462402
	model : 0.06541604995727539
			 train-loss:  2.1662875580233196 	 ± 0.20289504497102623
	data : 0.11418123245239258
	model : 0.06542806625366211
			 train-loss:  2.164773331447081 	 ± 0.20082179468782632
	data : 0.1141434669494629
	model : 0.06536154747009278
			 train-loss:  2.1617160876592 	 ± 0.19961072372203972
	data : 0.11399435997009277
	model : 0.06484026908874511
			 train-loss:  2.1591308039167654 	 ± 0.19818936314948116
	data : 0.11399040222167969
	model : 0.06483831405639648
			 train-loss:  2.156721817686203 	 ± 0.19674919982590558
	data : 0.11406593322753907
	model : 0.0648643970489502
			 train-loss:  2.1586808040738106 	 ± 0.19515161449534346
	data : 0.11413168907165527
	model : 0.06489214897155762
			 train-loss:  2.15966902703655 	 ± 0.193271314729306
	data : 0.11419534683227539
	model : 0.06497573852539062
			 train-loss:  2.1595686411857606 	 ± 0.19133013056986992
	data : 0.11416215896606445
	model : 0.06498246192932129
			 train-loss:  2.1625980045281206 	 ± 0.19065225509212055
	data : 0.1141519546508789
	model : 0.06491675376892089
			 train-loss:  2.1609920928111443 	 ± 0.18915814419703045
	data : 0.1140073299407959
	model : 0.06480998992919922
			 train-loss:  2.161730750551764 	 ± 0.1874408333559309
	data : 0.11403660774230957
	model : 0.06476407051086426
			 train-loss:  2.157439112663269 	 ± 0.18830718895660953
	data : 0.11404809951782227
	model : 0.06473817825317382
			 train-loss:  2.1626762693578545 	 ± 0.190515036710854
	data : 0.11413321495056153
	model : 0.06474456787109376
			 train-loss:  2.1623553335666656 	 ± 0.18882134867383604
	data : 0.11419811248779296
	model : 0.06476297378540039
			 train-loss:  2.156917779069198 	 ± 0.19153003321683723
	data : 0.1142688274383545
	model : 0.06483845710754395
			 train-loss:  2.1623064587856162 	 ± 0.19418144517742494
	data : 0.11418790817260742
	model : 0.06483206748962403
			 train-loss:  2.1628405663926724 	 ± 0.19257177173639295
	data : 0.11412773132324219
	model : 0.06478147506713867
			 train-loss:  2.159068842728933 	 ± 0.1931454129566798
	data : 0.11408057212829589
	model : 0.06476759910583496
			 train-loss:  2.161551600596944 	 ± 0.19251866376810553
	data : 0.11397514343261719
	model : 0.06476244926452637
			 train-loss:  2.1630329316662205 	 ± 0.19130994065074433
	data : 0.11387205123901367
	model : 0.06473331451416016
			 train-loss:  2.1665541709415495 	 ± 0.19180014303006815
	data : 0.11394000053405762
	model : 0.06478042602539062
			 train-loss:  2.1649696864187717 	 ± 0.19071093474117295
	data : 0.1139892578125
	model : 0.06484470367431641
			 train-loss:  2.169866349146916 	 ± 0.1932502511787814
	data : 0.11407127380371093
	model : 0.06483254432678223
			 train-loss:  2.167861754244024 	 ± 0.1924604174909427
	data : 0.11413989067077637
	model : 0.06486101150512695
			 train-loss:  2.160496738419604 	 ± 0.20017050133605294
	data : 0.11421036720275879
	model : 0.06488370895385742
			 train-loss:  2.159998639541514 	 ± 0.19873503415716842
	data : 0.11411590576171875
	model : 0.06480345726013184
			 train-loss:  2.1568677131680474 	 ± 0.19897185026641942
	data : 0.11417937278747559
	model : 0.0647737979888916
			 train-loss:  2.1636636342321123 	 ± 0.20545306554431633
	data : 0.11416573524475097
	model : 0.06479339599609375
			 train-loss:  2.1638976369105594 	 ± 0.20401047658333754
	data : 0.11418037414550782
	model : 0.06477775573730468
			 train-loss:  2.173287393318282 	 ± 0.21749048137808902
	data : 0.11421375274658203
	model : 0.06474599838256836
			 train-loss:  2.1721417985550344 	 ± 0.21621430867891922
	data : 0.1143458366394043
	model : 0.06483025550842285
			 train-loss:  2.1726464854704366 	 ± 0.21479171962503216
	data : 0.11427693367004395
	model : 0.06479291915893555
			 train-loss:  2.1818597173690795 	 ± 0.22759993971417627
	data : 0.1142305850982666
	model : 0.0647200107574463
			 train-loss:  2.1784359185319198 	 ± 0.22803357362352167
	data : 0.1142470359802246
	model : 0.06470179557800293
			 train-loss:  2.175310091538863 	 ± 0.22818101443771258
	data : 0.11428260803222656
	model : 0.0647742748260498
			 train-loss:  2.1754408310621214 	 ± 0.22671649984153672
	data : 0.11427869796752929
	model : 0.06478023529052734
			 train-loss:  2.1711301154728178 	 ± 0.22847133444387616
	data : 0.11436080932617188
	model : 0.064845609664917
			 train-loss:  2.1722186639904977 	 ± 0.22724495895714728
	data : 0.11438961029052734
	model : 0.06498250961303711
			 train-loss:  2.1719488759099703 	 ± 0.22585074718859124
	data : 0.11435089111328126
	model : 0.06502323150634766
			 train-loss:  2.169737818764477 	 ± 0.2253497172520335
	data : 0.11427440643310546
	model : 0.06497626304626465
			 train-loss:  2.1692724802407874 	 ± 0.2240277082211896
	data : 0.11423110961914062
	model : 0.06490693092346192
			 train-loss:  2.1695456362905956 	 ± 0.22270412190184538
	data : 0.11402130126953125
	model : 0.0647810935974121
			 train-loss:  2.1720962973201976 	 ± 0.22262103120525834
	data : 0.11391754150390625
	model : 0.06472988128662109
			 train-loss:  2.17082478833753 	 ± 0.22163317782222558
	data : 0.11393880844116211
	model : 0.06478099822998047
			 train-loss:  2.1710784243441177 	 ± 0.22036829569965094
	data : 0.11402688026428223
	model : 0.06479802131652831
			 train-loss:  2.1720117005434902 	 ± 0.21928547630448028
	data : 0.11397700309753418
	model : 0.06486706733703614
			 train-loss:  2.170642298259092 	 ± 0.21842813404839415
	data : 0.11409487724304199
	model : 0.06498374938964843
			 train-loss:  2.169840701421102 	 ± 0.21734285543312398
	data : 0.1143312931060791
	model : 0.06497111320495605
			 train-loss:  2.17156020625607 	 ± 0.21676005522212283
	data : 0.11431517601013183
	model : 0.06489772796630859
			 train-loss:  2.1770703377931016 	 ± 0.22189438226445968
	data : 0.11417255401611329
	model : 0.06481404304504394
			 train-loss:  2.1751089134523944 	 ± 0.2214985927203526
	data : 0.11411709785461426
	model : 0.06482143402099609
			 train-loss:  2.177753467509087 	 ± 0.22178842470298912
	data : 0.11418952941894531
	model : 0.06478095054626465
			 train-loss:  2.177807036199068 	 ± 0.22061864040152243
	data : 0.11417198181152344
	model : 0.0648005485534668
			 train-loss:  2.1771789776782193 	 ± 0.2195519343280411
	data : 0.11416807174682617
	model : 0.06480884552001953
			 train-loss:  2.1770052012709 	 ± 0.21842392776240532
	data : 0.11425666809082032
	model : 0.06488065719604492
			 train-loss:  2.1813076625064927 	 ± 0.22139956936314398
	data : 0.11437826156616211
	model : 0.0648233413696289
			 train-loss:  2.187253375246067 	 ± 0.2280067703586996
	data : 0.11432013511657715
	model : 0.06482157707214356
			 train-loss:  2.1902717077732086 	 ± 0.22884304500968844
	data : 0.11406755447387695
	model : 0.06474452018737793
			 train-loss:  2.191090910741598 	 ± 0.2278546516322348
	data : 0.11405010223388672
	model : 0.06475696563720704
			 train-loss:  2.1887976480465308 	 ± 0.2279032892036768
	data : 0.11407389640808105
	model : 0.06478681564331054
			 train-loss:  2.1894439366257306 	 ± 0.2268881718756483
	data : 0.11410775184631347
	model : 0.06486263275146484
			 train-loss:  2.1901246985563865 	 ± 0.225900405990776
	data : 0.11407217979431153
	model : 0.064898681640625
			 train-loss:  2.1865539176123483 	 ± 0.22775213177228815
	data : 0.11421542167663574
	model : 0.06509103775024414
			 train-loss:  2.1833501908014403 	 ± 0.229040149382624
	data : 0.11433272361755371
	model : 0.06513738632202148
			 train-loss:  2.1852985687344986 	 ± 0.22884822497119675
	data : 0.11425890922546386
	model : 0.06505107879638672
			 train-loss:  2.184702225305416 	 ± 0.22786978871681626
	data : 0.11418867111206055
	model : 0.0649644374847412
			 train-loss:  2.183538440170638 	 ± 0.22714432088636358
	data : 0.11433777809143067
	model : 0.06497621536254883
			 train-loss:  2.1815769878300753 	 ± 0.22703492330776343
	data : 0.11437301635742188
	model : 0.06490936279296874
			 train-loss:  2.180476144627408 	 ± 0.22630464524381413
	data : 0.11425909996032715
	model : 0.06489119529724122
			 train-loss:  2.1798612251877785 	 ± 0.22538522230093966
	data : 0.11434059143066407
	model : 0.06496729850769042
			 train-loss:  2.1810243456764558 	 ± 0.22472310365449286
	data : 0.11443767547607422
	model : 0.06503047943115234
			 train-loss:  2.1803226251351204 	 ± 0.22385961947662825
	data : 0.11439056396484375
	model : 0.06503024101257324
			 train-loss:  2.1816337575083193 	 ± 0.2233233902986356
	data : 0.11437220573425293
	model : 0.06495609283447265
			 train-loss:  2.178950644772628 	 ± 0.22421259978341926
	data : 0.1142383098602295
	model : 0.06485924720764161
			 train-loss:  2.178383490978143 	 ± 0.22333592123674764
	data : 0.11428751945495605
	model : 0.0648167610168457
			 train-loss:  2.1772785550456937 	 ± 0.2227084961246653
	data : 0.11420483589172363
	model : 0.06479063034057617
			 train-loss:  2.1760848089426506 	 ± 0.22214956406523073
	data : 0.11422257423400879
	model : 0.06477437019348145
			 train-loss:  2.175428628921509 	 ± 0.2213377810838364
	data : 0.11428108215332031
	model : 0.06482610702514649
			 train-loss:  2.1769282344944223 	 ± 0.22103255848626296
	data : 0.11450424194335937
	model : 0.06488418579101562
			 train-loss:  2.176093384867809 	 ± 0.2203162992624736
	data : 0.1144498348236084
	model : 0.06492071151733399
			 train-loss:  2.173007181020287 	 ± 0.22205101059516064
	data : 0.11443743705749512
	model : 0.06494617462158203
			 train-loss:  2.1737860989186073 	 ± 0.22132248582545663
	data : 0.11430587768554687
	model : 0.06489090919494629
			 train-loss:  2.1728540887832644 	 ± 0.22067959875973847
	data : 0.11408333778381348
	model : 0.0648000717163086
			 train-loss:  2.1775143969626654 	 ± 0.22589331662972365
	data : 0.11406593322753907
	model : 0.06478314399719239
			 train-loss:  2.179162844898194 	 ± 0.22576179222163106
	data : 0.11404356956481934
	model : 0.06477775573730468
			 train-loss:  2.1778389206156135 	 ± 0.22537257773362482
	data : 0.11407685279846191
	model : 0.06478056907653809
			 train-loss:  2.1784364798272304 	 ± 0.22459911403530525
	data : 0.11414847373962403
	model : 0.0648472785949707
			 train-loss:  2.178622936285459 	 ± 0.22374362607653042
	data : 0.11437439918518066
	model : 0.06488070487976075
			 train-loss:  2.1789389848709106 	 ± 0.22291713446735845
	data : 0.11423454284667969
	model : 0.06486215591430664
			 train-loss:  2.1808080664186766 	 ± 0.22309916617439088
	data : 0.11409053802490235
	model : 0.0647857666015625
			 train-loss:  2.181441437032886 	 ± 0.2223779574870985
	data : 0.11395931243896484
	model : 0.06475419998168945
			 train-loss:  2.1803433726082986 	 ± 0.22190825959721083
	data : 0.11408023834228516
	model : 0.06474261283874512
			 train-loss:  2.1785911039069847 	 ± 0.22201340135915193
	data : 0.11399354934692382
	model : 0.06480388641357422
			 train-loss:  2.1776663319152942 	 ± 0.22145648943809496
	data : 0.11405220031738281
	model : 0.06486153602600098
			 train-loss:  2.178758009506838 	 ± 0.22101375101362317
	data : 0.11426420211791992
	model : 0.06495757102966308
			 train-loss:  2.1785863940266594 	 ± 0.2202206818384481
	data : 0.11444153785705566
	model : 0.06494641304016113
			 train-loss:  2.178999508885171 	 ± 0.2194807508080749
	data : 0.11416091918945312
	model : 0.06492910385131836
			 train-loss:  2.178471096072878 	 ± 0.21878420254594638
	data : 0.11419610977172852
	model : 0.06488018035888672
			 train-loss:  2.180397174882551 	 ± 0.21919493038870672
	data : 0.11418619155883789
	model : 0.06486001014709472
			 train-loss:  2.1816038220701084 	 ± 0.21889120149883803
	data : 0.11417112350463868
	model : 0.06483488082885742
			 train-loss:  2.178826441297998 	 ± 0.22062109030363544
	data : 0.1140172004699707
	model : 0.0648775577545166
			 train-loss:  2.176967367529869 	 ± 0.2209748495310244
	data : 0.11421747207641601
	model : 0.0649181842803955
			 train-loss:  2.181784249996317 	 ± 0.22767139844053252
	data : 0.11420674324035644
	model : 0.06492319107055664
			 train-loss:  2.1828906601422453 	 ± 0.22728118541921036
	data : 0.11430826187133789
	model : 0.06488556861877441
			 train-loss:  2.181954236257644 	 ± 0.2267892342346138
	data : 0.11423792839050292
	model : 0.064874267578125
			 train-loss:  2.1815105853854 	 ± 0.22608575258086677
	data : 0.11419835090637206
	model : 0.06478757858276367
			 train-loss:  2.180979866309454 	 ± 0.22541828114465473
	data : 0.11423196792602539
	model : 0.06470165252685547
			 train-loss:  2.1811289024353027 	 ± 0.22467299574634447
	data : 0.11429119110107422
	model : 0.06473250389099121
			 train-loss:  2.183268104957429 	 ± 0.22545529942630668
	data : 0.11428461074829102
	model : 0.06476173400878907
			 train-loss:  2.1830103240515055 	 ± 0.2247347717632067
	data : 0.11428136825561523
	model : 0.06480884552001953
			 train-loss:  2.1829295002557094 	 ± 0.22400135679869865
	data : 0.11447820663452149
	model : 0.06489877700805664
			 train-loss:  2.1822714062480184 	 ± 0.22342123475710562
	data : 0.11444072723388672
	model : 0.06498446464538574
			 train-loss:  2.1810712029857022 	 ± 0.223196858850278
	data : 0.11438727378845215
	model : 0.06499667167663574
			 train-loss:  2.1818409195313087 	 ± 0.22268662049766424
	data : 0.11421027183532714
	model : 0.06494250297546386
			 train-loss:  2.1827604573243744 	 ± 0.22227321273374656
	data : 0.11426563262939453
	model : 0.0648730754852295
			 train-loss:  2.184057223645947 	 ± 0.22216367945212998
	data : 0.1141197681427002
	model : 0.06482701301574707
			 train-loss:  2.184710959968327 	 ± 0.22161634780454567
	data : 0.11408114433288574
	model : 0.06482076644897461
			 train-loss:  2.1834889903664587 	 ± 0.22145939734563286
	data : 0.11403899192810059
	model : 0.06483669281005859
			 train-loss:  2.1831415277090134 	 ± 0.22081430831918433
	data : 0.11418461799621582
	model : 0.06488933563232421
			 train-loss:  2.1830028884204817 	 ± 0.22013875644420236
	data : 0.11410665512084961
	model : 0.06495451927185059
			 train-loss:  2.1843134216004354 	 ± 0.22009543259010178
	data : 0.11408929824829102
	model : 0.06494951248168945
			 train-loss:  2.183149624161604 	 ± 0.2199258797677803
	data : 0.11389803886413574
	model : 0.06489815711975097
			 train-loss:  2.1845218181610107 	 ± 0.21996148563774726
	data : 0.11394205093383789
	model : 0.06482086181640626
			 train-loss:  2.1821527832961944 	 ± 0.22139924299260416
	data : 0.11398143768310547
	model : 0.0648409366607666
			 train-loss:  2.182470776363761 	 ± 0.22077339499687032
	data : 0.1141247272491455
	model : 0.06484293937683105
			 train-loss:  2.1833855488470624 	 ± 0.2204325625535708
	data : 0.1142648696899414
	model : 0.06487345695495605
			 train-loss:  2.1829376439371053 	 ± 0.21985609101334525
	data : 0.11441926956176758
	model : 0.06496391296386719
			 train-loss:  2.183799121660345 	 ± 0.21949439522557812
	data : 0.114398193359375
	model : 0.06502904891967773
			 train-loss:  2.182975241315295 	 ± 0.21911513064157329
	data : 0.11431975364685058
	model : 0.06497936248779297
			 train-loss:  2.1828598082065582 	 ± 0.21848245406777966
	data : 0.11416072845458984
	model : 0.06490540504455566
			 train-loss:  2.1844998377596023 	 ± 0.21890931493364396
	data : 0.11406145095825196
	model : 0.0648625373840332
			 train-loss:  2.188079672983323 	 ± 0.22330004828086542
	data : 0.1141657829284668
	model : 0.06483707427978516
			 train-loss:  2.1880880226407733 	 ± 0.22266116134104677
	data : 0.11412200927734376
	model : 0.06483817100524902
			 train-loss:  2.1870016252452675 	 ± 0.22249234819803287
	data : 0.1141937255859375
	model : 0.06490559577941894
			 train-loss:  2.186868663561546 	 ± 0.22186996053248173
	data : 0.11431002616882324
	model : 0.06499152183532715
			 train-loss:  2.1864143114411427 	 ± 0.22132841299893669
	data : 0.11463313102722168
	model : 0.06500859260559082
			 train-loss:  2.1852865678638054 	 ± 0.22122156653557806
	data : 0.11459426879882813
	model : 0.06500053405761719
			 train-loss:  2.1859543171193865 	 ± 0.22078702954007068
	data : 0.11446833610534668
	model : 0.06489748954772949
			 train-loss:  2.1870834689113976 	 ± 0.22069682719408718
	data : 0.11473040580749512
	model : 0.06488757133483887
			 train-loss:  2.188636446392143 	 ± 0.22107915535290892
	data : 0.11467161178588867
	model : 0.0648615837097168
			 train-loss:  2.190762540030349 	 ± 0.22233218592862425
	data : 0.11445627212524415
	model : 0.06490578651428222
			 train-loss:  2.1903270580198453 	 ± 0.22180544595054372
	data : 0.11458568572998047
	model : 0.06491971015930176
			 train-loss:  2.1899258375167845 	 ± 0.22127210061500932
	data : 0.11482257843017578
	model : 0.06503801345825196
			 train-loss:  2.1913250780874685 	 ± 0.22149563473946407
	data : 0.11495981216430665
	model : 0.06502141952514648
			 train-loss:  2.19155029370823 	 ± 0.22092395950329075
	data : 0.11500167846679688
	model : 0.06503529548645019
			 train-loss:  2.1912505835928817 	 ± 0.2203737270306552
	data : 0.11494851112365723
	model : 0.06499919891357422
			 train-loss:  2.191552025300485 	 ± 0.21982881324641101
	data : 0.11463122367858887
	model : 0.06490182876586914
			 train-loss:  2.1905960013991908 	 ± 0.21964314036804497
	data : 0.11461319923400878
	model : 0.06482858657836914
			 train-loss:  2.19205613473323 	 ± 0.21999001196587686
	data : 0.11421570777893067
	model : 0.06480693817138672
			 train-loss:  2.1913876179605722 	 ± 0.21961080488702195
	data : 0.11417198181152344
	model : 0.06476178169250488
			 train-loss:  2.1904686442310948 	 ± 0.2194109417549302
	data : 0.11427063941955566
	model : 0.0648073673248291
			 train-loss:  2.190675196573906 	 ± 0.2188635309329982
	data : 0.11452693939208984
	model : 0.06487617492675782
			 train-loss:  2.191436900236668 	 ± 0.2185592713669962
	data : 0.11454901695251465
	model : 0.06491961479187011
			 train-loss:  2.190945892309656 	 ± 0.21810880847958616
	data : 0.114432954788208
	model : 0.06494703292846679
			 train-loss:  2.1909407014169062 	 ± 0.21755454066553667
	data : 0.11447229385375976
	model : 0.06495141983032227
			 train-loss:  2.1899169243947423 	 ± 0.217479694202661
	data : 0.11421942710876465
	model : 0.06489367485046386
			 train-loss:  2.1889683918737286 	 ± 0.21734278291907108
	data : 0.11406359672546387
	model : 0.06493167877197266
			 train-loss:  2.1886506098508836 	 ± 0.21684508746560552
	data : 0.11398544311523437
	model : 0.064937162399292
			 train-loss:  2.1888397694820196 	 ± 0.21632154069279352
	data : 0.11404428482055665
	model : 0.06490707397460938
			 train-loss:  2.187598227864445 	 ± 0.21650214098407883
	data : 0.11402449607849122
	model : 0.06494684219360351
			 train-loss:  2.187262695411156 	 ± 0.21602086998756154
	data : 0.1141179084777832
	model : 0.06496496200561523
			 train-loss:  2.1871333408589457 	 ± 0.21549863783389261
	data : 0.11401009559631348
	model : 0.06493010520935058
			 train-loss:  2.1872483119731996 	 ± 0.2149786605970423
	data : 0.1139531135559082
	model : 0.06490330696105957
			 train-loss:  2.187527632829055 	 ± 0.21449351966486993
	data : 0.11396942138671876
	model : 0.06493096351623535
			 train-loss:  2.1865616530036003 	 ± 0.21442349166060737
	data : 0.1139744758605957
	model : 0.06493158340454101
			 train-loss:  2.1855835874493303 	 ± 0.21436979067145545
	data : 0.11408438682556152
	model : 0.06499958038330078
			 train-loss:  2.1854924967414453 	 ± 0.21386036446683712
	data : 0.1142580509185791
	model : 0.06497273445129395
			 train-loss:  2.1889582446643283 	 ± 0.2191548498392351
	data : 0.11419901847839356
	model : 0.06494073867797852
			 train-loss:  2.1861555898359035 	 ± 0.22237523893942238
	data : 0.11407194137573243
	model : 0.06492218971252442
			 train-loss:  2.1863254797908493 	 ± 0.22186387416888123
	data : 0.11406302452087402
	model : 0.06490597724914551
			 train-loss:  2.185507769316015 	 ± 0.22166243691020546
	data : 0.1140434741973877
	model : 0.06489195823669433
			 train-loss:  2.1841426898385876 	 ± 0.2220395231603941
	data : 0.11407756805419922
	model : 0.06492695808410645
			 train-loss:  2.183464803252109 	 ± 0.22174440191151226
	data : 0.11411128044128419
	model : 0.06496353149414062
			 train-loss:  2.1821378800604077 	 ± 0.22208443014927468
	data : 0.11423816680908203
	model : 0.06497211456298828
			 train-loss:  2.180600120724621 	 ± 0.22272175946407377
	data : 0.11403365135192871
	model : 0.06490745544433593
			 train-loss:  2.1790874419956032 	 ± 0.22332481967869722
	data : 0.11392054557800294
	model : 0.06483573913574218
			 train-loss:  2.1789282554905163 	 ± 0.2228267583422964
	data : 0.11403031349182129
	model : 0.06480460166931153
			 train-loss:  2.1814692117951133 	 ± 0.22547735785258258
	data : 0.1141385555267334
	model : 0.06475286483764649
			 train-loss:  2.182275193848761 	 ± 0.22528405854576625
	data : 0.11407995223999023
	model : 0.06470818519592285
			 train-loss:  2.1825257904894717 	 ± 0.2248069590648101
	data : 0.11441178321838379
	model : 0.06470737457275391
			 train-loss:  2.1826100702243 	 ± 0.22430585629111602
	data : 0.11450600624084473
	model : 0.06465239524841308
			 train-loss:  2.1825456214802608 	 ± 0.22380668287193498
	data : 0.1142235279083252
	model : 0.06448602676391602
			 train-loss:  2.1832566187116833 	 ± 0.22356217798548322
	data : 0.11427621841430664
	model : 0.06430902481079101
			 train-loss:  2.1834150900882956 	 ± 0.22307968808961154
	data : 0.1144014835357666
	model : 0.06414508819580078
			 train-loss:  2.1830029046482977 	 ± 0.2226740147456904
	data : 0.11448092460632324
	model : 0.06405272483825683
			 train-loss:  2.1833041467164693 	 ± 0.22223150973828096
	data : 0.11466264724731445
	model : 0.06392159461975097
			 train-loss:  2.1833750129266596 	 ± 0.2217483390403839
	data : 0.11491756439208985
	model : 0.06389408111572266
			 train-loss:  2.182777508445408 	 ± 0.22145042064439385
	data : 0.11488080024719238
	model : 0.06390728950500488
			 train-loss:  2.18273232819198 	 ± 0.22097163317894813
	data : 0.11490592956542969
	model : 0.06386790275573731
			 train-loss:  2.183716182051034 	 ± 0.22100134810076197
	data : 0.11473627090454101
	model : 0.06381187438964844
			 train-loss:  2.1876847610964796 	 ± 0.22866107459671087
	data : 0.11473274230957031
	model : 0.06382408142089843
			 train-loss:  2.1889364118250008 	 ± 0.22897045171440314
	data : 0.11479930877685547
	model : 0.06380991935729981
			 train-loss:  2.187781612416531 	 ± 0.22916462550421102
	data : 0.11488380432128906
	model : 0.06381430625915527
			 train-loss:  2.1876994325953016 	 ± 0.22868206189675203
	data : 0.1148676872253418
	model : 0.06381168365478515
			 train-loss:  2.18829418184385 	 ± 0.22838193639268048
	data : 0.11493287086486817
	model : 0.06374497413635254
			 train-loss:  2.1868558666285347 	 ± 0.22897478074731545
	data : 0.11484012603759766
	model : 0.06373176574707032
			 train-loss:  2.185823137291306 	 ± 0.22905002564084448
	data : 0.11485729217529297
	model : 0.06376070976257324
			 train-loss:  2.1861743768056234 	 ± 0.2286368295820327
	data : 0.11499438285827637
	model : 0.06376028060913086
			 train-loss:  2.1867741331519923 	 ± 0.22835109327561098
	data : 0.11506114006042481
	model : 0.06380696296691894
			 train-loss:  2.186319848722663 	 ± 0.22798790763076715
	data : 0.11512541770935059
	model : 0.06382989883422852
			 train-loss:  2.1879738897943692 	 ± 0.22896868550736096
	data : 0.11499390602111817
	model : 0.06376314163208008
			 train-loss:  2.188526746679525 	 ± 0.22866147186476968
	data : 0.11484370231628419
	model : 0.06376948356628417
			 train-loss:  2.188860660669755 	 ± 0.22825394155828863
	data : 0.11479058265686035
	model : 0.06377997398376464
			 train-loss:  2.1894386725697093 	 ± 0.22796913767215687
	data : 0.11477389335632324
	model : 0.06376910209655762
			 train-loss:  2.190911486081266 	 ± 0.22867693665228095
	data : 0.11471662521362305
	model : 0.06380181312561035
			 train-loss:  2.1896663798439886 	 ± 0.22905283982657254
	data : 0.11482105255126954
	model : 0.06386342048645019
			 train-loss:  2.1907271419663026 	 ± 0.22920199266552593
	data : 0.11472516059875489
	model : 0.06375961303710938
			 train-loss:  2.1913240604400634 	 ± 0.22893698004580937
	data : 0.1147273063659668
	model : 0.06374115943908691
			 train-loss:  2.1902803234845045 	 ± 0.22907569541639644
	data : 0.11477904319763184
	model : 0.06376433372497559
			 train-loss:  2.190770451038603 	 ± 0.2287525604399753
	data : 0.11487278938293458
	model : 0.06377754211425782
			 train-loss:  2.1906263950785156 	 ± 0.2283114855941657
	data : 0.11496567726135254
	model : 0.06380639076232911
			 train-loss:  2.1914730869878936 	 ± 0.22825925153506102
	data : 0.11514110565185547
	model : 0.06383929252624512
			 train-loss:  2.1916155104543646 	 ± 0.22782255259351186
	data : 0.11495318412780761
	model : 0.06378145217895508
			 train-loss:  2.192880413495004 	 ± 0.2282725646009254
	data : 0.1146580696105957
	model : 0.055333948135375975
#epoch  66    val-loss:  2.409447983691567  train-loss:  2.192880413495004  lr:  1.9073486328125e-08
			 train-loss:  1.8368120193481445 	 ± 0.0
	data : 5.603899955749512
	model : 0.07150602340698242
			 train-loss:  1.9189683198928833 	 ± 0.08215630054473877
	data : 2.8646490573883057
	model : 0.0681447982788086
			 train-loss:  1.925605336825053 	 ± 0.06773383054511319
	data : 1.9478750228881836
	model : 0.06696144739786784
			 train-loss:  1.9197576344013214 	 ± 0.05952722672143501
	data : 1.4893890619277954
	model : 0.06642156839370728
			 train-loss:  1.9822939157485961 	 ± 0.13593358126636013
	data : 1.214350652694702
	model : 0.06609144210815429
			 train-loss:  1.9672183394432068 	 ± 0.12858711872221054
	data : 0.11648616790771485
	model : 0.06476759910583496
			 train-loss:  2.0026647533689226 	 ± 0.14734735905132126
	data : 0.1142042636871338
	model : 0.06477370262145996
			 train-loss:  2.0076934546232224 	 ± 0.13847149001187603
	data : 0.1141286849975586
	model : 0.06482195854187012
			 train-loss:  2.0636866754955716 	 ± 0.20524569688069247
	data : 0.11410965919494628
	model : 0.064827299118042
			 train-loss:  2.1255431056022642 	 ± 0.2689780251045326
	data : 0.11414098739624023
	model : 0.0648587703704834
			 train-loss:  2.111969330094077 	 ± 0.2600277942943579
	data : 0.1139498233795166
	model : 0.06489019393920899
			 train-loss:  2.1443662146727243 	 ± 0.27115503945691377
	data : 0.11396784782409668
	model : 0.06492118835449219
			 train-loss:  2.124068076793964 	 ± 0.26983968341397585
	data : 0.11403183937072754
	model : 0.06494207382202148
			 train-loss:  2.113905123301915 	 ± 0.26259323448939786
	data : 0.11411089897155761
	model : 0.06497359275817871
			 train-loss:  2.133199946085612 	 ± 0.26376174025154514
	data : 0.11414146423339844
	model : 0.06496133804321289
			 train-loss:  2.125164344906807 	 ± 0.2572754903577351
	data : 0.1141080379486084
	model : 0.06492371559143066
			 train-loss:  2.1204238919650806 	 ± 0.25031312181701176
	data : 0.11415538787841797
	model : 0.06491398811340332
			 train-loss:  2.1270019080903797 	 ± 0.24476790739556892
	data : 0.11406340599060058
	model : 0.06489157676696777
			 train-loss:  2.140194943076686 	 ± 0.2447266061837137
	data : 0.11399788856506347
	model : 0.06489925384521485
			 train-loss:  2.1835874795913695 	 ± 0.30442057825297175
	data : 0.11391596794128418
	model : 0.06491122245788575
			 train-loss:  2.1808735756647017 	 ± 0.29733188003412797
	data : 0.11408014297485351
	model : 0.06495676040649415
			 train-loss:  2.159821716221896 	 ± 0.306095700105602
	data : 0.11394648551940918
	model : 0.06497550010681152
			 train-loss:  2.1733734711356787 	 ± 0.30604117644939066
	data : 0.11400895118713379
	model : 0.06495785713195801
			 train-loss:  2.1698367496331534 	 ± 0.30007723137404063
	data : 0.11397442817687989
	model : 0.06498494148254394
			 train-loss:  2.1836021041870115 	 ± 0.3016490189962293
	data : 0.11392841339111329
	model : 0.06497421264648437
			 train-loss:  2.182394605416518 	 ± 0.2958528091133762
	data : 0.1138110637664795
	model : 0.06492500305175782
			 train-loss:  2.172230844144468 	 ± 0.29491172787866277
	data : 0.113991117477417
	model : 0.06494269371032715
			 train-loss:  2.175963248525347 	 ± 0.29024624720053266
	data : 0.1140519618988037
	model : 0.06502642631530761
			 train-loss:  2.164720457175682 	 ± 0.2913368616984918
	data : 0.11406240463256836
	model : 0.06498799324035645
			 train-loss:  2.1701322118441264 	 ± 0.28791883171130017
	data : 0.11415514945983887
	model : 0.0649998664855957
			 train-loss:  2.177486600414399 	 ± 0.28608697914217646
	data : 0.11424365043640136
	model : 0.06499719619750977
			 train-loss:  2.174089442938566 	 ± 0.28221594691387597
	data : 0.11402430534362792
	model : 0.06491327285766602
			 train-loss:  2.170137250062191 	 ± 0.2788048867604492
	data : 0.11393327713012695
	model : 0.06486916542053223
			 train-loss:  2.170216977596283 	 ± 0.274674597481753
	data : 0.1140106201171875
	model : 0.06485128402709961
			 train-loss:  2.1848070110593523 	 ± 0.2837746980586429
	data : 0.11401619911193847
	model : 0.0648561954498291
			 train-loss:  2.1785402132405176 	 ± 0.2822511905897683
	data : 0.11406731605529785
	model : 0.06489558219909668
			 train-loss:  2.1757896300908683 	 ± 0.2788995735087102
	data : 0.11421175003051758
	model : 0.06492400169372559
			 train-loss:  2.169301848662527 	 ± 0.27802046245581263
	data : 0.11430139541625976
	model : 0.06491494178771973
			 train-loss:  2.1625296030289087 	 ± 0.2775900765546926
	data : 0.1142873764038086
	model : 0.06521053314208984
			 train-loss:  2.1558195292949676 	 ± 0.27728292920351444
	data : 0.1142317771911621
	model : 0.0652552604675293
			 train-loss:  2.1626655183187347 	 ± 0.27728191698128424
	data : 0.11408448219299316
	model : 0.06517047882080078
			 train-loss:  2.1668403091884794 	 ± 0.2752621369748153
	data : 0.11401467323303223
	model : 0.0651632308959961
			 train-loss:  2.168085031731184 	 ± 0.2721621587319274
	data : 0.11394352912902832
	model : 0.065155029296875
			 train-loss:  2.168330143798481 	 ± 0.26905643286817654
	data : 0.11396203041076661
	model : 0.06490597724914551
			 train-loss:  2.1727300855848526 	 ± 0.267646192852236
	data : 0.1139653205871582
	model : 0.06487026214599609
			 train-loss:  2.1733582641767417 	 ± 0.26475454768619433
	data : 0.11401400566101075
	model : 0.06497735977172851
			 train-loss:  2.1691215697755206 	 ± 0.2634943430582345
	data : 0.11408615112304688
	model : 0.06498937606811524
			 train-loss:  2.167799567182859 	 ± 0.26089263537589985
	data : 0.11410083770751953
	model : 0.06500005722045898
			 train-loss:  2.173279484924005 	 ± 0.2609929159112158
	data : 0.11395030021667481
	model : 0.06491756439208984
			 train-loss:  2.1777168703079224 	 ± 0.2602302541518702
	data : 0.11397280693054199
	model : 0.06489291191101074
			 train-loss:  2.181104907802507 	 ± 0.2587776770785377
	data : 0.11397690773010254
	model : 0.06482701301574707
			 train-loss:  2.177448788514504 	 ± 0.25760397565226495
	data : 0.11400136947631836
	model : 0.06482486724853516
			 train-loss:  2.1756265950652787 	 ± 0.25550028697424115
	data : 0.11399388313293457
	model : 0.06487293243408203
			 train-loss:  2.1726121571328907 	 ± 0.2540730263533079
	data : 0.11415939331054688
	model : 0.06491336822509766
			 train-loss:  2.1716548507863824 	 ± 0.2518509429353261
	data : 0.11408829689025879
	model : 0.0649329662322998
			 train-loss:  2.1723543469394957 	 ± 0.24964604902140336
	data : 0.1141270637512207
	model : 0.06493511199951171
			 train-loss:  2.1668694939529685 	 ± 0.25082751740398906
	data : 0.11410102844238282
	model : 0.06499977111816406
			 train-loss:  2.166601275575572 	 ± 0.24866405519369594
	data : 0.11411809921264648
	model : 0.06495828628540039
			 train-loss:  2.1670714879440047 	 ± 0.24657373219505369
	data : 0.11409621238708496
	model : 0.06496610641479492
			 train-loss:  2.1654859224955243 	 ± 0.24481344401022648
	data : 0.11422648429870605
	model : 0.06496095657348633
			 train-loss:  2.1629604355233614 	 ± 0.24358528082678615
	data : 0.1142547607421875
	model : 0.06501617431640624
			 train-loss:  2.1758197007640714 	 ± 0.26165587188615713
	data : 0.11435589790344239
	model : 0.06493821144104003
			 train-loss:  2.170557735458253 	 ± 0.26285688439460336
	data : 0.1143031120300293
	model : 0.06495141983032227
			 train-loss:  2.1668475195765495 	 ± 0.26245264559341835
	data : 0.11434845924377442
	model : 0.06491804122924805
			 train-loss:  2.1666745039132924 	 ± 0.2604296320916677
	data : 0.11430349349975585
	model : 0.06484718322753906
			 train-loss:  2.1630172476624 	 ± 0.2601256851674295
	data : 0.1144186019897461
	model : 0.06479268074035645
			 train-loss:  2.1607397100818693 	 ± 0.2588393224910554
	data : 0.11443972587585449
	model : 0.0648115634918213
			 train-loss:  2.160455107688904 	 ± 0.25693960395271537
	data : 0.11447625160217285
	model : 0.06481118202209472
			 train-loss:  2.1574394996615425 	 ± 0.25628024248841536
	data : 0.11454353332519532
	model : 0.06485567092895508
			 train-loss:  2.1662759116717747 	 ± 0.26481871808980584
	data : 0.11457438468933105
	model : 0.06491036415100097
			 train-loss:  2.16388720861623 	 ± 0.263705583695369
	data : 0.11451764106750488
	model : 0.06492648124694825
			 train-loss:  2.1624081002341375 	 ± 0.262164306867159
	data : 0.11434202194213867
	model : 0.06491689682006836
			 train-loss:  2.1650595664978027 	 ± 0.26133272839621247
	data : 0.1142756462097168
	model : 0.06481084823608399
			 train-loss:  2.1671183012627266 	 ± 0.26015628895255266
	data : 0.11409602165222169
	model : 0.06478590965270996
			 train-loss:  2.17323655128479 	 ± 0.2637212888368071
	data : 0.11417145729064941
	model : 0.0647733211517334
			 train-loss:  2.1717880462345325 	 ± 0.2622806957388286
	data : 0.11408581733703613
	model : 0.06477169990539551
			 train-loss:  2.175633009378012 	 ± 0.2627191199456375
	data : 0.11418156623840332
	model : 0.06479134559631347
			 train-loss:  2.177628740286216 	 ± 0.2616163863086339
	data : 0.11427884101867676
	model : 0.06487841606140136
			 train-loss:  2.180822752699067 	 ± 0.2614813542081311
	data : 0.11436252593994141
	model : 0.06486325263977051
			 train-loss:  2.1826972901821136 	 ± 0.2603755743868632
	data : 0.11419119834899902
	model : 0.06482200622558594
			 train-loss:  2.1863068710138767 	 ± 0.2607696011851729
	data : 0.11410279273986816
	model : 0.06474623680114747
			 train-loss:  2.1871338443058295 	 ± 0.2592815105411222
	data : 0.11411356925964355
	model : 0.0646751880645752
			 train-loss:  2.188605044261519 	 ± 0.2580589510958463
	data : 0.11414337158203125
	model : 0.06467223167419434
			 train-loss:  2.1882902610869635 	 ± 0.2565343170821935
	data : 0.11417360305786133
	model : 0.06474857330322266
			 train-loss:  2.1914900022394517 	 ± 0.25670146686746037
	data : 0.11426925659179688
	model : 0.06483306884765624
			 train-loss:  2.1904924525771032 	 ± 0.2553703166713251
	data : 0.11436901092529297
	model : 0.06492447853088379
			 train-loss:  2.1881553181286515 	 ± 0.25482182289705296
	data : 0.1143531322479248
	model : 0.06497111320495605
			 train-loss:  2.191709666089578 	 ± 0.25552960319060736
	data : 0.11425247192382812
	model : 0.06493177413940429
			 train-loss:  2.189492060897056 	 ± 0.2549401619566383
	data : 0.11417851448059083
	model : 0.06485562324523926
			 train-loss:  2.1877566589249504 	 ± 0.2540479470520562
	data : 0.11392583847045898
	model : 0.06483845710754395
			 train-loss:  2.18482690853077 	 ± 0.25417244942394174
	data : 0.113978910446167
	model : 0.06488223075866699
			 train-loss:  2.1837136615877566 	 ± 0.25301027378236757
	data : 0.11406035423278808
	model : 0.06491069793701172
			 train-loss:  2.1868202404309343 	 ± 0.2534043221353823
	data : 0.11413602828979492
	model : 0.0649648666381836
			 train-loss:  2.1833355883334544 	 ± 0.25428312152831356
	data : 0.11413531303405762
	model : 0.06496996879577636
			 train-loss:  2.181417979692158 	 ± 0.2536236074550903
	data : 0.11433572769165039
	model : 0.06497082710266114
			 train-loss:  2.18270101894935 	 ± 0.2526089287513001
	data : 0.11426897048950195
	model : 0.06491580009460449
			 train-loss:  2.189998673409531 	 ± 0.26127758371311977
	data : 0.11416592597961425
	model : 0.06489095687866211
			 train-loss:  2.1961496825120888 	 ± 0.2669070450938051
	data : 0.11412925720214843
	model : 0.06486020088195801
			 train-loss:  2.1932976366293553 	 ± 0.267052297046392
	data : 0.1141690731048584
	model : 0.06486430168151855
			 train-loss:  2.192590253353119 	 ± 0.26580688249747636
	data : 0.1144406795501709
	model : 0.0648874282836914
			 train-loss:  2.19159468093721 	 ± 0.2646750415412189
	data : 0.11451997756958007
	model : 0.06489768028259277
			 train-loss:  2.1945849586935604 	 ± 0.2650833879444842
	data : 0.11449050903320312
	model : 0.0648688793182373
			 train-loss:  2.193720146290307 	 ± 0.2639379911016616
	data : 0.11428284645080566
	model : 0.06481337547302246
			 train-loss:  2.1925251690241008 	 ± 0.26294582152363055
	data : 0.11426768302917481
	model : 0.06487154960632324
			 train-loss:  2.1902676763988675 	 ± 0.26270142071048397
	data : 0.1140181541442871
	model : 0.06486601829528808
			 train-loss:  2.1901045048011922 	 ± 0.26146467279081614
	data : 0.11396188735961914
	model : 0.06486420631408692
			 train-loss:  2.18961457448585 	 ± 0.26028888699507546
	data : 0.11401963233947754
	model : 0.06493654251098632
			 train-loss:  2.187008947134018 	 ± 0.26047925350020784
	data : 0.11434130668640137
	model : 0.0650374412536621
			 train-loss:  2.189232223624483 	 ± 0.26030906450938585
	data : 0.11436977386474609
	model : 0.06498556137084961
			 train-loss:  2.1883522608063437 	 ± 0.2592859505985447
	data : 0.11448240280151367
	model : 0.0649216651916504
			 train-loss:  2.1895706449543035 	 ± 0.2584314730527314
	data : 0.11431946754455566
	model : 0.06484665870666503
			 train-loss:  2.191243225974696 	 ± 0.2578779572634358
	data : 0.1143376350402832
	model : 0.06478543281555176
			 train-loss:  2.1937036925712516 	 ± 0.2580514917490242
	data : 0.11427745819091797
	model : 0.06475973129272461
			 train-loss:  2.1918179601953742 	 ± 0.25769802298608746
	data : 0.11427121162414551
	model : 0.0647824764251709
			 train-loss:  2.193925230399422 	 ± 0.2575597694199836
	data : 0.11420526504516601
	model : 0.06486501693725585
			 train-loss:  2.19601129560635 	 ± 0.25742106685212224
	data : 0.11442365646362304
	model : 0.06493382453918457
			 train-loss:  2.194803075912671 	 ± 0.25664872717668524
	data : 0.11438264846801757
	model : 0.06496310234069824
			 train-loss:  2.195080717741433 	 ± 0.25557656365839665
	data : 0.11448149681091309
	model : 0.06497130393981934
			 train-loss:  2.1939223624077164 	 ± 0.2548113192437597
	data : 0.1144193172454834
	model : 0.06493091583251953
			 train-loss:  2.193664367000262 	 ± 0.2537629914808124
	data : 0.11429519653320312
	model : 0.0648714542388916
			 train-loss:  2.1977965664272467 	 ± 0.25673423806528395
	data : 0.11424603462219238
	model : 0.06489729881286621
			 train-loss:  2.197408407437997 	 ± 0.25571553259661817
	data : 0.1143378734588623
	model : 0.06492156982421875
			 train-loss:  2.197815335863005 	 ± 0.25471357667049
	data : 0.11417226791381836
	model : 0.06494503021240235
			 train-loss:  2.1964937285069497 	 ± 0.2541075083645896
	data : 0.11425585746765136
	model : 0.06498823165893555
			 train-loss:  2.1992404317855834 	 ± 0.2549305077555613
	data : 0.11444849967956543
	model : 0.06501889228820801
			 train-loss:  2.1987402770254345 	 ± 0.2539784300757739
	data : 0.11454877853393555
	model : 0.06498370170593262
			 train-loss:  2.2017945802117898 	 ± 0.2552891575880468
	data : 0.11428742408752442
	model : 0.06494841575622559
			 train-loss:  2.2027665758505464 	 ± 0.25452579416494103
	data : 0.11430063247680664
	model : 0.06487250328063965
			 train-loss:  2.2026255380275637 	 ± 0.2535423619148505
	data : 0.11438846588134766
	model : 0.06485352516174317
			 train-loss:  2.20018389408405 	 ± 0.2540832318435019
	data : 0.1143789291381836
	model : 0.06489572525024415
			 train-loss:  2.1992923358014522 	 ± 0.2533156356053663
	data : 0.11446142196655273
	model : 0.06491646766662598
			 train-loss:  2.197310468464187 	 ± 0.2533717139716384
	data : 0.11465306282043457
	model : 0.06498098373413086
			 train-loss:  2.196485525683353 	 ± 0.252595268311686
	data : 0.11483922004699706
	model : 0.06504640579223633
			 train-loss:  2.1972908840250613 	 ± 0.251822321559267
	data : 0.11477499008178711
	model : 0.06505060195922852
			 train-loss:  2.2018060481106794 	 ± 0.2562743909074462
	data : 0.11474127769470215
	model : 0.06499156951904297
			 train-loss:  2.200597483445616 	 ± 0.2557163123102897
	data : 0.11442332267761231
	model : 0.06497316360473633
			 train-loss:  2.200607589561574 	 ± 0.2547813590628042
	data : 0.11442384719848633
	model : 0.06489315032958984
			 train-loss:  2.2049321445865906 	 ± 0.25885381257128215
	data : 0.11429710388183593
	model : 0.06488089561462403
			 train-loss:  2.2082019100943917 	 ± 0.26076551222340466
	data : 0.1143143653869629
	model : 0.06488122940063476
			 train-loss:  2.206384972163609 	 ± 0.26071406430061184
	data : 0.11425418853759765
	model : 0.06489176750183105
			 train-loss:  2.2068126624357616 	 ± 0.2598371842343261
	data : 0.11437435150146484
	model : 0.06493010520935058
			 train-loss:  2.206004918461115 	 ± 0.259098239108623
	data : 0.11431984901428223
	model : 0.06499433517456055
			 train-loss:  2.206937401444762 	 ± 0.25842971222665945
	data : 0.11479921340942383
	model : 0.06498379707336426
			 train-loss:  2.2077450818485684 	 ± 0.2577118755293591
	data : 0.11473097801208496
	model : 0.06497950553894043
			 train-loss:  2.2087428602679022 	 ± 0.25710063082097523
	data : 0.11451530456542969
	model : 0.06489472389221192
			 train-loss:  2.2090475934825533 	 ± 0.2562449114631364
	data : 0.11450986862182617
	model : 0.06482172012329102
			 train-loss:  2.2111006804874966 	 ± 0.2565739510569638
	data : 0.11451711654663085
	model : 0.06479411125183106
			 train-loss:  2.211837438312737 	 ± 0.2558616562551457
	data : 0.11396903991699218
	model : 0.06480846405029297
			 train-loss:  2.211284578246558 	 ± 0.2550902983238397
	data : 0.11410117149353027
	model : 0.06483588218688965
			 train-loss:  2.2094043922424316 	 ± 0.2552723703074675
	data : 0.11436524391174316
	model : 0.06494932174682617
			 train-loss:  2.2077794682900636 	 ± 0.2552028412135483
	data : 0.11449589729309081
	model : 0.06504712104797364
			 train-loss:  2.209634136996771 	 ± 0.2553809359608798
	data : 0.11454987525939941
	model : 0.06506271362304687
			 train-loss:  2.2085193535860848 	 ± 0.2549157674787687
	data : 0.11448020935058593
	model : 0.06505427360534669
			 train-loss:  2.2058790070669994 	 ± 0.25617711823206
	data : 0.11435580253601074
	model : 0.06503658294677735
			 train-loss:  2.205353312338552 	 ± 0.2554327238246248
	data : 0.11436781883239747
	model : 0.06505393981933594
			 train-loss:  2.2031800341911807 	 ± 0.25604632389166193
	data : 0.11432700157165528
	model : 0.06501550674438476
			 train-loss:  2.203066316379863 	 ± 0.2552335391278184
	data : 0.11468362808227539
	model : 0.0650294303894043
			 train-loss:  2.201912707165827 	 ± 0.2548348338063579
	data : 0.11494083404541015
	model : 0.06504831314086915
			 train-loss:  2.200899394803077 	 ± 0.25435132115045084
	data : 0.11490049362182617
	model : 0.06507959365844726
			 train-loss:  2.1994253642857076 	 ± 0.25423556624423527
	data : 0.11486606597900391
	model : 0.06506080627441406
			 train-loss:  2.1979762278728603 	 ± 0.2541067855046562
	data : 0.11476335525512696
	model : 0.06507019996643067
			 train-loss:  2.1973100562154513 	 ± 0.253462277134703
	data : 0.1148538589477539
	model : 0.06499338150024414
			 train-loss:  2.196150740231473 	 ± 0.253114058318753
	data : 0.11462006568908692
	model : 0.06498565673828124
			 train-loss:  2.1952519489497675 	 ± 0.25260196227925413
	data : 0.11458864212036132
	model : 0.06494183540344238
			 train-loss:  2.1941104296481972 	 ± 0.25225927253245034
	data : 0.11501889228820801
	model : 0.0649540901184082
			 train-loss:  2.195413569369948 	 ± 0.25205475066949934
	data : 0.11500892639160157
	model : 0.0649268627166748
			 train-loss:  2.1969993557044845 	 ± 0.252128165515294
	data : 0.11458368301391601
	model : 0.06499772071838379
			 train-loss:  2.1967591373693374 	 ± 0.2513958312333196
	data : 0.11473870277404785
	model : 0.06502327919006348
			 train-loss:  2.196632170818261 	 ± 0.25065635542365045
	data : 0.11482744216918946
	model : 0.06509580612182617
			 train-loss:  2.1978982224183925 	 ± 0.2504594105014671
	data : 0.1143143653869629
	model : 0.06506476402282715
			 train-loss:  2.1989968659584984 	 ± 0.25013649811592586
	data : 0.11456727981567383
	model : 0.06506843566894531
			 train-loss:  2.1973095328308814 	 ± 0.2503824107416297
	data : 0.11448945999145507
	model : 0.06505784988403321
			 train-loss:  2.1962500142224264 	 ± 0.25004411122654363
	data : 0.11431865692138672
	model : 0.06497793197631836
			 train-loss:  2.195954535199308 	 ± 0.2493548468503855
	data : 0.11456437110900879
	model : 0.06487140655517579
			 train-loss:  2.1953803471156528 	 ± 0.24875671670766267
	data : 0.1145247459411621
	model : 0.0648618221282959
			 train-loss:  2.1951320469379425 	 ± 0.24807076211058193
	data : 0.11432375907897949
	model : 0.06488752365112305
			 train-loss:  2.1968642412605934 	 ± 0.24843411997588624
	data : 0.11444673538208008
	model : 0.06488327980041504
			 train-loss:  2.1957626141858904 	 ± 0.24816844519073167
	data : 0.11457805633544922
	model : 0.0649726390838623
			 train-loss:  2.1987994329889395 	 ± 0.25076896995866377
	data : 0.11432642936706543
	model : 0.06500658988952637
			 train-loss:  2.1988198664453296 	 ± 0.2500715687485103
	data : 0.11440157890319824
	model : 0.06501364707946777
			 train-loss:  2.197714768720595 	 ± 0.2498201578606737
	data : 0.11436696052551269
	model : 0.06496691703796387
			 train-loss:  2.197432284826761 	 ± 0.24916187882263754
	data : 0.11467781066894531
	model : 0.06492500305175782
			 train-loss:  2.195844922560812 	 ± 0.24940125654289813
	data : 0.11433396339416504
	model : 0.06490917205810547
			 train-loss:  2.195425335479819 	 ± 0.24878737042171864
	data : 0.11433238983154297
	model : 0.06494131088256835
			 train-loss:  2.193805472915237 	 ± 0.249085115858499
	data : 0.1143986701965332
	model : 0.06494612693786621
			 train-loss:  2.19379509392605 	 ± 0.2484146699202757
	data : 0.11448163986206054
	model : 0.06499710083007812
			 train-loss:  2.1950528098937663 	 ± 0.2483426524822276
	data : 0.1140592098236084
	model : 0.06505417823791504
			 train-loss:  2.1951397847622 	 ± 0.24768414169519518
	data : 0.11437654495239258
	model : 0.06503424644470215
			 train-loss:  2.1944862976276056 	 ± 0.24719047107634803
	data : 0.11442680358886718
	model : 0.06504259109497071
			 train-loss:  2.1970201906404996 	 ± 0.2489880109233277
	data : 0.114262056350708
	model : 0.0649643898010254
			 train-loss:  2.196514260706477 	 ± 0.24843325390430646
	data : 0.11423063278198242
	model : 0.06490955352783204
			 train-loss:  2.1956510059535503 	 ± 0.24807249539200993
	data : 0.11423225402832031
	model : 0.0649071216583252
			 train-loss:  2.197450205452084 	 ± 0.2486817843644314
	data : 0.11429481506347657
	model : 0.06489648818969726
			 train-loss:  2.1980764878164862 	 ± 0.24819257391150343
	data : 0.11427316665649415
	model : 0.06487498283386231
			 train-loss:  2.1971650942777976 	 ± 0.24788062127723146
	data : 0.11440958976745605
	model : 0.0649322509765625
			 train-loss:  2.2001508960918503 	 ± 0.25073838147990457
	data : 0.11446895599365234
	model : 0.06494202613830566
			 train-loss:  2.198309179489988 	 ± 0.2514267618210984
	data : 0.1144589900970459
	model : 0.06490387916564941
			 train-loss:  2.1974927825157087 	 ± 0.25105267987237573
	data : 0.11434054374694824
	model : 0.06483702659606934
			 train-loss:  2.1980305748369227 	 ± 0.2505354124699999
	data : 0.11424107551574707
	model : 0.06480989456176758
			 train-loss:  2.199690682888031 	 ± 0.2510031647739427
	data : 0.1142080307006836
	model : 0.06480932235717773
			 train-loss:  2.1989451166409166 	 ± 0.25059991384125596
	data : 0.11418595314025878
	model : 0.06481261253356933
			 train-loss:  2.1982124441921123 	 ± 0.25019456978939103
	data : 0.11430373191833496
	model : 0.0648561954498291
			 train-loss:  2.1977171498566426 	 ± 0.24967682208203157
	data : 0.1144484043121338
	model : 0.0649488925933838
			 train-loss:  2.1979445148916805 	 ± 0.2490851833873941
	data : 0.11484270095825196
	model : 0.06494369506835937
			 train-loss:  2.197961889825216 	 ± 0.2484770397787507
	data : 0.11492743492126464
	model : 0.06494841575622559
			 train-loss:  2.199617499286689 	 ± 0.24900409682637056
	data : 0.11490769386291504
	model : 0.06494112014770508
			 train-loss:  2.199727719532695 	 ± 0.2484069468282111
	data : 0.11468706130981446
	model : 0.0648911476135254
			 train-loss:  2.199590293260721 	 ± 0.24781698310422573
	data : 0.11464910507202149
	model : 0.06486539840698242
			 train-loss:  2.201173143523732 	 ± 0.24827512982589653
	data : 0.11428775787353515
	model : 0.06498351097106933
			 train-loss:  2.2009014822187876 	 ± 0.24771442808279606
	data : 0.11415777206420899
	model : 0.06499195098876953
			 train-loss:  2.202037533312612 	 ± 0.2476744822901245
	data : 0.11421270370483398
	model : 0.06503572463989257
			 train-loss:  2.2017940867621943 	 ± 0.24711495752372548
	data : 0.11454081535339355
	model : 0.0650710105895996
			 train-loss:  2.201973218873073 	 ± 0.24654798933908942
	data : 0.11446022987365723
	model : 0.06509194374084473
			 train-loss:  2.201136683749261 	 ± 0.24627407541130134
	data : 0.11459684371948242
	model : 0.06505742073059081
			 train-loss:  2.200844453101934 	 ± 0.2457378649071174
	data : 0.11470599174499511
	model : 0.06505393981933594
			 train-loss:  2.2009671849233134 	 ± 0.24517497204303446
	data : 0.11467170715332031
	model : 0.06501636505126954
			 train-loss:  2.2008505504801525 	 ± 0.24461540660798714
	data : 0.11430850028991699
	model : 0.06496105194091797
			 train-loss:  2.199261337792108 	 ± 0.24517396156577634
	data : 0.11427187919616699
	model : 0.06487364768981933
			 train-loss:  2.1989391476051994 	 ± 0.2446598151009112
	data : 0.11431822776794434
	model : 0.0648303508758545
			 train-loss:  2.199675316702236 	 ± 0.24434612195895078
	data : 0.11436352729797364
	model : 0.06478638648986816
			 train-loss:  2.1988003485882444 	 ± 0.24413785869021745
	data : 0.11438655853271484
	model : 0.06479706764221191
			 train-loss:  2.197737086463619 	 ± 0.24409968673008023
	data : 0.11462249755859374
	model : 0.06481971740722656
			 train-loss:  2.200500704782426 	 ± 0.2470081080232396
	data : 0.11470322608947754
	model : 0.06479010581970215
			 train-loss:  2.2021643311849664 	 ± 0.24770509182636735
	data : 0.11461071968078614
	model : 0.06460270881652833
			 train-loss:  2.2038708861668903 	 ± 0.24847026588057566
	data : 0.11441187858581543
	model : 0.0643951416015625
			 train-loss:  2.2030021864756018 	 ± 0.24826214406736197
	data : 0.11440019607543946
	model : 0.06421098709106446
			 train-loss:  2.201479253264776 	 ± 0.2487704643440687
	data : 0.11447024345397949
	model : 0.06407380104064941
			 train-loss:  2.2037224727764464 	 ± 0.25051463532107837
	data : 0.11469907760620117
	model : 0.06400122642517089
			 train-loss:  2.2029059859863014 	 ± 0.2502709097879737
	data : 0.11477174758911132
	model : 0.06397857666015624
			 train-loss:  2.203315385528233 	 ± 0.24980308671190843
	data : 0.11494221687316894
	model : 0.06400666236877442
			 train-loss:  2.2029712757506927 	 ± 0.24931642558320538
	data : 0.11488986015319824
	model : 0.06395187377929687
			 train-loss:  2.2037093659927107 	 ± 0.24903131982727447
	data : 0.11478247642517089
	model : 0.06388864517211915
			 train-loss:  2.2027430273432587 	 ± 0.24893187141582782
	data : 0.11466393470764161
	model : 0.06387619972229004
			 train-loss:  2.201543489582518 	 ± 0.24907332576067145
	data : 0.11469426155090331
	model : 0.06387748718261718
			 train-loss:  2.2007578651955786 	 ± 0.24883319368673879
	data : 0.11477108001708984
	model : 0.06391568183898926
			 train-loss:  2.2025328243182876 	 ± 0.24979182697569047
	data : 0.114902925491333
	model : 0.06392474174499511
			 train-loss:  2.200874927174693 	 ± 0.2505620836399042
	data : 0.11515531539916993
	model : 0.06390905380249023
			 train-loss:  2.199804430248357 	 ± 0.25057766047699237
	data : 0.11503071784973144
	model : 0.06385631561279297
			 train-loss:  2.1997141458998164 	 ± 0.25005676908211455
	data : 0.1150090217590332
	model : 0.06384754180908203
			 train-loss:  2.1988825142383575 	 ± 0.24986626002009898
	data : 0.1150050163269043
	model : 0.06385035514831543
			 train-loss:  2.1977704966216662 	 ± 0.24994173151882215
	data : 0.11508045196533204
	model : 0.06388893127441406
			 train-loss:  2.1966167629257707 	 ± 0.25006703204241637
	data : 0.11492552757263183
	model : 0.06395163536071777
			 train-loss:  2.195529391245587 	 ± 0.25012460134752384
	data : 0.11520276069641114
	model : 0.06396946907043458
			 train-loss:  2.194619351723155 	 ± 0.2500143176771786
	data : 0.11520166397094726
	model : 0.06398859024047851
			 train-loss:  2.195089849160642 	 ± 0.2496117816426563
	data : 0.1150092601776123
	model : 0.0639420509338379
			 train-loss:  2.196085493739058 	 ± 0.24959093620205033
	data : 0.11488714218139648
	model : 0.06390771865844727
			 train-loss:  2.196294695742217 	 ± 0.24910678973382866
	data : 0.11486968994140626
	model : 0.06387929916381836
			 train-loss:  2.1972734533971354 	 ± 0.24907948913891076
	data : 0.11465110778808593
	model : 0.06387462615966796
			 train-loss:  2.197484363035026 	 ± 0.24860101500021153
	data : 0.114654541015625
	model : 0.06385483741760253
			 train-loss:  2.1963473052978517 	 ± 0.24875125522219613
	data : 0.11477689743041992
	model : 0.06392931938171387
			 train-loss:  2.1956858834422444 	 ± 0.24847541937808965
	data : 0.1147878646850586
	model : 0.06392092704772949
			 train-loss:  2.195041476734101 	 ± 0.24819199052636265
	data : 0.11484622955322266
	model : 0.0639122486114502
			 train-loss:  2.1944928574467837 	 ± 0.24785406302070231
	data : 0.11497502326965332
	model : 0.06392016410827636
			 train-loss:  2.195660375234649 	 ± 0.24806177347313427
	data : 0.1150179386138916
	model : 0.06390471458435058
			 train-loss:  2.196032248291315 	 ± 0.247645829123362
	data : 0.1150827407836914
	model : 0.06384162902832032
			 train-loss:  2.196232228539884 	 ± 0.24718230192256108
	data : 0.1147986888885498
	model : 0.05544857978820801
#epoch  67    val-loss:  2.4013722256610266  train-loss:  2.196232228539884  lr:  1.9073486328125e-08
			 train-loss:  2.004631519317627 	 ± 0.0
	data : 4.903002738952637
	model : 0.0870966911315918
			 train-loss:  1.943678617477417 	 ± 0.06095290184020996
	data : 2.7838587760925293
	model : 0.07718861103057861
			 train-loss:  2.003020922342936 	 ± 0.09756974832736816
	data : 1.892891804377238
	model : 0.07304843266805013
			 train-loss:  1.9822696447372437 	 ± 0.091824497865793
	data : 1.44829523563385
	model : 0.07093602418899536
			 train-loss:  2.0562421798706056 	 ± 0.16921328122481938
	data : 1.1814797878265382
	model : 0.06961898803710938
			 train-loss:  2.067140817642212 	 ± 0.15638045587212918
	data : 0.2235854148864746
	model : 0.0651200294494629
			 train-loss:  2.09037607056754 	 ± 0.15556528944916426
	data : 0.11347718238830566
	model : 0.06461305618286133
			 train-loss:  2.067174017429352 	 ± 0.15793617780615707
	data : 0.11413226127624512
	model : 0.06463160514831542
			 train-loss:  2.118843528959486 	 ± 0.20863893757373136
	data : 0.11407794952392578
	model : 0.06467599868774414
			 train-loss:  2.0886688351631166 	 ± 0.21765062590318437
	data : 0.11409173011779786
	model : 0.06479997634887695
			 train-loss:  2.113652912053195 	 ± 0.2220524918174873
	data : 0.11416068077087402
	model : 0.06486711502075196
			 train-loss:  2.1038972238699594 	 ± 0.21504714521868462
	data : 0.11399264335632324
	model : 0.06491775512695312
			 train-loss:  2.1000300279030433 	 ± 0.20704445972290247
	data : 0.11395764350891113
	model : 0.06496076583862305
			 train-loss:  2.118067647729601 	 ± 0.20984535983240551
	data : 0.11385273933410645
	model : 0.06497502326965332
			 train-loss:  2.096580187479655 	 ± 0.21809024885479844
	data : 0.11429166793823242
	model : 0.06501631736755371
			 train-loss:  2.1005661487579346 	 ± 0.21172851759949657
	data : 0.11430196762084961
	model : 0.06503586769104004
			 train-loss:  2.1076728456160603 	 ± 0.2073645327864379
	data : 0.11446352005004883
	model : 0.0650289535522461
			 train-loss:  2.119090106752184 	 ± 0.2069472717932134
	data : 0.11444692611694336
	model : 0.06500115394592285
			 train-loss:  2.1075055599212646 	 ± 0.20733726776651246
	data : 0.1145108699798584
	model : 0.06499953269958496
			 train-loss:  2.1276705384254457 	 ± 0.22037514895751256
	data : 0.11396069526672363
	model : 0.06503381729125976
			 train-loss:  2.135524727049328 	 ± 0.21791361358679087
	data : 0.11380167007446289
	model : 0.06502256393432618
			 train-loss:  2.124585883183913 	 ± 0.2187251637513524
	data : 0.11388716697692872
	model : 0.0650254249572754
			 train-loss:  2.120020384373872 	 ± 0.21498658118344707
	data : 0.11392168998718262
	model : 0.06510763168334961
			 train-loss:  2.1113807559013367 	 ± 0.21449993468190154
	data : 0.11398091316223144
	model : 0.06516475677490234
			 train-loss:  2.1117576789855956 	 ± 0.21017426770105127
	data : 0.11406459808349609
	model : 0.0654557228088379
			 train-loss:  2.109069090623122 	 ± 0.20653078555123522
	data : 0.11430726051330567
	model : 0.06545567512512207
			 train-loss:  2.1466008292304144 	 ± 0.27874642297766244
	data : 0.11421771049499511
	model : 0.06545047760009766
			 train-loss:  2.1417191284043446 	 ± 0.2748963819690521
	data : 0.11412277221679687
	model : 0.06533594131469726
			 train-loss:  2.1414205123638284 	 ± 0.2701198322993282
	data : 0.11395745277404785
	model : 0.06524887084960937
			 train-loss:  2.1427329858144124 	 ± 0.2656737121951777
	data : 0.11397500038146972
	model : 0.06525816917419433
			 train-loss:  2.1538949320393224 	 ± 0.2684088991258108
	data : 0.113792085647583
	model : 0.06517863273620605
			 train-loss:  2.1647317931056023 	 ± 0.27098440386023104
	data : 0.11378130912780762
	model : 0.06516227722167969
			 train-loss:  2.159832087430087 	 ± 0.268282586346977
	data : 0.11388792991638183
	model : 0.06519861221313476
			 train-loss:  2.154433786869049 	 ± 0.26612082354585603
	data : 0.11405472755432129
	model : 0.06526684761047363
			 train-loss:  2.1769608054842267 	 ± 0.29334402855886405
	data : 0.11414813995361328
	model : 0.06485595703125
			 train-loss:  2.1716962887181177 	 ± 0.2909131332991593
	data : 0.11425104141235351
	model : 0.06491732597351074
			 train-loss:  2.16982224503079 	 ± 0.28717516443236657
	data : 0.11406769752502441
	model : 0.06486349105834961
			 train-loss:  2.1708342558459233 	 ± 0.28343820707385914
	data : 0.11406598091125489
	model : 0.06483292579650879
			 train-loss:  2.16411000643021 	 ± 0.28283471981020886
	data : 0.11396551132202148
	model : 0.06482057571411133
			 train-loss:  2.165710413455963 	 ± 0.27945568980634744
	data : 0.11383638381958008
	model : 0.06484880447387695
			 train-loss:  2.1744191123218073 	 ± 0.28146824356423783
	data : 0.11387429237365723
	model : 0.06484966278076172
			 train-loss:  2.192006911550249 	 ± 0.30003439054580183
	data : 0.11411256790161133
	model : 0.06490416526794433
			 train-loss:  2.1848995796469755 	 ± 0.3000812012834015
	data : 0.11403732299804688
	model : 0.06494612693786621
			 train-loss:  2.1815920038656755 	 ± 0.2974434195822119
	data : 0.11401519775390626
	model : 0.06532402038574218
			 train-loss:  2.1833555804358586 	 ± 0.29435247450455077
	data : 0.11399588584899903
	model : 0.06530041694641113
			 train-loss:  2.181793881499249 	 ± 0.29132383730958195
	data : 0.11382517814636231
	model : 0.06526761054992676
			 train-loss:  2.181740573112001 	 ± 0.28820821154295995
	data : 0.1137359619140625
	model : 0.06530175209045411
			 train-loss:  2.1834598730007806 	 ± 0.2854337145260939
	data : 0.11384549140930175
	model : 0.06526532173156738
			 train-loss:  2.1791805710111345 	 ± 0.28405756399412624
	data : 0.11392998695373535
	model : 0.0648848533630371
			 train-loss:  2.1799415040016172 	 ± 0.2812530846067574
	data : 0.11403126716613769
	model : 0.06490707397460938
			 train-loss:  2.1772904138939055 	 ± 0.279112282939644
	data : 0.11419577598571777
	model : 0.06494131088256835
			 train-loss:  2.1752301110671115 	 ± 0.2768068031858419
	data : 0.11429290771484375
	model : 0.06490979194641114
			 train-loss:  2.176594781425764 	 ± 0.27435952501752886
	data : 0.11410598754882813
	model : 0.06481599807739258
			 train-loss:  2.1780180511651217 	 ± 0.2720047126292801
	data : 0.11414146423339844
	model : 0.06474251747131347
			 train-loss:  2.1767851591110228 	 ± 0.26967282895335093
	data : 0.11402435302734375
	model : 0.06471853256225586
			 train-loss:  2.1793399091277803 	 ± 0.2679249391793975
	data : 0.11411280632019043
	model : 0.06471328735351563
			 train-loss:  2.176513335161042 	 ± 0.2664053710736605
	data : 0.11417326927185059
	model : 0.06473708152770996
			 train-loss:  2.1738006184841026 	 ± 0.2648917194795553
	data : 0.11444916725158691
	model : 0.06486740112304687
			 train-loss:  2.170387597407325 	 ± 0.26392037937365814
	data : 0.11440014839172363
	model : 0.06493177413940429
			 train-loss:  2.175661987066269 	 ± 0.26482899991273284
	data : 0.11442680358886718
	model : 0.06493196487426758
			 train-loss:  2.173564365652741 	 ± 0.26315139346268523
	data : 0.11420493125915528
	model : 0.06492018699645996
			 train-loss:  2.1748444399526043 	 ± 0.2612119753759739
	data : 0.11411128044128419
	model : 0.06489439010620117
			 train-loss:  2.174857879441882 	 ± 0.2591305934916461
	data : 0.11386251449584961
	model : 0.06490139961242676
			 train-loss:  2.1813248079270124 	 ± 0.26217208076906967
	data : 0.11383404731750488
	model : 0.06487569808959961
			 train-loss:  2.1867097799594584 	 ± 0.26369038250871457
	data : 0.11393365859985352
	model : 0.06489024162292481
			 train-loss:  2.1876791950428123 	 ± 0.2618017915891756
	data : 0.11407904624938965
	model : 0.064888334274292
			 train-loss:  2.188683362149481 	 ± 0.25996873187160563
	data : 0.11407132148742676
	model : 0.06485872268676758
			 train-loss:  2.191288469468846 	 ± 0.2589296509410496
	data : 0.11413297653198243
	model : 0.06475744247436524
			 train-loss:  2.185443173284116 	 ± 0.26152684935925996
	data : 0.11416611671447754
	model : 0.06475949287414551
			 train-loss:  2.1864567177636283 	 ± 0.2597885386869924
	data : 0.11409182548522949
	model : 0.06474013328552246
			 train-loss:  2.179688344539051 	 ± 0.26409521310601314
	data : 0.1141082763671875
	model : 0.06475825309753418
			 train-loss:  2.1769555475976734 	 ± 0.26326379156252266
	data : 0.11415367126464844
	model : 0.0647951602935791
			 train-loss:  2.1814002696781944 	 ± 0.2641605602978522
	data : 0.11416397094726563
	model : 0.06484308242797851
			 train-loss:  2.177501623694961 	 ± 0.26447566422627633
	data : 0.11418795585632324
	model : 0.06485671997070312
			 train-loss:  2.174249448776245 	 ± 0.26419200737525506
	data : 0.11421828269958496
	model : 0.06488938331604004
			 train-loss:  2.1728393153140417 	 ± 0.26273211723524476
	data : 0.11406178474426269
	model : 0.06484565734863282
			 train-loss:  2.1758694122363993 	 ± 0.262353747420006
	data : 0.11400303840637208
	model : 0.06481709480285644
			 train-loss:  2.1761984550035915 	 ± 0.26068255823108333
	data : 0.11406383514404297
	model : 0.06486496925354004
			 train-loss:  2.173972621748719 	 ± 0.2597722835574437
	data : 0.11412463188171387
	model : 0.06488103866577148
			 train-loss:  2.1748638689517974 	 ± 0.2582651161460321
	data : 0.11409411430358887
	model : 0.0648989200592041
			 train-loss:  2.1732531211994313 	 ± 0.25706995808555516
	data : 0.11422276496887207
	model : 0.06497182846069335
			 train-loss:  2.175937120507403 	 ± 0.25663702226688434
	data : 0.11416454315185547
	model : 0.06500535011291504
			 train-loss:  2.1771114820457367 	 ± 0.25530790159367395
	data : 0.11419458389282226
	model : 0.06489648818969726
			 train-loss:  2.1736155393577756 	 ± 0.2557743947509137
	data : 0.11392874717712402
	model : 0.06484899520874024
			 train-loss:  2.1748023187412935 	 ± 0.2544979312139469
	data : 0.1139366626739502
	model : 0.0648115634918213
			 train-loss:  2.174306424551232 	 ± 0.25305526912841203
	data : 0.11395244598388672
	model : 0.06475863456726075
			 train-loss:  2.177110336292749 	 ± 0.2529368247738704
	data : 0.11397151947021485
	model : 0.06473746299743652
			 train-loss:  2.17667455971241 	 ± 0.2515284217453724
	data : 0.11399388313293457
	model : 0.06482186317443847
			 train-loss:  2.1725337049934303 	 ± 0.25310984673536807
	data : 0.11422386169433593
	model : 0.06489071846008301
			 train-loss:  2.1681282295121087 	 ± 0.2551080099717263
	data : 0.11432948112487792
	model : 0.06491742134094239
			 train-loss:  2.1677398065944296 	 ± 0.2537292047952536
	data : 0.11437821388244629
	model : 0.0649235725402832
			 train-loss:  2.1682419971279474 	 ± 0.252391942690505
	data : 0.11438145637512206
	model : 0.06491832733154297
			 train-loss:  2.164334043379753 	 ± 0.2538144237959974
	data : 0.11420192718505859
	model : 0.0648871898651123
			 train-loss:  2.1674187005834376 	 ± 0.25420725613592793
	data : 0.11421246528625488
	model : 0.0648162841796875
			 train-loss:  2.164029298330608 	 ± 0.2549921195507185
	data : 0.1140899658203125
	model : 0.06481614112854003
			 train-loss:  2.1623589657247066 	 ± 0.2541824742061692
	data : 0.11406660079956055
	model : 0.06483378410339355
			 train-loss:  2.1637363470706745 	 ± 0.25322873065642476
	data : 0.11417441368103028
	model : 0.06483025550842285
			 train-loss:  2.1651493079808293 	 ± 0.25231748238178897
	data : 0.11424517631530762
	model : 0.06485548019409179
			 train-loss:  2.1643802722295127 	 ± 0.25115532812317753
	data : 0.11426396369934082
	model : 0.0648871898651123
			 train-loss:  2.1603333234786986 	 ± 0.253119755217158
	data : 0.1141435146331787
	model : 0.06482229232788086
			 train-loss:  2.159435973309054 	 ± 0.25202337517553486
	data : 0.11420059204101562
	model : 0.06485886573791504
			 train-loss:  2.154939141927981 	 ± 0.2548243485870227
	data : 0.11403751373291016
	model : 0.06486949920654297
			 train-loss:  2.1565402804069147 	 ± 0.25409938826307327
	data : 0.1140357494354248
	model : 0.0648725986480713
			 train-loss:  2.1611722111701965 	 ± 0.25720713442821336
	data : 0.11389927864074707
	model : 0.06484003067016601
			 train-loss:  2.161710870833624 	 ± 0.25603834393466846
	data : 0.11405553817749023
	model : 0.06489458084106445
			 train-loss:  2.1626978460347877 	 ± 0.25502836469021334
	data : 0.1139552116394043
	model : 0.06483225822448731
			 train-loss:  2.1643225923876894 	 ± 0.25438443385529474
	data : 0.1140864372253418
	model : 0.06483049392700195
			 train-loss:  2.16372510459688 	 ± 0.25327940755966927
	data : 0.11398024559020996
	model : 0.06478486061096192
			 train-loss:  2.164867814527739 	 ± 0.2523944271683754
	data : 0.1139291763305664
	model : 0.06479201316833497
			 train-loss:  2.1668043678457085 	 ± 0.252056749941333
	data : 0.11393280029296875
	model : 0.06479172706604004
			 train-loss:  2.166838454770612 	 ± 0.250919045091212
	data : 0.11397390365600586
	model : 0.06482906341552734
			 train-loss:  2.1688982461180006 	 ± 0.2507372434196511
	data : 0.1139129638671875
	model : 0.06486096382141113
			 train-loss:  2.1685598213060766 	 ± 0.249651013326842
	data : 0.11402649879455566
	model : 0.06493425369262695
			 train-loss:  2.1699877036245248 	 ± 0.24901667058747698
	data : 0.11407608985900879
	model : 0.06496176719665528
			 train-loss:  2.16861425275388 	 ± 0.2483649266266341
	data : 0.11396384239196777
	model : 0.06500372886657715
			 train-loss:  2.1683835654423156 	 ± 0.24730444450284064
	data : 0.1139411449432373
	model : 0.06496968269348144
			 train-loss:  2.167492161449204 	 ± 0.24643240721754375
	data : 0.11386327743530274
	model : 0.06494483947753907
			 train-loss:  2.1660342095261913 	 ± 0.24589220677439802
	data : 0.11393747329711915
	model : 0.06493172645568848
			 train-loss:  2.1657812775684 	 ± 0.24487228121126062
	data : 0.11407947540283203
	model : 0.0649444580078125
			 train-loss:  2.1663026909033456 	 ± 0.2439161739343331
	data : 0.11412386894226074
	model : 0.0648961067199707
			 train-loss:  2.1668213261060476 	 ± 0.2429725967326972
	data : 0.11406178474426269
	model : 0.06491575241088868
			 train-loss:  2.167301170161513 	 ± 0.24203232018144616
	data : 0.11421136856079102
	model : 0.06493315696716309
			 train-loss:  2.166783881381275 	 ± 0.24111414815651935
	data : 0.1139531135559082
	model : 0.06499462127685547
			 train-loss:  2.1674855505266497 	 ± 0.24026600100609488
	data : 0.11387033462524414
	model : 0.06497359275817871
			 train-loss:  2.168811191558838 	 ± 0.23975787249572542
	data : 0.11384968757629395
	model : 0.06499271392822266
			 train-loss:  2.1689142915937634 	 ± 0.2388073390704163
	data : 0.11395421028137206
	model : 0.06505408287048339
			 train-loss:  2.1743685880045254 	 ± 0.24561823303194508
	data : 0.11390476226806641
	model : 0.06507225036621093
			 train-loss:  2.1759082432836294 	 ± 0.24527140028944197
	data : 0.11404805183410645
	model : 0.06500139236450195
			 train-loss:  2.1757047305735506 	 ± 0.24432973556501847
	data : 0.11408181190490722
	model : 0.06501893997192383
			 train-loss:  2.1739887631856476 	 ± 0.24416727467746044
	data : 0.11416807174682617
	model : 0.06498222351074219
			 train-loss:  2.1740985526383376 	 ± 0.24323677431958207
	data : 0.11409382820129395
	model : 0.06491198539733886
			 train-loss:  2.177393121249748 	 ± 0.24523012289178453
	data : 0.11407179832458496
	model : 0.06484260559082031
			 train-loss:  2.1743953048734737 	 ± 0.2467223548122659
	data : 0.1140507698059082
	model : 0.0648167610168457
			 train-loss:  2.175490760091525 	 ± 0.24612447104181565
	data : 0.11396427154541015
	model : 0.06482582092285157
			 train-loss:  2.174011701124686 	 ± 0.24580820837322923
	data : 0.11395401954650879
	model : 0.06486425399780274
			 train-loss:  2.172105355297818 	 ± 0.2459024382248649
	data : 0.11398606300354004
	model : 0.06488356590270997
			 train-loss:  2.1704631643573733 	 ± 0.24575068745193346
	data : 0.11405258178710938
	model : 0.06498560905456544
			 train-loss:  2.171776288661404 	 ± 0.24534057047698407
	data : 0.1141733169555664
	model : 0.06496601104736328
			 train-loss:  2.1720412540778837 	 ± 0.24447627308941738
	data : 0.11418447494506836
	model : 0.06493511199951171
			 train-loss:  2.173061136688505 	 ± 0.24389815805710416
	data : 0.11400947570800782
	model : 0.06484684944152833
			 train-loss:  2.1746197651464043 	 ± 0.24373044115035788
	data : 0.11385478973388671
	model : 0.06488852500915528
			 train-loss:  2.1749581258061905 	 ± 0.24290395014543711
	data : 0.11387205123901367
	model : 0.06483492851257325
			 train-loss:  2.1731050831454617 	 ± 0.24305826508377532
	data : 0.11381664276123046
	model : 0.06491265296936036
			 train-loss:  2.1738153133127422 	 ± 0.24236170098149923
	data : 0.11378583908081055
	model : 0.06493697166442872
			 train-loss:  2.172009193486181 	 ± 0.24249501876017998
	data : 0.11395177841186524
	model : 0.06500945091247559
			 train-loss:  2.173767091476754 	 ± 0.2425884331772965
	data : 0.11404886245727539
	model : 0.06493659019470215
			 train-loss:  2.1747282589373946 	 ± 0.2420406879709314
	data : 0.11385126113891601
	model : 0.0648573398590088
			 train-loss:  2.1751302674009994 	 ± 0.24127083463858318
	data : 0.11379785537719726
	model : 0.06482186317443847
			 train-loss:  2.177185994666695 	 ± 0.2417568692352833
	data : 0.11374597549438477
	model : 0.06483960151672363
			 train-loss:  2.1784089326858522 	 ± 0.2414116449056174
	data : 0.11374473571777344
	model : 0.06482000350952148
			 train-loss:  2.18007942382863 	 ± 0.2414792038584311
	data : 0.11381998062133789
	model : 0.06485724449157715
			 train-loss:  2.1826870363009605 	 ± 0.24280716428116017
	data : 0.11390109062194824
	model : 0.06494088172912597
			 train-loss:  2.1823792114756464 	 ± 0.24204213078396458
	data : 0.11395716667175293
	model : 0.06497735977172851
			 train-loss:  2.179903782033301 	 ± 0.2431902975633929
	data : 0.11389174461364746
	model : 0.0649001121520996
			 train-loss:  2.182251061931733 	 ± 0.24414843837304973
	data : 0.11374759674072266
	model : 0.0648768424987793
			 train-loss:  2.1840136486750383 	 ± 0.2443519895800408
	data : 0.11360597610473633
	model : 0.06488313674926757
			 train-loss:  2.1847434142592608 	 ± 0.24374303839753889
	data : 0.1136129379272461
	model : 0.06488938331604004
			 train-loss:  2.1880537414852577 	 ± 0.24648549590637361
	data : 0.11359977722167969
	model : 0.06484031677246094
			 train-loss:  2.1897769461637773 	 ± 0.2466620416207168
	data : 0.11374378204345703
	model : 0.06491985321044921
			 train-loss:  2.1914834551513196 	 ± 0.24682976877628138
	data : 0.11391286849975586
	model : 0.06497082710266114
			 train-loss:  2.1913379371536443 	 ± 0.24606890719544694
	data : 0.11393041610717773
	model : 0.06491398811340332
			 train-loss:  2.1910071733557146 	 ± 0.24534415906005325
	data : 0.11401247978210449
	model : 0.06490769386291503
			 train-loss:  2.191339484753053 	 ± 0.2446269799838019
	data : 0.11400647163391113
	model : 0.06495637893676758
			 train-loss:  2.1909295880213016 	 ± 0.24393616665005619
	data : 0.11409587860107422
	model : 0.06495695114135742
			 train-loss:  2.1901082089453032 	 ± 0.24342321736629186
	data : 0.11405348777770996
	model : 0.06498003005981445
			 train-loss:  2.191723146352423 	 ± 0.24357386870350464
	data : 0.11413564682006835
	model : 0.0650242805480957
			 train-loss:  2.1930067018120587 	 ± 0.24340595325998324
	data : 0.11403417587280273
	model : 0.06500387191772461
			 train-loss:  2.1929380120266053 	 ± 0.242682072983733
	data : 0.11407642364501953
	model : 0.06495480537414551
			 train-loss:  2.19207410628979 	 ± 0.24222197221260452
	data : 0.11387619972229004
	model : 0.0648423194885254
			 train-loss:  2.1920842430170846 	 ± 0.24150853983334525
	data : 0.1137474536895752
	model : 0.064796781539917
			 train-loss:  2.1927044189464278 	 ± 0.24093706645598967
	data : 0.11375517845153808
	model : 0.06482534408569336
			 train-loss:  2.191681552071904 	 ± 0.24060772220164903
	data : 0.11379094123840332
	model : 0.0648381233215332
			 train-loss:  2.191531957918509 	 ± 0.23991933809585403
	data : 0.11384968757629395
	model : 0.06486654281616211
			 train-loss:  2.19437669405992 	 ± 0.24213732341455405
	data : 0.1140364646911621
	model : 0.0649341106414795
			 train-loss:  2.1944781378337317 	 ± 0.24144821942340716
	data : 0.11407914161682128
	model : 0.064945650100708
			 train-loss:  2.1941236901012333 	 ± 0.24080696447270908
	data : 0.11396217346191406
	model : 0.06482877731323242
			 train-loss:  2.1937617723551175 	 ± 0.24017375297175764
	data : 0.11405158042907715
	model : 0.06486635208129883
			 train-loss:  2.195461592647467 	 ± 0.24056348278910933
	data : 0.11398520469665527
	model : 0.06484112739562989
			 train-loss:  2.1952600072882027 	 ± 0.23990565245106676
	data : 0.11391196250915528
	model : 0.06488018035888672
			 train-loss:  2.193093729019165 	 ± 0.24098750577041836
	data : 0.11401205062866211
	model : 0.0649404525756836
			 train-loss:  2.1923778807919327 	 ± 0.2405127037579231
	data : 0.11406359672546387
	model : 0.06509132385253906
			 train-loss:  2.1927287002186198 	 ± 0.23989747803032305
	data : 0.11397008895874024
	model : 0.06505537033081055
			 train-loss:  2.192359775793357 	 ± 0.23929288721510378
	data : 0.11400933265686035
	model : 0.06505289077758789
			 train-loss:  2.1902691456286805 	 ± 0.2403117323996646
	data : 0.11390852928161621
	model : 0.0650017261505127
			 train-loss:  2.1905640788980434 	 ± 0.23969475044238528
	data : 0.11394948959350586
	model : 0.06492376327514648
			 train-loss:  2.1918677200553236 	 ± 0.2397062516070659
	data : 0.11393280029296875
	model : 0.06481761932373047
			 train-loss:  2.1909385052594272 	 ± 0.23940012339123345
	data : 0.1139289379119873
	model : 0.06488051414489746
			 train-loss:  2.1907001093347023 	 ± 0.2387848267502053
	data : 0.11391305923461914
	model : 0.06490721702575683
			 train-loss:  2.189468742678405 	 ± 0.23875001052366063
	data : 0.11413817405700684
	model : 0.06494293212890626
			 train-loss:  2.188984702135387 	 ± 0.2382138557612741
	data : 0.11417083740234375
	model : 0.06499347686767579
			 train-loss:  2.187424964305618 	 ± 0.23856020420602259
	data : 0.11414504051208496
	model : 0.06504979133605956
			 train-loss:  2.1899067163467407 	 ± 0.24039747611684234
	data : 0.11413555145263672
	model : 0.0649693489074707
			 train-loss:  2.1880901971629245 	 ± 0.24109139764981505
	data : 0.11396450996398926
	model : 0.0649289608001709
			 train-loss:  2.1861220057477655 	 ± 0.2420187747803857
	data : 0.11393566131591797
	model : 0.0649810791015625
			 train-loss:  2.1879674263489552 	 ± 0.2427620120924662
	data : 0.11384096145629882
	model : 0.06493892669677734
			 train-loss:  2.1880223337484868 	 ± 0.24214314324653835
	data : 0.11393594741821289
	model : 0.06493177413940429
			 train-loss:  2.187985891012976 	 ± 0.24152832370118368
	data : 0.11394214630126953
	model : 0.06495318412780762
			 train-loss:  2.189214446327903 	 ± 0.24153394548776544
	data : 0.11407766342163086
	model : 0.06497178077697754
			 train-loss:  2.188521714665782 	 ± 0.24112341958104302
	data : 0.11411566734313965
	model : 0.06489520072937012
			 train-loss:  2.1874226182699203 	 ± 0.24101907708248993
	data : 0.11408405303955078
	model : 0.06485471725463868
			 train-loss:  2.1897154014502 	 ± 0.24259546680059899
	data : 0.11405401229858399
	model : 0.06481986045837403
			 train-loss:  2.1896188725339303 	 ± 0.24199810761016521
	data : 0.11411013603210449
	model : 0.06483235359191894
			 train-loss:  2.1887085443647036 	 ± 0.24174778769999064
	data : 0.11407294273376464
	model : 0.06484694480895996
			 train-loss:  2.188999119342542 	 ± 0.24119007559536088
	data : 0.11399054527282715
	model : 0.06485705375671387
			 train-loss:  2.187513877124321 	 ± 0.24153446171495774
	data : 0.11401486396789551
	model : 0.06490125656127929
			 train-loss:  2.186736186731209 	 ± 0.24120464786557688
	data : 0.1140784740447998
	model : 0.06492209434509277
			 train-loss:  2.1857344793236773 	 ± 0.24105046139338726
	data : 0.11408782005310059
	model : 0.06486358642578124
			 train-loss:  2.1851597737807493 	 ± 0.2406124305011414
	data : 0.11398959159851074
	model : 0.06481165885925293
			 train-loss:  2.186843074679945 	 ± 0.24126065508237768
	data : 0.11405868530273437
	model : 0.06477761268615723
			 train-loss:  2.188686880611238 	 ± 0.24215707620584365
	data : 0.11402683258056641
	model : 0.0647432804107666
			 train-loss:  2.1885706937708562 	 ± 0.24158842997947724
	data : 0.11395316123962403
	model : 0.06473689079284668
			 train-loss:  2.186860851521762 	 ± 0.24229431340641389
	data : 0.11396279335021972
	model : 0.06481881141662597
			 train-loss:  2.187333533461665 	 ± 0.24182283506521
	data : 0.1140857219696045
	model : 0.06488065719604492
			 train-loss:  2.1892038126972233 	 ± 0.242796381177582
	data : 0.11400294303894043
	model : 0.06491961479187011
			 train-loss:  2.188939023572345 	 ± 0.24226204928499778
	data : 0.11395463943481446
	model : 0.06497707366943359
			 train-loss:  2.189010231583207 	 ± 0.24170286213983183
	data : 0.11392054557800294
	model : 0.0649200439453125
			 train-loss:  2.190055059397825 	 ± 0.24163372155203097
	data : 0.11391549110412598
	model : 0.06484441757202149
			 train-loss:  2.191688499319444 	 ± 0.2422767201287088
	data : 0.11384596824645996
	model : 0.06478004455566407
			 train-loss:  2.191243264228786 	 ± 0.24181231720442858
	data : 0.11381850242614747
	model : 0.06476006507873536
			 train-loss:  2.1915633450854908 	 ± 0.24130861242011256
	data : 0.11393332481384277
	model : 0.06468877792358399
			 train-loss:  2.191035052769864 	 ± 0.24088952523861368
	data : 0.1139753818511963
	model : 0.06477022171020508
			 train-loss:  2.1912842870832563 	 ± 0.24037492604137498
	data : 0.11391963958740234
	model : 0.0648993968963623
			 train-loss:  2.189964047461882 	 ± 0.24064071706438614
	data : 0.11392641067504883
	model : 0.06486625671386718
			 train-loss:  2.1944272677813257 	 ± 0.2491820055653037
	data : 0.11382102966308594
	model : 0.06468925476074219
			 train-loss:  2.1954840003119576 	 ± 0.24913017891653727
	data : 0.11379203796386719
	model : 0.06453142166137696
			 train-loss:  2.196320197223562 	 ± 0.24889464457171367
	data : 0.11405420303344727
	model : 0.06435723304748535
			 train-loss:  2.199223506818259 	 ± 0.2521520250034997
	data : 0.11420011520385742
	model : 0.06403818130493164
			 train-loss:  2.1992805474682857 	 ± 0.25159992014357835
	data : 0.11438465118408203
	model : 0.06392436027526856
			 train-loss:  2.1991823739880556 	 ± 0.25105435082878574
	data : 0.11459579467773437
	model : 0.06389503479003907
			 train-loss:  2.1979241459266 	 ± 0.25123055070599004
	data : 0.11466383934020996
	model : 0.06388449668884277
			 train-loss:  2.197413574049483 	 ± 0.2508057295800489
	data : 0.11451954841613769
	model : 0.06384902000427246
			 train-loss:  2.1978785894040405 	 ± 0.2503643932904398
	data : 0.11453442573547364
	model : 0.06385593414306641
			 train-loss:  2.1982938777735304 	 ± 0.24990661916470167
	data : 0.11461338996887208
	model : 0.06389117240905762
			 train-loss:  2.1980834695009084 	 ± 0.24939274061621852
	data : 0.11472296714782715
	model : 0.0639157772064209
			 train-loss:  2.197213582282371 	 ± 0.24921705555633614
	data : 0.11477618217468262
	model : 0.06392221450805664
			 train-loss:  2.196970787593874 	 ± 0.24871634351427205
	data : 0.11475110054016113
	model : 0.06390419006347656
			 train-loss:  2.1963501152609974 	 ± 0.24837415906757698
	data : 0.11465082168579102
	model : 0.06384015083312988
			 train-loss:  2.194940852517841 	 ± 0.2487995367092806
	data : 0.11470279693603516
	model : 0.06380524635314941
			 train-loss:  2.1965088285661643 	 ± 0.24945409041684194
	data : 0.11464509963989258
	model : 0.06378636360168458
			 train-loss:  2.1973638027906417 	 ± 0.2492845102887076
	data : 0.11464242935180664
	model : 0.06379833221435546
			 train-loss:  2.1972036332015676 	 ± 0.24877915968978384
	data : 0.1147651195526123
	model : 0.06382203102111816
			 train-loss:  2.1962522494891457 	 ± 0.2487035557344174
	data : 0.1148569107055664
	model : 0.06392216682434082
			 train-loss:  2.1969932930950273 	 ± 0.24845887163543515
	data : 0.1148033618927002
	model : 0.0638850212097168
			 train-loss:  2.1973258354624763 	 ± 0.24800339450283915
	data : 0.11469993591308594
	model : 0.06385374069213867
			 train-loss:  2.197738928697547 	 ± 0.24758085084905848
	data : 0.11477346420288086
	model : 0.06382365226745605
			 train-loss:  2.1976495738921127 	 ± 0.24708108384789979
	data : 0.1147301197052002
	model : 0.06381330490112305
			 train-loss:  2.196797059615131 	 ± 0.24694268125798693
	data : 0.11471567153930665
	model : 0.06384396553039551
			 train-loss:  2.1985273490990362 	 ± 0.24794009695095232
	data : 0.11470637321472169
	model : 0.06385126113891601
			 train-loss:  2.199674450728788 	 ± 0.2481002535739699
	data : 0.1148383617401123
	model : 0.0638664722442627
			 train-loss:  2.199364166736603 	 ± 0.24765196070891518
	data : 0.11481504440307617
	model : 0.06390695571899414
			 train-loss:  2.198819031753388 	 ± 0.2473083866353916
	data : 0.11475343704223633
	model : 0.06386051177978516
			 train-loss:  2.1986219338954442 	 ± 0.24683695979273895
	data : 0.11469969749450684
	model : 0.06377754211425782
			 train-loss:  2.197908474522617 	 ± 0.24660887004965718
	data : 0.11472978591918945
	model : 0.06380348205566407
			 train-loss:  2.1992757878904268 	 ± 0.2470819642791257
	data : 0.1147392749786377
	model : 0.06383004188537597
			 train-loss:  2.198288435561984 	 ± 0.2470985691848376
	data : 0.11458826065063477
	model : 0.06381068229675294
			 train-loss:  2.1972539527341723 	 ± 0.24716813244253805
	data : 0.1144442081451416
	model : 0.055434465408325195
#epoch  68    val-loss:  2.386947600465072  train-loss:  2.1972539527341723  lr:  1.9073486328125e-08
			 train-loss:  2.1450207233428955 	 ± 0.0
	data : 5.59185004234314
	model : 0.07283353805541992
			 train-loss:  2.1933889389038086 	 ± 0.048368215560913086
	data : 2.8606961965560913
	model : 0.06876099109649658
			 train-loss:  2.1951942443847656 	 ± 0.039574921858719865
	data : 1.945395787556966
	model : 0.06723165512084961
			 train-loss:  2.222427725791931 	 ± 0.058306246309946325
	data : 1.4873711466789246
	model : 0.06651097536087036
			 train-loss:  2.265998888015747 	 ± 0.10155530216163623
	data : 1.2126707553863525
	model : 0.06609249114990234
			 train-loss:  2.243050535519918 	 ± 0.10596084454008721
	data : 0.11713972091674804
	model : 0.06450004577636718
			 train-loss:  2.205285429954529 	 ± 0.1348367996533058
	data : 0.11408367156982421
	model : 0.0645106315612793
			 train-loss:  2.200388863682747 	 ± 0.126791863902737
	data : 0.11391806602478027
	model : 0.06462397575378417
			 train-loss:  2.1839066478941174 	 ± 0.12830916720573063
	data : 0.1141848087310791
	model : 0.06469039916992188
			 train-loss:  2.1650948882102967 	 ± 0.13417100607710222
	data : 0.11421656608581543
	model : 0.06473212242126465
			 train-loss:  2.16930767622861 	 ± 0.12861882473382522
	data : 0.11426143646240235
	model : 0.06469669342041015
			 train-loss:  2.1815419097741446 	 ± 0.12965599257714527
	data : 0.11408696174621583
	model : 0.06470494270324707
			 train-loss:  2.1858585522725034 	 ± 0.1254637329508548
	data : 0.11406631469726562
	model : 0.06475710868835449
			 train-loss:  2.1748826929501126 	 ± 0.12721196442746874
	data : 0.11398539543151856
	model : 0.06480512619018555
			 train-loss:  2.199257318178813 	 ± 0.15304162158647158
	data : 0.11405811309814454
	model : 0.06484599113464355
			 train-loss:  2.2039466872811317 	 ± 0.1492907634630153
	data : 0.11394586563110351
	model : 0.06481542587280273
			 train-loss:  2.190445949049557 	 ± 0.1545736326740414
	data : 0.11383624076843261
	model : 0.06477537155151367
			 train-loss:  2.2084080709351435 	 ± 0.16748272022517494
	data : 0.11398229598999024
	model : 0.06471409797668456
			 train-loss:  2.208972134088215 	 ± 0.16303327421576427
	data : 0.11379599571228027
	model : 0.06470041275024414
			 train-loss:  2.2106471598148345 	 ± 0.15907282794037728
	data : 0.11376194953918457
	model : 0.0647430419921875
			 train-loss:  2.2046679258346558 	 ± 0.15752532859899707
	data : 0.11384949684143067
	model : 0.06482477188110351
			 train-loss:  2.2129801349206404 	 ± 0.1585473394953866
	data : 0.11403732299804688
	model : 0.06490087509155273
			 train-loss:  2.2209455707798833 	 ± 0.1594998252919666
	data : 0.11384291648864746
	model : 0.06505403518676758
			 train-loss:  2.211453447739283 	 ± 0.16264223119925642
	data : 0.11370515823364258
	model : 0.06502761840820312
			 train-loss:  2.1963365840911866 	 ± 0.17572383205478145
	data : 0.11357359886169434
	model : 0.06498327255249023
			 train-loss:  2.1892934762514553 	 ± 0.17587310963696554
	data : 0.11351099014282226
	model : 0.06498498916625976
			 train-loss:  2.1875441250977694 	 ± 0.17281582994063777
	data : 0.11353917121887207
	model : 0.06495623588562012
			 train-loss:  2.187210372516087 	 ± 0.16971063719313298
	data : 0.11363439559936524
	model : 0.06481370925903321
			 train-loss:  2.1994859267925393 	 ± 0.1789632275434998
	data : 0.1139495849609375
	model : 0.06483688354492187
			 train-loss:  2.1961338678995768 	 ± 0.1768787576793637
	data : 0.11406388282775878
	model : 0.06482410430908203
			 train-loss:  2.1926069028915895 	 ± 0.17507155826944923
	data : 0.11392579078674317
	model : 0.0647510051727295
			 train-loss:  2.183892611414194 	 ± 0.1790149200230139
	data : 0.11385016441345215
	model : 0.06474361419677735
			 train-loss:  2.1745761380051123 	 ± 0.1839911247521075
	data : 0.11380519866943359
	model : 0.0647775650024414
			 train-loss:  2.172118670800153 	 ± 0.18181407296451754
	data : 0.11387805938720703
	model : 0.0648231029510498
			 train-loss:  2.1610732351030624 	 ± 0.1904204449961857
	data : 0.11383671760559082
	model : 0.06491150856018066
			 train-loss:  2.1642828981081643 	 ± 0.18871484556092782
	data : 0.11390848159790039
	model : 0.06547102928161622
			 train-loss:  2.1716173597284265 	 ± 0.1912782457200358
	data : 0.11355805397033691
	model : 0.06549935340881348
			 train-loss:  2.1725728637293766 	 ± 0.18883411339848635
	data : 0.11346487998962403
	model : 0.0654365062713623
			 train-loss:  2.1676887701719236 	 ± 0.18881332707151896
	data : 0.11314444541931153
	model : 0.06566143035888672
			 train-loss:  2.166203609108925 	 ± 0.18666877912430838
	data : 0.11304283142089844
	model : 0.06558928489685059
			 train-loss:  2.171007234875749 	 ± 0.18686450220929374
	data : 0.11309075355529785
	model : 0.06511898040771484
			 train-loss:  2.1656853017352877 	 ± 0.18774502643925164
	data : 0.11358070373535156
	model : 0.06510004997253419
			 train-loss:  2.16970708758332 	 ± 0.18737078072039898
	data : 0.11363410949707031
	model : 0.06515278816223144
			 train-loss:  2.1674258925698022 	 ± 0.18583237147170495
	data : 0.11372113227844238
	model : 0.06495480537414551
			 train-loss:  2.1627231491936576 	 ± 0.1863849521368552
	data : 0.11376080513000489
	model : 0.06490793228149414
			 train-loss:  2.154950683531554 	 ± 0.1915793624713977
	data : 0.11378517150878906
	model : 0.06487536430358887
			 train-loss:  2.157214324525062 	 ± 0.19015112904769094
	data : 0.11374835968017578
	model : 0.06487340927124023
			 train-loss:  2.160435882707437 	 ± 0.18945173046294664
	data : 0.11387276649475098
	model : 0.06491436958312988
			 train-loss:  2.162202881307018 	 ± 0.18790779352515963
	data : 0.1139976978302002
	model : 0.06487064361572266
			 train-loss:  2.168795545101166 	 ± 0.19165815836894287
	data : 0.11416559219360352
	model : 0.06491322517395019
			 train-loss:  2.1711770997327915 	 ± 0.19051558423983134
	data : 0.11414999961853027
	model : 0.06491751670837402
			 train-loss:  2.1731502299125376 	 ± 0.18920026322388617
	data : 0.11400690078735351
	model : 0.06488451957702637
			 train-loss:  2.1739774852428795 	 ± 0.1875017751772569
	data : 0.11397261619567871
	model : 0.0648277759552002
			 train-loss:  2.1805430540332087 	 ± 0.19180854114957077
	data : 0.11391816139221192
	model : 0.06476187705993652
			 train-loss:  2.1779173612594604 	 ± 0.19103373564412005
	data : 0.11383500099182128
	model : 0.06482415199279785
			 train-loss:  2.1751823318856105 	 ± 0.19040386758712807
	data : 0.11401834487915039
	model : 0.06485366821289062
			 train-loss:  2.169596692972016 	 ± 0.19329968552160626
	data : 0.11422805786132813
	model : 0.06490130424499511
			 train-loss:  2.171653443369372 	 ± 0.19225418316063356
	data : 0.11423144340515137
	model : 0.06491165161132813
			 train-loss:  2.16982866545855 	 ± 0.1911238634506806
	data : 0.11431035995483399
	model : 0.06493139266967773
			 train-loss:  2.173450469970703 	 ± 0.19155536084425687
	data : 0.11426405906677246
	model : 0.06483621597290039
			 train-loss:  2.168349551372841 	 ± 0.1940440321955934
	data : 0.11415739059448242
	model : 0.06478819847106934
			 train-loss:  2.1661878862688617 	 ± 0.19321185041915787
	data : 0.114019775390625
	model : 0.06476640701293945
			 train-loss:  2.167238920453995 	 ± 0.19185086962224293
	data : 0.11390814781188965
	model : 0.06477246284484864
			 train-loss:  2.1717241033911705 	 ± 0.19364661929998944
	data : 0.11387033462524414
	model : 0.06478157043457031
			 train-loss:  2.176824503678542 	 ± 0.19643575690107806
	data : 0.11394686698913574
	model : 0.06482987403869629
			 train-loss:  2.179435881701383 	 ± 0.19607551902491932
	data : 0.11396827697753906
	model : 0.06481437683105469
			 train-loss:  2.1793865944022563 	 ± 0.19460717979685915
	data : 0.11400671005249023
	model : 0.06482434272766113
			 train-loss:  2.179875398383421 	 ± 0.1932123758650826
	data : 0.11413774490356446
	model : 0.0647852897644043
			 train-loss:  2.178739184918611 	 ± 0.19203588099863952
	data : 0.11396522521972656
	model : 0.06480274200439454
			 train-loss:  2.1754737683704923 	 ± 0.19257906788967072
	data : 0.11390161514282227
	model : 0.06482181549072266
			 train-loss:  2.171157872173148 	 ± 0.1945976277803481
	data : 0.11387543678283692
	model : 0.06490087509155273
			 train-loss:  2.1726414776510663 	 ± 0.19364546484612546
	data : 0.11397504806518555
	model : 0.06488070487976075
			 train-loss:  2.1856953950777447 	 ± 0.22193248928796647
	data : 0.11397409439086914
	model : 0.06490411758422851
			 train-loss:  2.1871284681397514 	 ± 0.22076764886982808
	data : 0.11420154571533203
	model : 0.06486411094665527
			 train-loss:  2.1866743421554564 	 ± 0.219325719210773
	data : 0.11417613029479981
	model : 0.06479039192199706
			 train-loss:  2.185266681407627 	 ± 0.2182187890838481
	data : 0.11408767700195313
	model : 0.06474347114562988
			 train-loss:  2.183804748894332 	 ± 0.2171714446511471
	data : 0.11408014297485351
	model : 0.06477265357971192
			 train-loss:  2.1826700140268374 	 ± 0.21600445347053251
	data : 0.11413578987121582
	model : 0.06481642723083496
			 train-loss:  2.184116365034369 	 ± 0.21501276223448013
	data : 0.11413917541503907
	model : 0.06486296653747559
			 train-loss:  2.1830439284443854 	 ± 0.21387722254503425
	data : 0.11419925689697266
	model : 0.06490578651428222
			 train-loss:  2.182557722668589 	 ± 0.2125973749977845
	data : 0.1142007827758789
	model : 0.06491613388061523
			 train-loss:  2.1815875870425527 	 ± 0.21147739191455622
	data : 0.11412053108215332
	model : 0.06487045288085938
			 train-loss:  2.1887240452938768 	 ± 0.2199091646926918
	data : 0.11375255584716797
	model : 0.06475825309753418
			 train-loss:  2.190631921802248 	 ± 0.21928621786473496
	data : 0.11367154121398926
	model : 0.06473474502563477
			 train-loss:  2.190865391843459 	 ± 0.21800298427511333
	data : 0.11378827095031738
	model : 0.06475448608398438
			 train-loss:  2.1911102713540545 	 ± 0.21674357771306269
	data : 0.11388654708862304
	model : 0.06477022171020508
			 train-loss:  2.18993953315691 	 ± 0.21576764821499633
	data : 0.11391606330871581
	model : 0.064837646484375
			 train-loss:  2.1923076144673606 	 ± 0.2156722411708892
	data : 0.11422271728515625
	model : 0.06493263244628907
			 train-loss:  2.1913642226979975 	 ± 0.214639697054123
	data : 0.11420965194702148
	model : 0.06494865417480469
			 train-loss:  2.1920998798476323 	 ± 0.2135567242764299
	data : 0.11397242546081543
	model : 0.06487622261047363
			 train-loss:  2.1931344192106645 	 ± 0.21260674703978125
	data : 0.11369557380676269
	model : 0.06484498977661132
			 train-loss:  2.196313307337139 	 ± 0.2136115442533432
	data : 0.1135939121246338
	model : 0.06481657028198243
			 train-loss:  2.1929140731852543 	 ± 0.21494717671887667
	data : 0.11369800567626953
	model : 0.0648160457611084
			 train-loss:  2.1926314196688064 	 ± 0.21381815900189624
	data : 0.11383132934570313
	model : 0.06482510566711426
			 train-loss:  2.1937044118580067 	 ± 0.21294408703913342
	data : 0.11395907402038574
	model : 0.06490464210510254
			 train-loss:  2.190878519167503 	 ± 0.21361525492918748
	data : 0.11418852806091309
	model : 0.06489386558532714
			 train-loss:  2.189026039900239 	 ± 0.21328499892143798
	data : 0.11416130065917969
	model : 0.06483960151672363
			 train-loss:  2.189800443697949 	 ± 0.21233104602037864
	data : 0.11410417556762695
	model : 0.06479291915893555
			 train-loss:  2.191329275718843 	 ± 0.2117973854087906
	data : 0.11394410133361817
	model : 0.06478271484375
			 train-loss:  2.190386964082718 	 ± 0.21094420663312347
	data : 0.11403141021728516
	model : 0.06476569175720215
			 train-loss:  2.1904593422861383 	 ± 0.20989857855243313
	data : 0.11410603523254395
	model : 0.06477198600769044
			 train-loss:  2.1942823926607766 	 ± 0.2123715258336295
	data : 0.11427621841430664
	model : 0.0648118019104004
			 train-loss:  2.1939535939577715 	 ± 0.2113641687188632
	data : 0.11410555839538575
	model : 0.06486530303955078
			 train-loss:  2.195386741023797 	 ± 0.21084781028691865
	data : 0.11414680480957032
	model : 0.06487569808959961
			 train-loss:  2.1951693954921905 	 ± 0.20985307689053934
	data : 0.11392111778259277
	model : 0.06483225822448731
			 train-loss:  2.1935110643224895 	 ± 0.20955098336033634
	data : 0.11387515068054199
	model : 0.06484875679016114
			 train-loss:  2.198632847482913 	 ± 0.21513225689381796
	data : 0.11386561393737793
	model : 0.0648676872253418
			 train-loss:  2.201910688921257 	 ± 0.21680172074407103
	data : 0.11400542259216309
	model : 0.06486458778381347
			 train-loss:  2.202223577630629 	 ± 0.21582942151306977
	data : 0.11419291496276855
	model : 0.06487140655517579
			 train-loss:  2.206849259679968 	 ± 0.220207014492712
	data : 0.11440472602844239
	model : 0.06491470336914062
			 train-loss:  2.2105106437528454 	 ± 0.2225508995216394
	data : 0.11432275772094727
	model : 0.0649040699005127
			 train-loss:  2.214788516717298 	 ± 0.22609290658679615
	data : 0.11419363021850586
	model : 0.06483988761901856
			 train-loss:  2.213795998455149 	 ± 0.22533521932048417
	data : 0.11423735618591309
	model : 0.06481976509094238
			 train-loss:  2.2132653259394464 	 ± 0.22441564162063535
	data : 0.11405959129333496
	model : 0.06480412483215332
			 train-loss:  2.2143537282943724 	 ± 0.22373978883364698
	data : 0.11399397850036622
	model : 0.06479897499084472
			 train-loss:  2.213947001202353 	 ± 0.22281599998563936
	data : 0.11404151916503906
	model : 0.0648529052734375
			 train-loss:  2.2131144154785027 	 ± 0.22204289643336883
	data : 0.11412992477416992
	model : 0.06494183540344238
			 train-loss:  2.2121002866049944 	 ± 0.22137198316316237
	data : 0.11410179138183593
	model : 0.06496129035949708
			 train-loss:  2.209261102836673 	 ± 0.2225869208896243
	data : 0.11415863037109375
	model : 0.0649947166442871
			 train-loss:  2.2073004891475043 	 ± 0.2226869991529331
	data : 0.11410870552062988
	model : 0.064967679977417
			 train-loss:  2.215647722078749 	 ± 0.23987668650707805
	data : 0.11413326263427734
	model : 0.06489315032958984
			 train-loss:  2.214282849773032 	 ± 0.2393628765514704
	data : 0.11416511535644532
	model : 0.06491589546203613
			 train-loss:  2.2138857250291157 	 ± 0.23842822275656655
	data : 0.11425061225891113
	model : 0.06494746208190919
			 train-loss:  2.215798367415705 	 ± 0.23841041161020657
	data : 0.1143000602722168
	model : 0.06493244171142579
			 train-loss:  2.2136015844345094 	 ± 0.23871157124671896
	data : 0.11448206901550292
	model : 0.06499300003051758
			 train-loss:  2.211844731890966 	 ± 0.23857238497451252
	data : 0.11450200080871582
	model : 0.06506423950195313
			 train-loss:  2.210185719287302 	 ± 0.23835983701742
	data : 0.1144484519958496
	model : 0.06503796577453613
			 train-loss:  2.2093558628112078 	 ± 0.23761102992577018
	data : 0.11423668861389161
	model : 0.06494998931884766
			 train-loss:  2.2073075595752214 	 ± 0.23782002600815777
	data : 0.1141930103302002
	model : 0.06490740776062012
			 train-loss:  2.20474336880904 	 ± 0.23868700217847022
	data : 0.11408720016479493
	model : 0.06489238739013672
			 train-loss:  2.207239904476486 	 ± 0.23947199819783482
	data : 0.11403841972351074
	model : 0.0648573875427246
			 train-loss:  2.2051340395754035 	 ± 0.23977767526112653
	data : 0.11403617858886719
	model : 0.06485991477966309
			 train-loss:  2.2041517791891456 	 ± 0.2391409858886297
	data : 0.11403422355651856
	model : 0.0649383544921875
			 train-loss:  2.2059203546438644 	 ± 0.23911845852992325
	data : 0.11408348083496093
	model : 0.06492757797241211
			 train-loss:  2.20367643126735 	 ± 0.2396430990042418
	data : 0.11386661529541016
	model : 0.0649017333984375
			 train-loss:  2.205772903912208 	 ± 0.23999978306635222
	data : 0.11393952369689941
	model : 0.06486744880676269
			 train-loss:  2.2032275983016856 	 ± 0.2409575598404847
	data : 0.11390466690063476
	model : 0.06487421989440918
			 train-loss:  2.200429565664651 	 ± 0.24230639327522818
	data : 0.11409158706665039
	model : 0.06489381790161133
			 train-loss:  2.1997449741089086 	 ± 0.24156711860770128
	data : 0.11407523155212403
	model : 0.06496667861938477
			 train-loss:  2.1971665390900204 	 ± 0.24261486181690164
	data : 0.11432533264160157
	model : 0.06502733230590821
			 train-loss:  2.1961423091009156 	 ± 0.24205655739071516
	data : 0.11431307792663574
	model : 0.0650761604309082
			 train-loss:  2.195964493382145 	 ± 0.24121198113617093
	data : 0.1144559383392334
	model : 0.06510601043701172
			 train-loss:  2.1984235448437137 	 ± 0.24214666799538176
	data : 0.11425962448120117
	model : 0.06503853797912598
			 train-loss:  2.196162465545866 	 ± 0.24281455030854704
	data : 0.11411423683166504
	model : 0.06492657661437988
			 train-loss:  2.1987533766647864 	 ± 0.2439650360997903
	data : 0.11402931213378906
	model : 0.06491107940673828
			 train-loss:  2.199201072732063 	 ± 0.24318786458410113
	data : 0.11406865119934081
	model : 0.06491603851318359
			 train-loss:  2.197977907803594 	 ± 0.2428095088677187
	data : 0.11401495933532715
	model : 0.06492576599121094
			 train-loss:  2.198336227520092 	 ± 0.24202681034792328
	data : 0.11415791511535645
	model : 0.06500391960144043
			 train-loss:  2.1987108832237703 	 ± 0.24125633080710024
	data : 0.11429190635681152
	model : 0.06514701843261719
			 train-loss:  2.2013633028666177 	 ± 0.24262079576527976
	data : 0.11424484252929687
	model : 0.06511750221252441
			 train-loss:  2.201904295296069 	 ± 0.2419068374639627
	data : 0.11404070854187012
	model : 0.06511917114257812
			 train-loss:  2.2009956789644143 	 ± 0.24136815916669713
	data : 0.11399412155151367
	model : 0.06507291793823242
			 train-loss:  2.201455376506631 	 ± 0.24064482976175736
	data : 0.11391634941101074
	model : 0.06502294540405273
			 train-loss:  2.2011043824158705 	 ± 0.2399015312087938
	data : 0.11405115127563477
	model : 0.06498007774353028
			 train-loss:  2.200763350148355 	 ± 0.23916385033969284
	data : 0.11413578987121582
	model : 0.06501660346984864
			 train-loss:  2.200768721409333 	 ± 0.23839607651631753
	data : 0.11425628662109374
	model : 0.06502289772033691
			 train-loss:  2.2046727147071983 	 ± 0.2425867250681233
	data : 0.11435527801513672
	model : 0.06501517295837403
			 train-loss:  2.204891980448856 	 ± 0.24183343363776366
	data : 0.11437711715698243
	model : 0.06501531600952148
			 train-loss:  2.2036290993480563 	 ± 0.24159382999017417
	data : 0.11411709785461426
	model : 0.06494712829589844
			 train-loss:  2.2042354136705398 	 ± 0.24095898496358062
	data : 0.11430330276489258
	model : 0.06485180854797364
			 train-loss:  2.2026809579837394 	 ± 0.24101289770248596
	data : 0.11428837776184082
	model : 0.0648193359375
			 train-loss:  2.20570327617504 	 ± 0.24330904906945425
	data : 0.11435427665710449
	model : 0.0648118019104004
			 train-loss:  2.202578129212549 	 ± 0.24580131725292692
	data : 0.11447577476501465
	model : 0.06488237380981446
			 train-loss:  2.2009785364313825 	 ± 0.2459002866864971
	data : 0.1146888256072998
	model : 0.06499457359313965
			 train-loss:  2.2011113571398186 	 ± 0.24515990251814554
	data : 0.11457986831665039
	model : 0.06505680084228516
			 train-loss:  2.19937806675233 	 ± 0.24543230844846145
	data : 0.11468386650085449
	model : 0.06506495475769043
			 train-loss:  2.1992204089364606 	 ± 0.24470480873370348
	data : 0.11458787918090821
	model : 0.06507625579833984
			 train-loss:  2.2007848705564226 	 ± 0.24481166519052233
	data : 0.11448688507080078
	model : 0.0649688720703125
			 train-loss:  2.2043312823278662 	 ± 0.24837685454210762
	data : 0.11422252655029297
	model : 0.06487822532653809
			 train-loss:  2.2048281333025765 	 ± 0.2477294749592259
	data : 0.11425065994262695
	model : 0.0648488998413086
			 train-loss:  2.2028623219819097 	 ± 0.24833033424661774
	data : 0.11400914192199707
	model : 0.06487951278686524
			 train-loss:  2.203019686216532 	 ± 0.2476159422258272
	data : 0.11416521072387695
	model : 0.06486153602600098
			 train-loss:  2.202228436580283 	 ± 0.2471172307312317
	data : 0.11422052383422851
	model : 0.06491308212280274
			 train-loss:  2.2008259638972665 	 ± 0.2470956195614017
	data : 0.11455135345458985
	model : 0.06500167846679687
			 train-loss:  2.2009061595371793 	 ± 0.24639089153204694
	data : 0.11453795433044434
	model : 0.06505508422851562
			 train-loss:  2.1985569189895284 	 ± 0.24764763250346847
	data : 0.11478123664855958
	model : 0.06505336761474609
			 train-loss:  2.1971036875988803 	 ± 0.24769850125093365
	data : 0.11453833580017089
	model : 0.06509189605712891
			 train-loss:  2.196320599384522 	 ± 0.2472213588235125
	data : 0.1144629955291748
	model : 0.0650515079498291
			 train-loss:  2.19699076167698 	 ± 0.24669191230432735
	data : 0.11418442726135254
	model : 0.06493330001831055
			 train-loss:  2.1959021263652376 	 ± 0.24643648961635636
	data : 0.11399531364440918
	model : 0.06493773460388183
			 train-loss:  2.196599807528501 	 ± 0.2459329785964515
	data : 0.11397800445556641
	model : 0.06487951278686524
			 train-loss:  2.197818478385171 	 ± 0.2458038228705445
	data : 0.1140364170074463
	model : 0.06486759185791016
			 train-loss:  2.197774548348182 	 ± 0.24513202414674545
	data : 0.11413331031799316
	model : 0.06490354537963867
			 train-loss:  2.1982645625653476 	 ± 0.24455485193543763
	data : 0.11424694061279297
	model : 0.06497268676757813
			 train-loss:  2.1981401778556204 	 ± 0.24389883329305356
	data : 0.11442952156066895
	model : 0.06497220993041992
			 train-loss:  2.198184791431632 	 ± 0.24324306454753544
	data : 0.11431570053100586
	model : 0.064998197555542
			 train-loss:  2.198788509011906 	 ± 0.24273149508811864
	data : 0.11424932479858399
	model : 0.06493973731994629
			 train-loss:  2.199028817897147 	 ± 0.24210737484810435
	data : 0.11407065391540527
	model : 0.06493644714355469
			 train-loss:  2.198597368109163 	 ± 0.24153848448506376
	data : 0.11419005393981933
	model : 0.06496667861938477
			 train-loss:  2.197859519406369 	 ± 0.2411154867511846
	data : 0.11414718627929688
	model : 0.06500706672668458
			 train-loss:  2.1978051537618586 	 ± 0.24048463361897873
	data : 0.11434168815612793
	model : 0.06505904197692872
			 train-loss:  2.196685833235582 	 ± 0.24035587361591806
	data : 0.11441612243652344
	model : 0.06514720916748047
			 train-loss:  2.1953064777690514 	 ± 0.24049307263188768
	data : 0.11451897621154786
	model : 0.06520018577575684
			 train-loss:  2.1939742399245192 	 ± 0.24058540494358766
	data : 0.11450467109680176
	model : 0.06518583297729492
			 train-loss:  2.1947214695123525 	 ± 0.24019331797807325
	data : 0.11450533866882324
	model : 0.06511330604553223
			 train-loss:  2.1932850668624955 	 ± 0.24041799657987412
	data : 0.1142876148223877
	model : 0.06502180099487305
			 train-loss:  2.1921962360440173 	 ± 0.24029102404582972
	data : 0.11434412002563477
	model : 0.06496653556823731
			 train-loss:  2.1916541198287347 	 ± 0.23980420666388524
	data : 0.11445031166076661
	model : 0.06489315032958984
			 train-loss:  2.1908796204993473 	 ± 0.2394490603740832
	data : 0.11440973281860352
	model : 0.06526613235473633
			 train-loss:  2.1923496460914613 	 ± 0.23974821651998507
	data : 0.114141845703125
	model : 0.06528878211975098
			 train-loss:  2.1945102535076995 	 ± 0.24109518037559335
	data : 0.11424064636230469
	model : 0.0653341293334961
			 train-loss:  2.194353955807072 	 ± 0.24050787796279682
	data : 0.11427836418151856
	model : 0.06535477638244629
			 train-loss:  2.19717498483329 	 ± 0.2432419592059386
	data : 0.11424040794372559
	model : 0.06535310745239258
			 train-loss:  2.1965516782274435 	 ± 0.24280750786401983
	data : 0.1141965389251709
	model : 0.06498761177062988
			 train-loss:  2.195300518594137 	 ± 0.2428728882079572
	data : 0.1143770694732666
	model : 0.06496620178222656
			 train-loss:  2.194709170211866 	 ± 0.24243056954201025
	data : 0.11441025733947754
	model : 0.06495070457458496
			 train-loss:  2.1934043934955687 	 ± 0.24256825564303883
	data : 0.11441159248352051
	model : 0.0649101734161377
			 train-loss:  2.191669974189538 	 ± 0.24326770573153855
	data : 0.11433887481689453
	model : 0.0649336814880371
			 train-loss:  2.190816704736372 	 ± 0.24299683330988753
	data : 0.1144026756286621
	model : 0.06492109298706054
			 train-loss:  2.191335421516782 	 ± 0.24253353919915024
	data : 0.11455202102661133
	model : 0.06497335433959961
			 train-loss:  2.1903687003664496 	 ± 0.24236334992393635
	data : 0.11454310417175292
	model : 0.06498527526855469
			 train-loss:  2.192299856892172 	 ± 0.24341284697405347
	data : 0.11448154449462891
	model : 0.06497712135314941
			 train-loss:  2.1924774909803006 	 ± 0.24285455596120706
	data : 0.11444993019104004
	model : 0.06494679450988769
			 train-loss:  2.193378553769299 	 ± 0.24264309874196388
	data : 0.11435089111328126
	model : 0.0648622989654541
			 train-loss:  2.1929751556973125 	 ± 0.2421500717464674
	data : 0.11422576904296874
	model : 0.06480121612548828
			 train-loss:  2.1921058501358384 	 ± 0.24192491615225795
	data : 0.11421794891357422
	model : 0.06484827995300294
			 train-loss:  2.190746850132393 	 ± 0.24219182193276004
	data : 0.11425113677978516
	model : 0.06490330696105957
			 train-loss:  2.194558962769465 	 ± 0.2480751869071669
	data : 0.11438913345336914
	model : 0.06486873626708985
			 train-loss:  2.195670536119644 	 ± 0.24805170356688228
	data : 0.11442036628723144
	model : 0.06489858627319336
			 train-loss:  2.1949243621392682 	 ± 0.247733528784952
	data : 0.11453137397766114
	model : 0.06495237350463867
			 train-loss:  2.195057354361763 	 ± 0.24718028137035908
	data : 0.11454792022705078
	model : 0.0648810863494873
			 train-loss:  2.1955623089730203 	 ± 0.246737158255892
	data : 0.11460537910461426
	model : 0.06476993560791015
			 train-loss:  2.1960000916981377 	 ± 0.24626971267015163
	data : 0.1144195556640625
	model : 0.06466856002807617
			 train-loss:  2.1966511319790567 	 ± 0.2459116452178505
	data : 0.11450781822204589
	model : 0.06451430320739746
			 train-loss:  2.196940607494778 	 ± 0.24540281323190322
	data : 0.1144937515258789
	model : 0.06429538726806641
			 train-loss:  2.1970594752151356 	 ± 0.24486577643329732
	data : 0.11459369659423828
	model : 0.06414093971252441
			 train-loss:  2.197200267850565 	 ± 0.24433499689931895
	data : 0.11454195976257324
	model : 0.06409697532653809
			 train-loss:  2.1964621366116037 	 ± 0.24405210214920442
	data : 0.11475577354431152
	model : 0.06410779953002929
			 train-loss:  2.1990873865685607 	 ± 0.24672392810924415
	data : 0.11474061012268066
	model : 0.0641066551208496
			 train-loss:  2.199027875195379 	 ± 0.2461886346486426
	data : 0.11465620994567871
	model : 0.06400561332702637
			 train-loss:  2.201937116069711 	 ± 0.24958588671992218
	data : 0.11468687057495117
	model : 0.0639078140258789
			 train-loss:  2.2017059644748427 	 ± 0.24907218355167457
	data : 0.11477241516113282
	model : 0.06385698318481445
			 train-loss:  2.2023773531033757 	 ± 0.2487474151581631
	data : 0.11486344337463379
	model : 0.06381635665893555
			 train-loss:  2.2019703408591766 	 ± 0.24829307446727178
	data : 0.11487712860107421
	model : 0.06377301216125489
			 train-loss:  2.2021649827348426 	 ± 0.24778211787246293
	data : 0.11505060195922852
	model : 0.06386308670043946
			 train-loss:  2.201630623663886 	 ± 0.24739225391754613
	data : 0.11508622169494628
	model : 0.06395530700683594
			 train-loss:  2.2022642898157176 	 ± 0.24706162963467002
	data : 0.11503105163574219
	model : 0.06387696266174317
			 train-loss:  2.2010594591373156 	 ± 0.24723877888731177
	data : 0.1149075984954834
	model : 0.06381068229675294
			 train-loss:  2.2021565282693967 	 ± 0.24730082827137223
	data : 0.114963960647583
	model : 0.06384773254394531
			 train-loss:  2.2022568667928377 	 ± 0.24678995547614452
	data : 0.11494626998901367
	model : 0.06386876106262207
			 train-loss:  2.2019926230442475 	 ± 0.24631143102388825
	data : 0.11483659744262695
	model : 0.0638770580291748
			 train-loss:  2.2015128012531058 	 ± 0.24591483598293748
	data : 0.11487336158752441
	model : 0.06391057968139649
			 train-loss:  2.2006283086023215 	 ± 0.2457937450685202
	data : 0.11494908332824708
	model : 0.06392402648925781
			 train-loss:  2.20067814676488 	 ± 0.24529078256596468
	data : 0.11469364166259766
	model : 0.06385407447814942
			 train-loss:  2.1996931489633056 	 ± 0.24527274602946247
	data : 0.11469488143920899
	model : 0.06385087966918945
			 train-loss:  2.199066721811527 	 ± 0.2449700246806124
	data : 0.11488933563232422
	model : 0.06383266448974609
			 train-loss:  2.198827451539908 	 ± 0.24450243316565845
	data : 0.11491303443908692
	model : 0.06391367912292481
			 train-loss:  2.1999111295707765 	 ± 0.2446026411867043
	data : 0.11487326622009278
	model : 0.06393914222717285
			 train-loss:  2.2004793106791483 	 ± 0.24427490849222158
	data : 0.11509895324707031
	model : 0.06399769783020019
			 train-loss:  2.1993095445632935 	 ± 0.24448368009724603
	data : 0.11506266593933105
	model : 0.063934326171875
			 train-loss:  2.198557905941845 	 ± 0.24428543438977865
	data : 0.11472539901733399
	model : 0.0639373779296875
			 train-loss:  2.1980594472279624 	 ± 0.24392812496908572
	data : 0.1148529052734375
	model : 0.06390800476074218
			 train-loss:  2.1962792656638404 	 ± 0.2450802895011096
	data : 0.11491184234619141
	model : 0.06395559310913086
			 train-loss:  2.1968567652965154 	 ± 0.2447697927924266
	data : 0.11500401496887207
	model : 0.06396560668945313
			 train-loss:  2.1958751972983865 	 ± 0.2447897552892792
	data : 0.1150296688079834
	model : 0.06403326988220215
			 train-loss:  2.19677373720333 	 ± 0.24473216839472128
	data : 0.11498723030090333
	model : 0.05559773445129394
#epoch  69    val-loss:  2.394795580914146  train-loss:  2.19677373720333  lr:  1.9073486328125e-08
			 train-loss:  2.3024230003356934 	 ± 0.0
	data : 5.499762535095215
	model : 0.07834172248840332
			 train-loss:  2.1805180311203003 	 ± 0.12190496921539307
	data : 2.8134957551956177
	model : 0.0742119550704956
			 train-loss:  2.18035888671875 	 ± 0.0995352450147771
	data : 1.9147961139678955
	model : 0.07106280326843262
			 train-loss:  2.1100451946258545 	 ± 0.14920621514306076
	data : 1.464434802532196
	model : 0.06943470239639282
			 train-loss:  2.150189447402954 	 ± 0.155744148634806
	data : 1.1943715572357179
	model : 0.06849479675292969
			 train-loss:  2.141664425532023 	 ± 0.14344655166466375
	data : 0.11726117134094238
	model : 0.06584553718566895
			 train-loss:  2.1520207609449113 	 ± 0.13520679072159722
	data : 0.11467475891113281
	model : 0.06479363441467285
			 train-loss:  2.129546597599983 	 ± 0.13975472389676552
	data : 0.11401371955871582
	model : 0.06482920646667481
			 train-loss:  2.114467157257928 	 ± 0.13849312390705787
	data : 0.11427392959594726
	model : 0.06487960815429687
			 train-loss:  2.1077085852622988 	 ± 0.13294139871147378
	data : 0.11421327590942383
	model : 0.0649019718170166
			 train-loss:  2.11234269358895 	 ± 0.12759894574649128
	data : 0.11402149200439453
	model : 0.06480011940002442
			 train-loss:  2.109533021847407 	 ± 0.12252157708295923
	data : 0.1140164852142334
	model : 0.06482295989990235
			 train-loss:  2.1382048771931577 	 ± 0.1540185271796616
	data : 0.11407575607299805
	model : 0.06479620933532715
			 train-loss:  2.1483273591314043 	 ± 0.15283762435929518
	data : 0.11394405364990234
	model : 0.06481924057006835
			 train-loss:  2.158959078788757 	 ± 0.1529199745139784
	data : 0.11395478248596191
	model : 0.06483087539672852
			 train-loss:  2.1723151579499245 	 ± 0.15683991504368877
	data : 0.11415810585021972
	model : 0.06490964889526367
			 train-loss:  2.1557663328507366 	 ± 0.1659324890577061
	data : 0.11408495903015137
	model : 0.06496453285217285
			 train-loss:  2.151609049903022 	 ± 0.16216583490439487
	data : 0.11407942771911621
	model : 0.0650050163269043
			 train-loss:  2.1482617102171244 	 ± 0.15847823123979787
	data : 0.1141385555267334
	model : 0.06497311592102051
			 train-loss:  2.1438820719718934 	 ± 0.1556406941647102
	data : 0.11415243148803711
	model : 0.06498346328735352
			 train-loss:  2.1641344797043574 	 ± 0.17684371921820632
	data : 0.11404094696044922
	model : 0.06498713493347168
			 train-loss:  2.208616527644071 	 ± 0.2672150323787897
	data : 0.11410555839538575
	model : 0.06489663124084473
			 train-loss:  2.2013333154761274 	 ± 0.2635647002871019
	data : 0.11405267715454101
	model : 0.064888334274292
			 train-loss:  2.1969940265019736 	 ± 0.2588532340889296
	data : 0.11416196823120117
	model : 0.06491937637329101
			 train-loss:  2.2136315155029296 	 ± 0.2663984645085868
	data : 0.11425414085388183
	model : 0.0649251937866211
			 train-loss:  2.2080660416529727 	 ± 0.2627031793735147
	data : 0.11424040794372559
	model : 0.06485905647277831
			 train-loss:  2.211316055721707 	 ± 0.2583245095964427
	data : 0.1142270565032959
	model : 0.06485915184020996
			 train-loss:  2.2364082166126797 	 ± 0.2852155834900199
	data : 0.1143570899963379
	model : 0.06483349800109864
			 train-loss:  2.2462293197368752 	 ± 0.2850325278409861
	data : 0.11429367065429688
	model : 0.06484599113464355
			 train-loss:  2.2407286008199057 	 ± 0.2818029510361395
	data : 0.11432075500488281
	model : 0.06486411094665527
			 train-loss:  2.2324706277539654 	 ± 0.28088613392647754
	data : 0.11445779800415039
	model : 0.06495399475097656
			 train-loss:  2.2281758338212967 	 ± 0.27749466887231306
	data : 0.11450061798095704
	model : 0.06499905586242676
			 train-loss:  2.2583205410928437 	 ± 0.3220999320581856
	data : 0.1142995834350586
	model : 0.06497001647949219
			 train-loss:  2.2684593200683594 	 ± 0.32262853777437234
	data : 0.11423873901367188
	model : 0.06488947868347168
			 train-loss:  2.2591279336384367 	 ± 0.32260771732231525
	data : 0.11421632766723633
	model : 0.06550178527832032
			 train-loss:  2.255289292997784 	 ± 0.318905122547652
	data : 0.11367692947387695
	model : 0.06556887626647949
			 train-loss:  2.2494774927964083 	 ± 0.3164929516094165
	data : 0.11369400024414063
	model : 0.06569771766662598
			 train-loss:  2.240193379552741 	 ± 0.3173657197289305
	data : 0.11368408203125
	model : 0.06577258110046387
			 train-loss:  2.2428620411799502 	 ± 0.3137021469474165
	data : 0.11378521919250488
	model : 0.06584663391113281
			 train-loss:  2.2374093532562256 	 ± 0.3116221293370881
	data : 0.11370110511779785
	model : 0.06524014472961426
			 train-loss:  2.233115876593241 	 ± 0.3089938717659896
	data : 0.1142467975616455
	model : 0.06514549255371094
			 train-loss:  2.234712981042408 	 ± 0.30546444353681357
	data : 0.11410603523254395
	model : 0.0649484634399414
			 train-loss:  2.236331590386324 	 ± 0.30207382570971153
	data : 0.11415319442749024
	model : 0.0652306079864502
			 train-loss:  2.2409010421146047 	 ± 0.30012097507294816
	data : 0.11381521224975585
	model : 0.065486478805542
			 train-loss:  2.246110921435886 	 ± 0.2987729429255683
	data : 0.11388692855834961
	model : 0.06548099517822266
			 train-loss:  2.242706407671389 	 ± 0.2963887720113273
	data : 0.11388216018676758
	model : 0.0654815673828125
			 train-loss:  2.2392502389055617 	 ± 0.2941542258570197
	data : 0.11405248641967773
	model : 0.06556344032287598
			 train-loss:  2.2359973887602487 	 ± 0.2919270070213613
	data : 0.11417560577392578
	model : 0.0652543544769287
			 train-loss:  2.2334181532567863 	 ± 0.289484858881682
	data : 0.1143369197845459
	model : 0.0649651050567627
			 train-loss:  2.2305301570892335 	 ± 0.287287555286752
	data : 0.11439342498779297
	model : 0.06488847732543945
			 train-loss:  2.2244283849117803 	 ± 0.287710625713838
	data : 0.11435790061950683
	model : 0.06486043930053711
			 train-loss:  2.228379100561142 	 ± 0.2863241988606466
	data : 0.11434311866760254
	model : 0.06482186317443847
			 train-loss:  2.228701989605742 	 ± 0.28361972180347056
	data : 0.11433048248291015
	model : 0.06482658386230469
			 train-loss:  2.2351963188913135 	 ± 0.28493132301238816
	data : 0.11451492309570313
	model : 0.06482434272766113
			 train-loss:  2.2269490242004393 	 ± 0.28876065742084933
	data : 0.11446547508239746
	model : 0.06486735343933106
			 train-loss:  2.227225269590105 	 ± 0.2861781565970445
	data : 0.114528226852417
	model : 0.06488122940063476
			 train-loss:  2.225193893700315 	 ± 0.2840637511612849
	data : 0.11432442665100098
	model : 0.06487703323364258
			 train-loss:  2.221580620469718 	 ± 0.2829225106250744
	data : 0.11440520286560059
	model : 0.06489362716674804
			 train-loss:  2.2216882988557978 	 ± 0.28051581434943007
	data : 0.11429896354675292
	model : 0.06490001678466797
			 train-loss:  2.2220983107884726 	 ± 0.27818618802719114
	data : 0.11434197425842285
	model : 0.06487865447998047
			 train-loss:  2.2191738300636166 	 ± 0.27682496638426984
	data : 0.114300537109375
	model : 0.06489787101745606
			 train-loss:  2.2207932356865174 	 ± 0.2748745749651874
	data : 0.11442503929138184
	model : 0.06493868827819824
			 train-loss:  2.2196461132594516 	 ± 0.27283385977392693
	data : 0.11441121101379395
	model : 0.06491045951843262
			 train-loss:  2.2183964028954506 	 ± 0.27087563244830176
	data : 0.11447057723999024
	model : 0.06491231918334961
			 train-loss:  2.2161319806025577 	 ± 0.2693936718714998
	data : 0.11432886123657227
	model : 0.06491947174072266
			 train-loss:  2.212774789694584 	 ± 0.26871166291711013
	data : 0.11424880027770996
	model : 0.06484589576721192
			 train-loss:  2.2103438128286332 	 ± 0.2674290442859187
	data : 0.11421060562133789
	model : 0.06484017372131348
			 train-loss:  2.2101333141326904 	 ± 0.2654609629757341
	data : 0.11416420936584473
	model : 0.06484408378601074
			 train-loss:  2.211958484373231 	 ± 0.2639597510736538
	data : 0.11411962509155274
	model : 0.06488332748413086
			 train-loss:  2.2106203624180387 	 ± 0.26230315661098713
	data : 0.11429429054260254
	model : 0.06489453315734864
			 train-loss:  2.211165431519629 	 ± 0.2604893230323622
	data : 0.11444787979125977
	model : 0.06494221687316895
			 train-loss:  2.210319701168272 	 ± 0.258772186773441
	data : 0.11449975967407226
	model : 0.06487064361572266
			 train-loss:  2.209862712311418 	 ± 0.2570229154389482
	data : 0.11425962448120117
	model : 0.06477994918823242
			 train-loss:  2.2130621897207723 	 ± 0.256739832701848
	data : 0.11425299644470215
	model : 0.06470026969909667
			 train-loss:  2.2079939222335816 	 ± 0.2587225024750835
	data : 0.11418399810791016
	model : 0.06475419998168945
			 train-loss:  2.2075429543068537 	 ± 0.2570444160197514
	data : 0.11419858932495117
	model : 0.06478219032287598
			 train-loss:  2.2096656560897827 	 ± 0.25603945330063044
	data : 0.11431384086608887
	model : 0.06484951972961425
			 train-loss:  2.2063058568881107 	 ± 0.2560955540903249
	data : 0.11455302238464356
	model : 0.06495857238769531
			 train-loss:  2.2057399221613436 	 ± 0.25451861583526336
	data : 0.11460156440734863
	model : 0.06503829956054688
			 train-loss:  2.210741637647152 	 ± 0.25680018438543145
	data : 0.11456499099731446
	model : 0.06493268013000489
			 train-loss:  2.205802654042656 	 ± 0.2590051488750334
	data : 0.11448311805725098
	model : 0.06489505767822265
			 train-loss:  2.2055680039452343 	 ± 0.2574296672028122
	data : 0.11442332267761231
	model : 0.06488924026489258
			 train-loss:  2.2071111992181067 	 ± 0.2562554944820453
	data : 0.11448698043823242
	model : 0.0649169921875
			 train-loss:  2.205652343375342 	 ± 0.25507209865044433
	data : 0.11451840400695801
	model : 0.06492176055908203
			 train-loss:  2.204451122003443 	 ± 0.25380612524280355
	data : 0.1145289421081543
	model : 0.06501092910766601
			 train-loss:  2.203534482523452 	 ± 0.2524676757833697
	data : 0.11451635360717774
	model : 0.06505036354064941
			 train-loss:  2.2052921632240556 	 ± 0.2515412022652428
	data : 0.11445670127868653
	model : 0.06504340171813965
			 train-loss:  2.204617114229636 	 ± 0.25018715089063187
	data : 0.1142585277557373
	model : 0.06496739387512207
			 train-loss:  2.204980280961883 	 ± 0.24880096021389142
	data : 0.11412649154663086
	model : 0.06494951248168945
			 train-loss:  2.202107197708554 	 ± 0.24889511600977723
	data : 0.11406135559082031
	model : 0.0649111270904541
			 train-loss:  2.202494224349221 	 ± 0.2475510129825269
	data : 0.11415114402770996
	model : 0.06490788459777833
			 train-loss:  2.1994157342807106 	 ± 0.24794720592152303
	data : 0.11406216621398926
	model : 0.06495075225830078
			 train-loss:  2.2014881859543505 	 ± 0.2474104085842912
	data : 0.11412744522094727
	model : 0.06498398780822753
			 train-loss:  2.1986592589540686 	 ± 0.24759843041198462
	data : 0.11425509452819824
	model : 0.06500763893127441
			 train-loss:  2.1992777887143586 	 ± 0.24636483021469016
	data : 0.11436738967895507
	model : 0.06498870849609376
			 train-loss:  2.200527905176083 	 ± 0.2453810273561126
	data : 0.1143068790435791
	model : 0.06495199203491211
			 train-loss:  2.202857606189767 	 ± 0.24517778964116818
	data : 0.11419038772583008
	model : 0.06482267379760742
			 train-loss:  2.200652430252153 	 ± 0.24488865010738325
	data : 0.11429405212402344
	model : 0.0648488998413086
			 train-loss:  2.2003165989211113 	 ± 0.24367138018962983
	data : 0.11432814598083496
	model : 0.06481809616088867
			 train-loss:  2.201537996530533 	 ± 0.2427543480726838
	data : 0.11435890197753906
	model : 0.06484336853027343
			 train-loss:  2.202335998563483 	 ± 0.2416813855445026
	data : 0.11436443328857422
	model : 0.06489720344543456
			 train-loss:  2.2047203884405246 	 ± 0.24168463530230105
	data : 0.11480660438537597
	model : 0.06497907638549805
			 train-loss:  2.204351792057741 	 ± 0.24053735555040046
	data : 0.1147082805633545
	model : 0.0649606704711914
			 train-loss:  2.2031571647295585 	 ± 0.2396849708075691
	data : 0.11459064483642578
	model : 0.06493401527404785
			 train-loss:  2.2051796538489206 	 ± 0.2394309108354126
	data : 0.11436634063720703
	model : 0.06485548019409179
			 train-loss:  2.2019977434626163 	 ± 0.24051905730829493
	data : 0.11437840461730957
	model : 0.06477866172790528
			 train-loss:  2.2013487125111517 	 ± 0.23948574015331622
	data : 0.11420550346374511
	model : 0.06482067108154296
			 train-loss:  2.2036521832148233 	 ± 0.2395623282640236
	data : 0.11433982849121094
	model : 0.06479854583740234
			 train-loss:  2.2087125909437826 	 ± 0.24419096791257144
	data : 0.11437382698059081
	model : 0.06484155654907227
			 train-loss:  2.2083434039896184 	 ± 0.24310903213081508
	data : 0.114585542678833
	model : 0.0649228572845459
			 train-loss:  2.205598158879323 	 ± 0.24371818114022914
	data : 0.11463027000427246
	model : 0.06497864723205567
			 train-loss:  2.2064833619764874 	 ± 0.24280688929212155
	data : 0.11458072662353516
	model : 0.06489925384521485
			 train-loss:  2.2084087924619693 	 ± 0.2425874560413438
	data : 0.1142733097076416
	model : 0.06485428810119628
			 train-loss:  2.208262320150409 	 ± 0.24152615119095883
	data : 0.11417527198791504
	model : 0.0648472785949707
			 train-loss:  2.210469409693842 	 ± 0.24162562718590236
	data : 0.11405444145202637
	model : 0.06484827995300294
			 train-loss:  2.2107668066846915 	 ± 0.24060302085727184
	data : 0.11405396461486816
	model : 0.06486282348632813
			 train-loss:  2.2105421856937246 	 ± 0.23958481096685227
	data : 0.11411929130554199
	model : 0.06492171287536622
			 train-loss:  2.2092854936244124 	 ± 0.2389544070873295
	data : 0.11435790061950683
	model : 0.06499733924865722
			 train-loss:  2.2089845933834042 	 ± 0.23797072764573451
	data : 0.11448225975036622
	model : 0.0650026798248291
			 train-loss:  2.207465785741806 	 ± 0.2375555859394754
	data : 0.11445655822753906
	model : 0.06498532295227051
			 train-loss:  2.2052803581411187 	 ± 0.23778015492596302
	data : 0.1143730640411377
	model : 0.0649801254272461
			 train-loss:  2.2041296577844465 	 ± 0.23714169144898986
	data : 0.11417832374572753
	model : 0.06499252319335938
			 train-loss:  2.2052343347208287 	 ± 0.23649070817124684
	data : 0.1141484260559082
	model : 0.06501350402832032
			 train-loss:  2.2036426307693606 	 ± 0.23619578129787974
	data : 0.11410861015319824
	model : 0.06501469612121583
			 train-loss:  2.203672755241394 	 ± 0.23524934017779014
	data : 0.11421856880187989
	model : 0.065037202835083
			 train-loss:  2.2075135906537375 	 ± 0.23821635106457947
	data : 0.11419410705566406
	model : 0.0649794101715088
			 train-loss:  2.2074992177993296 	 ± 0.2372766927792718
	data : 0.11417665481567382
	model : 0.06490378379821778
			 train-loss:  2.205876709893346 	 ± 0.23705424474081174
	data : 0.11403355598449708
	model : 0.06483535766601563
			 train-loss:  2.204873022182967 	 ± 0.23640652021221023
	data : 0.11410226821899414
	model : 0.06481571197509765
			 train-loss:  2.2067163852544933 	 ± 0.23642435397744788
	data : 0.11415328979492187
	model : 0.06481828689575195
			 train-loss:  2.2087957222043104 	 ± 0.23671049402854832
	data : 0.1141664981842041
	model : 0.06483693122863769
			 train-loss:  2.208350745114413 	 ± 0.23586715077479312
	data : 0.1143867015838623
	model : 0.06488118171691895
			 train-loss:  2.208239895956857 	 ± 0.23498221035700376
	data : 0.11455264091491699
	model : 0.0648991584777832
			 train-loss:  2.2098671767249036 	 ± 0.2348547729801975
	data : 0.11443166732788086
	model : 0.06487574577331542
			 train-loss:  2.2083012792799206 	 ± 0.23468440176278563
	data : 0.11419553756713867
	model : 0.06483712196350097
			 train-loss:  2.208753484136918 	 ± 0.23387902479424816
	data : 0.11417312622070312
	model : 0.06480865478515625
			 train-loss:  2.2113799787785884 	 ± 0.2350283494306686
	data : 0.11423325538635254
	model : 0.06481895446777344
			 train-loss:  2.2098206592642744 	 ± 0.23488541826833215
	data : 0.1141171932220459
	model : 0.06486153602600098
			 train-loss:  2.2087031045405983 	 ± 0.23440690484354718
	data : 0.11405549049377442
	model : 0.06493163108825684
			 train-loss:  2.2093273316110884 	 ± 0.23368415437667103
	data : 0.11428017616271972
	model : 0.06498913764953614
			 train-loss:  2.2076985759938017 	 ± 0.23365014336157333
	data : 0.11436977386474609
	model : 0.06507554054260253
			 train-loss:  2.2042820117842985 	 ± 0.23633412046596397
	data : 0.11424479484558106
	model : 0.0650876522064209
			 train-loss:  2.2064499838368876 	 ± 0.23691906839178212
	data : 0.11430082321166993
	model : 0.06502032279968262
			 train-loss:  2.206690369380845 	 ± 0.23611249883692037
	data : 0.11431012153625489
	model : 0.06500744819641113
			 train-loss:  2.207338132529423 	 ± 0.23542526936102756
	data : 0.11426935195922852
	model : 0.06500387191772461
			 train-loss:  2.2059672483026165 	 ± 0.2351976523887986
	data : 0.11475872993469238
	model : 0.06498689651489258
			 train-loss:  2.205990217169937 	 ± 0.23439645950093813
	data : 0.11476497650146485
	model : 0.0649693489074707
			 train-loss:  2.207823877399032 	 ± 0.2346587566407845
	data : 0.11481261253356934
	model : 0.06498308181762695
			 train-loss:  2.2100296004506568 	 ± 0.23540437932782562
	data : 0.11482744216918946
	model : 0.06494708061218261
			 train-loss:  2.2152457729975383 	 ± 0.24310460082948146
	data : 0.1147552490234375
	model : 0.0649226188659668
			 train-loss:  2.2139317831456267 	 ± 0.2428321284946383
	data : 0.1140669345855713
	model : 0.0648726463317871
			 train-loss:  2.214134704125555 	 ± 0.24204486493885125
	data : 0.11388216018676758
	model : 0.06480770111083985
			 train-loss:  2.2126608346801957 	 ± 0.2419359248151672
	data : 0.11375966072082519
	model : 0.06484546661376953
			 train-loss:  2.212401111404617 	 ± 0.24117053748312242
	data : 0.1138737678527832
	model : 0.06486806869506836
			 train-loss:  2.2119840668093773 	 ± 0.24044701318713335
	data : 0.1139862060546875
	model : 0.06489052772521972
			 train-loss:  2.211810061564812 	 ± 0.23968490093909783
	data : 0.11409211158752441
	model : 0.06493520736694336
			 train-loss:  2.2111419219120294 	 ± 0.23906604848449844
	data : 0.11430306434631347
	model : 0.06503686904907227
			 train-loss:  2.2097272948373723 	 ± 0.23896659605211582
	data : 0.11443800926208496
	model : 0.06500201225280762
			 train-loss:  2.207718663245627 	 ± 0.23954822021671562
	data : 0.11434969902038575
	model : 0.06497611999511718
			 train-loss:  2.206821244955063 	 ± 0.2390664260142353
	data : 0.11430363655090332
	model : 0.06492595672607422
			 train-loss:  2.2055408776917074 	 ± 0.23887248572714773
	data : 0.11435089111328126
	model : 0.06496644020080566
			 train-loss:  2.2037651465262895 	 ± 0.2391976388389893
	data : 0.11443195343017579
	model : 0.06495747566223145
			 train-loss:  2.202541661408781 	 ± 0.23897069985600322
	data : 0.11447100639343262
	model : 0.06500568389892578
			 train-loss:  2.2023692581711747 	 ± 0.23825118432441963
	data : 0.11458320617675781
	model : 0.06502776145935059
			 train-loss:  2.2035156640139495 	 ± 0.23798138846058892
	data : 0.11459660530090332
	model : 0.06506423950195313
			 train-loss:  2.20278470774731 	 ± 0.2374492045036707
	data : 0.11470918655395508
	model : 0.065055513381958
			 train-loss:  2.2026544188311 	 ± 0.23674316270932963
	data : 0.11459841728210449
	model : 0.0650087833404541
			 train-loss:  2.2017696755273 	 ± 0.23631426695926575
	data : 0.11432576179504395
	model : 0.06491146087646485
			 train-loss:  2.2011939364777513 	 ± 0.2357322214166866
	data : 0.11434249877929688
	model : 0.06486735343933106
			 train-loss:  2.202322439586415 	 ± 0.23549527489569072
	data : 0.11424274444580078
	model : 0.06486344337463379
			 train-loss:  2.202497659370913 	 ± 0.23481679646236653
	data : 0.11415514945983887
	model : 0.06480827331542968
			 train-loss:  2.2021930661312368 	 ± 0.23416707200790954
	data : 0.11420011520385742
	model : 0.06486697196960449
			 train-loss:  2.202827144909456 	 ± 0.23363734810939366
	data : 0.11435952186584472
	model : 0.06499080657958985
			 train-loss:  2.201610613828418 	 ± 0.23351386770557223
	data : 0.11437101364135742
	model : 0.06501598358154297
			 train-loss:  2.200843516077314 	 ± 0.2330654887954547
	data : 0.11442046165466309
	model : 0.0650219440460205
			 train-loss:  2.200859586623582 	 ± 0.2324025249761363
	data : 0.11431593894958496
	model : 0.06499032974243164
			 train-loss:  2.200356820882377 	 ± 0.23184105592930768
	data : 0.11420888900756836
	model : 0.0649287223815918
			 train-loss:  2.200778494390209 	 ± 0.23125695554726314
	data : 0.11415362358093262
	model : 0.06488709449768067
			 train-loss:  2.199183254934556 	 ± 0.2315901167992463
	data : 0.11405348777770996
	model : 0.06492180824279785
			 train-loss:  2.199323410458035 	 ± 0.2309535274259351
	data : 0.11408276557922363
	model : 0.06492338180541993
			 train-loss:  2.1981535682362088 	 ± 0.23084881163387325
	data : 0.11419081687927246
	model : 0.06501426696777343
			 train-loss:  2.196621773662148 	 ± 0.23113429561845164
	data : 0.11420373916625977
	model : 0.0650296688079834
			 train-loss:  2.197090217324554 	 ± 0.23058853243332533
	data : 0.11429061889648437
	model : 0.0650390625
			 train-loss:  2.1950956498799115 	 ± 0.23153860768026996
	data : 0.11414413452148438
	model : 0.06500830650329589
			 train-loss:  2.194686057760909 	 ± 0.23097881129359896
	data : 0.11409139633178711
	model : 0.06495304107666015
			 train-loss:  2.1945334551154927 	 ± 0.23036641464717786
	data : 0.11413664817810058
	model : 0.06490459442138671
			 train-loss:  2.1951599101969266 	 ± 0.22990843870614064
	data : 0.11419448852539063
	model : 0.06490979194641114
			 train-loss:  2.1952252368977727 	 ± 0.2292979049947659
	data : 0.11425547599792481
	model : 0.06489691734313965
			 train-loss:  2.193691758882432 	 ± 0.22965503163754744
	data : 0.11440792083740234
	model : 0.06491785049438477
			 train-loss:  2.1921500588718215 	 ± 0.23002841046365102
	data : 0.11445555686950684
	model : 0.06496787071228027
			 train-loss:  2.1941165768039164 	 ± 0.2310212208217986
	data : 0.1143033504486084
	model : 0.06495161056518554
			 train-loss:  2.193125022575259 	 ± 0.23082594929865366
	data : 0.1142463207244873
	model : 0.06492028236389161
			 train-loss:  2.1944148904919007 	 ± 0.23091988927205384
	data : 0.11413273811340333
	model : 0.06486787796020507
			 train-loss:  2.194895384852419 	 ± 0.23042067647836303
	data : 0.11411881446838379
	model : 0.06484689712524414
			 train-loss:  2.1952686181435217 	 ± 0.22988788055112136
	data : 0.11411490440368652
	model : 0.06484670639038086
			 train-loss:  2.1941147422303957 	 ± 0.2298661165002629
	data : 0.11421136856079102
	model : 0.06488804817199707
			 train-loss:  2.1939897331489524 	 ± 0.2292886370309675
	data : 0.11419134140014649
	model : 0.06493234634399414
			 train-loss:  2.193424135747582 	 ± 0.22884662515071644
	data : 0.1143280029296875
	model : 0.06497073173522949
			 train-loss:  2.192868613717544 	 ± 0.22840471063594206
	data : 0.11439671516418456
	model : 0.065032958984375
			 train-loss:  2.1934274291992186 	 ± 0.22796932020615032
	data : 0.11440653800964355
	model : 0.06503286361694335
			 train-loss:  2.192030985557025 	 ± 0.228257452574905
	data : 0.11420688629150391
	model : 0.06501703262329102
			 train-loss:  2.1923306997459715 	 ± 0.22773140354536053
	data : 0.11428699493408204
	model : 0.06495375633239746
			 train-loss:  2.1907230862255753 	 ± 0.22831594211955902
	data : 0.11417226791381836
	model : 0.0650251865386963
			 train-loss:  2.1906581573626576 	 ± 0.22775753551173114
	data : 0.11411385536193848
	model : 0.06495919227600097
			 train-loss:  2.190942716017002 	 ± 0.22723769969682098
	data : 0.11411910057067871
	model : 0.06495795249938965
			 train-loss:  2.1889067535261506 	 ± 0.22855209826041648
	data : 0.11434917449951172
	model : 0.06499204635620118
			 train-loss:  2.187000060427016 	 ± 0.2296358466829118
	data : 0.11435513496398926
	model : 0.0650589942932129
			 train-loss:  2.1870176631670732 	 ± 0.22908331235713672
	data : 0.11440353393554688
	model : 0.06498126983642578
			 train-loss:  2.186492483011273 	 ± 0.22866009025368572
	data : 0.11441030502319335
	model : 0.06495156288146972
			 train-loss:  2.1853902663503373 	 ± 0.22867087451538037
	data : 0.1143862247467041
	model : 0.0648961067199707
			 train-loss:  2.1850355614983075 	 ± 0.22818625826292616
	data : 0.11430153846740723
	model : 0.06486539840698242
			 train-loss:  2.1871845322959826 	 ± 0.22977766220082474
	data : 0.1143533706665039
	model : 0.06482763290405273
			 train-loss:  2.1880545632939943 	 ± 0.22958739383461124
	data : 0.11429247856140137
	model : 0.0648155689239502
			 train-loss:  2.189005350955179 	 ± 0.22947028709537176
	data : 0.11426215171813965
	model : 0.06483736038208007
			 train-loss:  2.1898812554603397 	 ± 0.22929431053649857
	data : 0.11422972679138184
	model : 0.06487793922424316
			 train-loss:  2.1915045750361903 	 ± 0.2299979016738955
	data : 0.11421875953674317
	model : 0.0648158073425293
			 train-loss:  2.1899969962335404 	 ± 0.2305345611646439
	data : 0.11403756141662598
	model : 0.06476211547851562
			 train-loss:  2.191631856314633 	 ± 0.231262587984312
	data : 0.11409435272216797
	model : 0.06469569206237794
			 train-loss:  2.190932897672261 	 ± 0.23096466226119158
	data : 0.11408286094665528
	model : 0.0646967887878418
			 train-loss:  2.193513690341603 	 ± 0.2335826339920571
	data : 0.11419444084167481
	model : 0.06519064903259278
			 train-loss:  2.1926584966581872 	 ± 0.23339850816667204
	data : 0.11380319595336914
	model : 0.06524052619934081
			 train-loss:  2.19322542564289 	 ± 0.23302470388047158
	data : 0.11397805213928222
	model : 0.06523165702819825
			 train-loss:  2.1932444401386073 	 ± 0.23250181252267352
	data : 0.11399774551391602
	model : 0.06525654792785644
			 train-loss:  2.1941097815121924 	 ± 0.23234188692220548
	data : 0.11406540870666504
	model : 0.06513333320617676
			 train-loss:  2.19291467666626 	 ± 0.23251400561529445
	data : 0.1139298915863037
	model : 0.06447920799255372
			 train-loss:  2.1925456914226564 	 ± 0.23206503574943071
	data : 0.11452512741088867
	model : 0.0642697811126709
			 train-loss:  2.1917640396151774 	 ± 0.23185128662132096
	data : 0.11465177536010743
	model : 0.06416964530944824
			 train-loss:  2.1911407029419614 	 ± 0.23153283139622197
	data : 0.1147268295288086
	model : 0.06410984992980957
			 train-loss:  2.19219614741063 	 ± 0.2315757800536616
	data : 0.11489367485046387
	model : 0.06406898498535156
			 train-loss:  2.1927993162818575 	 ± 0.23125201114090957
	data : 0.11509990692138672
	model : 0.06407928466796875
			 train-loss:  2.1922934994553076 	 ± 0.23087839658955767
	data : 0.11499156951904296
	model : 0.06409277915954589
			 train-loss:  2.193341670365169 	 ± 0.23093042843548756
	data : 0.11469092369079589
	model : 0.0639878273010254
			 train-loss:  2.1951416459717974 	 ± 0.23205956813888107
	data : 0.11469898223876954
	model : 0.06389212608337402
			 train-loss:  2.1961980589434633 	 ± 0.232123970944506
	data : 0.11463375091552734
	model : 0.06386756896972656
			 train-loss:  2.195589068595399 	 ± 0.2318168202486722
	data : 0.11456122398376464
	model : 0.06390366554260254
			 train-loss:  2.1943406760692596 	 ± 0.23211543265748966
	data : 0.11458067893981934
	model : 0.06394805908203124
			 train-loss:  2.195249312537632 	 ± 0.2320454457994194
	data : 0.1148676872253418
	model : 0.06402926445007324
			 train-loss:  2.1967699943470356 	 ± 0.23273784648627108
	data : 0.11486096382141113
	model : 0.06402182579040527
			 train-loss:  2.1970074032141076 	 ± 0.2322793141958815
	data : 0.11467771530151367
	model : 0.06400394439697266
			 train-loss:  2.1962494934598604 	 ± 0.23209084623024853
	data : 0.11466217041015625
	model : 0.06393284797668457
			 train-loss:  2.195584570718504 	 ± 0.23183778640086145
	data : 0.11469154357910157
	model : 0.06394085884094239
			 train-loss:  2.1960014602369515 	 ± 0.23144878901888144
	data : 0.11471891403198242
	model : 0.06398820877075195
			 train-loss:  2.1957163629217895 	 ± 0.2310146426952385
	data : 0.11482472419738769
	model : 0.06406674385070801
			 train-loss:  2.1943349896884357 	 ± 0.2315442408030562
	data : 0.11504993438720704
	model : 0.06408514976501464
			 train-loss:  2.196500116465043 	 ± 0.23353313427301456
	data : 0.11509871482849121
	model : 0.06406927108764648
			 train-loss:  2.1964937874941324 	 ± 0.23305801112332586
	data : 0.11498265266418457
	model : 0.06394424438476562
			 train-loss:  2.196202690302119 	 ± 0.2326305633623411
	data : 0.11480469703674316
	model : 0.06394987106323242
			 train-loss:  2.1962578565843645 	 ± 0.23216269529137712
	data : 0.11480927467346191
	model : 0.06390480995178223
			 train-loss:  2.1948003615720206 	 ± 0.23283014732901464
	data : 0.11467857360839843
	model : 0.06395955085754394
			 train-loss:  2.1958858385086057 	 ± 0.232994473711319
	data : 0.11476402282714844
	model : 0.06395044326782226
			 train-loss:  2.1955061832747136 	 ± 0.23260734882491824
	data : 0.1149559497833252
	model : 0.06401600837707519
			 train-loss:  2.194917917251587 	 ± 0.232332373883453
	data : 0.1150294303894043
	model : 0.06394844055175782
			 train-loss:  2.1958131356672808 	 ± 0.23230784682951405
	data : 0.11474456787109374
	model : 0.06388473510742188
			 train-loss:  2.1956839448823704 	 ± 0.23185920314554342
	data : 0.11474051475524902
	model : 0.06382231712341309
			 train-loss:  2.1955107333613375 	 ± 0.23142059600969309
	data : 0.11465773582458497
	model : 0.06383585929870605
			 train-loss:  2.1958687203004956 	 ± 0.23103889402272887
	data : 0.11430583000183106
	model : 0.05544953346252442
#epoch  70    val-loss:  2.40446222455878  train-loss:  2.1958687203004956  lr:  1.9073486328125e-08
			 train-loss:  2.3795149326324463 	 ± 0.0
	data : 5.642827749252319
	model : 0.07273030281066895
			 train-loss:  2.2966352701187134 	 ± 0.08287966251373291
	data : 2.8861459493637085
	model : 0.06879651546478271
			 train-loss:  2.33292826016744 	 ± 0.08493362835540516
	data : 1.9621182282765706
	model : 0.06740244229634602
			 train-loss:  2.3218336701393127 	 ± 0.07602342206772338
	data : 1.5000935196876526
	model : 0.06670790910720825
			 train-loss:  2.2859536170959474 	 ± 0.09885930096803562
	data : 1.222923469543457
	model : 0.06631646156311036
			 train-loss:  2.283775051434835 	 ± 0.09037716469480975
	data : 0.11712322235107422
	model : 0.06471996307373047
			 train-loss:  2.296757629939488 	 ± 0.08951231348980103
	data : 0.11394767761230469
	model : 0.06468114852905274
			 train-loss:  2.309528023004532 	 ± 0.09029107406748135
	data : 0.11394290924072266
	model : 0.06469321250915527
			 train-loss:  2.2786494890848794 	 ± 0.12196113322422195
	data : 0.11379461288452149
	model : 0.0647240161895752
			 train-loss:  2.237118124961853 	 ± 0.1700316266724444
	data : 0.11378493309020996
	model : 0.06479449272155761
			 train-loss:  2.2347998835823755 	 ± 0.16228446053123752
	data : 0.11380572319030761
	model : 0.06483020782470703
			 train-loss:  2.221430321534475 	 ± 0.16157894675761808
	data : 0.11396222114562989
	model : 0.06491084098815918
			 train-loss:  2.207984374119685 	 ± 0.16207710859983954
	data : 0.11381535530090332
	model : 0.06498432159423828
			 train-loss:  2.2070032017571584 	 ± 0.15622147053197125
	data : 0.1140214443206787
	model : 0.06507220268249511
			 train-loss:  2.182523250579834 	 ± 0.1765443007666315
	data : 0.11388850212097168
	model : 0.06500301361083985
			 train-loss:  2.190366879105568 	 ± 0.17361663119240844
	data : 0.11385641098022461
	model : 0.0649759292602539
			 train-loss:  2.1781899788800407 	 ± 0.1753341466325969
	data : 0.11377673149108887
	model : 0.06496577262878418
			 train-loss:  2.168371876080831 	 ± 0.1751367694450521
	data : 0.11398658752441407
	model : 0.06500802040100098
			 train-loss:  2.168380523982801 	 ± 0.17046561773821772
	data : 0.11389565467834473
	model : 0.06544532775878906
			 train-loss:  2.1680559277534486 	 ± 0.16615535616060848
	data : 0.11400351524353028
	model : 0.06548223495483399
			 train-loss:  2.193046183813186 	 ± 0.19693453888808252
	data : 0.11404805183410645
	model : 0.06546688079833984
			 train-loss:  2.1981326666745273 	 ± 0.1938134612781419
	data : 0.1141366958618164
	model : 0.0657613754272461
			 train-loss:  2.1957942921182383 	 ± 0.1898703535658003
	data : 0.11366586685180664
	model : 0.06564064025878906
			 train-loss:  2.191870858271917 	 ± 0.18682259818814898
	data : 0.11361098289489746
	model : 0.06529045104980469
			 train-loss:  2.197267656326294 	 ± 0.18494752274029855
	data : 0.1134115219116211
	model : 0.06527204513549804
			 train-loss:  2.1913502858235288 	 ± 0.18375355381993816
	data : 0.11344165802001953
	model : 0.06531801223754882
			 train-loss:  2.1867748543068215 	 ± 0.18182161106118205
	data : 0.11345400810241699
	model : 0.06504158973693848
			 train-loss:  2.178028162036623 	 ± 0.1842390866826152
	data : 0.11386957168579101
	model : 0.06508164405822754
			 train-loss:  2.182936294325467 	 ± 0.18288813586156652
	data : 0.11400041580200196
	model : 0.06488575935363769
			 train-loss:  2.1921232899030048 	 ± 0.18649600614704234
	data : 0.11416096687316894
	model : 0.06488666534423829
			 train-loss:  2.183960714647847 	 ± 0.1888322783390595
	data : 0.11401519775390626
	model : 0.06481809616088867
			 train-loss:  2.203713908791542 	 ± 0.21596105544980557
	data : 0.11390724182128906
	model : 0.06474161148071289
			 train-loss:  2.201677828124075 	 ± 0.21297541887805713
	data : 0.11389656066894531
	model : 0.06475687026977539
			 train-loss:  2.2076810668496525 	 ± 0.21263522178615324
	data : 0.11401362419128418
	model : 0.06483559608459473
			 train-loss:  2.2073722907475064 	 ± 0.20958329638242224
	data : 0.11406664848327637
	model : 0.0652270793914795
			 train-loss:  2.211290968788995 	 ± 0.20794825355624078
	data : 0.11391644477844239
	model : 0.06525254249572754
			 train-loss:  2.2011332866307853 	 ± 0.21398174415369373
	data : 0.11391944885253906
	model : 0.06533164978027343
			 train-loss:  2.2105505874282434 	 ± 0.21877979809729237
	data : 0.11387515068054199
	model : 0.06545886993408204
			 train-loss:  2.2147394785514245 	 ± 0.21749501455013293
	data : 0.11348123550415039
	model : 0.0654078483581543
			 train-loss:  2.212051996588707 	 ± 0.2154139251139671
	data : 0.11340241432189942
	model : 0.06499543190002441
			 train-loss:  2.2081687595786117 	 ± 0.2141834632995065
	data : 0.11374959945678711
	model : 0.06504907608032226
			 train-loss:  2.216199610914503 	 ± 0.21777644987268838
	data : 0.11381845474243164
	model : 0.06499853134155273
			 train-loss:  2.222446084022522 	 ± 0.21900322429837737
	data : 0.11387419700622559
	model : 0.06487975120544434
			 train-loss:  2.2209728278897027 	 ± 0.21671568523151688
	data : 0.11405973434448242
	model : 0.06488056182861328
			 train-loss:  2.216761755943298 	 ± 0.2161070657106451
	data : 0.11417970657348633
	model : 0.06491937637329101
			 train-loss:  2.2173116388528245 	 ± 0.21377699583740853
	data : 0.1140709400177002
	model : 0.06487631797790527
			 train-loss:  2.215478625703365 	 ± 0.2118556295811636
	data : 0.11405925750732422
	model : 0.06486349105834961
			 train-loss:  2.216938408712546 	 ± 0.2098759269475623
	data : 0.11392860412597657
	model : 0.06484570503234863
			 train-loss:  2.219743322352974 	 ± 0.2086303185701726
	data : 0.11402816772460937
	model : 0.06485347747802735
			 train-loss:  2.2243560910224915 	 ± 0.20904229652221964
	data : 0.11403255462646485
	model : 0.06482453346252441
			 train-loss:  2.2177695269678153 	 ± 0.2121579273579729
	data : 0.11405887603759765
	model : 0.06485638618469239
			 train-loss:  2.2195851917450247 	 ± 0.21050776427782975
	data : 0.11413178443908692
	model : 0.06485972404479981
			 train-loss:  2.220717783244151 	 ± 0.20867227509841832
	data : 0.11418581008911133
	model : 0.06483268737792969
			 train-loss:  2.220582655182591 	 ± 0.20673343607243203
	data : 0.1141904354095459
	model : 0.06478309631347656
			 train-loss:  2.2192549683830953 	 ± 0.20507763088966682
	data : 0.11403927803039551
	model : 0.06477270126342774
			 train-loss:  2.220044453229223 	 ± 0.2033226513053597
	data : 0.11400375366210938
	model : 0.06471781730651856
			 train-loss:  2.2164938679912636 	 ± 0.2032752034115019
	data : 0.11391496658325195
	model : 0.06477570533752441
			 train-loss:  2.2150204859930893 	 ± 0.20182199904746015
	data : 0.1140444278717041
	model : 0.06481771469116211
			 train-loss:  2.218997387562768 	 ± 0.20238344218249754
	data : 0.11399073600769043
	model : 0.06488642692565919
			 train-loss:  2.214387450615565 	 ± 0.2037897109994755
	data : 0.1140059471130371
	model : 0.06491408348083497
			 train-loss:  2.2087153997577604 	 ± 0.20683266728269326
	data : 0.11394429206848145
	model : 0.06491799354553222
			 train-loss:  2.205182775374382 	 ± 0.20700483518080567
	data : 0.11391944885253906
	model : 0.06485228538513184
			 train-loss:  2.2003612348011563 	 ± 0.20883523858206363
	data : 0.11382040977478028
	model : 0.06491217613220215
			 train-loss:  2.2017584200948477 	 ± 0.20749385751729182
	data : 0.11391758918762207
	model : 0.06488523483276368
			 train-loss:  2.2009370968892026 	 ± 0.20599638072981258
	data : 0.11401557922363281
	model : 0.06490201950073242
			 train-loss:  2.199381360501954 	 ± 0.20481426290803886
	data : 0.11406798362731933
	model : 0.06492676734924316
			 train-loss:  2.1975869758805233 	 ± 0.20380207907919876
	data : 0.11404023170471192
	model : 0.06493773460388183
			 train-loss:  2.1993331330664017 	 ± 0.2028022725118042
	data : 0.11410512924194335
	model : 0.06484065055847169
			 train-loss:  2.19915873589723 	 ± 0.20133246360839063
	data : 0.11386747360229492
	model : 0.06480064392089843
			 train-loss:  2.200798557485853 	 ± 0.2003527761571143
	data : 0.11380324363708497
	model : 0.0647918701171875
			 train-loss:  2.2039820627427438 	 ± 0.20071196976881892
	data : 0.11392393112182617
	model : 0.06480417251586915
			 train-loss:  2.2058066146241293 	 ± 0.199905315297885
	data : 0.1140380859375
	model : 0.06482019424438476
			 train-loss:  2.2057821375049955 	 ± 0.1985314879651489
	data : 0.11407904624938965
	model : 0.06484737396240234
			 train-loss:  2.209098817528905 	 ± 0.1992113116255068
	data : 0.11424722671508789
	model : 0.06489582061767578
			 train-loss:  2.204200094540914 	 ± 0.2023161521531622
	data : 0.11427640914916992
	model : 0.06486773490905762
			 train-loss:  2.2022834749598252 	 ± 0.20166495994140166
	data : 0.11409087181091308
	model : 0.0648167610168457
			 train-loss:  2.1978585302055658 	 ± 0.20403107789474417
	data : 0.11393947601318359
	model : 0.06480164527893066
			 train-loss:  2.194973361797822 	 ± 0.20429376608387953
	data : 0.11393356323242188
	model : 0.0648427963256836
			 train-loss:  2.192395011080971 	 ± 0.20426985924320046
	data : 0.11396112442016601
	model : 0.06486363410949707
			 train-loss:  2.191891172528267 	 ± 0.20303854957245082
	data : 0.11409230232238769
	model : 0.06489725112915039
			 train-loss:  2.1899082748978227 	 ± 0.2025592676846979
	data : 0.11425518989562988
	model : 0.06497912406921387
			 train-loss:  2.1923225303975546 	 ± 0.20248952303052187
	data : 0.11433138847351074
	model : 0.06495428085327148
			 train-loss:  2.192686273390988 	 ± 0.2012929611007317
	data : 0.11424651145935058
	model : 0.06490325927734375
			 train-loss:  2.1919675242333185 	 ± 0.20019831838031996
	data : 0.11426515579223633
	model : 0.06487693786621093
			 train-loss:  2.1926101572373335 	 ± 0.1991043314553427
	data : 0.11403603553771972
	model : 0.06486811637878417
			 train-loss:  2.194156009097432 	 ± 0.19845577851246593
	data : 0.11392350196838379
	model : 0.0648463249206543
			 train-loss:  2.1926470049496354 	 ± 0.19780755398134584
	data : 0.11397600173950195
	model : 0.06489801406860352
			 train-loss:  2.1968352036042647 	 ± 0.20052247178746546
	data : 0.1140279769897461
	model : 0.0649104118347168
			 train-loss:  2.193808661418015 	 ± 0.2014039436153062
	data : 0.11397204399108887
	model : 0.06495704650878906
			 train-loss:  2.1951752410994634 	 ± 0.20069642117288106
	data : 0.11409573554992676
	model : 0.06492247581481933
			 train-loss:  2.1932795899254933 	 ± 0.20039920387561355
	data : 0.11416354179382324
	model : 0.06492295265197753
			 train-loss:  2.1901963715967923 	 ± 0.20146560081792378
	data : 0.11416926383972167
	model : 0.06488299369812012
			 train-loss:  2.19200607269041 	 ± 0.20112994727397124
	data : 0.11405735015869141
	model : 0.06487908363342285
			 train-loss:  2.1901536926310112 	 ± 0.20085321484045077
	data : 0.11410069465637207
	model : 0.06485114097595215
			 train-loss:  2.1879993777526052 	 ± 0.20088210960084565
	data : 0.11408076286315919
	model : 0.06494536399841308
			 train-loss:  2.187239839384953 	 ± 0.19997019050221995
	data : 0.1140756607055664
	model : 0.06492562294006347
			 train-loss:  2.186794142133182 	 ± 0.19898466999228706
	data : 0.11399927139282226
	model : 0.06496062278747558
			 train-loss:  2.183710795276019 	 ± 0.20028242811600386
	data : 0.11401352882385254
	model : 0.06498818397521973
			 train-loss:  2.1832740583805124 	 ± 0.1993152305067156
	data : 0.11397271156311035
	model : 0.06499814987182617
			 train-loss:  2.185646592378616 	 ± 0.19971619448725114
	data : 0.1138498306274414
	model : 0.06493110656738281
			 train-loss:  2.1850612883520597 	 ± 0.19881121699141444
	data : 0.11380534172058106
	model : 0.0649439811706543
			 train-loss:  2.1822154405070284 	 ± 0.19989090780654847
	data : 0.11389274597167968
	model : 0.0649564266204834
			 train-loss:  2.1837237552531716 	 ± 0.1995006276098198
	data : 0.11403455734252929
	model : 0.06494374275207519
			 train-loss:  2.1835211721750407 	 ± 0.19854981845215988
	data : 0.1140200138092041
	model : 0.06497664451599121
			 train-loss:  2.182500074023292 	 ± 0.19787626741991146
	data : 0.11421427726745606
	model : 0.06496329307556152
			 train-loss:  2.184739220817134 	 ± 0.198272733832725
	data : 0.11419811248779296
	model : 0.06492438316345214
			 train-loss:  2.1832783422737476 	 ± 0.1979163863894403
	data : 0.1142235279083252
	model : 0.06487689018249512
			 train-loss:  2.1832184946095503 	 ± 0.19699894864300835
	data : 0.11423687934875489
	model : 0.06484251022338867
			 train-loss:  2.1817980096974505 	 ± 0.1966480705545649
	data : 0.11432366371154785
	model : 0.0647965431213379
			 train-loss:  2.1819228345697574 	 ± 0.19575651289133472
	data : 0.11430330276489258
	model : 0.06482000350952148
			 train-loss:  2.1813484913594015 	 ± 0.19496581062109372
	data : 0.11440386772155761
	model : 0.06488656997680664
			 train-loss:  2.1820129581860135 	 ± 0.19421968416872865
	data : 0.11431937217712403
	model : 0.06488018035888672
			 train-loss:  2.184354041529968 	 ± 0.19493923209000188
	data : 0.1142162799835205
	model : 0.06480441093444825
			 train-loss:  2.1851964226940224 	 ± 0.19428881829630396
	data : 0.11394810676574707
	model : 0.06478753089904785
			 train-loss:  2.18543409679247 	 ± 0.19345888431387542
	data : 0.1139641284942627
	model : 0.06478943824768066
			 train-loss:  2.184852026659867 	 ± 0.1927243152980606
	data : 0.11399664878845214
	model : 0.06478495597839355
			 train-loss:  2.186931137345795 	 ± 0.19320102588853005
	data : 0.11399140357971191
	model : 0.06483349800109864
			 train-loss:  2.1852782419172385 	 ± 0.19320962808064365
	data : 0.11418561935424805
	model : 0.06495709419250488
			 train-loss:  2.1839822280306778 	 ± 0.19291050301187646
	data : 0.11443963050842285
	model : 0.06498947143554687
			 train-loss:  2.184205518166224 	 ± 0.1921204695337381
	data : 0.11427931785583496
	model : 0.06492681503295898
			 train-loss:  2.183451329381013 	 ± 0.1915032305398779
	data : 0.11429104804992676
	model : 0.06488008499145508
			 train-loss:  2.190404305692579 	 ± 0.20548112901803367
	data : 0.11426753997802734
	model : 0.06486153602600098
			 train-loss:  2.1916831722104453 	 ± 0.20513106302575462
	data : 0.11409521102905273
	model : 0.06485819816589355
			 train-loss:  2.190733023228184 	 ± 0.2045738266630334
	data : 0.11401948928833008
	model : 0.06488637924194336
			 train-loss:  2.197055694580078 	 ± 0.2155752592625053
	data : 0.11424336433410645
	model : 0.06495833396911621
			 train-loss:  2.1975734744753157 	 ± 0.2147961208133739
	data : 0.11424636840820312
	model : 0.06498799324035645
			 train-loss:  2.1965197972425328 	 ± 0.21427546945556683
	data : 0.1142806053161621
	model : 0.06495981216430664
			 train-loss:  2.1976205557584763 	 ± 0.21379699686617662
	data : 0.11421470642089844
	model : 0.06485605239868164
			 train-loss:  2.1981492615485374 	 ± 0.2130507008750938
	data : 0.1142420768737793
	model : 0.06484975814819335
			 train-loss:  2.20026920575362 	 ± 0.21359117433978506
	data : 0.11420331001281739
	model : 0.06482033729553223
			 train-loss:  2.1974788076095 	 ± 0.21513985737953267
	data : 0.1140787124633789
	model : 0.06483030319213867
			 train-loss:  2.1972745024796687 	 ± 0.21433614047082813
	data : 0.1140639305114746
	model : 0.0648660659790039
			 train-loss:  2.204797255365472 	 ± 0.23035773650907682
	data : 0.11409873962402343
	model : 0.0649533748626709
			 train-loss:  2.204405359367826 	 ± 0.22954108153736713
	data : 0.11409902572631836
	model : 0.06488986015319824
			 train-loss:  2.2030824149096455 	 ± 0.2292015331905516
	data : 0.11409935951232911
	model : 0.06487536430358887
			 train-loss:  2.2022392101147594 	 ± 0.22856739153122757
	data : 0.11410670280456543
	model : 0.06488995552062989
			 train-loss:  2.2018281856592554 	 ± 0.22778211587386388
	data : 0.11415152549743653
	model : 0.06490774154663086
			 train-loss:  2.20141161179197 	 ± 0.2270076879965393
	data : 0.11423516273498535
	model : 0.06497831344604492
			 train-loss:  2.200390237698452 	 ± 0.22650764968059664
	data : 0.11439385414123535
	model : 0.06507396697998047
			 train-loss:  2.2005684307643345 	 ± 0.22570702160815745
	data : 0.11430377960205078
	model : 0.06509933471679688
			 train-loss:  2.2005039850870767 	 ± 0.2249065105931622
	data : 0.11442685127258301
	model : 0.06507291793823242
			 train-loss:  2.2013367196203957 	 ± 0.22433122080887763
	data : 0.11434483528137207
	model : 0.06502742767333984
			 train-loss:  2.2001106905770467 	 ± 0.2240223738319461
	data : 0.11418852806091309
	model : 0.0649223804473877
			 train-loss:  2.1970287933945656 	 ± 0.22626474941251118
	data : 0.11399993896484376
	model : 0.06484875679016114
			 train-loss:  2.1964067237130527 	 ± 0.22560670769902308
	data : 0.11413989067077637
	model : 0.06488027572631835
			 train-loss:  2.1957710156702017 	 ± 0.22496303153308642
	data : 0.11417994499206544
	model : 0.06492629051208496
			 train-loss:  2.194712827805759 	 ± 0.22456085177113078
	data : 0.11428756713867187
	model : 0.06500205993652344
			 train-loss:  2.1922605271274977 	 ± 0.22576730219176996
	data : 0.11435027122497558
	model : 0.06508607864379883
			 train-loss:  2.1896874896631946 	 ± 0.22717531898651963
	data : 0.1144249439239502
	model : 0.06512269973754883
			 train-loss:  2.1959283836682637 	 ± 0.23888895535857244
	data : 0.1143381118774414
	model : 0.06507477760314942
			 train-loss:  2.1970594095078524 	 ± 0.2384992297235103
	data : 0.11402416229248047
	model : 0.06499028205871582
			 train-loss:  2.1968854814767838 	 ± 0.2377230059585408
	data : 0.11391401290893555
	model : 0.06495146751403809
			 train-loss:  2.195263883646797 	 ± 0.23778680090552623
	data : 0.11394844055175782
	model : 0.06495065689086914
			 train-loss:  2.1957337198319373 	 ± 0.23708474715000816
	data : 0.11392297744750976
	model : 0.06493625640869141
			 train-loss:  2.198359361002522 	 ± 0.238554421835272
	data : 0.11396589279174804
	model : 0.06497259140014648
			 train-loss:  2.197337651099914 	 ± 0.23812857675979282
	data : 0.1142043113708496
	model : 0.06503257751464844
			 train-loss:  2.197337373047118 	 ± 0.2373689940628479
	data : 0.1142460823059082
	model : 0.06501502990722656
			 train-loss:  2.1974857374082637 	 ± 0.2366239366165295
	data : 0.11413865089416504
	model : 0.06490421295166016
			 train-loss:  2.1957157505383282 	 ± 0.23692558808671854
	data : 0.1144484519958496
	model : 0.06491670608520508
			 train-loss:  2.195417370647192 	 ± 0.23621400108772875
	data : 0.11437554359436035
	model : 0.0649329662322998
			 train-loss:  2.1928545199566005 	 ± 0.23770023420500913
	data : 0.11444191932678223
	model : 0.06493463516235351
			 train-loss:  2.1957339460467113 	 ± 0.2397654968729192
	data : 0.11446590423583984
	model : 0.06497054100036621
			 train-loss:  2.1968258802144804 	 ± 0.23943259042641724
	data : 0.11511907577514649
	model : 0.06504359245300292
			 train-loss:  2.195240815965141 	 ± 0.23955778106309505
	data : 0.11479110717773437
	model : 0.06503868103027344
			 train-loss:  2.1960758209228515 	 ± 0.2390700129386712
	data : 0.11505274772644043
	model : 0.06503281593322754
			 train-loss:  2.196276914642518 	 ± 0.23836283147059945
	data : 0.11495027542114258
	model : 0.06497454643249512
			 train-loss:  2.195615047466255 	 ± 0.23780104740246277
	data : 0.1148444652557373
	model : 0.06492094993591309
			 train-loss:  2.1945926461900984 	 ± 0.23746010353776156
	data : 0.11416826248168946
	model : 0.0649034023284912
			 train-loss:  2.1948046049422767 	 ± 0.2367724556594253
	data : 0.11425118446350098
	model : 0.0648725986480713
			 train-loss:  2.194170216953053 	 ± 0.23621904612385283
	data : 0.11396903991699218
	model : 0.06488637924194336
			 train-loss:  2.193809489757694 	 ± 0.23557429050896325
	data : 0.11407980918884278
	model : 0.06493382453918457
			 train-loss:  2.1937696046607438 	 ± 0.23488906232110437
	data : 0.11418704986572266
	model : 0.06499128341674805
			 train-loss:  2.196449472725047 	 ± 0.23683159509235022
	data : 0.11438508033752441
	model : 0.0649376392364502
			 train-loss:  2.195598348803904 	 ± 0.23641526166495228
	data : 0.11418719291687011
	model : 0.06488490104675293
			 train-loss:  2.1954806423187256 	 ± 0.2357439348607825
	data : 0.11422605514526367
	model : 0.06480121612548828
			 train-loss:  2.195961982011795 	 ± 0.23515947781942573
	data : 0.11422538757324219
	model : 0.06479458808898926
			 train-loss:  2.194095646594204 	 ± 0.23579778483168476
	data : 0.11421937942504883
	model : 0.06479172706604004
			 train-loss:  2.1934597719921154 	 ± 0.23528663370154365
	data : 0.1142045497894287
	model : 0.06486711502075196
			 train-loss:  2.1932843397449515 	 ± 0.23464016191856257
	data : 0.11444058418273925
	model : 0.06491336822509766
			 train-loss:  2.19456834130817 	 ± 0.23461723941356497
	data : 0.11449599266052246
	model : 0.06495747566223145
			 train-loss:  2.1949615518032517 	 ± 0.23402769545698146
	data : 0.1144139289855957
	model : 0.06491022109985352
			 train-loss:  2.1957243523754917 	 ± 0.2336093991901854
	data : 0.11429548263549805
	model : 0.06478838920593262
			 train-loss:  2.195648013568315 	 ± 0.23297252408971947
	data : 0.11428637504577636
	model : 0.06473398208618164
			 train-loss:  2.1968682239884916 	 ± 0.232924212597655
	data : 0.11413745880126953
	model : 0.06471800804138184
			 train-loss:  2.1970162108137803 	 ± 0.232302508057796
	data : 0.11408629417419433
	model : 0.06472549438476563
			 train-loss:  2.1964395302598194 	 ± 0.231809938215443
	data : 0.11408147811889649
	model : 0.06482176780700684
			 train-loss:  2.1969362758697675 	 ± 0.2312885354612506
	data : 0.1143181324005127
	model : 0.0649169921875
			 train-loss:  2.19545652511272 	 ± 0.23155843512319466
	data : 0.11433353424072265
	model : 0.06494064331054687
			 train-loss:  2.1943008994299267 	 ± 0.23148796367167265
	data : 0.11420459747314453
	model : 0.06496100425720215
			 train-loss:  2.194420388497804 	 ± 0.23088382501704247
	data : 0.11419939994812012
	model : 0.06493401527404785
			 train-loss:  2.1933184300417676 	 ± 0.23077903716219345
	data : 0.11423578262329101
	model : 0.06491761207580567
			 train-loss:  2.1930135233948627 	 ± 0.23021583450348995
	data : 0.1141047477722168
	model : 0.06494603157043458
			 train-loss:  2.1929846285538352 	 ± 0.2296189949228648
	data : 0.11406526565551758
	model : 0.06497397422790527
			 train-loss:  2.1933752832953464 	 ± 0.2290907222703856
	data : 0.11420388221740722
	model : 0.06498808860778808
			 train-loss:  2.195896022136395 	 ± 0.23118416428711458
	data : 0.11417741775512695
	model : 0.06498751640319825
			 train-loss:  2.1945115090632927 	 ± 0.23140273215808113
	data : 0.11417641639709472
	model : 0.06492824554443359
			 train-loss:  2.1967340090553167 	 ± 0.2329024567199617
	data : 0.11404213905334473
	model : 0.06483383178710937
			 train-loss:  2.1974019275771246 	 ± 0.2325026487718158
	data : 0.11403656005859375
	model : 0.06478319168090821
			 train-loss:  2.1988219046712523 	 ± 0.232776868199938
	data : 0.1140322208404541
	model : 0.06479449272155761
			 train-loss:  2.196545584797859 	 ± 0.23440411647017023
	data : 0.11400442123413086
	model : 0.06483988761901856
			 train-loss:  2.196948515835093 	 ± 0.233889719422376
	data : 0.11405038833618164
	model : 0.06487207412719727
			 train-loss:  2.197849494986015 	 ± 0.23365947752099833
	data : 0.11414999961853027
	model : 0.0649223804473877
			 train-loss:  2.1979242621971466 	 ± 0.2330856733657599
	data : 0.11412968635559081
	model : 0.06495556831359864
			 train-loss:  2.1983286685803356 	 ± 0.232585064818543
	data : 0.11404972076416016
	model : 0.06487960815429687
			 train-loss:  2.1966769189369386 	 ± 0.23321341910250937
	data : 0.11420130729675293
	model : 0.06498236656188965
			 train-loss:  2.1987461239388844 	 ± 0.23452549312414503
	data : 0.11414108276367188
	model : 0.06495723724365235
			 train-loss:  2.1993598563659593 	 ± 0.23412408949939023
	data : 0.11422991752624512
	model : 0.06496233940124511
			 train-loss:  2.1988088230674085 	 ± 0.23369512834376807
	data : 0.1144129753112793
	model : 0.06499390602111817
			 train-loss:  2.1981705780805014 	 ± 0.2333170268140569
	data : 0.11461787223815918
	model : 0.06506924629211426
			 train-loss:  2.197317939145224 	 ± 0.2330870085326742
	data : 0.11453413963317871
	model : 0.06496620178222656
			 train-loss:  2.19762290993008 	 ± 0.2325760069711916
	data : 0.1144824504852295
	model : 0.06500139236450195
			 train-loss:  2.197243476251386 	 ± 0.23209228254958447
	data : 0.11432690620422363
	model : 0.06499319076538086
			 train-loss:  2.1963991922951642 	 ± 0.23187291471686147
	data : 0.1140749454498291
	model : 0.06489949226379395
			 train-loss:  2.1940599936191165 	 ± 0.23383608399802727
	data : 0.11399955749511718
	model : 0.06488285064697266
			 train-loss:  2.1941709529521853 	 ± 0.23329729220535136
	data : 0.11411848068237304
	model : 0.06486520767211915
			 train-loss:  2.195752449609615 	 ± 0.23390893550366018
	data : 0.1143117904663086
	model : 0.064841890335083
			 train-loss:  2.1986447885838523 	 ± 0.23720925280101524
	data : 0.1144679069519043
	model : 0.06481947898864746
			 train-loss:  2.197396813182656 	 ± 0.23737751265447998
	data : 0.11470375061035157
	model : 0.06486616134643555
			 train-loss:  2.197244206519976 	 ± 0.23684565287548343
	data : 0.11477322578430176
	model : 0.06490602493286132
			 train-loss:  2.196686998280612 	 ± 0.23645058137606834
	data : 0.11461553573608399
	model : 0.06485342979431152
			 train-loss:  2.195753200561213 	 ± 0.2363212459492286
	data : 0.11442728042602539
	model : 0.06485195159912109
			 train-loss:  2.194724516825633 	 ± 0.23628378059063046
	data : 0.11441950798034668
	model : 0.06478919982910156
			 train-loss:  2.1961110418687486 	 ± 0.23665681986827272
	data : 0.11432528495788574
	model : 0.06470975875854493
			 train-loss:  2.1948965436645915 	 ± 0.23682345303364966
	data : 0.11432061195373536
	model : 0.06462416648864747
			 train-loss:  2.1949944241841632 	 ± 0.23630113361427454
	data : 0.114475679397583
	model : 0.0645115852355957
			 train-loss:  2.1938055637663445 	 ± 0.23645119263388492
	data : 0.11466069221496582
	model : 0.06435141563415528
			 train-loss:  2.193020146323721 	 ± 0.236225074064343
	data : 0.11475820541381836
	model : 0.06423254013061523
			 train-loss:  2.192778195205488 	 ± 0.2357346546606265
	data : 0.11491231918334961
	model : 0.0641054630279541
			 train-loss:  2.1934783573233925 	 ± 0.23545685760537674
	data : 0.11478090286254883
	model : 0.06387009620666503
			 train-loss:  2.192162913861482 	 ± 0.2357862342579139
	data : 0.11477527618408204
	model : 0.06380572319030761
			 train-loss:  2.191251220641198 	 ± 0.23568124462806628
	data : 0.11489462852478027
	model : 0.06375370025634766
			 train-loss:  2.1908236346368133 	 ± 0.23526253813564915
	data : 0.11502132415771485
	model : 0.06379985809326172
			 train-loss:  2.1910610418974588 	 ± 0.2347849885226472
	data : 0.11504297256469727
	model : 0.06385846138000488
			 train-loss:  2.190915131670797 	 ± 0.2342933604420537
	data : 0.1153182029724121
	model : 0.06393795013427735
			 train-loss:  2.1899092095963497 	 ± 0.2343001699507374
	data : 0.11532034873962402
	model : 0.06395497322082519
			 train-loss:  2.1907226554418013 	 ± 0.23413554873074469
	data : 0.11507716178894042
	model : 0.06389927864074707
			 train-loss:  2.191205152479405 	 ± 0.23375861708552295
	data : 0.11502175331115723
	model : 0.06382694244384765
			 train-loss:  2.192788251307832 	 ± 0.23453670947760683
	data : 0.11496982574462891
	model : 0.06378097534179687
			 train-loss:  2.1939037544457984 	 ± 0.23467736569174033
	data : 0.11490030288696289
	model : 0.06379470825195313
			 train-loss:  2.193382576107979 	 ± 0.2343265072770112
	data : 0.1148759365081787
	model : 0.06382164955139161
			 train-loss:  2.1930161452392323 	 ± 0.23390874157038027
	data : 0.11503114700317382
	model : 0.06393108367919922
			 train-loss:  2.1915663106382386 	 ± 0.23450756550330543
	data : 0.11501693725585938
	model : 0.0639538288116455
			 train-loss:  2.1922511603116006 	 ± 0.23426691820953122
	data : 0.11481585502624511
	model : 0.06392574310302734
			 train-loss:  2.1931446810237696 	 ± 0.23420092451223684
	data : 0.11475329399108887
	model : 0.06393756866455078
			 train-loss:  2.1927004181608862 	 ± 0.23382547626193387
	data : 0.11487598419189453
	model : 0.06393656730651856
			 train-loss:  2.1918383134089834 	 ± 0.23373957711097706
	data : 0.11496667861938477
	model : 0.06390299797058105
			 train-loss:  2.1916972205706453 	 ± 0.23327643692465097
	data : 0.11491107940673828
	model : 0.06390881538391113
			 train-loss:  2.19114973420097 	 ± 0.23296460059724314
	data : 0.1151059627532959
	model : 0.06394314765930176
			 train-loss:  2.1921792791550416 	 ± 0.23306096636057302
	data : 0.11526789665222167
	model : 0.06387982368469239
			 train-loss:  2.1924366030693054 	 ± 0.2326298176682787
	data : 0.11501827239990234
	model : 0.06381902694702149
			 train-loss:  2.192003170807523 	 ± 0.23226707437331454
	data : 0.11482067108154297
	model : 0.06382007598876953
			 train-loss:  2.191701598110653 	 ± 0.2318550020298849
	data : 0.11482710838317871
	model : 0.06387081146240234
			 train-loss:  2.1916779581265957 	 ± 0.23139664119259326
	data : 0.11488747596740723
	model : 0.06389265060424805
			 train-loss:  2.192243149430733 	 ± 0.23111559755917013
	data : 0.11478886604309083
	model : 0.06393499374389648
			 train-loss:  2.1931121064167396 	 ± 0.23107735291343137
	data : 0.11502833366394043
	model : 0.06395473480224609
			 train-loss:  2.190493129659444 	 ± 0.23438689599060245
	data : 0.11485214233398437
	model : 0.055500030517578125
#epoch  71    val-loss:  2.4068297210492586  train-loss:  2.190493129659444  lr:  1.9073486328125e-08
			 train-loss:  2.434627056121826 	 ± 0.0
	data : 5.6068243980407715
	model : 0.07389473915100098
			 train-loss:  2.372598648071289 	 ± 0.06202840805053711
	data : 2.867644190788269
	model : 0.0708310604095459
			 train-loss:  2.318632205327352 	 ± 0.09159568487866622
	data : 1.950015942255656
	model : 0.06870333353678386
			 train-loss:  2.284773349761963 	 ± 0.09864883890288605
	data : 1.491199553012848
	model : 0.06770163774490356
			 train-loss:  2.220971608161926 	 ± 0.1551384016235227
	data : 1.2156866550445558
	model : 0.06701650619506835
			 train-loss:  2.1978190541267395 	 ± 0.15078729004908117
	data : 0.11699862480163574
	model : 0.06512742042541504
			 train-loss:  2.1676426274435863 	 ± 0.1579632613667545
	data : 0.11404561996459961
	model : 0.06453447341918946
			 train-loss:  2.2318992912769318 	 ± 0.22524603188038647
	data : 0.11396384239196777
	model : 0.0646444320678711
			 train-loss:  2.2122723526424832 	 ± 0.21949988683227406
	data : 0.11380696296691895
	model : 0.06467509269714355
			 train-loss:  2.1848214864730835 	 ± 0.22392885202335058
	data : 0.11389284133911133
	model : 0.06472988128662109
			 train-loss:  2.1924260529604824 	 ± 0.2148577849195087
	data : 0.11394619941711426
	model : 0.06486587524414063
			 train-loss:  2.221714655558268 	 ± 0.22749268872995146
	data : 0.11404085159301758
	model : 0.06483478546142578
			 train-loss:  2.2304425606360803 	 ± 0.2206491451161548
	data : 0.11391105651855468
	model : 0.06478271484375
			 train-loss:  2.223973717008318 	 ± 0.2138982650067565
	data : 0.1139441967010498
	model : 0.06479024887084961
			 train-loss:  2.2244193077087404 	 ± 0.20665208193899115
	data : 0.11395039558410644
	model : 0.06489558219909668
			 train-loss:  2.212555944919586 	 ± 0.20529760333262126
	data : 0.11397819519042969
	model : 0.06486873626708985
			 train-loss:  2.199546722804799 	 ± 0.20585359401548445
	data : 0.11393280029296875
	model : 0.06495833396911621
			 train-loss:  2.1990069217152066 	 ± 0.20006611417656103
	data : 0.11403055191040039
	model : 0.06496930122375488
			 train-loss:  2.198329342039008 	 ± 0.1947512742763765
	data : 0.11393351554870605
	model : 0.06491451263427735
			 train-loss:  2.1920377790927885 	 ± 0.1917908950197515
	data : 0.11404685974121094
	model : 0.06487350463867188
			 train-loss:  2.187840637706575 	 ± 0.1881075764267695
	data : 0.11402378082275391
	model : 0.06481490135192872
			 train-loss:  2.2123499512672424 	 ± 0.21538549446321467
	data : 0.11407628059387206
	model : 0.06477937698364258
			 train-loss:  2.216462835021641 	 ± 0.21153265224555115
	data : 0.114082670211792
	model : 0.06481246948242188
			 train-loss:  2.207584018508593 	 ± 0.21141147432731788
	data : 0.11415185928344726
	model : 0.06491127014160156
			 train-loss:  2.2142831754684447 	 ± 0.20972388493804336
	data : 0.1139979362487793
	model : 0.06491703987121582
			 train-loss:  2.211263587841621 	 ± 0.206204653660376
	data : 0.11412091255187988
	model : 0.06496572494506836
			 train-loss:  2.212794635030958 	 ± 0.2025005625076104
	data : 0.11401476860046386
	model : 0.06490550041198731
			 train-loss:  2.2306067645549774 	 ± 0.21933605180616506
	data : 0.11400980949401855
	model : 0.06489348411560059
			 train-loss:  2.238606128199347 	 ± 0.21963859297602398
	data : 0.11400322914123535
	model : 0.06486568450927735
			 train-loss:  2.23316083351771 	 ± 0.2179287963179654
	data : 0.1140932559967041
	model : 0.06486401557922364
			 train-loss:  2.2381254511494793 	 ± 0.21610264259191736
	data : 0.11410307884216309
	model : 0.06487483978271484
			 train-loss:  2.2274623438715935 	 ± 0.2208296259390828
	data : 0.11422500610351563
	model : 0.06495943069458007
			 train-loss:  2.2114335587530425 	 ± 0.23560449266864408
	data : 0.11428632736206054
	model : 0.06499319076538086
			 train-loss:  2.2113958281629227 	 ± 0.232113963967927
	data : 0.11428074836730957
	model : 0.06495661735534668
			 train-loss:  2.2018223898751397 	 ± 0.23548604415525828
	data : 0.11406803131103516
	model : 0.06495842933654786
			 train-loss:  2.207987176047431 	 ± 0.23503926836744432
	data : 0.11407003402709961
	model : 0.06492075920104981
			 train-loss:  2.2060792639448836 	 ± 0.23212375121977624
	data : 0.11382975578308105
	model : 0.06513943672180175
			 train-loss:  2.2077538339715255 	 ± 0.22927550718255196
	data : 0.11374692916870117
	model : 0.06512928009033203
			 train-loss:  2.2157567831186147 	 ± 0.2316315461903332
	data : 0.11380863189697266
	model : 0.06516757011413574
			 train-loss:  2.2152204275131226 	 ± 0.2287423512969287
	data : 0.11399083137512207
	model : 0.06516509056091309
			 train-loss:  2.2185546246970573 	 ± 0.22691753091433192
	data : 0.11403026580810546
	model : 0.06521067619323731
			 train-loss:  2.21883872009459 	 ± 0.2242072373753346
	data : 0.11425771713256835
	model : 0.06493926048278809
			 train-loss:  2.2157200547151787 	 ± 0.2225046888156264
	data : 0.11420345306396484
	model : 0.06494655609130859
			 train-loss:  2.2147104902700945 	 ± 0.22006129499270377
	data : 0.11404099464416503
	model : 0.06488256454467774
			 train-loss:  2.2161773575676813 	 ± 0.2178198645014412
	data : 0.11400980949401855
	model : 0.06489768028259277
			 train-loss:  2.218964162080184 	 ± 0.2162488200547531
	data : 0.11385507583618164
	model : 0.06485705375671387
			 train-loss:  2.2213768299589765 	 ± 0.21456082454482003
	data : 0.11380767822265625
	model : 0.06485681533813477
			 train-loss:  2.218508635958036 	 ± 0.2132226631687606
	data : 0.11383166313171386
	model : 0.06483902931213378
			 train-loss:  2.214809612352021 	 ± 0.21258608266317383
	data : 0.11391010284423828
	model : 0.06495518684387207
			 train-loss:  2.2234672355651854 	 ± 0.21900171971148436
	data : 0.11396584510803223
	model : 0.0649198055267334
			 train-loss:  2.2221686466067445 	 ± 0.21703834538322042
	data : 0.11391758918762207
	model : 0.0648676872253418
			 train-loss:  2.221190768938798 	 ± 0.21505472347995822
	data : 0.11387286186218262
	model : 0.06486101150512695
			 train-loss:  2.227832060939861 	 ± 0.21833339582235833
	data : 0.11394295692443848
	model : 0.06483931541442871
			 train-loss:  2.229676295209814 	 ± 0.2167186363434142
	data : 0.1140671730041504
	model : 0.06474895477294922
			 train-loss:  2.2281195857308127 	 ± 0.21504390964534847
	data : 0.11420817375183105
	model : 0.0648183822631836
			 train-loss:  2.2195750709090913 	 ± 0.222336636193479
	data : 0.11429615020751953
	model : 0.06492018699645996
			 train-loss:  2.215155793909441 	 ± 0.22284524792589713
	data : 0.11441802978515625
	model : 0.06497197151184082
			 train-loss:  2.218604322137504 	 ± 0.22244473768597658
	data : 0.11438546180725098
	model : 0.06494660377502441
			 train-loss:  2.2207829144041416 	 ± 0.22117475539870296
	data : 0.11413784027099609
	model : 0.06490240097045899
			 train-loss:  2.2168468217055004 	 ± 0.22139793269914188
	data : 0.1139129638671875
	model : 0.06485209465026856
			 train-loss:  2.2112525271587686 	 ± 0.22381075573737744
	data : 0.11402406692504882
	model : 0.06479949951171875
			 train-loss:  2.2086314828165117 	 ± 0.222940335529537
	data : 0.11402587890625
	model : 0.06476359367370606
			 train-loss:  2.215301791826884 	 ± 0.22731483390213547
	data : 0.1140047550201416
	model : 0.06476445198059082
			 train-loss:  2.2132457848638296 	 ± 0.226121581360626
	data : 0.11422414779663086
	model : 0.06484112739562989
			 train-loss:  2.20815293605511 	 ± 0.22804454440177974
	data : 0.11428070068359375
	model : 0.06483340263366699
			 train-loss:  2.2051763209429653 	 ± 0.22757918515447312
	data : 0.11428146362304688
	model : 0.06486096382141113
			 train-loss:  2.2036115340332487 	 ± 0.2262318968704027
	data : 0.11403946876525879
	model : 0.06480989456176758
			 train-loss:  2.2003059527453255 	 ± 0.22618645278658678
	data : 0.11391334533691407
	model : 0.06484799385070801
			 train-loss:  2.2018759596175044 	 ± 0.22491436659932032
	data : 0.11383047103881835
	model : 0.06483950614929199
			 train-loss:  2.1995285306658063 	 ± 0.22415179583906839
	data : 0.11392297744750976
	model : 0.06490073204040528
			 train-loss:  2.2055378094525406 	 ± 0.22817573364666988
	data : 0.11389975547790528
	model : 0.06487360000610351
			 train-loss:  2.203843434651693 	 ± 0.22703498924192467
	data : 0.1140838623046875
	model : 0.06493511199951171
			 train-loss:  2.2062870704964417 	 ± 0.2264259915488071
	data : 0.1142653465270996
	model : 0.06489930152893067
			 train-loss:  2.2078108948630257 	 ± 0.2252674365039123
	data : 0.11424570083618164
	model : 0.06484651565551758
			 train-loss:  2.2057646814982097 	 ± 0.22445188698768437
	data : 0.11413345336914063
	model : 0.06478314399719239
			 train-loss:  2.2048606778445996 	 ± 0.2231077414607251
	data : 0.11404352188110352
	model : 0.06478738784790039
			 train-loss:  2.2058686343106357 	 ± 0.22182836408636813
	data : 0.11411657333374023
	model : 0.06479310989379883
			 train-loss:  2.204521533770439 	 ± 0.22071856273493903
	data : 0.11414599418640137
	model : 0.06481919288635254
			 train-loss:  2.2030101456219637 	 ± 0.21972298922295688
	data : 0.114178466796875
	model : 0.06488022804260254
			 train-loss:  2.2034964472055436 	 ± 0.21838818018002581
	data : 0.1141632080078125
	model : 0.0649148941040039
			 train-loss:  2.206395673163143 	 ± 0.21857957535240738
	data : 0.11423768997192382
	model : 0.06489801406860352
			 train-loss:  2.204777089560904 	 ± 0.21773054192673721
	data : 0.11409087181091308
	model : 0.06482305526733398
			 train-loss:  2.2063765841794303 	 ± 0.21689908385010956
	data : 0.11388249397277832
	model : 0.06482348442077637
			 train-loss:  2.2089747531073436 	 ± 0.2168996092853194
	data : 0.11390624046325684
	model : 0.06481413841247559
			 train-loss:  2.2121802470263314 	 ± 0.21761222576793
	data : 0.11395010948181153
	model : 0.06481165885925293
			 train-loss:  2.2114464100017104 	 ± 0.21644910315222168
	data : 0.11385726928710938
	model : 0.06483926773071289
			 train-loss:  2.2134900613762865 	 ± 0.2160344555539193
	data : 0.11395549774169922
	model : 0.06490774154663086
			 train-loss:  2.213332170789892 	 ± 0.21480852839308298
	data : 0.11409206390380859
	model : 0.06490802764892578
			 train-loss:  2.2164380630750333 	 ± 0.21557630762583996
	data : 0.11403818130493164
	model : 0.06484408378601074
			 train-loss:  2.2123063007990518 	 ± 0.21789019260705167
	data : 0.11397714614868164
	model : 0.06484460830688477
			 train-loss:  2.2090115874678222 	 ± 0.21893236966672902
	data : 0.1140291690826416
	model : 0.06487250328063965
			 train-loss:  2.2051481526830923 	 ± 0.22083628976810174
	data : 0.11406221389770507
	model : 0.06490349769592285
			 train-loss:  2.2043051873483965 	 ± 0.21979455621588007
	data : 0.11403579711914062
	model : 0.0648836612701416
			 train-loss:  2.202088418159079 	 ± 0.2196650236889283
	data : 0.11416020393371581
	model : 0.06498804092407226
			 train-loss:  2.198486822529843 	 ± 0.2212783753495186
	data : 0.11426262855529785
	model : 0.06497611999511718
			 train-loss:  2.196641763051351 	 ± 0.2208562421858507
	data : 0.11414809226989746
	model : 0.06494050025939942
			 train-loss:  2.1974830725758348 	 ± 0.2198694341230667
	data : 0.11417245864868164
	model : 0.06486282348632813
			 train-loss:  2.1932638080752627 	 ± 0.22265688685636095
	data : 0.11426310539245606
	model : 0.06491999626159668
			 train-loss:  2.1931930214467674 	 ± 0.22153061129180357
	data : 0.11440200805664062
	model : 0.06492180824279785
			 train-loss:  2.195114541053772 	 ± 0.2212477912756128
	data : 0.1144068717956543
	model : 0.06491093635559082
			 train-loss:  2.1923254043749063 	 ± 0.22190956317117494
	data : 0.11458625793457031
	model : 0.0649907112121582
			 train-loss:  2.1905041827875027 	 ± 0.2215763367270025
	data : 0.11466383934020996
	model : 0.06506476402282715
			 train-loss:  2.1890665709393695 	 ± 0.22097560574124925
	data : 0.11471295356750488
	model : 0.06507325172424316
			 train-loss:  2.1882014607007685 	 ± 0.2200858555706336
	data : 0.11455121040344238
	model : 0.06505823135375977
			 train-loss:  2.188790354274568 	 ± 0.21911763585359903
	data : 0.11447744369506836
	model : 0.0650167465209961
			 train-loss:  2.187984733086712 	 ± 0.21823780044820174
	data : 0.11432037353515626
	model : 0.06485462188720703
			 train-loss:  2.1893041523817542 	 ± 0.2176399559668841
	data : 0.11419730186462403
	model : 0.06481595039367676
			 train-loss:  2.1920507517125873 	 ± 0.2184851325620014
	data : 0.1141690731048584
	model : 0.06477565765380859
			 train-loss:  2.190146190310837 	 ± 0.21837940541073905
	data : 0.11417875289916993
	model : 0.06477704048156738
			 train-loss:  2.1909964973276312 	 ± 0.21756569722888947
	data : 0.11435275077819824
	model : 0.06485586166381836
			 train-loss:  2.1913787094322412 	 ± 0.2166205487655391
	data : 0.11452007293701172
	model : 0.06495332717895508
			 train-loss:  2.1959612582411085 	 ± 0.22098974271096739
	data : 0.1145939826965332
	model : 0.06495413780212403
			 train-loss:  2.196782112121582 	 ± 0.22018117764115938
	data : 0.11449031829833985
	model : 0.06493492126464843
			 train-loss:  2.196408702616106 	 ± 0.21924927851720696
	data : 0.11424775123596191
	model : 0.06485800743103028
			 train-loss:  2.1991022089253303 	 ± 0.2201801791287418
	data : 0.11410365104675294
	model : 0.064766263961792
			 train-loss:  2.199749739005648 	 ± 0.21933901871869213
	data : 0.11404428482055665
	model : 0.06477928161621094
			 train-loss:  2.1985137360727687 	 ± 0.2188049942373992
	data : 0.1139531135559082
	model : 0.06482329368591308
			 train-loss:  2.1987161838402183 	 ± 0.21788688606683107
	data : 0.11412382125854492
	model : 0.06486692428588867
			 train-loss:  2.1980819381585643 	 ± 0.21707882364956782
	data : 0.11440873146057129
	model : 0.06492390632629394
			 train-loss:  2.198499894142151 	 ± 0.21622051241823978
	data : 0.11444158554077148
	model : 0.06498804092407226
			 train-loss:  2.197558875911492 	 ± 0.2155717916725828
	data : 0.11437177658081055
	model : 0.06493558883666992
			 train-loss:  2.200270646908244 	 ± 0.21674889583505277
	data : 0.11429729461669921
	model : 0.06486325263977051
			 train-loss:  2.199314532241201 	 ± 0.21612417469091041
	data : 0.11421880722045899
	model : 0.0648618221282959
			 train-loss:  2.199068734722753 	 ± 0.21526820319784823
	data : 0.11423201560974121
	model : 0.06485276222229004
			 train-loss:  2.1988552112579347 	 ± 0.21441858491056529
	data : 0.11417627334594727
	model : 0.06489253044128418
			 train-loss:  2.1982049733873397 	 ± 0.21368972167185893
	data : 0.11423635482788086
	model : 0.06497774124145508
			 train-loss:  2.197639046691534 	 ± 0.21294153664444573
	data : 0.11434702873229981
	model : 0.06508111953735352
			 train-loss:  2.1998443119227886 	 ± 0.21355906207570083
	data : 0.1143531322479248
	model : 0.06509361267089844
			 train-loss:  2.198779958163121 	 ± 0.21307024981179545
	data : 0.11429977416992188
	model : 0.06511983871459961
			 train-loss:  2.197341152337881 	 ± 0.212877334937591
	data : 0.11425247192382812
	model : 0.06504182815551758
			 train-loss:  2.1966557047749293 	 ± 0.2122072318596727
	data : 0.1142430305480957
	model : 0.06497101783752442
			 train-loss:  2.195681566541845 	 ± 0.21169570203429736
	data : 0.11426024436950684
	model : 0.06494193077087403
			 train-loss:  2.1958937644958496 	 ± 0.21091244282063845
	data : 0.11439085006713867
	model : 0.06491899490356445
			 train-loss:  2.1966312429798185 	 ± 0.21029603719727916
	data : 0.11440100669860839
	model : 0.06488556861877441
			 train-loss:  2.1959399770807337 	 ± 0.20966846840958764
	data : 0.11454620361328124
	model : 0.06493406295776367
			 train-loss:  2.1942770542467342 	 ± 0.20978785033030692
	data : 0.11458630561828613
	model : 0.06498174667358399
			 train-loss:  2.192699757805706 	 ± 0.209828606023752
	data : 0.114558744430542
	model : 0.06499176025390625
			 train-loss:  2.191657030064127 	 ± 0.20942291571375526
	data : 0.11437501907348632
	model : 0.06495375633239746
			 train-loss:  2.190527012022279 	 ± 0.20909005375191672
	data : 0.11418085098266602
	model : 0.06496315002441407
			 train-loss:  2.189636949130467 	 ± 0.20860606885626407
	data : 0.11409993171691894
	model : 0.06498045921325683
			 train-loss:  2.1905127062019725 	 ± 0.20812313043928135
	data : 0.11441850662231445
	model : 0.06498632431030274
			 train-loss:  2.1906369152203413 	 ± 0.20739425226291594
	data : 0.11488151550292969
	model : 0.06496253013610839
			 train-loss:  2.188942099784638 	 ± 0.20765228324223517
	data : 0.11500692367553711
	model : 0.06501750946044922
			 train-loss:  2.190676731367906 	 ± 0.20796708717741497
	data : 0.11519689559936523
	model : 0.06502389907836914
			 train-loss:  2.1899279339560147 	 ± 0.20744341831634885
	data : 0.11521267890930176
	model : 0.06503658294677735
			 train-loss:  2.188499253090114 	 ± 0.20744635571113243
	data : 0.11495733261108398
	model : 0.06503777503967285
			 train-loss:  2.191175616517359 	 ± 0.20925350357300126
	data : 0.11442832946777344
	model : 0.06499900817871093
			 train-loss:  2.1926317617699906 	 ± 0.2092913357683113
	data : 0.11460676193237304
	model : 0.06490578651428222
			 train-loss:  2.191762256942339 	 ± 0.20885587853673862
	data : 0.11453409194946289
	model : 0.06487851142883301
			 train-loss:  2.192921806971232 	 ± 0.20863919022285055
	data : 0.11452174186706543
	model : 0.06485934257507324
			 train-loss:  2.191245364827036 	 ± 0.20895836960112274
	data : 0.1144805908203125
	model : 0.06488571166992188
			 train-loss:  2.1915888472607263 	 ± 0.20831263693322766
	data : 0.11459541320800781
	model : 0.06494326591491699
			 train-loss:  2.1907250226712693 	 ± 0.2079037135405148
	data : 0.11430525779724121
	model : 0.06507081985473633
			 train-loss:  2.190371397253755 	 ± 0.20727376068977496
	data : 0.11443076133728028
	model : 0.06511173248291016
			 train-loss:  2.189904228333504 	 ± 0.2066853769063313
	data : 0.11441578865051269
	model : 0.06510286331176758
			 train-loss:  2.1906031522995386 	 ± 0.20620553570851546
	data : 0.11431007385253907
	model : 0.06505193710327148
			 train-loss:  2.18826331226689 	 ± 0.20761494922467727
	data : 0.11446294784545899
	model : 0.06497235298156738
			 train-loss:  2.187513884864276 	 ± 0.20716982146958268
	data : 0.11448807716369629
	model : 0.06489071846008301
			 train-loss:  2.187427546992992 	 ± 0.20652016795386294
	data : 0.11427888870239258
	model : 0.06493196487426758
			 train-loss:  2.1876121275126934 	 ± 0.20588693686844475
	data : 0.11459736824035645
	model : 0.06490907669067383
			 train-loss:  2.185723689031897 	 ± 0.20663188129254514
	data : 0.11468720436096191
	model : 0.06492123603820801
			 train-loss:  2.1860880182113176 	 ± 0.2060450065731313
	data : 0.1145547866821289
	model : 0.06502113342285157
			 train-loss:  2.1858635994554296 	 ± 0.20543185326551008
	data : 0.11455044746398926
	model : 0.06507396697998047
			 train-loss:  2.183216094970703 	 ± 0.20757511460966008
	data : 0.11504635810852051
	model : 0.06504850387573242
			 train-loss:  2.1828145518447415 	 ± 0.207009021845296
	data : 0.11477813720703126
	model : 0.06508355140686035
			 train-loss:  2.1822980699768983 	 ± 0.20649116312650226
	data : 0.11513195037841797
	model : 0.06504855155944825
			 train-loss:  2.180008335741694 	 ± 0.20797499379643525
	data : 0.11500048637390137
	model : 0.06499409675598145
			 train-loss:  2.181009755248115 	 ± 0.20775853918227735
	data : 0.11533398628234863
	model : 0.06491742134094239
			 train-loss:  2.1794802077423188 	 ± 0.20808950775911705
	data : 0.11485171318054199
	model : 0.06485223770141602
			 train-loss:  2.178094903160544 	 ± 0.208256698976167
	data : 0.11480822563171386
	model : 0.06483936309814453
			 train-loss:  2.1788805674391183 	 ± 0.2078993929093041
	data : 0.11450347900390626
	model : 0.06486477851867675
			 train-loss:  2.1778218669946803 	 ± 0.20775594030538877
	data : 0.11460585594177246
	model : 0.06492538452148437
			 train-loss:  2.176369688414425 	 ± 0.2080282524732166
	data : 0.11425762176513672
	model : 0.06500706672668458
			 train-loss:  2.1759817442674745 	 ± 0.20749235940763855
	data : 0.11443891525268554
	model : 0.0650566577911377
			 train-loss:  2.1758395556041172 	 ± 0.20690717600467648
	data : 0.11456789970397949
	model : 0.06508851051330566
			 train-loss:  2.176474800841375 	 ± 0.20648960371616557
	data : 0.1143908977508545
	model : 0.06501688957214355
			 train-loss:  2.1770136861477867 	 ± 0.2060295463551467
	data : 0.11430177688598633
	model : 0.06493349075317383
			 train-loss:  2.17724825893895 	 ± 0.20547369759078404
	data : 0.1142885684967041
	model : 0.06490077972412109
			 train-loss:  2.1764794201824254 	 ± 0.20515554032503808
	data : 0.11426763534545899
	model : 0.06490464210510254
			 train-loss:  2.177289899852541 	 ± 0.20487203352107583
	data : 0.1141420841217041
	model : 0.06487364768981933
			 train-loss:  2.176810304762909 	 ± 0.20440660370439698
	data : 0.11430530548095703
	model : 0.06496481895446778
			 train-loss:  2.176682791212103 	 ± 0.20385149223990126
	data : 0.1144594669342041
	model : 0.06501803398132325
			 train-loss:  2.176190306580132 	 ± 0.2034022972181905
	data : 0.11451478004455566
	model : 0.06505999565124512
			 train-loss:  2.174548703043357 	 ± 0.20406078451638635
	data : 0.11446452140808105
	model : 0.06504020690917969
			 train-loss:  2.1756492530977405 	 ± 0.20405533825240735
	data : 0.11446156501770019
	model : 0.064994478225708
			 train-loss:  2.176808295070484 	 ± 0.2041157581244518
	data : 0.11438069343566895
	model : 0.06496806144714355
			 train-loss:  2.1754212691822152 	 ± 0.20444627415748012
	data : 0.11435403823852539
	model : 0.06497254371643066
			 train-loss:  2.1757549725948495 	 ± 0.20395286633301102
	data : 0.1143280029296875
	model : 0.06494002342224121
			 train-loss:  2.1752860842558444 	 ± 0.20351416640119227
	data : 0.11430015563964843
	model : 0.06495118141174316
			 train-loss:  2.175323643809871 	 ± 0.20297855304087106
	data : 0.11431159973144531
	model : 0.06497960090637207
			 train-loss:  2.1763298967121782 	 ± 0.2029210885687501
	data : 0.11452689170837402
	model : 0.06497869491577149
			 train-loss:  2.1773834644506374 	 ± 0.20291504567440743
	data : 0.11430959701538086
	model : 0.06486706733703614
			 train-loss:  2.176686545727784 	 ± 0.20261892773565765
	data : 0.11428313255310059
	model : 0.06481847763061524
			 train-loss:  2.1767466357073832 	 ± 0.20209776344550973
	data : 0.11432509422302246
	model : 0.06486740112304687
			 train-loss:  2.1774405155426417 	 ± 0.2018104485466697
	data : 0.11432814598083496
	model : 0.06489448547363282
			 train-loss:  2.1786312369667753 	 ± 0.20198053967734123
	data : 0.11416635513305665
	model : 0.06489529609680175
			 train-loss:  2.177068609876681 	 ± 0.20265153560287136
	data : 0.11423215866088868
	model : 0.06502876281738282
			 train-loss:  2.177873881176265 	 ± 0.2024548826822922
	data : 0.1143301010131836
	model : 0.06507477760314942
			 train-loss:  2.1798081757435246 	 ± 0.20377150409024336
	data : 0.1143465518951416
	model : 0.06498351097106933
			 train-loss:  2.1810116350650786 	 ± 0.20396917978751705
	data : 0.11423931121826172
	model : 0.06490812301635743
			 train-loss:  2.1801689608180106 	 ± 0.20380987233231093
	data : 0.11429505348205567
	model : 0.0649111270904541
			 train-loss:  2.1800193810226896 	 ± 0.20331582647833418
	data : 0.11439990997314453
	model : 0.06487431526184081
			 train-loss:  2.1799256120409285 	 ± 0.2028188089450335
	data : 0.11435508728027344
	model : 0.0648770809173584
			 train-loss:  2.181207303907357 	 ± 0.20314354485174063
	data : 0.11433162689208984
	model : 0.06493163108825684
			 train-loss:  2.1815415533577522 	 ± 0.20270369346213993
	data : 0.11444544792175293
	model : 0.06500511169433594
			 train-loss:  2.1822859916872193 	 ± 0.2024918167182738
	data : 0.11445703506469726
	model : 0.06500186920166015
			 train-loss:  2.1815486668388626 	 ± 0.20227912768305575
	data : 0.11430478096008301
	model : 0.06499600410461426
			 train-loss:  2.1821255076390047 	 ± 0.20196288810515028
	data : 0.11406636238098145
	model : 0.06492724418640136
			 train-loss:  2.1833065571397117 	 ± 0.20219787353297555
	data : 0.11400017738342286
	model : 0.06489224433898926
			 train-loss:  2.183575415611267 	 ± 0.20175331962910265
	data : 0.11402173042297363
	model : 0.06488161087036133
			 train-loss:  2.183657026968861 	 ± 0.20127813793481353
	data : 0.11399588584899903
	model : 0.06492052078247071
			 train-loss:  2.184062416823405 	 ± 0.2008891888964911
	data : 0.11410675048828126
	model : 0.06491799354553222
			 train-loss:  2.184008259168813 	 ± 0.200418614516919
	data : 0.11424031257629394
	model : 0.0649979591369629
			 train-loss:  2.183824069032045 	 ± 0.19996786772004402
	data : 0.11436491012573242
	model : 0.06502513885498047
			 train-loss:  2.1875061877938204 	 ± 0.2066460091080438
	data : 0.11419205665588379
	model : 0.06498589515686035
			 train-loss:  2.1862139177543147 	 ± 0.20703603015299962
	data : 0.11401953697204589
	model : 0.06494274139404296
			 train-loss:  2.186112606580356 	 ± 0.20656380423622384
	data : 0.11398344039916992
	model : 0.06497335433959961
			 train-loss:  2.185792745253362 	 ± 0.20614334628826553
	data : 0.11414241790771484
	model : 0.06493825912475586
			 train-loss:  2.1867003076152716 	 ± 0.20610821841786836
	data : 0.11421341896057129
	model : 0.06491107940673828
			 train-loss:  2.1863283368674193 	 ± 0.20571291987486076
	data : 0.11443672180175782
	model : 0.06490836143493653
			 train-loss:  2.186107110653528 	 ± 0.2052732061165538
	data : 0.11456761360168458
	model : 0.06489801406860352
			 train-loss:  2.1865281655981734 	 ± 0.20490598558208561
	data : 0.1146726131439209
	model : 0.06479549407958984
			 train-loss:  2.1877677809497165 	 ± 0.20527863572555252
	data : 0.1144927978515625
	model : 0.06468691825866699
			 train-loss:  2.187189414565052 	 ± 0.20500193057680685
	data : 0.11420621871948242
	model : 0.0645066738128662
			 train-loss:  2.18752637386322 	 ± 0.2046080241726969
	data : 0.11430425643920898
	model : 0.06435713768005372
			 train-loss:  2.1862133171706075 	 ± 0.20510272835269353
	data : 0.11446938514709473
	model : 0.06416397094726563
			 train-loss:  2.1869314998257003 	 ± 0.2049350607820206
	data : 0.11452126502990723
	model : 0.06403079032897949
			 train-loss:  2.1855024099349976 	 ± 0.20561560579714175
	data : 0.1147850513458252
	model : 0.06396322250366211
			 train-loss:  2.185455277497071 	 ± 0.20516740658954702
	data : 0.11510801315307617
	model : 0.06395435333251953
			 train-loss:  2.1842556683913523 	 ± 0.2055241932050338
	data : 0.11506304740905762
	model : 0.06390581130981446
			 train-loss:  2.1836997317029283 	 ± 0.2052520915807714
	data : 0.11498322486877441
	model : 0.06386427879333496
			 train-loss:  2.1856577046986283 	 ± 0.2069599191449704
	data : 0.11509695053100585
	model : 0.06382694244384765
			 train-loss:  2.1872232195645442 	 ± 0.2078874026755938
	data : 0.11500239372253418
	model : 0.063816499710083
			 train-loss:  2.1866420721396422 	 ± 0.20763230734623933
	data : 0.11507577896118164
	model : 0.06381807327270508
			 train-loss:  2.1869102751955074 	 ± 0.20723068194384275
	data : 0.11510400772094727
	model : 0.06383371353149414
			 train-loss:  2.186406981136839 	 ± 0.20693504714515384
	data : 0.1150787353515625
	model : 0.06384620666503907
			 train-loss:  2.1874386298505564 	 ± 0.20710529759745375
	data : 0.11497316360473633
	model : 0.06381559371948242
			 train-loss:  2.1872874257945214 	 ± 0.20668285298607642
	data : 0.11505007743835449
	model : 0.06379623413085937
			 train-loss:  2.1876170076585715 	 ± 0.20631267219256466
	data : 0.11499481201171875
	model : 0.06377668380737304
			 train-loss:  2.188037543495496 	 ± 0.20598502887960768
	data : 0.1149672508239746
	model : 0.0638002872467041
			 train-loss:  2.187925384252398 	 ± 0.2055645734430631
	data : 0.1149641990661621
	model : 0.0638310432434082
			 train-loss:  2.186208533846642 	 ± 0.20686358948660513
	data : 0.11497483253479004
	model : 0.06387286186218262
			 train-loss:  2.187696915104556 	 ± 0.20773189719093862
	data : 0.11485528945922852
	model : 0.06383314132690429
			 train-loss:  2.1913105610941277 	 ± 0.21482291941733186
	data : 0.11475753784179688
	model : 0.06379313468933105
			 train-loss:  2.19340015333526 	 ± 0.21685461596019706
	data : 0.11466422080993652
	model : 0.06394038200378419
			 train-loss:  2.1928147949823518 	 ± 0.21660727145458833
	data : 0.11464457511901856
	model : 0.0640411376953125
			 train-loss:  2.191447250756175 	 ± 0.2172298767414573
	data : 0.11461029052734376
	model : 0.06417841911315918
			 train-loss:  2.1899425690212557 	 ± 0.21807743219144193
	data : 0.11463809013366699
	model : 0.06438241004943848
			 train-loss:  2.190224006472821 	 ± 0.2176842087313917
	data : 0.11465802192687988
	model : 0.06451959609985351
			 train-loss:  2.18941423368454 	 ± 0.21762386434460645
	data : 0.11456341743469238
	model : 0.06442532539367676
			 train-loss:  2.1906264440946845 	 ± 0.2180339973718186
	data : 0.1146303653717041
	model : 0.06449570655822753
			 train-loss:  2.1888599206530857 	 ± 0.21939336761946504
	data : 0.11456422805786133
	model : 0.0645113468170166
			 train-loss:  2.189521740547753 	 ± 0.2192112596794539
	data : 0.1145181655883789
	model : 0.0644200325012207
			 train-loss:  2.190186046239898 	 ± 0.21903433174052991
	data : 0.11459317207336425
	model : 0.06436529159545898
			 train-loss:  2.191170067880668 	 ± 0.21916624967220175
	data : 0.11479473114013672
	model : 0.06434249877929688
			 train-loss:  2.190419267397374 	 ± 0.2190661010668284
	data : 0.11452970504760743
	model : 0.05583834648132324
#epoch  72    val-loss:  2.421064991700022  train-loss:  2.190419267397374  lr:  1.9073486328125e-08
			 train-loss:  1.8340249061584473 	 ± 0.0
	data : 5.249761343002319
	model : 0.08492207527160645
			 train-loss:  2.133866548538208 	 ± 0.29984164237976074
	data : 2.7253612279891968
	model : 0.07519996166229248
			 train-loss:  2.131098826726278 	 ± 0.24485096331861014
	data : 1.8546831607818604
	model : 0.07169771194458008
			 train-loss:  2.2086769938468933 	 ± 0.25103607653068716
	data : 1.419474720954895
	model : 0.06995254755020142
			 train-loss:  2.260746145248413 	 ± 0.24750772813517233
	data : 1.1583436489105225
	model : 0.06882596015930176
			 train-loss:  2.2182023525238037 	 ± 0.2451528796536995
	data : 0.13116650581359862
	model : 0.0647578239440918
			 train-loss:  2.183766790798732 	 ± 0.24213441918856868
	data : 0.1138953685760498
	model : 0.06464114189147949
			 train-loss:  2.204412266612053 	 ± 0.23298946765298464
	data : 0.11404528617858886
	model : 0.06466217041015625
			 train-loss:  2.2159930732515125 	 ± 0.22209332928899977
	data : 0.11408753395080566
	model : 0.06470270156860351
			 train-loss:  2.2073734402656555 	 ± 0.21227714167687184
	data : 0.11415729522705079
	model : 0.06483368873596192
			 train-loss:  2.1964625770395454 	 ± 0.20531815926913055
	data : 0.11409621238708496
	model : 0.06488027572631835
			 train-loss:  2.183646867672602 	 ± 0.20111999101916614
	data : 0.11386785507202149
	model : 0.06482186317443847
			 train-loss:  2.1814467998651357 	 ± 0.19338007533876747
	data : 0.11373934745788575
	model : 0.06489725112915039
			 train-loss:  2.179974785872868 	 ± 0.18642126989688865
	data : 0.11377153396606446
	model : 0.06489720344543456
			 train-loss:  2.173575838406881 	 ± 0.18168456716716083
	data : 0.11375107765197753
	model : 0.06491875648498535
			 train-loss:  2.2047721818089485 	 ± 0.21341129175829654
	data : 0.11385140419006348
	model : 0.06493167877197266
			 train-loss:  2.1913383147295784 	 ± 0.21389903894391238
	data : 0.11400380134582519
	model : 0.06497130393981934
			 train-loss:  2.1868458721372814 	 ± 0.20869612212435595
	data : 0.1141573429107666
	model : 0.06491556167602539
			 train-loss:  2.210437297821045 	 ± 0.22645032220595523
	data : 0.11398224830627442
	model : 0.06482887268066406
			 train-loss:  2.21591032743454 	 ± 0.222002000026296
	data : 0.11388540267944336
	model : 0.06477880477905273
			 train-loss:  2.204858740170797 	 ± 0.22221777653311228
	data : 0.11386103630065918
	model : 0.06482720375061035
			 train-loss:  2.203247617591511 	 ± 0.217234138988932
	data : 0.1139176368713379
	model : 0.06486916542053223
			 train-loss:  2.221860787142878 	 ± 0.22969720200338523
	data : 0.11391587257385254
	model : 0.06487798690795898
			 train-loss:  2.2210337072610855 	 ± 0.22489591182482518
	data : 0.11409459114074708
	model : 0.06509361267089844
			 train-loss:  2.2247088384628295 	 ± 0.2210864139962009
	data : 0.11419186592102051
	model : 0.06512222290039063
			 train-loss:  2.210843631854424 	 ± 0.2276078469339349
	data : 0.11418981552124023
	model : 0.06509985923767089
			 train-loss:  2.201261321703593 	 ± 0.22863497840426447
	data : 0.11404123306274414
	model : 0.06503558158874512
			 train-loss:  2.1975010335445404 	 ± 0.22536370638286066
	data : 0.11389727592468261
	model : 0.06504054069519043
			 train-loss:  2.204973709994349 	 ± 0.22494667641757385
	data : 0.11390371322631836
	model : 0.06535906791687011
			 train-loss:  2.2021358211835227 	 ± 0.22169317031529126
	data : 0.11359915733337403
	model : 0.06537280082702637
			 train-loss:  2.199381709098816 	 ± 0.21860924241685506
	data : 0.1137692928314209
	model : 0.06537933349609375
			 train-loss:  2.206099633127451 	 ± 0.21839324381438963
	data : 0.11385316848754883
	model : 0.0654529094696045
			 train-loss:  2.206693501183481 	 ± 0.21508503712997326
	data : 0.11388540267944336
	model : 0.06548395156860351
			 train-loss:  2.201902035404654 	 ± 0.21367863125174913
	data : 0.11387815475463867
	model : 0.06502585411071778
			 train-loss:  2.199746905054365 	 ± 0.21097853727990784
	data : 0.11415271759033203
	model : 0.0652252197265625
			 train-loss:  2.19510754942894 	 ± 0.20983047216258738
	data : 0.11391468048095703
	model : 0.06558256149291992
			 train-loss:  2.186192825033858 	 ± 0.21377526037133032
	data : 0.11362171173095703
	model : 0.06567344665527344
			 train-loss:  2.1765076298462716 	 ± 0.219015856084124
	data : 0.11383767127990722
	model : 0.06606884002685547
			 train-loss:  2.175454720472678 	 ± 0.21628713644756192
	data : 0.1138580322265625
	model : 0.06609854698181153
			 train-loss:  2.168595027923584 	 ± 0.21782052738602242
	data : 0.11409006118774415
	model : 0.06597204208374023
			 train-loss:  2.179961233604245 	 ± 0.22683957594505078
	data : 0.11426687240600586
	model : 0.0656132698059082
			 train-loss:  2.1912703059968495 	 ± 0.2355307742335594
	data : 0.11461272239685058
	model : 0.06549639701843261
			 train-loss:  2.187728393909543 	 ± 0.23390496185384504
	data : 0.11447372436523437
	model : 0.06506853103637696
			 train-loss:  2.1878297708251258 	 ± 0.23123263007714798
	data : 0.11425437927246093
	model : 0.06497812271118164
			 train-loss:  2.1876403437720406 	 ± 0.2286523966835102
	data : 0.11412138938903808
	model : 0.06488876342773438
			 train-loss:  2.183587970940963 	 ± 0.22778132652514665
	data : 0.1141397476196289
	model : 0.06491565704345703
			 train-loss:  2.190145269353339 	 ± 0.22969180702519246
	data : 0.11427087783813476
	model : 0.06515421867370605
			 train-loss:  2.1876237938801446 	 ± 0.22794300346966806
	data : 0.11425800323486328
	model : 0.06516938209533692
			 train-loss:  2.1833343457202523 	 ± 0.22755398209385236
	data : 0.11450076103210449
	model : 0.06527667045593262
			 train-loss:  2.196644344329834 	 ± 0.24377416917672032
	data : 0.11450495719909667
	model : 0.06524477005004883
			 train-loss:  2.1943613734899783 	 ± 0.24191161752298246
	data : 0.11442551612854004
	model : 0.06516690254211426
			 train-loss:  2.193716828639691 	 ± 0.23961846704981166
	data : 0.11421656608581543
	model : 0.0648848533630371
			 train-loss:  2.1961068612224652 	 ± 0.23797207240242066
	data : 0.11426997184753418
	model : 0.06483635902404786
			 train-loss:  2.1970061152069658 	 ± 0.23584920882274601
	data : 0.11418714523315429
	model : 0.06476817131042481
			 train-loss:  2.1978094924579965 	 ± 0.2337698458234362
	data : 0.11421008110046386
	model : 0.0648158073425293
			 train-loss:  2.1997462255614146 	 ± 0.23211802820701688
	data : 0.1143716812133789
	model : 0.06488313674926757
			 train-loss:  2.2099809897573373 	 ± 0.24248620492252107
	data : 0.11444058418273925
	model : 0.0649489402770996
			 train-loss:  2.209257717790275 	 ± 0.24044873058731536
	data : 0.1144301414489746
	model : 0.06497311592102051
			 train-loss:  2.2118189577328957 	 ± 0.2391989629048446
	data : 0.11452045440673828
	model : 0.06500344276428223
			 train-loss:  2.209104537963867 	 ± 0.23811185998064244
	data : 0.11438713073730469
	model : 0.06493830680847168
			 train-loss:  2.2094488808366117 	 ± 0.23616712078657076
	data : 0.11432790756225586
	model : 0.06485109329223633
			 train-loss:  2.2077962006292036 	 ± 0.23461015770650334
	data : 0.11429476737976074
	model : 0.06480841636657715
			 train-loss:  2.2071008341653005 	 ± 0.23280511994499595
	data : 0.11427645683288574
	model : 0.06482610702514649
			 train-loss:  2.2034957129508257 	 ± 0.23274488449753547
	data : 0.11426868438720703
	model : 0.06484022140502929
			 train-loss:  2.2075710021532498 	 ± 0.23323744276833416
	data : 0.11435928344726562
	model : 0.06487584114074707
			 train-loss:  2.2097472331740637 	 ± 0.2321277785048805
	data : 0.11449828147888183
	model : 0.06490559577941894
			 train-loss:  2.2094434962343814 	 ± 0.2304021831862602
	data : 0.11451473236083984
	model : 0.06494021415710449
			 train-loss:  2.210425869506948 	 ± 0.2288430917602463
	data : 0.11446523666381836
	model : 0.06487865447998047
			 train-loss:  2.2081449394640713 	 ± 0.2279560636399064
	data : 0.11449632644653321
	model : 0.06481456756591797
			 train-loss:  2.2059673292296273 	 ± 0.2270436557629082
	data : 0.11449546813964843
	model : 0.06481976509094238
			 train-loss:  2.206173791012294 	 ± 0.2254457046247888
	data : 0.11439423561096192
	model : 0.06486167907714843
			 train-loss:  2.202233206894663 	 ± 0.22632356206801704
	data : 0.11446533203125
	model : 0.06484832763671874
			 train-loss:  2.2010657836313117 	 ± 0.22498623469934179
	data : 0.11481614112854004
	model : 0.06491084098815918
			 train-loss:  2.204911665336506 	 ± 0.2258638898374634
	data : 0.11475958824157714
	model : 0.06496162414550781
			 train-loss:  2.2056028032302857 	 ± 0.224431840745183
	data : 0.11473474502563477
	model : 0.0649871826171875
			 train-loss:  2.201552844361255 	 ± 0.2256923904027316
	data : 0.1146627426147461
	model : 0.06498785018920898
			 train-loss:  2.2034934604322753 	 ± 0.22485940092788795
	data : 0.1145164966583252
	model : 0.0649491786956787
			 train-loss:  2.1997696390518775 	 ± 0.22579032250297046
	data : 0.11427898406982422
	model : 0.06490931510925294
			 train-loss:  2.199331070803389 	 ± 0.2243901510587332
	data : 0.11424541473388672
	model : 0.0649254322052002
			 train-loss:  2.201984520256519 	 ± 0.2242270631960698
	data : 0.11430525779724121
	model : 0.06486973762512208
			 train-loss:  2.200520881900081 	 ± 0.22322285180121987
	data : 0.11435379981994628
	model : 0.06486954689025878
			 train-loss:  2.1953369073751494 	 ± 0.22671025249559312
	data : 0.1144566535949707
	model : 0.06494135856628418
			 train-loss:  2.1937564898686235 	 ± 0.22579438463422724
	data : 0.11442270278930664
	model : 0.06498236656188965
			 train-loss:  2.192508633647646 	 ± 0.22473407721970276
	data : 0.11454448699951172
	model : 0.06498780250549316
			 train-loss:  2.1940180876675774 	 ± 0.22383613234653804
	data : 0.11453666687011718
	model : 0.06500654220581055
			 train-loss:  2.193352392939634 	 ± 0.22261557293307088
	data : 0.11424527168273926
	model : 0.0649026870727539
			 train-loss:  2.1986692842395827 	 ± 0.22675808100212744
	data : 0.11427202224731445
	model : 0.06482744216918945
			 train-loss:  2.197467404333028 	 ± 0.22574452536126877
	data : 0.11425714492797852
	model : 0.06483230590820313
			 train-loss:  2.1997565272149076 	 ± 0.22549751035768364
	data : 0.11423587799072266
	model : 0.06483006477355957
			 train-loss:  2.200389153427548 	 ± 0.22432065462827785
	data : 0.11425318717956542
	model : 0.06485428810119628
			 train-loss:  2.1991977966748752 	 ± 0.22337083856735385
	data : 0.11440958976745605
	model : 0.06493597030639649
			 train-loss:  2.1965851744879847 	 ± 0.22354719182944466
	data : 0.11433053016662598
	model : 0.06496376991271972
			 train-loss:  2.195937165649988 	 ± 0.22242893557196206
	data : 0.11433000564575195
	model : 0.06491503715515137
			 train-loss:  2.194641165276791 	 ± 0.2215953742565404
	data : 0.11426496505737305
	model : 0.06489272117614746
			 train-loss:  2.1976674845344144 	 ± 0.22237025315787723
	data : 0.11429042816162109
	model : 0.064849853515625
			 train-loss:  2.19923880075415 	 ± 0.22173858241987984
	data : 0.11434178352355957
	model : 0.06485867500305176
			 train-loss:  2.1969884093274774 	 ± 0.22169186150525413
	data : 0.11432232856750488
	model : 0.06489706039428711
			 train-loss:  2.1967436695585447 	 ± 0.22057105124751533
	data : 0.1144282341003418
	model : 0.06495471000671386
			 train-loss:  2.196612980630663 	 ± 0.21945804214122883
	data : 0.11431336402893066
	model : 0.06497130393981934
			 train-loss:  2.1969483101367953 	 ± 0.21838348399031313
	data : 0.11428337097167969
	model : 0.06503896713256836
			 train-loss:  2.1969074695417197 	 ± 0.21730007208886043
	data : 0.11417140960693359
	model : 0.06501593589782714
			 train-loss:  2.1979910965059317 	 ± 0.2165063181390715
	data : 0.11412553787231446
	model : 0.06494216918945313
			 train-loss:  2.199366082265539 	 ± 0.21589981148734183
	data : 0.11412153244018555
	model : 0.06491503715515137
			 train-loss:  2.19738730100485 	 ± 0.21579581446451648
	data : 0.11431317329406739
	model : 0.06493577957153321
			 train-loss:  2.1959648586454845 	 ± 0.21525509972928245
	data : 0.11435089111328126
	model : 0.06491560935974121
			 train-loss:  2.194880280854567 	 ± 0.21452540652503863
	data : 0.11468009948730469
	model : 0.06495857238769531
			 train-loss:  2.1963307434153334 	 ± 0.2140421758732777
	data : 0.11479644775390625
	model : 0.06504526138305664
			 train-loss:  2.19950160936073 	 ± 0.2155589679830311
	data : 0.1148719310760498
	model : 0.06505160331726074
			 train-loss:  2.197740832600025 	 ± 0.2153467304095706
	data : 0.11481056213378907
	model : 0.06503853797912598
			 train-loss:  2.1973312031139027 	 ± 0.2144083028480455
	data : 0.11480197906494141
	model : 0.06502003669738769
			 train-loss:  2.1947121072459863 	 ± 0.21520067574930865
	data : 0.11446657180786132
	model : 0.06498541831970214
			 train-loss:  2.1934954981718744 	 ± 0.21462090321198452
	data : 0.11436963081359863
	model : 0.06493310928344727
			 train-loss:  2.1891849853296197 	 ± 0.21848460118699523
	data : 0.11426210403442383
	model : 0.06497688293457031
			 train-loss:  2.189102675831109 	 ± 0.2175259843658395
	data : 0.11430916786193848
	model : 0.0649653434753418
			 train-loss:  2.192358322765516 	 ± 0.21934997127359918
	data : 0.11435818672180176
	model : 0.06498351097106933
			 train-loss:  2.192466029833103 	 ± 0.21840550495691644
	data : 0.11443629264831542
	model : 0.0649991512298584
			 train-loss:  2.190849303180336 	 ± 0.2181661418082112
	data : 0.11448040008544921
	model : 0.06503310203552246
			 train-loss:  2.1884749966152643 	 ± 0.21875253909324285
	data : 0.11447453498840332
	model : 0.06498451232910156
			 train-loss:  2.1873072255559327 	 ± 0.21820051646443034
	data : 0.11430249214172364
	model : 0.06494741439819336
			 train-loss:  2.1882183889547986 	 ± 0.21751666419792315
	data : 0.1141275405883789
	model : 0.06487617492675782
			 train-loss:  2.1846280285149566 	 ± 0.22015758247167627
	data : 0.11407604217529296
	model : 0.06490254402160645
			 train-loss:  2.1852192282676697 	 ± 0.21934986402865506
	data : 0.11417994499206544
	model : 0.06486892700195312
			 train-loss:  2.1845022885780025 	 ± 0.21859985698750756
	data : 0.11416831016540527
	model : 0.06494865417480469
			 train-loss:  2.184525013931336 	 ± 0.21771676751311286
	data : 0.11437830924987794
	model : 0.06496982574462891
			 train-loss:  2.184970986366272 	 ± 0.21690101121176353
	data : 0.11449055671691895
	model : 0.06502299308776856
			 train-loss:  2.1867242020273965 	 ± 0.21692599652964262
	data : 0.1147247314453125
	model : 0.0649609088897705
			 train-loss:  2.1851309002853756 	 ± 0.2168091930915949
	data : 0.11454591751098633
	model : 0.06498184204101562
			 train-loss:  2.186499061062932 	 ± 0.21651031565205497
	data : 0.11457271575927734
	model : 0.06483936309814453
			 train-loss:  2.1845272104869515 	 ± 0.21682024883105186
	data : 0.11444025039672852
	model : 0.06491217613220215
			 train-loss:  2.1839690923690798 	 ± 0.21607771757536492
	data : 0.11439318656921386
	model : 0.06493735313415527
			 train-loss:  2.183579484925015 	 ± 0.21529724624239396
	data : 0.11433820724487305
	model : 0.06500096321105957
			 train-loss:  2.17988844351335 	 ± 0.21860114805771502
	data : 0.11448125839233399
	model : 0.06505637168884278
			 train-loss:  2.182905466036689 	 ± 0.22051912686551803
	data : 0.11446337699890137
	model : 0.06513652801513672
			 train-loss:  2.181173811207956 	 ± 0.22060054909306873
	data : 0.11452445983886719
	model : 0.06507902145385742
			 train-loss:  2.18062584400177 	 ± 0.21987350828332278
	data : 0.11452951431274414
	model : 0.06509346961975097
			 train-loss:  2.178159519153483 	 ± 0.22092998614236734
	data : 0.11430625915527344
	model : 0.06502494812011719
			 train-loss:  2.180517880585942 	 ± 0.22183371049379916
	data : 0.11421680450439453
	model : 0.06497864723205567
			 train-loss:  2.181192914644877 	 ± 0.22116967789246353
	data : 0.11424336433410645
	model : 0.06497406959533691
			 train-loss:  2.1853127599620135 	 ± 0.22562447533512398
	data : 0.11426768302917481
	model : 0.06497082710266114
			 train-loss:  2.1822607806750707 	 ± 0.22767852990936596
	data : 0.11428117752075195
	model : 0.06498656272888184
			 train-loss:  2.1835140674672227 	 ± 0.22735384961623883
	data : 0.11445255279541015
	model : 0.06505384445190429
			 train-loss:  2.185585356094468 	 ± 0.227883048529877
	data : 0.11457080841064453
	model : 0.06507725715637207
			 train-loss:  2.1857433185710775 	 ± 0.22709265813034285
	data : 0.11477904319763184
	model : 0.06505360603332519
			 train-loss:  2.185198626584477 	 ± 0.2263964873564277
	data : 0.11466770172119141
	model : 0.06506843566894531
			 train-loss:  2.184989225453344 	 ± 0.225628452134423
	data : 0.11451139450073242
	model : 0.06499414443969727
			 train-loss:  2.1854998869438695 	 ± 0.22493849052568762
	data : 0.11449036598205567
	model : 0.0649181842803955
			 train-loss:  2.185510130966602 	 ± 0.22417212216215615
	data : 0.11450929641723633
	model : 0.064910888671875
			 train-loss:  2.1937547696603312 	 ± 0.244756572916838
	data : 0.11428728103637695
	model : 0.064874267578125
			 train-loss:  2.1949444761212242 	 ± 0.24436285967360444
	data : 0.11436152458190918
	model : 0.06487135887145996
			 train-loss:  2.193530868689219 	 ± 0.2441574568792518
	data : 0.1145927906036377
	model : 0.06492562294006347
			 train-loss:  2.191964549734103 	 ± 0.24410260103687245
	data : 0.11454439163208008
	model : 0.06497631072998047
			 train-loss:  2.1939947346323416 	 ± 0.24457398759625132
	data : 0.1148613452911377
	model : 0.06494245529174805
			 train-loss:  2.194524447902355 	 ± 0.24386088027627092
	data : 0.11479554176330567
	model : 0.06497187614440918
			 train-loss:  2.195362491267068 	 ± 0.2432887722439614
	data : 0.11462883949279785
	model : 0.0649022102355957
			 train-loss:  2.194530217109188 	 ± 0.24272254158425163
	data : 0.11451559066772461
	model : 0.06485171318054199
			 train-loss:  2.1936697554893985 	 ± 0.24218038310196147
	data : 0.11485199928283692
	model : 0.0648503303527832
			 train-loss:  2.1961975348223546 	 ± 0.24346365400599818
	data : 0.11447677612304688
	model : 0.06483793258666992
			 train-loss:  2.1970131268984154 	 ± 0.242907040522141
	data : 0.11461067199707031
	model : 0.06483101844787598
			 train-loss:  2.1975499781422645 	 ± 0.24223598850179956
	data : 0.11478877067565918
	model : 0.06489272117614746
			 train-loss:  2.1991431660950185 	 ± 0.24231202265004342
	data : 0.1148402214050293
	model : 0.06491627693176269
			 train-loss:  2.197578984017698 	 ± 0.24236726810490292
	data : 0.11453609466552735
	model : 0.06494426727294922
			 train-loss:  2.196066975593567 	 ± 0.24237855032663802
	data : 0.11457700729370117
	model : 0.06496763229370117
			 train-loss:  2.2056185292320016 	 ± 0.2704931811632483
	data : 0.11444268226623536
	model : 0.06495614051818847
			 train-loss:  2.208056487688204 	 ± 0.27145761555382797
	data : 0.11427507400512696
	model : 0.06492314338684083
			 train-loss:  2.2090071042378745 	 ± 0.27090743397841593
	data : 0.11425795555114746
	model : 0.06494693756103516
			 train-loss:  2.208011441920177 	 ± 0.2703928548862455
	data : 0.11423263549804688
	model : 0.0649406909942627
			 train-loss:  2.207540988922119 	 ± 0.26965021400683764
	data : 0.11427001953125
	model : 0.06498990058898926
			 train-loss:  2.2071612221854076 	 ± 0.2688912756666914
	data : 0.11427569389343262
	model : 0.06506476402282715
			 train-loss:  2.204445858678874 	 ± 0.27039487792115513
	data : 0.11437201499938965
	model : 0.06509828567504883
			 train-loss:  2.2054437398910522 	 ± 0.2699103478002588
	data : 0.11488285064697265
	model : 0.06506872177124023
			 train-loss:  2.2051467100779214 	 ± 0.26914784350997634
	data : 0.1148040771484375
	model : 0.06505565643310547
			 train-loss:  2.204149377900501 	 ± 0.26868100869172906
	data : 0.11466693878173828
	model : 0.06499338150024414
			 train-loss:  2.2023529331119076 	 ± 0.26893732534155734
	data : 0.11468863487243652
	model : 0.06495103836059571
			 train-loss:  2.201725622703289 	 ± 0.2682903084062137
	data : 0.11470603942871094
	model : 0.06497602462768555
			 train-loss:  2.2014039162227084 	 ± 0.26755632157205217
	data : 0.11420235633850098
	model : 0.06498866081237793
			 train-loss:  2.2015940181233664 	 ± 0.2668069876639356
	data : 0.1143538475036621
	model : 0.0649946689605713
			 train-loss:  2.2007547987382963 	 ± 0.2662850783416044
	data : 0.1144552230834961
	model : 0.06503310203552246
			 train-loss:  2.1991722282398953 	 ± 0.26636945497116393
	data : 0.11453056335449219
	model : 0.06506123542785644
			 train-loss:  2.2004266417892286 	 ± 0.26615107611040817
	data : 0.11465630531311036
	model : 0.06499705314636231
			 train-loss:  2.2010279993216195 	 ± 0.2655326565849061
	data : 0.11465249061584473
	model : 0.0649749755859375
			 train-loss:  2.2013276907620507 	 ± 0.2648286496966388
	data : 0.11444902420043945
	model : 0.06491708755493164
			 train-loss:  2.2013783081547245 	 ± 0.2641009742909264
	data : 0.11435518264770508
	model : 0.06482901573181152
			 train-loss:  2.2006740537497516 	 ± 0.2635497070527805
	data : 0.1143254280090332
	model : 0.06481924057006835
			 train-loss:  2.202491973405299 	 ± 0.26398056889638605
	data : 0.11432151794433594
	model : 0.06484313011169433
			 train-loss:  2.2034408163379977 	 ± 0.2635805698656774
	data : 0.11435503959655761
	model : 0.06490092277526856
			 train-loss:  2.203966469533982 	 ± 0.2629682764424539
	data : 0.11460041999816895
	model : 0.06495132446289062
			 train-loss:  2.20497983343461 	 ± 0.26262810308836826
	data : 0.11460747718811035
	model : 0.06501421928405762
			 train-loss:  2.207442360355499 	 ± 0.2640844852803462
	data : 0.11450061798095704
	model : 0.06498255729675292
			 train-loss:  2.208091426147986 	 ± 0.2635352334027589
	data : 0.11428775787353515
	model : 0.06496872901916503
			 train-loss:  2.2064135388324133 	 ± 0.2638510591631451
	data : 0.11420145034790039
	model : 0.06490416526794433
			 train-loss:  2.207630224877003 	 ± 0.26369329580111855
	data : 0.1140857219696045
	model : 0.06491661071777344
			 train-loss:  2.2076169811189175 	 ± 0.2630057617200047
	data : 0.1141672134399414
	model : 0.06491856575012207
			 train-loss:  2.2075402959023114 	 ± 0.26232566682140585
	data : 0.11431398391723632
	model : 0.06498165130615234
			 train-loss:  2.2104048937866367 	 ± 0.2646578602637769
	data : 0.11443080902099609
	model : 0.06502714157104492
			 train-loss:  2.210307621344542 	 ± 0.26398185491858095
	data : 0.11437692642211914
	model : 0.06506609916687012
			 train-loss:  2.2099330060336055 	 ± 0.26335953070194623
	data : 0.11446490287780761
	model : 0.06506214141845704
			 train-loss:  2.2090263354596753 	 ± 0.26299675324013355
	data : 0.11425933837890626
	model : 0.06506862640380859
			 train-loss:  2.2073948058215054 	 ± 0.26332936520896305
	data : 0.1141517162322998
	model : 0.06502332687377929
			 train-loss:  2.2063091286462755 	 ± 0.26311077920552495
	data : 0.11420669555664062
	model : 0.06502799987792969
			 train-loss:  2.2065888530015947 	 ± 0.2624818405542459
	data : 0.1144099235534668
	model : 0.06501197814941406
			 train-loss:  2.207262827389276 	 ± 0.2620015175172905
	data : 0.11448178291320801
	model : 0.06500024795532226
			 train-loss:  2.20822407762603 	 ± 0.26170726751122736
	data : 0.11472091674804688
	model : 0.065006685256958
			 train-loss:  2.2074688027057743 	 ± 0.26128247206616345
	data : 0.11480073928833008
	model : 0.06537785530090331
			 train-loss:  2.2080981199647867 	 ± 0.2607954691214053
	data : 0.11441512107849121
	model : 0.06539692878723144
			 train-loss:  2.2074163361293513 	 ± 0.2603407861075888
	data : 0.1142967700958252
	model : 0.065407133102417
			 train-loss:  2.2081078227283886 	 ± 0.25989676851143206
	data : 0.11412639617919922
	model : 0.06533102989196778
			 train-loss:  2.2078839056733726 	 ± 0.2592881565188873
	data : 0.11398844718933106
	model : 0.06530275344848632
			 train-loss:  2.208184529382449 	 ± 0.2587002760506827
	data : 0.11394724845886231
	model : 0.06497006416320801
			 train-loss:  2.209032636510128 	 ± 0.2583703252280968
	data : 0.11431574821472168
	model : 0.06492009162902831
			 train-loss:  2.2087020368803114 	 ± 0.2577987312283457
	data : 0.11437602043151855
	model : 0.06488680839538574
			 train-loss:  2.207566605360022 	 ± 0.2577129054466924
	data : 0.11443877220153809
	model : 0.0650167465209961
			 train-loss:  2.2094169523356095 	 ± 0.25850546860649615
	data : 0.1145162582397461
	model : 0.06502351760864258
			 train-loss:  2.2084360743912175 	 ± 0.25829307832199117
	data : 0.11448554992675782
	model : 0.06499619483947754
			 train-loss:  2.2068532137113195 	 ± 0.25872228530487384
	data : 0.11444201469421386
	model : 0.06502022743225097
			 train-loss:  2.2075873607812926 	 ± 0.2583432309798466
	data : 0.11434497833251953
	model : 0.06499657630920411
			 train-loss:  2.2074700362152524 	 ± 0.25775026154579705
	data : 0.11435980796813965
	model : 0.06494808197021484
			 train-loss:  2.2074121835594354 	 ± 0.25715708677904825
	data : 0.11442179679870605
	model : 0.06495471000671386
			 train-loss:  2.2056202051836418 	 ± 0.2579210096031424
	data : 0.11455011367797852
	model : 0.06493611335754394
			 train-loss:  2.20333477568953 	 ± 0.2595344737276934
	data : 0.11459956169128419
	model : 0.06495757102966308
			 train-loss:  2.2026873545213177 	 ± 0.2591211385406802
	data : 0.11469731330871583
	model : 0.06500411033630371
			 train-loss:  2.2030877915982208 	 ± 0.25860244288781375
	data : 0.11473336219787597
	model : 0.06510224342346191
			 train-loss:  2.2026239128800125 	 ± 0.2581114861218042
	data : 0.11468100547790527
	model : 0.06508851051330566
			 train-loss:  2.201630503072867 	 ± 0.2579571118748532
	data : 0.11446118354797363
	model : 0.06503806114196778
			 train-loss:  2.200970054204975 	 ± 0.25756956461092456
	data : 0.11449408531188965
	model : 0.06486520767211915
			 train-loss:  2.2026191133923 	 ± 0.2581789528055756
	data : 0.11445069313049316
	model : 0.06470699310302734
			 train-loss:  2.201922341258125 	 ± 0.25781905969242763
	data : 0.11450719833374023
	model : 0.0644566535949707
			 train-loss:  2.201129972672147 	 ± 0.25752619062787724
	data : 0.1146148681640625
	model : 0.06430625915527344
			 train-loss:  2.2019453221245815 	 ± 0.2572542928996384
	data : 0.11491870880126953
	model : 0.06415829658508301
			 train-loss:  2.202310458541437 	 ± 0.2567511919945639
	data : 0.1149184226989746
	model : 0.0641369342803955
			 train-loss:  2.2022193768750067 	 ± 0.2561961368605746
	data : 0.11493449211120606
	model : 0.06407551765441895
			 train-loss:  2.2003425808696004 	 ± 0.2572206515173563
	data : 0.11489934921264648
	model : 0.06398158073425293
			 train-loss:  2.2033021234232804 	 ± 0.26057740618814923
	data : 0.11497058868408203
	model : 0.06394462585449219
			 train-loss:  2.2026263910301767 	 ± 0.2602212526055765
	data : 0.11488885879516601
	model : 0.0639836311340332
			 train-loss:  2.200919976601234 	 ± 0.2609677770040578
	data : 0.1148801326751709
	model : 0.06397814750671386
			 train-loss:  2.1994887590408325 	 ± 0.26133062851009325
	data : 0.11497831344604492
	model : 0.06403841972351074
			 train-loss:  2.1993354432663677 	 ± 0.2607869651066492
	data : 0.11495246887207031
	model : 0.06407637596130371
			 train-loss:  2.198160018598983 	 ± 0.26086192301193206
	data : 0.114666748046875
	model : 0.0640721321105957
			 train-loss:  2.196764973532252 	 ± 0.26119774247566246
	data : 0.11468567848205566
	model : 0.06403093338012696
			 train-loss:  2.1960949553605404 	 ± 0.2608556066335358
	data : 0.11483941078186036
	model : 0.06402401924133301
			 train-loss:  2.1957922503352165 	 ± 0.2603536510792243
	data : 0.11483702659606934
	model : 0.06404280662536621
			 train-loss:  2.1958135861084176 	 ± 0.2598131470366646
	data : 0.1148913860321045
	model : 0.06405329704284668
			 train-loss:  2.195748586792591 	 ± 0.25927775086120997
	data : 0.1150503158569336
	model : 0.06408629417419434
			 train-loss:  2.197516191643452 	 ± 0.2602007254221108
	data : 0.1149014949798584
	model : 0.06407175064086915
			 train-loss:  2.19766965508461 	 ± 0.25967799923943863
	data : 0.11477742195129395
	model : 0.06405420303344726
			 train-loss:  2.1975639406515626 	 ± 0.2591527633669476
	data : 0.11482768058776856
	model : 0.06397781372070313
			 train-loss:  2.1988559888630377 	 ± 0.25941500766884196
	data : 0.1147834300994873
	model : 0.06394920349121094
			 train-loss:  2.199054192434921 	 ± 0.25890800720712487
	data : 0.11484980583190918
	model : 0.06394848823547364
			 train-loss:  2.1993407554203466 	 ± 0.25842473495385854
	data : 0.11508359909057617
	model : 0.06402106285095215
			 train-loss:  2.199678429159295 	 ± 0.2579601040432738
	data : 0.1150787353515625
	model : 0.06396794319152832
			 train-loss:  2.1996684403419495 	 ± 0.25744371513254327
	data : 0.11498508453369141
	model : 0.06394915580749512
			 train-loss:  2.198233722690567 	 ± 0.25792986894040765
	data : 0.11503868103027344
	model : 0.06404333114624024
			 train-loss:  2.198975831270218 	 ± 0.25768595232916935
	data : 0.11509428024291993
	model : 0.06401910781860351
			 train-loss:  2.1983827757741152 	 ± 0.25734844757067465
	data : 0.11501111984252929
	model : 0.06401596069335938
			 train-loss:  2.1992167257887174 	 ± 0.2571836639531343
	data : 0.11512289047241211
	model : 0.06403446197509766
			 train-loss:  2.199111544384676 	 ± 0.25668436066761197
	data : 0.11512393951416015
	model : 0.06406378746032715
			 train-loss:  2.201262743677944 	 ± 0.2584754211764655
	data : 0.11473884582519531
	model : 0.05550827980041504
#epoch  73    val-loss:  2.4116310069435523  train-loss:  2.201262743677944  lr:  1.9073486328125e-08
			 train-loss:  2.704658269882202 	 ± 0.0
	data : 5.611148118972778
	model : 0.0741279125213623
			 train-loss:  2.564233422279358 	 ± 0.14042484760284424
	data : 2.868817687034607
	model : 0.06966114044189453
			 train-loss:  2.4908127784729004 	 ± 0.1546844328197382
	data : 1.9504720369974773
	model : 0.06800540288289388
			 train-loss:  2.5094226002693176 	 ± 0.13778400385914286
	data : 1.4914058446884155
	model : 0.06718599796295166
			 train-loss:  2.4177751541137695 	 ± 0.22087227739782417
	data : 1.2159174919128417
	model : 0.06667904853820801
			 train-loss:  2.3880362113316855 	 ± 0.2123106827852323
	data : 0.11645159721374512
	model : 0.06479172706604004
			 train-loss:  2.380899054663522 	 ± 0.19733741748271716
	data : 0.11384096145629882
	model : 0.06466617584228515
			 train-loss:  2.3813570737838745 	 ± 0.1845962290162151
	data : 0.11390566825866699
	model : 0.06467771530151367
			 train-loss:  2.3693943553500705 	 ± 0.17729755894276267
	data : 0.11390299797058105
	model : 0.06473002433776856
			 train-loss:  2.344397783279419 	 ± 0.18415873453058282
	data : 0.11394987106323243
	model : 0.0647592544555664
			 train-loss:  2.341234098781239 	 ± 0.17587324309273
	data : 0.1139028549194336
	model : 0.06485171318054199
			 train-loss:  2.332714796066284 	 ± 0.1707399920128
	data : 0.11401481628417968
	model : 0.06494650840759278
			 train-loss:  2.356550308374258 	 ± 0.1836498079330103
	data : 0.11397824287414551
	model : 0.06497201919555665
			 train-loss:  2.3357872792652676 	 ± 0.19215230011558077
	data : 0.11398530006408691
	model : 0.06495742797851563
			 train-loss:  2.3315492471059165 	 ± 0.18631279838638587
	data : 0.11385741233825683
	model : 0.06492071151733399
			 train-loss:  2.306145876646042 	 ± 0.20548211266611513
	data : 0.11390461921691894
	model : 0.06490802764892578
			 train-loss:  2.2953243676353905 	 ± 0.20399235187342502
	data : 0.11406197547912597
	model : 0.06492114067077637
			 train-loss:  2.2850161128573947 	 ± 0.20274978411528446
	data : 0.11421017646789551
	model : 0.06496944427490234
			 train-loss:  2.272664082677741 	 ± 0.20418185849197565
	data : 0.11418075561523437
	model : 0.06498827934265136
			 train-loss:  2.305250883102417 	 ± 0.24450319312191235
	data : 0.11426806449890137
	model : 0.0650291919708252
			 train-loss:  2.302009037562779 	 ± 0.2390507263549516
	data : 0.11434135437011719
	model : 0.06504545211791993
			 train-loss:  2.2940134460275825 	 ± 0.23641120443321273
	data : 0.11416888236999512
	model : 0.06494383811950684
			 train-loss:  2.291759729385376 	 ± 0.23145623697329437
	data : 0.11385722160339355
	model : 0.06489500999450684
			 train-loss:  2.2935075958569846 	 ± 0.22673793078078874
	data : 0.11383934020996093
	model : 0.06477651596069336
			 train-loss:  2.2994399166107176 	 ± 0.22404978016763993
	data : 0.11388163566589356
	model : 0.06479101181030274
			 train-loss:  2.294719806084266 	 ± 0.22096286130926748
	data : 0.11385598182678222
	model : 0.06474347114562988
			 train-loss:  2.2912337338482893 	 ± 0.2175597349363759
	data : 0.11389832496643067
	model : 0.06494026184082032
			 train-loss:  2.290712262902941 	 ± 0.21365660145953155
	data : 0.11397042274475097
	model : 0.06492347717285156
			 train-loss:  2.2736775628451644 	 ± 0.22847341886002567
	data : 0.11401429176330566
	model : 0.06501650810241699
			 train-loss:  2.276633922259013 	 ± 0.22519671724206103
	data : 0.11403522491455079
	model : 0.06496319770812989
			 train-loss:  2.267226988269437 	 ± 0.22744747158809736
	data : 0.11404552459716796
	model : 0.06493887901306153
			 train-loss:  2.2634837329387665 	 ± 0.22483346524936487
	data : 0.11401939392089844
	model : 0.06508479118347169
			 train-loss:  2.259924570719401 	 ± 0.22231426094037599
	data : 0.11417098045349121
	model : 0.06557421684265137
			 train-loss:  2.2510898814481846 	 ± 0.22482371241032253
	data : 0.11408576965332032
	model : 0.06559100151062011
			 train-loss:  2.250410876955305 	 ± 0.2216240383143894
	data : 0.11416592597961425
	model : 0.06589388847351074
			 train-loss:  2.250178939766354 	 ± 0.2185285567456642
	data : 0.11395063400268554
	model : 0.06590209007263184
			 train-loss:  2.2458380364082955 	 ± 0.21712306566232298
	data : 0.11391758918762207
	model : 0.06560382843017579
			 train-loss:  2.247705334111264 	 ± 0.21454800656366327
	data : 0.11393218040466309
	model : 0.06514964103698731
			 train-loss:  2.2444899632380557 	 ± 0.2127050449884411
	data : 0.11395182609558105
	model : 0.06513524055480957
			 train-loss:  2.245110958814621 	 ± 0.21006520426806718
	data : 0.11369514465332031
	model : 0.06532511711120606
			 train-loss:  2.2531625294103854 	 ± 0.21364508840732163
	data : 0.1136960506439209
	model : 0.06525483131408691
			 train-loss:  2.243436035655794 	 ± 0.22008236748614818
	data : 0.11371831893920899
	model : 0.06532406806945801
			 train-loss:  2.242556849191355 	 ± 0.21758283201194917
	data : 0.11370530128479003
	model : 0.0653186321258545
			 train-loss:  2.236106815663251 	 ± 0.21921508564797118
	data : 0.1137298583984375
	model : 0.06533384323120117
			 train-loss:  2.235193599594964 	 ± 0.216850302386564
	data : 0.11391148567199708
	model : 0.06487245559692383
			 train-loss:  2.2365647476652395 	 ± 0.2146774178290724
	data : 0.11417531967163086
	model : 0.06515989303588868
			 train-loss:  2.230389075076326 	 ± 0.2164722222958312
	data : 0.11401653289794922
	model : 0.06512913703918458
			 train-loss:  2.2313968911767006 	 ± 0.21431683546576374
	data : 0.11402502059936523
	model : 0.06507711410522461
			 train-loss:  2.2320409234689205 	 ± 0.21216558046529355
	data : 0.11400599479675293
	model : 0.06505966186523438
			 train-loss:  2.2279945158958436 	 ± 0.21193453272559618
	data : 0.11400537490844727
	model : 0.06507911682128906
			 train-loss:  2.2273206827687284 	 ± 0.2099005431310872
	data : 0.11399292945861816
	model : 0.06487607955932617
			 train-loss:  2.220084621356084 	 ± 0.21419933245005549
	data : 0.11416172981262207
	model : 0.06489391326904297
			 train-loss:  2.220108778971546 	 ± 0.21216903271638177
	data : 0.11422901153564453
	model : 0.0649226188659668
			 train-loss:  2.2166306177775064 	 ± 0.21171501476988056
	data : 0.11426386833190919
	model : 0.06491870880126953
			 train-loss:  2.212963715466586 	 ± 0.21150501805194072
	data : 0.11417689323425292
	model : 0.06490979194641114
			 train-loss:  2.2129579952784946 	 ± 0.20960807803504916
	data : 0.11409392356872558
	model : 0.06484675407409668
			 train-loss:  2.220279492829975 	 ± 0.2148641189135285
	data : 0.11409115791320801
	model : 0.06476416587829589
			 train-loss:  2.2135255480634757 	 ± 0.21902217421667125
	data : 0.11400055885314941
	model : 0.06481361389160156
			 train-loss:  2.2137750225552058 	 ± 0.21716643305558406
	data : 0.11403603553771972
	model : 0.0648188591003418
			 train-loss:  2.2099284648895265 	 ± 0.21736651765769377
	data : 0.11405267715454101
	model : 0.0648472785949707
			 train-loss:  2.207585092450752 	 ± 0.21634030127774487
	data : 0.11419610977172852
	model : 0.0649336814880371
			 train-loss:  2.206886041548944 	 ± 0.2146579740670492
	data : 0.11427454948425293
	model : 0.06496391296386719
			 train-loss:  2.2157823736705478 	 ± 0.22417319474284142
	data : 0.11435894966125489
	model : 0.06486473083496094
			 train-loss:  2.216151110827923 	 ± 0.22243420226229915
	data : 0.11417279243469239
	model : 0.06478991508483886
			 train-loss:  2.2159260786496677 	 ± 0.22072387963768345
	data : 0.11408843994140624
	model : 0.0647498607635498
			 train-loss:  2.210922436280684 	 ± 0.22272905582229396
	data : 0.1139305591583252
	model : 0.06471776962280273
			 train-loss:  2.211233864969282 	 ± 0.2210751277878636
	data : 0.1139841079711914
	model : 0.06472840309143066
			 train-loss:  2.2083520854220673 	 ± 0.2207076924074745
	data : 0.11387386322021484
	model : 0.06484422683715821
			 train-loss:  2.21161877590677 	 ± 0.2207522644733005
	data : 0.11405248641967773
	model : 0.06492791175842286
			 train-loss:  2.213485475948879 	 ± 0.21971761959173955
	data : 0.11411108970642089
	model : 0.06490645408630372
			 train-loss:  2.2113099232525895 	 ± 0.21892282458809953
	data : 0.11413936614990235
	model : 0.06492290496826172
			 train-loss:  2.2106650041209326 	 ± 0.2174651188016079
	data : 0.11402654647827148
	model : 0.06486263275146484
			 train-loss:  2.2186973094940186 	 ± 0.22646974597547742
	data : 0.11403870582580566
	model : 0.06477313041687012
			 train-loss:  2.219581391360309 	 ± 0.2250611344487324
	data : 0.11406216621398926
	model : 0.06477384567260742
			 train-loss:  2.2212871074676515 	 ± 0.2240367112178281
	data : 0.1142000675201416
	model : 0.06479935646057129
			 train-loss:  2.2194077780372217 	 ± 0.22315221701652282
	data : 0.11426763534545899
	model : 0.06478996276855468
			 train-loss:  2.216483863917264 	 ± 0.22315900973164196
	data : 0.11425676345825195
	model : 0.06485238075256347
			 train-loss:  2.217434089917403 	 ± 0.22188061739697482
	data : 0.11431012153625489
	model : 0.06486797332763672
			 train-loss:  2.2173295398301716 	 ± 0.2204737708922751
	data : 0.11411538124084472
	model : 0.06485090255737305
			 train-loss:  2.217745377123356 	 ± 0.21912265021561322
	data : 0.11401796340942383
	model : 0.06487245559692383
			 train-loss:  2.2172006957324935 	 ± 0.21782032858850803
	data : 0.11389970779418945
	model : 0.06488966941833496
			 train-loss:  2.216495029809998 	 ± 0.21658121945146724
	data : 0.11405763626098633
	model : 0.06490025520324708
			 train-loss:  2.212757310235357 	 ± 0.2179170939362234
	data : 0.11403465270996094
	model : 0.06499199867248535
			 train-loss:  2.2108908622037795 	 ± 0.217282464341006
	data : 0.11422815322875976
	model : 0.06502289772033691
			 train-loss:  2.2073873043060304 	 ± 0.21837429110051948
	data : 0.1142357349395752
	model : 0.06505727767944336
			 train-loss:  2.2051189084385716 	 ± 0.21810594867778416
	data : 0.11437425613403321
	model : 0.06507720947265624
			 train-loss:  2.2061023465518295 	 ± 0.21704053959987077
	data : 0.11428413391113282
	model : 0.06505155563354492
			 train-loss:  2.204399222677404 	 ± 0.2163877272294268
	data : 0.11421656608581543
	model : 0.06496472358703613
			 train-loss:  2.204139200489173 	 ± 0.2151824572329994
	data : 0.11423311233520508
	model : 0.06492743492126465
			 train-loss:  2.203287892871433 	 ± 0.21413432040299665
	data : 0.11420340538024902
	model : 0.06490588188171387
			 train-loss:  2.2039368938613726 	 ± 0.2130434948217694
	data : 0.11418642997741699
	model : 0.0648728370666504
			 train-loss:  2.2003229817618495 	 ± 0.2146687699255925
	data : 0.1141995906829834
	model : 0.06489024162292481
			 train-loss:  2.1968062987891575 	 ± 0.21615951870040748
	data : 0.1141932487487793
	model : 0.06489849090576172
			 train-loss:  2.196896620253299 	 ± 0.21500842404951687
	data : 0.11410460472106934
	model : 0.0648993968963623
			 train-loss:  2.194803776239094 	 ± 0.21483418015903746
	data : 0.11420197486877441
	model : 0.06486530303955078
			 train-loss:  2.1953265431026616 	 ± 0.2137730551066263
	data : 0.1140099048614502
	model : 0.06477370262145996
			 train-loss:  2.1935464954867805 	 ± 0.21338223657518762
	data : 0.1139230728149414
	model : 0.06476497650146484
			 train-loss:  2.1917585049356734 	 ± 0.21301987500426905
	data : 0.11390957832336426
	model : 0.06481270790100098
			 train-loss:  2.1908989253670277 	 ± 0.2121120431334336
	data : 0.11397004127502441
	model : 0.06482701301574707
			 train-loss:  2.1961850678920745 	 ± 0.21750400141269324
	data : 0.11393465995788574
	model : 0.06483850479125977
			 train-loss:  2.1946791504869365 	 ± 0.2169478587283259
	data : 0.11404662132263184
	model : 0.06493167877197266
			 train-loss:  2.194149577150158 	 ± 0.21594736306152787
	data : 0.11406507492065429
	model : 0.06490950584411621
			 train-loss:  2.194463814346536 	 ± 0.21491995131362276
	data : 0.11406855583190918
	model : 0.06481823921203614
			 train-loss:  2.196385701115315 	 ± 0.21477171845747636
	data : 0.11403856277465821
	model : 0.0647796630859375
			 train-loss:  2.1972473814373923 	 ± 0.21392710579670596
	data : 0.1139378547668457
	model : 0.0647799015045166
			 train-loss:  2.1963492418235204 	 ± 0.21311443378276024
	data : 0.11392722129821778
	model : 0.06475944519042968
			 train-loss:  2.1941308685552294 	 ± 0.21334231155042319
	data : 0.11390748023986816
	model : 0.06480345726013184
			 train-loss:  2.1924412890716836 	 ± 0.213070312069555
	data : 0.11397790908813477
	model : 0.0649111270904541
			 train-loss:  2.1902028628445547 	 ± 0.2133625864528693
	data : 0.11411409378051758
	model : 0.06496467590332031
			 train-loss:  2.1866296432235024 	 ± 0.21564193326406975
	data : 0.11419663429260254
	model : 0.06496253013610839
			 train-loss:  2.1858008004523612 	 ± 0.21484431409005542
	data : 0.11416134834289551
	model : 0.06491456031799317
			 train-loss:  2.1871457770466805 	 ± 0.21435192645548484
	data : 0.11417102813720703
	model : 0.064862060546875
			 train-loss:  2.1871170333001464 	 ± 0.21340157571397864
	data : 0.11421546936035157
	model : 0.06479873657226562
			 train-loss:  2.184387843859823 	 ± 0.2144351521288111
	data : 0.1140477180480957
	model : 0.06477947235107422
			 train-loss:  2.1889245997304503 	 ± 0.21892682053334794
	data : 0.11402101516723633
	model : 0.06477789878845215
			 train-loss:  2.188638246264951 	 ± 0.21800275669592795
	data : 0.11409173011779786
	model : 0.0648608684539795
			 train-loss:  2.1891301014484505 	 ± 0.21713375259607487
	data : 0.11413450241088867
	model : 0.06490149497985839
			 train-loss:  2.1877459378565773 	 ± 0.21672950062379123
	data : 0.11398286819458008
	model : 0.06484169960021972
			 train-loss:  2.186664099452876 	 ± 0.21613667120988922
	data : 0.11417322158813477
	model : 0.06483874320983887
			 train-loss:  2.1872166007757188 	 ± 0.2153185875333921
	data : 0.11421575546264648
	model : 0.06484227180480957
			 train-loss:  2.1846888715570625 	 ± 0.21620746086786344
	data : 0.1142435073852539
	model : 0.06482667922973633
			 train-loss:  2.1857019623772045 	 ± 0.21560773096365068
	data : 0.11431698799133301
	model : 0.06481709480285644
			 train-loss:  2.185632486653522 	 ± 0.21473085928994248
	data : 0.11445527076721192
	model : 0.06487517356872559
			 train-loss:  2.1852318740660146 	 ± 0.2139094030763033
	data : 0.11429481506347657
	model : 0.06484894752502442
			 train-loss:  2.1848077564239503 	 ± 0.21310438629619796
	data : 0.11426424980163574
	model : 0.06483850479125977
			 train-loss:  2.1892802034105574 	 ± 0.2180674271287033
	data : 0.1142463207244873
	model : 0.06478891372680665
			 train-loss:  2.1872805582256767 	 ± 0.21836388831777018
	data : 0.1139401912689209
	model : 0.06472806930541992
			 train-loss:  2.1892986511811614 	 ± 0.21869499019789942
	data : 0.11392803192138672
	model : 0.06472649574279785
			 train-loss:  2.1881527189136474 	 ± 0.21823113323032714
	data : 0.11397981643676758
	model : 0.06476001739501953
			 train-loss:  2.187987069900219 	 ± 0.2173983035624335
	data : 0.11408252716064453
	model : 0.06476988792419433
			 train-loss:  2.187993612908225 	 ± 0.21656696230118716
	data : 0.11410765647888184
	model : 0.06480951309204101
			 train-loss:  2.1861291500655087 	 ± 0.21679788215838186
	data : 0.11433215141296386
	model : 0.06489405632019044
			 train-loss:  2.1848617451531545 	 ± 0.2164716192021984
	data : 0.1143524169921875
	model : 0.06489739418029786
			 train-loss:  2.1849331206350184 	 ± 0.21566394755444157
	data : 0.11436553001403808
	model : 0.06481494903564453
			 train-loss:  2.1863512807422216 	 ± 0.21548993171393802
	data : 0.11405792236328124
	model : 0.06473894119262695
			 train-loss:  2.185818283873446 	 ± 0.21478552491923555
	data : 0.11398286819458008
	model : 0.06479640007019043
			 train-loss:  2.189982065319145 	 ± 0.21944003307521662
	data : 0.11402769088745117
	model : 0.06476216316223145
			 train-loss:  2.188661189182945 	 ± 0.21918944407536786
	data : 0.11388888359069824
	model : 0.06476902961730957
			 train-loss:  2.188746954039704 	 ± 0.21840189351606695
	data : 0.11387362480163574
	model : 0.06486330032348633
			 train-loss:  2.1901526391506194 	 ± 0.21825062272837525
	data : 0.11404337882995605
	model : 0.06494240760803223
			 train-loss:  2.192945920829232 	 ± 0.219972381016947
	data : 0.11407089233398438
	model : 0.06488380432128907
			 train-loss:  2.1921284878757636 	 ± 0.21941126826061044
	data : 0.11407861709594727
	model : 0.06483874320983887
			 train-loss:  2.1915341815748417 	 ± 0.2187574148612895
	data : 0.11415686607360839
	model : 0.0648162841796875
			 train-loss:  2.193277376393477 	 ± 0.21899091223242284
	data : 0.1141049861907959
	model : 0.06480011940002442
			 train-loss:  2.1945049359880646 	 ± 0.21873105756223374
	data : 0.11404209136962891
	model : 0.06480445861816406
			 train-loss:  2.1980064429648936 	 ± 0.2220210841000664
	data : 0.11408710479736328
	model : 0.06478271484375
			 train-loss:  2.1970040530574564 	 ± 0.22159587423981494
	data : 0.11400752067565918
	model : 0.06485571861267089
			 train-loss:  2.1956512428618766 	 ± 0.22145420922033718
	data : 0.11396541595458984
	model : 0.0648998737335205
			 train-loss:  2.196867545979135 	 ± 0.22120528083470997
	data : 0.11401042938232422
	model : 0.06492829322814941
			 train-loss:  2.199066958427429 	 ± 0.22209533826694228
	data : 0.1140143871307373
	model : 0.0648488998413086
			 train-loss:  2.200056641307098 	 ± 0.2216903146547263
	data : 0.11384849548339844
	model : 0.06485228538513184
			 train-loss:  2.1960863023996353 	 ± 0.22628204743837319
	data : 0.11382207870483399
	model : 0.0648313045501709
			 train-loss:  2.194577947161556 	 ± 0.22630669846273618
	data : 0.11395583152770997
	model : 0.06481809616088867
			 train-loss:  2.193332679085917 	 ± 0.22609602961021757
	data : 0.11393270492553711
	model : 0.0648350715637207
			 train-loss:  2.1938231552800826 	 ± 0.2254476862440037
	data : 0.11403684616088867
	model : 0.0649404525756836
			 train-loss:  2.1936377111153726 	 ± 0.22473579530871535
	data : 0.11422114372253418
	model : 0.06496095657348633
			 train-loss:  2.1907442800558297 	 ± 0.22691519439638475
	data : 0.11417694091796875
	model : 0.06497955322265625
			 train-loss:  2.1918166634402696 	 ± 0.22659472000536135
	data : 0.11419281959533692
	model : 0.06487135887145996
			 train-loss:  2.1930791267059133 	 ± 0.22643777089387893
	data : 0.11410126686096192
	model : 0.06486005783081054
			 train-loss:  2.1938885301351547 	 ± 0.22595965896313033
	data : 0.11405758857727051
	model : 0.06485419273376465
			 train-loss:  2.1957063482414862 	 ± 0.22642736697452462
	data : 0.114036226272583
	model : 0.06483950614929199
			 train-loss:  2.1959498899954335 	 ± 0.22574858661060956
	data : 0.1141444206237793
	model : 0.06487102508544922
			 train-loss:  2.1955025313090695 	 ± 0.22512705850962295
	data : 0.11410655975341796
	model : 0.06498661041259765
			 train-loss:  2.196218226014114 	 ± 0.2246255693296679
	data : 0.11414690017700195
	model : 0.06494612693786621
			 train-loss:  2.1958790677966493 	 ± 0.22398596648082572
	data : 0.11415729522705079
	model : 0.06489100456237792
			 train-loss:  2.195869058011526 	 ± 0.22331032785078
	data : 0.1141585350036621
	model : 0.06491999626159668
			 train-loss:  2.1938743705521087 	 ± 0.22411910200462318
	data : 0.11412677764892579
	model : 0.06489901542663574
			 train-loss:  2.194593396924791 	 ± 0.22364419587219433
	data : 0.11401982307434082
	model : 0.06488380432128907
			 train-loss:  2.1932643793038364 	 ± 0.22364593835629817
	data : 0.11405935287475585
	model : 0.06488308906555176
			 train-loss:  2.1925135956091037 	 ± 0.22320068636019139
	data : 0.1141014575958252
	model : 0.06495833396911621
			 train-loss:  2.194252445683842 	 ± 0.22369895348898045
	data : 0.11409697532653809
	model : 0.06492552757263184
			 train-loss:  2.1945300788380377 	 ± 0.2230772627697531
	data : 0.11404986381530761
	model : 0.06490120887756348
			 train-loss:  2.1936193994014936 	 ± 0.22275201693229224
	data : 0.11406154632568359
	model : 0.06484761238098144
			 train-loss:  2.1931819799302636 	 ± 0.22218550486135583
	data : 0.11407699584960937
	model : 0.06487808227539063
			 train-loss:  2.1917787565503803 	 ± 0.22232165234845824
	data : 0.11408300399780273
	model : 0.06483807563781738
			 train-loss:  2.190624549984932 	 ± 0.2222143471649246
	data : 0.1140380859375
	model : 0.06486525535583496
			 train-loss:  2.191236819251109 	 ± 0.22173456029030947
	data : 0.11413011550903321
	model : 0.0648726463317871
			 train-loss:  2.1910325985276295 	 ± 0.22112752545813832
	data : 0.11423535346984863
	model : 0.0649078369140625
			 train-loss:  2.1893369432268197 	 ± 0.22166643121719862
	data : 0.11425843238830566
	model : 0.06490192413330079
			 train-loss:  2.191604320208232 	 ± 0.22312164310681554
	data : 0.11424040794372559
	model : 0.06486811637878417
			 train-loss:  2.189844311271583 	 ± 0.22375387283237488
	data : 0.11409296989440917
	model : 0.0648423194885254
			 train-loss:  2.188947844636309 	 ± 0.22346402374416177
	data : 0.11409225463867187
	model : 0.06482281684875488
			 train-loss:  2.186899178015078 	 ± 0.22455991409615583
	data : 0.11398820877075196
	model : 0.06491813659667969
			 train-loss:  2.1867029802954714 	 ± 0.22396459254207587
	data : 0.11399655342102051
	model : 0.06495199203491211
			 train-loss:  2.18907813188192 	 ± 0.22567013608005268
	data : 0.11394405364990234
	model : 0.06499671936035156
			 train-loss:  2.188655638566581 	 ± 0.22513602954419373
	data : 0.11413750648498536
	model : 0.06502809524536132
			 train-loss:  2.18897584734116 	 ± 0.22457571928338205
	data : 0.11411633491516113
	model : 0.06502904891967773
			 train-loss:  2.18924369393511 	 ± 0.22400759384668975
	data : 0.11417875289916993
	model : 0.06493463516235351
			 train-loss:  2.190210165170135 	 ± 0.22380685232766248
	data : 0.11396684646606445
	model : 0.064815092086792
			 train-loss:  2.1901357205290544 	 ± 0.2232194561472835
	data : 0.11395812034606934
	model : 0.06478700637817383
			 train-loss:  2.191342753889673 	 ± 0.22325516401204884
	data : 0.11398262977600097
	model : 0.06476316452026368
			 train-loss:  2.190894595657786 	 ± 0.22275913344647374
	data : 0.11384463310241699
	model : 0.0647646427154541
			 train-loss:  2.189534642535788 	 ± 0.22297897513787943
	data : 0.11385703086853027
	model : 0.0648836612701416
			 train-loss:  2.191549540180521 	 ± 0.22415815945742595
	data : 0.11403799057006836
	model : 0.06499280929565429
			 train-loss:  2.1931533416112265 	 ± 0.2246958096471443
	data : 0.11400008201599121
	model : 0.06499505043029785
			 train-loss:  2.1925795607420864 	 ± 0.22426505023187607
	data : 0.11377792358398438
	model : 0.06493668556213379
			 train-loss:  2.193800641195423 	 ± 0.22434739241245438
	data : 0.11390247344970703
	model : 0.06493639945983887
			 train-loss:  2.194224695364634 	 ± 0.2238592786417957
	data : 0.11387271881103515
	model : 0.06485986709594727
			 train-loss:  2.193767059388472 	 ± 0.22338894311671126
	data : 0.11387386322021484
	model : 0.06483683586120606
			 train-loss:  2.1943141907453536 	 ± 0.22296340058153527
	data : 0.11397323608398438
	model : 0.06484308242797851
			 train-loss:  2.1940249809578285 	 ± 0.22244567809447077
	data : 0.11409482955932618
	model : 0.0649024486541748
			 train-loss:  2.1928090411837737 	 ± 0.22256302327354407
	data : 0.11418638229370118
	model : 0.06496148109436035
			 train-loss:  2.1961903536848246 	 ± 0.22715591428868856
	data : 0.1141366958618164
	model : 0.06491422653198242
			 train-loss:  2.1973814321499243 	 ± 0.22723304940066735
	data : 0.11391644477844239
	model : 0.06483230590820313
			 train-loss:  2.196635050889922 	 ± 0.2269286820065211
	data : 0.11391201019287109
	model : 0.06482343673706055
			 train-loss:  2.1965963944648075 	 ± 0.22637789076633005
	data : 0.11405572891235352
	model : 0.06486964225769043
			 train-loss:  2.1967988417344393 	 ± 0.22584911448263753
	data : 0.11397008895874024
	model : 0.06483664512634277
			 train-loss:  2.1949020010920672 	 ± 0.22695237278366234
	data : 0.11398158073425294
	model : 0.06488165855407715
			 train-loss:  2.1945691867308184 	 ± 0.22645964745007766
	data : 0.11413688659667968
	model : 0.0649796485900879
			 train-loss:  2.1941337034815835 	 ± 0.22600751862740995
	data : 0.11413483619689942
	model : 0.06502928733825683
			 train-loss:  2.1935801647285715 	 ± 0.22561396477304838
	data : 0.11411199569702149
	model : 0.06498231887817382
			 train-loss:  2.1920748420481413 	 ± 0.22614085026847616
	data : 0.11403741836547851
	model : 0.06503124237060547
			 train-loss:  2.1918830479814413 	 ± 0.22562666101022827
	data : 0.11395392417907715
	model : 0.06504502296447753
			 train-loss:  2.192053814914739 	 ± 0.22511267517506964
	data : 0.11402788162231445
	model : 0.06501221656799316
			 train-loss:  2.1914538217145343 	 ± 0.22475999148248843
	data : 0.11401972770690919
	model : 0.06501998901367187
			 train-loss:  2.1926356300159737 	 ± 0.2249076760750019
	data : 0.1140315055847168
	model : 0.06500558853149414
			 train-loss:  2.192905540290516 	 ± 0.22442391846371504
	data : 0.11407852172851562
	model : 0.06495981216430664
			 train-loss:  2.193315852672682 	 ± 0.22399015904994443
	data : 0.11427721977233887
	model : 0.06492242813110352
			 train-loss:  2.1929811538626613 	 ± 0.22353281281447734
	data : 0.11419239044189453
	model : 0.06486983299255371
			 train-loss:  2.193582010269165 	 ± 0.223201391735396
	data : 0.1141425609588623
	model : 0.06487431526184081
			 train-loss:  2.194045571719899 	 ± 0.22280195732773606
	data : 0.11392536163330078
	model : 0.06483588218688965
			 train-loss:  2.1938411244400986 	 ± 0.22232036093877222
	data : 0.11408305168151855
	model : 0.06477179527282714
			 train-loss:  2.193029707322741 	 ± 0.22215054443471044
	data : 0.11403579711914062
	model : 0.06473002433776856
			 train-loss:  2.192140080034733 	 ± 0.2220518822717391
	data : 0.11403656005859375
	model : 0.06472506523132324
			 train-loss:  2.1929496659172907 	 ± 0.22188896365281374
	data : 0.11406044960021973
	model : 0.06451945304870606
			 train-loss:  2.1922172635002473 	 ± 0.22166991783893622
	data : 0.11435623168945312
	model : 0.06438655853271484
			 train-loss:  2.1924885390613573 	 ± 0.2212187128681429
	data : 0.11419839859008789
	model : 0.06418280601501465
			 train-loss:  2.1923603509601794 	 ± 0.22074150020311478
	data : 0.11428370475769042
	model : 0.06403574943542481
			 train-loss:  2.192959279472651 	 ± 0.2204445875620169
	data : 0.11438884735107421
	model : 0.06388654708862304
			 train-loss:  2.193005869699561 	 ± 0.21996596806491847
	data : 0.11456184387207032
	model : 0.06387877464294434
			 train-loss:  2.193893727802095 	 ± 0.21990196742477489
	data : 0.11450905799865722
	model : 0.06392378807067871
			 train-loss:  2.1928282405795723 	 ± 0.22002428554137163
	data : 0.11466660499572753
	model : 0.06400666236877442
			 train-loss:  2.1949043575786216 	 ± 0.221817257621248
	data : 0.11464390754699708
	model : 0.0639615535736084
			 train-loss:  2.1937098182164707 	 ± 0.2220925490696907
	data : 0.11454181671142578
	model : 0.06390910148620606
			 train-loss:  2.1946087061090673 	 ± 0.22204566687505894
	data : 0.11447410583496094
	model : 0.06391386985778809
			 train-loss:  2.1948103637008343 	 ± 0.2215962955564615
	data : 0.11448516845703124
	model : 0.06385622024536133
			 train-loss:  2.193452030294555 	 ± 0.22211069513264303
	data : 0.11450614929199218
	model : 0.0638577938079834
			 train-loss:  2.1923574139090145 	 ± 0.22228326197927464
	data : 0.11460561752319336
	model : 0.06389236450195312
			 train-loss:  2.1928333029088614 	 ± 0.22193920953515903
	data : 0.11469306945800781
	model : 0.0639521598815918
			 train-loss:  2.19303303360939 	 ± 0.22149787684841746
	data : 0.11459121704101563
	model : 0.0638615608215332
			 train-loss:  2.1920006611535165 	 ± 0.22161571659804738
	data : 0.11446146965026856
	model : 0.06389575004577637
			 train-loss:  2.1903761551399863 	 ± 0.22259061336515346
	data : 0.11448636054992675
	model : 0.06392927169799804
			 train-loss:  2.1911270515418346 	 ± 0.22243906210665648
	data : 0.11444830894470215
	model : 0.06392498016357422
			 train-loss:  2.1905248209101256 	 ± 0.2221811978239708
	data : 0.11446070671081543
	model : 0.06394109725952149
			 train-loss:  2.1894161307081883 	 ± 0.22240260857544528
	data : 0.11461062431335449
	model : 0.06406960487365723
			 train-loss:  2.1899409231131637 	 ± 0.22210206269008187
	data : 0.11471791267395019
	model : 0.06406469345092773
			 train-loss:  2.1913658293635256 	 ± 0.2227758523808572
	data : 0.11448907852172852
	model : 0.06393876075744628
			 train-loss:  2.1928493539171834 	 ± 0.22354545820558805
	data : 0.11450028419494629
	model : 0.06396098136901855
			 train-loss:  2.194915152457823 	 ± 0.22545559358768683
	data : 0.11449189186096191
	model : 0.06396255493164063
			 train-loss:  2.1943427243232727 	 ± 0.22518546711453943
	data : 0.11446213722229004
	model : 0.06391711235046386
			 train-loss:  2.194090938663103 	 ± 0.22477170135353194
	data : 0.1144371509552002
	model : 0.06391520500183105
			 train-loss:  2.194324172205395 	 ± 0.22435571356830708
	data : 0.11461439132690429
	model : 0.06395878791809081
			 train-loss:  2.1927590186416865 	 ± 0.2252861681298832
	data : 0.11451559066772461
	model : 0.06391396522521972
			 train-loss:  2.193114570276005 	 ± 0.22491336711252408
	data : 0.11455130577087402
	model : 0.0638763427734375
			 train-loss:  2.194541476287094 	 ± 0.22562093338261036
	data : 0.114555025100708
	model : 0.06384835243225098
			 train-loss:  2.199030500371009 	 ± 0.23631451431569847
	data : 0.11450557708740235
	model : 0.05542712211608887
#epoch  74    val-loss:  2.450022019838032  train-loss:  2.199030500371009  lr:  1.9073486328125e-08
			 train-loss:  2.652003765106201 	 ± 0.0
	data : 5.619633436203003
	model : 0.07134532928466797
			 train-loss:  2.6172595024108887 	 ± 0.0347442626953125
	data : 2.8744982481002808
	model : 0.06813669204711914
			 train-loss:  2.411342144012451 	 ± 0.2925896321467008
	data : 1.9543254375457764
	model : 0.06728609402974446
			 train-loss:  2.3616851568222046 	 ± 0.267589179079131
	data : 1.4940913915634155
	model : 0.06663912534713745
			 train-loss:  2.285643196105957 	 ± 0.28357132112835376
	data : 1.2180704116821288
	model : 0.06620974540710449
			 train-loss:  2.2490257819493613 	 ± 0.27150461058634884
	data : 0.11680974960327148
	model : 0.06481595039367676
			 train-loss:  2.2298017910548618 	 ± 0.2557370631014164
	data : 0.11354479789733887
	model : 0.0647456169128418
			 train-loss:  2.2078678905963898 	 ± 0.24615835693416088
	data : 0.11356511116027831
	model : 0.06454787254333497
			 train-loss:  2.180387099583944 	 ± 0.24475054249955247
	data : 0.11365876197814942
	model : 0.0646287441253662
			 train-loss:  2.1540209531784056 	 ± 0.24529392233593364
	data : 0.11355853080749512
	model : 0.06471114158630371
			 train-loss:  2.160207293250344 	 ± 0.2346953289485394
	data : 0.11366195678710937
	model : 0.06483283042907714
			 train-loss:  2.1684656143188477 	 ± 0.226366821435143
	data : 0.11388125419616699
	model : 0.06481208801269531
			 train-loss:  2.181340217590332 	 ± 0.22201197092220484
	data : 0.11387405395507813
	model : 0.06478428840637207
			 train-loss:  2.1772113697869435 	 ± 0.21445341184830635
	data : 0.11375179290771484
	model : 0.06466598510742187
			 train-loss:  2.149939775466919 	 ± 0.23094719336767178
	data : 0.11381921768188477
	model : 0.06462926864624023
			 train-loss:  2.140856921672821 	 ± 0.22636373768186427
	data : 0.11378612518310546
	model : 0.06461129188537598
			 train-loss:  2.1500012594110824 	 ± 0.22263039960007056
	data : 0.11378421783447265
	model : 0.06471214294433594
			 train-loss:  2.130026956399282 	 ± 0.23150218177163565
	data : 0.11382255554199219
	model : 0.06479525566101074
			 train-loss:  2.1446334249094914 	 ± 0.23369390848655056
	data : 0.11404070854187012
	model : 0.06482090950012206
			 train-loss:  2.1423759877681734 	 ± 0.22798908997368114
	data : 0.11401157379150391
	model : 0.06481962203979492
			 train-loss:  2.129180669784546 	 ± 0.23018723206802782
	data : 0.1140364170074463
	model : 0.06478114128112793
			 train-loss:  2.1180517456748267 	 ± 0.23060488263576107
	data : 0.11396360397338867
	model : 0.06473674774169921
			 train-loss:  2.120316609092381 	 ± 0.22578607059202097
	data : 0.11383514404296875
	model : 0.06477541923522949
			 train-loss:  2.122521311044693 	 ± 0.22128489882892344
	data : 0.11370081901550293
	model : 0.06485142707824706
			 train-loss:  2.112520794868469 	 ± 0.22228039491309107
	data : 0.11376323699951171
	model : 0.06489801406860352
			 train-loss:  2.134937135072855 	 ± 0.24509294542865098
	data : 0.11379761695861816
	model : 0.06493735313415527
			 train-loss:  2.12870223875399 	 ± 0.24260346015289863
	data : 0.11378669738769531
	model : 0.06501965522766114
			 train-loss:  2.1354647236210957 	 ± 0.24080939692325282
	data : 0.11385045051574708
	model : 0.06498160362243652
			 train-loss:  2.143881818343853 	 ± 0.2407763875153628
	data : 0.11389946937561035
	model : 0.06495528221130371
			 train-loss:  2.1574060400327046 	 ± 0.24767934731927774
	data : 0.11391019821166992
	model : 0.06495904922485352
			 train-loss:  2.1628781249446254 	 ± 0.24548828406500142
	data : 0.11381993293762208
	model : 0.06495990753173828
			 train-loss:  2.1694907285273075 	 ± 0.24441103570248154
	data : 0.1138193130493164
	model : 0.06486868858337402
			 train-loss:  2.163413492116061 	 ± 0.24312219020880613
	data : 0.11370129585266113
	model : 0.0648590087890625
			 train-loss:  2.1541494657011593 	 ± 0.24536105876073003
	data : 0.1138117790222168
	model : 0.06486673355102539
			 train-loss:  2.148470640182495 	 ± 0.24408698987061042
	data : 0.11383934020996093
	model : 0.0648378849029541
			 train-loss:  2.1509694192144604 	 ± 0.24112660123442103
	data : 0.11398682594299317
	model : 0.06482090950012206
			 train-loss:  2.153677830824981 	 ± 0.23840031283429677
	data : 0.11406388282775878
	model : 0.06487441062927246
			 train-loss:  2.1532709284832605 	 ± 0.23525557377144574
	data : 0.11423516273498535
	model : 0.0648740291595459
			 train-loss:  2.1532034323765683 	 ± 0.23222026354435676
	data : 0.11412134170532226
	model : 0.06486077308654785
			 train-loss:  2.150621956586838 	 ± 0.22986515874194777
	data : 0.11414241790771484
	model : 0.06485300064086914
			 train-loss:  2.146473448450972 	 ± 0.2285556048335441
	data : 0.1140437126159668
	model : 0.06483936309814453
			 train-loss:  2.1488675503503707 	 ± 0.22633804643548913
	data : 0.11393027305603028
	model : 0.06481132507324219
			 train-loss:  2.1611578353615695 	 ± 0.23744828420606776
	data : 0.11388540267944336
	model : 0.06481766700744629
			 train-loss:  2.1642879356037485 	 ± 0.2356301749850234
	data : 0.11388692855834961
	model : 0.06480679512023926
			 train-loss:  2.162576013141208 	 ± 0.23327390840706969
	data : 0.11383438110351562
	model : 0.06485657691955567
			 train-loss:  2.160276656565459 	 ± 0.2312394020899027
	data : 0.11395950317382812
	model : 0.06488032341003418
			 train-loss:  2.15630818681514 	 ± 0.23034410945723446
	data : 0.11406712532043457
	model : 0.06487674713134765
			 train-loss:  2.158870774010817 	 ± 0.22860810865571862
	data : 0.11398077011108398
	model : 0.06480917930603028
			 train-loss:  2.157875180244446 	 ± 0.22636846210763423
	data : 0.11390457153320313
	model : 0.06484808921813964
			 train-loss:  2.1632133507728577 	 ± 0.22718744285097667
	data : 0.11388673782348632
	model : 0.06480474472045898
			 train-loss:  2.1677191140604952 	 ± 0.22719416251860564
	data : 0.11383304595947266
	model : 0.0648223876953125
			 train-loss:  2.164872318506241 	 ± 0.22591561644345573
	data : 0.11384215354919433
	model : 0.06482930183410644
			 train-loss:  2.1681555824459724 	 ± 0.22502319414452776
	data : 0.11394658088684081
	model : 0.06488971710205078
			 train-loss:  2.1647085547447205 	 ± 0.2243378938277821
	data : 0.11404452323913575
	model : 0.06482672691345215
			 train-loss:  2.166118203509938 	 ± 0.22253033331199834
	data : 0.1138235092163086
	model : 0.06478776931762695
			 train-loss:  2.1707335902111873 	 ± 0.22317496880152993
	data : 0.11372475624084473
	model : 0.06475620269775391
			 train-loss:  2.1724994078017117 	 ± 0.22160296200768745
	data : 0.11375770568847657
	model : 0.06476454734802246
			 train-loss:  2.1700561026047014 	 ± 0.2204573893189018
	data : 0.11383299827575684
	model : 0.0647892951965332
			 train-loss:  2.169383794574414 	 ± 0.21864108220208622
	data : 0.11383223533630371
	model : 0.06484856605529785
			 train-loss:  2.1727263311545055 	 ± 0.21832629542947116
	data : 0.11409602165222169
	model : 0.06490502357482911
			 train-loss:  2.1781466104945197 	 ± 0.2205622838130474
	data : 0.11419377326965333
	model : 0.06487927436828614
			 train-loss:  2.176716475717483 	 ± 0.21906127634337463
	data : 0.11413898468017578
	model : 0.06487188339233399
			 train-loss:  2.174966091201419 	 ± 0.21775235808590696
	data : 0.11401209831237794
	model : 0.0648879051208496
			 train-loss:  2.176032269373536 	 ± 0.21621014642160535
	data : 0.11410636901855468
	model : 0.0648686408996582
			 train-loss:  2.1727166671019336 	 ± 0.21617403043500039
	data : 0.11405482292175292
	model : 0.06488752365112305
			 train-loss:  2.1741921269532405 	 ± 0.2148596439344696
	data : 0.11409163475036621
	model : 0.06490302085876465
			 train-loss:  2.1814928072602 	 ± 0.22134461206787273
	data : 0.11407723426818847
	model : 0.064841890335083
			 train-loss:  2.176954101113712 	 ± 0.2228298358614521
	data : 0.11393613815307617
	model : 0.0647465705871582
			 train-loss:  2.1727762654207754 	 ± 0.22387589919472242
	data : 0.1138197898864746
	model : 0.06473822593688965
			 train-loss:  2.1780651041439603 	 ± 0.22657110915369383
	data : 0.11396055221557617
	model : 0.06470842361450195
			 train-loss:  2.181549755620285 	 ± 0.2268511462344381
	data : 0.11387181282043457
	model : 0.06473255157470703
			 train-loss:  2.1814018736282983 	 ± 0.22527372903914322
	data : 0.11382231712341309
	model : 0.0647850513458252
			 train-loss:  2.184700667041622 	 ± 0.22546968351012162
	data : 0.11407270431518554
	model : 0.0648582935333252
			 train-loss:  2.183509306327717 	 ± 0.2241722758092658
	data : 0.11404037475585938
	model : 0.0648371696472168
			 train-loss:  2.183075623512268 	 ± 0.2227040287683758
	data : 0.1138124942779541
	model : 0.06476306915283203
			 train-loss:  2.1816066142759825 	 ± 0.22159950433994127
	data : 0.11371884346008301
	model : 0.0647341251373291
			 train-loss:  2.1770103844729336 	 ± 0.22377247507851358
	data : 0.11379432678222656
	model : 0.06475944519042968
			 train-loss:  2.1750955214867225 	 ± 0.22296744295369536
	data : 0.11373181343078613
	model : 0.0647695541381836
			 train-loss:  2.17628324786319 	 ± 0.22179994932831543
	data : 0.1137974739074707
	model : 0.0647963047027588
			 train-loss:  2.1718046009540557 	 ± 0.22397518004416678
	data : 0.11397876739501953
	model : 0.06486778259277344
			 train-loss:  2.1734837514382823 	 ± 0.2230944317139885
	data : 0.11413264274597168
	model : 0.0648923397064209
			 train-loss:  2.1762236182282613 	 ± 0.22309687803576428
	data : 0.11407608985900879
	model : 0.06479825973510742
			 train-loss:  2.1787985548915634 	 ± 0.22297137983968884
	data : 0.11401901245117188
	model : 0.06480269432067871
			 train-loss:  2.1816425011271523 	 ± 0.22314946286820977
	data : 0.11404252052307129
	model : 0.06480803489685058
			 train-loss:  2.17952239373151 	 ± 0.2226823276941232
	data : 0.11392078399658204
	model : 0.06481714248657226
			 train-loss:  2.178302296372347 	 ± 0.22166947323489952
	data : 0.1140059471130371
	model : 0.06485605239868164
			 train-loss:  2.1808971372143975 	 ± 0.22170163122936748
	data : 0.1141364574432373
	model : 0.06501755714416504
			 train-loss:  2.1796930703249844 	 ± 0.2207242687950404
	data : 0.11422944068908691
	model : 0.06502928733825683
			 train-loss:  2.1803075543950112 	 ± 0.21955642564234135
	data : 0.11418838500976562
	model : 0.0650062084197998
			 train-loss:  2.180227009455363 	 ± 0.2183345828057923
	data : 0.11420011520385742
	model : 0.0649299144744873
			 train-loss:  2.176914922483675 	 ± 0.21939333812631476
	data : 0.11411614418029785
	model : 0.06488204002380371
			 train-loss:  2.175173922725346 	 ± 0.21882887281627603
	data : 0.11411705017089843
	model : 0.06478848457336425
			 train-loss:  2.172801422816451 	 ± 0.21883559371088906
	data : 0.11418161392211915
	model : 0.06477346420288085
			 train-loss:  2.1730751357180007 	 ± 0.21768446635054647
	data : 0.1141655445098877
	model : 0.06482100486755371
			 train-loss:  2.1727952279542624 	 ± 0.21655273281487625
	data : 0.11434259414672851
	model : 0.06489591598510742
			 train-loss:  2.1741557866334915 	 ± 0.21582968285631096
	data : 0.11425204277038574
	model : 0.06490278244018555
			 train-loss:  2.175316962999167 	 ± 0.21501548844427606
	data : 0.11417579650878906
	model : 0.06487669944763183
			 train-loss:  2.178979832298902 	 ± 0.2169362117113831
	data : 0.11400585174560547
	model : 0.06478533744812012
			 train-loss:  2.178023114348903 	 ± 0.21604548916513042
	data : 0.11409482955932618
	model : 0.06474580764770507
			 train-loss:  2.177068636417389 	 ± 0.21517223017566053
	data : 0.11389703750610351
	model : 0.0647730827331543
			 train-loss:  2.173718774672782 	 ± 0.21670911192508696
	data : 0.11389970779418945
	model : 0.0648378849029541
			 train-loss:  2.1761288210457446 	 ± 0.21700013796030304
	data : 0.11399607658386231
	model : 0.06488451957702637
			 train-loss:  2.1739988141846887 	 ± 0.21701302107816278
	data : 0.11418662071228028
	model : 0.06496953964233398
			 train-loss:  2.174036225447288 	 ± 0.21596750285816346
	data : 0.11417932510375976
	model : 0.06499481201171875
			 train-loss:  2.174372200738816 	 ± 0.21496393321521276
	data : 0.11424612998962402
	model : 0.06496901512145996
			 train-loss:  2.1757028732659682 	 ± 0.21438161385343163
	data : 0.1142085075378418
	model : 0.06495022773742676
			 train-loss:  2.177455630257865 	 ± 0.21413920050207616
	data : 0.11401987075805664
	model : 0.06498551368713379
			 train-loss:  2.1763233608669705 	 ± 0.21346706060643902
	data : 0.11393585205078124
	model : 0.06497530937194824
			 train-loss:  2.176042900172942 	 ± 0.21250558650935947
	data : 0.11392030715942383
	model : 0.06496248245239258
			 train-loss:  2.176126859404824 	 ± 0.2115392628028222
	data : 0.1139707088470459
	model : 0.06497535705566407
			 train-loss:  2.181218877568975 	 ± 0.21725070360107426
	data : 0.11411166191101074
	model : 0.06492953300476074
			 train-loss:  2.1829669837440764 	 ± 0.2170614217788661
	data : 0.11427450180053711
	model : 0.06488127708435058
			 train-loss:  2.182199790414456 	 ± 0.21625131135270298
	data : 0.1142427921295166
	model : 0.06491012573242187
			 train-loss:  2.182493577923691 	 ± 0.2153234004154331
	data : 0.11413712501525879
	model : 0.06486978530883789
			 train-loss:  2.181435844172602 	 ± 0.21468242447248548
	data : 0.11407036781311035
	model : 0.06486191749572753
			 train-loss:  2.1829088104182275 	 ± 0.21433790086626195
	data : 0.11401844024658203
	model : 0.06484541893005372
			 train-loss:  2.1838311150542693 	 ± 0.21365101170904058
	data : 0.11398677825927735
	model : 0.06485295295715332
			 train-loss:  2.183863486273814 	 ± 0.21274407277992602
	data : 0.11405105590820312
	model : 0.06484541893005372
			 train-loss:  2.187098176539445 	 ± 0.21474255311813367
	data : 0.11413393020629883
	model : 0.0648871898651123
			 train-loss:  2.1863832414150237 	 ± 0.21398808959298174
	data : 0.11419610977172852
	model : 0.06488981246948242
			 train-loss:  2.1884159549208713 	 ± 0.214262213609014
	data : 0.11411609649658203
	model : 0.06490497589111328
			 train-loss:  2.1904009463357146 	 ± 0.21449653090881105
	data : 0.11390414237976074
	model : 0.06483831405639648
			 train-loss:  2.1918639516442773 	 ± 0.2142331293501798
	data : 0.11370177268981933
	model : 0.0648200511932373
			 train-loss:  2.1919937383744026 	 ± 0.2133723925562435
	data : 0.11370706558227539
	model : 0.06480188369750976
			 train-loss:  2.1926038055419923 	 ± 0.2126257421941348
	data : 0.1136960506439209
	model : 0.06480035781860352
			 train-loss:  2.1951493876320973 	 ± 0.21368410697424417
	data : 0.1138223648071289
	model : 0.06480002403259277
			 train-loss:  2.1954160618969776 	 ± 0.21286221707345196
	data : 0.11399993896484376
	model : 0.06486949920654297
			 train-loss:  2.194522736594081 	 ± 0.21226795890502936
	data : 0.11412334442138672
	model : 0.06487302780151367
			 train-loss:  2.198961769887643 	 ± 0.21732612385444097
	data : 0.11406798362731933
	model : 0.0648193359375
			 train-loss:  2.1992046466240516 	 ± 0.21650621482458954
	data : 0.11393136978149414
	model : 0.06481738090515136
			 train-loss:  2.201094012224037 	 ± 0.21675142072956527
	data : 0.11392631530761718
	model : 0.06483898162841797
			 train-loss:  2.2062478950529387 	 ± 0.2238413477135803
	data : 0.11394782066345215
	model : 0.06481719017028809
			 train-loss:  2.2054824094126997 	 ± 0.22317161068966868
	data : 0.11398987770080567
	model : 0.06481699943542481
			 train-loss:  2.205466512423843 	 ± 0.22233739695987467
	data : 0.11409735679626465
	model : 0.06490139961242676
			 train-loss:  2.2063155863020154 	 ± 0.2217303431203286
	data : 0.11423754692077637
	model : 0.06487550735473632
			 train-loss:  2.205523231450249 	 ± 0.2211054026059783
	data : 0.11425113677978516
	model : 0.06486401557922364
			 train-loss:  2.205267848759672 	 ± 0.22031710155164358
	data : 0.11405982971191406
	model : 0.0648122787475586
			 train-loss:  2.2040936704994976 	 ± 0.2199471981629592
	data : 0.11426386833190919
	model : 0.06484355926513671
			 train-loss:  2.203922767433331 	 ± 0.21916378886062024
	data : 0.1142845630645752
	model : 0.06482863426208496
			 train-loss:  2.2033953411238536 	 ± 0.2184681715323826
	data : 0.11471362113952636
	model : 0.06488838195800781
			 train-loss:  2.2030268239636794 	 ± 0.21773574759118336
	data : 0.11526098251342773
	model : 0.06489439010620117
			 train-loss:  2.202132629676604 	 ± 0.21722737364383202
	data : 0.11544179916381836
	model : 0.06499409675598145
			 train-loss:  2.2011626583712918 	 ± 0.21677487773406498
	data : 0.11522850990295411
	model : 0.06495680809020996
			 train-loss:  2.199620708823204 	 ± 0.2168064040017981
	data : 0.11515841484069825
	model : 0.06498851776123046
			 train-loss:  2.2008553932453023 	 ± 0.21656491972620331
	data : 0.11461272239685058
	model : 0.06495175361633301
			 train-loss:  2.2021267985644406 	 ± 0.21636431648595092
	data : 0.1140944480895996
	model : 0.06496262550354004
			 train-loss:  2.202392273208722 	 ± 0.21565098598645188
	data : 0.11403036117553711
	model : 0.06490139961242676
			 train-loss:  2.2026454693562276 	 ± 0.21494312352979875
	data : 0.11407232284545898
	model : 0.06485753059387207
			 train-loss:  2.2035625300951454 	 ± 0.21451094058552658
	data : 0.1139676570892334
	model : 0.06482748985290528
			 train-loss:  2.2044076442718508 	 ± 0.2140434437626935
	data : 0.11414580345153809
	model : 0.06483135223388672
			 train-loss:  2.204874693952649 	 ± 0.21341018768821757
	data : 0.11407151222229003
	model : 0.0648355484008789
			 train-loss:  2.202592684250129 	 ± 0.21454747736736948
	data : 0.11402430534362792
	model : 0.06488509178161621
			 train-loss:  2.202543650577271 	 ± 0.21384604689515024
	data : 0.11389002799987794
	model : 0.06490135192871094
			 train-loss:  2.199960616501895 	 ± 0.21553192064136276
	data : 0.1139183521270752
	model : 0.06487884521484374
			 train-loss:  2.198580356567137 	 ± 0.21551726990139278
	data : 0.11386022567749024
	model : 0.06486048698425292
			 train-loss:  2.1970077890616198 	 ± 0.21571569745072317
	data : 0.11393899917602539
	model : 0.06484570503234863
			 train-loss:  2.1965296541809276 	 ± 0.21511051937185346
	data : 0.11415138244628906
	model : 0.06485571861267089
			 train-loss:  2.1987350696249854 	 ± 0.21620198192549625
	data : 0.11424374580383301
	model : 0.06490998268127442
			 train-loss:  2.197306753704383 	 ± 0.21626753717304445
	data : 0.11435275077819824
	model : 0.06500287055969238
			 train-loss:  2.1954278074204923 	 ± 0.21688859965447196
	data : 0.1143979549407959
	model : 0.06509051322937012
			 train-loss:  2.1949044762190826 	 ± 0.21631529466528693
	data : 0.11436238288879394
	model : 0.06506757736206055
			 train-loss:  2.195392432772083 	 ± 0.21573548540730514
	data : 0.11414108276367188
	model : 0.06498456001281738
			 train-loss:  2.1941444771421468 	 ± 0.21565844490268604
	data : 0.11419501304626464
	model : 0.06492919921875
			 train-loss:  2.192834878113212 	 ± 0.21564908795364723
	data : 0.11424794197082519
	model : 0.06492753028869629
			 train-loss:  2.19249097144965 	 ± 0.21503971744356656
	data : 0.11413888931274414
	model : 0.06488656997680664
			 train-loss:  2.1930922564253748 	 ± 0.21453010984098547
	data : 0.11423544883728028
	model : 0.06491951942443848
			 train-loss:  2.1922553720588454 	 ± 0.2141584518981407
	data : 0.11445026397705078
	model : 0.0650252342224121
			 train-loss:  2.191624200769833 	 ± 0.2136758583662109
	data : 0.11441349983215332
	model : 0.06508488655090332
			 train-loss:  2.1917936230552266 	 ± 0.21305406060058626
	data : 0.1148369312286377
	model : 0.06503543853759766
			 train-loss:  2.1913255642442144 	 ± 0.2125136351876258
	data : 0.11481294631958008
	model : 0.06498799324035645
			 train-loss:  2.1896360310894702 	 ± 0.21303335044374466
	data : 0.11457767486572265
	model : 0.06495037078857421
			 train-loss:  2.1897656460141026 	 ± 0.21241992689810127
	data : 0.11438407897949218
	model : 0.064888334274292
			 train-loss:  2.1888781406975895 	 ± 0.21212468439509696
	data : 0.1143646240234375
	model : 0.06488595008850098
			 train-loss:  2.190469419819185 	 ± 0.21254727530638165
	data : 0.11391382217407227
	model : 0.0648686408996582
			 train-loss:  2.188734367915562 	 ± 0.21317130245530175
	data : 0.1140970230102539
	model : 0.06492629051208496
			 train-loss:  2.1874084662307394 	 ± 0.2132872796452705
	data : 0.1142740249633789
	model : 0.06500248908996582
			 train-loss:  2.18787827060721 	 ± 0.21277522360999643
	data : 0.11436696052551269
	model : 0.06501092910766601
			 train-loss:  2.187769916620147 	 ± 0.21218159559153388
	data : 0.11436657905578614
	model : 0.0649444580078125
			 train-loss:  2.1893819523923224 	 ± 0.21267834101600078
	data : 0.11414260864257812
	model : 0.0648798942565918
			 train-loss:  2.187664810154173 	 ± 0.21332740749498524
	data : 0.11408123970031739
	model : 0.06478629112243653
			 train-loss:  2.1861691646154413 	 ± 0.21368155315397666
	data : 0.11403446197509766
	model : 0.06474556922912597
			 train-loss:  2.1856663868977475 	 ± 0.21320103720650163
	data : 0.11406984329223632
	model : 0.06476840972900391
			 train-loss:  2.1855347000184606 	 ± 0.2126251447149604
	data : 0.11409173011779786
	model : 0.06481857299804687
			 train-loss:  2.1861852614775947 	 ± 0.21222912088000276
	data : 0.11431260108947754
	model : 0.0649169921875
			 train-loss:  2.1855329165587554 	 ± 0.21183964624508764
	data : 0.1143409252166748
	model : 0.0649796962738037
			 train-loss:  2.1857273168461298 	 ± 0.21128596293710605
	data : 0.1142465591430664
	model : 0.06489830017089844
			 train-loss:  2.1888921745320693 	 ± 0.21509548682148388
	data : 0.11423220634460449
	model : 0.06483888626098633
			 train-loss:  2.188015912441497 	 ± 0.2148570630307421
	data : 0.1141331672668457
	model : 0.06484146118164062
			 train-loss:  2.18841214407058 	 ± 0.21435676292984485
	data : 0.11405844688415527
	model : 0.06483397483825684
			 train-loss:  2.187582288290325 	 ± 0.21409610637726928
	data : 0.11408495903015137
	model : 0.06486215591430664
			 train-loss:  2.187251190864603 	 ± 0.2135836758070676
	data : 0.11425986289978027
	model : 0.06494431495666504
			 train-loss:  2.188077407578627 	 ± 0.21333254790673586
	data : 0.11428804397583008
	model : 0.06499371528625489
			 train-loss:  2.1872474912534723 	 ± 0.21308967830320286
	data : 0.1143106460571289
	model : 0.06499829292297363
			 train-loss:  2.1864878860945556 	 ± 0.21280158413464514
	data : 0.11431336402893066
	model : 0.06496763229370117
			 train-loss:  2.185248398169493 	 ± 0.21295617862271615
	data : 0.11427817344665528
	model : 0.06491851806640625
			 train-loss:  2.188273633012966 	 ± 0.21657239828925753
	data : 0.1141897201538086
	model : 0.06489052772521972
			 train-loss:  2.188692745218422 	 ± 0.21610169534240836
	data : 0.11414861679077148
	model : 0.06490001678466797
			 train-loss:  2.1869187204524723 	 ± 0.2169886534261127
	data : 0.11433939933776856
	model : 0.06490530967712402
			 train-loss:  2.187333754558659 	 ± 0.21652154275120644
	data : 0.11442356109619141
	model : 0.06491937637329101
			 train-loss:  2.186505209803581 	 ± 0.21629558797813683
	data : 0.11439304351806641
	model : 0.06496577262878418
			 train-loss:  2.1902156214215864 	 ± 0.22204606982015096
	data : 0.11439189910888672
	model : 0.06496500968933105
			 train-loss:  2.1884242060160872 	 ± 0.22294712075251913
	data : 0.11419110298156739
	model : 0.06493220329284669
			 train-loss:  2.188620212630098 	 ± 0.22241475879198735
	data : 0.11408472061157227
	model : 0.06491446495056152
			 train-loss:  2.187895985210643 	 ± 0.2221087745378739
	data : 0.11405558586120605
	model : 0.06492252349853515
			 train-loss:  2.1860393652101844 	 ± 0.22314761490927104
	data : 0.11401934623718261
	model : 0.06490030288696289
			 train-loss:  2.185534476076515 	 ± 0.22272268092966888
	data : 0.11412587165832519
	model : 0.06492047309875489
			 train-loss:  2.186374104541281 	 ± 0.22251062462327734
	data : 0.11434049606323242
	model : 0.06497688293457031
			 train-loss:  2.1874029212273083 	 ± 0.22246807978433727
	data : 0.11436967849731446
	model : 0.06496891975402833
			 train-loss:  2.1933441949232915 	 ± 0.23790203648506608
	data : 0.11437721252441406
	model : 0.06491298675537109
			 train-loss:  2.194114883740743 	 ± 0.23759630792016112
	data : 0.1142117977142334
	model : 0.06482434272766113
			 train-loss:  2.1945081706295646 	 ± 0.23710112220001248
	data : 0.11404886245727539
	model : 0.06485557556152344
			 train-loss:  2.195308987824422 	 ± 0.23682711820857635
	data : 0.11399345397949219
	model : 0.06483988761901856
			 train-loss:  2.195476145811484 	 ± 0.23628306739207164
	data : 0.1140070915222168
	model : 0.06489729881286621
			 train-loss:  2.194611812306342 	 ± 0.23606763430616387
	data : 0.1140164852142334
	model : 0.06495871543884277
			 train-loss:  2.1938323109648947 	 ± 0.23579389195436828
	data : 0.1142188549041748
	model : 0.0651024341583252
			 train-loss:  2.1932390685434693 	 ± 0.2354082075489077
	data : 0.11433272361755371
	model : 0.0650944709777832
			 train-loss:  2.191759744547479 	 ± 0.23586932924514506
	data : 0.1144294261932373
	model : 0.0650721549987793
			 train-loss:  2.1909650925102584 	 ± 0.2356186887229879
	data : 0.11429734230041504
	model : 0.06496677398681641
			 train-loss:  2.1893381707744513 	 ± 0.23630422468375673
	data : 0.11425080299377441
	model : 0.06491060256958008
			 train-loss:  2.188288733634082 	 ± 0.23627750328023342
	data : 0.11422996520996094
	model : 0.06480884552001953
			 train-loss:  2.189182817126831 	 ± 0.23611504017299018
	data : 0.11430974006652832
	model : 0.06503043174743653
			 train-loss:  2.188503562330126 	 ± 0.235798963542896
	data : 0.11429796218872071
	model : 0.06499643325805664
			 train-loss:  2.189240303274762 	 ± 0.23552561952791126
	data : 0.11441311836242676
	model : 0.06497931480407715
			 train-loss:  2.188812896077122 	 ± 0.23508596327974735
	data : 0.11446013450622558
	model : 0.06492075920104981
			 train-loss:  2.188398904270596 	 ± 0.2346447895660467
	data : 0.11451492309570313
	model : 0.06480002403259277
			 train-loss:  2.1891528288875004 	 ± 0.23439805362007468
	data : 0.11443867683410644
	model : 0.0642998218536377
			 train-loss:  2.1893593261945616 	 ± 0.23390178969856348
	data : 0.11447720527648926
	model : 0.06407737731933594
			 train-loss:  2.189936293321743 	 ± 0.2335501169956121
	data : 0.11464428901672363
	model : 0.06392970085144042
			 train-loss:  2.1921539343080148 	 ± 0.23543312192239063
	data : 0.11475143432617188
	model : 0.06380906105041503
			 train-loss:  2.1929123023281925 	 ± 0.23520089978990788
	data : 0.1147550106048584
	model : 0.06379175186157227
			 train-loss:  2.1923565271096828 	 ± 0.2348425618740142
	data : 0.11489062309265137
	model : 0.06384711265563965
			 train-loss:  2.191822957889787 	 ± 0.23447616876242228
	data : 0.11493124961853027
	model : 0.06391639709472656
			 train-loss:  2.1917744455419386 	 ± 0.23397362677797806
	data : 0.11482405662536621
	model : 0.06383910179138183
			 train-loss:  2.19226898240228 	 ± 0.2335951515200289
	data : 0.11487483978271484
	model : 0.06379432678222656
			 train-loss:  2.191293359310069 	 ± 0.23357488471613427
	data : 0.11492400169372559
	model : 0.06379027366638183
			 train-loss:  2.1926778383174184 	 ± 0.23404379051594676
	data : 0.11497683525085449
	model : 0.06377339363098145
			 train-loss:  2.1925967655101406 	 ± 0.23355282615439546
	data : 0.11509060859680176
	model : 0.0637843132019043
			 train-loss:  2.1909322012372376 	 ± 0.23446621791652492
	data : 0.11514849662780761
	model : 0.06390752792358398
			 train-loss:  2.1902593873035956 	 ± 0.23420530823950855
	data : 0.11528606414794922
	model : 0.06399059295654297
			 train-loss:  2.189912930627664 	 ± 0.23377823590622843
	data : 0.11536931991577148
	model : 0.06396050453186035
			 train-loss:  2.189800447942805 	 ± 0.23329922261024144
	data : 0.11516194343566895
	model : 0.06391739845275879
			 train-loss:  2.1895016663330646 	 ± 0.23286289996338383
	data : 0.1150588035583496
	model : 0.0638303279876709
			 train-loss:  2.1890976404456937 	 ± 0.23246824477671438
	data : 0.11507978439331054
	model : 0.06378850936889649
			 train-loss:  2.189732080111738 	 ± 0.2322020976670904
	data : 0.11492300033569336
	model : 0.06377067565917968
			 train-loss:  2.189253748193079 	 ± 0.23184815935799735
	data : 0.11486301422119141
	model : 0.06380667686462402
			 train-loss:  2.189667491893458 	 ± 0.23146705726250572
	data : 0.11504058837890625
	model : 0.0638606071472168
			 train-loss:  2.1906599163526463 	 ± 0.23152186640539407
	data : 0.1150907039642334
	model : 0.06391582489013672
			 train-loss:  2.191000645679812 	 ± 0.23111666284726745
	data : 0.11493706703186035
	model : 0.06385717391967774
			 train-loss:  2.1915992862249474 	 ± 0.23084468807396716
	data : 0.11479144096374512
	model : 0.0638382911682129
			 train-loss:  2.193359302043915 	 ± 0.23205048846531262
	data : 0.11477255821228027
	model : 0.06378464698791504
			 train-loss:  2.192894000456153 	 ± 0.23170460504370088
	data : 0.11472148895263672
	model : 0.06382484436035156
			 train-loss:  2.193012510500257 	 ± 0.231252038811987
	data : 0.11474123001098632
	model : 0.0638300895690918
			 train-loss:  2.1929786624644585 	 ± 0.2307951919540678
	data : 0.11483874320983886
	model : 0.06388740539550782
			 train-loss:  2.192667905270584 	 ± 0.23039345157830413
	data : 0.11487412452697754
	model : 0.06386919021606445
			 train-loss:  2.192260879161311 	 ± 0.2300327399118021
	data : 0.1148615837097168
	model : 0.06385221481323242
			 train-loss:  2.1920404280535877 	 ± 0.22961000552849012
	data : 0.11469902992248535
	model : 0.05537490844726563
#epoch  75    val-loss:  2.4002639494444193  train-loss:  2.1920404280535877  lr:  1.9073486328125e-08
			 train-loss:  2.2537522315979004 	 ± 0.0
	data : 5.685664653778076
	model : 0.07848167419433594
			 train-loss:  1.9691321849822998 	 ± 0.2846200466156006
	data : 2.905685544013977
	model : 0.07428872585296631
			 train-loss:  2.048591216405233 	 ± 0.25813405297697367
	data : 1.9754521052042644
	model : 0.07106630007425944
			 train-loss:  2.054551422595978 	 ± 0.2237888829974534
	data : 1.510070562362671
	model : 0.06942534446716309
			 train-loss:  2.0815006256103517 	 ± 0.20729256980180943
	data : 1.2307898998260498
	model : 0.06844582557678222
			 train-loss:  2.077894608179728 	 ± 0.18940307474030482
	data : 0.11662006378173828
	model : 0.06569194793701172
			 train-loss:  2.078909294945853 	 ± 0.17537078722128382
	data : 0.11444182395935058
	model : 0.06464314460754395
			 train-loss:  2.051693916320801 	 ± 0.1791515747076226
	data : 0.1143214225769043
	model : 0.06468305587768555
			 train-loss:  2.040661017100016 	 ± 0.17176420661195788
	data : 0.11444997787475586
	model : 0.06479444503784179
			 train-loss:  2.03112336397171 	 ± 0.16544288999680462
	data : 0.11454882621765136
	model : 0.06486740112304687
			 train-loss:  2.0188651084899902 	 ± 0.16243673720308102
	data : 0.11425991058349609
	model : 0.06489372253417969
			 train-loss:  2.0267420212427774 	 ± 0.15770031418299763
	data : 0.11417770385742188
	model : 0.06488323211669922
			 train-loss:  2.0620451156909647 	 ± 0.19471019720231672
	data : 0.11411075592041016
	model : 0.06486616134643555
			 train-loss:  2.07604912349156 	 ± 0.1943026391822351
	data : 0.11406302452087402
	model : 0.06483330726623535
			 train-loss:  2.0686683177948 	 ± 0.18973476922246724
	data : 0.1141139030456543
	model : 0.06485924720764161
			 train-loss:  2.0696678161621094 	 ± 0.18375068011017753
	data : 0.11429576873779297
	model : 0.06488566398620606
			 train-loss:  2.077633451013004 	 ± 0.18108947465896247
	data : 0.11421694755554199
	model : 0.06493582725524902
			 train-loss:  2.0835220681296454 	 ± 0.17765424620149764
	data : 0.11422662734985352
	model : 0.06496248245239258
			 train-loss:  2.0849798855028654 	 ± 0.1730265252942198
	data : 0.11402788162231445
	model : 0.06490626335144042
			 train-loss:  2.074503964185715 	 ± 0.17471812485841656
	data : 0.11391191482543946
	model : 0.06485633850097657
			 train-loss:  2.0986286855879284 	 ± 0.20177419956703657
	data : 0.11389150619506835
	model : 0.06485934257507324
			 train-loss:  2.0841016498478977 	 ± 0.2080720419780839
	data : 0.11405601501464843
	model : 0.06484818458557129
			 train-loss:  2.0913267291110493 	 ± 0.2063009134172297
	data : 0.11412591934204101
	model : 0.06487655639648438
			 train-loss:  2.0887607286373773 	 ± 0.2023318338789714
	data : 0.11426272392272949
	model : 0.06495633125305175
			 train-loss:  2.0923402643203737 	 ± 0.1990179840560516
	data : 0.1143423080444336
	model : 0.06497654914855958
			 train-loss:  2.0975501308074365 	 ± 0.19688406488745855
	data : 0.11427073478698731
	model : 0.06488971710205078
			 train-loss:  2.0926208098729453 	 ± 0.19483174602473208
	data : 0.114113187789917
	model : 0.06488566398620606
			 train-loss:  2.081749690430505 	 ± 0.19948586289604905
	data : 0.11419153213500977
	model : 0.06489276885986328
			 train-loss:  2.080219552434724 	 ± 0.19618343101740054
	data : 0.11433382034301758
	model : 0.06493897438049316
			 train-loss:  2.0911633213361105 	 ± 0.2016884319294049
	data : 0.11442131996154785
	model : 0.06497721672058106
			 train-loss:  2.104041726358475 	 ± 0.2105745053212475
	data : 0.11445822715759277
	model : 0.06506137847900391
			 train-loss:  2.102462325245142 	 ± 0.2074446343940139
	data : 0.11455225944519043
	model : 0.06506509780883789
			 train-loss:  2.0983968611919517 	 ± 0.20556783210633844
	data : 0.11438102722167968
	model : 0.06502237319946289
			 train-loss:  2.108241659753463 	 ± 0.21027032423119066
	data : 0.11410870552062988
	model : 0.06489081382751465
			 train-loss:  2.1101048503603255 	 ± 0.20752925954604137
	data : 0.11406431198120118
	model : 0.06487655639648438
			 train-loss:  2.109301073683633 	 ± 0.2046818538490108
	data : 0.1141629695892334
	model : 0.06486067771911622
			 train-loss:  2.115135054330568 	 ± 0.20490887008315484
	data : 0.11400580406188965
	model : 0.06486921310424805
			 train-loss:  2.1186199533311942 	 ± 0.20330286345762208
	data : 0.11405701637268066
	model : 0.06490416526794433
			 train-loss:  2.115644947076455 	 ± 0.2015157144999334
	data : 0.11412177085876465
	model : 0.0650031566619873
			 train-loss:  2.1176698714494706 	 ± 0.19938224855939166
	data : 0.11410102844238282
	model : 0.06499557495117188
			 train-loss:  2.114948077899654 	 ± 0.19768665919886855
	data : 0.11397466659545899
	model : 0.0650259017944336
			 train-loss:  2.112063461825961 	 ± 0.19619046887610325
	data : 0.11405363082885742
	model : 0.06497063636779785
			 train-loss:  2.1253984501195506 	 ± 0.21228303713453678
	data : 0.11393671035766602
	model : 0.06495680809020996
			 train-loss:  2.1215311830694024 	 ± 0.21138354117805228
	data : 0.1141634464263916
	model : 0.0649489402770996
			 train-loss:  2.1226873662736683 	 ± 0.20916228941897624
	data : 0.11413421630859374
	model : 0.06496014595031738
			 train-loss:  2.1163920138193215 	 ± 0.21114264581701586
	data : 0.11427888870239258
	model : 0.0649339199066162
			 train-loss:  2.1250113959008075 	 ± 0.21691056391208388
	data : 0.11431336402893066
	model : 0.06495461463928223
			 train-loss:  2.124533288180828 	 ± 0.21466421214677897
	data : 0.1144322395324707
	model : 0.06496286392211914
			 train-loss:  2.127199423556425 	 ± 0.21326391718869814
	data : 0.11430253982543945
	model : 0.06489062309265137
			 train-loss:  2.1265434861183166 	 ± 0.21117043081214626
	data : 0.11427788734436035
	model : 0.06481714248657226
			 train-loss:  2.1302033896539725 	 ± 0.21068536721524034
	data : 0.11425247192382812
	model : 0.06484370231628418
			 train-loss:  2.137358016692675 	 ± 0.21481463040314877
	data : 0.11423254013061523
	model : 0.06484837532043457
			 train-loss:  2.1392938798328616 	 ± 0.21323586085522067
	data : 0.11427669525146485
	model : 0.06486144065856933
			 train-loss:  2.136997326656624 	 ± 0.21191279874459432
	data : 0.11432876586914062
	model : 0.06490201950073242
			 train-loss:  2.1415558099746703 	 ± 0.2126326652427654
	data : 0.11441316604614257
	model : 0.06496706008911132
			 train-loss:  2.144696897694043 	 ± 0.21200928124901514
	data : 0.11431522369384765
	model : 0.06489906311035157
			 train-loss:  2.145451355398747 	 ± 0.2102171511239145
	data : 0.11415238380432129
	model : 0.06483535766601563
			 train-loss:  2.1463180348790925 	 ± 0.2084997532654085
	data : 0.11408524513244629
	model : 0.06481938362121582
			 train-loss:  2.1453006772671714 	 ± 0.20687039908262825
	data : 0.11403975486755372
	model : 0.06485500335693359
			 train-loss:  2.143913612763087 	 ± 0.20541572216207496
	data : 0.11401715278625488
	model : 0.06487197875976562
			 train-loss:  2.1503313271725766 	 ± 0.20970243429276042
	data : 0.11422157287597656
	model : 0.06492524147033692
			 train-loss:  2.1475234108586467 	 ± 0.20915731639333177
	data : 0.11441354751586914
	model : 0.06499390602111817
			 train-loss:  2.143572892461504 	 ± 0.2098094338857911
	data : 0.11437740325927734
	model : 0.06499710083007812
			 train-loss:  2.1622671727091074 	 ± 0.25563485922435364
	data : 0.1142432689666748
	model : 0.06487975120544434
			 train-loss:  2.159965076813331 	 ± 0.2543285012425243
	data : 0.11419434547424316
	model : 0.0648127555847168
			 train-loss:  2.163886505545992 	 ± 0.25436683256361253
	data : 0.11409134864807129
	model : 0.06480093002319336
			 train-loss:  2.1604821770938476 	 ± 0.25397181067074776
	data : 0.11415915489196778
	model : 0.06480264663696289
			 train-loss:  2.1598819231285766 	 ± 0.25214532895100505
	data : 0.11426787376403809
	model : 0.06481924057006835
			 train-loss:  2.159105872762376 	 ± 0.2503933116719412
	data : 0.11440458297729492
	model : 0.06491508483886718
			 train-loss:  2.155479289804186 	 ± 0.25041692630674117
	data : 0.11447091102600097
	model : 0.06492934226989747
			 train-loss:  2.1593821669968083 	 ± 0.2507821530101299
	data : 0.11439847946166992
	model : 0.06483612060546876
			 train-loss:  2.164279399646653 	 ± 0.2524301440388965
	data : 0.11442179679870605
	model : 0.06481847763061524
			 train-loss:  2.167711994419359 	 ± 0.25238154134171786
	data : 0.11450052261352539
	model : 0.06478171348571778
			 train-loss:  2.168055787279799 	 ± 0.2506876698254336
	data : 0.11451930999755859
	model : 0.06479134559631347
			 train-loss:  2.165679561297099 	 ± 0.24984839601769682
	data : 0.11444363594055176
	model : 0.0648500919342041
			 train-loss:  2.1629362278863002 	 ± 0.2493336931282571
	data : 0.11460518836975098
	model : 0.06497664451599121
			 train-loss:  2.1688591861105584 	 ± 0.2530338161454077
	data : 0.11453132629394532
	model : 0.06498565673828124
			 train-loss:  2.1682016314604344 	 ± 0.25147277712904154
	data : 0.11424760818481446
	model : 0.06494836807250977
			 train-loss:  2.1685854709601102 	 ± 0.24989910268153454
	data : 0.11430439949035645
	model : 0.06489520072937012
			 train-loss:  2.1645034104585648 	 ± 0.2509687953095902
	data : 0.11436047554016113
	model : 0.06491479873657227
			 train-loss:  2.167002780937854 	 ± 0.25041463155958
	data : 0.11434097290039062
	model : 0.06488976478576661
			 train-loss:  2.170274795555487 	 ± 0.25061914270796043
	data : 0.11431913375854492
	model : 0.06489772796630859
			 train-loss:  2.1666101360895547 	 ± 0.25130548348117454
	data : 0.11452884674072265
	model : 0.064994478225708
			 train-loss:  2.1663522195248377 	 ± 0.24981618967499164
	data : 0.11449494361877441
	model : 0.06504960060119629
			 train-loss:  2.170075950903051 	 ± 0.25067643303166887
	data : 0.11448559761047364
	model : 0.06501636505126954
			 train-loss:  2.174672682617986 	 ± 0.2527924779928951
	data : 0.11442828178405762
	model : 0.06495828628540039
			 train-loss:  2.1762978222178315 	 ± 0.2517868950908858
	data : 0.11443881988525391
	model : 0.0649306297302246
			 train-loss:  2.181095915761861 	 ± 0.2543208843448032
	data : 0.11430897712707519
	model : 0.06486268043518066
			 train-loss:  2.18026895067665 	 ± 0.25300703801713387
	data : 0.11434354782104492
	model : 0.06485552787780761
			 train-loss:  2.1813742995262144 	 ± 0.2518135231799464
	data : 0.1144181728363037
	model : 0.06484193801879883
			 train-loss:  2.181318761228205 	 ± 0.2504266645552117
	data : 0.11448373794555664
	model : 0.0649341106414795
			 train-loss:  2.1808194064575694 	 ± 0.24910748083577464
	data : 0.11447129249572754
	model : 0.06498756408691406
			 train-loss:  2.1815583205992177 	 ± 0.2478659221842275
	data : 0.11455760002136231
	model : 0.06499776840209961
			 train-loss:  2.181897179877504 	 ± 0.24656561717494357
	data : 0.1144831657409668
	model : 0.06495900154113769
			 train-loss:  2.178794823194805 	 ± 0.24710195137656416
	data : 0.11435046195983886
	model : 0.06491894721984863
			 train-loss:  2.1792740002274513 	 ± 0.2458559583050492
	data : 0.11428861618041992
	model : 0.06487507820129394
			 train-loss:  2.183303616710545 	 ± 0.24775156104221055
	data : 0.11428589820861816
	model : 0.06485476493835449
			 train-loss:  2.1837297823964334 	 ± 0.24652001501956547
	data : 0.11431431770324707
	model : 0.06491708755493164
			 train-loss:  2.183323450762816 	 ± 0.24530478665996525
	data : 0.11439857482910157
	model : 0.06497726440429688
			 train-loss:  2.179712553024292 	 ± 0.24670531741338278
	data : 0.1146130084991455
	model : 0.06502070426940917
			 train-loss:  2.182223168930205 	 ± 0.24676147149990663
	data : 0.11455631256103516
	model : 0.06500492095947266
			 train-loss:  2.1816738166061103 	 ± 0.24561093546472315
	data : 0.11439008712768554
	model : 0.06493654251098632
			 train-loss:  2.1797984838485718 	 ± 0.24514847675417242
	data : 0.11436018943786622
	model : 0.06487503051757812
			 train-loss:  2.177298162992184 	 ± 0.24528315816436236
	data : 0.11426467895507812
	model : 0.06484556198120117
			 train-loss:  2.1793017183031353 	 ± 0.2449659551202678
	data : 0.11409072875976563
	model : 0.06484842300415039
			 train-loss:  2.178224262201561 	 ± 0.24405757245526155
	data : 0.1141047477722168
	model : 0.06488852500915528
			 train-loss:  2.1768827817150367 	 ± 0.24330675959029718
	data : 0.11432032585144043
	model : 0.06496520042419433
			 train-loss:  2.1772670569243253 	 ± 0.24221033914465204
	data : 0.11430130004882813
	model : 0.06496615409851074
			 train-loss:  2.173924011921664 	 ± 0.2435870115258864
	data : 0.11425700187683105
	model : 0.06495203971862792
			 train-loss:  2.1764708508144723 	 ± 0.24393081553319496
	data : 0.11426753997802734
	model : 0.06489062309265137
			 train-loss:  2.1768179723808356 	 ± 0.24285683189391652
	data : 0.11422929763793946
	model : 0.06490960121154785
			 train-loss:  2.177209139934608 	 ± 0.24180534110248825
	data : 0.11417293548583984
	model : 0.06489439010620117
			 train-loss:  2.177410955977651 	 ± 0.24074250279494921
	data : 0.11427912712097169
	model : 0.06490774154663086
			 train-loss:  2.179288367430369 	 ± 0.24051371367484156
	data : 0.11436100006103515
	model : 0.06493902206420898
			 train-loss:  2.1776611297026927 	 ± 0.24009517135584205
	data : 0.11443552970886231
	model : 0.06501164436340331
			 train-loss:  2.177138551555831 	 ± 0.23912371443337657
	data : 0.1145327091217041
	model : 0.06494154930114746
			 train-loss:  2.1742035527514596 	 ± 0.24018884414694008
	data : 0.11446652412414551
	model : 0.06493310928344727
			 train-loss:  2.1748671935776533 	 ± 0.23927663032090082
	data : 0.11446561813354492
	model : 0.06488580703735351
			 train-loss:  2.1747470202566195 	 ± 0.23827272129382704
	data : 0.11443753242492676
	model : 0.06489295959472656
			 train-loss:  2.1755586326122285 	 ± 0.23744296368693862
	data : 0.11447200775146485
	model : 0.06486244201660156
			 train-loss:  2.172704001103551 	 ± 0.23851852768936302
	data : 0.11446871757507324
	model : 0.06490969657897949
			 train-loss:  2.1726378261065875 	 ± 0.23754009668263787
	data : 0.11462540626525879
	model : 0.06494197845458985
			 train-loss:  2.1716958119617247 	 ± 0.23680121818696817
	data : 0.11456360816955566
	model : 0.06504573822021484
			 train-loss:  2.1721419057538434 	 ± 0.23589632813760952
	data : 0.11460719108581544
	model : 0.06501622200012207
			 train-loss:  2.173558027267456 	 ± 0.2354794484138682
	data : 0.11444716453552246
	model : 0.06501116752624511
			 train-loss:  2.17118162295175 	 ± 0.23604321408464904
	data : 0.11434850692749024
	model : 0.06496157646179199
			 train-loss:  2.1727530421234493 	 ± 0.2357728276281835
	data : 0.11412515640258789
	model : 0.0649648666381836
			 train-loss:  2.172164942137897 	 ± 0.2349435314390249
	data : 0.11405715942382813
	model : 0.06491789817810059
			 train-loss:  2.173632077468458 	 ± 0.23461902335766102
	data : 0.11408228874206543
	model : 0.06494016647338867
			 train-loss:  2.172573961661412 	 ± 0.23402368311451918
	data : 0.11422300338745117
	model : 0.0650031566619873
			 train-loss:  2.172633279370898 	 ± 0.23312973283085514
	data : 0.11426877975463867
	model : 0.06503763198852539
			 train-loss:  2.1733630507281334 	 ± 0.23239513781419424
	data : 0.11437311172485351
	model : 0.06499104499816895
			 train-loss:  2.1727449006604074 	 ± 0.23162872685686062
	data : 0.11427388191223145
	model : 0.06499004364013672
			 train-loss:  2.1717973938628807 	 ± 0.23102139150271478
	data : 0.11407904624938965
	model : 0.06497759819030761
			 train-loss:  2.1717605299419827 	 ± 0.23016456189960993
	data : 0.11396603584289551
	model : 0.0649592399597168
			 train-loss:  2.1693799907670304 	 ± 0.2309788706218104
	data : 0.11396822929382325
	model : 0.06500020027160644
			 train-loss:  2.1698663591468423 	 ± 0.23020422442961078
	data : 0.11405410766601562
	model : 0.06510968208312988
			 train-loss:  2.1682639700779016 	 ± 0.2301341754528194
	data : 0.11478562355041504
	model : 0.06516995429992675
			 train-loss:  2.1682319821213647 	 ± 0.22930516831079467
	data : 0.11503429412841797
	model : 0.06521100997924804
			 train-loss:  2.1679601303168705 	 ± 0.22850723230170583
	data : 0.11514019966125488
	model : 0.06517510414123535
			 train-loss:  2.1654645999272666 	 ± 0.2296020605245554
	data : 0.11513514518737793
	model : 0.06518187522888183
			 train-loss:  2.1649538975366402 	 ± 0.228872528184935
	data : 0.1152106761932373
	model : 0.06514253616333007
			 train-loss:  2.170382330467651 	 ± 0.23706699806397885
	data : 0.11456708908081055
	model : 0.0650625228881836
			 train-loss:  2.1710582251350083 	 ± 0.2363806373407002
	data : 0.11432447433471679
	model : 0.0649951457977295
			 train-loss:  2.1688888590911337 	 ± 0.2369981873053991
	data : 0.11408376693725586
	model : 0.06500544548034667
			 train-loss:  2.1692712445781654 	 ± 0.23623003449771227
	data : 0.11414599418640137
	model : 0.06494646072387696
			 train-loss:  2.171956739458097 	 ± 0.2376508799740233
	data : 0.11412854194641113
	model : 0.06492447853088379
			 train-loss:  2.1724215383465224 	 ± 0.23691367759507498
	data : 0.11421380043029786
	model : 0.06495671272277832
			 train-loss:  2.170674184024734 	 ± 0.237072294061135
	data : 0.11446266174316407
	model : 0.06501317024230957
			 train-loss:  2.172423853079478 	 ± 0.23724401795556693
	data : 0.11494278907775879
	model : 0.06502938270568848
			 train-loss:  2.1729728030842663 	 ± 0.2365526991072808
	data : 0.11518673896789551
	model : 0.0650261402130127
			 train-loss:  2.172267919308261 	 ± 0.235932333833685
	data : 0.11508016586303711
	model : 0.0649996280670166
			 train-loss:  2.1721461575015697 	 ± 0.2351648406223059
	data : 0.11503071784973144
	model : 0.06498103141784668
			 train-loss:  2.1710492364771956 	 ± 0.234792440624924
	data : 0.1148331642150879
	model : 0.06493239402770996
			 train-loss:  2.1717229666248445 	 ± 0.23418311519770288
	data : 0.1145395278930664
	model : 0.06493868827819824
			 train-loss:  2.174034454119511 	 ± 0.23519851810981737
	data : 0.11416678428649903
	model : 0.06497535705566407
			 train-loss:  2.176910419372996 	 ± 0.2371841062666769
	data : 0.1141402244567871
	model : 0.06499137878417968
			 train-loss:  2.179181470901151 	 ± 0.23813862079476195
	data : 0.11432385444641113
	model : 0.06500844955444336
			 train-loss:  2.1778996523071386 	 ± 0.2379347375600136
	data : 0.11482071876525879
	model : 0.0650550365447998
			 train-loss:  2.18091279938817 	 ± 0.24021381780544698
	data : 0.1147529125213623
	model : 0.06500883102416992
			 train-loss:  2.1822018556713316 	 ± 0.24002113055416976
	data : 0.11477875709533691
	model : 0.0649876594543457
			 train-loss:  2.1840385721053606 	 ± 0.2404114448894744
	data : 0.11487946510314942
	model : 0.06492533683776855
			 train-loss:  2.1825659465204716 	 ± 0.24040464507917308
	data : 0.11453380584716796
	model : 0.06485671997070312
			 train-loss:  2.1866846288122783 	 ± 0.2453712415520278
	data : 0.11407003402709961
	model : 0.06474413871765136
			 train-loss:  2.1871894720828893 	 ± 0.24471197995844357
	data : 0.11407546997070313
	model : 0.06476454734802246
			 train-loss:  2.1891793070069276 	 ± 0.24530901844135355
	data : 0.11415395736694336
	model : 0.06478428840637207
			 train-loss:  2.1890874936909017 	 ± 0.2445763182444675
	data : 0.11409010887145996
	model : 0.06487231254577637
			 train-loss:  2.188166165635699 	 ± 0.24413782097139763
	data : 0.11433143615722656
	model : 0.0649296760559082
			 train-loss:  2.1885999448200653 	 ± 0.24347937313112009
	data : 0.11437292098999023
	model : 0.06502418518066407
			 train-loss:  2.188355468301212 	 ± 0.24278300436479144
	data : 0.11445407867431641
	model : 0.0650744915008545
			 train-loss:  2.1875143232401353 	 ± 0.24232038114928753
	data : 0.11422953605651856
	model : 0.06500129699707032
			 train-loss:  2.1895107363545616 	 ± 0.24302124471768533
	data : 0.11415553092956543
	model : 0.06492810249328614
			 train-loss:  2.1919257902685616 	 ± 0.24437906925615457
	data : 0.11403899192810059
	model : 0.06490550041198731
			 train-loss:  2.191912714092211 	 ± 0.24367587937467297
	data : 0.11405963897705078
	model : 0.06488189697265626
			 train-loss:  2.1931358460017614 	 ± 0.24351374642553553
	data : 0.1140864372253418
	model : 0.06485424041748047
			 train-loss:  2.1949691731821406 	 ± 0.24402911582915016
	data : 0.11419706344604492
	model : 0.06489453315734864
			 train-loss:  2.1954307690852106 	 ± 0.24341583343812426
	data : 0.11418633460998535
	model : 0.0649336814880371
			 train-loss:  2.195219287711583 	 ± 0.24274742406492344
	data : 0.11417789459228515
	model : 0.06490607261657715
			 train-loss:  2.196823499722188 	 ± 0.2430127505629507
	data : 0.11399717330932617
	model : 0.06484355926513671
			 train-loss:  2.1959496670299106 	 ± 0.2426186183468001
	data : 0.11385455131530761
	model : 0.064837646484375
			 train-loss:  2.1939776437717247 	 ± 0.24338976410017976
	data : 0.11398921012878419
	model : 0.06488018035888672
			 train-loss:  2.1926251564707075 	 ± 0.24340127309495557
	data : 0.11415724754333496
	model : 0.06489710807800293
			 train-loss:  2.192294778068209 	 ± 0.24277624747261942
	data : 0.11420965194702148
	model : 0.06497154235839844
			 train-loss:  2.192722470216129 	 ± 0.24218474982951854
	data : 0.11445741653442383
	model : 0.06505475044250489
			 train-loss:  2.1919514636735657 	 ± 0.24175563356029714
	data : 0.11457176208496093
	model : 0.06508593559265137
			 train-loss:  2.194347019477557 	 ± 0.2432965657549607
	data : 0.1145329475402832
	model : 0.06505494117736817
			 train-loss:  2.19346774261903 	 ± 0.2429413085505371
	data : 0.11444759368896484
	model : 0.06498513221740723
			 train-loss:  2.194706476115166 	 ± 0.2428857443864402
	data : 0.11434998512268066
	model : 0.06487374305725098
			 train-loss:  2.1957931323026223 	 ± 0.24270011219551538
	data : 0.11426343917846679
	model : 0.06487317085266113
			 train-loss:  2.1950104656972385 	 ± 0.24229961209866982
	data : 0.11426091194152832
	model : 0.06485347747802735
			 train-loss:  2.195704489478266 	 ± 0.24185376085002783
	data : 0.11430363655090332
	model : 0.06489944458007812
			 train-loss:  2.1963056406627097 	 ± 0.24136613979947721
	data : 0.11431031227111817
	model : 0.06496629714965821
			 train-loss:  2.1962802366889203 	 ± 0.24074028418982046
	data : 0.11438713073730469
	model : 0.06506228446960449
			 train-loss:  2.195895103449674 	 ± 0.24017862100648618
	data : 0.11438417434692383
	model : 0.06505155563354492
			 train-loss:  2.195635219109364 	 ± 0.23958933252952452
	data : 0.1142115592956543
	model : 0.06500678062438965
			 train-loss:  2.19625817208874 	 ± 0.2391356295025046
	data : 0.11433358192443847
	model : 0.06491589546203613
			 train-loss:  2.195033883685388 	 ± 0.23914294195966734
	data : 0.11422724723815918
	model : 0.06489286422729493
			 train-loss:  2.1974457965956793 	 ± 0.24092846326356282
	data : 0.11439499855041504
	model : 0.06487579345703125
			 train-loss:  2.199142721430141 	 ± 0.24150566246793678
	data : 0.1143979549407959
	model : 0.0648758888244629
			 train-loss:  2.198184624314308 	 ± 0.24127998703922118
	data : 0.11461586952209472
	model : 0.06494245529174805
			 train-loss:  2.1980827192762007 	 ± 0.24068335437308594
	data : 0.11445155143737792
	model : 0.06501545906066894
			 train-loss:  2.199985050328887 	 ± 0.24159696600650046
	data : 0.11462092399597168
	model : 0.06501083374023438
			 train-loss:  2.1997262251200933 	 ± 0.24102923801810316
	data : 0.11450076103210449
	model : 0.06499161720275878
			 train-loss:  2.1999670121015287 	 ± 0.2404622284648803
	data : 0.11434860229492187
	model : 0.06497039794921874
			 train-loss:  2.199804192636071 	 ± 0.23988629067051728
	data : 0.11414966583251954
	model : 0.0648961067199707
			 train-loss:  2.2005712795026096 	 ± 0.23955523842248366
	data : 0.11401782035827637
	model : 0.06485710144042969
			 train-loss:  2.1996325484796424 	 ± 0.23935540989073423
	data : 0.11391801834106445
	model : 0.0649179458618164
			 train-loss:  2.2006987132705174 	 ± 0.2392715468017159
	data : 0.11384687423706055
	model : 0.06493830680847168
			 train-loss:  2.1999787274730265 	 ± 0.2389241897181231
	data : 0.11396870613098145
	model : 0.06496763229370117
			 train-loss:  2.1993818243344623 	 ± 0.23851079957395604
	data : 0.11414341926574707
	model : 0.06506390571594238
			 train-loss:  2.199277286280953 	 ± 0.23794975918962857
	data : 0.11417274475097657
	model : 0.06507835388183594
			 train-loss:  2.1998268438960022 	 ± 0.23752207660728206
	data : 0.11416635513305665
	model : 0.06498980522155762
			 train-loss:  2.1992318479108137 	 ± 0.23712216651697926
	data : 0.1141237735748291
	model : 0.06500205993652344
			 train-loss:  2.197893454649738 	 ± 0.23737254556341789
	data : 0.11423616409301758
	model : 0.06500024795532226
			 train-loss:  2.196170717616414 	 ± 0.23815702035321554
	data : 0.11427745819091797
	model : 0.06496834754943848
			 train-loss:  2.1989607054878166 	 ± 0.24110111242309756
	data : 0.11435537338256836
	model : 0.06498432159423828
			 train-loss:  2.1978235612816523 	 ± 0.24112481484945808
	data : 0.11440763473510743
	model : 0.06503381729125976
			 train-loss:  2.1998624807104057 	 ± 0.242438826204177
	data : 0.11451630592346192
	model : 0.06496391296386719
			 train-loss:  2.1987280568031418 	 ± 0.24246390712431276
	data : 0.11440949440002442
	model : 0.06488041877746582
			 train-loss:  2.1997738355940037 	 ± 0.24240675475794776
	data : 0.11428322792053222
	model : 0.06522316932678222
			 train-loss:  2.1992885963949145 	 ± 0.24196476659810395
	data : 0.11378254890441894
	model : 0.06510224342346191
			 train-loss:  2.198677634870684 	 ± 0.2415899766818351
	data : 0.11397318840026856
	model : 0.06501026153564453
			 train-loss:  2.199818244964018 	 ± 0.24164603710532
	data : 0.11404023170471192
	model : 0.06498274803161622
			 train-loss:  2.2002782805689742 	 ± 0.24120389544632942
	data : 0.11416845321655274
	model : 0.06495170593261719
			 train-loss:  2.200478739738464 	 ± 0.24068598965330978
	data : 0.11429204940795898
	model : 0.06439108848571777
			 train-loss:  2.2006462617258054 	 ± 0.24016605426457555
	data : 0.11497111320495605
	model : 0.06427903175354004
			 train-loss:  2.199515994950013 	 ± 0.24023811844226392
	data : 0.11492996215820313
	model : 0.06415224075317383
			 train-loss:  2.199409027894338 	 ± 0.239716119078253
	data : 0.11477713584899903
	model : 0.06392946243286132
			 train-loss:  2.198976947751108 	 ± 0.23928111104939204
	data : 0.11486396789550782
	model : 0.06382584571838379
			 train-loss:  2.2019925563231757 	 ± 0.24308232213422795
	data : 0.11498613357543945
	model : 0.06376590728759765
			 train-loss:  2.202258181262326 	 ± 0.24258904911578139
	data : 0.11496753692626953
	model : 0.06376848220825196
			 train-loss:  2.201419789215614 	 ± 0.24240081585100243
	data : 0.11496047973632813
	model : 0.0637807846069336
			 train-loss:  2.201350336934364 	 ± 0.2418823963828799
	data : 0.11527562141418457
	model : 0.06389350891113281
			 train-loss:  2.201246719075064 	 ± 0.241370182514352
	data : 0.11524710655212403
	model : 0.06390013694763183
			 train-loss:  2.2003703239116263 	 ± 0.2412288954278343
	data : 0.11516866683959961
	model : 0.06388835906982422
			 train-loss:  2.1993881792335186 	 ± 0.24118766399831304
	data : 0.11507134437561035
	model : 0.06382880210876465
			 train-loss:  2.2009593741300235 	 ± 0.24188559722776312
	data : 0.11501421928405761
	model : 0.06378788948059082
			 train-loss:  2.2006511132256326 	 ± 0.24142354564012986
	data : 0.11497135162353515
	model : 0.06378087997436524
			 train-loss:  2.201010246655931 	 ± 0.24098164495639668
	data : 0.11494660377502441
	model : 0.06377592086791992
			 train-loss:  2.1999679217735926 	 ± 0.2410183506253994
	data : 0.11498069763183594
	model : 0.0637960433959961
			 train-loss:  2.200147502155225 	 ± 0.2405338820681444
	data : 0.11503615379333496
	model : 0.06383705139160156
			 train-loss:  2.2011604259821995 	 ± 0.24055091210406343
	data : 0.11506261825561523
	model : 0.0637664794921875
			 train-loss:  2.200130172227145 	 ± 0.24058985665273552
	data : 0.11503710746765136
	model : 0.0637387752532959
			 train-loss:  2.198956358628195 	 ± 0.2407925797723815
	data : 0.11515111923217773
	model : 0.06373038291931152
			 train-loss:  2.1996613084053505 	 ± 0.2405528338665116
	data : 0.11519975662231445
	model : 0.06375279426574706
			 train-loss:  2.199578018692451 	 ± 0.24006694734037617
	data : 0.11519398689270019
	model : 0.06381392478942871
			 train-loss:  2.19848316980277 	 ± 0.24019510705458566
	data : 0.11518979072570801
	model : 0.06392946243286132
			 train-loss:  2.198891712773231 	 ± 0.23979632967582626
	data : 0.11519846916198731
	model : 0.06393580436706543
			 train-loss:  2.1978794867733873 	 ± 0.23984463313035068
	data : 0.11500010490417481
	model : 0.06389379501342773
			 train-loss:  2.197708258628845 	 ± 0.23937971238089525
	data : 0.11490373611450196
	model : 0.06391744613647461
			 train-loss:  2.197519638623849 	 ± 0.23892099882371556
	data : 0.11490421295166016
	model : 0.06388144493103028
			 train-loss:  2.197388960255517 	 ± 0.23845546577190396
	data : 0.11500110626220703
	model : 0.06384282112121582
			 train-loss:  2.196988669308749 	 ± 0.2380685631513137
	data : 0.11497550010681153
	model : 0.06386427879333496
			 train-loss:  2.1964647732381746 	 ± 0.23774554581411542
	data : 0.11511926651000977
	model : 0.06391944885253906
			 train-loss:  2.1961660487979064 	 ± 0.2373266777383528
	data : 0.11521644592285156
	model : 0.06387224197387695
			 train-loss:  2.1957778753712773 	 ± 0.23694378990445497
	data : 0.11480484008789063
	model : 0.055402088165283206
#epoch  76    val-loss:  2.412488636217619  train-loss:  2.1957778753712773  lr:  1.9073486328125e-08
			 train-loss:  2.0127322673797607 	 ± 0.0
	data : 5.886124849319458
	model : 0.07117962837219238
			 train-loss:  2.0435733795166016 	 ± 0.03084111213684082
	data : 3.007943868637085
	model : 0.06793546676635742
			 train-loss:  2.059927543004354 	 ± 0.034191131286632204
	data : 2.0434648195902505
	model : 0.06681394577026367
			 train-loss:  2.0988506078720093 	 ± 0.07363280539318007
	data : 1.5610483288764954
	model : 0.06626325845718384
			 train-loss:  2.1798541069030763 	 ± 0.17488195860396852
	data : 1.2716970443725586
	model : 0.06592278480529785
			 train-loss:  2.2099053064982095 	 ± 0.17321024562603934
	data : 0.11729650497436524
	model : 0.06463727951049805
			 train-loss:  2.229114191872733 	 ± 0.16712183271362396
	data : 0.11425609588623047
	model : 0.064609956741333
			 train-loss:  2.2233814895153046 	 ± 0.15706221930368072
	data : 0.1141214370727539
	model : 0.0645859718322754
			 train-loss:  2.223502582973904 	 ± 0.14808007655088046
	data : 0.11403708457946778
	model : 0.06463255882263183
			 train-loss:  2.1914940595626833 	 ± 0.17016418043431844
	data : 0.11398520469665527
	model : 0.06471223831176758
			 train-loss:  2.177729845046997 	 ± 0.1679822445066884
	data : 0.11400384902954101
	model : 0.06479892730712891
			 train-loss:  2.217026114463806 	 ± 0.2070089280391283
	data : 0.11390762329101563
	model : 0.06485142707824706
			 train-loss:  2.210886845221886 	 ± 0.20002155523065587
	data : 0.11396656036376954
	model : 0.06493959426879883
			 train-loss:  2.2492439235959734 	 ± 0.23722840218020474
	data : 0.11408495903015137
	model : 0.06495237350463867
			 train-loss:  2.2548375288645426 	 ± 0.2301380709331886
	data : 0.11394777297973632
	model : 0.0648913860321045
			 train-loss:  2.239646300673485 	 ± 0.23046672682046346
	data : 0.11380281448364257
	model : 0.06483445167541504
			 train-loss:  2.2355545969570385 	 ± 0.2241838045039717
	data : 0.11382288932800293
	model : 0.06486425399780274
			 train-loss:  2.225856807496813 	 ± 0.2215063139655422
	data : 0.11388897895812988
	model : 0.06485648155212402
			 train-loss:  2.206806778907776 	 ± 0.23024973579046165
	data : 0.11392011642456054
	model : 0.06491236686706543
			 train-loss:  2.207531839609146 	 ± 0.2244419353360492
	data : 0.11413722038269043
	model : 0.06497464179992676
			 train-loss:  2.1991710606075467 	 ± 0.22220140472141062
	data : 0.11416058540344239
	model : 0.06494803428649902
			 train-loss:  2.2203195799480784 	 ± 0.23774287194559507
	data : 0.11403098106384277
	model : 0.06484441757202149
			 train-loss:  2.2226861922637275 	 ± 0.23278193206351036
	data : 0.11377954483032227
	model : 0.06483044624328613
			 train-loss:  2.2281204015016556 	 ± 0.22936613283251508
	data : 0.11375198364257813
	model : 0.0647695541381836
			 train-loss:  2.226237425804138 	 ± 0.22492124024729152
	data : 0.11371574401855469
	model : 0.06476926803588867
			 train-loss:  2.2208967988307657 	 ± 0.22216405931758543
	data : 0.11385183334350586
	model : 0.06478772163391114
			 train-loss:  2.218117285657812 	 ± 0.21847129122238462
	data : 0.1139364242553711
	model : 0.06491737365722657
			 train-loss:  2.2157790618283406 	 ± 0.2148783134987387
	data : 0.11408185958862305
	model : 0.06493182182312011
			 train-loss:  2.225368331218588 	 ± 0.2171525785008917
	data : 0.1141892433166504
	model : 0.06484160423278809
			 train-loss:  2.2284789443016053 	 ± 0.2141588252600111
	data : 0.11403856277465821
	model : 0.06474390029907226
			 train-loss:  2.2417124125265304 	 ± 0.22279645177825153
	data : 0.11398043632507324
	model : 0.06475305557250977
			 train-loss:  2.229031875729561 	 ± 0.23037304475743595
	data : 0.11403965950012207
	model : 0.06469955444335937
			 train-loss:  2.224412795269128 	 ± 0.2283555432883263
	data : 0.1140676498413086
	model : 0.06473150253295898
			 train-loss:  2.2257738674388214 	 ± 0.22510813816702757
	data : 0.11402606964111328
	model : 0.06485180854797364
			 train-loss:  2.2305847236088345 	 ± 0.22363533280015407
	data : 0.11408333778381348
	model : 0.06496481895446778
			 train-loss:  2.2343363761901855 	 ± 0.22162161465842758
	data : 0.11408486366271972
	model : 0.06494283676147461
			 train-loss:  2.2299347117140487 	 ± 0.22019574074917458
	data : 0.11391048431396485
	model : 0.06489982604980468
			 train-loss:  2.2312431460932682 	 ± 0.21742482970640417
	data : 0.11392049789428711
	model : 0.06486139297485352
			 train-loss:  2.229316246815217 	 ± 0.21494768165320222
	data : 0.11386795043945312
	model : 0.06484942436218262
			 train-loss:  2.2256286263465883 	 ± 0.21348954645600227
	data : 0.11390805244445801
	model : 0.06484794616699219
			 train-loss:  2.223558518944717 	 ± 0.21127599699031085
	data : 0.11396932601928711
	model : 0.0648888111114502
			 train-loss:  2.227137633732387 	 ± 0.2099999065181468
	data : 0.11408610343933105
	model : 0.06493110656738281
			 train-loss:  2.220313914986544 	 ± 0.21220280872622657
	data : 0.11411924362182617
	model : 0.06495199203491211
			 train-loss:  2.2241627953269263 	 ± 0.2112903666687467
	data : 0.11406311988830567
	model : 0.06494011878967285
			 train-loss:  2.220304595099555 	 ± 0.21049111577333607
	data : 0.1139791488647461
	model : 0.06486635208129883
			 train-loss:  2.219040741091189 	 ± 0.20836315541951894
	data : 0.11411166191101074
	model : 0.06481647491455078
			 train-loss:  2.2152044773101807 	 ± 0.20777019779306516
	data : 0.11408953666687012
	model : 0.06484441757202149
			 train-loss:  2.210467852652073 	 ± 0.20814318652002176
	data : 0.11411657333374023
	model : 0.06482925415039062
			 train-loss:  2.209120212769022 	 ± 0.20621979941959967
	data : 0.11426277160644531
	model : 0.06483707427978516
			 train-loss:  2.2081028962135316 	 ± 0.20427135206615826
	data : 0.11443629264831542
	model : 0.06484661102294922
			 train-loss:  2.205078543401232 	 ± 0.20338620531222548
	data : 0.11425795555114746
	model : 0.064876127243042
			 train-loss:  2.207261477525418 	 ± 0.20202345104564257
	data : 0.11432399749755859
	model : 0.06481456756591797
			 train-loss:  2.20904822394533 	 ± 0.20052285973085338
	data : 0.1142146110534668
	model : 0.06475749015808105
			 train-loss:  2.205927555207853 	 ± 0.19995235138597575
	data : 0.1141247272491455
	model : 0.06470723152160644
			 train-loss:  2.212874497066845 	 ± 0.20459730363810325
	data : 0.11415810585021972
	model : 0.06474833488464356
			 train-loss:  2.2106239135776247 	 ± 0.20344812011069416
	data : 0.11417613029479981
	model : 0.06478543281555176
			 train-loss:  2.2068317082890294 	 ± 0.20364258700346088
	data : 0.11429634094238281
	model : 0.06487760543823243
			 train-loss:  2.2020746880564195 	 ± 0.20504917648771026
	data : 0.11441073417663575
	model : 0.06499547958374023
			 train-loss:  2.1995429911855924 	 ± 0.20421627073141263
	data : 0.11442027091979981
	model : 0.06502957344055176
			 train-loss:  2.2041536450386046 	 ± 0.20558074671307883
	data : 0.11437973976135254
	model : 0.06500067710876464
			 train-loss:  2.2010392751850065 	 ± 0.2053108814930852
	data : 0.11429505348205567
	model : 0.06496796607971192
			 train-loss:  2.2054932309735205 	 ± 0.20659811021133492
	data : 0.11404380798339844
	model : 0.06483488082885742
			 train-loss:  2.202875746621026 	 ± 0.20598555925321474
	data : 0.11393475532531738
	model : 0.06478338241577149
			 train-loss:  2.2108348682522774 	 ± 0.21391115202297228
	data : 0.11398887634277344
	model : 0.06475973129272461
			 train-loss:  2.2182280943943904 	 ± 0.2203457255324261
	data : 0.11393980979919434
	model : 0.06475872993469238
			 train-loss:  2.2181123928590254 	 ± 0.2186720579409644
	data : 0.11410503387451172
	model : 0.06476073265075684
			 train-loss:  2.2233311809710603 	 ± 0.22113646197710915
	data : 0.11419658660888672
	model : 0.06483688354492187
			 train-loss:  2.22056181641186 	 ± 0.22067180609037657
	data : 0.11427197456359864
	model : 0.06483702659606934
			 train-loss:  2.2165583493053047 	 ± 0.22154049997556488
	data : 0.11417336463928222
	model : 0.06483316421508789
			 train-loss:  2.2147320543016704 	 ± 0.22047491217975193
	data : 0.11414117813110351
	model : 0.06485304832458497
			 train-loss:  2.214508096936723 	 ± 0.21892478491148654
	data : 0.11403365135192871
	model : 0.06483774185180664
			 train-loss:  2.2117999494075775 	 ± 0.21859348664641123
	data : 0.11414408683776855
	model : 0.06487412452697754
			 train-loss:  2.20573519680598 	 ± 0.2231071432230556
	data : 0.11408033370971679
	model : 0.06494326591491699
			 train-loss:  2.207768430580964 	 ± 0.22227443132987743
	data : 0.11412668228149414
	model : 0.06494650840759278
			 train-loss:  2.2087991015116373 	 ± 0.2209655767203637
	data : 0.1142035961151123
	model : 0.06494746208190919
			 train-loss:  2.2069144092108073 	 ± 0.22011303101278937
	data : 0.11421432495117187
	model : 0.06494145393371582
			 train-loss:  2.2114604138708733 	 ± 0.22224121348595954
	data : 0.11413736343383789
	model : 0.06484308242797851
			 train-loss:  2.2129250581447897 	 ± 0.22118570498546675
	data : 0.11418242454528808
	model : 0.06474270820617675
			 train-loss:  2.2156749646874925 	 ± 0.22111913315430984
	data : 0.11413483619689942
	model : 0.06474313735961915
			 train-loss:  2.210261602699757 	 ± 0.22493900220201393
	data : 0.11406092643737793
	model : 0.06478219032287598
			 train-loss:  2.2069124380747476 	 ± 0.22554433231006057
	data : 0.11405415534973144
	model : 0.06483316421508789
			 train-loss:  2.199920200720066 	 ± 0.23283058009350643
	data : 0.11416182518005372
	model : 0.06499061584472657
			 train-loss:  2.199968271944896 	 ± 0.23142414531229755
	data : 0.11420679092407227
	model : 0.06506085395812988
			 train-loss:  2.198997755845388 	 ± 0.23021235396282
	data : 0.1143272876739502
	model : 0.06508398056030273
			 train-loss:  2.198048521490658 	 ± 0.22901946006394694
	data : 0.11425552368164063
	model : 0.06507515907287598
			 train-loss:  2.199420596277991 	 ± 0.22803519566402328
	data : 0.11432642936706543
	model : 0.06499485969543457
			 train-loss:  2.198976818172411 	 ± 0.2267582093038482
	data : 0.11425714492797852
	model : 0.0649233341217041
			 train-loss:  2.204270923679525 	 ± 0.23081025225457139
	data : 0.11422624588012695
	model : 0.06487560272216797
			 train-loss:  2.2041033841250988 	 ± 0.22951528367933388
	data : 0.11421871185302734
	model : 0.06486258506774903
			 train-loss:  2.2015824251704745 	 ± 0.2294723912011533
	data : 0.11435976028442382
	model : 0.06485819816589355
			 train-loss:  2.2005285234241696 	 ± 0.2284269848702217
	data : 0.11438999176025391
	model : 0.06491556167602539
			 train-loss:  2.1972980317862136 	 ± 0.2292627499909998
	data : 0.11425786018371582
	model : 0.06493234634399414
			 train-loss:  2.2013095014838764 	 ± 0.23125026680641167
	data : 0.11416521072387695
	model : 0.06486806869506836
			 train-loss:  2.2025023470533656 	 ± 0.23030439178535905
	data : 0.11407914161682128
	model : 0.06485967636108399
			 train-loss:  2.19941229694768 	 ± 0.23103970742194252
	data : 0.11411247253417969
	model : 0.06486372947692871
			 train-loss:  2.1983200969795385 	 ± 0.23007963212856053
	data : 0.11403107643127441
	model : 0.0648660659790039
			 train-loss:  2.196935733569037 	 ± 0.22929212579245578
	data : 0.11408758163452148
	model : 0.06489248275756836
			 train-loss:  2.1975336768189253 	 ± 0.22819527076257484
	data : 0.11425862312316895
	model : 0.06500306129455566
			 train-loss:  2.1967605231988308 	 ± 0.22716881826996999
	data : 0.11423873901367188
	model : 0.06502327919006348
			 train-loss:  2.2005278193950653 	 ± 0.22911716395027876
	data : 0.11403546333312989
	model : 0.06495366096496583
			 train-loss:  2.2025553767043764 	 ± 0.22887993451865318
	data : 0.11399011611938477
	model : 0.06492280960083008
			 train-loss:  2.2050956475968455 	 ± 0.22918155804389034
	data : 0.11403226852416992
	model : 0.06489748954772949
			 train-loss:  2.2054645031401257 	 ± 0.22809673504615685
	data : 0.11398730278015137
	model : 0.0649174690246582
			 train-loss:  2.2084543876923046 	 ± 0.2290166136798015
	data : 0.11403388977050781
	model : 0.06493587493896484
			 train-loss:  2.20594082900456 	 ± 0.2293603462087293
	data : 0.11431770324707032
	model : 0.06502108573913574
			 train-loss:  2.2063995206131124 	 ± 0.2283242772778508
	data : 0.1144782543182373
	model : 0.06505584716796875
			 train-loss:  2.206424961580294 	 ± 0.2272549877934675
	data : 0.11442689895629883
	model : 0.06502690315246581
			 train-loss:  2.2037113904953003 	 ± 0.2279353585801158
	data : 0.11423888206481933
	model : 0.064910888671875
			 train-loss:  2.201668392627611 	 ± 0.2278785979398815
	data : 0.11429152488708497
	model : 0.06486201286315918
			 train-loss:  2.201904109391299 	 ± 0.22685377011282581
	data : 0.11416983604431152
	model : 0.06484045982360839
			 train-loss:  2.2010869840243914 	 ± 0.2259921503396203
	data : 0.1141587257385254
	model : 0.06485462188720703
			 train-loss:  2.199684776365757 	 ± 0.22546550658707604
	data : 0.11417021751403808
	model : 0.06487917900085449
			 train-loss:  2.199551532753801 	 ± 0.22447008383832084
	data : 0.11434965133666992
	model : 0.06495161056518554
			 train-loss:  2.202348797990565 	 ± 0.22545292011642784
	data : 0.1143944263458252
	model : 0.06498808860778808
			 train-loss:  2.20594059177067 	 ± 0.22772294510988297
	data : 0.11439251899719238
	model : 0.06494765281677246
			 train-loss:  2.2086013483590095 	 ± 0.22852756331713478
	data : 0.11408367156982421
	model : 0.06486067771911622
			 train-loss:  2.2064540426955266 	 ± 0.2287211137769333
	data : 0.11403827667236328
	model : 0.06486287117004394
			 train-loss:  2.2037959583735063 	 ± 0.22955754761160818
	data : 0.11405458450317382
	model : 0.06490707397460938
			 train-loss:  2.208481462061906 	 ± 0.23418882128202226
	data : 0.11397147178649902
	model : 0.06490354537963867
			 train-loss:  2.2107385098934174 	 ± 0.2345071104211653
	data : 0.11400532722473145
	model : 0.06492743492126465
			 train-loss:  2.208323387075062 	 ± 0.23502985042061214
	data : 0.11420607566833496
	model : 0.06500740051269531
			 train-loss:  2.208847746497295 	 ± 0.234135689160643
	data : 0.11438689231872559
	model : 0.06498055458068848
			 train-loss:  2.2113555883004414 	 ± 0.23482147427098318
	data : 0.11424503326416016
	model : 0.06495833396911621
			 train-loss:  2.2134419535436938 	 ± 0.23501456856931527
	data : 0.11406459808349609
	model : 0.06488962173461914
			 train-loss:  2.2163231534957886 	 ± 0.23626120183524155
	data : 0.11399893760681153
	model : 0.06487894058227539
			 train-loss:  2.2153063984144303 	 ± 0.23559619817961458
	data : 0.11415314674377441
	model : 0.06487159729003907
			 train-loss:  2.2133070080299078 	 ± 0.23573758647292392
	data : 0.11414680480957032
	model : 0.06495485305786133
			 train-loss:  2.213363694958389 	 ± 0.23481579991726112
	data : 0.1142995834350586
	model : 0.06493678092956542
			 train-loss:  2.2169884554175443 	 ± 0.23747169869816107
	data : 0.11444687843322754
	model : 0.06502285003662109
			 train-loss:  2.2170788682424103 	 ± 0.23655881163474068
	data : 0.11454367637634277
	model : 0.0650240421295166
			 train-loss:  2.2134749661875137 	 ± 0.23920984416938412
	data : 0.11440939903259277
	model : 0.06499567031860351
			 train-loss:  2.2140176485885275 	 ± 0.2383829577043788
	data : 0.11430950164794922
	model : 0.06487364768981933
			 train-loss:  2.212555248038213 	 ± 0.23807869566107315
	data : 0.1141430377960205
	model : 0.06489090919494629
			 train-loss:  2.2112188792940395 	 ± 0.23768885448561433
	data : 0.11429305076599121
	model : 0.0648965835571289
			 train-loss:  2.2113943338394164 	 ± 0.23681559876046157
	data : 0.11409130096435546
	model : 0.06493520736694336
			 train-loss:  2.2079347766497555 	 ± 0.2393428821981433
	data : 0.11410984992980958
	model : 0.06496267318725586
			 train-loss:  2.206518530845642 	 ± 0.23903903187046036
	data : 0.11418237686157226
	model : 0.06504836082458496
			 train-loss:  2.208019742931145 	 ± 0.23881865845812605
	data : 0.11439352035522461
	model : 0.065020751953125
			 train-loss:  2.2064951160828845 	 ± 0.2386311206882471
	data : 0.1141230583190918
	model : 0.06497006416320801
			 train-loss:  2.203913233961378 	 ± 0.23971786151789762
	data : 0.11418910026550293
	model : 0.06492471694946289
			 train-loss:  2.20308814319313 	 ± 0.23906570408134484
	data : 0.11406054496765136
	model : 0.06493396759033203
			 train-loss:  2.202414892089199 	 ± 0.23835653914909669
	data : 0.11448721885681153
	model : 0.06493673324584961
			 train-loss:  2.203846432945945 	 ± 0.23813345436185848
	data : 0.11435785293579101
	model : 0.06495060920715331
			 train-loss:  2.2042510790957346 	 ± 0.2373544908299548
	data : 0.11481475830078125
	model : 0.06501755714416504
			 train-loss:  2.205606125141012 	 ± 0.23709286743262153
	data : 0.11494431495666504
	model : 0.06506309509277344
			 train-loss:  2.203773701027648 	 ± 0.23730757442227207
	data : 0.11515598297119141
	model : 0.0650609016418457
			 train-loss:  2.2040301196429195 	 ± 0.23651932267183204
	data : 0.11472096443176269
	model : 0.06509795188903808
			 train-loss:  2.2050897317963676 	 ± 0.23606875266733154
	data : 0.11479921340942383
	model : 0.0650559902191162
			 train-loss:  2.2042337744027978 	 ± 0.23550557028572536
	data : 0.11446294784545899
	model : 0.06502308845520019
			 train-loss:  2.204777043660482 	 ± 0.2348128983385837
	data : 0.11482825279235839
	model : 0.06496310234069824
			 train-loss:  2.2022504340733913 	 ± 0.23607099416861133
	data : 0.11472487449645996
	model : 0.0649345874786377
			 train-loss:  2.2022379879888736 	 ± 0.23529321311301712
	data : 0.11463136672973633
	model : 0.06490211486816407
			 train-loss:  2.200307740105523 	 ± 0.23572733695977466
	data : 0.11454873085021973
	model : 0.0649601936340332
			 train-loss:  2.1996396253635355 	 ± 0.23510603138250535
	data : 0.11464576721191407
	model : 0.06499242782592773
			 train-loss:  2.1988285172370174 	 ± 0.23456246529935318
	data : 0.11425361633300782
	model : 0.06502995491027833
			 train-loss:  2.1982400860541906 	 ± 0.2339241963719022
	data : 0.11419463157653809
	model : 0.06507530212402343
			 train-loss:  2.1972421172318186 	 ± 0.23351093798583406
	data : 0.11419987678527832
	model : 0.06502785682678222
			 train-loss:  2.1953596140764935 	 ± 0.23396287684937953
	data : 0.11407318115234374
	model : 0.06493644714355469
			 train-loss:  2.197688641788075 	 ± 0.23505618502279144
	data : 0.11393437385559083
	model : 0.06487302780151367
			 train-loss:  2.2011897452175617 	 ± 0.238443006009497
	data : 0.11372241973876954
	model : 0.06487140655517579
			 train-loss:  2.2022761373046023 	 ± 0.23809823538763444
	data : 0.11370091438293457
	model : 0.06493148803710938
			 train-loss:  2.2030664601443726 	 ± 0.2375739651406687
	data : 0.11368808746337891
	model : 0.06499834060668945
			 train-loss:  2.2043333938516723 	 ± 0.2373924015802518
	data : 0.11379952430725097
	model : 0.06507229804992676
			 train-loss:  2.206840405376946 	 ± 0.23882210064575268
	data : 0.11386227607727051
	model : 0.06507997512817383
			 train-loss:  2.2080398508996675 	 ± 0.23859225650355356
	data : 0.11381316184997559
	model : 0.06500210762023925
			 train-loss:  2.207663882927722 	 ± 0.2379215387962581
	data : 0.11396522521972656
	model : 0.06489653587341308
			 train-loss:  2.206297262700018 	 ± 0.2378607294953488
	data : 0.11404805183410645
	model : 0.06484880447387695
			 train-loss:  2.20760848905359 	 ± 0.23775634583245586
	data : 0.11407146453857422
	model : 0.06485137939453126
			 train-loss:  2.2087978576061995 	 ± 0.2375526187864223
	data : 0.11416363716125488
	model : 0.0648806095123291
			 train-loss:  2.207772226193372 	 ± 0.2372278917574491
	data : 0.11442203521728515
	model : 0.06500020027160644
			 train-loss:  2.2084999300583066 	 ± 0.23672344821618263
	data : 0.1142993450164795
	model : 0.06501960754394531
			 train-loss:  2.2075477207815926 	 ± 0.23636250716033325
	data : 0.11419110298156739
	model : 0.06500349044799805
			 train-loss:  2.206657413113324 	 ± 0.23596745159047872
	data : 0.1141860008239746
	model : 0.06497001647949219
			 train-loss:  2.2060196077686616 	 ± 0.2354379115414706
	data : 0.11409754753112793
	model : 0.06498947143554687
			 train-loss:  2.204787685530526 	 ± 0.23532600706060477
	data : 0.11413893699645997
	model : 0.06496629714965821
			 train-loss:  2.203839062967084 	 ± 0.23499182929087273
	data : 0.11420526504516601
	model : 0.06496648788452149
			 train-loss:  2.20715396619786 	 ± 0.23841804999451147
	data : 0.11429190635681152
	model : 0.06496615409851074
			 train-loss:  2.2055943119391967 	 ± 0.23865116639096928
	data : 0.11435332298278808
	model : 0.06501555442810059
			 train-loss:  2.206058723300529 	 ± 0.23806425392886127
	data : 0.11435432434082031
	model : 0.06502623558044433
			 train-loss:  2.2063753220770095 	 ± 0.23743982858294413
	data : 0.11418976783752441
	model : 0.0649451732635498
			 train-loss:  2.206361330675157 	 ± 0.2367830833954501
	data : 0.11416068077087402
	model : 0.06490950584411621
			 train-loss:  2.205902554176666 	 ± 0.23621233776992465
	data : 0.11411480903625489
	model : 0.06492781639099121
			 train-loss:  2.2037250272563247 	 ± 0.23739070400405227
	data : 0.11414833068847656
	model : 0.06493782997131348
			 train-loss:  2.202045345435972 	 ± 0.2378326644981944
	data : 0.11414899826049804
	model : 0.06501789093017578
			 train-loss:  2.2028814502664513 	 ± 0.23746000091342176
	data : 0.11427135467529297
	model : 0.0651352882385254
			 train-loss:  2.2037964296597305 	 ± 0.23714757939619913
	data : 0.11433553695678711
	model : 0.0651822566986084
			 train-loss:  2.2023037550921107 	 ± 0.23738713870943215
	data : 0.11438360214233398
	model : 0.06521706581115723
			 train-loss:  2.201840877532959 	 ± 0.2368395475227365
	data : 0.11428017616271972
	model : 0.06517171859741211
			 train-loss:  2.2011309767526295 	 ± 0.23641262101918387
	data : 0.11427502632141114
	model : 0.0650712490081787
			 train-loss:  2.201132919913844 	 ± 0.23578966329152656
	data : 0.11413493156433105
	model : 0.06503214836120605
			 train-loss:  2.1989376445091207 	 ± 0.23711039043809384
	data : 0.1141383171081543
	model : 0.06505208015441895
			 train-loss:  2.198864762981733 	 ± 0.23649425432089433
	data : 0.11417965888977051
	model : 0.06504087448120117
			 train-loss:  2.1978404744301434 	 ± 0.23630738895546005
	data : 0.1143221378326416
	model : 0.0651017189025879
			 train-loss:  2.1989680676116157 	 ± 0.23621755637998423
	data : 0.11445384025573731
	model : 0.06508903503417969
			 train-loss:  2.198423627706674 	 ± 0.23573309284387622
	data : 0.11450824737548829
	model : 0.06510591506958008
			 train-loss:  2.1973336789072775 	 ± 0.2356230632262266
	data : 0.11452131271362305
	model : 0.0650364875793457
			 train-loss:  2.198028242527531 	 ± 0.2352253463346956
	data : 0.1143655776977539
	model : 0.06493077278137208
			 train-loss:  2.1974130676250265 	 ± 0.23478940981021557
	data : 0.11422662734985352
	model : 0.06483621597290039
			 train-loss:  2.197719318782864 	 ± 0.23423838695219643
	data : 0.11408519744873047
	model : 0.06485052108764648
			 train-loss:  2.197328872680664 	 ± 0.23371696771082534
	data : 0.11413187980651855
	model : 0.06482834815979004
			 train-loss:  2.198081415090988 	 ± 0.23337764610898928
	data : 0.11407437324523925
	model : 0.0648984432220459
			 train-loss:  2.1987056047609537 	 ± 0.2329673979975952
	data : 0.11420044898986817
	model : 0.06495804786682129
			 train-loss:  2.1993957341010937 	 ± 0.23259978100870665
	data : 0.11423239707946778
	model : 0.06499671936035156
			 train-loss:  2.199111115698721 	 ± 0.23206441684743412
	data : 0.11419539451599121
	model : 0.0649749755859375
			 train-loss:  2.1991768883495797 	 ± 0.23149962021435053
	data : 0.11403331756591797
	model : 0.06490225791931152
			 train-loss:  2.1984956299217 	 ± 0.23114294632093968
	data : 0.11393013000488281
	model : 0.06484823226928711
			 train-loss:  2.1986579365200467 	 ± 0.23059572119591804
	data : 0.11390256881713867
	model : 0.06483473777770996
			 train-loss:  2.1989837988064838 	 ± 0.23008850708766868
	data : 0.11394147872924805
	model : 0.06484932899475097
			 train-loss:  2.1973052988782453 	 ± 0.23081037131400284
	data : 0.11393675804138184
	model : 0.06489391326904297
			 train-loss:  2.196585972536178 	 ± 0.23049487514636743
	data : 0.11408462524414062
	model : 0.06498379707336426
			 train-loss:  2.1958575491656624 	 ± 0.2301901880191214
	data : 0.11415133476257325
	model : 0.065010404586792
			 train-loss:  2.1950955014183835 	 ± 0.22991327224399535
	data : 0.11402435302734375
	model : 0.06496601104736328
			 train-loss:  2.194496495622984 	 ± 0.2295386906262107
	data : 0.11407318115234374
	model : 0.06493673324584961
			 train-loss:  2.1968718048568086 	 ± 0.2316108176458288
	data : 0.11402578353881836
	model : 0.06489777565002441
			 train-loss:  2.1964491383973943 	 ± 0.2311542695131426
	data : 0.11409354209899902
	model : 0.06490178108215332
			 train-loss:  2.198107902098585 	 ± 0.23189759901836607
	data : 0.11414861679077148
	model : 0.0648888111114502
			 train-loss:  2.197241621083378 	 ± 0.23171269736205768
	data : 0.11437497138977051
	model : 0.06494956016540528
			 train-loss:  2.1980278716174833 	 ± 0.2314705885700033
	data : 0.11420011520385742
	model : 0.06494898796081543
			 train-loss:  2.19824925786284 	 ± 0.23096464375870035
	data : 0.1143369197845459
	model : 0.06491503715515137
			 train-loss:  2.197033878889951 	 ± 0.23113997035013945
	data : 0.11409411430358887
	model : 0.06483941078186035
			 train-loss:  2.19673820858088 	 ± 0.23065813082003322
	data : 0.11420187950134278
	model : 0.06479811668395996
			 train-loss:  2.196907604062879 	 ± 0.2301518214936122
	data : 0.11409244537353516
	model : 0.06471915245056152
			 train-loss:  2.195576642126246 	 ± 0.23048989560582597
	data : 0.11435775756835938
	model : 0.06470808982849122
			 train-loss:  2.1955535848225867 	 ± 0.22997509150039405
	data : 0.11472940444946289
	model : 0.06465015411376954
			 train-loss:  2.195006106694539 	 ± 0.22960971790441329
	data : 0.1148996353149414
	model : 0.06454195976257324
			 train-loss:  2.1975000505953766 	 ± 0.23213528513736395
	data : 0.11488323211669922
	model : 0.06438121795654297
			 train-loss:  2.197140886920139 	 ± 0.2316863343711844
	data : 0.11493768692016601
	model : 0.06426820755004883
			 train-loss:  2.1983261526676645 	 ± 0.23186639973271486
	data : 0.11486806869506835
	model : 0.06407284736633301
			 train-loss:  2.197345485333272 	 ± 0.23183297471814876
	data : 0.11438794136047363
	model : 0.06392974853515625
			 train-loss:  2.1968970303950104 	 ± 0.2314279633468427
	data : 0.1145702838897705
	model : 0.06390514373779296
			 train-loss:  2.1964957533460674 	 ± 0.231006668413289
	data : 0.11460938453674316
	model : 0.06394076347351074
			 train-loss:  2.1968923277896026 	 ± 0.2305870614959798
	data : 0.11469573974609375
	model : 0.06395187377929687
			 train-loss:  2.196894203132826 	 ± 0.23009170918133845
	data : 0.11475534439086914
	model : 0.06396174430847168
			 train-loss:  2.1986283232004213 	 ± 0.2311203520035217
	data : 0.11507558822631836
	model : 0.06397757530212403
			 train-loss:  2.1980646321114072 	 ± 0.23078922267997776
	data : 0.11496090888977051
	model : 0.0639425277709961
			 train-loss:  2.198220673759105 	 ± 0.23031216603164736
	data : 0.11466302871704101
	model : 0.06382131576538086
			 train-loss:  2.198149559870048 	 ± 0.22982835826720382
	data : 0.11469807624816894
	model : 0.06385035514831543
			 train-loss:  2.1991149202114393 	 ± 0.22982602584673667
	data : 0.11479353904724121
	model : 0.06388649940490723
			 train-loss:  2.198002844176033 	 ± 0.2299855118984898
	data : 0.11471166610717773
	model : 0.06391921043395996
			 train-loss:  2.197364664077759 	 ± 0.22971783829683864
	data : 0.11479434967041016
	model : 0.06391391754150391
			 train-loss:  2.1961609357620175 	 ± 0.22999798311851483
	data : 0.11505289077758789
	model : 0.06397843360900879
			 train-loss:  2.200242781442059 	 ± 0.23810899582440023
	data : 0.11483526229858398
	model : 0.06386270523071289
			 train-loss:  2.1992352445429737 	 ± 0.23813491777281037
	data : 0.11467947959899902
	model : 0.06382064819335938
			 train-loss:  2.199748745218652 	 ± 0.23778120868185887
	data : 0.11457333564758301
	model : 0.06380043029785157
			 train-loss:  2.2004038922640743 	 ± 0.2375160146052666
	data : 0.11453962326049805
	model : 0.06382074356079101
			 train-loss:  2.1996423505186065 	 ± 0.23733229721131835
	data : 0.11463866233825684
	model : 0.06387648582458497
			 train-loss:  2.1988585627513375 	 ± 0.237170191696721
	data : 0.11493401527404785
	model : 0.06395149230957031
			 train-loss:  2.198962603365221 	 ± 0.23669719086352028
	data : 0.11496796607971191
	model : 0.06395282745361328
			 train-loss:  2.1989469054233597 	 ± 0.23622154649084479
	data : 0.11488261222839355
	model : 0.06395363807678223
			 train-loss:  2.1988773379325868 	 ± 0.23575118582888946
	data : 0.11481676101684571
	model : 0.0638960361480713
			 train-loss:  2.198488440171656 	 ± 0.23536143102778856
	data : 0.11474370956420898
	model : 0.06385941505432129
			 train-loss:  2.1971506869036053 	 ± 0.23584818699635676
	data : 0.11457948684692383
	model : 0.06388964653015136
			 train-loss:  2.1985523450987143 	 ± 0.23643096004016312
	data : 0.11459379196166992
	model : 0.06391725540161133
			 train-loss:  2.1999001817440424 	 ± 0.23693699078260289
	data : 0.11482162475585937
	model : 0.06393613815307617
			 train-loss:  2.199188319842021 	 ± 0.23674395019025138
	data : 0.11487598419189453
	model : 0.06403155326843261
			 train-loss:  2.19899986917153 	 ± 0.23630027002447246
	data : 0.11451563835144044
	model : 0.055601978302001955
#epoch  77    val-loss:  2.4051224056043123  train-loss:  2.19899986917153  lr:  1.9073486328125e-08
			 train-loss:  2.038869619369507 	 ± 0.0
	data : 5.7616095542907715
	model : 0.07327389717102051
			 train-loss:  2.1951444149017334 	 ± 0.15627479553222656
	data : 2.940951943397522
	model : 0.06978428363800049
			 train-loss:  2.4290099143981934 	 ± 0.35449591185496565
	data : 1.9992873668670654
	model : 0.06801772117614746
			 train-loss:  2.3992886543273926 	 ± 0.3112885695652849
	data : 1.5279486775398254
	model : 0.06715530157089233
			 train-loss:  2.343326711654663 	 ± 0.3000790145018882
	data : 1.2451873779296876
	model : 0.06664071083068848
			 train-loss:  2.3517125447591147 	 ± 0.2745744406130432
	data : 0.11563420295715332
	model : 0.06487340927124023
			 train-loss:  2.2799535989761353 	 ± 0.30905831285719376
	data : 0.1142852783203125
	model : 0.06454544067382813
			 train-loss:  2.2370014637708664 	 ± 0.3106309902178307
	data : 0.1138798713684082
	model : 0.06463241577148438
			 train-loss:  2.2621021138297186 	 ± 0.3013480760170909
	data : 0.11396851539611816
	model : 0.06468510627746582
			 train-loss:  2.2992673993110655 	 ± 0.30685651798759683
	data : 0.11391510963439941
	model : 0.06474928855895996
			 train-loss:  2.293891657482494 	 ± 0.293069658542058
	data : 0.1139552116394043
	model : 0.06483263969421386
			 train-loss:  2.300454447666804 	 ± 0.2814358039171598
	data : 0.11405649185180664
	model : 0.06500310897827148
			 train-loss:  2.3188204490221462 	 ± 0.27777878603449185
	data : 0.1138801097869873
	model : 0.06490950584411621
			 train-loss:  2.311487989766257 	 ± 0.26897675471934324
	data : 0.11369924545288086
	model : 0.06485710144042969
			 train-loss:  2.3014270226160685 	 ± 0.2625688184978436
	data : 0.1136887550354004
	model : 0.064841890335083
			 train-loss:  2.29017885774374 	 ± 0.25793662712829324
	data : 0.1137664794921875
	model : 0.06482696533203125
			 train-loss:  2.269694959416109 	 ± 0.2633080685632475
	data : 0.11390123367309571
	model : 0.06474246978759765
			 train-loss:  2.248690108458201 	 ± 0.2701478724915089
	data : 0.1140861988067627
	model : 0.06481318473815918
			 train-loss:  2.2425330877304077 	 ± 0.2642369908625012
	data : 0.11421775817871094
	model : 0.06489524841308594
			 train-loss:  2.2529344260692596 	 ± 0.26150659058351133
	data : 0.11416206359863282
	model : 0.06488404273986817
			 train-loss:  2.2860350665592013 	 ± 0.295029292066856
	data : 0.11405019760131836
	model : 0.06482715606689453
			 train-loss:  2.2824216051535173 	 ± 0.28872134477771155
	data : 0.1137629508972168
	model : 0.06478581428527833
			 train-loss:  2.2832208560860674 	 ± 0.2823999293558551
	data : 0.11375389099121094
	model : 0.06484832763671874
			 train-loss:  2.2859268337488174 	 ± 0.2767584295799808
	data : 0.1137089729309082
	model : 0.06481809616088867
			 train-loss:  2.282953085899353 	 ± 0.27155783074764533
	data : 0.11376667022705078
	model : 0.06486907005310058
			 train-loss:  2.2758352985748878 	 ± 0.2686520675711628
	data : 0.11380701065063477
	model : 0.06493687629699707
			 train-loss:  2.2757451136906943 	 ± 0.2636304918471741
	data : 0.11394734382629394
	model : 0.06492133140563965
			 train-loss:  2.2897613091128215 	 ± 0.2689295580392255
	data : 0.11386003494262695
	model : 0.06481261253356933
			 train-loss:  2.297300120879864 	 ± 0.26724623488346627
	data : 0.11374926567077637
	model : 0.06478409767150879
			 train-loss:  2.2820352713267007 	 ± 0.27531312787796647
	data : 0.11368169784545898
	model : 0.06483187675476074
			 train-loss:  2.2841594834481516 	 ± 0.2710859861917296
	data : 0.11366729736328125
	model : 0.06488223075866699
			 train-loss:  2.273388020694256 	 ± 0.2734737295298853
	data : 0.11374578475952149
	model : 0.06490983963012695
			 train-loss:  2.268862926598751 	 ± 0.270512159757879
	data : 0.11385517120361328
	model : 0.06497268676757813
			 train-loss:  2.259911488084232 	 ± 0.27141997202059526
	data : 0.11405429840087891
	model : 0.0649993896484375
			 train-loss:  2.249403350693839 	 ± 0.27444178010649517
	data : 0.11407661437988281
	model : 0.06489429473876954
			 train-loss:  2.2467848824130163 	 ± 0.2710462858850164
	data : 0.11402091979980469
	model : 0.06487669944763183
			 train-loss:  2.2380897805497453 	 ± 0.27240097611325975
	data : 0.11394791603088379
	model : 0.06485948562622071
			 train-loss:  2.236125500578629 	 ± 0.26905828465798154
	data : 0.11392364501953126
	model : 0.06485319137573242
			 train-loss:  2.228155649625338 	 ± 0.2700922992061553
	data : 0.11393179893493652
	model : 0.06489424705505371
			 train-loss:  2.2258906424045564 	 ± 0.2670696236731223
	data : 0.1140367031097412
	model : 0.06491632461547851
			 train-loss:  2.2220025062561035 	 ± 0.26493626587460395
	data : 0.11412320137023926
	model : 0.06485123634338379
			 train-loss:  2.2305701006026495 	 ± 0.26745009987223556
	data : 0.11412196159362793
	model : 0.06482338905334473
			 train-loss:  2.2294115465740827 	 ± 0.26442853905987035
	data : 0.1140517234802246
	model : 0.06477589607238769
			 train-loss:  2.227479414506392 	 ± 0.261713259891266
	data : 0.11392993927001953
	model : 0.06476225852966308
			 train-loss:  2.2228290610843233 	 ± 0.26062095063262847
	data : 0.11393065452575683
	model : 0.06478915214538575
			 train-loss:  2.2207998918450396 	 ± 0.2581317020790685
	data : 0.11402854919433594
	model : 0.0648691177368164
			 train-loss:  2.2163857906422715 	 ± 0.2571197217699162
	data : 0.11411881446838379
	model : 0.06496119499206543
			 train-loss:  2.2115511496861777 	 ± 0.25657711232623587
	data : 0.11426620483398438
	model : 0.0650252342224121
			 train-loss:  2.2123029232025146 	 ± 0.2539988897496095
	data : 0.11427435874938965
	model : 0.06504402160644532
			 train-loss:  2.2116037893295286 	 ± 0.2514936935483652
	data : 0.11421089172363282
	model : 0.06502013206481934
			 train-loss:  2.2093577104456283 	 ± 0.24952182953461383
	data : 0.11406354904174805
	model : 0.06490936279296874
			 train-loss:  2.2082881652391873 	 ± 0.24722895058555802
	data : 0.1138789176940918
	model : 0.06483755111694336
			 train-loss:  2.209697480471629 	 ± 0.24509628032444994
	data : 0.11380348205566407
	model : 0.06480374336242675
			 train-loss:  2.2102506867161504 	 ± 0.24284966274887498
	data : 0.11388196945190429
	model : 0.0647657871246338
			 train-loss:  2.2085958697579122 	 ± 0.2409388785024852
	data : 0.1140207290649414
	model : 0.06478986740112305
			 train-loss:  2.211867707116263 	 ± 0.2400076645817202
	data : 0.11392297744750976
	model : 0.06483855247497558
			 train-loss:  2.2125736746871683 	 ± 0.23795167166702322
	data : 0.11405816078186035
	model : 0.0647953987121582
			 train-loss:  2.2146310765167763 	 ± 0.23640230408200033
	data : 0.11392035484313964
	model : 0.06473870277404785
			 train-loss:  2.2145306781186895 	 ± 0.23439158011765265
	data : 0.11388640403747559
	model : 0.06472501754760743
			 train-loss:  2.21512770652771 	 ± 0.23247534510545118
	data : 0.11382503509521484
	model : 0.06468763351440429
			 train-loss:  2.20660252649276 	 ± 0.23983229069475634
	data : 0.11388096809387208
	model : 0.06474032402038574
			 train-loss:  2.205913124545928 	 ± 0.2379512242466953
	data : 0.11391291618347169
	model : 0.06481070518493652
			 train-loss:  2.1991637396434 	 ± 0.24196363918244884
	data : 0.11413183212280273
	model : 0.06489953994750977
			 train-loss:  2.2077321968972683 	 ± 0.24951348281456778
	data : 0.11417498588562011
	model : 0.06488051414489746
			 train-loss:  2.20746216407189 	 ± 0.24759613320404764
	data : 0.1140782356262207
	model : 0.06485280990600586
			 train-loss:  2.2115422451134883 	 ± 0.247905340852981
	data : 0.11416850090026856
	model : 0.06479921340942382
			 train-loss:  2.2127224580565494 	 ± 0.24623509106481722
	data : 0.11405777931213379
	model : 0.06473789215087891
			 train-loss:  2.210276856141932 	 ± 0.2452362168074453
	data : 0.11396121978759766
	model : 0.06470727920532227
			 train-loss:  2.205315907796224 	 ± 0.24686584369791906
	data : 0.11401171684265136
	model : 0.06474690437316895
			 train-loss:  2.2084917954036167 	 ± 0.24651183790934822
	data : 0.1141141414642334
	model : 0.06477017402648926
			 train-loss:  2.2088880068819288 	 ± 0.24479212924130161
	data : 0.11409139633178711
	model : 0.06480669975280762
			 train-loss:  2.2104976144101887 	 ± 0.24346430835511806
	data : 0.11415395736694336
	model : 0.06483001708984375
			 train-loss:  2.209034504955762 	 ± 0.24210950943274812
	data : 0.1141779899597168
	model : 0.06479716300964355
			 train-loss:  2.211989235233616 	 ± 0.241789607310784
	data : 0.11408090591430664
	model : 0.06478762626647949
			 train-loss:  2.211160596211751 	 ± 0.24027802556338004
	data : 0.11401991844177246
	model : 0.06482911109924316
			 train-loss:  2.2155305429508814 	 ± 0.24167356175531118
	data : 0.11405224800109863
	model : 0.06480770111083985
			 train-loss:  2.213889307789988 	 ± 0.2405250655407838
	data : 0.11409544944763184
	model : 0.06489243507385253
			 train-loss:  2.209027816087772 	 ± 0.24275592006949598
	data : 0.11412863731384278
	model : 0.06499347686767579
			 train-loss:  2.2055585595625864 	 ± 0.2431527701236704
	data : 0.11415815353393555
	model : 0.06504826545715332
			 train-loss:  2.2078117072582244 	 ± 0.24245677148124703
	data : 0.11421270370483398
	model : 0.06498551368713379
			 train-loss:  2.207134947364713 	 ± 0.24103149603594096
	data : 0.1141425609588623
	model : 0.0650134563446045
			 train-loss:  2.2044997404261335 	 ± 0.2407284395751404
	data : 0.11411614418029785
	model : 0.06500492095947266
			 train-loss:  2.2001867524112564 	 ± 0.24244038368545834
	data : 0.11418027877807617
	model : 0.06501665115356445
			 train-loss:  2.201749500774202 	 ± 0.24141315322644089
	data : 0.11428031921386719
	model : 0.06498394012451172
			 train-loss:  2.1982617069693173 	 ± 0.24210843667849405
	data : 0.11441974639892578
	model : 0.0651099681854248
			 train-loss:  2.1940838458926177 	 ± 0.24375918942355582
	data : 0.11454305648803711
	model : 0.06516995429992675
			 train-loss:  2.1951671852462593 	 ± 0.2425623679669227
	data : 0.11458745002746581
	model : 0.06512341499328614
			 train-loss:  2.1936181621118025 	 ± 0.2416126235494517
	data : 0.11456885337829589
	model : 0.06503844261169434
			 train-loss:  2.1924678984652743 	 ± 0.2404936084392343
	data : 0.11436347961425782
	model : 0.06496644020080566
			 train-loss:  2.1912646187676326 	 ± 0.23942306053365925
	data : 0.11433768272399902
	model : 0.06485795974731445
			 train-loss:  2.1909721945668315 	 ± 0.2381200757893352
	data : 0.11421284675598145
	model : 0.06481075286865234
			 train-loss:  2.1886058955088905 	 ± 0.23789576752310948
	data : 0.11417112350463868
	model : 0.0648465633392334
			 train-loss:  2.189776832057584 	 ± 0.23687970536039082
	data : 0.1141209602355957
	model : 0.06489391326904297
			 train-loss:  2.1899535465747753 	 ± 0.23562250084090605
	data : 0.11438198089599609
	model : 0.06495938301086426
			 train-loss:  2.1895106403451217 	 ± 0.23441843547443006
	data : 0.11426539421081543
	model : 0.06499996185302734
			 train-loss:  2.190208292255799 	 ± 0.2332934298719332
	data : 0.11430268287658692
	model : 0.06498260498046875
			 train-loss:  2.1881353019439067 	 ± 0.23297483378249478
	data : 0.11423716545104981
	model : 0.0649139404296875
			 train-loss:  2.186817013487524 	 ± 0.23214650167890988
	data : 0.1142850399017334
	model : 0.06491565704345703
			 train-loss:  2.189004411601057 	 ± 0.23198391378034625
	data : 0.1140854835510254
	model : 0.06492338180541993
			 train-loss:  2.1908826637268066 	 ± 0.23157639379707745
	data : 0.11409721374511719
	model : 0.06497015953063964
			 train-loss:  2.1921836763325304 	 ± 0.2307941137646291
	data : 0.11420502662658691
	model : 0.06497335433959961
			 train-loss:  2.1965059836705527 	 ± 0.23373194995697574
	data : 0.11438722610473633
	model : 0.0649935245513916
			 train-loss:  2.195762483819017 	 ± 0.2327157381994685
	data : 0.11435050964355468
	model : 0.064994478225708
			 train-loss:  2.193120771875748 	 ± 0.23314089784933456
	data : 0.11448879241943359
	model : 0.06500072479248047
			 train-loss:  2.1960266101927983 	 ± 0.23391276122818952
	data : 0.11448211669921875
	model : 0.06491608619689941
			 train-loss:  2.194830171342166 	 ± 0.23312936933217235
	data : 0.11428532600402833
	model : 0.06485528945922851
			 train-loss:  2.197748630960411 	 ± 0.23397480746661745
	data : 0.11413321495056153
	model : 0.06483893394470215
			 train-loss:  2.1973864005671606 	 ± 0.2329192118580879
	data : 0.11412100791931153
	model : 0.06483283042907714
			 train-loss:  2.2000825809776234 	 ± 0.23353529572933873
	data : 0.11404242515563964
	model : 0.06483373641967774
			 train-loss:  2.2010709968480198 	 ± 0.23270027280205047
	data : 0.11405668258666993
	model : 0.06485834121704101
			 train-loss:  2.2037184635798135 	 ± 0.2333079149695939
	data : 0.11420302391052246
	model : 0.06491689682006836
			 train-loss:  2.2026732957788875 	 ± 0.2325249055191522
	data : 0.11425385475158692
	model : 0.06493816375732422
			 train-loss:  2.2022375501362625 	 ± 0.23153967525625122
	data : 0.11426362991333008
	model : 0.0649219036102295
			 train-loss:  2.1993643610100997 	 ± 0.2325364315346691
	data : 0.11432204246520997
	model : 0.06490440368652343
			 train-loss:  2.199241600865903 	 ± 0.2315269062278385
	data : 0.11424422264099121
	model : 0.0648991584777832
			 train-loss:  2.197427482440554 	 ± 0.23134620529435299
	data : 0.11423664093017578
	model : 0.06490154266357422
			 train-loss:  2.2000004328214207 	 ± 0.23201627148622703
	data : 0.11418285369873046
	model : 0.06490607261657715
			 train-loss:  2.200508653107336 	 ± 0.23109645245139115
	data : 0.11426572799682617
	model : 0.06491084098815918
			 train-loss:  2.20492782312281 	 ± 0.23507703745390593
	data : 0.11422162055969239
	model : 0.06488780975341797
			 train-loss:  2.20705441236496 	 ± 0.23524214516124015
	data : 0.11437568664550782
	model : 0.06487736701965333
			 train-loss:  2.206039700626342 	 ± 0.23453161331140995
	data : 0.11427474021911621
	model : 0.06485214233398437
			 train-loss:  2.2072055163930675 	 ± 0.23392022282166403
	data : 0.11423406600952149
	model : 0.06479096412658691
			 train-loss:  2.2068731145160956 	 ± 0.23299631608004998
	data : 0.11411147117614746
	model : 0.06480264663696289
			 train-loss:  2.2083498585608696 	 ± 0.23263215106863333
	data : 0.11414308547973633
	model : 0.06482620239257812
			 train-loss:  2.208414422988892 	 ± 0.23170086937954773
	data : 0.11408004760742188
	model : 0.06486024856567382
			 train-loss:  2.2066859744843983 	 ± 0.23158726410875355
	data : 0.11416845321655274
	model : 0.06492323875427246
			 train-loss:  2.208602216300063 	 ± 0.2316743971624223
	data : 0.11429390907287598
	model : 0.06499528884887695
			 train-loss:  2.2092267274856567 	 ± 0.23087493928241073
	data : 0.11423149108886718
	model : 0.06498379707336426
			 train-loss:  2.209010225857875 	 ± 0.2299913779573383
	data : 0.11399316787719727
	model : 0.06493177413940429
			 train-loss:  2.207687520980835 	 ± 0.22959711035359714
	data : 0.11392903327941895
	model : 0.0648880958557129
			 train-loss:  2.2072218028643658 	 ± 0.22878073763773876
	data : 0.11387057304382324
	model : 0.06485066413879395
			 train-loss:  2.206025844270533 	 ± 0.2283231866068334
	data : 0.11399664878845214
	model : 0.06487936973571777
			 train-loss:  2.2074016431220493 	 ± 0.2280117631024292
	data : 0.11414403915405273
	model : 0.06492705345153808
			 train-loss:  2.207821449237083 	 ± 0.22721096656642145
	data : 0.11435317993164062
	model : 0.06500873565673829
			 train-loss:  2.206108170085483 	 ± 0.22723501235460275
	data : 0.11448903083801269
	model : 0.06508927345275879
			 train-loss:  2.2065436655984207 	 ± 0.2264545866336339
	data : 0.11456708908081055
	model : 0.06509437561035156
			 train-loss:  2.2044696390193743 	 ± 0.22691931761482798
	data : 0.11437811851501464
	model : 0.06506714820861817
			 train-loss:  2.2036415614943574 	 ± 0.22630330563055498
	data : 0.11442050933837891
	model : 0.06499013900756836
			 train-loss:  2.2014660577979877 	 ± 0.2269314315056564
	data : 0.11435165405273437
	model : 0.06493864059448243
			 train-loss:  2.2022240451404027 	 ± 0.22629603260443165
	data : 0.11420154571533203
	model : 0.06485657691955567
			 train-loss:  2.200265312025733 	 ± 0.2266800224772927
	data : 0.1141176700592041
	model : 0.06487798690795898
			 train-loss:  2.1987153862563655 	 ± 0.2266289802052801
	data : 0.11426005363464356
	model : 0.06490020751953125
			 train-loss:  2.201522146905219 	 ± 0.22829847027558678
	data : 0.11431674957275391
	model : 0.06498250961303711
			 train-loss:  2.204964836438497 	 ± 0.2311992596451846
	data : 0.11435914039611816
	model : 0.06501131057739258
			 train-loss:  2.20451612965814 	 ± 0.23046355079971514
	data : 0.11438512802124023
	model : 0.06506400108337403
			 train-loss:  2.203787677908597 	 ± 0.22984038067482485
	data : 0.11452841758728027
	model : 0.06497015953063964
			 train-loss:  2.2019932886370186 	 ± 0.23008114124402831
	data : 0.11425576210021973
	model : 0.06487164497375489
			 train-loss:  2.198991493598835 	 ± 0.2321728463468835
	data : 0.1141481876373291
	model : 0.06481952667236328
			 train-loss:  2.1994755524116876 	 ± 0.231467353066231
	data : 0.11452269554138184
	model : 0.06480937004089356
			 train-loss:  2.199146054585775 	 ± 0.23072956331101618
	data : 0.11461052894592286
	model : 0.06478371620178222
			 train-loss:  2.1993477297145008 	 ± 0.22997755358475658
	data : 0.11456632614135742
	model : 0.06489148139953613
			 train-loss:  2.200285670004393 	 ± 0.2295093817118682
	data : 0.11474776268005371
	model : 0.06495952606201172
			 train-loss:  2.1987761227913154 	 ± 0.229513933197762
	data : 0.11481504440307617
	model : 0.06500086784362794
			 train-loss:  2.1983561384213437 	 ± 0.22882652071679832
	data : 0.1147047996520996
	model : 0.06502008438110352
			 train-loss:  2.1985291150308424 	 ± 0.22809727701132626
	data : 0.11464471817016601
	model : 0.06506428718566895
			 train-loss:  2.198687274486591 	 ± 0.22737354691341843
	data : 0.11452536582946778
	model : 0.06496753692626953
			 train-loss:  2.197439770030368 	 ± 0.22718322181138714
	data : 0.1145369529724121
	model : 0.06497931480407715
			 train-loss:  2.1958080899866323 	 ± 0.22738414698273182
	data : 0.11441183090209961
	model : 0.0649289608001709
			 train-loss:  2.1948653129661606 	 ± 0.22697754435641251
	data : 0.11419339179992676
	model : 0.06493544578552246
			 train-loss:  2.1947442896664144 	 ± 0.2262722738702607
	data : 0.11473097801208496
	model : 0.06490974426269532
			 train-loss:  2.1952981882213805 	 ± 0.22567725496204405
	data : 0.11486806869506835
	model : 0.06498565673828124
			 train-loss:  2.193867199214888 	 ± 0.2257111511811197
	data : 0.11500840187072754
	model : 0.06498847007751465
			 train-loss:  2.193211754406888 	 ± 0.22517231406446245
	data : 0.11512212753295899
	model : 0.06503419876098633
			 train-loss:  2.1911669162715355 	 ± 0.2259977238197091
	data : 0.11506009101867676
	model : 0.06499314308166504
			 train-loss:  2.1904086019053604 	 ± 0.22552102465855475
	data : 0.11447224617004395
	model : 0.06498708724975585
			 train-loss:  2.191169084554695 	 ± 0.22505282428539253
	data : 0.11455812454223632
	model : 0.06495885848999024
			 train-loss:  2.1898822763008985 	 ± 0.22498969527220047
	data : 0.11438236236572266
	model : 0.06491312980651856
			 train-loss:  2.1899897945778712 	 ± 0.22432338675296704
	data : 0.11425890922546386
	model : 0.06487536430358887
			 train-loss:  2.1883707406252797 	 ± 0.22464106693375757
	data : 0.11428976058959961
	model : 0.0649177074432373
			 train-loss:  2.1879328720709856 	 ± 0.22405170468836202
	data : 0.11436648368835449
	model : 0.06491208076477051
			 train-loss:  2.188204699092441 	 ± 0.22342373482989025
	data : 0.11407604217529296
	model : 0.06486735343933106
			 train-loss:  2.187253035085146 	 ± 0.22312062209396719
	data : 0.11403489112854004
	model : 0.064888334274292
			 train-loss:  2.188448665459032 	 ± 0.22302674634883582
	data : 0.11415338516235352
	model : 0.06491765975952149
			 train-loss:  2.187529415234752 	 ± 0.22271338263376944
	data : 0.11418628692626953
	model : 0.06492395401000976
			 train-loss:  2.186986930710929 	 ± 0.22219140723954442
	data : 0.11414332389831543
	model : 0.06497907638549805
			 train-loss:  2.1861593933268026 	 ± 0.2218295715606055
	data : 0.1143099308013916
	model : 0.06505069732666016
			 train-loss:  2.1893385662197393 	 ± 0.22518703821921532
	data : 0.11441807746887207
	model : 0.06504955291748046
			 train-loss:  2.189298678649945 	 ± 0.2245542264884865
	data : 0.11430392265319825
	model : 0.0649794578552246
			 train-loss:  2.1886156091476954 	 ± 0.22411146980133959
	data : 0.11427202224731445
	model : 0.06492524147033692
			 train-loss:  2.1879802061451805 	 ± 0.22364969652251657
	data : 0.11429591178894043
	model : 0.06491379737854004
			 train-loss:  2.187161070865821 	 ± 0.22330162230844294
	data : 0.11433467864990235
	model : 0.06489992141723633
			 train-loss:  2.1875757417836033 	 ± 0.2227571815411922
	data : 0.11429286003112793
	model : 0.06491765975952149
			 train-loss:  2.188497748531279 	 ± 0.22249568035409673
	data : 0.11434445381164551
	model : 0.06497478485107422
			 train-loss:  2.1881742082212283 	 ± 0.22193341039910286
	data : 0.11439747810363769
	model : 0.06501007080078125
			 train-loss:  2.188038434853425 	 ± 0.22134043998648942
	data : 0.11435875892639161
	model : 0.06495509147644044
			 train-loss:  2.187247605093064 	 ± 0.22100655169603067
	data : 0.11422038078308105
	model : 0.06490473747253418
			 train-loss:  2.1862343772847384 	 ± 0.22084757612064213
	data : 0.11407780647277832
	model : 0.06487746238708496
			 train-loss:  2.185360678967009 	 ± 0.22058323548308525
	data : 0.1141444206237793
	model : 0.06483602523803711
			 train-loss:  2.1871578314947704 	 ± 0.22137460079808927
	data : 0.11412386894226074
	model : 0.06484155654907227
			 train-loss:  2.1884144933600176 	 ± 0.22146614284062163
	data : 0.11425228118896484
	model : 0.06486167907714843
			 train-loss:  2.187670686482135 	 ± 0.22112344465205927
	data : 0.11434807777404785
	model : 0.06492023468017578
			 train-loss:  2.1888158209621906 	 ± 0.22111394778688442
	data : 0.11448416709899903
	model : 0.06493806838989258
			 train-loss:  2.188703018148946 	 ± 0.22054590860737783
	data : 0.11441373825073242
	model : 0.06494903564453125
			 train-loss:  2.1885373088502393 	 ± 0.2199888026361932
	data : 0.11426749229431152
	model : 0.06488604545593261
			 train-loss:  2.187063036209498 	 ± 0.22038273187170715
	data : 0.11412076950073242
	model : 0.06485848426818848
			 train-loss:  2.1887868034596347 	 ± 0.2211338228974229
	data : 0.11413412094116211
	model : 0.06483602523803711
			 train-loss:  2.188744558295623 	 ± 0.22057264839554822
	data : 0.114143705368042
	model : 0.06485753059387207
			 train-loss:  2.1894483951607135 	 ± 0.22023661293057734
	data : 0.11417984962463379
	model : 0.0648930549621582
			 train-loss:  2.191093910878627 	 ± 0.22089942304304816
	data : 0.11431446075439453
	model : 0.06496677398681641
			 train-loss:  2.190919439792633 	 ± 0.2203602276207246
	data : 0.11442065238952637
	model : 0.0650064468383789
			 train-loss:  2.190378002859467 	 ± 0.2199447100616628
	data : 0.11429052352905274
	model : 0.06497540473937988
			 train-loss:  2.191203068978716 	 ± 0.21971121841279753
	data : 0.11404738426208497
	model : 0.06486272811889648
			 train-loss:  2.1917374991431027 	 ± 0.21930097063661239
	data : 0.11402907371520996
	model : 0.06486129760742188
			 train-loss:  2.192316059972726 	 ± 0.2189180596933532
	data : 0.11407976150512696
	model : 0.06482677459716797
			 train-loss:  2.1931387610551787 	 ± 0.2186993611434372
	data : 0.1140366554260254
	model : 0.06483244895935059
			 train-loss:  2.193095511603124 	 ± 0.21816877048620276
	data : 0.11426405906677246
	model : 0.0648930549621582
			 train-loss:  2.191865016296866 	 ± 0.2183565449232103
	data : 0.11447706222534179
	model : 0.0650050163269043
			 train-loss:  2.1926049870940356 	 ± 0.21809102735092534
	data : 0.11448783874511718
	model : 0.06503405570983886
			 train-loss:  2.193048146923193 	 ± 0.2176625093328174
	data : 0.11430087089538574
	model : 0.06503052711486816
			 train-loss:  2.1915877790678113 	 ± 0.21816757844085335
	data : 0.11433992385864258
	model : 0.06497182846069335
			 train-loss:  2.190722560995563 	 ± 0.21801082572195662
	data : 0.11431007385253907
	model : 0.06498427391052246
			 train-loss:  2.1901100463462324 	 ± 0.2176779497587114
	data : 0.11430788040161133
	model : 0.06497511863708497
			 train-loss:  2.1903661030558914 	 ± 0.21719836776946558
	data : 0.1142991065979004
	model : 0.06495227813720703
			 train-loss:  2.1914578971461713 	 ± 0.21727536785837528
	data : 0.11444568634033203
	model : 0.06495699882507325
			 train-loss:  2.1910079695457636 	 ± 0.21686938861622648
	data : 0.11444201469421386
	model : 0.06499624252319336
			 train-loss:  2.189811479162287 	 ± 0.21707690141610372
	data : 0.11438136100769043
	model : 0.06498599052429199
			 train-loss:  2.1885127301589686 	 ± 0.21741565048688186
	data : 0.11430826187133789
	model : 0.06494131088256835
			 train-loss:  2.1888271564737374 	 ± 0.2169658629306
	data : 0.11430320739746094
	model : 0.06485066413879395
			 train-loss:  2.1879898962909228 	 ± 0.21682263272887722
	data : 0.11425113677978516
	model : 0.06481070518493652
			 train-loss:  2.1879687455567445 	 ± 0.21632951920409846
	data : 0.11425118446350098
	model : 0.0649035930633545
			 train-loss:  2.1884882638896754 	 ± 0.21597703809237995
	data : 0.11413440704345704
	model : 0.0649829387664795
			 train-loss:  2.187054738805101 	 ± 0.2165412598384019
	data : 0.1141045093536377
	model : 0.06497602462768555
			 train-loss:  2.187612033745633 	 ± 0.21621469854511532
	data : 0.11423983573913574
	model : 0.06494979858398438
			 train-loss:  2.1882750620799407 	 ± 0.21595862601661286
	data : 0.11414608955383301
	model : 0.0648282527923584
			 train-loss:  2.1888828971650867 	 ± 0.21567013563158868
	data : 0.11415591239929199
	model : 0.06452312469482421
			 train-loss:  2.1877141236203963 	 ± 0.21590542394977474
	data : 0.11442031860351562
	model : 0.06425824165344238
			 train-loss:  2.1876597252186176 	 ± 0.21543088862575535
	data : 0.11466083526611329
	model : 0.06418190002441407
			 train-loss:  2.1893699362612606 	 ± 0.21649676127327858
	data : 0.11458330154418946
	model : 0.06408219337463379
			 train-loss:  2.189277398534217 	 ± 0.21602806268606536
	data : 0.11480855941772461
	model : 0.06406192779541016
			 train-loss:  2.188888492273248 	 ± 0.21563824983474383
	data : 0.11475129127502441
	model : 0.06406583786010742
			 train-loss:  2.1886286234958865 	 ± 0.2152070840581986
	data : 0.11467719078063965
	model : 0.06400065422058106
			 train-loss:  2.188351417923796 	 ± 0.21478410102097295
	data : 0.11470465660095215
	model : 0.0639566421508789
			 train-loss:  2.1878416359168775 	 ± 0.2144633053897854
	data : 0.11488475799560546
	model : 0.06397218704223633
			 train-loss:  2.1886350522693405 	 ± 0.214346978849381
	data : 0.1149515151977539
	model : 0.06398501396179199
			 train-loss:  2.186857232134393 	 ± 0.21561240476020663
	data : 0.11507711410522461
	model : 0.06402344703674316
			 train-loss:  2.185741065417306 	 ± 0.2158344102408284
	data : 0.11511759757995606
	model : 0.06404256820678711
			 train-loss:  2.1873797872398475 	 ± 0.21684485278020577
	data : 0.11491098403930664
	model : 0.06392512321472169
			 train-loss:  2.1883215578664252 	 ± 0.21687397933794392
	data : 0.11484770774841309
	model : 0.06384286880493165
			 train-loss:  2.188042818751794 	 ± 0.21646250971304334
	data : 0.11486525535583496
	model : 0.06382875442504883
			 train-loss:  2.188521913190683 	 ± 0.21613801777225783
	data : 0.1148686408996582
	model : 0.06381902694702149
			 train-loss:  2.1901239433723863 	 ± 0.2171123258916297
	data : 0.11481475830078125
	model : 0.06385664939880371
			 train-loss:  2.1896657554571295 	 ± 0.21678000952413617
	data : 0.11497912406921387
	model : 0.06392183303833007
			 train-loss:  2.189482685469796 	 ± 0.2163522449156114
	data : 0.11490974426269532
	model : 0.06398243904113769
			 train-loss:  2.188816970977627 	 ± 0.21615769319133674
	data : 0.11475882530212403
	model : 0.06393189430236816
			 train-loss:  2.1886688023197407 	 ± 0.21572851985862895
	data : 0.11478080749511718
	model : 0.0639307975769043
			 train-loss:  2.188611443934402 	 ± 0.21529147274333574
	data : 0.11486930847167968
	model : 0.06389431953430176
			 train-loss:  2.188741211466461 	 ± 0.21486485814463396
	data : 0.11493043899536133
	model : 0.06394386291503906
			 train-loss:  2.1897025584213194 	 ± 0.21496284518348655
	data : 0.11500263214111328
	model : 0.06396164894104003
			 train-loss:  2.189649639838192 	 ± 0.2145323772530564
	data : 0.1150625228881836
	model : 0.063991117477417
			 train-loss:  2.190357713222504 	 ± 0.2143942278555172
	data : 0.1149672031402588
	model : 0.06391077041625977
			 train-loss:  2.1901561449248477 	 ± 0.21399045621427573
	data : 0.1147303581237793
	model : 0.06384749412536621
			 train-loss:  2.189274941171919 	 ± 0.21402127916304817
	data : 0.1146697998046875
	model : 0.06384639739990235
			 train-loss:  2.1891937613958428 	 ± 0.21360178088316925
	data : 0.11462993621826172
	model : 0.0638512134552002
			 train-loss:  2.1899747144518873 	 ± 0.21354248653696953
	data : 0.11467885971069336
	model : 0.06387720108032227
			 train-loss:  2.1893249119029328 	 ± 0.2133748305100422
	data : 0.11468782424926757
	model : 0.06397643089294433
			 train-loss:  2.2044934798032045 	 ± 0.32252559843482165
	data : 0.11472997665405274
	model : 0.05561504364013672
#epoch  78    val-loss:  2.460169873739544  train-loss:  2.2044934798032045  lr:  1.9073486328125e-08
			 train-loss:  2.2776286602020264 	 ± 0.0
	data : 5.657316446304321
	model : 0.07377910614013672
			 train-loss:  2.4936954975128174 	 ± 0.21606683731079102
	data : 2.9471012353897095
	model : 0.07015120983123779
			 train-loss:  2.474341074625651 	 ± 0.1785285397191415
	data : 2.0018557707468667
	model : 0.06824080149332683
			 train-loss:  2.332653909921646 	 ± 0.2900518704409028
	data : 1.5298830270767212
	model : 0.06739920377731323
			 train-loss:  2.303451180458069 	 ± 0.26592342444678424
	data : 1.2468297004699707
	model : 0.06686620712280274
			 train-loss:  2.236813187599182 	 ± 0.28483767339643173
	data : 0.1381523609161377
	model : 0.0650712013244629
			 train-loss:  2.24590379851205 	 ± 0.26464689353380677
	data : 0.11365370750427246
	model : 0.06470584869384766
			 train-loss:  2.193046987056732 	 ± 0.284323985262444
	data : 0.11423892974853515
	model : 0.06479902267456054
			 train-loss:  2.1997730996873646 	 ± 0.26873744959321344
	data : 0.11422767639160156
	model : 0.06477656364440917
			 train-loss:  2.174387848377228 	 ± 0.2660780599545696
	data : 0.11420369148254395
	model : 0.06481542587280273
			 train-loss:  2.1955812085758555 	 ± 0.26239851314501167
	data : 0.11417021751403808
	model : 0.06483206748962403
			 train-loss:  2.1767086883385978 	 ± 0.2589075580462705
	data : 0.11407785415649414
	model : 0.0650087833404541
			 train-loss:  2.1774982213974 	 ± 0.24876537106032298
	data : 0.11391148567199708
	model : 0.0650062084197998
			 train-loss:  2.1658956578799655 	 ± 0.243339192030999
	data : 0.1139246940612793
	model : 0.06503214836120605
			 train-loss:  2.143004854520162 	 ± 0.2502043348641538
	data : 0.1137685775756836
	model : 0.06503090858459473
			 train-loss:  2.143516071140766 	 ± 0.24226739617055465
	data : 0.11372756958007812
	model : 0.06496992111206054
			 train-loss:  2.138385443126454 	 ± 0.2359281806816176
	data : 0.11363129615783692
	model : 0.06485881805419921
			 train-loss:  2.1398148867819042 	 ± 0.2293567165985677
	data : 0.11385807991027833
	model : 0.06487689018249512
			 train-loss:  2.1382529296373067 	 ± 0.2233377714467644
	data : 0.11388192176818848
	model : 0.06488976478576661
			 train-loss:  2.1529670894145965 	 ± 0.22693478271994258
	data : 0.11402182579040528
	model : 0.06486010551452637
			 train-loss:  2.149622218949454 	 ± 0.221970283157234
	data : 0.11410408020019532
	model : 0.06494226455688476
			 train-loss:  2.1543794707818464 	 ± 0.21795982389291557
	data : 0.11426219940185547
	model : 0.06489033699035644
			 train-loss:  2.160405029421267 	 ± 0.21503429341179503
	data : 0.11403708457946778
	model : 0.0648345947265625
			 train-loss:  2.14740027487278 	 ± 0.21955166805046375
	data : 0.11389951705932617
	model : 0.0648406982421875
			 train-loss:  2.1399611043930054 	 ± 0.21818113512456327
	data : 0.1138913631439209
	model : 0.06485157012939453
			 train-loss:  2.143221859748547 	 ± 0.21456452630616593
	data : 0.11402435302734375
	model : 0.06479873657226562
			 train-loss:  2.1331415132240013 	 ± 0.21673664952556088
	data : 0.11402382850646972
	model : 0.06485519409179688
			 train-loss:  2.1318508854934146 	 ± 0.21293679611885255
	data : 0.11426448822021484
	model : 0.06488947868347168
			 train-loss:  2.138597944687153 	 ± 0.21225738570979458
	data : 0.1144498348236084
	model : 0.06485424041748047
			 train-loss:  2.13674236536026 	 ± 0.20892887927057374
	data : 0.11441078186035156
	model : 0.06482048034667968
			 train-loss:  2.1397402786439463 	 ± 0.20618631239341853
	data : 0.11414780616760253
	model : 0.0648045539855957
			 train-loss:  2.1379127390682697 	 ± 0.20319401440370263
	data : 0.1141749382019043
	model : 0.06476340293884278
			 train-loss:  2.13038749405832 	 ± 0.20456979201901995
	data : 0.11403365135192871
	model : 0.0647575855255127
			 train-loss:  2.13860943738152 	 ± 0.2069994308443717
	data : 0.11406207084655762
	model : 0.06474456787109376
			 train-loss:  2.147992297581264 	 ± 0.21122927871693203
	data : 0.11409850120544433
	model : 0.06479396820068359
			 train-loss:  2.154486338297526 	 ± 0.21178872584576863
	data : 0.11430196762084961
	model : 0.06482648849487305
			 train-loss:  2.1569320253423743 	 ± 0.20942185043172007
	data : 0.11416363716125488
	model : 0.06483821868896485
			 train-loss:  2.1683852735318636 	 ± 0.21807547198556895
	data : 0.11415309906005859
	model : 0.06480646133422852
			 train-loss:  2.1640462080637612 	 ± 0.21691691515924663
	data : 0.11396918296813965
	model : 0.06480803489685058
			 train-loss:  2.1613179087638854 	 ± 0.214864900451114
	data : 0.1139434814453125
	model : 0.06479668617248535
			 train-loss:  2.1673936087910723 	 ± 0.2156790859282164
	data : 0.11378331184387207
	model : 0.06480469703674316
			 train-loss:  2.170055071512858 	 ± 0.2137763506073118
	data : 0.11396346092224122
	model : 0.06478099822998047
			 train-loss:  2.17526644329692 	 ± 0.21395836599248566
	data : 0.11401877403259278
	model : 0.06483263969421386
			 train-loss:  2.1743620850823144 	 ± 0.21159616557220703
	data : 0.11410937309265137
	model : 0.06484007835388184
			 train-loss:  2.1705835978190104 	 ± 0.21072771442534274
	data : 0.11401371955871582
	model : 0.06484551429748535
			 train-loss:  2.1656523372816 	 ± 0.21103340490370653
	data : 0.11406850814819336
	model : 0.06485333442687988
			 train-loss:  2.1667694842561764 	 ± 0.20891374178751093
	data : 0.11400723457336426
	model : 0.06486186981201172
			 train-loss:  2.1655728220939636 	 ± 0.2068888244930468
	data : 0.11393089294433593
	model : 0.06484436988830566
			 train-loss:  2.165288127198511 	 ± 0.20477633124097142
	data : 0.1140367031097412
	model : 0.06484045982360839
			 train-loss:  2.1673226881027223 	 ± 0.20321789140072305
	data : 0.11430225372314454
	model : 0.06480937004089356
			 train-loss:  2.171383198569803 	 ± 0.20325388931780988
	data : 0.11432571411132812
	model : 0.0647953987121582
			 train-loss:  2.1727429811771097 	 ± 0.20152413917443415
	data : 0.11427111625671386
	model : 0.06478462219238282
			 train-loss:  2.166799916411346 	 ± 0.20416257223014622
	data : 0.11438140869140626
	model : 0.06484518051147461
			 train-loss:  2.1630300173053034 	 ± 0.2041168875945914
	data : 0.1142819881439209
	model : 0.06482524871826172
			 train-loss:  2.171692833033475 	 ± 0.21203437984529874
	data : 0.1141352653503418
	model : 0.06482605934143067
			 train-loss:  2.1714475687061037 	 ± 0.21014056006545906
	data : 0.114188814163208
	model : 0.06481657028198243
			 train-loss:  2.178588714515954 	 ± 0.2150351335863021
	data : 0.11414799690246583
	model : 0.06482009887695313
			 train-loss:  2.171203048064791 	 ± 0.22034542280774175
	data : 0.11409716606140137
	model : 0.06482424736022949
			 train-loss:  2.173582038636935 	 ± 0.21922008322393483
	data : 0.11407122611999512
	model : 0.0648813247680664
			 train-loss:  2.1840350687503816 	 ± 0.2317394484081242
	data : 0.11401309967041015
	model : 0.06489834785461426
			 train-loss:  2.1894333147611773 	 ± 0.23360491364711894
	data : 0.11397027969360352
	model : 0.0649043083190918
			 train-loss:  2.1900397827548366 	 ± 0.23176175298335605
	data : 0.11406011581420898
	model : 0.06484804153442383
			 train-loss:  2.193886713376121 	 ± 0.2319018003584385
	data : 0.11407437324523925
	model : 0.06476507186889649
			 train-loss:  2.194973783567548 	 ± 0.23024466392748488
	data : 0.11408390998840331
	model : 0.06471385955810546
			 train-loss:  2.1955584104244528 	 ± 0.2285145535681833
	data : 0.11418581008911133
	model : 0.064719820022583
			 train-loss:  2.1934059406771804 	 ± 0.22743979183663726
	data : 0.1142822265625
	model : 0.06473793983459472
			 train-loss:  2.195971996036928 	 ± 0.22669665334285571
	data : 0.11428675651550294
	model : 0.06480622291564941
			 train-loss:  2.197870712069904 	 ± 0.22555966033257624
	data : 0.11424098014831544
	model : 0.06481475830078125
			 train-loss:  2.1945518618044644 	 ± 0.22558549239879438
	data : 0.11418099403381347
	model : 0.06479754447937011
			 train-loss:  2.195636054447719 	 ± 0.22414936761604745
	data : 0.11395740509033203
	model : 0.06479148864746094
			 train-loss:  2.196113280847039 	 ± 0.22260106497274862
	data : 0.11391472816467285
	model : 0.06476788520812989
			 train-loss:  2.1971377664142184 	 ± 0.2212183131207656
	data : 0.11394758224487304
	model : 0.06478843688964844
			 train-loss:  2.1939541411726444 	 ± 0.22135247590297422
	data : 0.11405448913574219
	model : 0.06482505798339844
			 train-loss:  2.192833774798625 	 ± 0.22006005813457064
	data : 0.11426973342895508
	model : 0.06491227149963379
			 train-loss:  2.1929309876759846 	 ± 0.21858966761749943
	data : 0.1143956184387207
	model : 0.0649496078491211
			 train-loss:  2.1869275601286637 	 ± 0.2232841805149438
	data : 0.1143071174621582
	model : 0.0649754524230957
			 train-loss:  2.185279589194756 	 ± 0.22229428341978927
	data : 0.11405153274536133
	model : 0.0649329662322998
			 train-loss:  2.188656816115746 	 ± 0.22284403060043376
	data : 0.11392774581909179
	model : 0.06491622924804688
			 train-loss:  2.186374631109117 	 ± 0.22234458375528873
	data : 0.1139336109161377
	model : 0.06489706039428711
			 train-loss:  2.1916934311389924 	 ± 0.2259513909408671
	data : 0.11394305229187011
	model : 0.06489148139953613
			 train-loss:  2.1904997649016202 	 ± 0.22480596414499027
	data : 0.11399955749511718
	model : 0.06490917205810547
			 train-loss:  2.188683073695113 	 ± 0.22402843024997612
	data : 0.11423702239990234
	model : 0.06498394012451172
			 train-loss:  2.1873078662228873 	 ± 0.22302271636253007
	data : 0.11422324180603027
	model : 0.06502313613891601
			 train-loss:  2.185523135321481 	 ± 0.2222866987342129
	data : 0.11402459144592285
	model : 0.06501445770263672
			 train-loss:  2.1878182439243092 	 ± 0.2219741834021738
	data : 0.11411867141723633
	model : 0.06495537757873535
			 train-loss:  2.186654251675273 	 ± 0.22094063961572474
	data : 0.11404781341552735
	model : 0.06490287780761719
			 train-loss:  2.185911540327401 	 ± 0.2197751494125951
	data : 0.1140655517578125
	model : 0.06488184928894043
			 train-loss:  2.184842811389403 	 ± 0.21875010805021056
	data : 0.11414852142333984
	model : 0.06482758522033691
			 train-loss:  2.185476244165656 	 ± 0.21759885132608994
	data : 0.1142453670501709
	model : 0.06482930183410644
			 train-loss:  2.1846963829464383 	 ± 0.21651162911456187
	data : 0.1142423152923584
	model : 0.06488404273986817
			 train-loss:  2.1849190481416474 	 ± 0.21532908005402207
	data : 0.11437554359436035
	model : 0.06489663124084473
			 train-loss:  2.182628614747006 	 ± 0.21526732576818672
	data : 0.11429824829101562
	model : 0.06485476493835449
			 train-loss:  2.1834500925515288 	 ± 0.21425178112190948
	data : 0.11436352729797364
	model : 0.06482892036437989
			 train-loss:  2.1849269930352557 	 ± 0.21358450786772312
	data : 0.11425290107727051
	model : 0.06479067802429199
			 train-loss:  2.1855675057361 	 ± 0.21254814287656418
	data : 0.11417531967163086
	model : 0.06478967666625976
			 train-loss:  2.184387200822433 	 ± 0.21175095902950875
	data : 0.11414847373962403
	model : 0.0648183822631836
			 train-loss:  2.1848459108588623 	 ± 0.21070457096597564
	data : 0.11427268981933594
	model : 0.06482868194580078
			 train-loss:  2.1843255770449734 	 ± 0.209689422853283
	data : 0.11417407989501953
	model : 0.06489324569702148
			 train-loss:  2.181671721766693 	 ± 0.2102753552485628
	data : 0.11440072059631348
	model : 0.06496634483337402
			 train-loss:  2.180556846857071 	 ± 0.209515200964051
	data : 0.11433734893798828
	model : 0.06494622230529785
			 train-loss:  2.1782375774761236 	 ± 0.20976153242847118
	data : 0.11417202949523926
	model : 0.06488914489746093
			 train-loss:  2.1779080465728162 	 ± 0.208757027583698
	data : 0.1141047477722168
	model : 0.06490931510925294
			 train-loss:  2.1775152382341405 	 ± 0.20777904873459294
	data : 0.11422042846679688
	model : 0.06497807502746582
			 train-loss:  2.1754678155367193 	 ± 0.20781911899051392
	data : 0.11405358314514161
	model : 0.06498379707336426
			 train-loss:  2.1763537440981184 	 ± 0.2070243726321676
	data : 0.114162015914917
	model : 0.06502251625061035
			 train-loss:  2.1785842618852294 	 ± 0.20730932643205674
	data : 0.11439146995544433
	model : 0.06506791114807128
			 train-loss:  2.1785282409079723 	 ± 0.20633912333712523
	data : 0.11437492370605469
	model : 0.0650477409362793
			 train-loss:  2.1769117127965996 	 ± 0.20606120836161487
	data : 0.11426820755004882
	model : 0.06495413780212403
			 train-loss:  2.177431608558795 	 ± 0.20518494241285695
	data : 0.11426348686218261
	model : 0.06485867500305176
			 train-loss:  2.180285071242939 	 ± 0.20641131429564738
	data : 0.11429276466369628
	model : 0.06477541923522949
			 train-loss:  2.1779055584658376 	 ± 0.2069894322783792
	data : 0.11414070129394531
	model : 0.06480727195739747
			 train-loss:  2.1742134605135237 	 ± 0.20970262270354675
	data : 0.11420331001281739
	model : 0.06484422683715821
			 train-loss:  2.1746807921249256 	 ± 0.208831246859978
	data : 0.11421709060668946
	model : 0.06490135192871094
			 train-loss:  2.1754191110008643 	 ± 0.20806138391904017
	data : 0.11430511474609376
	model : 0.06498675346374512
			 train-loss:  2.1763386062953782 	 ± 0.20738730062916158
	data : 0.11429524421691895
	model : 0.06506543159484864
			 train-loss:  2.1802086665712554 	 ± 0.21062079224873007
	data : 0.11446285247802734
	model : 0.06503586769104004
			 train-loss:  2.179024443667159 	 ± 0.21010625836118316
	data : 0.11435937881469727
	model : 0.06498517990112304
			 train-loss:  2.177407984006203 	 ± 0.20994343733567616
	data : 0.11424307823181153
	model : 0.06486496925354004
			 train-loss:  2.1791874821446524 	 ± 0.20995122923858417
	data : 0.11424717903137208
	model : 0.06478538513183593
			 train-loss:  2.186642120281855 	 ± 0.22433281810220324
	data : 0.11420559883117676
	model : 0.06474227905273437
			 train-loss:  2.1870717903799264 	 ± 0.22345347701775162
	data : 0.11418323516845703
	model : 0.06477913856506348
			 train-loss:  2.1873257726919455 	 ± 0.2225533363205327
	data : 0.11425509452819824
	model : 0.06483693122863769
			 train-loss:  2.1866551492272355 	 ± 0.22177054009396727
	data : 0.1144841194152832
	model : 0.06495132446289062
			 train-loss:  2.186049332541804 	 ± 0.22097666106376024
	data : 0.1145179271697998
	model : 0.06504998207092286
			 train-loss:  2.1853597831726073 	 ± 0.2202248818145812
	data : 0.11456303596496582
	model : 0.06513948440551758
			 train-loss:  2.1850772471654984 	 ± 0.219371976805596
	data : 0.11457996368408203
	model : 0.06507201194763183
			 train-loss:  2.183777811020378 	 ± 0.21899289917567175
	data : 0.1143789291381836
	model : 0.06504273414611816
			 train-loss:  2.184670314192772 	 ± 0.2183675393402654
	data : 0.11430106163024903
	model : 0.06494369506835937
			 train-loss:  2.1810405808825823 	 ± 0.22136199141493249
	data : 0.11414284706115722
	model : 0.06491193771362305
			 train-loss:  2.186572892849262 	 ± 0.22928679602031288
	data : 0.11419410705566406
	model : 0.06485648155212402
			 train-loss:  2.1855291719655043 	 ± 0.22871977277810704
	data : 0.1140406608581543
	model : 0.0649174690246582
			 train-loss:  2.1848708535685684 	 ± 0.2279763124820559
	data : 0.11432504653930664
	model : 0.06499300003051758
			 train-loss:  2.18374494681681 	 ± 0.22748572490156332
	data : 0.11433873176574708
	model : 0.06506280899047852
			 train-loss:  2.184411548856479 	 ± 0.22676565543174573
	data : 0.11452579498291016
	model : 0.06505751609802246
			 train-loss:  2.186916312464961 	 ± 0.22777719054112444
	data : 0.11447262763977051
	model : 0.06506409645080566
			 train-loss:  2.187052944127251 	 ± 0.2269437819090299
	data : 0.11459016799926758
	model : 0.06499757766723632
			 train-loss:  2.1868419090326685 	 ± 0.22612739596562792
	data : 0.1143808364868164
	model : 0.06486320495605469
			 train-loss:  2.187445612921231 	 ± 0.22541738326351127
	data : 0.11433601379394531
	model : 0.06479225158691407
			 train-loss:  2.1904929233111923 	 ± 0.22743992049496312
	data : 0.11418442726135254
	model : 0.06478438377380372
			 train-loss:  2.189762008190155 	 ± 0.22678995590288478
	data : 0.11410751342773437
	model : 0.06481990814208985
			 train-loss:  2.1911090840684606 	 ± 0.22654569533781235
	data : 0.11412539482116699
	model : 0.06487421989440918
			 train-loss:  2.192622641442527 	 ± 0.22646088739887907
	data : 0.1142807960510254
	model : 0.06491050720214844
			 train-loss:  2.193366839335515 	 ± 0.22584185695193132
	data : 0.11458892822265625
	model : 0.06500134468078614
			 train-loss:  2.192726809117529 	 ± 0.22518642156680793
	data : 0.1145395278930664
	model : 0.06499075889587402
			 train-loss:  2.1920392727029734 	 ± 0.2245601866937607
	data : 0.11444830894470215
	model : 0.06491098403930665
			 train-loss:  2.191590371197217 	 ± 0.2238550970117067
	data : 0.11439528465270996
	model : 0.06483378410339355
			 train-loss:  2.1895761919670362 	 ± 0.22441595891305743
	data : 0.11431527137756348
	model : 0.06486520767211915
			 train-loss:  2.1911192206112116 	 ± 0.22443759287878493
	data : 0.11406826972961426
	model : 0.06480460166931153
			 train-loss:  2.190867300001567 	 ± 0.22370417316126623
	data : 0.11412172317504883
	model : 0.06482253074645997
			 train-loss:  2.1898542777697245 	 ± 0.22329988640418702
	data : 0.11428074836730957
	model : 0.06494007110595704
			 train-loss:  2.1895834681214086 	 ± 0.22258396723342214
	data : 0.11425609588623047
	model : 0.06503190994262695
			 train-loss:  2.1894526269875074 	 ± 0.2218564008868183
	data : 0.11428594589233398
	model : 0.06502857208251953
			 train-loss:  2.188533665307986 	 ± 0.2214202432435079
	data : 0.11422109603881836
	model : 0.0650491714477539
			 train-loss:  2.189897999360964 	 ± 0.22134444380046853
	data : 0.11414079666137696
	model : 0.06498246192932129
			 train-loss:  2.1942771273274575 	 ± 0.2272234536722474
	data : 0.11403369903564453
	model : 0.06485629081726074
			 train-loss:  2.1939626664687424 	 ± 0.22652783590881648
	data : 0.11396703720092774
	model : 0.06485967636108399
			 train-loss:  2.1929147144791425 	 ± 0.22618429194529
	data : 0.11395444869995117
	model : 0.06487994194030762
			 train-loss:  2.19304727225364 	 ± 0.22547350045093606
	data : 0.11404695510864257
	model : 0.06492185592651367
			 train-loss:  2.195549581785622 	 ± 0.22695349093264774
	data : 0.11429004669189453
	model : 0.06500263214111328
			 train-loss:  2.196345590800047 	 ± 0.22646569271810463
	data : 0.11445255279541015
	model : 0.06507854461669922
			 train-loss:  2.197500533198718 	 ± 0.22623346723178842
	data : 0.11445164680480957
	model : 0.0650404930114746
			 train-loss:  2.198622108241658 	 ± 0.2259826825867976
	data : 0.11444816589355469
	model : 0.06497454643249512
			 train-loss:  2.198936796627162 	 ± 0.22532401961992926
	data : 0.114223051071167
	model : 0.06489400863647461
			 train-loss:  2.1983220540895694 	 ± 0.22477307254664236
	data : 0.11408190727233887
	model : 0.06483044624328613
			 train-loss:  2.1970498142820416 	 ± 0.22468240691733293
	data : 0.11388354301452637
	model : 0.06479229927062988
			 train-loss:  2.196887789002384 	 ± 0.22401429875721054
	data : 0.113946533203125
	model : 0.06481542587280273
			 train-loss:  2.196568821718593 	 ± 0.22338039631885087
	data : 0.11441454887390137
	model : 0.06489181518554688
			 train-loss:  2.195320235831397 	 ± 0.2232983032315047
	data : 0.11453533172607422
	model : 0.06495103836059571
			 train-loss:  2.1940582101867045 	 ± 0.22323679152377338
	data : 0.1149625301361084
	model : 0.06501379013061523
			 train-loss:  2.1928362621980555 	 ± 0.22314538649714838
	data : 0.11498088836669922
	model : 0.06502361297607422
			 train-loss:  2.1915993613806384 	 ± 0.2230756785221672
	data : 0.11514134407043457
	model : 0.06500940322875977
			 train-loss:  2.191612948511922 	 ± 0.2224263283740698
	data : 0.11463875770568847
	model : 0.06493921279907226
			 train-loss:  2.1954476523261537 	 ± 0.22741316959431832
	data : 0.11448535919189454
	model : 0.06483273506164551
			 train-loss:  2.196297289996312 	 ± 0.22703394672820595
	data : 0.1141049861907959
	model : 0.06483635902404786
			 train-loss:  2.195608278683254 	 ± 0.22656671783581578
	data : 0.11415295600891114
	model : 0.06483187675476074
			 train-loss:  2.1967815546826883 	 ± 0.22645466814811255
	data : 0.11403188705444336
	model : 0.06484103202819824
			 train-loss:  2.1951716781336037 	 ± 0.2268218004370605
	data : 0.11409239768981934
	model : 0.06491661071777344
			 train-loss:  2.195694253685769 	 ± 0.22629058929135706
	data : 0.11423354148864746
	model : 0.06501517295837403
			 train-loss:  2.194999421775008 	 ± 0.22584794140263242
	data : 0.11422243118286132
	model : 0.06501140594482421
			 train-loss:  2.195601831542121 	 ± 0.22536387786256928
	data : 0.11417646408081054
	model : 0.06497325897216796
			 train-loss:  2.195116209061765 	 ± 0.2248348842311291
	data : 0.11405749320983886
	model : 0.06489596366882325
			 train-loss:  2.195671858368339 	 ± 0.2243409391607692
	data : 0.11405649185180664
	model : 0.06485695838928222
			 train-loss:  2.1974933850960654 	 ± 0.22507266506159868
	data : 0.11413135528564453
	model : 0.06484179496765137
			 train-loss:  2.1960483776486437 	 ± 0.22530979441633395
	data : 0.11408567428588867
	model : 0.0648655891418457
			 train-loss:  2.1948754574801472 	 ± 0.22526259613018815
	data : 0.11414561271667481
	model : 0.06496634483337402
			 train-loss:  2.1937739035134673 	 ± 0.22515529534786216
	data : 0.11428651809692383
	model : 0.06506233215332032
			 train-loss:  2.1948758057731994 	 ± 0.22505477186996345
	data : 0.11433954238891601
	model : 0.06505112648010254
			 train-loss:  2.19411327420397 	 ± 0.22469750617085538
	data : 0.11420822143554688
	model : 0.06503634452819824
			 train-loss:  2.1933652402231933 	 ± 0.22433686318804022
	data : 0.11410822868347167
	model : 0.06495122909545899
			 train-loss:  2.192248567154533 	 ± 0.2242717643655303
	data : 0.11415886878967285
	model : 0.06484956741333008
			 train-loss:  2.1929119251161344 	 ± 0.2238707064727826
	data : 0.11409478187561035
	model : 0.06483182907104493
			 train-loss:  2.192908983056744 	 ± 0.22328695244989308
	data : 0.11401100158691406
	model : 0.06484837532043457
			 train-loss:  2.1930826321784695 	 ± 0.22272073534748563
	data : 0.11410865783691407
	model : 0.064888334274292
			 train-loss:  2.1954958027171108 	 ± 0.22466140564490936
	data : 0.11419801712036133
	model : 0.06493182182312011
			 train-loss:  2.1947664205844584 	 ± 0.22431477939428035
	data : 0.11417012214660645
	model : 0.06497292518615723
			 train-loss:  2.195562393689642 	 ± 0.22401773819124102
	data : 0.1140683650970459
	model : 0.06494030952453614
			 train-loss:  2.194552665434513 	 ± 0.22389515001831686
	data : 0.11408348083496093
	model : 0.0649068832397461
			 train-loss:  2.193728842518546 	 ± 0.22362817807220448
	data : 0.11403069496154786
	model : 0.06483731269836426
			 train-loss:  2.1938584372026835 	 ± 0.22307304423206795
	data : 0.11408977508544922
	model : 0.0648348331451416
			 train-loss:  2.195735074877739 	 ± 0.22408392885562164
	data : 0.11404237747192383
	model : 0.06482114791870117
			 train-loss:  2.195501877300775 	 ± 0.2235501385644853
	data : 0.11417341232299805
	model : 0.0648430347442627
			 train-loss:  2.1970836992311007 	 ± 0.2241209481679488
	data : 0.11420207023620606
	model : 0.06486639976501465
			 train-loss:  2.195610678841915 	 ± 0.22454633685116374
	data : 0.11430435180664063
	model : 0.06491680145263672
			 train-loss:  2.194782067163318 	 ± 0.2243062072301034
	data : 0.11425223350524902
	model : 0.06488146781921386
			 train-loss:  2.1947893869586106 	 ± 0.2237584745307715
	data : 0.11421370506286621
	model : 0.0648590087890625
			 train-loss:  2.1947856371842542 	 ± 0.22321471718362337
	data : 0.11421232223510742
	model : 0.06489548683166504
			 train-loss:  2.193908796794173 	 ± 0.22303025200779147
	data : 0.1142542839050293
	model : 0.06492524147033692
			 train-loss:  2.1943035062689047 	 ± 0.22256593717610662
	data : 0.11419739723205566
	model : 0.06494474411010742
			 train-loss:  2.197657285694871 	 ± 0.22724025600070402
	data : 0.11427149772644044
	model : 0.0649561882019043
			 train-loss:  2.197890663714636 	 ± 0.2267236673112206
	data : 0.11443729400634765
	model : 0.06500015258789063
			 train-loss:  2.1963581236617826 	 ± 0.22727345831293394
	data : 0.11451120376586914
	model : 0.064979887008667
			 train-loss:  2.1971051850408876 	 ± 0.22699633656746931
	data : 0.11435580253601074
	model : 0.06489772796630859
			 train-loss:  2.1973172935521657 	 ± 0.22648391183480654
	data : 0.1141502857208252
	model : 0.0648458480834961
			 train-loss:  2.1974238848017755 	 ± 0.22595947927740503
	data : 0.11409306526184082
	model : 0.06482448577880859
			 train-loss:  2.196334161869315 	 ± 0.22599631152656574
	data : 0.11414012908935547
	model : 0.06483321189880371
			 train-loss:  2.1961290499678365 	 ± 0.22549262254199443
	data : 0.11408700942993164
	model : 0.06487002372741699
			 train-loss:  2.1947495717606786 	 ± 0.22588413975082314
	data : 0.11415290832519531
	model : 0.06494030952453614
			 train-loss:  2.1953998585359766 	 ± 0.22556895751093978
	data : 0.11445164680480957
	model : 0.06496086120605468
			 train-loss:  2.194263354284034 	 ± 0.22567808395476885
	data : 0.1144418716430664
	model : 0.0649531364440918
			 train-loss:  2.1943343167955227 	 ± 0.22516704393566214
	data : 0.11419329643249512
	model : 0.06514830589294433
			 train-loss:  2.1928156754549812 	 ± 0.22578344658151317
	data : 0.11380610466003419
	model : 0.06502013206481934
			 train-loss:  2.1916940265947633 	 ± 0.22589062094315027
	data : 0.11393294334411622
	model : 0.06494293212890626
			 train-loss:  2.1917798144934957 	 ± 0.2253871951594603
	data : 0.11392078399658204
	model : 0.06489667892456055
			 train-loss:  2.1902648535157954 	 ± 0.22601861332470002
	data : 0.11405339241027831
	model : 0.06482415199279785
			 train-loss:  2.190673055118985 	 ± 0.22559852952995157
	data : 0.11418113708496094
	model : 0.06440725326538085
			 train-loss:  2.190427245819463 	 ± 0.22512906021097473
	data : 0.11474156379699707
	model : 0.06422290802001954
			 train-loss:  2.189746599890587 	 ± 0.22486556258732296
	data : 0.11461553573608399
	model : 0.06399264335632324
			 train-loss:  2.190143401685514 	 ± 0.2244515282852629
	data : 0.11473422050476074
	model : 0.06381092071533204
			 train-loss:  2.19008918568557 	 ± 0.22396241945297538
	data : 0.11485238075256347
	model : 0.06372895240783691
			 train-loss:  2.188836942030036 	 ± 0.22427701653378976
	data : 0.11495761871337891
	model : 0.06372203826904296
			 train-loss:  2.1877624395089748 	 ± 0.2243835516504841
	data : 0.11494626998901367
	model : 0.06377115249633789
			 train-loss:  2.1884122557681183 	 ± 0.2241171645235369
	data : 0.11510004997253417
	model : 0.06383218765258789
			 train-loss:  2.188076834310278 	 ± 0.2236940593993822
	data : 0.1149831771850586
	model : 0.06384243965148925
			 train-loss:  2.188852212877355 	 ± 0.22352913136875985
	data : 0.11469650268554688
	model : 0.06376118659973144
			 train-loss:  2.18966607996758 	 ± 0.2234002041745045
	data : 0.11466584205627442
	model : 0.06376132965087891
			 train-loss:  2.191431279404689 	 ± 0.22456273389456033
	data : 0.114509916305542
	model : 0.06375336647033691
			 train-loss:  2.1912695159388997 	 ± 0.22410225080408108
	data : 0.11436934471130371
	model : 0.06378612518310547
			 train-loss:  2.1901485459143375 	 ± 0.2242958108384024
	data : 0.11444721221923829
	model : 0.06379690170288085
			 train-loss:  2.189575982393081 	 ± 0.22400030754212175
	data : 0.11449928283691406
	model : 0.06383628845214843
			 train-loss:  2.1892514914274215 	 ± 0.22358943601311618
	data : 0.11448454856872559
	model : 0.06380133628845215
			 train-loss:  2.1886826451883277 	 ± 0.22329903690720937
	data : 0.11458883285522461
	model : 0.06382222175598144
			 train-loss:  2.1883342551790976 	 ± 0.22290282227974012
	data : 0.11475520133972168
	model : 0.06379318237304688
			 train-loss:  2.1898057539276627 	 ± 0.22361843636696163
	data : 0.11469006538391113
	model : 0.06380090713500977
			 train-loss:  2.190978114722205 	 ± 0.22390679348207984
	data : 0.11484179496765137
	model : 0.06385159492492676
			 train-loss:  2.1918472504129216 	 ± 0.22386142900881661
	data : 0.1147799015045166
	model : 0.06386833190917969
			 train-loss:  2.1950285463798336 	 ± 0.2288881429523874
	data : 0.11472415924072266
	model : 0.0637977123260498
			 train-loss:  2.194774423050977 	 ± 0.22845910784118886
	data : 0.11463603973388672
	model : 0.06381897926330567
			 train-loss:  2.193934878514659 	 ± 0.22837950934452533
	data : 0.11466727256774903
	model : 0.06383609771728516
			 train-loss:  2.193934458326623 	 ± 0.22792045468629493
	data : 0.11472187042236329
	model : 0.06381816864013672
			 train-loss:  2.1938861575126647 	 ± 0.22746543394313437
	data : 0.11489529609680176
	model : 0.06381373405456543
			 train-loss:  2.1932691164700633 	 ± 0.22722141405823496
	data : 0.11496739387512207
	model : 0.06382861137390136
			 train-loss:  2.1926885706091683 	 ± 0.22695657550343135
	data : 0.11487321853637696
	model : 0.06375627517700196
			 train-loss:  2.1921070861250986 	 ± 0.22669561187536602
	data : 0.11492557525634765
	model : 0.06370692253112793
			 train-loss:  2.1931203617824346 	 ± 0.22682225670325684
	data : 0.11485891342163086
	model : 0.06369180679321289
			 train-loss:  2.1922434105592616 	 ± 0.22680810168682292
	data : 0.11478219032287598
	model : 0.06374130249023438
			 train-loss:  2.1925183730199933 	 ± 0.22640726384289095
	data : 0.11451468467712403
	model : 0.055363035202026366
#epoch  79    val-loss:  2.3802905961086878  train-loss:  2.1925183730199933  lr:  1.9073486328125e-08
			 train-loss:  2.044898271560669 	 ± 0.0
	data : 5.452097654342651
	model : 0.10308957099914551
			 train-loss:  2.38367760181427 	 ± 0.3387793302536011
	data : 2.783480405807495
	model : 0.09085214138031006
			 train-loss:  2.3364762465159097 	 ± 0.2845526763572193
	data : 1.8909351825714111
	model : 0.08223215738932292
			 train-loss:  2.212095558643341 	 ± 0.3273214563139765
	data : 1.4466912746429443
	model : 0.0778205394744873
			 train-loss:  2.2391430854797365 	 ± 0.2977209153597779
	data : 1.1800172328948975
	model : 0.0751612663269043
			 train-loss:  2.2339497407277427 	 ± 0.27202874912550207
	data : 0.1123380184173584
	model : 0.06738491058349609
			 train-loss:  2.2055308478219167 	 ± 0.2612930615262731
	data : 0.11209111213684082
	model : 0.06460518836975097
			 train-loss:  2.1654162853956223 	 ± 0.26646585678492235
	data : 0.11369328498840332
	model : 0.06457552909851075
			 train-loss:  2.155292789141337 	 ± 0.2528529118674289
	data : 0.11377668380737305
	model : 0.06464934349060059
			 train-loss:  2.1328222870826723 	 ± 0.24916951398750645
	data : 0.11398181915283204
	model : 0.06474242210388184
			 train-loss:  2.174457452513955 	 ± 0.2716177188392192
	data : 0.11403899192810059
	model : 0.06487007141113281
			 train-loss:  2.1756296257177987 	 ± 0.2600832244596986
	data : 0.11417698860168457
	model : 0.06489443778991699
			 train-loss:  2.189585878298833 	 ± 0.25451380702712434
	data : 0.11418719291687011
	model : 0.06483440399169922
			 train-loss:  2.1659205555915833 	 ± 0.2596746946201958
	data : 0.11401996612548829
	model : 0.06481828689575195
			 train-loss:  2.1928568760553997 	 ± 0.2703580322452018
	data : 0.1139948844909668
	model : 0.06479496955871582
			 train-loss:  2.18710208684206 	 ± 0.2627201704911111
	data : 0.11406464576721191
	model : 0.06486668586730956
			 train-loss:  2.202566392281476 	 ± 0.26227484211917657
	data : 0.1139444351196289
	model : 0.06485610008239746
			 train-loss:  2.1812817321883307 	 ± 0.26957031909470386
	data : 0.1139857292175293
	model : 0.06489901542663574
			 train-loss:  2.1830468491504065 	 ± 0.26248733032292626
	data : 0.11402215957641601
	model : 0.06487946510314942
			 train-loss:  2.176708883047104 	 ± 0.2573282827892415
	data : 0.11402330398559571
	model : 0.06484818458557129
			 train-loss:  2.1773654222488403 	 ± 0.25114385375233245
	data : 0.11371731758117676
	model : 0.06480107307434083
			 train-loss:  2.1799250786954705 	 ± 0.24564987014306267
	data : 0.11371917724609375
	model : 0.06483635902404786
			 train-loss:  2.1861485864805137 	 ± 0.24201718482810874
	data : 0.11376495361328125
	model : 0.06487665176391602
			 train-loss:  2.1723320384820304 	 ± 0.24601307989642018
	data : 0.11384315490722656
	model : 0.06492795944213867
			 train-loss:  2.161863212585449 	 ± 0.24643832826203746
	data : 0.11377239227294922
	model : 0.06500325202941895
			 train-loss:  2.146016946205726 	 ± 0.25431007298252734
	data : 0.11409306526184082
	model : 0.06495475769042969
			 train-loss:  2.1488630330121077 	 ± 0.24997779836429904
	data : 0.11409249305725097
	model : 0.0648679256439209
			 train-loss:  2.143273881503514 	 ± 0.24718534716264645
	data : 0.11396136283874511
	model : 0.06476702690124511
			 train-loss:  2.149256500704535 	 ± 0.2449404973941631
	data : 0.11399669647216797
	model : 0.06474442481994629
			 train-loss:  2.1528583129247028 	 ± 0.24160340265620125
	data : 0.1140587329864502
	model : 0.06469531059265136
			 train-loss:  2.177982314940422 	 ± 0.2746373866854298
	data : 0.11407265663146973
	model : 0.06469988822937012
			 train-loss:  2.1760228127241135 	 ± 0.2705321983419732
	data : 0.11417322158813477
	model : 0.06476020812988281
			 train-loss:  2.175102443406076 	 ± 0.2664525636908718
	data : 0.11435432434082031
	model : 0.06487178802490234
			 train-loss:  2.175415172296412 	 ± 0.2625110472687201
	data : 0.11426849365234375
	model : 0.06482672691345215
			 train-loss:  2.172373288018363 	 ± 0.25934096912034144
	data : 0.11408467292785644
	model : 0.06486964225769043
			 train-loss:  2.1769279109107122 	 ± 0.25712940010068824
	data : 0.11396517753601074
	model : 0.0648801326751709
			 train-loss:  2.1865518673046216 	 ± 0.2601210517114259
	data : 0.11389856338500977
	model : 0.06486587524414063
			 train-loss:  2.1823842713707373 	 ± 0.2579244171447044
	data : 0.11370668411254883
	model : 0.06485199928283691
			 train-loss:  2.1937843224941154 	 ± 0.26411694359676285
	data : 0.11378159523010253
	model : 0.06489481925964355
			 train-loss:  2.1908406257629394 	 ± 0.26144170395115535
	data : 0.11387977600097657
	model : 0.06486124992370605
			 train-loss:  2.1966424337247523 	 ± 0.260827696729113
	data : 0.11390399932861328
	model : 0.06486368179321289
			 train-loss:  2.1999876612708684 	 ± 0.2585925598832555
	data : 0.1139150619506836
	model : 0.06484565734863282
			 train-loss:  2.1957643475643422 	 ± 0.25702941669486673
	data : 0.11383399963378907
	model : 0.06475620269775391
			 train-loss:  2.1881063011559574 	 ± 0.2590066172245222
	data : 0.11382932662963867
	model : 0.0647359848022461
			 train-loss:  2.1897091415193346 	 ± 0.25633318743924327
	data : 0.11384768486022949
	model : 0.06472182273864746
			 train-loss:  2.188084154025368 	 ± 0.25376588165508
	data : 0.1139249324798584
	model : 0.06472344398498535
			 train-loss:  2.2059784924730343 	 ± 0.27884854164151635
	data : 0.11394028663635254
	model : 0.06475167274475098
			 train-loss:  2.206746958196163 	 ± 0.27597887104049673
	data : 0.11413922309875488
	model : 0.06482019424438476
			 train-loss:  2.213713998697242 	 ± 0.27738036410580946
	data : 0.11406583786010742
	model : 0.06480002403259277
			 train-loss:  2.2104799008369445 	 ± 0.2755241897241892
	data : 0.11404218673706054
	model : 0.06480765342712402
			 train-loss:  2.205020079425737 	 ± 0.27552778455874527
	data : 0.11397185325622558
	model : 0.06482458114624023
			 train-loss:  2.2088217299718123 	 ± 0.2742129197135614
	data : 0.11405510902404785
	model : 0.06481871604919434
			 train-loss:  2.2126739047608286 	 ± 0.27303046661966374
	data : 0.11404261589050294
	model : 0.06481285095214843
			 train-loss:  2.217306810396689 	 ± 0.2725852951696966
	data : 0.11415505409240723
	model : 0.0648590087890625
			 train-loss:  2.2123340433294123 	 ± 0.2725566334562597
	data : 0.11422085762023926
	model : 0.06490187644958496
			 train-loss:  2.2068980761936734 	 ± 0.27310400653067907
	data : 0.11408638954162598
	model : 0.06489090919494629
			 train-loss:  2.2083512941996255 	 ± 0.27091611024178003
	data : 0.11398005485534668
	model : 0.06490478515625
			 train-loss:  2.2049118485944024 	 ± 0.26982289797320674
	data : 0.11387138366699219
	model : 0.06493372917175293
			 train-loss:  2.206543243537515 	 ± 0.26781483812742096
	data : 0.11382322311401367
	model : 0.06496181488037109
			 train-loss:  2.2037821054458617 	 ± 0.2664191876084989
	data : 0.11391339302062989
	model : 0.06494402885437012
			 train-loss:  2.1994023870249264 	 ± 0.26639539478042135
	data : 0.11415343284606934
	model : 0.06498517990112304
			 train-loss:  2.198872116304213 	 ± 0.2642707658806825
	data : 0.11420750617980957
	model : 0.06498517990112304
			 train-loss:  2.1978535576472207 	 ± 0.2622876363326091
	data : 0.11432619094848633
	model : 0.06493396759033203
			 train-loss:  2.201280552893877 	 ± 0.2616481913414324
	data : 0.1141934871673584
	model : 0.06482067108154296
			 train-loss:  2.201146463247446 	 ± 0.25962992784114813
	data : 0.11414446830749511
	model : 0.06478071212768555
			 train-loss:  2.2038735837647407 	 ± 0.2585919329021607
	data : 0.11401429176330566
	model : 0.06470975875854493
			 train-loss:  2.204197075829577 	 ± 0.2566683422979352
	data : 0.1139826774597168
	model : 0.06471920013427734
			 train-loss:  2.1988636798718395 	 ± 0.258487247214436
	data : 0.11394944190979003
	model : 0.0647963523864746
			 train-loss:  2.2035483190978784 	 ± 0.2594988083902055
	data : 0.11408400535583496
	model : 0.06486115455627442
			 train-loss:  2.2192814571516855 	 ± 0.28888980511228485
	data : 0.11404628753662109
	model : 0.06485962867736816
			 train-loss:  2.2177064234102275 	 ± 0.28715068428212126
	data : 0.11404185295104981
	model : 0.0648411750793457
			 train-loss:  2.216066653529803 	 ± 0.285484164191947
	data : 0.11378178596496583
	model : 0.064754056930542
			 train-loss:  2.216624186463552 	 ± 0.28356151677229496
	data : 0.1137967586517334
	model : 0.06476526260375977
			 train-loss:  2.217061927189698 	 ± 0.2816638758200215
	data : 0.11372871398925781
	model : 0.06476817131042481
			 train-loss:  2.2142532300949096 	 ± 0.2808211437246086
	data : 0.11369490623474121
	model : 0.06480622291564941
			 train-loss:  2.2106640762404393 	 ± 0.28069383376730017
	data : 0.11381120681762695
	model : 0.06486091613769532
			 train-loss:  2.2095507228529296 	 ± 0.2790340488528242
	data : 0.11407408714294434
	model : 0.06495904922485352
			 train-loss:  2.2061844972463756 	 ± 0.2788087495898504
	data : 0.11414928436279297
	model : 0.06489381790161133
			 train-loss:  2.2078515879715543 	 ± 0.2774294812455394
	data : 0.11418132781982422
	model : 0.06484522819519042
			 train-loss:  2.203743430972099 	 ± 0.2780976581374044
	data : 0.1142049789428711
	model : 0.06483216285705566
			 train-loss:  2.200618518723382 	 ± 0.27778538418571536
	data : 0.11411938667297364
	model : 0.06486797332763672
			 train-loss:  2.1991751092236216 	 ± 0.276391831301571
	data : 0.11414532661437989
	model : 0.0648420810699463
			 train-loss:  2.1961686639900666 	 ± 0.27606743374581716
	data : 0.11404142379760743
	model : 0.06487140655517579
			 train-loss:  2.1968430479367576 	 ± 0.2744880243648026
	data : 0.11405444145202637
	model : 0.06494159698486328
			 train-loss:  2.198613234127269 	 ± 0.27335050449126735
	data : 0.1141143798828125
	model : 0.06491680145263672
			 train-loss:  2.1978615023369015 	 ± 0.2718449721077654
	data : 0.11408843994140624
	model : 0.0648463249206543
			 train-loss:  2.2001808347373175 	 ± 0.271132600748463
	data : 0.113917875289917
	model : 0.06478328704833984
			 train-loss:  2.20034766739065 	 ± 0.2695921641169302
	data : 0.11378064155578613
	model : 0.06477236747741699
			 train-loss:  2.2000396492775907 	 ± 0.2680888947430136
	data : 0.11373872756958008
	model : 0.06475992202758789
			 train-loss:  2.1992228507995604 	 ± 0.2667066904912266
	data : 0.11381282806396484
	model : 0.06477761268615723
			 train-loss:  2.200980225762168 	 ± 0.26576067502586087
	data : 0.11387982368469238
	model : 0.06481084823608399
			 train-loss:  2.2011008340379465 	 ± 0.26431488125377833
	data : 0.11407604217529296
	model : 0.0648958683013916
			 train-loss:  2.1998175087795464 	 ± 0.26317801067041746
	data : 0.1142153263092041
	model : 0.0648529052734375
			 train-loss:  2.2004202274566 	 ± 0.2618389056792007
	data : 0.11413064002990722
	model : 0.06478734016418457
			 train-loss:  2.1987470250380667 	 ± 0.26096186577880215
	data : 0.1139826774597168
	model : 0.064780855178833
			 train-loss:  2.197822074095408 	 ± 0.25975562502828314
	data : 0.11392555236816407
	model : 0.06477117538452148
			 train-loss:  2.2017578783723497 	 ± 0.2612747218207044
	data : 0.11388945579528809
	model : 0.06474609375
			 train-loss:  2.2005880900791714 	 ± 0.2601934649148794
	data : 0.11391205787658691
	model : 0.0648256778717041
			 train-loss:  2.2030133141411676 	 ± 0.2599869267785416
	data : 0.11406140327453614
	model : 0.06487545967102051
			 train-loss:  2.203467094898224 	 ± 0.25872312586115154
	data : 0.11408286094665528
	model : 0.0649073600769043
			 train-loss:  2.2033541957930765 	 ± 0.25744160776433295
	data : 0.11415686607360839
	model : 0.06490559577941894
			 train-loss:  2.2026468258278045 	 ± 0.25627515003006907
	data : 0.11414732933044433
	model : 0.06488537788391113
			 train-loss:  2.2034924516400087 	 ± 0.25517102240386325
	data : 0.11416454315185547
	model : 0.06483092308044433
			 train-loss:  2.2021238941412706 	 ± 0.25432083114216236
	data : 0.11415629386901856
	model : 0.06482043266296386
			 train-loss:  2.1994812624795097 	 ± 0.2545375769587734
	data : 0.11410837173461914
	model : 0.0648120403289795
			 train-loss:  2.207353378241917 	 ± 0.2658665628217256
	data : 0.11406660079956055
	model : 0.06483988761901856
			 train-loss:  2.2055513546845624 	 ± 0.2652708693022861
	data : 0.1141143798828125
	model : 0.06488847732543945
			 train-loss:  2.20319124283614 	 ± 0.26516612974284814
	data : 0.11412487030029297
	model : 0.06491389274597167
			 train-loss:  2.2063003439422046 	 ± 0.265917252757328
	data : 0.1141268253326416
	model : 0.06492714881896973
			 train-loss:  2.2039045940745963 	 ± 0.26588487618430906
	data : 0.11407008171081542
	model : 0.06487956047058105
			 train-loss:  2.2045943157092944 	 ± 0.264783319602969
	data : 0.11409683227539062
	model : 0.06485977172851562
			 train-loss:  2.2071556470223834 	 ± 0.2649762796240077
	data : 0.11395525932312012
	model : 0.06482272148132324
			 train-loss:  2.2075592711963483 	 ± 0.26383579369247334
	data : 0.1139246940612793
	model : 0.06484317779541016
			 train-loss:  2.209114196007712 	 ± 0.26319560845943873
	data : 0.11401715278625488
	model : 0.06486220359802246
			 train-loss:  2.2132877951082977 	 ± 0.265810687663022
	data : 0.11416764259338379
	model : 0.06491856575012207
			 train-loss:  2.2140931117123572 	 ± 0.2648033337800025
	data : 0.11406583786010742
	model : 0.06493449211120605
			 train-loss:  2.2112691993387337 	 ± 0.2654176374381934
	data : 0.11410441398620605
	model : 0.06495800018310546
			 train-loss:  2.210419634641227 	 ± 0.26445030493046606
	data : 0.11401405334472656
	model : 0.0649177074432373
			 train-loss:  2.211821489975232 	 ± 0.26377675564476516
	data : 0.11391873359680176
	model : 0.06488142013549805
			 train-loss:  2.2117194016774495 	 ± 0.2626777472470192
	data : 0.11388859748840333
	model : 0.06487884521484374
			 train-loss:  2.2104972472860793 	 ± 0.26193242190025323
	data : 0.11397728919982911
	model : 0.06488542556762696
			 train-loss:  2.2107335387683307 	 ± 0.26086966859772504
	data : 0.1141080379486084
	model : 0.06488227844238281
			 train-loss:  2.21073217895942 	 ± 0.2598070590404314
	data : 0.11421184539794922
	model : 0.06491546630859375
			 train-loss:  2.2098610170425905 	 ± 0.25893764304065575
	data : 0.1141517162322998
	model : 0.06496715545654297
			 train-loss:  2.2128253536224367 	 ± 0.26000372591186144
	data : 0.11409511566162109
	model : 0.0649137020111084
			 train-loss:  2.2111665199673367 	 ± 0.2596331648914766
	data : 0.11397628784179688
	model : 0.06486139297485352
			 train-loss:  2.21066271038506 	 ± 0.2586707939165549
	data : 0.11387076377868652
	model : 0.06486797332763672
			 train-loss:  2.209204228594899 	 ± 0.2581820892352729
	data : 0.1139439582824707
	model : 0.06484193801879883
			 train-loss:  2.207550659660221 	 ± 0.25785897645089656
	data : 0.11408514976501465
	model : 0.06486248970031738
			 train-loss:  2.209845248552469 	 ± 0.2581840107266383
	data : 0.11420197486877441
	model : 0.06492295265197753
			 train-loss:  2.211129233127332 	 ± 0.25761299719522757
	data : 0.11430864334106446
	model : 0.0649881362915039
			 train-loss:  2.2101373356400114 	 ± 0.25688631927970135
	data : 0.1142798900604248
	model : 0.0649627685546875
			 train-loss:  2.2096268781145714 	 ± 0.2559859489888573
	data : 0.11403431892395019
	model : 0.06492586135864258
			 train-loss:  2.2085794824272837 	 ± 0.2553148862803673
	data : 0.11391353607177734
	model : 0.06487727165222168
			 train-loss:  2.208336255285475 	 ± 0.2543830999695636
	data : 0.11380438804626465
	model : 0.06482763290405273
			 train-loss:  2.2091131850200543 	 ± 0.2536068528286375
	data : 0.11407451629638672
	model : 0.06477856636047363
			 train-loss:  2.2088523302635137 	 ± 0.25269789653723473
	data : 0.11418318748474121
	model : 0.06478872299194335
			 train-loss:  2.2054841190144634 	 ± 0.2548484739484469
	data : 0.11441020965576172
	model : 0.06488699913024902
			 train-loss:  2.204280038531736 	 ± 0.25432374734931096
	data : 0.11443881988525391
	model : 0.06492958068847657
			 train-loss:  2.204309160368783 	 ± 0.2534140530606774
	data : 0.11442084312438965
	model : 0.06495957374572754
			 train-loss:  2.202933313153314 	 ± 0.2530380295916931
	data : 0.11399688720703124
	model : 0.0650033950805664
			 train-loss:  2.2020377676251908 	 ± 0.2523696169168062
	data : 0.11399335861206054
	model : 0.06506681442260742
			 train-loss:  2.201774355414864 	 ± 0.25150524602173924
	data : 0.11397209167480468
	model : 0.06506190299987794
			 train-loss:  2.2008333851893744 	 ± 0.250882909911257
	data : 0.11400446891784669
	model : 0.06507363319396972
			 train-loss:  2.201820657993185 	 ± 0.2502968397775217
	data : 0.11410388946533204
	model : 0.06508908271789551
			 train-loss:  2.2018151021983527 	 ± 0.24943819496706512
	data : 0.11425418853759765
	model : 0.0650942325592041
			 train-loss:  2.2014255426367937 	 ± 0.2486328784894651
	data : 0.11452875137329102
	model : 0.06501460075378418
			 train-loss:  2.2018481750746033 	 ± 0.2478444549833236
	data : 0.11446280479431152
	model : 0.06497335433959961
			 train-loss:  2.2032156918672907 	 ± 0.24757097586336158
	data : 0.11447606086730958
	model : 0.06487479209899902
			 train-loss:  2.2047159020105997 	 ± 0.24742296331715707
	data : 0.11427197456359864
	model : 0.06482582092285157
			 train-loss:  2.203468362227181 	 ± 0.247075207318929
	data : 0.1143453598022461
	model : 0.06481866836547852
			 train-loss:  2.2033874141542533 	 ± 0.24626312767416256
	data : 0.11416134834289551
	model : 0.06489377021789551
			 train-loss:  2.204537039488749 	 ± 0.24586590062430042
	data : 0.11426887512207032
	model : 0.06491293907165527
			 train-loss:  2.205765310820047 	 ± 0.24553682387033568
	data : 0.11431345939636231
	model : 0.06499767303466797
			 train-loss:  2.2033882733314267 	 ± 0.24651475048913402
	data : 0.11480593681335449
	model : 0.06504478454589843
			 train-loss:  2.2015884694380636 	 ± 0.2467429118898674
	data : 0.11481142044067383
	model : 0.06510491371154785
			 train-loss:  2.20036148341598 	 ± 0.24643282632410918
	data : 0.11466574668884277
	model : 0.06502585411071778
			 train-loss:  2.2027258110951773 	 ± 0.24743163375301827
	data : 0.11449136734008789
	model : 0.06490235328674317
			 train-loss:  2.2026434237102293 	 ± 0.24665449359432942
	data : 0.11435537338256836
	model : 0.06490216255187989
			 train-loss:  2.200489155203104 	 ± 0.24737845043684936
	data : 0.1140639305114746
	model : 0.06489043235778809
			 train-loss:  2.199676936457616 	 ± 0.24682291149942737
	data : 0.114082670211792
	model : 0.06483421325683594
			 train-loss:  2.2015694504902688 	 ± 0.2472289016280706
	data : 0.11418671607971191
	model : 0.0648965835571289
			 train-loss:  2.20077593268061 	 ± 0.24667621294344055
	data : 0.11429519653320312
	model : 0.06499695777893066
			 train-loss:  2.200409513421175 	 ± 0.245967492742645
	data : 0.11438713073730469
	model : 0.06500415802001953
			 train-loss:  2.2009374856948853 	 ± 0.2453141996469884
	data : 0.11427841186523438
	model : 0.06495003700256348
			 train-loss:  2.1983884788421264 	 ± 0.2467561732576242
	data : 0.11429419517517089
	model : 0.06493048667907715
			 train-loss:  2.198819160461426 	 ± 0.24607884382143957
	data : 0.11426982879638672
	model : 0.06486830711364747
			 train-loss:  2.1980083400294896 	 ± 0.24556901866091027
	data : 0.11419539451599121
	model : 0.06486544609069825
			 train-loss:  2.197714651830098 	 ± 0.24487099489613054
	data : 0.11425118446350098
	model : 0.0648575782775879
			 train-loss:  2.1988741608227 	 ± 0.24461459803747349
	data : 0.11438188552856446
	model : 0.0648916244506836
			 train-loss:  2.1993738252517074 	 ± 0.24398529597676452
	data : 0.1143369197845459
	model : 0.06490864753723144
			 train-loss:  2.1983509174613065 	 ± 0.24364246558137298
	data : 0.11429486274719239
	model : 0.06496524810791016
			 train-loss:  2.1972504364961836 	 ± 0.24336561412969945
	data : 0.11434354782104492
	model : 0.06494221687316895
			 train-loss:  2.196421438250048 	 ± 0.2429101280213871
	data : 0.11424479484558106
	model : 0.06490159034729004
			 train-loss:  2.1972820486341202 	 ± 0.2424809891928956
	data : 0.11414356231689453
	model : 0.06486020088195801
			 train-loss:  2.1957690769975837 	 ± 0.24261810657999436
	data : 0.11400923728942872
	model : 0.06480288505554199
			 train-loss:  2.1974198144707975 	 ± 0.24292091646434316
	data : 0.1141097068786621
	model : 0.06478185653686523
			 train-loss:  2.1967653946930104 	 ± 0.24239400681299428
	data : 0.11420011520385742
	model : 0.06481709480285644
			 train-loss:  2.1979285804919027 	 ± 0.2422136440048636
	data : 0.11424579620361328
	model : 0.06485323905944824
			 train-loss:  2.197167830997043 	 ± 0.2417542419299094
	data : 0.11435141563415527
	model : 0.06491169929504395
			 train-loss:  2.1975320676413688 	 ± 0.24113500909562846
	data : 0.11443357467651367
	model : 0.06495919227600097
			 train-loss:  2.199282114322369 	 ± 0.24162150289040624
	data : 0.1142756462097168
	model : 0.0649259090423584
			 train-loss:  2.198214887921276 	 ± 0.24139018623281783
	data : 0.1141477108001709
	model : 0.06488094329833985
			 train-loss:  2.1976726547531458 	 ± 0.2408450674834186
	data : 0.11400947570800782
	model : 0.06488375663757324
			 train-loss:  2.196724712526476 	 ± 0.24053719066619322
	data : 0.11403675079345703
	model : 0.06491451263427735
			 train-loss:  2.195550234727962 	 ± 0.24042101295980073
	data : 0.1140963077545166
	model : 0.06490731239318848
			 train-loss:  2.1945898717737453 	 ± 0.2401347698733086
	data : 0.11416893005371094
	model : 0.06496882438659668
			 train-loss:  2.193013667426211 	 ± 0.24046323440523823
	data : 0.11425561904907226
	model : 0.06506505012512206
			 train-loss:  2.191947373763594 	 ± 0.24027147196670623
	data : 0.11435556411743164
	model : 0.06505589485168457
			 train-loss:  2.191643232420871 	 ± 0.2396748192953666
	data : 0.11429057121276856
	model : 0.06497669219970703
			 train-loss:  2.191191467939247 	 ± 0.23912766944870506
	data : 0.1140017032623291
	model : 0.06493496894836426
			 train-loss:  2.1903370153158903 	 ± 0.23879628601080286
	data : 0.11400375366210938
	model : 0.06490502357482911
			 train-loss:  2.191081105736253 	 ± 0.23839989838175513
	data : 0.11396646499633789
	model : 0.06485037803649903
			 train-loss:  2.1903047911899605 	 ± 0.23802912501147958
	data : 0.11403751373291016
	model : 0.06486539840698242
			 train-loss:  2.1902466205450204 	 ± 0.23741939193132808
	data : 0.1140528678894043
	model : 0.06495356559753418
			 train-loss:  2.191553239311491 	 ± 0.23751482111745187
	data : 0.11431183815002441
	model : 0.06499748229980469
			 train-loss:  2.1915679008222475 	 ± 0.23691131358200512
	data : 0.11434855461120605
	model : 0.06503815650939941
			 train-loss:  2.192283068642472 	 ± 0.2365253888324446
	data : 0.11444277763366699
	model : 0.0650263786315918
			 train-loss:  2.191432300524496 	 ± 0.236233880498113
	data : 0.11431741714477539
	model : 0.06496009826660157
			 train-loss:  2.1898945170640944 	 ± 0.23663897582597523
	data : 0.11423716545104981
	model : 0.06491918563842773
			 train-loss:  2.190053131449875 	 ± 0.23606024558454985
	data : 0.11412634849548339
	model : 0.06491069793701172
			 train-loss:  2.189248246131557 	 ± 0.23575154697018688
	data : 0.11416316032409668
	model : 0.06491317749023437
			 train-loss:  2.18874103623658 	 ± 0.23528062318408827
	data : 0.11406559944152832
	model : 0.06495876312255859
			 train-loss:  2.1895256749555174 	 ± 0.23496934374372214
	data : 0.11420831680297852
	model : 0.06500062942504883
			 train-loss:  2.188394183647342 	 ± 0.23495201242458286
	data : 0.11431612968444824
	model : 0.06504268646240234
			 train-loss:  2.1893840738870565 	 ± 0.2348091800673399
	data : 0.11450285911560058
	model : 0.06505646705627441
			 train-loss:  2.187741212222887 	 ± 0.23542512514010153
	data : 0.11439728736877441
	model : 0.06510434150695801
			 train-loss:  2.1859831592211356 	 ± 0.23621665600720146
	data : 0.11430411338806153
	model : 0.06504325866699219
			 train-loss:  2.1868567808963464 	 ± 0.23598745667336618
	data : 0.1142000675201416
	model : 0.06501121520996093
			 train-loss:  2.1859168699809484 	 ± 0.2358167217352332
	data : 0.11417012214660645
	model : 0.06497468948364257
			 train-loss:  2.183974105599932 	 ± 0.23693582256652126
	data : 0.11424503326416016
	model : 0.06501359939575195
			 train-loss:  2.1839718683710636 	 ± 0.23637635336426588
	data : 0.11434803009033204
	model : 0.06495723724365235
			 train-loss:  2.1833809104883617 	 ± 0.23597775155777548
	data : 0.11449575424194336
	model : 0.06500024795532226
			 train-loss:  2.1839490872677243 	 ± 0.23557174769195682
	data : 0.11453766822814941
	model : 0.06505303382873535
			 train-loss:  2.1834107321362164 	 ± 0.2351551812634078
	data : 0.11453719139099121
	model : 0.06508054733276367
			 train-loss:  2.183236393663618 	 ± 0.23462413536006782
	data : 0.11438150405883789
	model : 0.0650136947631836
			 train-loss:  2.183642545603387 	 ± 0.23415899826737177
	data : 0.11421041488647461
	model : 0.06492929458618164
			 train-loss:  2.1838066829453915 	 ± 0.23363383084969697
	data : 0.11414484977722168
	model : 0.06479721069335938
			 train-loss:  2.1836394719337218 	 ± 0.2331128837759816
	data : 0.11400136947631836
	model : 0.06474833488464356
			 train-loss:  2.1838887854055926 	 ± 0.23261174016511246
	data : 0.11411991119384765
	model : 0.06467275619506836
			 train-loss:  2.1847559251396906 	 ± 0.23244098814649108
	data : 0.11408057212829589
	model : 0.06502976417541503
			 train-loss:  2.1847340674013704 	 ± 0.231917109166699
	data : 0.11424212455749512
	model : 0.06506786346435547
			 train-loss:  2.186143456018559 	 ± 0.23234743439517525
	data : 0.11433091163635253
	model : 0.0650862216949463
			 train-loss:  2.186068005859852 	 ± 0.23183095955741742
	data : 0.11448421478271484
	model : 0.06499857902526855
			 train-loss:  2.186844375398424 	 ± 0.2316068661059694
	data : 0.11437792778015136
	model : 0.06484432220458984
			 train-loss:  2.1880507743464106 	 ± 0.2318013206143616
	data : 0.11443266868591309
	model : 0.06424241065979004
			 train-loss:  2.188511298091401 	 ± 0.2313937739065827
	data : 0.11446075439453125
	model : 0.06399106979370117
			 train-loss:  2.188708170464164 	 ± 0.23090482621776529
	data : 0.11449851989746093
	model : 0.06386332511901856
			 train-loss:  2.189303327335541 	 ± 0.2305753099136895
	data : 0.11461186408996582
	model : 0.06375494003295898
			 train-loss:  2.190996141019075 	 ± 0.23149524410491926
	data : 0.11473946571350098
	model : 0.0637519359588623
			 train-loss:  2.19014129648993 	 ± 0.23135715053159156
	data : 0.11484751701354981
	model : 0.06380319595336914
			 train-loss:  2.191225290812295 	 ± 0.23144513412980455
	data : 0.11478924751281738
	model : 0.0637927532196045
			 train-loss:  2.190868394569266 	 ± 0.23101190543231104
	data : 0.11461272239685058
	model : 0.06375012397766114
			 train-loss:  2.195586997729081 	 ± 0.24150824781034644
	data : 0.11473116874694825
	model : 0.0637166976928711
			 train-loss:  2.2015444608444863 	 ± 0.25764901871640744
	data : 0.11465644836425781
	model : 0.06368160247802734
			 train-loss:  2.200492459333549 	 ± 0.2576078590860759
	data : 0.11467509269714356
	model : 0.06368908882141114
			 train-loss:  2.200190673397563 	 ± 0.25710561073420435
	data : 0.11477899551391602
	model : 0.06375260353088379
			 train-loss:  2.199787897723062 	 ± 0.2566398219831754
	data : 0.11502437591552735
	model : 0.06377077102661133
			 train-loss:  2.2010463815353902 	 ± 0.25683721818189936
	data : 0.11477994918823242
	model : 0.06373963356018067
			 train-loss:  2.1999863564968107 	 ± 0.2568249483158382
	data : 0.1147737979888916
	model : 0.06372556686401368
			 train-loss:  2.200046654064131 	 ± 0.2562932649281965
	data : 0.11474494934082032
	model : 0.06376142501831054
			 train-loss:  2.1995432642865773 	 ± 0.255882544576798
	data : 0.11480035781860351
	model : 0.0638094425201416
			 train-loss:  2.1988465413144587 	 ± 0.25558540824729603
	data : 0.11484203338623047
	model : 0.06383776664733887
			 train-loss:  2.1989774195874325 	 ± 0.25506928937114376
	data : 0.11494388580322265
	model : 0.06393871307373047
			 train-loss:  2.197817311968122 	 ± 0.2551924318752156
	data : 0.11496443748474121
	model : 0.06398653984069824
			 train-loss:  2.1972720380721054 	 ± 0.254816194953823
	data : 0.11496162414550781
	model : 0.06395535469055176
			 train-loss:  2.1967925484846478 	 ± 0.25441102846812425
	data : 0.11490387916564941
	model : 0.06391029357910157
			 train-loss:  2.1990075976617875 	 ± 0.2562730477517989
	data : 0.11489801406860352
	model : 0.06389579772949219
			 train-loss:  2.20001644686044 	 ± 0.2562509026185807
	data : 0.11481318473815919
	model : 0.0638519287109375
			 train-loss:  2.2002037115097046 	 ± 0.255754958772324
	data : 0.11489033699035645
	model : 0.06384215354919434
			 train-loss:  2.199719751973551 	 ± 0.2553596545052255
	data : 0.11483492851257324
	model : 0.06380267143249511
			 train-loss:  2.1999104486571417 	 ± 0.2548703920049143
	data : 0.11475133895874023
	model : 0.0637690544128418
			 train-loss:  2.1996082345487573 	 ± 0.2544114347228826
	data : 0.11466870307922364
	model : 0.06378321647644043
			 train-loss:  2.198697993135828 	 ± 0.25432258070726094
	data : 0.11488690376281738
	model : 0.06384458541870117
			 train-loss:  2.198727029912612 	 ± 0.25382384098569266
	data : 0.11484303474426269
	model : 0.06387004852294922
			 train-loss:  2.2005198169499636 	 ± 0.25494012598880134
	data : 0.11461563110351562
	model : 0.05548138618469238
#epoch  80    val-loss:  2.411193508850901  train-loss:  2.2005198169499636  lr:  1.9073486328125e-08
			 train-loss:  2.633228302001953 	 ± 0.0
	data : 5.7064409255981445
	model : 0.0755014419555664
			 train-loss:  2.3366464376449585 	 ± 0.29658186435699463
	data : 2.913021445274353
	model : 0.07015430927276611
			 train-loss:  2.3540800412495932 	 ± 0.24340993388301602
	data : 1.9800455570220947
	model : 0.06827553113301595
			 train-loss:  2.2936697602272034 	 ± 0.23533912359468193
	data : 1.5136075019836426
	model : 0.06739407777786255
			 train-loss:  2.3357559204101563 	 ± 0.22669932066777584
	data : 1.2337515830993653
	model : 0.06683788299560547
			 train-loss:  2.300541361172994 	 ± 0.22142149301885145
	data : 0.11528592109680176
	model : 0.06471624374389648
			 train-loss:  2.2697856426239014 	 ± 0.21840109327629473
	data : 0.11394438743591309
	model : 0.06464838981628418
			 train-loss:  2.2761003375053406 	 ± 0.2049775242590402
	data : 0.11381683349609376
	model : 0.0646397590637207
			 train-loss:  2.272056314680311 	 ± 0.1935928661147745
	data : 0.1137624740600586
	model : 0.06462855339050293
			 train-loss:  2.2411351203918457 	 ± 0.20575582697825376
	data : 0.11379237174987793
	model : 0.06468672752380371
			 train-loss:  2.2330398559570312 	 ± 0.19784366349691915
	data : 0.11385469436645508
	model : 0.06473774909973144
			 train-loss:  2.259914497534434 	 ± 0.20934417167138386
	data : 0.11405487060546875
	model : 0.06480727195739747
			 train-loss:  2.24026527771583 	 ± 0.21233683573188578
	data : 0.11405291557312011
	model : 0.06483941078186035
			 train-loss:  2.2280578102384294 	 ± 0.20929339714428447
	data : 0.11417598724365234
	model : 0.06488347053527832
			 train-loss:  2.2132059415181478 	 ± 0.20969399072242925
	data : 0.1139857292175293
	model : 0.06487598419189453
			 train-loss:  2.199794940650463 	 ± 0.2095737836660998
	data : 0.11396799087524415
	model : 0.06483330726623535
			 train-loss:  2.211356716997483 	 ± 0.20850987332627946
	data : 0.11397728919982911
	model : 0.06484041213989258
			 train-loss:  2.214081519179874 	 ± 0.20294637428377563
	data : 0.11416068077087402
	model : 0.06490416526794433
			 train-loss:  2.2233897573069523 	 ± 0.20144245131095206
	data : 0.11413288116455078
	model : 0.06491069793701172
			 train-loss:  2.229725021123886 	 ± 0.19827426438050377
	data : 0.11427369117736816
	model : 0.06488304138183594
			 train-loss:  2.211350685074216 	 ± 0.21022125452375934
	data : 0.1140556812286377
	model : 0.06483831405639648
			 train-loss:  2.208287374539809 	 ± 0.20586710646402717
	data : 0.11403017044067383
	model : 0.0648068904876709
			 train-loss:  2.1912083522133203 	 ± 0.2166929807014361
	data : 0.11387839317321777
	model : 0.06480751037597657
			 train-loss:  2.1828722755114236 	 ± 0.21586483790400987
	data : 0.11378540992736816
	model : 0.06485915184020996
			 train-loss:  2.192426223754883 	 ± 0.2166203899971564
	data : 0.11373848915100097
	model : 0.06489815711975097
			 train-loss:  2.1957553808505716 	 ± 0.21306499284635577
	data : 0.1139195442199707
	model : 0.06493306159973145
			 train-loss:  2.1876611797897905 	 ± 0.21311675345986186
	data : 0.11401810646057128
	model : 0.06498456001281738
			 train-loss:  2.1973370569092885 	 ± 0.21523118388426668
	data : 0.11405763626098633
	model : 0.06491875648498535
			 train-loss:  2.196305168086085 	 ± 0.21155822249267364
	data : 0.11397819519042969
	model : 0.06484055519104004
			 train-loss:  2.196891673405965 	 ± 0.20802634717827165
	data : 0.11400818824768066
	model : 0.06482887268066406
			 train-loss:  2.177595857651003 	 ± 0.23032335921930613
	data : 0.11404767036437988
	model : 0.06487250328063965
			 train-loss:  2.187058012932539 	 ± 0.23273714165444215
	data : 0.11402597427368164
	model : 0.06486396789550782
			 train-loss:  2.2046407677910547 	 ± 0.24983608679954522
	data : 0.11409072875976563
	model : 0.06489949226379395
			 train-loss:  2.2121989411466263 	 ± 0.24993479581446318
	data : 0.11426620483398438
	model : 0.06488232612609864
			 train-loss:  2.211297883306231 	 ± 0.24639444802566704
	data : 0.11417555809020996
	model : 0.06480584144592286
			 train-loss:  2.2046672701835632 	 ± 0.24609470731743247
	data : 0.11388416290283203
	model : 0.06470513343811035
			 train-loss:  2.1994364583814465 	 ± 0.24476680314993823
	data : 0.11383614540100098
	model : 0.0647470474243164
			 train-loss:  2.2052751716814543 	 ± 0.24412197656706816
	data : 0.11391716003417969
	model : 0.06476225852966308
			 train-loss:  2.1984715920228224 	 ± 0.24459439720192772
	data : 0.11382722854614258
	model : 0.0648383617401123
			 train-loss:  2.197933170199394 	 ± 0.24154102071562888
	data : 0.11394848823547363
	model : 0.0649172306060791
			 train-loss:  2.196294328061546 	 ± 0.23880226054821735
	data : 0.1141739845275879
	model : 0.06500201225280762
			 train-loss:  2.1968667138190496 	 ± 0.23597071430454453
	data : 0.1141942024230957
	model : 0.06494841575622559
			 train-loss:  2.2024976902229843 	 ± 0.23604867133300167
	data : 0.11396684646606445
	model : 0.06491541862487793
			 train-loss:  2.206611094149676 	 ± 0.23490465904684169
	data : 0.11390266418457032
	model : 0.06483831405639648
			 train-loss:  2.200294484032525 	 ± 0.23602871221390387
	data : 0.11378870010375977
	model : 0.06483354568481445
			 train-loss:  2.1983234156732974 	 ± 0.23382323606072286
	data : 0.11384758949279786
	model : 0.06479177474975586
			 train-loss:  2.1994536227368293 	 ± 0.2314493526159345
	data : 0.11388769149780273
	model : 0.06478924751281738
			 train-loss:  2.198395465811094 	 ± 0.2291405939953072
	data : 0.11399250030517578
	model : 0.06481747627258301
			 train-loss:  2.1960744176592146 	 ± 0.22735976226451962
	data : 0.11410093307495117
	model : 0.06483831405639648
			 train-loss:  2.199970660209656 	 ± 0.22672112133989825
	data : 0.11419787406921386
	model : 0.06476964950561523
			 train-loss:  2.198004820767571 	 ± 0.22491732167650644
	data : 0.11415510177612305
	model : 0.06477627754211426
			 train-loss:  2.1964539610422573 	 ± 0.22301933259726356
	data : 0.11426258087158203
	model : 0.06481966972351075
			 train-loss:  2.196649025071342 	 ± 0.2209098359043849
	data : 0.11439909934997558
	model : 0.06480393409729004
			 train-loss:  2.1938055312192 	 ± 0.21983166225962886
	data : 0.11439857482910157
	model : 0.06486563682556153
			 train-loss:  2.2004128152673896 	 ± 0.2231697579269476
	data : 0.11434760093688964
	model : 0.06493310928344727
			 train-loss:  2.201546175139291 	 ± 0.2213278523678218
	data : 0.11434893608093262
	model : 0.06492795944213867
			 train-loss:  2.201882190871657 	 ± 0.21939219943549557
	data : 0.11422863006591796
	model : 0.0649071216583252
			 train-loss:  2.206050724818789 	 ± 0.21975788822319736
	data : 0.11415915489196778
	model : 0.06489410400390624
			 train-loss:  2.2037302154605674 	 ± 0.21860309085789703
	data : 0.1141085147857666
	model : 0.06484756469726563
			 train-loss:  2.204960489273071 	 ± 0.21697962344266145
	data : 0.11410374641418457
	model : 0.06488804817199707
			 train-loss:  2.199416432224336 	 ± 0.2194368821566078
	data : 0.11411399841308593
	model : 0.06493644714355469
			 train-loss:  2.196046704246152 	 ± 0.21924541126423241
	data : 0.11415157318115235
	model : 0.06495399475097656
			 train-loss:  2.192824333433121 	 ± 0.21897338939759847
	data : 0.11406130790710449
	model : 0.06499323844909669
			 train-loss:  2.1911089718341827 	 ± 0.21768213538476255
	data : 0.11401901245117188
	model : 0.06496338844299317
			 train-loss:  2.191240959901076 	 ± 0.2160037479353635
	data : 0.11412200927734376
	model : 0.06492233276367188
			 train-loss:  2.193465247298732 	 ± 0.21510990222869952
	data : 0.11406569480895996
	model : 0.06490187644958496
			 train-loss:  2.1915628340706896 	 ± 0.2140572464616034
	data : 0.11398439407348633
	model : 0.06490283012390137
			 train-loss:  2.1895030246061435 	 ± 0.21314535513906047
	data : 0.11412124633789063
	model : 0.06494593620300293
			 train-loss:  2.19135974801105 	 ± 0.21214840947928518
	data : 0.1143043041229248
	model : 0.06499147415161133
			 train-loss:  2.1879368901252745 	 ± 0.21253797830269425
	data : 0.11431288719177246
	model : 0.06501526832580566
			 train-loss:  2.185355933619217 	 ± 0.21213782052872068
	data : 0.11447339057922364
	model : 0.06500568389892578
			 train-loss:  2.185231273372968 	 ± 0.21066210896938112
	data : 0.11447138786315918
	model : 0.06495652198791504
			 train-loss:  2.188200984915642 	 ± 0.21072631683194817
	data : 0.11431713104248047
	model : 0.06487712860107422
			 train-loss:  2.2026695029155627 	 ± 0.24307854728316644
	data : 0.11417226791381836
	model : 0.06485357284545898
			 train-loss:  2.202331337928772 	 ± 0.24147010869246505
	data : 0.11414027214050293
	model : 0.06486940383911133
			 train-loss:  2.203002802635494 	 ± 0.23994670280070976
	data : 0.11404061317443848
	model : 0.06487851142883301
			 train-loss:  2.2030734659789446 	 ± 0.23838431142524222
	data : 0.11409063339233398
	model : 0.06492009162902831
			 train-loss:  2.208011216078049 	 ± 0.24078183814818288
	data : 0.1143214225769043
	model : 0.06495404243469238
			 train-loss:  2.2053015066098562 	 ± 0.24044695567362948
	data : 0.11441559791564941
	model : 0.06493487358093261
			 train-loss:  2.2032450422644616 	 ± 0.23963753642587782
	data : 0.11420135498046875
	model : 0.0648500919342041
			 train-loss:  2.199948658177882 	 ± 0.23997182315257531
	data : 0.1142486572265625
	model : 0.06480422019958496
			 train-loss:  2.199701015542193 	 ± 0.23851450540295838
	data : 0.11428594589233398
	model : 0.06479072570800781
			 train-loss:  2.1979100819093635 	 ± 0.2376273720727143
	data : 0.11421751976013184
	model : 0.06486315727233886
			 train-loss:  2.195603234427316 	 ± 0.23714179891040138
	data : 0.11410675048828126
	model : 0.06492652893066406
			 train-loss:  2.1942237517412972 	 ± 0.23608151079691966
	data : 0.11447725296020508
	model : 0.06499080657958985
			 train-loss:  2.1917626746865206 	 ± 0.2357991537670393
	data : 0.1144317626953125
	model : 0.06507282257080078
			 train-loss:  2.193071965513558 	 ± 0.23475427801525753
	data : 0.11441831588745117
	model : 0.06506714820861817
			 train-loss:  2.1948560286651957 	 ± 0.23400905238166003
	data : 0.11421432495117187
	model : 0.06492962837219238
			 train-loss:  2.1984543827142606 	 ± 0.23512632860483945
	data : 0.1143111228942871
	model : 0.06483774185180664
			 train-loss:  2.1964360316594442 	 ± 0.23459045663874253
	data : 0.11408023834228516
	model : 0.06481890678405762
			 train-loss:  2.196427290256207 	 ± 0.23329795213848495
	data : 0.11418366432189941
	model : 0.06478362083435059
			 train-loss:  2.197528766549152 	 ± 0.23226435852449706
	data : 0.1142080307006836
	model : 0.0647810935974121
			 train-loss:  2.200255552927653 	 ± 0.23248809288838052
	data : 0.11441102027893066
	model : 0.06485414505004883
			 train-loss:  2.2017349182291235 	 ± 0.2316878034421761
	data : 0.11439371109008789
	model : 0.0649200439453125
			 train-loss:  2.202006136743646 	 ± 0.2304801688690871
	data : 0.11443901062011719
	model : 0.06491403579711914
			 train-loss:  2.1994540939728418 	 ± 0.23062196552645223
	data : 0.11439719200134277
	model : 0.06490693092346192
			 train-loss:  2.2000720943372274 	 ± 0.22951000281139555
	data : 0.11441993713378906
	model : 0.06487340927124023
			 train-loss:  2.1973434492033355 	 ± 0.2299120634564926
	data : 0.11427874565124511
	model : 0.06487236022949219
			 train-loss:  2.1968117651313244 	 ± 0.22880849071603118
	data : 0.11420040130615235
	model : 0.06487884521484374
			 train-loss:  2.1975045347213746 	 ± 0.2277659000164326
	data : 0.1141995906829834
	model : 0.06492180824279785
			 train-loss:  2.1982640016196977 	 ± 0.22676275602931362
	data : 0.11416888236999512
	model : 0.06496000289916992
			 train-loss:  2.197788584466074 	 ± 0.22569901380470633
	data : 0.11414928436279297
	model : 0.06502733230590821
			 train-loss:  2.1951478164172866 	 ± 0.22617867757788163
	data : 0.1141657829284668
	model : 0.06503734588623047
			 train-loss:  2.1967498786174335 	 ± 0.2256751250763744
	data : 0.11415591239929199
	model : 0.06497030258178711
			 train-loss:  2.195285412243434 	 ± 0.22509390553747924
	data : 0.11400332450866699
	model : 0.06495394706726074
			 train-loss:  2.1938704960751085 	 ± 0.22449828940745692
	data : 0.11401710510253907
	model : 0.0648946762084961
			 train-loss:  2.1938548834524423 	 ± 0.2234468272073508
	data : 0.11398229598999024
	model : 0.06486129760742188
			 train-loss:  2.193130024053432 	 ± 0.2225362978677221
	data : 0.11412544250488281
	model : 0.0648829460144043
			 train-loss:  2.1912202441364252 	 ± 0.2224004808808123
	data : 0.11429657936096191
	model : 0.06493802070617676
			 train-loss:  2.1940942092375324 	 ± 0.2234113341053456
	data : 0.11448554992675782
	model : 0.06494112014770508
			 train-loss:  2.1945350621197677 	 ± 0.22245075761750943
	data : 0.11455025672912597
	model : 0.06497368812561036
			 train-loss:  2.192735626229218 	 ± 0.22226544878003773
	data : 0.11457195281982421
	model : 0.06498332023620605
			 train-loss:  2.1954600526168284 	 ± 0.2231503179618326
	data : 0.1143989086151123
	model : 0.064892578125
			 train-loss:  2.1955554391208447 	 ± 0.22217174654774605
	data : 0.11425318717956542
	model : 0.06482596397399902
			 train-loss:  2.200352418941 	 ± 0.22705576594715807
	data : 0.11413440704345704
	model : 0.06480698585510254
			 train-loss:  2.199212023924137 	 ± 0.22640548721439333
	data : 0.11406736373901367
	model : 0.06478681564331054
			 train-loss:  2.198588989738725 	 ± 0.22553571254789193
	data : 0.11404128074645996
	model : 0.06482815742492676
			 train-loss:  2.200181460986703 	 ± 0.22523764044698194
	data : 0.11419734954833985
	model : 0.06488866806030273
			 train-loss:  2.1991790412854746 	 ± 0.22455343936694125
	data : 0.11428008079528809
	model : 0.06499333381652832
			 train-loss:  2.1997699707746508 	 ± 0.22370873830445992
	data : 0.1144221305847168
	model : 0.06502575874328613
			 train-loss:  2.198766433503017 	 ± 0.2230534690243519
	data : 0.11440529823303222
	model : 0.065024995803833
			 train-loss:  2.1995394747765338 	 ± 0.22230013164101975
	data : 0.11430139541625976
	model : 0.06492147445678711
			 train-loss:  2.1976644401627827 	 ± 0.2223612006451482
	data : 0.11419830322265626
	model : 0.06490793228149414
			 train-loss:  2.1977755859974892 	 ± 0.22146619837740558
	data : 0.11406507492065429
	model : 0.06487135887145996
			 train-loss:  2.1967370595932008 	 ± 0.2208815008256884
	data : 0.11410059928894042
	model : 0.06489734649658203
			 train-loss:  2.196325130878933 	 ± 0.22005144093420798
	data : 0.11419763565063476
	model : 0.06488213539123536
			 train-loss:  2.1941256091350647 	 ± 0.22056956031429523
	data : 0.11423826217651367
	model : 0.06492395401000976
			 train-loss:  2.1938579212874174 	 ± 0.21972698047994502
	data : 0.11435117721557617
	model : 0.06495399475097656
			 train-loss:  2.1918079354042233 	 ± 0.22009905814304234
	data : 0.11435437202453613
	model : 0.06496224403381348
			 train-loss:  2.19124383192796 	 ± 0.21934448202854365
	data : 0.11423535346984863
	model : 0.06487889289855957
			 train-loss:  2.1909607803548568 	 ± 0.2185295173933708
	data : 0.11399755477905274
	model : 0.06490149497985839
			 train-loss:  2.1929961876435713 	 ± 0.21894311175246245
	data : 0.1140749454498291
	model : 0.0649113655090332
			 train-loss:  2.19205057172847 	 ± 0.21838886760042217
	data : 0.1139683723449707
	model : 0.06492586135864258
			 train-loss:  2.1917657514116655 	 ± 0.21759725109406894
	data : 0.11419391632080078
	model : 0.06493682861328125
			 train-loss:  2.190421309294524 	 ± 0.21734774413272556
	data : 0.1143035888671875
	model : 0.0649838924407959
			 train-loss:  2.193012544337441 	 ± 0.2186301580338395
	data : 0.11444101333618165
	model : 0.06498289108276367
			 train-loss:  2.192138346442341 	 ± 0.21806921220683073
	data : 0.11443471908569336
	model : 0.06495919227600097
			 train-loss:  2.191487001336139 	 ± 0.21741137989314305
	data : 0.1143925666809082
	model : 0.06486930847167968
			 train-loss:  2.191276406212676 	 ± 0.21664203884579908
	data : 0.11422238349914551
	model : 0.06481986045837403
			 train-loss:  2.1940417562212264 	 ± 0.21831510962982992
	data : 0.11417188644409179
	model : 0.06479978561401367
			 train-loss:  2.1926975317880615 	 ± 0.21812022801031364
	data : 0.11407656669616699
	model : 0.06485786437988281
			 train-loss:  2.192320210832945 	 ± 0.2173970167318166
	data : 0.11414422988891601
	model : 0.06489996910095215
			 train-loss:  2.1910247252537656 	 ± 0.217184896604804
	data : 0.11429328918457031
	model : 0.06498508453369141
			 train-loss:  2.1941119763586254 	 ± 0.21955560786630346
	data : 0.11488156318664551
	model : 0.0650205135345459
			 train-loss:  2.192891711202161 	 ± 0.2192866657047347
	data : 0.11490907669067382
	model : 0.06504106521606445
			 train-loss:  2.1917380306818712 	 ± 0.21897550802235408
	data : 0.11498017311096191
	model : 0.06500749588012696
			 train-loss:  2.1918998040309567 	 ± 0.218238176517187
	data : 0.11470699310302734
	model : 0.06494131088256835
			 train-loss:  2.1900841569578327 	 ± 0.21861081392336285
	data : 0.11498208045959472
	model : 0.06489453315734864
			 train-loss:  2.1890057437371886 	 ± 0.21827062423288196
	data : 0.11439690589904786
	model : 0.06487336158752441
			 train-loss:  2.1885828312238056 	 ± 0.21760308138388335
	data : 0.11439170837402343
	model : 0.06486549377441406
			 train-loss:  2.187656504428939 	 ± 0.2171778758918116
	data : 0.11434369087219239
	model : 0.06487231254577637
			 train-loss:  2.187613831538903 	 ± 0.21646293123977906
	data : 0.11455802917480469
	model : 0.06497125625610352
			 train-loss:  2.1888182591768652 	 ± 0.21626476621474972
	data : 0.11424808502197266
	model : 0.06501870155334473
			 train-loss:  2.1955406363908345 	 ± 0.23104300307129416
	data : 0.1143226146697998
	model : 0.06505188941955567
			 train-loss:  2.194676221570661 	 ± 0.23054619355428888
	data : 0.11423892974853515
	model : 0.06501832008361816
			 train-loss:  2.1938948333263397 	 ± 0.23001189163035304
	data : 0.11434516906738282
	model : 0.0649838924407959
			 train-loss:  2.1923695028207866 	 ± 0.23006835407151632
	data : 0.11413331031799316
	model : 0.06489686965942383
			 train-loss:  2.1915150224408015 	 ± 0.22958891490771832
	data : 0.1143733024597168
	model : 0.0648353099822998
			 train-loss:  2.191086405478184 	 ± 0.22892920360023114
	data : 0.11460113525390625
	model : 0.06481232643127441
			 train-loss:  2.192033491283655 	 ± 0.22852493322731723
	data : 0.11475005149841308
	model : 0.06483154296875
			 train-loss:  2.1934788219676995 	 ± 0.228546519478744
	data : 0.11472229957580567
	model : 0.06486778259277344
			 train-loss:  2.1940520081991033 	 ± 0.2279560874763016
	data : 0.11512627601623535
	model : 0.06492781639099121
			 train-loss:  2.193446916305214 	 ± 0.2273862231358532
	data : 0.1150177001953125
	model : 0.0649721622467041
			 train-loss:  2.192331794558502 	 ± 0.2271385328282292
	data : 0.11477818489074706
	model : 0.06499338150024414
			 train-loss:  2.1926117759762387 	 ± 0.22647757223980955
	data : 0.11473269462585449
	model : 0.06501641273498535
			 train-loss:  2.1929426487669885 	 ± 0.2258343771481272
	data : 0.11463804244995117
	model : 0.06497673988342285
			 train-loss:  2.1942935525300262 	 ± 0.22582893946759197
	data : 0.11472601890563965
	model : 0.06490354537963867
			 train-loss:  2.1967723617951074 	 ± 0.22742312300836845
	data : 0.11438746452331543
	model : 0.06483068466186523
			 train-loss:  2.1970347024985317 	 ± 0.22677476937421778
	data : 0.11439013481140137
	model : 0.06481866836547852
			 train-loss:  2.197831234511207 	 ± 0.226343786635856
	data : 0.11437950134277344
	model : 0.06478943824768066
			 train-loss:  2.1991604036755033 	 ± 0.22634541676359865
	data : 0.1144587516784668
	model : 0.06482496261596679
			 train-loss:  2.199777221263841 	 ± 0.22583056727996056
	data : 0.11419191360473632
	model : 0.06488919258117676
			 train-loss:  2.199952611344398 	 ± 0.22518868010306275
	data : 0.11438522338867188
	model : 0.06498932838439941
			 train-loss:  2.199605986304667 	 ± 0.22458693408824595
	data : 0.11438460350036621
	model : 0.06494321823120117
			 train-loss:  2.200515423502241 	 ± 0.22426541790969537
	data : 0.1142538070678711
	model : 0.06493310928344727
			 train-loss:  2.1987625963308592 	 ± 0.2248263358524927
	data : 0.11424164772033692
	model : 0.06492514610290527
			 train-loss:  2.1978807819765165 	 ± 0.22449535114586586
	data : 0.11417803764343262
	model : 0.06499280929565429
			 train-loss:  2.198942182439097 	 ± 0.22430878270234084
	data : 0.11424574851989747
	model : 0.06496047973632812
			 train-loss:  2.195968526035714 	 ± 0.22717247272603477
	data : 0.11432018280029296
	model : 0.06506023406982422
			 train-loss:  2.195172784725825 	 ± 0.22679058296148763
	data : 0.11453328132629395
	model : 0.06511640548706055
			 train-loss:  2.1960695731705724 	 ± 0.22648303305261402
	data : 0.11459770202636718
	model : 0.06511902809143066
			 train-loss:  2.195657941666278 	 ± 0.22592785292034487
	data : 0.11467013359069825
	model : 0.06506433486938476
			 train-loss:  2.1951809118354255 	 ± 0.22540160720055702
	data : 0.11455106735229492
	model : 0.06504559516906738
			 train-loss:  2.193538496027822 	 ± 0.22588362827638883
	data : 0.11433868408203125
	model : 0.0649324893951416
			 train-loss:  2.1932441982063087 	 ± 0.22530767352726858
	data : 0.11411833763122559
	model : 0.0648841381072998
			 train-loss:  2.192752226706474 	 ± 0.2248008053867196
	data : 0.11426920890808105
	model : 0.0648618221282959
			 train-loss:  2.1924193198668127 	 ± 0.22424489542865816
	data : 0.11424312591552735
	model : 0.06487393379211426
			 train-loss:  2.1924624645963626 	 ± 0.22364848245324812
	data : 0.1143880844116211
	model : 0.06488399505615235
			 train-loss:  2.191915176532887 	 ± 0.2231822245371427
	data : 0.1144803524017334
	model : 0.0649454116821289
			 train-loss:  2.1915663167050012 	 ± 0.22264578987359443
	data : 0.11466293334960938
	model : 0.0649960994720459
			 train-loss:  2.1933726190896556 	 ± 0.2234536446813633
	data : 0.1144641399383545
	model : 0.06498651504516602
			 train-loss:  2.192824679116408 	 ± 0.22299958915062476
	data : 0.11442008018493652
	model : 0.06488966941833496
			 train-loss:  2.193681797215358 	 ± 0.22273797963966632
	data : 0.11415319442749024
	model : 0.06483316421508789
			 train-loss:  2.1937065345724833 	 ± 0.22216343679606304
	data : 0.11415791511535645
	model : 0.0647998332977295
			 train-loss:  2.1937593105511786 	 ± 0.22159427399295914
	data : 0.1141664981842041
	model : 0.06478543281555176
			 train-loss:  2.1955614187279524 	 ± 0.22245622568804943
	data : 0.11419272422790527
	model : 0.06487398147583008
			 train-loss:  2.1941952723537 	 ± 0.22271366418968863
	data : 0.11423311233520508
	model : 0.06501588821411133
			 train-loss:  2.194068699774116 	 ± 0.22215764735302107
	data : 0.11442790031433106
	model : 0.06510334014892578
			 train-loss:  2.1941092691229818 	 ± 0.22159949460220052
	data : 0.11448211669921875
	model : 0.06514997482299804
			 train-loss:  2.195288276076317 	 ± 0.22166963199707249
	data : 0.1143681526184082
	model : 0.06510324478149414
			 train-loss:  2.1964673551160896 	 ± 0.22174536392312455
	data : 0.11417479515075683
	model : 0.06495976448059082
			 train-loss:  2.1963175493891876 	 ± 0.22120600441771238
	data : 0.11413087844848632
	model : 0.0648423671722412
			 train-loss:  2.195154539470015 	 ± 0.22127872735355028
	data : 0.11404080390930176
	model : 0.06484904289245605
			 train-loss:  2.19543009353619 	 ± 0.2207706230938648
	data : 0.11398968696594239
	model : 0.06483736038208007
			 train-loss:  2.1943608731758304 	 ± 0.2207603517564202
	data : 0.11395378112792968
	model : 0.06490859985351563
			 train-loss:  2.193660309592497 	 ± 0.22045218621293575
	data : 0.11418709754943848
	model : 0.06498641967773437
			 train-loss:  2.1931526252037084 	 ± 0.2200397304414517
	data : 0.11428775787353515
	model : 0.06505718231201171
			 train-loss:  2.1932338665311155 	 ± 0.2195132634808875
	data : 0.11438422203063965
	model : 0.06505560874938965
			 train-loss:  2.1916901283857353 	 ± 0.22011635101364513
	data : 0.11432890892028809
	model : 0.06502141952514648
			 train-loss:  2.1929660394078208 	 ± 0.22036499103357507
	data : 0.11438517570495606
	model : 0.06493978500366211
			 train-loss:  2.1922408138971194 	 ± 0.22009323834581973
	data : 0.11432251930236817
	model : 0.06496157646179199
			 train-loss:  2.192245790980897 	 ± 0.2195735488417502
	data : 0.11428489685058593
	model : 0.0649336814880371
			 train-loss:  2.1926707141276256 	 ± 0.21914486540749545
	data : 0.11436505317687988
	model : 0.06491565704345703
			 train-loss:  2.1913586598690427 	 ± 0.21946921244709422
	data : 0.11441922187805176
	model : 0.06495881080627441
			 train-loss:  2.1918956457182417 	 ± 0.2190990908225296
	data : 0.11445040702819824
	model : 0.06500120162963867
			 train-loss:  2.1922633802449263 	 ± 0.21865782199505485
	data : 0.11445789337158203
	model : 0.06497650146484375
			 train-loss:  2.19093862272078 	 ± 0.21902052650341935
	data : 0.11434540748596192
	model : 0.06499185562133789
			 train-loss:  2.19128649880033 	 ± 0.21857768883122972
	data : 0.11424832344055176
	model : 0.06489439010620117
			 train-loss:  2.192300041516622 	 ± 0.21859092952224685
	data : 0.11422080993652343
	model : 0.06485633850097657
			 train-loss:  2.1919165031476453 	 ± 0.2181674101543871
	data : 0.11439523696899415
	model : 0.06482863426208496
			 train-loss:  2.1931077166380386 	 ± 0.21838916052536025
	data : 0.11444730758666992
	model : 0.06479158401489257
			 train-loss:  2.192144240344967 	 ± 0.21836698403109953
	data : 0.11462235450744629
	model : 0.06503834724426269
			 train-loss:  2.1913472740104916 	 ± 0.21820016951643678
	data : 0.11439247131347656
	model : 0.06506481170654296
			 train-loss:  2.192007783268179 	 ± 0.21793589018585802
	data : 0.11500835418701172
	model : 0.06498980522155762
			 train-loss:  2.191315858629015 	 ± 0.2176974984694086
	data : 0.11483559608459473
	model : 0.06486406326293945
			 train-loss:  2.1930687438070247 	 ± 0.21880090787462833
	data : 0.11491212844848633
	model : 0.06472554206848144
			 train-loss:  2.1932434142948773 	 ± 0.21833422666928273
	data : 0.11470389366149902
	model : 0.06422061920166015
			 train-loss:  2.191936762186519 	 ± 0.2187425940068709
	data : 0.1149634838104248
	model : 0.06409225463867188
			 train-loss:  2.1923974669135813 	 ± 0.2183752974358423
	data : 0.11449394226074219
	model : 0.06400885581970214
			 train-loss:  2.191713993963988 	 ± 0.2181453789905479
	data : 0.11469182968139649
	model : 0.06400847434997559
			 train-loss:  2.1912766292497707 	 ± 0.2177737277592784
	data : 0.11477518081665039
	model : 0.0639765739440918
			 train-loss:  2.190616772606455 	 ± 0.2175351846991427
	data : 0.11502275466918946
	model : 0.06402740478515626
			 train-loss:  2.1901196374402025 	 ± 0.21719990109432072
	data : 0.11503958702087402
	model : 0.06400532722473144
			 train-loss:  2.1911384076134772 	 ± 0.21729247583916309
	data : 0.11503052711486816
	model : 0.06392970085144042
			 train-loss:  2.190696953205352 	 ± 0.21693479022216122
	data : 0.11489171981811523
	model : 0.06387014389038086
			 train-loss:  2.1910220561391216 	 ± 0.21653205547908433
	data : 0.11470026969909668
	model : 0.0638615608215332
			 train-loss:  2.1899321577217004 	 ± 0.21672249217292855
	data : 0.11466383934020996
	model : 0.06390724182128907
			 train-loss:  2.195594805128434 	 ± 0.23317554078920272
	data : 0.11481175422668458
	model : 0.06387100219726563
			 train-loss:  2.1956558751261883 	 ± 0.2326891218548835
	data : 0.11477313041687012
	model : 0.06393003463745117
			 train-loss:  2.195086541275183 	 ± 0.23237060095691023
	data : 0.11472363471984863
	model : 0.06393485069274903
			 train-loss:  2.194558219296309 	 ± 0.23203240244549145
	data : 0.11477031707763671
	model : 0.06393613815307617
			 train-loss:  2.193324512686611 	 ± 0.23234321625960647
	data : 0.11475520133972168
	model : 0.06389484405517579
			 train-loss:  2.1944838855492237 	 ± 0.2325650436983817
	data : 0.11476068496704102
	model : 0.06393771171569824
			 train-loss:  2.1951066099229406 	 ± 0.23229090701220342
	data : 0.11476850509643555
	model : 0.06397881507873535
			 train-loss:  2.1957905613646216 	 ± 0.2320624162874181
	data : 0.11487393379211426
	model : 0.06398253440856934
			 train-loss:  2.1953040564932476 	 ± 0.23171542649031218
	data : 0.11487312316894531
	model : 0.06397266387939453
			 train-loss:  2.194812183920671 	 ± 0.23137454289611964
	data : 0.1146738052368164
	model : 0.06391611099243164
			 train-loss:  2.193191735013839 	 ± 0.2323077733867683
	data : 0.11458449363708496
	model : 0.06391253471374511
			 train-loss:  2.193975002411379 	 ± 0.23216872521900264
	data : 0.11461958885192872
	model : 0.06385374069213867
			 train-loss:  2.19386492061615 	 ± 0.2317104336976349
	data : 0.11463766098022461
	model : 0.06387872695922851
			 train-loss:  2.194457160524163 	 ± 0.23143791570163755
	data : 0.11466712951660156
	model : 0.06393265724182129
			 train-loss:  2.193145940228114 	 ± 0.23191054133594294
	data : 0.11483712196350097
	model : 0.06397371292114258
			 train-loss:  2.1932777962665786 	 ± 0.23146123090126056
	data : 0.1147608757019043
	model : 0.06396713256835937
			 train-loss:  2.1939198323122158 	 ± 0.23123076820712135
	data : 0.11473417282104492
	model : 0.06392927169799804
			 train-loss:  2.1930135273465923 	 ± 0.23122850984045085
	data : 0.11463971138000488
	model : 0.06396570205688476
			 train-loss:  2.1952810450457036 	 ± 0.2335998439115185
	data : 0.11445822715759277
	model : 0.055539274215698244
#epoch  81    val-loss:  2.368320621942219  train-loss:  2.1952810450457036  lr:  1.9073486328125e-08
			 train-loss:  2.434303045272827 	 ± 0.0
	data : 5.13337516784668
	model : 0.08817219734191895
			 train-loss:  2.254062533378601 	 ± 0.18024051189422607
	data : 2.885599136352539
	model : 0.07698345184326172
			 train-loss:  2.205512205759684 	 ± 0.16239467368852778
	data : 1.9611591498057048
	model : 0.07292469342549641
			 train-loss:  2.269958794116974 	 ± 0.17955252931798088
	data : 1.4993996620178223
	model : 0.07078641653060913
			 train-loss:  2.26137375831604 	 ± 0.16151191902271722
	data : 1.2223305225372314
	model : 0.0695108413696289
			 train-loss:  2.248189926147461 	 ± 0.1503578525005604
	data : 0.2184892177581787
	model : 0.06479735374450683
			 train-loss:  2.1990682567868913 	 ± 0.1839985697474632
	data : 0.11373786926269532
	model : 0.06456155776977539
			 train-loss:  2.17005755007267 	 ± 0.1884539391705121
	data : 0.11409597396850586
	model : 0.06459808349609375
			 train-loss:  2.173557506667243 	 ± 0.1779516400083227
	data : 0.11412687301635742
	model : 0.06471920013427734
			 train-loss:  2.176934039592743 	 ± 0.16912337607456684
	data : 0.1141899585723877
	model : 0.06481661796569824
			 train-loss:  2.1764245358380405 	 ± 0.16126086114146623
	data : 0.11416335105895996
	model : 0.06483612060546876
			 train-loss:  2.1958909134070077 	 ± 0.1673508738259635
	data : 0.1140904426574707
	model : 0.06482510566711426
			 train-loss:  2.210936738894536 	 ± 0.1690222039399972
	data : 0.11407570838928223
	model : 0.06477441787719726
			 train-loss:  2.2304339153426036 	 ± 0.17739705876143452
	data : 0.11401472091674805
	model : 0.06473965644836426
			 train-loss:  2.2293455680211385 	 ± 0.17143021425218666
	data : 0.11389760971069336
	model : 0.06481800079345704
			 train-loss:  2.2350108548998833 	 ± 0.167430524673425
	data : 0.113895845413208
	model : 0.06486172676086426
			 train-loss:  2.2254298785153557 	 ± 0.16689129237241473
	data : 0.11396594047546386
	model : 0.0649111270904541
			 train-loss:  2.2633537782563105 	 ± 0.22528894329572233
	data : 0.11392226219177246
	model : 0.06491117477416992
			 train-loss:  2.2503587634939897 	 ± 0.22610496203144712
	data : 0.11383070945739746
	model : 0.06485528945922851
			 train-loss:  2.283490628004074 	 ± 0.26348428726369494
	data : 0.1136688232421875
	model : 0.06467795372009277
			 train-loss:  2.2787676595506214 	 ± 0.25800037778878915
	data : 0.11363720893859863
	model : 0.0646634578704834
			 train-loss:  2.2766012332656165 	 ± 0.2522639712175844
	data : 0.11365184783935547
	model : 0.06466712951660156
			 train-loss:  2.2655059513838394 	 ± 0.25214796531923744
	data : 0.11381134986877442
	model : 0.06469864845275879
			 train-loss:  2.258234664797783 	 ± 0.24929006243504656
	data : 0.11392302513122558
	model : 0.0647507667541504
			 train-loss:  2.2415402698516846 	 ± 0.2575821840937493
	data : 0.11416745185852051
	model : 0.06490654945373535
			 train-loss:  2.2361312737831702 	 ± 0.25402390439208944
	data : 0.11416864395141602
	model : 0.06489920616149902
			 train-loss:  2.2308991661778204 	 ± 0.25069894583561525
	data : 0.11405825614929199
	model : 0.06484832763671874
			 train-loss:  2.2330580268587386 	 ± 0.24643692561758687
	data : 0.11390914916992187
	model : 0.0648430347442627
			 train-loss:  2.235794133153455 	 ± 0.24258317379721803
	data : 0.11402807235717774
	model : 0.0648719310760498
			 train-loss:  2.2364752689997354 	 ± 0.23853405928239782
	data : 0.11402196884155273
	model : 0.06482200622558594
			 train-loss:  2.240616821473645 	 ± 0.23574909489401097
	data : 0.11404800415039062
	model : 0.06484355926513671
			 train-loss:  2.233272135257721 	 ± 0.23561220577526876
	data : 0.11409940719604492
	model : 0.06489176750183105
			 train-loss:  2.242877822933775 	 ± 0.23829291125279523
	data : 0.11430439949035645
	model : 0.0648946762084961
			 train-loss:  2.237592339515686 	 ± 0.2367177799112852
	data : 0.11409764289855957
	model : 0.0648998737335205
			 train-loss:  2.233873428617205 	 ± 0.23431715405959883
	data : 0.11398706436157227
	model : 0.06490345001220703
			 train-loss:  2.2267041537496777 	 ± 0.23490072469908532
	data : 0.11396198272705078
	model : 0.06488661766052246
			 train-loss:  2.224601281655801 	 ± 0.23204792231866536
	data : 0.11400179862976074
	model : 0.06491856575012207
			 train-loss:  2.2237573924817537 	 ± 0.2290318351088291
	data : 0.11394596099853516
	model : 0.0649219036102295
			 train-loss:  2.2181550722855787 	 ± 0.2286990036342324
	data : 0.11412906646728516
	model : 0.06492986679077148
			 train-loss:  2.215579068660736 	 ± 0.22639445531867883
	data : 0.11433954238891601
	model : 0.06492404937744141
			 train-loss:  2.209198495236839 	 ± 0.22722854038571663
	data : 0.11437277793884278
	model : 0.06494369506835937
			 train-loss:  2.2075323973383223 	 ± 0.22476046833822502
	data : 0.11430692672729492
	model : 0.06485586166381836
			 train-loss:  2.200739544491435 	 ± 0.22645186603378042
	data : 0.1142247200012207
	model : 0.06482620239257812
			 train-loss:  2.1979359659281643 	 ± 0.22461737506259852
	data : 0.11412057876586915
	model : 0.06483969688415528
			 train-loss:  2.1988446182674832 	 ± 0.22218937133728234
	data : 0.11405715942382813
	model : 0.06483101844787598
			 train-loss:  2.197782760081084 	 ± 0.21987641126912746
	data : 0.1140434741973877
	model : 0.06486301422119141
			 train-loss:  2.2061792028711196 	 ± 0.22485554750697642
	data : 0.11410670280456543
	model : 0.06491250991821289
			 train-loss:  2.2056464602549872 	 ± 0.22253094805606718
	data : 0.11403861045837402
	model : 0.06487689018249512
			 train-loss:  2.204274284596346 	 ± 0.22045359534715675
	data : 0.11381478309631347
	model : 0.06474361419677735
			 train-loss:  2.201800241470337 	 ± 0.21892399486473937
	data : 0.11378874778747558
	model : 0.06474766731262208
			 train-loss:  2.198710100323546 	 ± 0.217865566381886
	data : 0.11374602317810059
	model : 0.06468715667724609
			 train-loss:  2.206228059071761 	 ± 0.22234008557915558
	data : 0.11387391090393066
	model : 0.06472630500793457
			 train-loss:  2.199612747948125 	 ± 0.22533978212357306
	data : 0.11396250724792481
	model : 0.06481924057006835
			 train-loss:  2.197332775151288 	 ± 0.223859760513996
	data : 0.11422486305236816
	model : 0.0649181842803955
			 train-loss:  2.2017785679210315 	 ± 0.22420829018135968
	data : 0.11423091888427735
	model : 0.0649268627166748
			 train-loss:  2.2027047276496887 	 ± 0.22230354864741497
	data : 0.11425113677978516
	model : 0.06491999626159668
			 train-loss:  2.2086387893609833 	 ± 0.22477500530885586
	data : 0.11418328285217286
	model : 0.06491236686706543
			 train-loss:  2.2082554718543745 	 ± 0.22284765671056955
	data : 0.11420631408691406
	model : 0.06492490768432617
			 train-loss:  2.209246417223397 	 ± 0.2210798935481179
	data : 0.11429314613342285
	model : 0.06494431495666504
			 train-loss:  2.2097174326578775 	 ± 0.21925967136227154
	data : 0.11424951553344727
	model : 0.06497392654418946
			 train-loss:  2.2102396761784786 	 ± 0.21749265792440975
	data : 0.11439003944396972
	model : 0.06496810913085938
			 train-loss:  2.212536865665067 	 ± 0.21647633954458204
	data : 0.11423850059509277
	model : 0.0649034023284912
			 train-loss:  2.213238617730519 	 ± 0.21482247669592588
	data : 0.11418380737304687
	model : 0.06483230590820313
			 train-loss:  2.216205183416605 	 ± 0.21443426816896066
	data : 0.11407346725463867
	model : 0.06479949951171875
			 train-loss:  2.2251446650578424 	 ± 0.22447529124085838
	data : 0.11427669525146485
	model : 0.06477227210998535
			 train-loss:  2.2240512840675586 	 ± 0.22294257251741834
	data : 0.1141672134399414
	model : 0.06483731269836426
			 train-loss:  2.2203325869432136 	 ± 0.22332542116226525
	data : 0.1142928123474121
	model : 0.06493601799011231
			 train-loss:  2.216483754270217 	 ± 0.22390467691354088
	data : 0.11448402404785156
	model : 0.0650254249572754
			 train-loss:  2.2161696510038515 	 ± 0.22229134858153785
	data : 0.11447482109069824
	model : 0.06505184173583985
			 train-loss:  2.221597014154707 	 ± 0.22525545498277882
	data : 0.11418771743774414
	model : 0.06496057510375977
			 train-loss:  2.2282123263453095 	 ± 0.23040993187584438
	data : 0.11424050331115723
	model : 0.06487522125244141
			 train-loss:  2.2238427483373218 	 ± 0.23174773179337899
	data : 0.11418776512145996
	model : 0.06488471031188965
			 train-loss:  2.2276597431261247 	 ± 0.23242267327646865
	data : 0.11406922340393066
	model : 0.06483120918273926
			 train-loss:  2.2258577524004757 	 ± 0.23135975981946885
	data : 0.11418986320495605
	model : 0.06483850479125977
			 train-loss:  2.22501079082489 	 ± 0.2299276496355271
	data : 0.11433596611022949
	model : 0.06491279602050781
			 train-loss:  2.2243526907343614 	 ± 0.22848105272921443
	data : 0.11432204246520997
	model : 0.06500287055969238
			 train-loss:  2.2243061390790073 	 ± 0.22699292374949015
	data : 0.11446218490600586
	model : 0.06498866081237793
			 train-loss:  2.2209481245432143 	 ± 0.22744993418094975
	data : 0.11434378623962402
	model : 0.06500749588012696
			 train-loss:  2.2197731899309763 	 ± 0.22624388442707316
	data : 0.11424427032470703
	model : 0.06497621536254883
			 train-loss:  2.2153482481837274 	 ± 0.22823955660226797
	data : 0.11418004035949707
	model : 0.06501975059509277
			 train-loss:  2.215019195168107 	 ± 0.22684538824441006
	data : 0.11428937911987305
	model : 0.06499342918395996
			 train-loss:  2.213764011859894 	 ± 0.2257407755690335
	data : 0.11427879333496094
	model : 0.06495423316955566
			 train-loss:  2.2121410815112563 	 ± 0.224857544169894
	data : 0.11437983512878418
	model : 0.0649341106414795
			 train-loss:  2.212408098436537 	 ± 0.22352833657170804
	data : 0.11439752578735352
	model : 0.06497530937194824
			 train-loss:  2.209776825063369 	 ± 0.223514377743427
	data : 0.11441230773925781
	model : 0.06494574546813965
			 train-loss:  2.2103045513463573 	 ± 0.2222643342645165
	data : 0.11431217193603516
	model : 0.06488313674926757
			 train-loss:  2.2080783679567535 	 ± 0.2219455078655231
	data : 0.11407256126403809
	model : 0.06486082077026367
			 train-loss:  2.211923924359408 	 ± 0.22357687797038853
	data : 0.11412982940673828
	model : 0.06481208801269531
			 train-loss:  2.211080221647627 	 ± 0.22245811821233558
	data : 0.11399140357971191
	model : 0.06479806900024414
			 train-loss:  2.2086356891526115 	 ± 0.22241760892163884
	data : 0.1140408992767334
	model : 0.06482601165771484
			 train-loss:  2.206125796496213 	 ± 0.22247006846578884
	data : 0.1140744686126709
	model : 0.06492471694946289
			 train-loss:  2.2097537258396978 	 ± 0.22394797388715254
	data : 0.11435127258300781
	model : 0.06493873596191406
			 train-loss:  2.2118347562769407 	 ± 0.2236332756643737
	data : 0.11425204277038574
	model : 0.06494231224060058
			 train-loss:  2.2103900858696472 	 ± 0.22287642060719332
	data : 0.11427464485168456
	model : 0.06485061645507813
			 train-loss:  2.211902673620927 	 ± 0.2221847887616338
	data : 0.11427922248840332
	model : 0.0647660732269287
			 train-loss:  2.2101213335990906 	 ± 0.22170543908372004
	data : 0.11428337097167969
	model : 0.06473560333251953
			 train-loss:  2.2153406536456237 	 ± 0.22641053778005893
	data : 0.1142690658569336
	model : 0.06478142738342285
			 train-loss:  2.215682788771026 	 ± 0.22527762249914446
	data : 0.1143010139465332
	model : 0.06483306884765624
			 train-loss:  2.215420436377477 	 ± 0.2241520155693636
	data : 0.11439943313598633
	model : 0.06492023468017578
			 train-loss:  2.2165729141235353 	 ± 0.22332303310122512
	data : 0.11439433097839355
	model : 0.06494560241699218
			 train-loss:  2.2137518708068544 	 ± 0.22399824066314114
	data : 0.11425933837890626
	model : 0.06490387916564941
			 train-loss:  2.2172702176898134 	 ± 0.22568463222488047
	data : 0.11426939964294433
	model : 0.06481242179870605
			 train-loss:  2.2145886652677964 	 ± 0.22621340917757973
	data : 0.11417975425720214
	model : 0.06483817100524902
			 train-loss:  2.2147608559865217 	 ± 0.22513000038507552
	data : 0.11431336402893066
	model : 0.06483473777770996
			 train-loss:  2.218344881421044 	 ± 0.22701700750289985
	data : 0.11423120498657227
	model : 0.06484794616699219
			 train-loss:  2.219808967608326 	 ± 0.22644115965538508
	data : 0.11427106857299804
	model : 0.06487298011779785
			 train-loss:  2.220050811767578 	 ± 0.22539429312604178
	data : 0.11417827606201172
	model : 0.06492767333984376
			 train-loss:  2.218873812092675 	 ± 0.22467848843929897
	data : 0.11429066658020019
	model : 0.06487970352172852
			 train-loss:  2.2173518609563145 	 ± 0.22420406826657363
	data : 0.11407561302185058
	model : 0.06483097076416015
			 train-loss:  2.2142848990180277 	 ± 0.22546788936039175
	data : 0.11421399116516114
	model : 0.06485142707824706
			 train-loss:  2.214088025393787 	 ± 0.22445946793389104
	data : 0.11432123184204102
	model : 0.06490788459777833
			 train-loss:  2.2158233991691043 	 ± 0.22420189996085876
	data : 0.11443552970886231
	model : 0.06489205360412598
			 train-loss:  2.2164208277136876 	 ± 0.2232971806161596
	data : 0.11438136100769043
	model : 0.06490097045898438
			 train-loss:  2.213311034336425 	 ± 0.22475998358727822
	data : 0.11455392837524414
	model : 0.0649846076965332
			 train-loss:  2.2132294198741085 	 ± 0.22378232921636593
	data : 0.11450443267822266
	model : 0.06499857902526855
			 train-loss:  2.2148265838623047 	 ± 0.223472988678914
	data : 0.11440973281860352
	model : 0.06496405601501465
			 train-loss:  2.214625680548513 	 ± 0.2225264468791729
	data : 0.11417651176452637
	model : 0.06497116088867187
			 train-loss:  2.212884032120139 	 ± 0.22238092548401525
	data : 0.1142838478088379
	model : 0.06496448516845703
			 train-loss:  2.2164242307678994 	 ± 0.2247589754248059
	data : 0.11413516998291015
	model : 0.06493854522705078
			 train-loss:  2.214019687970479 	 ± 0.2253523079396984
	data : 0.11407680511474609
	model : 0.06490550041198731
			 train-loss:  2.216637319769741 	 ± 0.22624367958121072
	data : 0.11417222023010254
	model : 0.06491398811340332
			 train-loss:  2.2149316482856625 	 ± 0.22609438439233795
	data : 0.11428356170654297
	model : 0.06497201919555665
			 train-loss:  2.2131080540215096 	 ± 0.22607251540657577
	data : 0.11412510871887208
	model : 0.06506881713867188
			 train-loss:  2.209579743685261 	 ± 0.22853410774902314
	data : 0.11425442695617676
	model : 0.06507296562194824
			 train-loss:  2.208162398338318 	 ± 0.22816466709075509
	data : 0.11426115036010742
	model : 0.06508035659790039
			 train-loss:  2.2063760464153592 	 ± 0.22813335875295124
	data : 0.1140223503112793
	model : 0.06507644653320313
			 train-loss:  2.2054180413719235 	 ± 0.2274877294522993
	data : 0.11398682594299317
	model : 0.06505961418151855
			 train-loss:  2.2039907751604915 	 ± 0.22716750482295525
	data : 0.11416611671447754
	model : 0.06500773429870606
			 train-loss:  2.2019553563391514 	 ± 0.2274540193447931
	data : 0.11403741836547851
	model : 0.06502032279968262
			 train-loss:  2.2038238222782427 	 ± 0.22756916995209767
	data : 0.11415014266967774
	model : 0.06504740715026855
			 train-loss:  2.2031174170151924 	 ± 0.2268419538863076
	data : 0.11443591117858887
	model : 0.06504526138305664
			 train-loss:  2.203034922029033 	 ± 0.22598304302497915
	data : 0.11441798210144043
	model : 0.06503186225891114
			 train-loss:  2.2023525390409886 	 ± 0.22526834804137186
	data : 0.11433854103088378
	model : 0.0649897575378418
			 train-loss:  2.202720425911804 	 ± 0.22446631982404344
	data : 0.1143113136291504
	model : 0.06490817070007324
			 train-loss:  2.2001557862317123 	 ± 0.2255953768050628
	data : 0.11455788612365722
	model : 0.06487073898315429
			 train-loss:  2.1979211998336456 	 ± 0.22625906522294775
	data : 0.11436767578125
	model : 0.06483416557312012
			 train-loss:  2.199485269776226 	 ± 0.22616850061864607
	data : 0.11447577476501465
	model : 0.06482529640197754
			 train-loss:  2.1991196827612063 	 ± 0.2253881833710866
	data : 0.11436820030212402
	model : 0.0648576259613037
			 train-loss:  2.198560960858846 	 ± 0.22467186352556884
	data : 0.1145207405090332
	model : 0.06492877006530762
			 train-loss:  2.204350344623838 	 ± 0.23404219944503923
	data : 0.11422319412231445
	model : 0.06494827270507812
			 train-loss:  2.205397185704387 	 ± 0.23353948979000566
	data : 0.1147911548614502
	model : 0.0649747371673584
			 train-loss:  2.204118784884332 	 ± 0.23321029441814395
	data : 0.11470112800598145
	model : 0.06496658325195312
			 train-loss:  2.207762852415338 	 ± 0.2364156569726635
	data : 0.11477890014648437
	model : 0.06490769386291503
			 train-loss:  2.2046661120322018 	 ± 0.2384859807119322
	data : 0.11451845169067383
	model : 0.06482276916503907
			 train-loss:  2.204848597789633 	 ± 0.23767228085228959
	data : 0.1144876480102539
	model : 0.06486883163452148
			 train-loss:  2.2039895245473677 	 ± 0.2370827264744115
	data : 0.11402087211608887
	model : 0.0648524284362793
			 train-loss:  2.205977130909355 	 ± 0.23749238913242507
	data : 0.1140756607055664
	model : 0.06486835479736328
			 train-loss:  2.205784386074221 	 ± 0.23670022633153986
	data : 0.11409907341003418
	model : 0.0649186134338379
			 train-loss:  2.2040860308896777 	 ± 0.23680766406431278
	data : 0.11439523696899415
	model : 0.06501202583312989
			 train-loss:  2.2017675948143007 	 ± 0.23770762298260073
	data : 0.11444110870361328
	model : 0.06498165130615234
			 train-loss:  2.2014501860599642 	 ± 0.23695109525596542
	data : 0.11439981460571289
	model : 0.06497182846069335
			 train-loss:  2.202306443923398 	 ± 0.23640463393289463
	data : 0.11425347328186035
	model : 0.06491060256958008
			 train-loss:  2.202335048345179 	 ± 0.2356310671726915
	data : 0.11415395736694336
	model : 0.06482472419738769
			 train-loss:  2.20117663330846 	 ± 0.2353014703234642
	data : 0.11409826278686523
	model : 0.06480507850646973
			 train-loss:  2.2014723200951853 	 ± 0.23456990688430854
	data : 0.11402850151062012
	model : 0.06480903625488281
			 train-loss:  2.1996309887140226 	 ± 0.2349379856634786
	data : 0.11408710479736328
	model : 0.06485443115234375
			 train-loss:  2.199403139436321 	 ± 0.23420587081569294
	data : 0.11432833671569824
	model : 0.06490111351013184
			 train-loss:  2.198434103138839 	 ± 0.23377906412036695
	data : 0.11445512771606445
	model : 0.06496667861938477
			 train-loss:  2.1968463299409398 	 ± 0.23389579989037726
	data : 0.11439452171325684
	model : 0.0649118423461914
			 train-loss:  2.1958669193089007 	 ± 0.2334905670651765
	data : 0.11427602767944336
	model : 0.06485276222229004
			 train-loss:  2.194911605082684 	 ± 0.23307776555698545
	data : 0.11434602737426758
	model : 0.0648569107055664
			 train-loss:  2.1952188625747775 	 ± 0.23238998113853684
	data : 0.1143941879272461
	model : 0.0648423671722412
			 train-loss:  2.1968419383639937 	 ± 0.23259525453899063
	data : 0.11445102691650391
	model : 0.06483592987060546
			 train-loss:  2.197326983620481 	 ± 0.23196771294215496
	data : 0.11452960968017578
	model : 0.06492791175842286
			 train-loss:  2.196611116149209 	 ± 0.23144534784390042
	data : 0.11486248970031739
	model : 0.06495742797851563
			 train-loss:  2.1950402123382293 	 ± 0.23162779080617452
	data : 0.1147773265838623
	model : 0.06497926712036133
			 train-loss:  2.193322241663219 	 ± 0.23199160256982637
	data : 0.11465120315551758
	model : 0.0650787353515625
			 train-loss:  2.194313869589851 	 ± 0.2316548323169891
	data : 0.11455984115600586
	model : 0.0650947093963623
			 train-loss:  2.1924068998302935 	 ± 0.23228723761452277
	data : 0.11445460319519044
	model : 0.06503329277038575
			 train-loss:  2.191432470433852 	 ± 0.23194920167237265
	data : 0.11436853408813477
	model : 0.06500253677368165
			 train-loss:  2.1895504060544466 	 ± 0.23256822229612126
	data : 0.11428241729736328
	model : 0.0649712085723877
			 train-loss:  2.1906485328840657 	 ± 0.23233535879581488
	data : 0.1143078327178955
	model : 0.06488842964172363
			 train-loss:  2.190030389438475 	 ± 0.2318046998593903
	data : 0.11427979469299317
	model : 0.06491265296936036
			 train-loss:  2.188601065641162 	 ± 0.2319009256822288
	data : 0.1143578052520752
	model : 0.06499657630920411
			 train-loss:  2.1885474729537964 	 ± 0.2312384829759616
	data : 0.11427597999572754
	model : 0.06502885818481445
			 train-loss:  2.1900293319062754 	 ± 0.2314124147711096
	data : 0.11440119743347169
	model : 0.06498556137084961
			 train-loss:  2.1901751702788186 	 ± 0.2307658922258084
	data : 0.11423249244689941
	model : 0.06498928070068359
			 train-loss:  2.189940075526077 	 ± 0.230138015427935
	data : 0.11422796249389648
	model : 0.06490769386291503
			 train-loss:  2.1908689444291527 	 ± 0.2298286287216584
	data : 0.11413474082946777
	model : 0.06484951972961425
			 train-loss:  2.1943055649598437 	 ± 0.23375585989903624
	data : 0.11428661346435547
	model : 0.0648409366607666
			 train-loss:  2.1936909566268077 	 ± 0.23325502661387526
	data : 0.1141434669494629
	model : 0.06486945152282715
			 train-loss:  2.1937203715135762 	 ± 0.23261367006669995
	data : 0.11429324150085449
	model : 0.06489357948303223
			 train-loss:  2.1950427932166012 	 ± 0.2326622502695487
	data : 0.11433868408203125
	model : 0.06496891975402833
			 train-loss:  2.194047100518061 	 ± 0.2324197828890977
	data : 0.11454472541809083
	model : 0.06500520706176757
			 train-loss:  2.1952610151187795 	 ± 0.23237491568007326
	data : 0.11440653800964355
	model : 0.06501235961914062
			 train-loss:  2.1948168425149817 	 ± 0.23182814234873572
	data : 0.114402437210083
	model : 0.06494340896606446
			 train-loss:  2.1933134354372075 	 ± 0.23211481703238837
	data : 0.1142148494720459
	model : 0.06491456031799317
			 train-loss:  2.1932226952086102 	 ± 0.23149999283772793
	data : 0.11421351432800293
	model : 0.06491084098815918
			 train-loss:  2.1926673467827853 	 ± 0.2310122751185126
	data : 0.11403031349182129
	model : 0.06488208770751953
			 train-loss:  2.1914594148334703 	 ± 0.23100122047642238
	data : 0.11403965950012207
	model : 0.06491284370422364
			 train-loss:  2.1916696401166664 	 ± 0.23041393389578893
	data : 0.11417078971862793
	model : 0.06499924659729003
			 train-loss:  2.19024888984859 	 ± 0.23065039989972214
	data : 0.11432180404663086
	model : 0.06506214141845704
			 train-loss:  2.1923327106268293 	 ± 0.23185703499525442
	data : 0.1142165184020996
	model : 0.06507091522216797
			 train-loss:  2.1923211081740783 	 ± 0.23125874943544966
	data : 0.11422214508056641
	model : 0.06501860618591308
			 train-loss:  2.192224241525699 	 ± 0.23066896190594507
	data : 0.11414847373962403
	model : 0.06497197151184082
			 train-loss:  2.1922845651908798 	 ± 0.23008131024906195
	data : 0.11404190063476563
	model : 0.06497745513916016
			 train-loss:  2.1907654123257863 	 ± 0.23047998892719948
	data : 0.11412863731384278
	model : 0.06496186256408691
			 train-loss:  2.191771545795479 	 ± 0.23033054788157784
	data : 0.11433587074279786
	model : 0.06495013236999511
			 train-loss:  2.1909910769917857 	 ± 0.23001342473802433
	data : 0.11440486907958984
	model : 0.06502113342285157
			 train-loss:  2.190476566553116 	 ± 0.2295524431567322
	data : 0.11445093154907227
	model : 0.06504611968994141
			 train-loss:  2.18979050508186 	 ± 0.22918616758212212
	data : 0.11453619003295898
	model : 0.06505966186523438
			 train-loss:  2.1897601998678526 	 ± 0.22861857497889992
	data : 0.1144782543182373
	model : 0.06503849029541016
			 train-loss:  2.189454178504756 	 ± 0.2280962510158904
	data : 0.11420507431030273
	model : 0.06500844955444336
			 train-loss:  2.1912109466160046 	 ± 0.22890907921111325
	data : 0.11412553787231446
	model : 0.06491093635559082
			 train-loss:  2.1908979741538444 	 ± 0.22839383099958632
	data : 0.1139552116394043
	model : 0.06493797302246093
			 train-loss:  2.1904558799799205 	 ± 0.2279267133926007
	data : 0.11396198272705078
	model : 0.06498031616210938
			 train-loss:  2.191691175175174 	 ± 0.22806570156291475
	data : 0.11395773887634278
	model : 0.06497011184692383
			 train-loss:  2.1914177456727395 	 ± 0.2275508144966223
	data : 0.11411609649658203
	model : 0.06500167846679687
			 train-loss:  2.1902415837967797 	 ± 0.22763866803953134
	data : 0.11416783332824706
	model : 0.06509790420532227
			 train-loss:  2.1909892280896504 	 ± 0.22735309423916855
	data : 0.11440563201904297
	model : 0.06507196426391601
			 train-loss:  2.192103657677275 	 ± 0.22738791976488032
	data : 0.11439704895019531
	model : 0.06500005722045898
			 train-loss:  2.1907165545337604 	 ± 0.22774404256352623
	data : 0.11426420211791992
	model : 0.06500873565673829
			 train-loss:  2.1900548117821206 	 ± 0.22741300705607964
	data : 0.11415262222290039
	model : 0.06495299339294433
			 train-loss:  2.189606438173312 	 ± 0.22697539592050364
	data : 0.11411666870117188
	model : 0.06490564346313477
			 train-loss:  2.1898246809493664 	 ± 0.22646943577062
	data : 0.11407771110534667
	model : 0.064915132522583
			 train-loss:  2.19029728240437 	 ± 0.22605083438540066
	data : 0.11412014961242675
	model : 0.0649064064025879
			 train-loss:  2.190698247900756 	 ± 0.22560635539855084
	data : 0.11428303718566894
	model : 0.06487622261047363
			 train-loss:  2.190593419818703 	 ± 0.2250936118264809
	data : 0.11446571350097656
	model : 0.06495981216430664
			 train-loss:  2.190678490895659 	 ± 0.2245826239798162
	data : 0.11450896263122559
	model : 0.06495552062988282
			 train-loss:  2.1907160379669883 	 ± 0.22407231652197623
	data : 0.11436376571655274
	model : 0.06489129066467285
			 train-loss:  2.190768628098846 	 ± 0.2235661516113828
	data : 0.11438207626342774
	model : 0.0648458480834961
			 train-loss:  2.191412133139533 	 ± 0.22326709702792394
	data : 0.11430387496948242
	model : 0.06478509902954102
			 train-loss:  2.190757253244853 	 ± 0.22297952983509428
	data : 0.1142606258392334
	model : 0.06467580795288086
			 train-loss:  2.189951274011816 	 ± 0.22280657167093432
	data : 0.11440191268920899
	model : 0.06458902359008789
			 train-loss:  2.190680464638604 	 ± 0.2225786128605951
	data : 0.11469621658325195
	model : 0.06448469161987305
			 train-loss:  2.191286542774302 	 ± 0.2222716340206969
	data : 0.11467142105102539
	model : 0.06435279846191407
			 train-loss:  2.1895743062317634 	 ± 0.2232702710397899
	data : 0.1148038387298584
	model : 0.06422910690307618
			 train-loss:  2.1899803516111875 	 ± 0.22286408730339927
	data : 0.11482372283935546
	model : 0.06406769752502442
			 train-loss:  2.190346798521983 	 ± 0.22244578087292413
	data : 0.11476359367370606
	model : 0.06394405364990234
			 train-loss:  2.1909427243730297 	 ± 0.22214479507149812
	data : 0.11475129127502441
	model : 0.06391119956970215
			 train-loss:  2.192122959471368 	 ± 0.22238493778565
	data : 0.11488595008850097
	model : 0.06388530731201172
			 train-loss:  2.192850795799288 	 ± 0.222180699996301
	data : 0.11492023468017579
	model : 0.0639033317565918
			 train-loss:  2.1924355956106227 	 ± 0.22179358515709768
	data : 0.11496891975402831
	model : 0.06397666931152343
			 train-loss:  2.1939678319499025 	 ± 0.22255155771087481
	data : 0.11512541770935059
	model : 0.0639918327331543
			 train-loss:  2.1934711329480434 	 ± 0.2222074783176439
	data : 0.11505789756774902
	model : 0.06393852233886718
			 train-loss:  2.1927541014501606 	 ± 0.2220084771321786
	data : 0.11489381790161132
	model : 0.06391324996948242
			 train-loss:  2.192303552406247 	 ± 0.22164770551038174
	data : 0.11487102508544922
	model : 0.06390066146850586
			 train-loss:  2.1934236763906076 	 ± 0.221852755341019
	data : 0.11501469612121581
	model : 0.06384406089782715
			 train-loss:  2.192967475208777 	 ± 0.2214999813643355
	data : 0.11498880386352539
	model : 0.0638700008392334
			 train-loss:  2.1947888190547626 	 ± 0.22282425554907917
	data : 0.11499714851379395
	model : 0.06394023895263672
			 train-loss:  2.194849938772526 	 ± 0.22236349997222463
	data : 0.11516370773315429
	model : 0.06396069526672363
			 train-loss:  2.1933233629573476 	 ± 0.2231654997763471
	data : 0.11510882377624512
	model : 0.06390624046325684
			 train-loss:  2.194391159363735 	 ± 0.22332446374822745
	data : 0.11489548683166503
	model : 0.0638758659362793
			 train-loss:  2.194002471009239 	 ± 0.2229487100916146
	data : 0.11473870277404785
	model : 0.06392507553100586
			 train-loss:  2.1939780060125855 	 ± 0.22249357568689535
	data : 0.11478385925292969
	model : 0.06394438743591309
			 train-loss:  2.1949727457713304 	 ± 0.22258613430593305
	data : 0.11476345062255859
	model : 0.06397223472595215
			 train-loss:  2.194651809298558 	 ± 0.22219212377141076
	data : 0.11473712921142579
	model : 0.06402826309204102
			 train-loss:  2.193993664556934 	 ± 0.22198481729380548
	data : 0.11491255760192871
	model : 0.06402153968811035
			 train-loss:  2.1934647847370927 	 ± 0.22169512287870358
	data : 0.11481666564941406
	model : 0.06389889717102051
			 train-loss:  2.1945885677337644 	 ± 0.22196078880254771
	data : 0.11488714218139648
	model : 0.06382260322570801
			 train-loss:  2.1937624591280263 	 ± 0.22190296163159437
	data : 0.11482172012329102
	model : 0.06385531425476074
			 train-loss:  2.193089829550849 	 ± 0.2217184790559179
	data : 0.11505122184753418
	model : 0.06384520530700684
			 train-loss:  2.192947255764083 	 ± 0.2212914407539152
	data : 0.11503033638000489
	model : 0.0638923168182373
			 train-loss:  2.191859723545435 	 ± 0.22153179596344513
	data : 0.11520614624023437
	model : 0.06393623352050781
			 train-loss:  2.1922316228642185 	 ± 0.22117642485803152
	data : 0.11514120101928711
	model : 0.06398172378540039
			 train-loss:  2.1929021277464926 	 ± 0.22100353641938814
	data : 0.11482996940612793
	model : 0.05548992156982422
#epoch  82    val-loss:  2.4452748424128483  train-loss:  2.1929021277464926  lr:  1.9073486328125e-08
			 train-loss:  2.2160208225250244 	 ± 0.0
	data : 5.864849090576172
	model : 0.07184982299804688
			 train-loss:  2.229467272758484 	 ± 0.013446450233459473
	data : 2.997459888458252
	model : 0.07047867774963379
			 train-loss:  2.190001885096232 	 ± 0.05688208560332711
	data : 2.0365939935048423
	model : 0.06843447685241699
			 train-loss:  2.189542770385742 	 ± 0.04926774914576816
	data : 1.5559197068214417
	model : 0.06763851642608643
			 train-loss:  2.3308670997619627 	 ± 0.2860631279617601
	data : 1.2676167964935303
	model : 0.06705279350280761
			 train-loss:  2.4073568979899087 	 ± 0.3121648183478529
	data : 0.11745266914367676
	model : 0.06560912132263183
			 train-loss:  2.34058940410614 	 ± 0.33207421049791397
	data : 0.11423172950744628
	model : 0.0646904468536377
			 train-loss:  2.310197129845619 	 ± 0.32086594313073075
	data : 0.11397590637207031
	model : 0.06480522155761718
			 train-loss:  2.280994004673428 	 ± 0.3135890532947395
	data : 0.11399006843566895
	model : 0.06469511985778809
			 train-loss:  2.2666905999183653 	 ± 0.3005754005236393
	data : 0.1140063762664795
	model : 0.06470437049865722
			 train-loss:  2.272773493419994 	 ± 0.28723222933688386
	data : 0.11405153274536133
	model : 0.06477994918823242
			 train-loss:  2.2895470956961312 	 ± 0.2805744972471928
	data : 0.11414160728454589
	model : 0.0648341178894043
			 train-loss:  2.283732460095332 	 ± 0.2703187491810808
	data : 0.11423883438110352
	model : 0.06485395431518555
			 train-loss:  2.260600975581578 	 ± 0.2735116705347772
	data : 0.11431694030761719
	model : 0.06490273475646972
			 train-loss:  2.2492351055145265 	 ± 0.267637728451748
	data : 0.11423172950744628
	model : 0.06492924690246582
			 train-loss:  2.235716924071312 	 ± 0.2643751123462851
	data : 0.1141472339630127
	model : 0.06489028930664062
			 train-loss:  2.2356503009796143 	 ± 0.25648167091475965
	data : 0.11399917602539063
	model : 0.06483211517333984
			 train-loss:  2.22633515463935 	 ± 0.25219708176004213
	data : 0.11390395164489746
	model : 0.06481213569641113
			 train-loss:  2.2096016595238135 	 ± 0.25553081970596064
	data : 0.11380414962768555
	model : 0.06482253074645997
			 train-loss:  2.1991130113601685 	 ± 0.2532220821617539
	data : 0.11379165649414062
	model : 0.06482563018798829
			 train-loss:  2.2181831314450218 	 ± 0.2614219030707684
	data : 0.11385350227355957
	model : 0.06481027603149414
			 train-loss:  2.2106503031470557 	 ± 0.25773357786578527
	data : 0.11391725540161132
	model : 0.06484403610229492
			 train-loss:  2.2154670279958975 	 ± 0.25307884676805453
	data : 0.1139763355255127
	model : 0.06486940383911133
			 train-loss:  2.2081365883350372 	 ± 0.250232114949999
	data : 0.11396498680114746
	model : 0.06479086875915527
			 train-loss:  2.203967170715332 	 ± 0.24602577900934633
	data : 0.11401529312133789
	model : 0.0647672176361084
			 train-loss:  2.198767909636864 	 ± 0.24264473109039938
	data : 0.11396470069885253
	model : 0.0648390769958496
			 train-loss:  2.2066193951500788 	 ± 0.24145112784062128
	data : 0.11408977508544922
	model : 0.06488828659057617
			 train-loss:  2.200182616710663 	 ± 0.23944773906901615
	data : 0.11415081024169922
	model : 0.06487793922424316
			 train-loss:  2.1934959395178435 	 ± 0.237928706664393
	data : 0.11420044898986817
	model : 0.06493115425109863
			 train-loss:  2.188664356867472 	 ± 0.23537214932132053
	data : 0.11417827606201172
	model : 0.06495246887207032
			 train-loss:  2.183803919822939 	 ± 0.2330700876982343
	data : 0.11426658630371093
	model : 0.0649458885192871
			 train-loss:  2.1814238652586937 	 ± 0.22978189261915416
	data : 0.11413612365722656
	model : 0.06488037109375
			 train-loss:  2.1869262565266 	 ± 0.22840439712285948
	data : 0.11405510902404785
	model : 0.06486263275146484
			 train-loss:  2.191245850394754 	 ± 0.22638450116725115
	data : 0.11409192085266114
	model : 0.06487879753112794
			 train-loss:  2.1818431854248046 	 ± 0.2297642313140329
	data : 0.114044189453125
	model : 0.06489782333374024
			 train-loss:  2.177748203277588 	 ± 0.2278422247086594
	data : 0.11403326988220215
	model : 0.0648719310760498
			 train-loss:  2.1845363668493323 	 ± 0.22840293368662637
	data : 0.11400036811828614
	model : 0.0649491786956787
			 train-loss:  2.194799630265487 	 ± 0.23386414849230372
	data : 0.11411914825439454
	model : 0.0649043083190918
			 train-loss:  2.1981299962752905 	 ± 0.23175750371833267
	data : 0.11406874656677246
	model : 0.06486272811889648
			 train-loss:  2.2088669657707216 	 ± 0.23846335160857726
	data : 0.11405296325683593
	model : 0.06478500366210938
			 train-loss:  2.2051505344669993 	 ± 0.23670720080846339
	data : 0.11376814842224121
	model : 0.06477031707763672
			 train-loss:  2.2095648788270497 	 ± 0.23557416679213838
	data : 0.11390247344970703
	model : 0.06474142074584961
			 train-loss:  2.2129285668217857 	 ± 0.23383713768746534
	data : 0.11391816139221192
	model : 0.06480445861816406
			 train-loss:  2.207466396418485 	 ± 0.2339230690066751
	data : 0.11397900581359863
	model : 0.06482763290405273
			 train-loss:  2.2020732243855794 	 ± 0.2340593932438502
	data : 0.11403951644897461
	model : 0.06487336158752441
			 train-loss:  2.1956119278202886 	 ± 0.2355239336031834
	data : 0.11434822082519532
	model : 0.06489853858947754
			 train-loss:  2.1891685698894743 	 ± 0.2370676142978069
	data : 0.11432337760925293
	model : 0.0649299144744873
			 train-loss:  2.1921258717775345 	 ± 0.23545964257575885
	data : 0.11416854858398437
	model : 0.06484947204589844
			 train-loss:  2.1897188838647335 	 ± 0.23364049669603337
	data : 0.11408414840698242
	model : 0.06485986709594727
			 train-loss:  2.204717354774475 	 ± 0.25400566216982146
	data : 0.11406793594360351
	model : 0.06484260559082031
			 train-loss:  2.1985743957407333 	 ± 0.25522656149138706
	data : 0.11399478912353515
	model : 0.06483430862426758
			 train-loss:  2.205305766600829 	 ± 0.2572912220139574
	data : 0.11402716636657714
	model : 0.06479511260986329
			 train-loss:  2.199767184707354 	 ± 0.2579629541665435
	data : 0.11411552429199219
	model : 0.06488599777221679
			 train-loss:  2.1941639096648604 	 ± 0.2587983725943141
	data : 0.11413307189941406
	model : 0.06487746238708496
			 train-loss:  2.1966544541445647 	 ± 0.2570871314825216
	data : 0.11415324211120606
	model : 0.06484999656677246
			 train-loss:  2.194307621036257 	 ± 0.255375148120051
	data : 0.11395044326782226
	model : 0.06478705406188964
			 train-loss:  2.1986767283657143 	 ± 0.2552279516081855
	data : 0.11373891830444335
	model : 0.06479253768920898
			 train-loss:  2.1966398748858222 	 ± 0.25348503171608555
	data : 0.11369013786315918
	model : 0.06478557586669922
			 train-loss:  2.1989110566801946 	 ± 0.2519221679789261
	data : 0.1137148380279541
	model : 0.06482658386230469
			 train-loss:  2.195666750272115 	 ± 0.25105385543208913
	data : 0.1136871337890625
	model : 0.06491012573242187
			 train-loss:  2.1966696293627628 	 ± 0.24910868669235084
	data : 0.11389722824096679
	model : 0.06498312950134277
			 train-loss:  2.2037727371338875 	 ± 0.25324286960735193
	data : 0.11400947570800782
	model : 0.06498618125915527
			 train-loss:  2.2005461170559837 	 ± 0.25250637677600807
	data : 0.11401338577270508
	model : 0.06491107940673828
			 train-loss:  2.2019177116453648 	 ± 0.2507623351046241
	data : 0.11402196884155273
	model : 0.06483430862426758
			 train-loss:  2.199924747760479 	 ± 0.24933619695408443
	data : 0.11395492553710937
	model : 0.06479859352111816
			 train-loss:  2.1961948094945964 	 ± 0.2492607071212877
	data : 0.11405963897705078
	model : 0.0647660255432129
			 train-loss:  2.190293427723557 	 ± 0.2519962489731164
	data : 0.11402459144592285
	model : 0.0648158073425293
			 train-loss:  2.191913192763048 	 ± 0.25048760197577236
	data : 0.11408700942993164
	model : 0.06483783721923828
			 train-loss:  2.1903686713481294 	 ± 0.24899181111839047
	data : 0.11415743827819824
	model : 0.06487689018249512
			 train-loss:  2.193154915741512 	 ± 0.2482879569758253
	data : 0.1141627311706543
	model : 0.06487164497375489
			 train-loss:  2.195289751173745 	 ± 0.2471794283708045
	data : 0.11392083168029785
	model : 0.06482644081115722
			 train-loss:  2.1943047758605747 	 ± 0.2455971774765681
	data : 0.11392741203308106
	model : 0.06479649543762207
			 train-loss:  2.191572860495685 	 ± 0.24500828966811947
	data : 0.11393890380859376
	model : 0.06485638618469239
			 train-loss:  2.1928657051679252 	 ± 0.24359777117570594
	data : 0.11391658782958984
	model : 0.06485252380371094
			 train-loss:  2.1933339929580686 	 ± 0.24200186676092686
	data : 0.1139941692352295
	model : 0.06487164497375489
			 train-loss:  2.1906735332388627 	 ± 0.2415060374480386
	data : 0.11414294242858887
	model : 0.06490240097045899
			 train-loss:  2.1894924578728614 	 ± 0.24015351714989844
	data : 0.11417684555053711
	model : 0.0648878574371338
			 train-loss:  2.1906748704421215 	 ± 0.23883458480337105
	data : 0.11409821510314941
	model : 0.06480469703674316
			 train-loss:  2.1921678011930443 	 ± 0.2376841569799577
	data : 0.11404242515563964
	model : 0.06480951309204101
			 train-loss:  2.1892674028873444 	 ± 0.23759663089885563
	data : 0.11399383544921875
	model : 0.06479253768920898
			 train-loss:  2.1904685144071228 	 ± 0.2363696941030635
	data : 0.11410670280456543
	model : 0.0648221492767334
			 train-loss:  2.1890196131496893 	 ± 0.2352856298613666
	data : 0.11420564651489258
	model : 0.06487360000610351
			 train-loss:  2.188244316951338 	 ± 0.23396930711619027
	data : 0.11428160667419433
	model : 0.06492056846618652
			 train-loss:  2.1827475357623327 	 ± 0.23790284278896617
	data : 0.1143345832824707
	model : 0.0649491786956787
			 train-loss:  2.18501092265634 	 ± 0.2374073121786374
	data : 0.11430273056030274
	model : 0.06492228507995605
			 train-loss:  2.1845378473747608 	 ± 0.23606329681995034
	data : 0.11423091888427735
	model : 0.06494779586791992
			 train-loss:  2.1866919542181082 	 ± 0.23555128509335127
	data : 0.1139974594116211
	model : 0.06490159034729004
			 train-loss:  2.187781787731431 	 ± 0.23442959796140597
	data : 0.1138960838317871
	model : 0.06492128372192382
			 train-loss:  2.1883134292752557 	 ± 0.23316220088701964
	data : 0.11382460594177246
	model : 0.06489953994750977
			 train-loss:  2.1887889742851256 	 ± 0.23190663511619608
	data : 0.11393260955810547
	model : 0.06495556831359864
			 train-loss:  2.188290303880042 	 ± 0.23067741836062536
	data : 0.11385641098022461
	model : 0.06496286392211914
			 train-loss:  2.1888025234574857 	 ± 0.22947234001337907
	data : 0.11401004791259765
	model : 0.06495294570922852
			 train-loss:  2.189406562876958 	 ± 0.2283088084531101
	data : 0.11392135620117187
	model : 0.06487154960632324
			 train-loss:  2.1890910242466215 	 ± 0.22711153908818105
	data : 0.11392889022827149
	model : 0.06485528945922851
			 train-loss:  2.1881094142010338 	 ± 0.22611342721910033
	data : 0.11382575035095215
	model : 0.06486306190490723
			 train-loss:  2.1866634028653302 	 ± 0.2253737922858588
	data : 0.1138824462890625
	model : 0.06485376358032227
			 train-loss:  2.1858340255992927 	 ± 0.22435627627239557
	data : 0.11394915580749512
	model : 0.06487131118774414
			 train-loss:  2.186891423196209 	 ± 0.22345147888258543
	data : 0.11411113739013672
	model : 0.06494474411010742
			 train-loss:  2.1876839736495355 	 ± 0.22245847171792998
	data : 0.11412601470947266
	model : 0.06496262550354004
			 train-loss:  2.1871313250064848 	 ± 0.2214116766442777
	data : 0.11415457725524902
	model : 0.06488885879516601
			 train-loss:  2.194147314175521 	 ± 0.2312145398243611
	data : 0.11409134864807129
	model : 0.06481990814208985
			 train-loss:  2.193112694749645 	 ± 0.23031317450356814
	data : 0.11398229598999024
	model : 0.06482448577880859
			 train-loss:  2.1919149044647956 	 ± 0.22951145087130626
	data : 0.11406140327453614
	model : 0.06485137939453126
			 train-loss:  2.1902139473419924 	 ± 0.22905679668814696
	data : 0.11418232917785645
	model : 0.06486196517944336
			 train-loss:  2.191101279712859 	 ± 0.22814297173026013
	data : 0.11427698135375977
	model : 0.06490998268127442
			 train-loss:  2.1916500746079213 	 ± 0.22713390032623185
	data : 0.11428313255310059
	model : 0.06493711471557617
			 train-loss:  2.190059010113511 	 ± 0.22666274169489659
	data : 0.11432819366455078
	model : 0.06491484642028808
			 train-loss:  2.191125555170907 	 ± 0.2258805198496488
	data : 0.11417322158813477
	model : 0.0648561954498291
			 train-loss:  2.1900341784188506 	 ± 0.2251278671484291
	data : 0.11396627426147461
	model : 0.06483254432678223
			 train-loss:  2.189913874322718 	 ± 0.22410574204556127
	data : 0.11389589309692383
	model : 0.06486072540283203
			 train-loss:  2.189885200680913 	 ± 0.22309417549964597
	data : 0.11403040885925293
	model : 0.06489481925964355
			 train-loss:  2.18982605529683 	 ± 0.2220968604267096
	data : 0.11405963897705078
	model : 0.06491990089416504
			 train-loss:  2.1908097551987233 	 ± 0.22135688759734998
	data : 0.1145869255065918
	model : 0.06496291160583496
			 train-loss:  2.190788443674121 	 ± 0.22038400201768002
	data : 0.11467609405517579
	model : 0.06497950553894043
			 train-loss:  2.1899992911711985 	 ± 0.21958543463414826
	data : 0.1146468162536621
	model : 0.06495752334594726
			 train-loss:  2.1917722954832275 	 ± 0.21946207035679466
	data : 0.11441612243652344
	model : 0.06486868858337402
			 train-loss:  2.191883876792386 	 ± 0.21852549019903356
	data : 0.11416335105895996
	model : 0.06484627723693848
			 train-loss:  2.1929328229467746 	 ± 0.217893170664358
	data : 0.11377062797546386
	model : 0.06482629776000977
			 train-loss:  2.197268747481979 	 ± 0.2220290348037765
	data : 0.11374602317810059
	model : 0.06481084823608399
			 train-loss:  2.2007156600554785 	 ± 0.2242764999025304
	data : 0.11424541473388672
	model : 0.06479692459106445
			 train-loss:  2.205233088209609 	 ± 0.2287643005604127
	data : 0.11434826850891114
	model : 0.06486434936523437
			 train-loss:  2.2064639476479075 	 ± 0.22822677788089554
	data : 0.11456928253173829
	model : 0.06484513282775879
			 train-loss:  2.206880130418917 	 ± 0.22734361268351697
	data : 0.11438927650451661
	model : 0.06479392051696778
			 train-loss:  2.208523012938038 	 ± 0.22715696666385402
	data : 0.11431598663330078
	model : 0.06477317810058594
			 train-loss:  2.2082694692611695 	 ± 0.22626412987702188
	data : 0.11386609077453613
	model : 0.0647733211517334
			 train-loss:  2.211569811616625 	 ± 0.22836522454246017
	data : 0.11391611099243164
	model : 0.06477065086364746
			 train-loss:  2.2106850081541407 	 ± 0.22768109948134413
	data : 0.11379485130310059
	model : 0.06485600471496582
			 train-loss:  2.2106813630089164 	 ± 0.2267899800191336
	data : 0.11390228271484375
	model : 0.06491713523864746
			 train-loss:  2.211967389712962 	 ± 0.22637729325691766
	data : 0.11400470733642579
	model : 0.0648989200592041
			 train-loss:  2.2109979290228625 	 ± 0.22577359201161581
	data : 0.11393189430236816
	model : 0.06482362747192383
			 train-loss:  2.208858557329833 	 ± 0.22622908960620253
	data : 0.11390118598937989
	model : 0.06483922004699708
			 train-loss:  2.20668798865694 	 ± 0.22673567427694888
	data : 0.11404223442077636
	model : 0.06480803489685058
			 train-loss:  2.204714093889509 	 ± 0.2270172637753652
	data : 0.11406254768371582
	model : 0.06482110023498536
			 train-loss:  2.2020579033823156 	 ± 0.22823364359446577
	data : 0.1141139030456543
	model : 0.06488471031188965
			 train-loss:  2.2024484166392573 	 ± 0.22743169278301517
	data : 0.11424188613891602
	model : 0.06498351097106933
			 train-loss:  2.1988383961074494 	 ± 0.23044347729173628
	data : 0.11423444747924805
	model : 0.06497101783752442
			 train-loss:  2.1992254405126084 	 ± 0.22964526473362085
	data : 0.11402192115783691
	model : 0.0648998737335205
			 train-loss:  2.202622027500816 	 ± 0.23223982831191003
	data : 0.11410760879516602
	model : 0.06483626365661621
			 train-loss:  2.2006679790483106 	 ± 0.2325386849106163
	data : 0.11394352912902832
	model : 0.06481332778930664
			 train-loss:  2.1998599810259685 	 ± 0.2319024429807599
	data : 0.11400880813598632
	model : 0.06480517387390136
			 train-loss:  2.200622836748759 	 ± 0.23125485157013342
	data : 0.11415352821350097
	model : 0.06482667922973633
			 train-loss:  2.199384480295047 	 ± 0.2309078221451925
	data : 0.11445560455322265
	model : 0.06488642692565919
			 train-loss:  2.1993086813213107 	 ± 0.23010080853058462
	data : 0.11425285339355469
	model : 0.06492099761962891
			 train-loss:  2.1999935052461095 	 ± 0.2294466464388527
	data : 0.11426992416381836
	model : 0.06494259834289551
			 train-loss:  2.200905144625697 	 ± 0.22891563023565079
	data : 0.11399827003479004
	model : 0.0648470401763916
			 train-loss:  2.1989199274206817 	 ± 0.22937938686513654
	data : 0.11387166976928711
	model : 0.06476922035217285
			 train-loss:  2.2028281112917427 	 ± 0.23342443783468395
	data : 0.11378188133239746
	model : 0.06472258567810059
			 train-loss:  2.2029435610448993 	 ± 0.23263871625104116
	data : 0.11387343406677246
	model : 0.06470704078674316
			 train-loss:  2.2014218120766964 	 ± 0.23259465210468075
	data : 0.11398372650146485
	model : 0.06470146179199218
			 train-loss:  2.2004513208071392 	 ± 0.23212052871037706
	data : 0.11414318084716797
	model : 0.06474380493164063
			 train-loss:  2.1999463727142636 	 ± 0.23143328399683388
	data : 0.11402912139892578
	model : 0.06474461555480956
			 train-loss:  2.2030346730822012 	 ± 0.23377160865721228
	data : 0.11400923728942872
	model : 0.06472592353820801
			 train-loss:  2.201991016568701 	 ± 0.23336139827461339
	data : 0.11400885581970215
	model : 0.06475481986999512
			 train-loss:  2.2014791663590962 	 ± 0.23268864683999588
	data : 0.11403207778930664
	model : 0.06474037170410156
			 train-loss:  2.2011178578099897 	 ± 0.23198015853458243
	data : 0.11408166885375977
	model : 0.06478967666625976
			 train-loss:  2.201608031988144 	 ± 0.2313159512186427
	data : 0.1141960620880127
	model : 0.06486687660217286
			 train-loss:  2.199131783406446 	 ± 0.23264312116984326
	data : 0.11415104866027832
	model : 0.06497583389282227
			 train-loss:  2.196860386600977 	 ± 0.2336456152888972
	data : 0.1140561580657959
	model : 0.06499862670898438
			 train-loss:  2.1947733103854103 	 ± 0.23438252634628673
	data : 0.11390476226806641
	model : 0.06498146057128906
			 train-loss:  2.1945462368428705 	 ± 0.23366647657070233
	data : 0.11383237838745117
	model : 0.06495757102966308
			 train-loss:  2.194536072126827 	 ± 0.2329397094341687
	data : 0.11375923156738281
	model : 0.06492300033569336
			 train-loss:  2.1951264091479925 	 ± 0.2323404237296473
	data : 0.1138270378112793
	model : 0.06485571861267089
			 train-loss:  2.1951004456888676 	 ± 0.23162686231173454
	data : 0.11398725509643555
	model : 0.06486339569091797
			 train-loss:  2.194352245185433 	 ± 0.2311170939826793
	data : 0.11410818099975586
	model : 0.06489033699035644
			 train-loss:  2.1947242093808725 	 ± 0.2304649079409511
	data : 0.11419911384582519
	model : 0.06487927436828614
			 train-loss:  2.193660606102771 	 ± 0.23017551142824433
	data : 0.11413731575012206
	model : 0.0648460865020752
			 train-loss:  2.1944299393785216 	 ± 0.2296992969024546
	data : 0.11400079727172852
	model : 0.06484179496765137
			 train-loss:  2.1952465041762306 	 ± 0.22925762974891697
	data : 0.11402673721313476
	model : 0.06479225158691407
			 train-loss:  2.198475949863005 	 ± 0.23237940120609255
	data : 0.11400036811828614
	model : 0.06477422714233398
			 train-loss:  2.1974416851997374 	 ± 0.23208472122239412
	data : 0.11400103569030762
	model : 0.06483044624328613
			 train-loss:  2.1953005379403545 	 ± 0.2330830208999354
	data : 0.11414885520935059
	model : 0.0649421215057373
			 train-loss:  2.1961983324483385 	 ± 0.23270081203866594
	data : 0.11421160697937012
	model : 0.06493439674377441
			 train-loss:  2.19705717756569 	 ± 0.23230052481978689
	data : 0.11403107643127441
	model : 0.06487960815429687
			 train-loss:  2.198143726792829 	 ± 0.23207248978545716
	data : 0.11410460472106934
	model : 0.06484770774841309
			 train-loss:  2.1961259276526315 	 ± 0.23293416644157494
	data : 0.11395430564880371
	model : 0.06486954689025878
			 train-loss:  2.1985469020225783 	 ± 0.2344690500060316
	data : 0.1140282154083252
	model : 0.06484718322753906
			 train-loss:  2.197692913524175 	 ± 0.23408010208917893
	data : 0.11404604911804199
	model : 0.06491761207580567
			 train-loss:  2.1978899113247903 	 ± 0.23343636095225764
	data : 0.11420416831970215
	model : 0.06499361991882324
			 train-loss:  2.197245145643224 	 ± 0.23294227976588464
	data : 0.11411795616149903
	model : 0.0650367259979248
			 train-loss:  2.194994592004352 	 ± 0.23423766364680376
	data : 0.11422262191772461
	model : 0.06500372886657715
			 train-loss:  2.1932826918133057 	 ± 0.23471612070252104
	data : 0.11405429840087891
	model : 0.06489958763122558
			 train-loss:  2.191330176133376 	 ± 0.23553977449202598
	data : 0.11418290138244629
	model : 0.06484074592590332
			 train-loss:  2.1914912911712148 	 ± 0.23490539772293842
	data : 0.11421422958374024
	model : 0.06488194465637206
			 train-loss:  2.1915791125401207 	 ± 0.23426921055225639
	data : 0.11427416801452636
	model : 0.06488127708435058
			 train-loss:  2.1913818075850204 	 ± 0.2336505214927732
	data : 0.11427521705627441
	model : 0.06487631797790527
			 train-loss:  2.190828300291492 	 ± 0.23314316692407225
	data : 0.11441798210144043
	model : 0.0649498462677002
			 train-loss:  2.1926136718076816 	 ± 0.2337903942994589
	data : 0.1143606185913086
	model : 0.06496157646179199
			 train-loss:  2.190765304134247 	 ± 0.23453377856881436
	data : 0.11423511505126953
	model : 0.06495862007141114
			 train-loss:  2.192738277571542 	 ± 0.23547158874432153
	data : 0.11411538124084472
	model : 0.06490731239318848
			 train-loss:  2.197151051696978 	 ± 0.24256001369768865
	data : 0.11409220695495606
	model : 0.06486425399780274
			 train-loss:  2.197973641425527 	 ± 0.2421897725821559
	data : 0.11405096054077149
	model : 0.06485219001770019
			 train-loss:  2.197686173642675 	 ± 0.24159091527443902
	data : 0.11399836540222168
	model : 0.06486825942993164
			 train-loss:  2.197758027926628 	 ± 0.24096627618005328
	data : 0.11405014991760254
	model : 0.06484732627868653
			 train-loss:  2.197661003501145 	 ± 0.24034820634234572
	data : 0.11414079666137696
	model : 0.06491580009460449
			 train-loss:  2.199367869817294 	 ± 0.24090706942724716
	data : 0.11416144371032715
	model : 0.06498475074768066
			 train-loss:  2.198803593309558 	 ± 0.2404208861813123
	data : 0.11399602890014648
	model : 0.06491808891296387
			 train-loss:  2.197552175086162 	 ± 0.24044902919414002
	data : 0.11383028030395508
	model : 0.06485128402709961
			 train-loss:  2.197492569383949 	 ± 0.23984252518054827
	data : 0.11367058753967285
	model : 0.06482491493225098
			 train-loss:  2.1984907514485883 	 ± 0.23965110080054305
	data : 0.11367669105529785
	model : 0.06479353904724121
			 train-loss:  2.1983930683135986 	 ± 0.23905519388064947
	data : 0.11365580558776855
	model : 0.06476540565490722
			 train-loss:  2.197284387711862 	 ± 0.238974695113941
	data : 0.11377220153808594
	model : 0.06484742164611816
			 train-loss:  2.19694725829776 	 ± 0.23843035137834198
	data : 0.11394901275634765
	model : 0.06491298675537109
			 train-loss:  2.196188047014434 	 ± 0.238087003260619
	data : 0.11410703659057617
	model : 0.0649505615234375
			 train-loss:  2.1958052586106693 	 ± 0.23756535181616653
	data : 0.11413555145263672
	model : 0.06494569778442383
			 train-loss:  2.197559548587334 	 ± 0.23830612517615926
	data : 0.11406364440917968
	model : 0.06487822532653809
			 train-loss:  2.196554255717009 	 ± 0.23816235309211004
	data : 0.11397790908813477
	model : 0.06490693092346192
			 train-loss:  2.1955807111113543 	 ± 0.23799692227361335
	data : 0.11396307945251465
	model : 0.06490440368652343
			 train-loss:  2.195738409001094 	 ± 0.23743496570030498
	data : 0.11398849487304688
	model : 0.0648918628692627
			 train-loss:  2.196730216154071 	 ± 0.23729776673122652
	data : 0.11394729614257812
	model : 0.06493358612060547
			 train-loss:  2.195958718231746 	 ± 0.23699469368114343
	data : 0.1139434814453125
	model : 0.06503481864929199
			 train-loss:  2.1964049943815476 	 ± 0.2365208596850764
	data : 0.11409406661987305
	model : 0.06497745513916016
			 train-loss:  2.1961451605805813 	 ± 0.23599255184422332
	data : 0.1139535903930664
	model : 0.06491236686706543
			 train-loss:  2.19547000009689 	 ± 0.2356430688527436
	data : 0.11382832527160644
	model : 0.06489048004150391
			 train-loss:  2.1939728510714023 	 ± 0.2361050842937078
	data : 0.11390976905822754
	model : 0.06494941711425781
			 train-loss:  2.1922239386758138 	 ± 0.2369406878438937
	data : 0.114044189453125
	model : 0.0649630069732666
			 train-loss:  2.1926287705147707 	 ± 0.23646609508775182
	data : 0.11397109031677247
	model : 0.06499977111816406
			 train-loss:  2.1921024503795783 	 ± 0.23604739037539904
	data : 0.11418013572692871
	model : 0.06507906913757325
			 train-loss:  2.1925157084377536 	 ± 0.23558404315622028
	data : 0.11434645652770996
	model : 0.06506061553955078
			 train-loss:  2.1926791608061422 	 ± 0.2350579539008891
	data : 0.11419010162353516
	model : 0.06493916511535644
			 train-loss:  2.1930068422447553 	 ± 0.23457325144045738
	data : 0.11415619850158691
	model : 0.06480951309204101
			 train-loss:  2.192670411653648 	 ± 0.2340951323151814
	data : 0.11414823532104493
	model : 0.0647613525390625
			 train-loss:  2.1942498259716205 	 ± 0.23474449342625614
	data : 0.1142730712890625
	model : 0.06485328674316407
			 train-loss:  2.193944449381978 	 ± 0.23426176011494898
	data : 0.11432747840881348
	model : 0.06481132507324219
			 train-loss:  2.1927618820752417 	 ± 0.23440442917748944
	data : 0.11449580192565918
	model : 0.0648108959197998
			 train-loss:  2.1911696026060317 	 ± 0.2350939253868465
	data : 0.11462736129760742
	model : 0.06472544670104981
			 train-loss:  2.1917649006421587 	 ± 0.2347431264601442
	data : 0.11478190422058106
	model : 0.06454052925109863
			 train-loss:  2.190984315809174 	 ± 0.23451927364291011
	data : 0.11470952033996581
	model : 0.06417970657348633
			 train-loss:  2.1910666033887027 	 ± 0.23400769608693167
	data : 0.11465997695922851
	model : 0.06397638320922852
			 train-loss:  2.189615935217345 	 ± 0.2345214037690266
	data : 0.11469388008117676
	model : 0.06381149291992187
			 train-loss:  2.1897533649983614 	 ± 0.23402026033142193
	data : 0.11468868255615235
	model : 0.06377811431884765
			 train-loss:  2.1890712377828954 	 ± 0.23374220985868943
	data : 0.1146759033203125
	model : 0.06380748748779297
			 train-loss:  2.1893892992159416 	 ± 0.2332880019141716
	data : 0.11470885276794433
	model : 0.0638547420501709
			 train-loss:  2.189601577402696 	 ± 0.23280929935876546
	data : 0.1148216724395752
	model : 0.06386256217956543
			 train-loss:  2.19079391620098 	 ± 0.23302316353115898
	data : 0.1146848201751709
	model : 0.06380128860473633
			 train-loss:  2.1919659903708926 	 ± 0.23321704724335124
	data : 0.114666748046875
	model : 0.06373367309570313
			 train-loss:  2.19146899298086 	 ± 0.2328470972949858
	data : 0.11470437049865723
	model : 0.06370582580566406
			 train-loss:  2.1913342400442195 	 ± 0.23236456082021414
	data : 0.11468820571899414
	model : 0.06374249458312989
			 train-loss:  2.1926197930544364 	 ± 0.232718937339216
	data : 0.1147803783416748
	model : 0.06377506256103516
			 train-loss:  2.19207720626847 	 ± 0.23238237487290686
	data : 0.11494102478027343
	model : 0.06381926536560059
			 train-loss:  2.1929624353845916 	 ± 0.23230120388871475
	data : 0.11491317749023437
	model : 0.06384472846984864
			 train-loss:  2.1923792980518577 	 ± 0.2319947084237292
	data : 0.11479272842407226
	model : 0.06380009651184082
			 train-loss:  2.193503496568065 	 ± 0.2321717528779355
	data : 0.11482977867126465
	model : 0.06372151374816895
			 train-loss:  2.1920330043682834 	 ± 0.23282006971789612
	data : 0.11470994949340821
	model : 0.06376686096191406
			 train-loss:  2.192966691783217 	 ± 0.2327979233334016
	data : 0.11471810340881347
	model : 0.06377859115600586
			 train-loss:  2.1938174685653373 	 ± 0.23270213127749306
	data : 0.1147763729095459
	model : 0.06380023956298828
			 train-loss:  2.193548770454841 	 ± 0.23226675930748658
	data : 0.11492552757263183
	model : 0.06380982398986816
			 train-loss:  2.1931353445477813 	 ± 0.23188678639039337
	data : 0.1147986888885498
	model : 0.06376972198486328
			 train-loss:  2.1933455842156566 	 ± 0.2314423876239212
	data : 0.1147791862487793
	model : 0.06368012428283691
			 train-loss:  2.192649549269772 	 ± 0.2312371147001729
	data : 0.11481180191040039
	model : 0.06372427940368652
			 train-loss:  2.1926294736862184 	 ± 0.23077439449868822
	data : 0.11479773521423339
	model : 0.0637317180633545
			 train-loss:  2.1937730302848664 	 ± 0.23102288416836658
	data : 0.11478943824768066
	model : 0.06376385688781738
			 train-loss:  2.1935332029584855 	 ± 0.23059535523693875
	data : 0.11490983963012695
	model : 0.06382808685302735
			 train-loss:  2.1926508232538877 	 ± 0.23056506404130223
	data : 0.11476325988769531
	model : 0.06388773918151855
			 train-loss:  2.1939802122867014 	 ± 0.23108023944979414
	data : 0.11471667289733886
	model : 0.06379365921020508
			 train-loss:  2.1936470237432744 	 ± 0.2306878205501599
	data : 0.11463413238525391
	model : 0.06380982398986816
			 train-loss:  2.1930656684562564 	 ± 0.23042390395069853
	data : 0.11435937881469727
	model : 0.05539894104003906
#epoch  83    val-loss:  2.41371718833321  train-loss:  2.1930656684562564  lr:  1.9073486328125e-08
			 train-loss:  2.2029306888580322 	 ± 0.0
	data : 5.357571363449097
	model : 0.07107758522033691
			 train-loss:  2.2125203609466553 	 ± 0.009589672088623047
	data : 2.7436349391937256
	model : 0.07071852684020996
			 train-loss:  2.2878169218699136 	 ± 0.10677289940609846
	data : 1.8677151997884114
	model : 0.06879814465840657
			 train-loss:  2.2047500610351562 	 ± 0.17102821153982614
	data : 1.4293639659881592
	model : 0.06775933504104614
			 train-loss:  2.2020572662353515 	 ± 0.15306705683897054
	data : 1.1662966728210449
	model : 0.06718306541442871
			 train-loss:  2.2167618672053018 	 ± 0.14354695987567226
	data : 0.11754255294799805
	model : 0.06594133377075195
			 train-loss:  2.2460219519478932 	 ± 0.15099327592470554
	data : 0.11442127227783203
	model : 0.0648200511932373
			 train-loss:  2.231293201446533 	 ± 0.1465184317483101
	data : 0.11414508819580078
	model : 0.06480870246887208
			 train-loss:  2.199514720175001 	 ± 0.16480695155055436
	data : 0.11404275894165039
	model : 0.06484942436218262
			 train-loss:  2.1947424530982973 	 ± 0.15700372272309548
	data : 0.11397686004638671
	model : 0.06476759910583496
			 train-loss:  2.1680817170576616 	 ± 0.17180568372814126
	data : 0.11381077766418457
	model : 0.0648038387298584
			 train-loss:  2.1850563287734985 	 ± 0.1738589622456202
	data : 0.11385588645935059
	model : 0.06479096412658691
			 train-loss:  2.203480977278489 	 ± 0.17881667061171283
	data : 0.11388950347900391
	model : 0.06485214233398437
			 train-loss:  2.1933992079326083 	 ± 0.1761044899372197
	data : 0.11389756202697754
	model : 0.0648500919342041
			 train-loss:  2.1920614719390867 	 ± 0.17020671396045312
	data : 0.11397385597229004
	model : 0.06490893363952636
			 train-loss:  2.1800502985715866 	 ± 0.1712416550371193
	data : 0.11424355506896973
	model : 0.0648725986480713
			 train-loss:  2.197178041233736 	 ± 0.1797012346358858
	data : 0.11419177055358887
	model : 0.06486144065856933
			 train-loss:  2.1847406360838146 	 ± 0.18201158719897378
	data : 0.11393084526062011
	model : 0.06474823951721191
			 train-loss:  2.245059327075356 	 ± 0.3112472138879747
	data : 0.11401410102844238
	model : 0.06474895477294922
			 train-loss:  2.249666488170624 	 ± 0.30403022790369477
	data : 0.11402521133422852
	model : 0.06477589607238769
			 train-loss:  2.2453348750159856 	 ± 0.2973348277809807
	data : 0.11384787559509277
	model : 0.06480097770690918
			 train-loss:  2.2411027388139204 	 ± 0.2911452985693836
	data : 0.11381196975708008
	model : 0.06485776901245117
			 train-loss:  2.2332900710727857 	 ± 0.2870939842038783
	data : 0.113946533203125
	model : 0.06491494178771973
			 train-loss:  2.2381337881088257 	 ± 0.28200759319768537
	data : 0.11385078430175781
	model : 0.06490778923034668
			 train-loss:  2.2449314117431642 	 ± 0.27830942457287344
	data : 0.11372623443603516
	model : 0.06480894088745118
			 train-loss:  2.242122668486375 	 ± 0.2732659503611089
	data : 0.11373786926269532
	model : 0.06474909782409669
			 train-loss:  2.244194993266353 	 ± 0.2683658386351477
	data : 0.11374883651733399
	model : 0.06474442481994629
			 train-loss:  2.253170967102051 	 ± 0.26762550585216016
	data : 0.11380891799926758
	model : 0.06471190452575684
			 train-loss:  2.243738162106481 	 ± 0.26766586868461895
	data : 0.11391148567199708
	model : 0.06472768783569335
			 train-loss:  2.2473918159802753 	 ± 0.26390145211610827
	data : 0.11398878097534179
	model : 0.06477942466735839
			 train-loss:  2.241122334234176 	 ± 0.26187132066174146
	data : 0.11390552520751954
	model : 0.06477370262145996
			 train-loss:  2.228799629956484 	 ± 0.26672249709017537
	data : 0.11389374732971191
	model : 0.06474871635437011
			 train-loss:  2.2300289580316255 	 ± 0.2627422041000047
	data : 0.11385135650634766
	model : 0.06480355262756347
			 train-loss:  2.228394469794105 	 ± 0.259019750156798
	data : 0.11373848915100097
	model : 0.06485414505004883
			 train-loss:  2.2254268203462875 	 ± 0.2558784374036999
	data : 0.11372857093811035
	model : 0.06493639945983887
			 train-loss:  2.2222713794973163 	 ± 0.2529892231040396
	data : 0.11388540267944336
	model : 0.06498131752014161
			 train-loss:  2.2307333462947123 	 ± 0.2546595711276056
	data : 0.11405739784240723
	model : 0.06499862670898438
			 train-loss:  2.221509171159644 	 ± 0.25747434605515196
	data : 0.11403827667236328
	model : 0.0649827003479004
			 train-loss:  2.2191631824542313 	 ± 0.25456306990822264
	data : 0.11404581069946289
	model : 0.06487526893615722
			 train-loss:  2.2289325445890427 	 ± 0.2586589930224608
	data : 0.11409344673156738
	model : 0.06482977867126465
			 train-loss:  2.221686246918469 	 ± 0.2595631164345506
	data : 0.11411280632019043
	model : 0.06485223770141602
			 train-loss:  2.2201765208017257 	 ± 0.2566365957627719
	data : 0.11396865844726563
	model : 0.0648608684539795
			 train-loss:  2.2159077400384946 	 ± 0.25513918643081995
	data : 0.11395750045776368
	model : 0.06484603881835938
			 train-loss:  2.213249824263833 	 ± 0.2528246902605333
	data : 0.11410942077636718
	model : 0.06487975120544434
			 train-loss:  2.20675421555837 	 ± 0.25368555612956945
	data : 0.11402606964111328
	model : 0.06492395401000976
			 train-loss:  2.20249712208043 	 ± 0.25253284553464905
	data : 0.11405220031738281
	model : 0.06488747596740722
			 train-loss:  2.1994467618617604 	 ± 0.25068702651307373
	data : 0.11400213241577148
	model : 0.06484532356262207
			 train-loss:  2.1964999263485274 	 ± 0.24888325782881696
	data : 0.11390571594238282
	model : 0.0648618221282959
			 train-loss:  2.1975947521170793 	 ± 0.24644729778725738
	data : 0.11380181312561036
	model : 0.06487383842468261
			 train-loss:  2.1990910840034483 	 ± 0.24419511997888577
	data : 0.11388397216796875
	model : 0.06483831405639648
			 train-loss:  2.202339296247445 	 ± 0.2428776655740355
	data : 0.11385931968688964
	model : 0.06484642028808593
			 train-loss:  2.20020942733838 	 ± 0.24101140892291545
	data : 0.11395416259765626
	model : 0.06485815048217773
			 train-loss:  2.1988535399706857 	 ± 0.23892703056834363
	data : 0.11406230926513672
	model : 0.06485280990600586
			 train-loss:  2.2001306856120073 	 ± 0.2368869430938119
	data : 0.11395635604858398
	model : 0.06477599143981934
			 train-loss:  2.1956147822466763 	 ± 0.2370577694751273
	data : 0.11386728286743164
	model : 0.06479215621948242
			 train-loss:  2.190298710550581 	 ± 0.23821673572554353
	data : 0.11393771171569825
	model : 0.06481246948242188
			 train-loss:  2.1886765287633527 	 ± 0.23642971591209708
	data : 0.11396441459655762
	model : 0.06484699249267578
			 train-loss:  2.1863046884536743 	 ± 0.23506572534068404
	data : 0.11393442153930664
	model : 0.06483044624328613
			 train-loss:  2.1845890627068987 	 ± 0.2334310823168118
	data : 0.11401867866516113
	model : 0.06491985321044921
			 train-loss:  2.18652001619339 	 ± 0.23195234134563952
	data : 0.11410555839538575
	model : 0.06487445831298828
			 train-loss:  2.1922685240135817 	 ± 0.23431306197173163
	data : 0.1140902042388916
	model : 0.06477947235107422
			 train-loss:  2.190765896151143 	 ± 0.2327118740988495
	data : 0.11409401893615723
	model : 0.06472859382629395
			 train-loss:  2.1885275235251775 	 ± 0.23152938351732924
	data : 0.11405882835388184
	model : 0.06469650268554687
			 train-loss:  2.183371303603053 	 ± 0.23333070947812581
	data : 0.11411008834838868
	model : 0.06467771530151367
			 train-loss:  2.1815719732871424 	 ± 0.23197594211313377
	data : 0.11409363746643067
	model : 0.06470627784729004
			 train-loss:  2.181110506707972 	 ± 0.23024190221257795
	data : 0.11406593322753907
	model : 0.06477227210998535
			 train-loss:  2.1826580442599397 	 ± 0.2288627999139432
	data : 0.11402349472045899
	model : 0.0648104190826416
			 train-loss:  2.1826755457064686 	 ± 0.22717379764451776
	data : 0.11400413513183594
	model : 0.06481099128723145
			 train-loss:  2.1813722503358037 	 ± 0.2257775378970922
	data : 0.11387262344360352
	model : 0.06477737426757812
			 train-loss:  2.181318168980735 	 ± 0.22415949020578615
	data : 0.11383352279663086
	model : 0.0647355079650879
			 train-loss:  2.178900636417765 	 ± 0.22349245750913535
	data : 0.11380419731140137
	model : 0.06476068496704102
			 train-loss:  2.182775826917754 	 ± 0.2243242259301828
	data : 0.11387977600097657
	model : 0.06473922729492188
			 train-loss:  2.1807377322079384 	 ± 0.22345268050892003
	data : 0.11399655342102051
	model : 0.06477231979370117
			 train-loss:  2.1792819354985213 	 ± 0.22228600482841854
	data : 0.11417784690856933
	model : 0.0648228645324707
			 train-loss:  2.1788077974319457 	 ± 0.220836793615562
	data : 0.11430215835571289
	model : 0.06485419273376465
			 train-loss:  2.1760527193546295 	 ± 0.2206727840989076
	data : 0.11425704956054687
	model : 0.06484799385070801
			 train-loss:  2.1699521371296475 	 ± 0.22559378475297467
	data : 0.11405525207519532
	model : 0.0648167610168457
			 train-loss:  2.169893688116318 	 ± 0.2241435926146818
	data : 0.11396617889404297
	model : 0.06480140686035156
			 train-loss:  2.173785703091682 	 ± 0.2253573233682287
	data : 0.11380701065063477
	model : 0.06480131149291993
			 train-loss:  2.175802905857563 	 ± 0.22466098595137046
	data : 0.11370019912719727
	model : 0.06478791236877442
			 train-loss:  2.1798762289094338 	 ± 0.2262228933064022
	data : 0.11388640403747559
	model : 0.06479043960571289
			 train-loss:  2.184231119911845 	 ± 0.2282298431339462
	data : 0.11405220031738281
	model : 0.06485610008239746
			 train-loss:  2.1823183384286353 	 ± 0.22751110030286367
	data : 0.11410446166992187
	model : 0.06487131118774414
			 train-loss:  2.1794350345929465 	 ± 0.22767325359672252
	data : 0.11419310569763183
	model : 0.06484436988830566
			 train-loss:  2.1784339455997244 	 ± 0.226515934595164
	data : 0.11426892280578613
	model : 0.06485061645507813
			 train-loss:  2.1796744701474213 	 ± 0.22548537312952072
	data : 0.11406965255737304
	model : 0.06481218338012695
			 train-loss:  2.1807843405624916 	 ± 0.2244218784926217
	data : 0.11399497985839843
	model : 0.06482000350952148
			 train-loss:  2.185656287453391 	 ± 0.2277232275078222
	data : 0.11399779319763184
	model : 0.06481370925903321
			 train-loss:  2.186251029539644 	 ± 0.22650899069496996
	data : 0.11401090621948243
	model : 0.0648420810699463
			 train-loss:  2.1844843175676134 	 ± 0.22586289147996297
	data : 0.11397342681884766
	model : 0.06485471725463868
			 train-loss:  2.1952353493197934 	 ± 0.2466902376962831
	data : 0.11435375213623047
	model : 0.06480779647827148
			 train-loss:  2.194113360798877 	 ± 0.24557921413703912
	data : 0.11411032676696778
	model : 0.06474647521972657
			 train-loss:  2.1925380896496516 	 ± 0.24472221193350926
	data : 0.11388068199157715
	model : 0.06483073234558105
			 train-loss:  2.1927550244838634 	 ± 0.2434260073498205
	data : 0.11401362419128418
	model : 0.06483345031738282
			 train-loss:  2.1938234379417016 	 ± 0.24236289535891084
	data : 0.11402316093444824
	model : 0.06486067771911622
			 train-loss:  2.194181983669599 	 ± 0.24112261017904366
	data : 0.11371002197265626
	model : 0.06492218971252442
			 train-loss:  2.1942639055940294 	 ± 0.23987783308432717
	data : 0.1139364242553711
	model : 0.06496829986572265
			 train-loss:  2.1932966782122243 	 ± 0.23884087634626466
	data : 0.11414999961853027
	model : 0.06480059623718262
			 train-loss:  2.1918622291449346 	 ± 0.23805545903954267
	data : 0.11405415534973144
	model : 0.06483440399169922
			 train-loss:  2.197145974636078 	 ± 0.2426264129023152
	data : 0.11413102149963379
	model : 0.06481661796569824
			 train-loss:  2.195454488886465 	 ± 0.24201413461299223
	data : 0.11420202255249023
	model : 0.06487598419189453
			 train-loss:  2.19252515891019 	 ± 0.24261759170090552
	data : 0.1144139289855957
	model : 0.06492137908935547
			 train-loss:  2.1943594518217067 	 ± 0.24214664937487201
	data : 0.11447386741638184
	model : 0.06506094932556153
			 train-loss:  2.1917073910052958 	 ± 0.24247813234508336
	data : 0.11451692581176758
	model : 0.06503882408142089
			 train-loss:  2.1940744468144007 	 ± 0.24252503639437054
	data : 0.11474785804748536
	model : 0.06500301361083985
			 train-loss:  2.1953493176766163 	 ± 0.24173158425323008
	data : 0.11458048820495606
	model : 0.06492509841918945
			 train-loss:  2.1925097236009403 	 ± 0.24236904589951597
	data : 0.11427054405212403
	model : 0.06484689712524414
			 train-loss:  2.19507505385964 	 ± 0.2426993992298452
	data : 0.11426663398742676
	model : 0.06480960845947266
			 train-loss:  2.194064576691444 	 ± 0.2418116598295492
	data : 0.11416730880737305
	model : 0.06484513282775879
			 train-loss:  2.192773250016299 	 ± 0.24108726093103747
	data : 0.11395430564880371
	model : 0.06489500999450684
			 train-loss:  2.1957677087268315 	 ± 0.24204500051052993
	data : 0.11419286727905273
	model : 0.06496386528015137
			 train-loss:  2.1937106900981496 	 ± 0.24193464422510375
	data : 0.11462988853454589
	model : 0.06505160331726074
			 train-loss:  2.194216725045601 	 ± 0.2409212868883786
	data : 0.11461095809936524
	model : 0.06507964134216308
			 train-loss:  2.1953902171369184 	 ± 0.24018644263740313
	data : 0.11463065147399902
	model : 0.06503477096557617
			 train-loss:  2.196638520904209 	 ± 0.23951100405962145
	data : 0.11450514793395997
	model : 0.06499414443969727
			 train-loss:  2.1987321510397155 	 ± 0.2395309348147505
	data : 0.11484827995300292
	model : 0.06491675376892089
			 train-loss:  2.2004208371170564 	 ± 0.23919756722810873
	data : 0.11443428993225098
	model : 0.06483640670776367
			 train-loss:  2.200545398865716 	 ± 0.23818567249057773
	data : 0.11446142196655273
	model : 0.06482667922973633
			 train-loss:  2.2040211964054266 	 ± 0.24016921011549286
	data : 0.11454958915710449
	model : 0.06484804153442383
			 train-loss:  2.204614946246147 	 ± 0.23925410035016442
	data : 0.11459765434265137
	model : 0.06488490104675293
			 train-loss:  2.2024907051039135 	 ± 0.23939702143270167
	data : 0.11419248580932617
	model : 0.06497015953063964
			 train-loss:  2.2021407680433303 	 ± 0.23844493973938802
	data : 0.11452741622924804
	model : 0.06501483917236328
			 train-loss:  2.2015663521076605 	 ± 0.23755841348891182
	data : 0.11449828147888183
	model : 0.06499009132385254
			 train-loss:  2.202820271253586 	 ± 0.2370069227769843
	data : 0.1143575668334961
	model : 0.06492481231689454
			 train-loss:  2.2035996503829955 	 ± 0.23621647855146405
	data : 0.11437749862670898
	model : 0.06488122940063476
			 train-loss:  2.2016742484910146 	 ± 0.23625998019443228
	data : 0.11434111595153809
	model : 0.06484308242797851
			 train-loss:  2.2013734635405653 	 ± 0.23535220356404746
	data : 0.11422481536865234
	model : 0.06482977867126465
			 train-loss:  2.1992452889680862 	 ± 0.23565466115432845
	data : 0.11426262855529785
	model : 0.064886474609375
			 train-loss:  2.1976349418477494 	 ± 0.23544545421651064
	data : 0.11438698768615722
	model : 0.06498837471008301
			 train-loss:  2.1980360517135034 	 ± 0.23458238830154723
	data : 0.11442050933837891
	model : 0.06504230499267578
			 train-loss:  2.2008598223897335 	 ± 0.23589278813506173
	data : 0.11443285942077637
	model : 0.06503329277038575
			 train-loss:  2.2011987070242562 	 ± 0.2350295636360793
	data : 0.11423001289367676
	model : 0.06501374244689942
			 train-loss:  2.200883262139514 	 ± 0.23417237339666233
	data : 0.11417922973632813
	model : 0.06493844985961914
			 train-loss:  2.199199268177374 	 ± 0.23410390320898206
	data : 0.11406540870666504
	model : 0.06487975120544434
			 train-loss:  2.2020311488045587 	 ± 0.23552769823680475
	data : 0.11402182579040528
	model : 0.06485795974731445
			 train-loss:  2.2007193381295487 	 ± 0.23515467064901588
	data : 0.11397280693054199
	model : 0.06490497589111328
			 train-loss:  2.2032226689540555 	 ± 0.2361066551607837
	data : 0.11417021751403808
	model : 0.06496267318725586
			 train-loss:  2.204759092434593 	 ± 0.23593599839490828
	data : 0.11425285339355469
	model : 0.06500029563903809
			 train-loss:  2.2077392322554004 	 ± 0.23767821144008594
	data : 0.11429238319396973
	model : 0.06499433517456055
			 train-loss:  2.208754472221647 	 ± 0.23713012151372528
	data : 0.11413793563842774
	model : 0.06494326591491699
			 train-loss:  2.210927798392925 	 ± 0.23768290600647154
	data : 0.11409454345703125
	model : 0.06489005088806152
			 train-loss:  2.212870652406988 	 ± 0.23796544864423383
	data : 0.1139686107635498
	model : 0.06485238075256347
			 train-loss:  2.212923505923131 	 ± 0.23713277828733556
	data : 0.11405625343322753
	model : 0.06485414505004883
			 train-loss:  2.2123295557167797 	 ± 0.23641468216502012
	data : 0.1141474723815918
	model : 0.06487245559692383
			 train-loss:  2.215436033544869 	 ± 0.23852897098247702
	data : 0.11430897712707519
	model : 0.0649369239807129
			 train-loss:  2.2151307512636054 	 ± 0.23773911014537782
	data : 0.11443142890930176
	model : 0.06497106552124024
			 train-loss:  2.2146833178137437 	 ± 0.23699076817378692
	data : 0.11450028419494629
	model : 0.0650141716003418
			 train-loss:  2.2161827047128937 	 ± 0.2368873427883635
	data : 0.11424922943115234
	model : 0.06496806144714355
			 train-loss:  2.2166274721990495 	 ± 0.23615307620502657
	data : 0.1140596866607666
	model : 0.06485896110534668
			 train-loss:  2.2195169885953265 	 ± 0.23799271699796185
	data : 0.11397972106933593
	model : 0.0648317813873291
			 train-loss:  2.22052901391162 	 ± 0.237526966385643
	data : 0.11391115188598633
	model : 0.06482281684875488
			 train-loss:  2.220367561045446 	 ± 0.23675265121399275
	data : 0.11390533447265624
	model : 0.06482992172241211
			 train-loss:  2.219544185532464 	 ± 0.23619592309337942
	data : 0.11402812004089355
	model : 0.06487803459167481
			 train-loss:  2.2184677441398817 	 ± 0.23580402052640687
	data : 0.11413493156433105
	model : 0.06497383117675781
			 train-loss:  2.219806170463562 	 ± 0.2356282599756598
	data : 0.11406793594360351
	model : 0.06494688987731934
			 train-loss:  2.219127590075517 	 ± 0.2350237183399445
	data : 0.11404566764831543
	model : 0.06491584777832031
			 train-loss:  2.2184850159724046 	 ± 0.23441147201664997
	data : 0.1141697883605957
	model : 0.06488862037658691
			 train-loss:  2.2162809960449796 	 ± 0.23529475321782253
	data : 0.11427502632141114
	model : 0.0648913860321045
			 train-loss:  2.2161880844044237 	 ± 0.23455657305824681
	data : 0.11429376602172851
	model : 0.06491966247558593
			 train-loss:  2.2155221119523048 	 ± 0.23397318355392455
	data : 0.11443724632263183
	model : 0.0649489402770996
			 train-loss:  2.2133856478685177 	 ± 0.23480576184355317
	data : 0.11447033882141114
	model : 0.06501235961914062
			 train-loss:  2.2116482500676757 	 ± 0.23511571780896093
	data : 0.11431326866149902
	model : 0.06496081352233887
			 train-loss:  2.2109744760887753 	 ± 0.23455022211785387
	data : 0.11418967247009278
	model : 0.06490931510925294
			 train-loss:  2.2139992866574265 	 ± 0.23700152262766677
	data : 0.11408114433288574
	model : 0.06487417221069336
			 train-loss:  2.212211025844921 	 ± 0.23738944996820582
	data : 0.11426382064819336
	model : 0.06493144035339356
			 train-loss:  2.2112203182944334 	 ± 0.23701522792451146
	data : 0.11435604095458984
	model : 0.06498804092407226
			 train-loss:  2.209993651527131 	 ± 0.23683246359612373
	data : 0.11450152397155762
	model : 0.06507740020751954
			 train-loss:  2.20941132803758 	 ± 0.23624643671425005
	data : 0.11456818580627441
	model : 0.06513419151306152
			 train-loss:  2.2082088536764743 	 ± 0.2360615317849679
	data : 0.11464815139770508
	model : 0.06514496803283691
			 train-loss:  2.205908947832444 	 ± 0.23725764308118336
	data : 0.11448702812194825
	model : 0.06508383750915528
			 train-loss:  2.205305646037498 	 ± 0.23669363400603638
	data : 0.11448955535888672
	model : 0.06499462127685547
			 train-loss:  2.2060018362000933 	 ± 0.23618009351857824
	data : 0.11418218612670898
	model : 0.06493382453918457
			 train-loss:  2.20573007853734 	 ± 0.23552347105991245
	data : 0.11416220664978027
	model : 0.06493129730224609
			 train-loss:  2.206137233767016 	 ± 0.23490675587383347
	data : 0.11422505378723144
	model : 0.0649953842163086
			 train-loss:  2.206891931806292 	 ± 0.23444608747577997
	data : 0.11418046951293945
	model : 0.06503796577453613
			 train-loss:  2.2060780078172684 	 ± 0.2340269207249924
	data : 0.11415019035339355
	model : 0.06506986618041992
			 train-loss:  2.2044793259626054 	 ± 0.2343266746101463
	data : 0.11433978080749511
	model : 0.06512336730957032
			 train-loss:  2.2059784236918674 	 ± 0.23451712943372477
	data : 0.11427359580993653
	model : 0.06505856513977051
			 train-loss:  2.2064410008531707 	 ± 0.23394255498471137
	data : 0.1140744686126709
	model : 0.0649254322052002
			 train-loss:  2.205709164010154 	 ± 0.23349719080961162
	data : 0.11406197547912597
	model : 0.06491889953613281
			 train-loss:  2.20491167260797 	 ± 0.23309696720460787
	data : 0.11397266387939453
	model : 0.0649139404296875
			 train-loss:  2.204644664958283 	 ± 0.23248346313071097
	data : 0.11399869918823242
	model : 0.06488752365112305
			 train-loss:  2.2052370453141426 	 ± 0.23198508516682548
	data : 0.1140169620513916
	model : 0.06493730545043945
			 train-loss:  2.204433020690213 	 ± 0.23160936290419096
	data : 0.11422443389892578
	model : 0.0650324821472168
			 train-loss:  2.2026610973719003 	 ± 0.2322297190990462
	data : 0.1142237663269043
	model : 0.06503949165344239
			 train-loss:  2.2023321717016158 	 ± 0.23164781086854394
	data : 0.11415591239929199
	model : 0.06491270065307617
			 train-loss:  2.200113921241964 	 ± 0.23299998014991513
	data : 0.11408824920654297
	model : 0.06488008499145508
			 train-loss:  2.200825509872842 	 ± 0.23258312178542154
	data : 0.11421022415161133
	model : 0.06488051414489746
			 train-loss:  2.202140524273827 	 ± 0.2326667004807851
	data : 0.11422643661499024
	model : 0.0648726463317871
			 train-loss:  2.20073798706657 	 ± 0.23285330729167286
	data : 0.11430716514587402
	model : 0.06485719680786133
			 train-loss:  2.1989590909468566 	 ± 0.23353379727204096
	data : 0.11454071998596191
	model : 0.06493515968322754
			 train-loss:  2.19785864589115 	 ± 0.2334208200989861
	data : 0.1146317481994629
	model : 0.06495122909545899
			 train-loss:  2.1967042530138876 	 ± 0.23336416971972138
	data : 0.11453361511230468
	model : 0.0649033546447754
			 train-loss:  2.1963041492344177 	 ± 0.23282829743529734
	data : 0.11443195343017579
	model : 0.06482253074645997
			 train-loss:  2.194449698008024 	 ± 0.2336625452793779
	data : 0.11441640853881836
	model : 0.06480011940002442
			 train-loss:  2.1950467782361165 	 ± 0.2332147965577935
	data : 0.11434736251831054
	model : 0.0648383617401123
			 train-loss:  2.193348701835284 	 ± 0.2338337297320657
	data : 0.11430339813232422
	model : 0.06489048004150391
			 train-loss:  2.1940958933396773 	 ± 0.23347814611244533
	data : 0.11442050933837891
	model : 0.0649479866027832
			 train-loss:  2.1941604302756152 	 ± 0.23289254928478725
	data : 0.11442646980285645
	model : 0.06501884460449218
			 train-loss:  2.193768799304962 	 ± 0.23237527063657823
	data : 0.11446337699890137
	model : 0.06508703231811523
			 train-loss:  2.1962573682490865 	 ± 0.23445300891606358
	data : 0.11445999145507812
	model : 0.06504497528076172
			 train-loss:  2.1951521139333745 	 ± 0.2343963150928614
	data : 0.11443471908569336
	model : 0.06492853164672852
			 train-loss:  2.194495137689149 	 ± 0.23400463859862747
	data : 0.11427607536315917
	model : 0.0649019718170166
			 train-loss:  2.194290175157435 	 ± 0.23344865910124968
	data : 0.11428723335266114
	model : 0.06495122909545899
			 train-loss:  2.1939538350919396 	 ± 0.23292811898041288
	data : 0.1141739845275879
	model : 0.06490449905395508
			 train-loss:  2.197267057826218 	 ± 0.23715503064067384
	data : 0.11420001983642578
	model : 0.06493620872497559
			 train-loss:  2.199208646580793 	 ± 0.23821708082758034
	data : 0.11424379348754883
	model : 0.06501660346984864
			 train-loss:  2.1998994327508488 	 ± 0.2378514895318747
	data : 0.11432151794433594
	model : 0.0650177001953125
			 train-loss:  2.1995604962253115 	 ± 0.2373321299262733
	data : 0.11438093185424805
	model : 0.06496729850769042
			 train-loss:  2.2012977497918267 	 ± 0.23809470955326884
	data : 0.11436586380004883
	model : 0.06490817070007324
			 train-loss:  2.2019198731788525 	 ± 0.23770086225694298
	data : 0.11429123878479004
	model : 0.06486821174621582
			 train-loss:  2.2014932272569188 	 ± 0.23722055161381467
	data : 0.11427149772644044
	model : 0.06486926078796387
			 train-loss:  2.201014607165341 	 ± 0.23676562084465896
	data : 0.11428470611572265
	model : 0.06490592956542969
			 train-loss:  2.198502449231727 	 ± 0.23904023890861167
	data : 0.11433024406433105
	model : 0.06497917175292969
			 train-loss:  2.1980065589727356 	 ± 0.23859398862644912
	data : 0.11442041397094727
	model : 0.06506223678588867
			 train-loss:  2.1989948087268405 	 ± 0.23848169141153971
	data : 0.1143653392791748
	model : 0.06509289741516114
			 train-loss:  2.1985910092630694 	 ± 0.23800556057218042
	data : 0.11439857482910157
	model : 0.06505160331726074
			 train-loss:  2.198073063421687 	 ± 0.23758159438905876
	data : 0.11429104804992676
	model : 0.06493163108825684
			 train-loss:  2.198072763338481 	 ± 0.23703855007605468
	data : 0.11425194740295411
	model : 0.06480274200439454
			 train-loss:  2.197041642665863 	 ± 0.23699097129729849
	data : 0.11427860260009766
	model : 0.06521358489990234
			 train-loss:  2.197518264546114 	 ± 0.2365598413117444
	data : 0.11396780014038085
	model : 0.06530251502990722
			 train-loss:  2.197987464097169 	 ± 0.23612949135980088
	data : 0.1138840675354004
	model : 0.06525368690490722
			 train-loss:  2.198845490211863 	 ± 0.23594605960545553
	data : 0.1139531135559082
	model : 0.06529083251953124
			 train-loss:  2.199663132429123 	 ± 0.23573522856100784
	data : 0.11391024589538574
	model : 0.06521658897399903
			 train-loss:  2.19962746726142 	 ± 0.23521139482588577
	data : 0.1138681411743164
	model : 0.06456995010375977
			 train-loss:  2.199169572475737 	 ± 0.2347909224023319
	data : 0.11438198089599609
	model : 0.06426324844360351
			 train-loss:  2.1988009041101395 	 ± 0.23433874007897407
	data : 0.11456103324890136
	model : 0.0641369342803955
			 train-loss:  2.199301963312584 	 ± 0.23394610936889404
	data : 0.11474103927612304
	model : 0.06399297714233398
			 train-loss:  2.198565811048949 	 ± 0.23369925296375624
	data : 0.11489982604980468
	model : 0.06396474838256835
			 train-loss:  2.1988921020341956 	 ± 0.23324292808571692
	data : 0.11504964828491211
	model : 0.06393566131591796
			 train-loss:  2.1993087741719695 	 ± 0.23282329666934637
	data : 0.1150022029876709
	model : 0.0639002799987793
			 train-loss:  2.199582441099759 	 ± 0.23235821133693227
	data : 0.11487998962402343
	model : 0.06377496719360351
			 train-loss:  2.1987043665202393 	 ± 0.232244473901178
	data : 0.11481924057006836
	model : 0.06381745338439941
			 train-loss:  2.198138507003458 	 ± 0.23190860147641437
	data : 0.11491851806640625
	model : 0.06377644538879394
			 train-loss:  2.1973074568078874 	 ± 0.23176356996914535
	data : 0.11489171981811523
	model : 0.06385231018066406
			 train-loss:  2.1967769305584794 	 ± 0.23141497754178386
	data : 0.11500382423400879
	model : 0.06392617225646972
			 train-loss:  2.1952093774256323 	 ± 0.23217845474611343
	data : 0.11511011123657226
	model : 0.06407103538513184
			 train-loss:  2.19540203619404 	 ± 0.2317091546880599
	data : 0.11501717567443848
	model : 0.06397700309753418
			 train-loss:  2.195425975272845 	 ± 0.23122419431793276
	data : 0.11487312316894531
	model : 0.06392183303833007
			 train-loss:  2.194253387550513 	 ± 0.23145296425946751
	data : 0.11489062309265137
	model : 0.06387300491333008
			 train-loss:  2.1950202181131515 	 ± 0.2312775767861789
	data : 0.1148061752319336
	model : 0.0638723373413086
			 train-loss:  2.1957756591237283 	 ± 0.23109700094638877
	data : 0.11486773490905762
	model : 0.06382150650024414
			 train-loss:  2.1959300742718417 	 ± 0.23063351241902663
	data : 0.11500802040100097
	model : 0.06385383605957032
			 train-loss:  2.196066904751981 	 ± 0.23017030083731713
	data : 0.11493482589721679
	model : 0.06383042335510254
			 train-loss:  2.19540555574456 	 ± 0.22993227369304323
	data : 0.11484918594360352
	model : 0.0638401985168457
			 train-loss:  2.1944370279467202 	 ± 0.2299646880558174
	data : 0.11491856575012208
	model : 0.06381344795227051
			 train-loss:  2.1963172161627393 	 ± 0.23138558721582883
	data : 0.11485490798950196
	model : 0.06380958557128906
			 train-loss:  2.1984153553362815 	 ± 0.2332611115710725
	data : 0.11480274200439453
	model : 0.06380071640014648
			 train-loss:  2.1978356656300493 	 ± 0.232971172201717
	data : 0.11490879058837891
	model : 0.0639042854309082
			 train-loss:  2.196221836566925 	 ± 0.23389521462769436
	data : 0.11501331329345703
	model : 0.06392025947570801
			 train-loss:  2.1946226695619258 	 ± 0.2347942698230683
	data : 0.11494054794311523
	model : 0.06396327018737794
			 train-loss:  2.1936987851347243 	 ± 0.2347846460538762
	data : 0.11486601829528809
	model : 0.06392273902893067
			 train-loss:  2.1944173301161514 	 ± 0.23459765260532867
	data : 0.1148590087890625
	model : 0.06393265724182129
			 train-loss:  2.1937534945217645 	 ± 0.2343733616881816
	data : 0.11488461494445801
	model : 0.06388025283813477
			 train-loss:  2.194840319483888 	 ± 0.23455378777723623
	data : 0.11489171981811523
	model : 0.06386151313781738
			 train-loss:  2.1974011952988803 	 ± 0.23764024659987326
	data : 0.11470351219177247
	model : 0.05543365478515625
#epoch  84    val-loss:  2.4240251716814543  train-loss:  2.1974011952988803  lr:  1.9073486328125e-08
			 train-loss:  1.8494296073913574 	 ± 0.0
	data : 5.807775020599365
	model : 0.0768282413482666
			 train-loss:  1.99627685546875 	 ± 0.14684724807739258
	data : 2.9675322771072388
	model : 0.07143330574035645
			 train-loss:  2.097891648610433 	 ± 0.18715557311781078
	data : 2.0165509382883706
	model : 0.06920131047566731
			 train-loss:  2.0866950154304504 	 ± 0.16323755755184963
	data : 1.5409266352653503
	model : 0.06807249784469604
			 train-loss:  2.0644510984420776 	 ± 0.15263147620440948
	data : 1.2556305885314942
	model : 0.06742124557495117
			 train-loss:  2.0401594042778015 	 ± 0.14954621905551913
	data : 0.11700434684753418
	model : 0.06501154899597168
			 train-loss:  2.0649135623659407 	 ± 0.15114831892369654
	data : 0.11440396308898926
	model : 0.06478676795959473
			 train-loss:  2.0410632640123367 	 ± 0.15482875910233376
	data : 0.11437692642211914
	model : 0.06479105949401856
			 train-loss:  2.044050097465515 	 ± 0.1462182089644309
	data : 0.11430773735046387
	model : 0.06478714942932129
			 train-loss:  2.0352298259735107 	 ± 0.14121601492827704
	data : 0.11427264213562012
	model : 0.06482992172241211
			 train-loss:  2.031147740103982 	 ± 0.13526156662935995
	data : 0.1140432357788086
	model : 0.06486635208129883
			 train-loss:  2.0250991384188333 	 ± 0.1310476701900664
	data : 0.11404290199279785
	model : 0.06488394737243652
			 train-loss:  2.0489683151245117 	 ± 0.1506296955648849
	data : 0.11400890350341797
	model : 0.0654597282409668
			 train-loss:  2.0626931190490723 	 ± 0.15335401424745432
	data : 0.11361432075500488
	model : 0.06552071571350097
			 train-loss:  2.0657090187072753 	 ± 0.14858318394274891
	data : 0.1135218620300293
	model : 0.06553006172180176
			 train-loss:  2.0663504600524902 	 ± 0.14388649726831468
	data : 0.11363816261291504
	model : 0.06550207138061523
			 train-loss:  2.0602188601213345 	 ± 0.1417287048755681
	data : 0.11342167854309082
	model : 0.06546554565429688
			 train-loss:  2.0779352519247265 	 ± 0.15590663572713795
	data : 0.1132619857788086
	model : 0.06490149497985839
			 train-loss:  2.076281578917252 	 ± 0.15191047712000655
	data : 0.11367783546447754
	model : 0.0648888111114502
			 train-loss:  2.076691836118698 	 ± 0.14807481661086755
	data : 0.11374897956848144
	model : 0.06484899520874024
			 train-loss:  2.07626812230973 	 ± 0.14451864771938658
	data : 0.11376848220825195
	model : 0.06487026214599609
			 train-loss:  2.0842558416453274 	 ± 0.14586352920025475
	data : 0.11401057243347168
	model : 0.06496014595031738
			 train-loss:  2.099111582921899 	 ± 0.15876512650262184
	data : 0.11405706405639648
	model : 0.064971923828125
			 train-loss:  2.104270492990812 	 ± 0.15737925622504625
	data : 0.11402549743652343
	model : 0.06493000984191895
			 train-loss:  2.1098596143722532 	 ± 0.15661168457668337
	data : 0.11402974128723145
	model : 0.0649190902709961
			 train-loss:  2.1093035432008596 	 ± 0.15359555826593624
	data : 0.11390132904052734
	model : 0.06495318412780762
			 train-loss:  2.1088337059374207 	 ± 0.15074339834775752
	data : 0.11386423110961914
	model : 0.06490802764892578
			 train-loss:  2.101697155407497 	 ± 0.1526012304700257
	data : 0.11392650604248047
	model : 0.0649531364440918
			 train-loss:  2.1121858235063224 	 ± 0.1598889342787188
	data : 0.11396188735961914
	model : 0.0649881362915039
			 train-loss:  2.1227179686228435 	 ± 0.1671202519241756
	data : 0.11395020484924316
	model : 0.06494617462158203
			 train-loss:  2.1225931798258135 	 ± 0.1644040891378658
	data : 0.11387171745300292
	model : 0.06485381126403808
			 train-loss:  2.1282515227794647 	 ± 0.1648532065300896
	data : 0.11386356353759766
	model : 0.06477556228637696
			 train-loss:  2.1393640113599375 	 ± 0.1740822904933418
	data : 0.11375989913940429
	model : 0.06475129127502441
			 train-loss:  2.1352565814467037 	 ± 0.17311866786930366
	data : 0.11386213302612305
	model : 0.06477680206298828
			 train-loss:  2.147935461997986 	 ± 0.1859554295235221
	data : 0.11384925842285157
	model : 0.06484818458557129
			 train-loss:  2.1466015213065677 	 ± 0.1835242795860381
	data : 0.11403589248657227
	model : 0.06486196517944336
			 train-loss:  2.160009722451906 	 ± 0.19809829486879577
	data : 0.11399250030517578
	model : 0.06487979888916015
			 train-loss:  2.158798496974142 	 ± 0.1956131568018427
	data : 0.11416006088256836
	model : 0.06485161781311036
			 train-loss:  2.1569398763852243 	 ± 0.19342863227735013
	data : 0.11409487724304199
	model : 0.06488218307495117
			 train-loss:  2.155701068043709 	 ± 0.19115208889322266
	data : 0.11417899131774903
	model : 0.06487746238708496
			 train-loss:  2.160923036133371 	 ± 0.19167337071721105
	data : 0.11424031257629394
	model : 0.06497659683227539
			 train-loss:  2.155777863093785 	 ± 0.1922220995905352
	data : 0.11434903144836425
	model : 0.0650705337524414
			 train-loss:  2.1641096292540083 	 ± 0.19749840112237046
	data : 0.11440143585205079
	model : 0.06511187553405762
			 train-loss:  2.1700774051926355 	 ± 0.19912444388543588
	data : 0.11438689231872559
	model : 0.06507129669189453
			 train-loss:  2.172931793000963 	 ± 0.19780776592122373
	data : 0.11419138908386231
	model : 0.06503372192382813
			 train-loss:  2.174559499906457 	 ± 0.1959503248534452
	data : 0.11398296356201172
	model : 0.0648953914642334
			 train-loss:  2.171709365033089 	 ± 0.19481594578482284
	data : 0.11384716033935546
	model : 0.06486868858337402
			 train-loss:  2.17977183063825 	 ± 0.2005435551036263
	data : 0.11367254257202149
	model : 0.06484651565551758
			 train-loss:  2.1788428218997256 	 ± 0.1985909728466378
	data : 0.11372523307800293
	model : 0.06485633850097657
			 train-loss:  2.1767326688766477 	 ± 0.19714916066143906
	data : 0.11402921676635742
	model : 0.06490812301635743
			 train-loss:  2.1743179349338306 	 ± 0.19595209871406147
	data : 0.11424989700317383
	model : 0.06499290466308594
			 train-loss:  2.1761721785251913 	 ± 0.19451006658179226
	data : 0.11441049575805665
	model : 0.0649322509765625
			 train-loss:  2.1706698323195837 	 ± 0.19670957304817105
	data : 0.11433634757995606
	model : 0.0648606300354004
			 train-loss:  2.1652340866901256 	 ± 0.1988569760952781
	data : 0.11435060501098633
	model : 0.06486024856567382
			 train-loss:  2.159479336305098 	 ± 0.20152776376636897
	data : 0.11415796279907227
	model : 0.06486539840698242
			 train-loss:  2.1562213003635406 	 ± 0.20117657288357924
	data : 0.11417498588562011
	model : 0.06489386558532714
			 train-loss:  2.1611785010287634 	 ± 0.20282532658767508
	data : 0.11424274444580078
	model : 0.06492891311645507
			 train-loss:  2.1632503846596025 	 ± 0.2016767693480837
	data : 0.1144759178161621
	model : 0.06504597663879394
			 train-loss:  2.1675771939552435 	 ± 0.20265727664016525
	data : 0.11455264091491699
	model : 0.06504054069519043
			 train-loss:  2.168376835187276 	 ± 0.20105521228140213
	data : 0.11455388069152832
	model : 0.06504130363464355
			 train-loss:  2.1695089105699883 	 ± 0.19959313285401153
	data : 0.11445536613464355
	model : 0.06500434875488281
			 train-loss:  2.170109687312957 	 ± 0.198032564407624
	data : 0.11430134773254394
	model : 0.06495432853698731
			 train-loss:  2.163897120763385 	 ± 0.20245334548585894
	data : 0.11415696144104004
	model : 0.06495637893676758
			 train-loss:  2.167583219707012 	 ± 0.20298505153176416
	data : 0.1141249656677246
	model : 0.06496996879577636
			 train-loss:  2.16471696633559 	 ± 0.2027185882790993
	data : 0.11412291526794434
	model : 0.06499786376953125
			 train-loss:  2.1665143352566343 	 ± 0.20169819516312418
	data : 0.1141474723815918
	model : 0.06502809524536132
			 train-loss:  2.163930572680573 	 ± 0.20128479873696023
	data : 0.11422262191772461
	model : 0.06509342193603515
			 train-loss:  2.1595239499036003 	 ± 0.2030290144824599
	data : 0.11440367698669433
	model : 0.0650439739227295
			 train-loss:  2.1591444741124692 	 ± 0.20157671053505974
	data : 0.11438226699829102
	model : 0.06499080657958985
			 train-loss:  2.156523747103555 	 ± 0.20131220247048423
	data : 0.114310884475708
	model : 0.0649031639099121
			 train-loss:  2.159821186267154 	 ± 0.2017843477396653
	data : 0.11438074111938476
	model : 0.06486244201660156
			 train-loss:  2.159784401456515 	 ± 0.20037840763181464
	data : 0.11429300308227539
	model : 0.0648655891418457
			 train-loss:  2.1643586469023197 	 ± 0.20275106301141071
	data : 0.1142162799835205
	model : 0.0648590087890625
			 train-loss:  2.1644044969532943 	 ± 0.20137684471770562
	data : 0.11415343284606934
	model : 0.06488585472106934
			 train-loss:  2.1651688909530638 	 ± 0.2001378771574958
	data : 0.11434445381164551
	model : 0.0649482250213623
			 train-loss:  2.1690956903131386 	 ± 0.2017042628585804
	data : 0.11427817344665528
	model : 0.0649674415588379
			 train-loss:  2.1720378104742473 	 ± 0.20202499721712813
	data : 0.11424612998962402
	model : 0.06499481201171875
			 train-loss:  2.1742152082614408 	 ± 0.2016330913347282
	data : 0.11417880058288574
	model : 0.06497793197631836
			 train-loss:  2.1726087060155748 	 ± 0.20085461928818812
	data : 0.11406407356262208
	model : 0.06497697830200196
			 train-loss:  2.171034701168537 	 ± 0.2000850261081693
	data : 0.11400675773620605
	model : 0.06496386528015137
			 train-loss:  2.1699471017460765 	 ± 0.19908390276776253
	data : 0.11403379440307618
	model : 0.06499943733215333
			 train-loss:  2.1670312896007444 	 ± 0.19959887973993012
	data : 0.11415929794311523
	model : 0.0649989128112793
			 train-loss:  2.1658750628850547 	 ± 0.19866891752772323
	data : 0.11426787376403809
	model : 0.06507320404052734
			 train-loss:  2.1642600823016394 	 ± 0.19803015724990866
	data : 0.11439294815063476
	model : 0.06510362625122071
			 train-loss:  2.167235619881574 	 ± 0.1987417931632232
	data : 0.11435604095458984
	model : 0.06506152153015136
			 train-loss:  2.1647837162017822 	 ± 0.19887187704197928
	data : 0.11427593231201172
	model : 0.06494431495666504
			 train-loss:  2.166372419773847 	 ± 0.19827376958291038
	data : 0.11418695449829101
	model : 0.06485652923583984
			 train-loss:  2.1683003441853956 	 ± 0.1979624321442832
	data : 0.11411404609680176
	model : 0.06479978561401367
			 train-loss:  2.1684184154767667 	 ± 0.19685025804270237
	data : 0.11416959762573242
	model : 0.0647778034210205
			 train-loss:  2.1684964895248413 	 ± 0.19575497636793637
	data : 0.11428570747375488
	model : 0.06481690406799316
			 train-loss:  2.1646263939993724 	 ± 0.19810830193909826
	data : 0.11434502601623535
	model : 0.0649446964263916
			 train-loss:  2.163210252056951 	 ± 0.19749126371513337
	data : 0.11432647705078125
	model : 0.06498451232910156
			 train-loss:  2.1605009289198023 	 ± 0.19813816905622192
	data : 0.11426301002502441
	model : 0.0650090217590332
			 train-loss:  2.1559676819659295 	 ± 0.20187191094612994
	data : 0.11417918205261231
	model : 0.06497974395751953
			 train-loss:  2.1542951169766877 	 ± 0.2014603179409905
	data : 0.11411595344543457
	model : 0.06493797302246093
			 train-loss:  2.1576012385388217 	 ± 0.20298245723641975
	data : 0.11394038200378417
	model : 0.06488180160522461
			 train-loss:  2.1568107641849323 	 ± 0.20208191875827852
	data : 0.11397123336791992
	model : 0.06489801406860352
			 train-loss:  2.1573560517661425 	 ± 0.20111996073660726
	data : 0.1141355037689209
	model : 0.06492381095886231
			 train-loss:  2.163072387377421 	 ± 0.20794940907678863
	data : 0.11427121162414551
	model : 0.06496434211730957
			 train-loss:  2.1627377808094024 	 ± 0.20693383329463674
	data : 0.1143733024597168
	model : 0.06506462097167968
			 train-loss:  2.161522129974743 	 ± 0.20626540113910138
	data : 0.11456503868103027
	model : 0.06509432792663575
			 train-loss:  2.157494741327622 	 ± 0.20920446835378093
	data : 0.11496219635009766
	model : 0.0651515007019043
			 train-loss:  2.1561400381106774 	 ± 0.208635530369758
	data : 0.11488099098205566
	model : 0.06511969566345215
			 train-loss:  2.1584475315534153 	 ± 0.2089465594460482
	data : 0.1146923542022705
	model : 0.06504673957824707
			 train-loss:  2.1579142706734795 	 ± 0.2080202924786527
	data : 0.11456375122070313
	model : 0.06494159698486328
			 train-loss:  2.158824090687734 	 ± 0.2072465382228179
	data : 0.11443862915039063
	model : 0.06493930816650391
			 train-loss:  2.155639827808487 	 ± 0.20886480169414637
	data : 0.114036226272583
	model : 0.064892578125
			 train-loss:  2.156312769209897 	 ± 0.2080120903822267
	data : 0.11406807899475098
	model : 0.06488218307495117
			 train-loss:  2.1567542148292613 	 ± 0.2071065247328152
	data : 0.11460108757019043
	model : 0.06501684188842774
			 train-loss:  2.15610157251358 	 ± 0.2062755508461974
	data : 0.11465072631835938
	model : 0.06511797904968261
			 train-loss:  2.1595237502106674 	 ± 0.20845746842510332
	data : 0.11477074623107911
	model : 0.06512656211853027
			 train-loss:  2.159798071852752 	 ± 0.20754489250102337
	data : 0.1146841049194336
	model : 0.06506199836730957
			 train-loss:  2.1588508565868954 	 ± 0.20686753498502533
	data : 0.1145054817199707
	model : 0.06503567695617676
			 train-loss:  2.157937333249209 	 ± 0.20618702875148803
	data : 0.11414923667907714
	model : 0.06493659019470215
			 train-loss:  2.1555362380069236 	 ± 0.20688318256455665
	data : 0.11404647827148437
	model : 0.06491208076477051
			 train-loss:  2.1557395201304863 	 ± 0.20600104930137106
	data : 0.11411161422729492
	model : 0.06488499641418458
			 train-loss:  2.155212042678116 	 ± 0.20519747252161746
	data : 0.11426448822021484
	model : 0.06493206024169922
			 train-loss:  2.15638254961725 	 ± 0.20471803124668767
	data : 0.11443676948547363
	model : 0.064990234375
			 train-loss:  2.155827873895148 	 ± 0.20394508174105613
	data : 0.11440343856811523
	model : 0.06504325866699219
			 train-loss:  2.1560917804638544 	 ± 0.20311393605037734
	data : 0.11439781188964844
	model : 0.0649747371673584
			 train-loss:  2.155483812340035 	 ± 0.20238249280406187
	data : 0.11420283317565919
	model : 0.06494951248168945
			 train-loss:  2.1544972941523692 	 ± 0.20184327077407654
	data : 0.11424555778503417
	model : 0.06491045951843262
			 train-loss:  2.1536697091125863 	 ± 0.20122882034853007
	data : 0.11411428451538086
	model : 0.06488533020019531
			 train-loss:  2.153037512494672 	 ± 0.20053837813498943
	data : 0.11416740417480468
	model : 0.06489629745483398
			 train-loss:  2.15325564289093 	 ± 0.19974938298784206
	data : 0.11427521705627441
	model : 0.06495184898376465
			 train-loss:  2.155260181616223 	 ± 0.20021344294350546
	data : 0.11446533203125
	model : 0.06498532295227051
			 train-loss:  2.153386045628645 	 ± 0.20053017250179542
	data : 0.11439027786254882
	model : 0.06505937576293945
			 train-loss:  2.1536221420392394 	 ± 0.19976303529679565
	data : 0.11457023620605469
	model : 0.06503806114196778
			 train-loss:  2.155329893725787 	 ± 0.19992305598886176
	data : 0.11446685791015625
	model : 0.06495742797851563
			 train-loss:  2.156896461890294 	 ± 0.19994588326221524
	data : 0.11438999176025391
	model : 0.06485919952392578
			 train-loss:  2.1564867177992375 	 ± 0.19923605012715137
	data : 0.11441283226013184
	model : 0.06480422019958496
			 train-loss:  2.156079222758611 	 ± 0.1985347243802951
	data : 0.11430277824401855
	model : 0.06475491523742676
			 train-loss:  2.1544109072004045 	 ± 0.19871353286517365
	data : 0.1142570972442627
	model : 0.0647815227508545
			 train-loss:  2.1535577685085694 	 ± 0.1982150145655057
	data : 0.11440596580505372
	model : 0.06483674049377441
			 train-loss:  2.152717030489886 	 ± 0.19771918865681615
	data : 0.11449923515319824
	model : 0.06493153572082519
			 train-loss:  2.1514207080883136 	 ± 0.19756591538714272
	data : 0.11440434455871581
	model : 0.06500201225280762
			 train-loss:  2.1495064992974275 	 ± 0.19810531100876955
	data : 0.11452126502990723
	model : 0.06501045227050781
			 train-loss:  2.147414187590281 	 ± 0.19889967136857595
	data : 0.1145162582397461
	model : 0.06497793197631836
			 train-loss:  2.152123083313592 	 ± 0.20575820188318308
	data : 0.11421432495117187
	model : 0.06486568450927735
			 train-loss:  2.151913808924811 	 ± 0.20503687992209388
	data : 0.11414542198181152
	model : 0.064794921875
			 train-loss:  2.1544896516394108 	 ± 0.20656926303376014
	data : 0.1140596866607666
	model : 0.0647810935974121
			 train-loss:  2.1534073713799597 	 ± 0.20624140957178208
	data : 0.11415696144104004
	model : 0.06477913856506348
			 train-loss:  2.154381110951617 	 ± 0.2058463208395206
	data : 0.11404695510864257
	model : 0.06480536460876465
			 train-loss:  2.1535038674871125 	 ± 0.20539839193527615
	data : 0.11426796913146972
	model : 0.0649439811706543
			 train-loss:  2.155580317563024 	 ± 0.20619995419352452
	data : 0.11416435241699219
	model : 0.06498565673828124
			 train-loss:  2.155424076400391 	 ± 0.20550118901273784
	data : 0.11420559883117676
	model : 0.0649111270904541
			 train-loss:  2.1553232012962806 	 ± 0.2048046396297208
	data : 0.1139286994934082
	model : 0.06482295989990235
			 train-loss:  2.158232224148673 	 ± 0.20713643693421116
	data : 0.1140589714050293
	model : 0.06475520133972168
			 train-loss:  2.1569934003305113 	 ± 0.20698956593363443
	data : 0.11394791603088379
	model : 0.06472215652465821
			 train-loss:  2.155180287361145 	 ± 0.2074822127798063
	data : 0.11410751342773437
	model : 0.06475677490234374
			 train-loss:  2.1539114571565032 	 ± 0.2073771112926179
	data : 0.11418814659118652
	model : 0.06484417915344239
			 train-loss:  2.1544029814632317 	 ± 0.20678205415934064
	data : 0.11441493034362793
	model : 0.06492013931274414
			 train-loss:  2.1523147720137454 	 ± 0.20770691506861189
	data : 0.1144484043121338
	model : 0.06499800682067872
			 train-loss:  2.1526725818584493 	 ± 0.20707874570062643
	data : 0.11450510025024414
	model : 0.06496834754943848
			 train-loss:  2.153387757270567 	 ± 0.20660038401807856
	data : 0.11437664031982422
	model : 0.0649256706237793
			 train-loss:  2.1536663281611905 	 ± 0.20596634036907438
	data : 0.11448302268981933
	model : 0.06487565040588379
			 train-loss:  2.1528468739454913 	 ± 0.20556430504085202
	data : 0.11430945396423339
	model : 0.06492571830749512
			 train-loss:  2.1529245723651935 	 ± 0.20491506523175682
	data : 0.11442031860351562
	model : 0.06493048667907715
			 train-loss:  2.15317994843489 	 ± 0.2042948825293675
	data : 0.11439743041992187
	model : 0.06495161056518554
			 train-loss:  2.1559746310114862 	 ± 0.2066818240573904
	data : 0.11457438468933105
	model : 0.06494874954223633
			 train-loss:  2.1569239382418046 	 ± 0.20638856691025262
	data : 0.11446638107299804
	model : 0.06498222351074219
			 train-loss:  2.156621460561399 	 ± 0.2057863725494684
	data : 0.11459684371948242
	model : 0.06500368118286133
			 train-loss:  2.158495418864525 	 ± 0.2065360158666375
	data : 0.11434755325317383
	model : 0.06498761177062988
			 train-loss:  2.1566392408638464 	 ± 0.2072646161255993
	data : 0.11431183815002441
	model : 0.06500873565673829
			 train-loss:  2.160269368056095 	 ± 0.21180044584736815
	data : 0.11414904594421386
	model : 0.06497635841369628
			 train-loss:  2.162619067243783 	 ± 0.21330768990560675
	data : 0.11402368545532227
	model : 0.06496262550354004
			 train-loss:  2.1646569653185543 	 ± 0.21428279726365276
	data : 0.11411490440368652
	model : 0.06490855216979981
			 train-loss:  2.165801852231934 	 ± 0.2141557819876154
	data : 0.11416659355163575
	model : 0.06490044593811035
			 train-loss:  2.16757880089551 	 ± 0.21475984229532563
	data : 0.11424527168273926
	model : 0.0648806095123291
			 train-loss:  2.1681541141341714 	 ± 0.2142578393804476
	data : 0.11426730155944824
	model : 0.06492886543273926
			 train-loss:  2.1675963060200565 	 ± 0.21375420138795845
	data : 0.11435055732727051
	model : 0.06492800712585449
			 train-loss:  2.170049033192701 	 ± 0.21553173305562756
	data : 0.11424965858459472
	model : 0.06488080024719238
			 train-loss:  2.170481134012255 	 ± 0.21498260964910662
	data : 0.11423430442810059
	model : 0.06483378410339355
			 train-loss:  2.1712386628677107 	 ± 0.21459538799955816
	data : 0.1142094612121582
	model : 0.0648353099822998
			 train-loss:  2.1721359872817994 	 ± 0.21430850239701413
	data : 0.1141922950744629
	model : 0.06488981246948242
			 train-loss:  2.1725099418650973 	 ± 0.21375605546279416
	data : 0.11420197486877441
	model : 0.06493043899536133
			 train-loss:  2.172588223791392 	 ± 0.2131538994793923
	data : 0.1141575813293457
	model : 0.06498613357543945
			 train-loss:  2.1752294067586404 	 ± 0.2154392243489536
	data : 0.11423745155334472
	model : 0.06503453254699706
			 train-loss:  2.1758501296602812 	 ± 0.2149961530325297
	data : 0.11421756744384766
	model : 0.06503586769104004
			 train-loss:  2.177492047018475 	 ± 0.2155205656788247
	data : 0.11416511535644532
	model : 0.06493439674377441
			 train-loss:  2.1781224433888386 	 ± 0.21509072786872682
	data : 0.11415629386901856
	model : 0.0648524284362793
			 train-loss:  2.1776383335773764 	 ± 0.21459786316355475
	data : 0.11411542892456054
	model : 0.06484847068786621
			 train-loss:  2.176696043196923 	 ± 0.21438794533573147
	data : 0.11410045623779297
	model : 0.06486592292785645
			 train-loss:  2.1769775560368663 	 ± 0.21383848868495764
	data : 0.11418805122375489
	model : 0.06487364768981933
			 train-loss:  2.1772531180768397 	 ± 0.21329251921081235
	data : 0.11422009468078613
	model : 0.06493401527404785
			 train-loss:  2.177305587517318 	 ± 0.21271957663991822
	data : 0.11428413391113282
	model : 0.06496219635009766
			 train-loss:  2.177568433756497 	 ± 0.21218032923093086
	data : 0.11435041427612305
	model : 0.06495275497436523
			 train-loss:  2.176057973440657 	 ± 0.21262093055061793
	data : 0.11418647766113281
	model : 0.06493830680847168
			 train-loss:  2.176184162891731 	 ± 0.21206475381252277
	data : 0.11411285400390625
	model : 0.06488270759582519
			 train-loss:  2.1761106748329966 	 ± 0.21150836534128878
	data : 0.11417126655578613
	model : 0.06484665870666503
			 train-loss:  2.177134472038109 	 ± 0.21142544869783497
	data : 0.11421060562133789
	model : 0.06487207412719727
			 train-loss:  2.181447704012195 	 ± 0.21913754216727932
	data : 0.11419672966003418
	model : 0.06487455368041992
			 train-loss:  2.1802322469227056 	 ± 0.2192170067669976
	data : 0.11427230834960937
	model : 0.06490263938903809
			 train-loss:  2.179882457575847 	 ± 0.21870527718797086
	data : 0.11441645622253419
	model : 0.06503629684448242
			 train-loss:  2.1804308267740105 	 ± 0.21827744609528757
	data : 0.11439614295959473
	model : 0.0650970458984375
			 train-loss:  2.178435652839894 	 ± 0.2194953227923551
	data : 0.11428637504577636
	model : 0.06512374877929687
			 train-loss:  2.1786660107259217 	 ± 0.21896127065208623
	data : 0.11427044868469238
	model : 0.06510095596313477
			 train-loss:  2.1783747721200037 	 ± 0.21844588795540207
	data : 0.11417765617370605
	model : 0.06502280235290528
			 train-loss:  2.177804095062179 	 ± 0.21804425489131532
	data : 0.11402864456176758
	model : 0.06495108604431152
			 train-loss:  2.1793821704387666 	 ± 0.2186347517267037
	data : 0.11412987709045411
	model : 0.06492400169372559
			 train-loss:  2.180892781831732 	 ± 0.21913403963977177
	data : 0.11412458419799805
	model : 0.06491379737854004
			 train-loss:  2.1804511842161123 	 ± 0.2186805949454669
	data : 0.11416063308715821
	model : 0.06497297286987305
			 train-loss:  2.17979767287306 	 ± 0.2183389560942454
	data : 0.11429820060729981
	model : 0.06501765251159668
			 train-loss:  2.179721163768394 	 ± 0.21780588204819776
	data : 0.11431341171264649
	model : 0.06498703956604004
			 train-loss:  2.179534606235783 	 ± 0.21729033686274235
	data : 0.11415934562683105
	model : 0.06495437622070313
			 train-loss:  2.1810725626436254 	 ± 0.21787790057175022
	data : 0.11419992446899414
	model : 0.06489505767822265
			 train-loss:  2.180799890831473 	 ± 0.2173862189813529
	data : 0.11402559280395508
	model : 0.06489791870117187
			 train-loss:  2.182579442858696 	 ± 0.2183691849219452
	data : 0.1140942096710205
	model : 0.06494035720825195
			 train-loss:  2.1844742446424856 	 ± 0.21955345475810645
	data : 0.1140103816986084
	model : 0.06496810913085938
			 train-loss:  2.184509555498759 	 ± 0.21903067951278563
	data : 0.11409964561462402
	model : 0.06500749588012696
			 train-loss:  2.1825743156586777 	 ± 0.22030332212773357
	data : 0.1141726016998291
	model : 0.06511421203613281
			 train-loss:  2.181284447885909 	 ± 0.22058031418301652
	data : 0.11433043479919433
	model : 0.06505956649780273
			 train-loss:  2.1814752621270124 	 ± 0.2200794482495134
	data : 0.1141808032989502
	model : 0.06504087448120117
			 train-loss:  2.1817709608612774 	 ± 0.2196070494665476
	data : 0.11427488327026367
	model : 0.06500453948974609
			 train-loss:  2.1820079016131024 	 ± 0.21912315603978408
	data : 0.1141514778137207
	model : 0.0650014877319336
			 train-loss:  2.1819117389343403 	 ± 0.2186198851978883
	data : 0.11407651901245117
	model : 0.0649559497833252
			 train-loss:  2.1819461776364233 	 ± 0.218116158325041
	data : 0.1140045166015625
	model : 0.06500744819641113
			 train-loss:  2.181294427005523 	 ± 0.2178270028301122
	data : 0.11412358283996582
	model : 0.06497960090637207
			 train-loss:  2.1828118089127213 	 ± 0.21848083784473862
	data : 0.11414713859558105
	model : 0.06495652198791504
			 train-loss:  2.181669223308563 	 ± 0.21863853559554697
	data : 0.1142733097076416
	model : 0.06488499641418458
			 train-loss:  2.1830574926747457 	 ± 0.2191130099888097
	data : 0.11429295539855958
	model : 0.06479144096374512
			 train-loss:  2.1838307810259296 	 ± 0.21892099038192947
	data : 0.11421709060668946
	model : 0.06471705436706543
			 train-loss:  2.185862737920787 	 ± 0.22051777224602742
	data : 0.11420812606811523
	model : 0.0646514892578125
			 train-loss:  2.1872248064194406 	 ± 0.22096315217932164
	data : 0.11425189971923828
	model : 0.06459760665893555
			 train-loss:  2.1873670260111493 	 ± 0.2204818509667419
	data : 0.11433959007263184
	model : 0.06451940536499023
			 train-loss:  2.1869572728081086 	 ± 0.22007936102478465
	data : 0.11441054344177246
	model : 0.06444950103759765
			 train-loss:  2.1869841535711076 	 ± 0.21959444149797866
	data : 0.11454939842224121
	model : 0.06424384117126465
			 train-loss:  2.1877411771238897 	 ± 0.21940900206718617
	data : 0.11460466384887695
	model : 0.0640674114227295
			 train-loss:  2.188268469931257 	 ± 0.21907414975219178
	data : 0.11463718414306641
	model : 0.06397480964660644
			 train-loss:  2.192212184615757 	 ± 0.22659749900270956
	data : 0.1147343635559082
	model : 0.06393651962280274
			 train-loss:  2.19405197890806 	 ± 0.22782155637732843
	data : 0.11474466323852539
	model : 0.06387076377868653
			 train-loss:  2.1945031120859344 	 ± 0.22743341127842845
	data : 0.11504597663879394
	model : 0.06392126083374024
			 train-loss:  2.1923689407340445 	 ± 0.22926108308977447
	data : 0.11509408950805664
	model : 0.06406593322753906
			 train-loss:  2.191309992574219 	 ± 0.2293410246175278
	data : 0.11504116058349609
	model : 0.06403002738952637
			 train-loss:  2.190344365099643 	 ± 0.22932875391723415
	data : 0.11488699913024902
	model : 0.06398696899414062
			 train-loss:  2.1909642916614724 	 ± 0.2290396120413482
	data : 0.11503791809082031
	model : 0.06398043632507325
			 train-loss:  2.1899499274507352 	 ± 0.22908650306568096
	data : 0.1148984432220459
	model : 0.06403965950012207
			 train-loss:  2.1892978609109126 	 ± 0.2288250188866011
	data : 0.11501579284667969
	model : 0.06396241188049316
			 train-loss:  2.190295878314573 	 ± 0.228864290070345
	data : 0.11508679389953613
	model : 0.06397390365600586
			 train-loss:  2.190168648461501 	 ± 0.22839546143228864
	data : 0.11510891914367676
	model : 0.06395578384399414
			 train-loss:  2.1916552999702232 	 ± 0.229081794344666
	data : 0.11499814987182617
	model : 0.06395554542541504
			 train-loss:  2.191219582537974 	 ± 0.22870804336012282
	data : 0.11498818397521973
	model : 0.063909912109375
			 train-loss:  2.194279537769993 	 ± 0.23314810171214406
	data : 0.11486592292785644
	model : 0.06389670372009278
			 train-loss:  2.1944029639001754 	 ± 0.23267780374326094
	data : 0.11490583419799805
	model : 0.06396298408508301
			 train-loss:  2.1937998601368496 	 ± 0.2323934941835484
	data : 0.11495327949523926
	model : 0.06399393081665039
			 train-loss:  2.194824590430996 	 ± 0.23247465290671068
	data : 0.11492419242858887
	model : 0.06397285461425781
			 train-loss:  2.193506272698221 	 ± 0.23292316181455835
	data : 0.11483922004699706
	model : 0.06390194892883301
			 train-loss:  2.1924136766502933 	 ± 0.23308645743985965
	data : 0.11479344367980956
	model : 0.06391010284423829
			 train-loss:  2.192213748353552 	 ± 0.23263924772880099
	data : 0.11473822593688965
	model : 0.06388120651245117
			 train-loss:  2.191540976047516 	 ± 0.2324160892168477
	data : 0.11486945152282715
	model : 0.06389708518981933
			 train-loss:  2.1929076102625324 	 ± 0.23295697601627305
	data : 0.11490378379821778
	model : 0.06396374702453614
			 train-loss:  2.193483494103901 	 ± 0.2326732513198032
	data : 0.11503691673278808
	model : 0.0639979362487793
			 train-loss:  2.192864116943872 	 ± 0.23242103291285746
	data : 0.1150597095489502
	model : 0.06397404670715331
			 train-loss:  2.193831396384502 	 ± 0.23247274134225926
	data : 0.11502494812011718
	model : 0.06386313438415528
			 train-loss:  2.194268559007084 	 ± 0.23212105055638002
	data : 0.11485681533813477
	model : 0.06382055282592773
			 train-loss:  2.1975220958702266 	 ± 0.23742159903972837
	data : 0.11457562446594238
	model : 0.055350875854492186
#epoch  85    val-loss:  2.4282868786862024  train-loss:  2.1975220958702266  lr:  1.9073486328125e-08
			 train-loss:  2.429263114929199 	 ± 0.0
	data : 5.685124158859253
	model : 0.10386800765991211
			 train-loss:  2.256639003753662 	 ± 0.1726241111755371
	data : 2.9047434329986572
	model : 0.08720982074737549
			 train-loss:  2.259934981664022 	 ± 0.1410240503609247
	data : 1.9736127058664958
	model : 0.07957832018534343
			 train-loss:  2.208620607852936 	 ± 0.15104744937939893
	data : 1.5087481141090393
	model : 0.07588541507720947
			 train-loss:  2.1445285797119142 	 ± 0.18623484597715795
	data : 1.2297374725341796
	model : 0.07365736961364747
			 train-loss:  2.1561293601989746 	 ± 0.17197598307028594
	data : 0.11551218032836914
	model : 0.06582407951354981
			 train-loss:  2.1616098199571883 	 ± 0.1597837471453779
	data : 0.11346955299377441
	model : 0.06466364860534668
			 train-loss:  2.1562465727329254 	 ± 0.15013607516470098
	data : 0.11397223472595215
	model : 0.06468782424926758
			 train-loss:  2.158687697516547 	 ± 0.141717944845837
	data : 0.11397771835327149
	model : 0.06465044021606445
			 train-loss:  2.1737192153930662 	 ± 0.14180654804783854
	data : 0.11405549049377442
	model : 0.06465964317321778
			 train-loss:  2.1521039225838403 	 ± 0.15150315563697744
	data : 0.11421399116516114
	model : 0.06471524238586426
			 train-loss:  2.1657365957895913 	 ± 0.15193678472766764
	data : 0.11419038772583008
	model : 0.06473231315612793
			 train-loss:  2.1528647587849545 	 ± 0.1526343473667391
	data : 0.11420660018920899
	model : 0.06482715606689453
			 train-loss:  2.1417159267834256 	 ± 0.15247625734581205
	data : 0.11421318054199218
	model : 0.06486153602600098
			 train-loss:  2.1335041761398315 	 ± 0.15047636026331995
	data : 0.11419734954833985
	model : 0.06486825942993164
			 train-loss:  2.131034754216671 	 ± 0.14601167669140103
	data : 0.11402921676635742
	model : 0.06477861404418946
			 train-loss:  2.134346983012031 	 ± 0.14227037835899406
	data : 0.11402182579040528
	model : 0.06476635932922363
			 train-loss:  2.1413668857680426 	 ± 0.14125902411928187
	data : 0.11409049034118653
	model : 0.06473984718322753
			 train-loss:  2.1435043874539828 	 ± 0.13779018804628917
	data : 0.11403441429138184
	model : 0.0647507667541504
			 train-loss:  2.144177347421646 	 ± 0.13433329353880824
	data : 0.11410274505615234
	model : 0.06474905014038086
			 train-loss:  2.1536344289779663 	 ± 0.13774924760248874
	data : 0.11411056518554688
	model : 0.06479067802429199
			 train-loss:  2.1496146863157097 	 ± 0.13583698593782112
	data : 0.11413273811340333
	model : 0.06479525566101074
			 train-loss:  2.1426628828048706 	 ± 0.1367941741592183
	data : 0.11399302482604981
	model : 0.06479058265686036
			 train-loss:  2.150719886024793 	 ± 0.13937720192860661
	data : 0.11398138999938964
	model : 0.06479182243347167
			 train-loss:  2.1461326456069947 	 ± 0.1383979437432727
	data : 0.11395187377929687
	model : 0.06483612060546876
			 train-loss:  2.1461622760846066 	 ± 0.13571043005754996
	data : 0.11400737762451171
	model : 0.06488614082336426
			 train-loss:  2.1592542904394643 	 ± 0.14896851905485795
	data : 0.1141550064086914
	model : 0.06490020751953125
			 train-loss:  2.1614379244191304 	 ± 0.14672356583966012
	data : 0.11430578231811524
	model : 0.06486420631408692
			 train-loss:  2.1567150929878496 	 ± 0.14632159958627702
	data : 0.11428461074829102
	model : 0.0648569107055664
			 train-loss:  2.1637030561765034 	 ± 0.14870259158767415
	data : 0.11418013572692871
	model : 0.06473631858825683
			 train-loss:  2.165887398104514 	 ± 0.14677294045335976
	data : 0.11408915519714355
	model : 0.06476120948791504
			 train-loss:  2.1705822832882404 	 ± 0.14680735557635968
	data : 0.11412506103515625
	model : 0.06478095054626465
			 train-loss:  2.1620007970116357 	 ± 0.15249864801526763
	data : 0.11402683258056641
	model : 0.06486172676086426
			 train-loss:  2.1563458547872654 	 ± 0.15371118594005984
	data : 0.11416301727294922
	model : 0.0649190902709961
			 train-loss:  2.157636339323861 	 ± 0.1516861555156405
	data : 0.11437301635742188
	model : 0.06501340866088867
			 train-loss:  2.1498938633335962 	 ± 0.15642144101649944
	data : 0.11449499130249023
	model : 0.06495699882507325
			 train-loss:  2.157528738717775 	 ± 0.16094988817474784
	data : 0.11444168090820313
	model : 0.0649111270904541
			 train-loss:  2.1547689971170927 	 ± 0.1597027173131649
	data : 0.11439790725708007
	model : 0.06483187675476074
			 train-loss:  2.148019870122274 	 ± 0.16303960198241288
	data : 0.11434397697448731
	model : 0.06474132537841797
			 train-loss:  2.1468711674213408 	 ± 0.1611484570733741
	data : 0.11426420211791992
	model : 0.064723539352417
			 train-loss:  2.1466164472626477 	 ± 0.15917925282709836
	data : 0.11422710418701172
	model : 0.06472091674804688
			 train-loss:  2.1530377978370305 	 ± 0.16255870159826788
	data : 0.11419110298156739
	model : 0.06474456787109376
			 train-loss:  2.148911703464597 	 ± 0.1628675055787607
	data : 0.11437358856201171
	model : 0.06481566429138183
			 train-loss:  2.1498153805732727 	 ± 0.16111511387271088
	data : 0.11446547508239746
	model : 0.06488704681396484
			 train-loss:  2.149323352177938 	 ± 0.15934831578417036
	data : 0.1144587516784668
	model : 0.06490468978881836
			 train-loss:  2.147059362867604 	 ± 0.15833680042546258
	data : 0.11445612907409668
	model : 0.06494107246398925
			 train-loss:  2.151891591701102 	 ± 0.16003514178721934
	data : 0.11439733505249024
	model : 0.06489758491516114
			 train-loss:  2.1495846013228097 	 ± 0.15914717272283707
	data : 0.11437482833862304
	model : 0.06492648124694825
			 train-loss:  2.152096130410019 	 ± 0.15847303117020436
	data : 0.11428093910217285
	model : 0.06492447853088379
			 train-loss:  2.146296422481537 	 ± 0.16204820753972096
	data : 0.11430001258850098
	model : 0.06498122215270996
			 train-loss:  2.1405779731039907 	 ± 0.1654683056977624
	data : 0.11433043479919433
	model : 0.06497182846069335
			 train-loss:  2.141675190283702 	 ± 0.16405677219542994
	data : 0.11437244415283203
	model : 0.06502985954284668
			 train-loss:  2.133923962431134 	 ± 0.17184598082851907
	data : 0.11435432434082031
	model : 0.06501188278198242
			 train-loss:  2.134532751860442 	 ± 0.17030505892986678
	data : 0.11423764228820801
	model : 0.064892578125
			 train-loss:  2.1392843203111127 	 ± 0.17232426059272837
	data : 0.11423888206481933
	model : 0.06477737426757812
			 train-loss:  2.1353925466537476 	 ± 0.17320045014799332
	data : 0.11417427062988281
	model : 0.0647658348083496
			 train-loss:  2.137057848143996 	 ± 0.17212614301883003
	data : 0.11411061286926269
	model : 0.06474776268005371
			 train-loss:  2.1350878682629815 	 ± 0.17128280267961066
	data : 0.11415491104125977
	model : 0.06474490165710449
			 train-loss:  2.133929608231884 	 ± 0.17005398706655758
	data : 0.11428937911987305
	model : 0.06488709449768067
			 train-loss:  2.1355565826098126 	 ± 0.16909335163523953
	data : 0.11419839859008789
	model : 0.06493778228759765
			 train-loss:  2.1322120232660264 	 ± 0.16969088273089245
	data : 0.11425695419311524
	model : 0.0649806022644043
			 train-loss:  2.1362127969341893 	 ± 0.1711926921756293
	data : 0.1143538475036621
	model : 0.06497483253479004
			 train-loss:  2.1364917395606873 	 ± 0.1698427876652369
	data : 0.1143561840057373
	model : 0.06491813659667969
			 train-loss:  2.1321494951844215 	 ± 0.171999172260527
	data : 0.11424918174743652
	model : 0.0648801326751709
			 train-loss:  2.1316561185396634 	 ± 0.17071660754091664
	data : 0.11439838409423828
	model : 0.0649294376373291
			 train-loss:  2.13700194792314 	 ± 0.1748146014322432
	data : 0.11443705558776855
	model : 0.06489782333374024
			 train-loss:  2.140764958822905 	 ± 0.17617775079937667
	data : 0.11439847946166992
	model : 0.06491236686706543
			 train-loss:  2.1390078909256878 	 ± 0.17546794051174708
	data : 0.11436028480529785
	model : 0.06498827934265136
			 train-loss:  2.139628821525021 	 ± 0.17426703219485673
	data : 0.11449203491210938
	model : 0.06497864723205567
			 train-loss:  2.142691925593785 	 ± 0.17487868802440013
	data : 0.11443815231323243
	model : 0.06489267349243164
			 train-loss:  2.14213224531899 	 ± 0.17370590734678878
	data : 0.11428093910217285
	model : 0.06480894088745118
			 train-loss:  2.1429238816102347 	 ± 0.17262432430856578
	data : 0.11411223411560059
	model : 0.06479902267456054
			 train-loss:  2.1394219904729765 	 ± 0.1739939743175962
	data : 0.11422214508056641
	model : 0.06475791931152344
			 train-loss:  2.1408512125144132 	 ± 0.173245235496367
	data : 0.11424050331115723
	model : 0.06478567123413086
			 train-loss:  2.1394705311457316 	 ± 0.1724957707626214
	data : 0.1142958641052246
	model : 0.06485443115234375
			 train-loss:  2.1351454775584373 	 ± 0.17540307056598742
	data : 0.11449146270751953
	model : 0.06492271423339843
			 train-loss:  2.1416343264765554 	 ± 0.18321208427342106
	data : 0.11467351913452148
	model : 0.06492853164672852
			 train-loss:  2.137722998093336 	 ± 0.18524122013520666
	data : 0.11453361511230468
	model : 0.06490130424499511
			 train-loss:  2.1381162646450576 	 ± 0.18409783990214873
	data : 0.11437516212463379
	model : 0.06479840278625489
			 train-loss:  2.141518588364124 	 ± 0.18542613947846168
	data : 0.11428804397583008
	model : 0.06471023559570313
			 train-loss:  2.1453862381570135 	 ± 0.1874968552439256
	data : 0.11416220664978027
	model : 0.06470265388488769
			 train-loss:  2.146052745784201 	 ± 0.18644659604679015
	data : 0.11420025825500488
	model : 0.06470694541931152
			 train-loss:  2.147037648292909 	 ± 0.18553450511475378
	data : 0.11419196128845215
	model : 0.06477456092834473
			 train-loss:  2.148110975821813 	 ± 0.18468587655198865
	data : 0.11440882682800294
	model : 0.06488795280456543
			 train-loss:  2.153676059666802 	 ± 0.1905494304056735
	data : 0.11446099281311035
	model : 0.06495518684387207
			 train-loss:  2.1533085787019064 	 ± 0.18946863949535991
	data : 0.11445817947387696
	model : 0.06495246887207032
			 train-loss:  2.1499989991900565 	 ± 0.19086048749285106
	data : 0.11429557800292969
	model : 0.06500244140625
			 train-loss:  2.150022812864997 	 ± 0.18977308448430086
	data : 0.11416153907775879
	model : 0.06490240097045899
			 train-loss:  2.149315150935998 	 ± 0.1888206636125425
	data : 0.11393389701843262
	model : 0.06486620903015136
			 train-loss:  2.1462688538763257 	 ± 0.18995528225506436
	data : 0.11403102874755859
	model : 0.06484975814819335
			 train-loss:  2.147036008782439 	 ± 0.18904882985583757
	data : 0.11414780616760253
	model : 0.06490821838378906
			 train-loss:  2.1482552069684733 	 ± 0.18837795545532698
	data : 0.1142995834350586
	model : 0.06489996910095215
			 train-loss:  2.1488038942378056 	 ± 0.18743633262251486
	data : 0.1145167350769043
	model : 0.06499552726745605
			 train-loss:  2.14783794195094 	 ± 0.1866692395971677
	data : 0.11466846466064454
	model : 0.06501126289367676
			 train-loss:  2.1469441275847587 	 ± 0.18588627776737784
	data : 0.11455965042114258
	model : 0.0650641918182373
			 train-loss:  2.145413588732481 	 ± 0.18551634961126068
	data : 0.11440095901489258
	model : 0.06503443717956543
			 train-loss:  2.1432620458996174 	 ± 0.18575765314473527
	data : 0.11433658599853516
	model : 0.06499519348144531
			 train-loss:  2.1439631508321177 	 ± 0.18493643446197425
	data : 0.1146115779876709
	model : 0.06497607231140137
			 train-loss:  2.1460274024443193 	 ± 0.18513132217582798
	data : 0.11453714370727539
	model : 0.06496682167053222
			 train-loss:  2.1455099976062773 	 ± 0.18427526544531417
	data : 0.11461067199707031
	model : 0.06497392654418946
			 train-loss:  2.1478773851205806 	 ± 0.18488270389861547
	data : 0.11478853225708008
	model : 0.06495051383972168
			 train-loss:  2.144426253496432 	 ± 0.18721496092555215
	data : 0.11484241485595703
	model : 0.06498088836669921
			 train-loss:  2.1453929329381407 	 ± 0.18655956583949906
	data : 0.11453156471252442
	model : 0.06501240730285644
			 train-loss:  2.1465537399053574 	 ± 0.18603387574673771
	data : 0.11461982727050782
	model : 0.06502370834350586
			 train-loss:  2.1454718169711886 	 ± 0.18547435169234697
	data : 0.11451921463012696
	model : 0.06495599746704102
			 train-loss:  2.148827207538317 	 ± 0.18777208981543142
	data : 0.11434750556945801
	model : 0.06497292518615723
			 train-loss:  2.1497901054186244 	 ± 0.18715533820493077
	data : 0.11425275802612304
	model : 0.0649491310119629
			 train-loss:  2.150852354588332 	 ± 0.186610641097
	data : 0.11425619125366211
	model : 0.0649488925933838
			 train-loss:  2.1539394625829993 	 ± 0.1885028246331951
	data : 0.11423406600952149
	model : 0.0649637222290039
			 train-loss:  2.1551063700155777 	 ± 0.1880391105900933
	data : 0.11420440673828125
	model : 0.06500000953674316
			 train-loss:  2.1549210258432336 	 ± 0.18720026434661682
	data : 0.11424851417541504
	model : 0.06505107879638672
			 train-loss:  2.1547510911311423 	 ± 0.18637127486741525
	data : 0.11427426338195801
	model : 0.06509675979614257
			 train-loss:  2.1537864092176995 	 ± 0.1858254497843032
	data : 0.114265775680542
	model : 0.06506633758544922
			 train-loss:  2.156742592652639 	 ± 0.18765847366318242
	data : 0.11416082382202149
	model : 0.06502737998962402
			 train-loss:  2.1577736574670543 	 ± 0.18716482582420027
	data : 0.11414790153503418
	model : 0.06497721672058106
			 train-loss:  2.1579831520031236 	 ± 0.1863698757560636
	data : 0.1140871524810791
	model : 0.06496009826660157
			 train-loss:  2.158834784458845 	 ± 0.18579825975103229
	data : 0.11411356925964355
	model : 0.06495747566223145
			 train-loss:  2.1607779557422058 	 ± 0.18619942144936227
	data : 0.11408100128173829
	model : 0.06500120162963867
			 train-loss:  2.160564824312675 	 ± 0.18542987427607616
	data : 0.11426367759704589
	model : 0.06507377624511719
			 train-loss:  2.1609678735335667 	 ± 0.18470797044890075
	data : 0.11434412002563477
	model : 0.0651219367980957
			 train-loss:  2.1588009467794875 	 ± 0.18546844468186213
	data : 0.11441583633422851
	model : 0.06515369415283204
			 train-loss:  2.1591664904453713 	 ± 0.18475052632405667
	data : 0.11427621841430664
	model : 0.0651289939880371
			 train-loss:  2.1596025509562917 	 ± 0.18406100357656527
	data : 0.11429810523986816
	model : 0.06503767967224121
			 train-loss:  2.1589403940785314 	 ± 0.18346435422050797
	data : 0.11413898468017578
	model : 0.06499452590942383
			 train-loss:  2.1601045265197754 	 ± 0.1831882672379577
	data : 0.11421074867248535
	model : 0.06501584053039551
			 train-loss:  2.1590946912765503 	 ± 0.1828088599845763
	data : 0.1143296241760254
	model : 0.06498594284057617
			 train-loss:  2.1582608823701155 	 ± 0.18232810214309797
	data : 0.11448554992675782
	model : 0.06502766609191894
			 train-loss:  2.1688857469707727 	 ± 0.21753290709215384
	data : 0.11449151039123535
	model : 0.06505670547485351
			 train-loss:  2.1683454033016236 	 ± 0.21677433404685018
	data : 0.11462812423706055
	model : 0.06507325172424316
			 train-loss:  2.1684763303169836 	 ± 0.21594409719382937
	data : 0.11444354057312012
	model : 0.06504783630371094
			 train-loss:  2.1718258348130086 	 ± 0.21848198602274063
	data : 0.11418886184692383
	model : 0.06496663093566894
			 train-loss:  2.1707371563622444 	 ± 0.2180092155387095
	data : 0.11402263641357421
	model : 0.06487421989440918
			 train-loss:  2.1712947465423356 	 ± 0.21728254471847502
	data : 0.11398248672485352
	model : 0.0648806095123291
			 train-loss:  2.172543625333416 	 ± 0.2169488832732865
	data : 0.11390914916992187
	model : 0.06482844352722168
			 train-loss:  2.173359583042286 	 ± 0.2163501567211152
	data : 0.11394948959350586
	model : 0.06485996246337891
			 train-loss:  2.172762825208552 	 ± 0.21566477346518675
	data : 0.11418085098266602
	model : 0.06487345695495605
			 train-loss:  2.173938361397625 	 ± 0.21531310322137218
	data : 0.1143195629119873
	model : 0.06489872932434082
			 train-loss:  2.1722719271977744 	 ± 0.21541643793345713
	data : 0.11442384719848633
	model : 0.06492452621459961
			 train-loss:  2.175733540555556 	 ± 0.21845828003898665
	data : 0.11414918899536133
	model : 0.06490764617919922
			 train-loss:  2.1767391511372156 	 ± 0.2179993082026638
	data : 0.11415271759033203
	model : 0.06486053466796875
			 train-loss:  2.1750321286789913 	 ± 0.21816186757732156
	data : 0.11405701637268066
	model : 0.06490960121154785
			 train-loss:  2.176287743407236 	 ± 0.21790301365256606
	data : 0.11411314010620117
	model : 0.06494183540344238
			 train-loss:  2.177059982206438 	 ± 0.21733468528453245
	data : 0.11408853530883789
	model : 0.0649674415588379
			 train-loss:  2.1754059435592756 	 ± 0.21748005580091048
	data : 0.11442718505859376
	model : 0.06502470970153809
			 train-loss:  2.17343684640424 	 ± 0.21801312341297432
	data : 0.1145012378692627
	model : 0.06506385803222656
			 train-loss:  2.172646851572272 	 ± 0.2174733759689395
	data : 0.1145357608795166
	model : 0.06509256362915039
			 train-loss:  2.1714320280113997 	 ± 0.21722891858333504
	data : 0.11432824134826661
	model : 0.06505484580993652
			 train-loss:  2.171437849869599 	 ± 0.2164938047382229
	data : 0.1142052173614502
	model : 0.06495442390441894
			 train-loss:  2.169879168472034 	 ± 0.21659771654812782
	data : 0.11420760154724122
	model : 0.06496286392211914
			 train-loss:  2.1701142700513203 	 ± 0.2158935910317215
	data : 0.114262056350708
	model : 0.06493797302246093
			 train-loss:  2.1696714778609625 	 ± 0.21524585154659528
	data : 0.1143643856048584
	model : 0.06492815017700196
			 train-loss:  2.16952378263599 	 ± 0.21454431421731532
	data : 0.114536714553833
	model : 0.0649693489074707
			 train-loss:  2.1712457970076917 	 ± 0.21489334238119825
	data : 0.11462893486022949
	model : 0.06501932144165039
			 train-loss:  2.1706901904824494 	 ± 0.21430472436680487
	data : 0.11451363563537598
	model : 0.06497755050659179
			 train-loss:  2.170049125148404 	 ± 0.2137603877319677
	data : 0.11434283256530761
	model : 0.06489052772521972
			 train-loss:  2.1729001165964665 	 ± 0.21601032356626557
	data : 0.11421875953674317
	model : 0.06482634544372559
			 train-loss:  2.174141038754943 	 ± 0.21587839633248557
	data : 0.11425642967224121
	model : 0.0648122787475586
			 train-loss:  2.1733737301222886 	 ± 0.21540881823791916
	data : 0.11424779891967773
	model : 0.06482176780700684
			 train-loss:  2.1713158829407124 	 ± 0.21628272510209573
	data : 0.11433162689208984
	model : 0.06488142013549805
			 train-loss:  2.1709307730197906 	 ± 0.21566046126902433
	data : 0.1144258975982666
	model : 0.06499977111816406
			 train-loss:  2.1723815506289466 	 ± 0.21577144618014066
	data : 0.11447114944458008
	model : 0.06503772735595703
			 train-loss:  2.1719249501640414 	 ± 0.21518246225607096
	data : 0.11440420150756836
	model : 0.06504197120666504
			 train-loss:  2.1723110456408166 	 ± 0.214577656929416
	data : 0.11442446708679199
	model : 0.06504201889038086
			 train-loss:  2.1712401138573156 	 ± 0.21435895418373818
	data : 0.11432652473449707
	model : 0.06497697830200196
			 train-loss:  2.1702494657400884 	 ± 0.21408462056582747
	data : 0.11418790817260742
	model : 0.0649609088897705
			 train-loss:  2.1718311431896256 	 ± 0.2144036103001793
	data : 0.11421790122985839
	model : 0.06493663787841797
			 train-loss:  2.172392952941849 	 ± 0.213883238706906
	data : 0.11424908638000489
	model : 0.06492085456848144
			 train-loss:  2.170031663207781 	 ± 0.21541792414183267
	data : 0.11432628631591797
	model : 0.06492815017700196
			 train-loss:  2.1685184670623237 	 ± 0.21567331094337056
	data : 0.11437020301818848
	model : 0.06497025489807129
			 train-loss:  2.167622452623704 	 ± 0.21535329070608622
	data : 0.11446332931518555
	model : 0.06494855880737305
			 train-loss:  2.1669672497531822 	 ± 0.21489255104594154
	data : 0.11447629928588868
	model : 0.06497411727905274
			 train-loss:  2.169456440348958 	 ± 0.2167252984537629
	data : 0.11430487632751465
	model : 0.064906644821167
			 train-loss:  2.1682015140621647 	 ± 0.21672384597772554
	data : 0.11414670944213867
	model : 0.06490955352783204
			 train-loss:  2.1686396146642752 	 ± 0.21617699137067414
	data : 0.11427116394042969
	model : 0.06489930152893067
			 train-loss:  2.1682343891688753 	 ± 0.21562472244414335
	data : 0.11429772377014161
	model : 0.06495118141174316
			 train-loss:  2.1674162677743216 	 ± 0.21528349173471853
	data : 0.11434025764465332
	model : 0.06497416496276856
			 train-loss:  2.167332421588359 	 ± 0.21467736669431323
	data : 0.11443648338317872
	model : 0.06504640579223633
			 train-loss:  2.1664430920997364 	 ± 0.21440021011675137
	data : 0.11455998420715333
	model : 0.06502838134765625
			 train-loss:  2.1658246090958237 	 ± 0.2139596632058526
	data : 0.114506196975708
	model : 0.06498003005981445
			 train-loss:  2.1652589307890997 	 ± 0.21349868786082077
	data : 0.11443753242492676
	model : 0.06489076614379882
			 train-loss:  2.168468466121189 	 ± 0.21721891779271701
	data : 0.11409530639648438
	model : 0.06473565101623535
			 train-loss:  2.168272262091165 	 ± 0.21663742280506543
	data : 0.11407523155212403
	model : 0.06476926803588867
			 train-loss:  2.1703657231044247 	 ± 0.21788286890997205
	data : 0.1141432762145996
	model : 0.0647517204284668
			 train-loss:  2.1686017914958624 	 ± 0.2185962846084998
	data : 0.11410198211669922
	model : 0.06484675407409668
			 train-loss:  2.167733669281006 	 ± 0.21832249310494692
	data : 0.11418046951293945
	model : 0.06494264602661133
			 train-loss:  2.1674457737194595 	 ± 0.21777002232066506
	data : 0.11449971199035644
	model : 0.06507220268249511
			 train-loss:  2.167624156105327 	 ± 0.21720059409950762
	data : 0.11455216407775878
	model : 0.06505012512207031
			 train-loss:  2.1668790794433432 	 ± 0.2168616435004281
	data : 0.11436724662780762
	model : 0.06509990692138672
			 train-loss:  2.165721263835039 	 ± 0.21686899924278136
	data : 0.11439900398254395
	model : 0.06502442359924317
			 train-loss:  2.1671694366555463 	 ± 0.21721187073306877
	data : 0.11436872482299805
	model : 0.06490349769592285
			 train-loss:  2.1679275659990562 	 ± 0.21689439891581858
	data : 0.11426239013671875
	model : 0.06489167213439942
			 train-loss:  2.168808220575253 	 ± 0.21667093538650406
	data : 0.11420698165893554
	model : 0.06488256454467774
			 train-loss:  2.1694803225561743 	 ± 0.2163094534211347
	data : 0.11431846618652344
	model : 0.06486515998840332
			 train-loss:  2.170081669522315 	 ± 0.21591291650973743
	data : 0.11434817314147949
	model : 0.0648730754852295
			 train-loss:  2.1713561816093248 	 ± 0.21608898349393296
	data : 0.11432209014892578
	model : 0.06494584083557128
			 train-loss:  2.172008998539983 	 ± 0.2157297266097045
	data : 0.11425361633300782
	model : 0.06492757797241211
			 train-loss:  2.1726065553384384 	 ± 0.21534405330700043
	data : 0.11429753303527831
	model : 0.0648684024810791
			 train-loss:  2.1746255327956847 	 ± 0.21666074655799336
	data : 0.1142575740814209
	model : 0.06483988761901856
			 train-loss:  2.175212457551429 	 ± 0.2162734320988516
	data : 0.11424169540405274
	model : 0.06485600471496582
			 train-loss:  2.1749278247356414 	 ± 0.21576943387973724
	data : 0.11430220603942871
	model : 0.06488046646118165
			 train-loss:  2.1739624892894307 	 ± 0.21566455197633286
	data : 0.11441349983215332
	model : 0.06492695808410645
			 train-loss:  2.1724819812444176 	 ± 0.2161516093791592
	data : 0.11451311111450195
	model : 0.06501846313476563
			 train-loss:  2.172106678849958 	 ± 0.21568452669623855
	data : 0.11444697380065919
	model : 0.0650482177734375
			 train-loss:  2.1727866080461764 	 ± 0.2153732211780081
	data : 0.1144174575805664
	model : 0.06504197120666504
			 train-loss:  2.173957508947791 	 ± 0.2154971899631589
	data : 0.11440491676330566
	model : 0.06495943069458007
			 train-loss:  2.1753102245840052 	 ± 0.21584421180770447
	data : 0.11426410675048829
	model : 0.06486520767211915
			 train-loss:  2.17762021223704 	 ± 0.21785977674104226
	data : 0.11422567367553711
	model : 0.0648080825805664
			 train-loss:  2.1775676854527912 	 ± 0.21733675831177798
	data : 0.11448140144348144
	model : 0.06478691101074219
			 train-loss:  2.1782800803344213 	 ± 0.21705948907802367
	data : 0.11454629898071289
	model : 0.06478366851806641
			 train-loss:  2.1788942444892156 	 ± 0.21672401760031285
	data : 0.11462874412536621
	model : 0.06483449935913085
			 train-loss:  2.1815304343734305 	 ± 0.219558855162788
	data : 0.11472282409667969
	model : 0.06487565040588379
			 train-loss:  2.18247541560317 	 ± 0.21947009921386149
	data : 0.11469616889953613
	model : 0.06485114097595215
			 train-loss:  2.1841772414149254 	 ± 0.22035195589391585
	data : 0.11445879936218262
	model : 0.0648202896118164
			 train-loss:  2.1834106707127297 	 ± 0.22012100613555557
	data : 0.11440353393554688
	model : 0.06479077339172364
			 train-loss:  2.186174897814906 	 ± 0.2233003738679954
	data : 0.11424665451049805
	model : 0.06480679512023926
			 train-loss:  2.186530080658418 	 ± 0.22284374071727076
	data : 0.11433877944946289
	model : 0.06485137939453126
			 train-loss:  2.1871847430681854 	 ± 0.22253777617619105
	data : 0.11440486907958984
	model : 0.06487369537353516
			 train-loss:  2.1869755081080515 	 ± 0.22204817476922561
	data : 0.11448359489440918
	model : 0.06489415168762207
			 train-loss:  2.188437903308433 	 ± 0.22259035891493412
	data : 0.11448874473571777
	model : 0.06491522789001465
			 train-loss:  2.1885169034654446 	 ± 0.22208697271264516
	data : 0.11464204788208007
	model : 0.06484770774841309
			 train-loss:  2.1882819994542393 	 ± 0.22161133484961892
	data : 0.11471905708312988
	model : 0.06488409042358398
			 train-loss:  2.18791843051309 	 ± 0.22117769482600544
	data : 0.11448478698730469
	model : 0.06477541923522949
			 train-loss:  2.189828107709842 	 ± 0.2225079928463188
	data : 0.11449975967407226
	model : 0.06465940475463867
			 train-loss:  2.1890229864844253 	 ± 0.22233608357340967
	data : 0.11444687843322754
	model : 0.06454954147338868
			 train-loss:  2.189011232058207 	 ± 0.2218415229418027
	data : 0.11442675590515136
	model : 0.06450085639953614
			 train-loss:  2.189982519213077 	 ± 0.22182913797274806
	data : 0.11435017585754395
	model : 0.06425318717956544
			 train-loss:  2.1903700424185932 	 ± 0.2214166429316629
	data : 0.11460995674133301
	model : 0.06416325569152832
			 train-loss:  2.1900945624761414 	 ± 0.22096953011458229
	data : 0.11463322639465331
	model : 0.06410565376281738
			 train-loss:  2.1892255175061623 	 ± 0.2208766784844838
	data : 0.11460871696472168
	model : 0.06398129463195801
			 train-loss:  2.1905239032662434 	 ± 0.22127006423906095
	data : 0.11468181610107422
	model : 0.06389508247375489
			 train-loss:  2.1906978650526567 	 ± 0.22080636718088117
	data : 0.11488041877746583
	model : 0.06391263008117676
			 train-loss:  2.1895168094799438 	 ± 0.2210599898027614
	data : 0.11492390632629394
	model : 0.06396455764770508
			 train-loss:  2.1898114824499695 	 ± 0.22063076011959748
	data : 0.11496415138244628
	model : 0.06396899223327637
			 train-loss:  2.1889730880403113 	 ± 0.2205304601808269
	data : 0.11523394584655762
	model : 0.06401267051696777
			 train-loss:  2.1897113602212133 	 ± 0.22035034091552116
	data : 0.1152946949005127
	model : 0.06402435302734374
			 train-loss:  2.187997668981552 	 ± 0.2214467603037257
	data : 0.11505374908447266
	model : 0.06402697563171386
			 train-loss:  2.1872138821123017 	 ± 0.22130687607401356
	data : 0.11499137878417968
	model : 0.06398253440856934
			 train-loss:  2.1884856499543712 	 ± 0.22170762515030432
	data : 0.11496820449829101
	model : 0.06395425796508789
			 train-loss:  2.1889380047012073 	 ± 0.22135334953454916
	data : 0.11489806175231934
	model : 0.06394977569580078
			 train-loss:  2.1891131018598875 	 ± 0.220908300920959
	data : 0.11475672721862792
	model : 0.0639756202697754
			 train-loss:  2.1888549263546575 	 ± 0.22048578845406513
	data : 0.11493520736694336
	model : 0.06397900581359864
			 train-loss:  2.189055685662041 	 ± 0.22005183939128717
	data : 0.11493406295776368
	model : 0.06400456428527831
			 train-loss:  2.1887742506623757 	 ± 0.21964222944966605
	data : 0.11496601104736329
	model : 0.06405100822448731
			 train-loss:  2.1882015549745715 	 ± 0.21937340805702327
	data : 0.11478424072265625
	model : 0.06401619911193848
			 train-loss:  2.1878398841741133 	 ± 0.2189981311677628
	data : 0.11486668586730957
	model : 0.06396684646606446
			 train-loss:  2.1873581598444685 	 ± 0.21868259111907248
	data : 0.11483883857727051
	model : 0.06395692825317383
			 train-loss:  2.1880038127242796 	 ± 0.21847428557142926
	data : 0.1149214744567871
	model : 0.06397995948791504
			 train-loss:  2.18915848914654 	 ± 0.2187872694402533
	data : 0.11484284400939941
	model : 0.06394491195678711
			 train-loss:  2.1893938042552596 	 ± 0.21837893988200258
	data : 0.11505026817321777
	model : 0.06401309967041016
			 train-loss:  2.1911385436058044 	 ± 0.21967382336861385
	data : 0.11515803337097168
	model : 0.06401023864746094
			 train-loss:  2.1906300270224945 	 ± 0.21938317762961732
	data : 0.1149132251739502
	model : 0.0639082908630371
			 train-loss:  2.189674932805319 	 ± 0.2194697110362353
	data : 0.11480231285095215
	model : 0.06390595436096191
			 train-loss:  2.1897354795056367 	 ± 0.21903765578600098
	data : 0.11489791870117187
	model : 0.06397128105163574
			 train-loss:  2.1901038950822485 	 ± 0.21868458239525954
	data : 0.1148252010345459
	model : 0.06396708488464356
			 train-loss:  2.190854424121333 	 ± 0.2185828955920391
	data : 0.11471457481384277
	model : 0.06394810676574707
			 train-loss:  2.1892594289965928 	 ± 0.21963736011515725
	data : 0.11466412544250489
	model : 0.055594873428344724
#epoch  86    val-loss:  2.395777783895794  train-loss:  2.1892594289965928  lr:  1.9073486328125e-08
			 train-loss:  2.2402760982513428 	 ± 0.0
	data : 5.591014385223389
	model : 0.07192063331604004
			 train-loss:  2.121169686317444 	 ± 0.11910641193389893
	data : 2.8560131788253784
	model : 0.06982755661010742
			 train-loss:  2.155564546585083 	 ± 0.10873624542061673
	data : 1.9411524931589763
	model : 0.06812866528828938
			 train-loss:  2.1463363766670227 	 ± 0.09551521167457686
	data : 1.4843355417251587
	model : 0.06725764274597168
			 train-loss:  2.2051526069641114 	 ± 0.14538198070748673
	data : 1.210261297225952
	model : 0.06666736602783203
			 train-loss:  2.219663977622986 	 ± 0.13662417912174107
	data : 0.11481175422668458
	model : 0.06514596939086914
			 train-loss:  2.206054142543248 	 ± 0.1308087795119489
	data : 0.11340227127075195
	model : 0.06452450752258301
			 train-loss:  2.2052477300167084 	 ± 0.12237900884494314
	data : 0.11392922401428222
	model : 0.06455445289611816
			 train-loss:  2.20238463083903 	 ± 0.11566387254255008
	data : 0.11396889686584473
	model : 0.0646012306213379
			 train-loss:  2.204449725151062 	 ± 0.10990313826749275
	data : 0.11398258209228515
	model : 0.0647207260131836
			 train-loss:  2.1984650004993784 	 ± 0.10648382729002377
	data : 0.1140404224395752
	model : 0.06485505104064941
			 train-loss:  2.179550518592199 	 ± 0.11970479899371073
	data : 0.11411595344543457
	model : 0.0649113655090332
			 train-loss:  2.1986274077342105 	 ± 0.13264283677548055
	data : 0.11405820846557617
	model : 0.06484375
			 train-loss:  2.2221809881074086 	 ± 0.15345821602782772
	data : 0.11397104263305664
	model : 0.06476335525512696
			 train-loss:  2.2225972096125286 	 ± 0.1482629010467299
	data : 0.11397676467895508
	model : 0.06475448608398438
			 train-loss:  2.2183643504977226 	 ± 0.14448797971424757
	data : 0.11405024528503419
	model : 0.06475543975830078
			 train-loss:  2.2032083553426407 	 ± 0.15272196189323026
	data : 0.1140322208404541
	model : 0.06478853225708008
			 train-loss:  2.189393778642019 	 ± 0.15897340342867072
	data : 0.11407957077026368
	model : 0.06486744880676269
			 train-loss:  2.1824476656160856 	 ± 0.15751470377953164
	data : 0.11416587829589844
	model : 0.06493477821350098
			 train-loss:  2.1779913127422335 	 ± 0.15475031615219878
	data : 0.11427092552185059
	model : 0.06493978500366211
			 train-loss:  2.1789755537396385 	 ± 0.15108497667700566
	data : 0.11400599479675293
	model : 0.0648691177368164
			 train-loss:  2.1944481947205285 	 ± 0.1637575929773223
	data : 0.11396379470825195
	model : 0.06483426094055175
			 train-loss:  2.1915105270302813 	 ± 0.16074971211657096
	data : 0.11396317481994629
	model : 0.06480970382690429
			 train-loss:  2.1995523422956467 	 ± 0.16202225751840635
	data : 0.11395115852355957
	model : 0.06484966278076172
			 train-loss:  2.2068012285232546 	 ± 0.162672296015903
	data : 0.11386733055114746
	model : 0.06481027603149414
			 train-loss:  2.213603776234847 	 ± 0.16309924105688853
	data : 0.11409335136413574
	model : 0.06484484672546387
			 train-loss:  2.199529029704906 	 ± 0.17540435434899537
	data : 0.11403546333312989
	model : 0.0648622989654541
			 train-loss:  2.208383228097643 	 ± 0.1782823368807385
	data : 0.11406736373901367
	model : 0.06486043930053711
			 train-loss:  2.2023175173792344 	 ± 0.1780976444108551
	data : 0.11410360336303711
	model : 0.0648869514465332
			 train-loss:  2.201431226730347 	 ± 0.17516922789744518
	data : 0.11417093276977539
	model : 0.06494522094726562
			 train-loss:  2.2027081058871363 	 ± 0.1724626226938784
	data : 0.11420860290527343
	model : 0.06497635841369628
			 train-loss:  2.207266226410866 	 ± 0.1716331747895746
	data : 0.11429886817932129
	model : 0.06494812965393067
			 train-loss:  2.201757308208581 	 ± 0.17186164025020068
	data : 0.1143265724182129
	model : 0.0649216651916504
			 train-loss:  2.1986996496424958 	 ± 0.17022406163473888
	data : 0.11430668830871582
	model : 0.06484365463256836
			 train-loss:  2.195421770640782 	 ± 0.1688598576514926
	data : 0.11412191390991211
	model : 0.06476192474365235
			 train-loss:  2.198250326845381 	 ± 0.16733687940035213
	data : 0.11410002708435059
	model : 0.06473565101623535
			 train-loss:  2.1996047947857833 	 ± 0.1652600234976996
	data : 0.11413192749023438
	model : 0.06479043960571289
			 train-loss:  2.1922483161876074 	 ± 0.16909915991067118
	data : 0.11408038139343261
	model : 0.06487774848937988
			 train-loss:  2.2240548531214395 	 ± 0.25749612229435814
	data : 0.11410408020019532
	model : 0.06488113403320313
			 train-loss:  2.217519152164459 	 ± 0.2575122264372688
	data : 0.1142310619354248
	model : 0.06501259803771972
			 train-loss:  2.2165725638226763 	 ± 0.25442289353524916
	data : 0.11423416137695312
	model : 0.06500329971313476
			 train-loss:  2.224848605337597 	 ± 0.2569007622606634
	data : 0.11405320167541504
	model : 0.06486191749572753
			 train-loss:  2.218418850455173 	 ± 0.2572926695653243
	data : 0.11391959190368653
	model : 0.06482152938842774
			 train-loss:  2.217416067015041 	 ± 0.2544370708550904
	data : 0.11393637657165527
	model : 0.06484971046447754
			 train-loss:  2.2172435575061376 	 ± 0.2515967115570611
	data : 0.11388096809387208
	model : 0.06480627059936524
			 train-loss:  2.2183317749396614 	 ± 0.24895398852663647
	data : 0.11390538215637207
	model : 0.06483573913574218
			 train-loss:  2.2248944297749946 	 ± 0.25028094981794013
	data : 0.11404080390930176
	model : 0.06490178108215332
			 train-loss:  2.219929357369741 	 ± 0.2499883670711548
	data : 0.11420869827270508
	model : 0.06491560935974121
			 train-loss:  2.2247580800737654 	 ± 0.24967576412044482
	data : 0.11427178382873535
	model : 0.06487598419189453
			 train-loss:  2.224871425628662 	 ± 0.24716766972820572
	data : 0.11428065299987793
	model : 0.06481037139892579
			 train-loss:  2.2279841853123084 	 ± 0.2457202495787943
	data : 0.1141554832458496
	model : 0.06486682891845703
			 train-loss:  2.2237861569111166 	 ± 0.24518587388342308
	data : 0.11416940689086914
	model : 0.06494545936584473
			 train-loss:  2.22864967022302 	 ± 0.24538101410635266
	data : 0.11420903205871583
	model : 0.06492505073547364
			 train-loss:  2.228898622371532 	 ± 0.2431051064073462
	data : 0.11415495872497558
	model : 0.06498279571533203
			 train-loss:  2.221925011548129 	 ± 0.24627552255084467
	data : 0.11415305137634277
	model : 0.06505837440490722
			 train-loss:  2.218366639954703 	 ± 0.24548926370492
	data : 0.11423773765563965
	model : 0.0649876594543457
			 train-loss:  2.215538765254774 	 ± 0.2442448019631177
	data : 0.11421098709106445
	model : 0.0649198055267334
			 train-loss:  2.2125038484047197 	 ± 0.24321182298033064
	data : 0.11400198936462402
	model : 0.06490445137023926
			 train-loss:  2.2132963366427663 	 ± 0.24121741417807901
	data : 0.1140636920928955
	model : 0.06490426063537598
			 train-loss:  2.2145886222521463 	 ± 0.23940469296024536
	data : 0.11414523124694824
	model : 0.06489181518554688
			 train-loss:  2.2121391413641756 	 ± 0.23819114317523055
	data : 0.11411056518554688
	model : 0.06491947174072266
			 train-loss:  2.208378768736316 	 ± 0.23808087753664772
	data : 0.11422786712646485
	model : 0.06501564979553223
			 train-loss:  2.2095809891110374 	 ± 0.23637341782946336
	data : 0.11445455551147461
	model : 0.06506190299987794
			 train-loss:  2.2105115577578545 	 ± 0.23463576427205585
	data : 0.11435675621032715
	model : 0.06505064964294434
			 train-loss:  2.2083827422215387 	 ± 0.2334459186021034
	data : 0.11430778503417968
	model : 0.06501188278198242
			 train-loss:  2.2021081429539304 	 ± 0.2371294447670841
	data : 0.11422066688537598
	model : 0.06491141319274903
			 train-loss:  2.196630444099654 	 ± 0.23952339439220804
	data : 0.11399197578430176
	model : 0.06488065719604492
			 train-loss:  2.1991606372244217 	 ± 0.238655995834356
	data : 0.11398768424987793
	model : 0.06485843658447266
			 train-loss:  2.1959028019421343 	 ± 0.23843855083364052
	data : 0.114019775390625
	model : 0.06489953994750977
			 train-loss:  2.193535336426326 	 ± 0.23754472262140638
	data : 0.11408839225769044
	model : 0.06495223045349122
			 train-loss:  2.195102569083093 	 ± 0.2362301358107228
	data : 0.11418385505676269
	model : 0.0650641918182373
			 train-loss:  2.1947550657722683 	 ± 0.23460218661179974
	data : 0.11435475349426269
	model : 0.06502113342285157
			 train-loss:  2.1930611476506274 	 ± 0.23343271413733505
	data : 0.11418275833129883
	model : 0.06500005722045898
			 train-loss:  2.1951240252804114 	 ± 0.23251907086292517
	data : 0.11422533988952636
	model : 0.06489729881286621
			 train-loss:  2.1953010892868043 	 ± 0.23096876432490437
	data : 0.11412153244018555
	model : 0.06490187644958496
			 train-loss:  2.1914221267951164 	 ± 0.2318903141375291
	data : 0.11413130760192872
	model : 0.06491174697875976
			 train-loss:  2.1905132795309092 	 ± 0.23051581695466047
	data : 0.1141211986541748
	model : 0.0649139404296875
			 train-loss:  2.1881665816673865 	 ± 0.22995723434141474
	data : 0.11432538032531739
	model : 0.06499681472778321
			 train-loss:  2.1872034314312514 	 ± 0.22865545197589102
	data : 0.1143843650817871
	model : 0.06508526802062989
			 train-loss:  2.184897577762604 	 ± 0.22814428373723558
	data : 0.11449017524719238
	model : 0.06505823135375977
			 train-loss:  2.1816882616207924 	 ± 0.2285414638769817
	data : 0.11453356742858886
	model : 0.06503167152404785
			 train-loss:  2.1823783502346132 	 ± 0.22722853882070576
	data : 0.11440658569335938
	model : 0.0649627685546875
			 train-loss:  2.1836881465222464 	 ± 0.2261667598433472
	data : 0.11440343856811523
	model : 0.06486477851867675
			 train-loss:  2.1822200531051275 	 ± 0.2252140043125917
	data : 0.11424384117126465
	model : 0.06484804153442383
			 train-loss:  2.1806839185602525 	 ± 0.22432753193705046
	data : 0.11418666839599609
	model : 0.06484870910644532
			 train-loss:  2.1779009076051934 	 ± 0.2244906011666661
	data : 0.11420340538024902
	model : 0.0648414134979248
			 train-loss:  2.180131377844975 	 ± 0.2241531063438001
	data : 0.11434006690979004
	model : 0.06492815017700196
			 train-loss:  2.177566955035383 	 ± 0.22415572387665508
	data : 0.11429824829101562
	model : 0.06496691703796387
			 train-loss:  2.178146312745769 	 ± 0.22295911462905915
	data : 0.11433019638061523
	model : 0.06496777534484863
			 train-loss:  2.1782753427823383 	 ± 0.22172033438921007
	data : 0.11440629959106445
	model : 0.06495013236999511
			 train-loss:  2.1820746765031918 	 ± 0.22342522738477152
	data : 0.11424860954284669
	model : 0.0649263858795166
			 train-loss:  2.182850621316744 	 ± 0.2223308940556857
	data : 0.11427245140075684
	model : 0.06492247581481933
			 train-loss:  2.1826798621044365 	 ± 0.22113840154828954
	data : 0.11421337127685546
	model : 0.06492233276367188
			 train-loss:  2.181012766158327 	 ± 0.22054573896251864
	data : 0.11424369812011718
	model : 0.06489434242248535
			 train-loss:  2.1815662396581548 	 ± 0.21944751950334526
	data : 0.1141744613647461
	model : 0.0649184226989746
			 train-loss:  2.1796600595116615 	 ± 0.21909075935234418
	data : 0.11429858207702637
	model : 0.0649653434753418
			 train-loss:  2.180862901137047 	 ± 0.21827689551299245
	data : 0.11418089866638184
	model : 0.06489019393920899
			 train-loss:  2.1801610747162177 	 ± 0.21727036154514345
	data : 0.11420831680297852
	model : 0.06485962867736816
			 train-loss:  2.186244911617703 	 ± 0.22440333906418225
	data : 0.1141284465789795
	model : 0.06489224433898926
			 train-loss:  2.1844170331954955 	 ± 0.2240179966217749
	data : 0.11404094696044922
	model : 0.06488885879516601
			 train-loss:  2.1840863393084837 	 ± 0.22293076671045214
	data : 0.11444449424743652
	model : 0.06489443778991699
			 train-loss:  2.185090831681794 	 ± 0.22206485473193482
	data : 0.11463260650634766
	model : 0.06496286392211914
			 train-loss:  2.18237801315715 	 ± 0.22267620408313837
	data : 0.11463637351989746
	model : 0.06506505012512206
			 train-loss:  2.1839642031834674 	 ± 0.22218700200038038
	data : 0.11472506523132324
	model : 0.06507363319396972
			 train-loss:  2.1860553957167124 	 ± 0.2221524303511867
	data : 0.11489105224609375
	model : 0.06504249572753906
			 train-loss:  2.1851531008504472 	 ± 0.22129528801211104
	data : 0.11443924903869629
	model : 0.0649287223815918
			 train-loss:  2.1869568323420587 	 ± 0.22104024828708074
	data : 0.11427345275878906
	model : 0.06489262580871583
			 train-loss:  2.186129191407451 	 ± 0.22018103693273008
	data : 0.114131498336792
	model : 0.0648268699645996
			 train-loss:  2.1841486462759314 	 ± 0.2201330452464976
	data : 0.11418981552124023
	model : 0.06481776237487794
			 train-loss:  2.183313775062561 	 ± 0.21930344126659243
	data : 0.11445937156677247
	model : 0.06486263275146484
			 train-loss:  2.186602126370679 	 ± 0.2210207638520649
	data : 0.11456365585327148
	model : 0.06493744850158692
			 train-loss:  2.1841877422162463 	 ± 0.22149731960796587
	data : 0.11464848518371581
	model : 0.06497511863708497
			 train-loss:  2.181984731581359 	 ± 0.22174412856720135
	data : 0.11476936340332031
	model : 0.06500611305236817
			 train-loss:  2.180313273480064 	 ± 0.22148326121624964
	data : 0.11484847068786622
	model : 0.06500635147094727
			 train-loss:  2.180969739996869 	 ± 0.22062955223900033
	data : 0.1144747257232666
	model : 0.06496901512145996
			 train-loss:  2.180896627491918 	 ± 0.21967790351746047
	data : 0.11425166130065918
	model : 0.06493043899536133
			 train-loss:  2.179586604110196 	 ± 0.219191676855395
	data : 0.11414127349853516
	model : 0.06491589546203613
			 train-loss:  2.177831217394037 	 ± 0.21908526197694528
	data : 0.1142310619354248
	model : 0.06489620208740235
			 train-loss:  2.175724392177678 	 ± 0.21935991564929952
	data : 0.11402792930603027
	model : 0.06489105224609375
			 train-loss:  2.1776483754316964 	 ± 0.21944996354044224
	data : 0.11412067413330078
	model : 0.06490449905395508
			 train-loss:  2.176636193409439 	 ± 0.2188223608859295
	data : 0.1143949031829834
	model : 0.0649876594543457
			 train-loss:  2.1772350639593405 	 ± 0.21802324721451227
	data : 0.11451663970947265
	model : 0.06502604484558105
			 train-loss:  2.1827947143616715 	 ± 0.22565164495255788
	data : 0.11496038436889648
	model : 0.06499547958374023
			 train-loss:  2.1817803767419632 	 ± 0.22502129413661046
	data : 0.11490912437438965
	model : 0.06498875617980956
			 train-loss:  2.1807943534851075 	 ± 0.22438819965277831
	data : 0.1147571086883545
	model : 0.0649648666381836
			 train-loss:  2.179959774017334 	 ± 0.22369069229796046
	data : 0.11455354690551758
	model : 0.06487593650817872
			 train-loss:  2.1797443994386927 	 ± 0.2228213953299492
	data : 0.11478276252746582
	model : 0.06480264663696289
			 train-loss:  2.1799172833561897 	 ± 0.22195784366525745
	data : 0.11428375244140625
	model : 0.06482210159301757
			 train-loss:  2.1808642084284346 	 ± 0.22135527173117325
	data : 0.11439523696899415
	model : 0.06480488777160645
			 train-loss:  2.1867713634784405 	 ± 0.23048348194529764
	data : 0.11484947204589843
	model : 0.06480584144592286
			 train-loss:  2.1862626021144953 	 ± 0.22967535371648776
	data : 0.11498141288757324
	model : 0.06485271453857422
			 train-loss:  2.1883341218485977 	 ± 0.23002888403165014
	data : 0.1146049976348877
	model : 0.06488595008850098
			 train-loss:  2.1862048807000756 	 ± 0.23046450351553668
	data : 0.11457238197326661
	model : 0.06486315727233886
			 train-loss:  2.1839271943960616 	 ± 0.23110062697537143
	data : 0.11452507972717285
	model : 0.0648508071899414
			 train-loss:  2.184083437036585 	 ± 0.23025021141156451
	data : 0.11411910057067871
	model : 0.06485295295715332
			 train-loss:  2.183716859887628 	 ± 0.22944167808404642
	data : 0.11398000717163086
	model : 0.06493310928344727
			 train-loss:  2.186128108170781 	 ± 0.23032573612999896
	data : 0.11407623291015626
	model : 0.06492528915405274
			 train-loss:  2.1875429913617563 	 ± 0.23008647151313608
	data : 0.11413893699645997
	model : 0.06494288444519043
			 train-loss:  2.189531954072362 	 ± 0.23044488894948484
	data : 0.11416635513305665
	model : 0.06495490074157714
			 train-loss:  2.1928514361381533 	 ± 0.23293166833446638
	data : 0.11417551040649414
	model : 0.06496315002441407
			 train-loss:  2.191139743683186 	 ± 0.2329861461403977
	data : 0.11425185203552246
	model : 0.06488590240478516
			 train-loss:  2.1900269515077833 	 ± 0.23254004829423267
	data : 0.11418585777282715
	model : 0.06482839584350586
			 train-loss:  2.189662688261979 	 ± 0.23176619629137105
	data : 0.11409850120544433
	model : 0.06478743553161621
			 train-loss:  2.1899682018491955 	 ± 0.2309889442514903
	data : 0.11400189399719238
	model : 0.06480445861816406
			 train-loss:  2.190179137525887 	 ± 0.2302049691269121
	data : 0.1139979362487793
	model : 0.06483373641967774
			 train-loss:  2.1903722792455596 	 ± 0.22942702979959925
	data : 0.11414828300476074
	model : 0.06486411094665527
			 train-loss:  2.190030796998212 	 ± 0.22868256135973883
	data : 0.11421971321105957
	model : 0.06494989395141601
			 train-loss:  2.1902998879149154 	 ± 0.2279320265133079
	data : 0.11420927047729493
	model : 0.06497855186462402
			 train-loss:  2.1907833406589177 	 ± 0.22724199068371134
	data : 0.1142500400543213
	model : 0.06491832733154297
			 train-loss:  2.189425594806671 	 ± 0.22708883821929948
	data : 0.11415619850158691
	model : 0.06484980583190918
			 train-loss:  2.1914225832515997 	 ± 0.22765328036354257
	data : 0.11396183967590331
	model : 0.06477627754211426
			 train-loss:  2.191749143757318 	 ± 0.22693866646002475
	data : 0.11387081146240234
	model : 0.06477904319763184
			 train-loss:  2.191047078643749 	 ± 0.22636136940973636
	data : 0.11383733749389649
	model : 0.06488080024719238
			 train-loss:  2.190662836873686 	 ± 0.22567528645960624
	data : 0.11395778656005859
	model : 0.06496729850769042
			 train-loss:  2.19261774786057 	 ± 0.2262505178274165
	data : 0.11406373977661133
	model : 0.06501965522766114
			 train-loss:  2.198856154313454 	 ± 0.23852338697742037
	data : 0.11411008834838868
	model : 0.0650437355041504
			 train-loss:  2.1992435265498558 	 ± 0.2378117673351784
	data : 0.11419091224670411
	model : 0.06502709388732911
			 train-loss:  2.197532161881652 	 ± 0.238025868232284
	data : 0.11415982246398926
	model : 0.06493682861328125
			 train-loss:  2.197258511429313 	 ± 0.23730110975565152
	data : 0.11420483589172363
	model : 0.06490755081176758
			 train-loss:  2.1967186138033865 	 ± 0.23665632182330099
	data : 0.11415791511535645
	model : 0.06494526863098145
			 train-loss:  2.196951150894165 	 ± 0.23593855499130545
	data : 0.11423239707946778
	model : 0.06496810913085938
			 train-loss:  2.200586211534194 	 ± 0.23968892020040125
	data : 0.11429505348205567
	model : 0.0649524211883545
			 train-loss:  2.1987925650883304 	 ± 0.2400406233328454
	data : 0.11442928314208985
	model : 0.06501989364624024
			 train-loss:  2.2022049972196904 	 ± 0.24324113015182922
	data : 0.11433801651000977
	model : 0.06501703262329102
			 train-loss:  2.2017363093116065 	 ± 0.24257718295468034
	data : 0.11433258056640624
	model : 0.06500906944274902
			 train-loss:  2.20027575291783 	 ± 0.24257203415368186
	data : 0.11428866386413575
	model : 0.06499099731445312
			 train-loss:  2.2024327780672177 	 ± 0.24343624839380573
	data : 0.11418819427490234
	model : 0.06496682167053222
			 train-loss:  2.2033622832525346 	 ± 0.24300770850664313
	data : 0.11417336463928222
	model : 0.06485047340393066
			 train-loss:  2.205278629381981 	 ± 0.24355755401287393
	data : 0.114070463180542
	model : 0.06483325958251954
			 train-loss:  2.2073514756034402 	 ± 0.24433067784689733
	data : 0.11427950859069824
	model : 0.06477723121643067
			 train-loss:  2.2077015003962823 	 ± 0.24365795701474577
	data : 0.11424479484558106
	model : 0.06480574607849121
			 train-loss:  2.2065781881642894 	 ± 0.24339228262708837
	data : 0.11431422233581542
	model : 0.06490659713745117
			 train-loss:  2.206226747160013 	 ± 0.24273158115119361
	data : 0.11427726745605468
	model : 0.06495876312255859
			 train-loss:  2.205375772783126 	 ± 0.24229173934582
	data : 0.11436405181884765
	model : 0.06497983932495117
			 train-loss:  2.2053436320168633 	 ± 0.24159885744688947
	data : 0.11409354209899902
	model : 0.06494240760803223
			 train-loss:  2.2045630636540325 	 ± 0.24113271318291885
	data : 0.11415839195251465
	model : 0.06492152214050292
			 train-loss:  2.204198011570731 	 ± 0.2404993490987234
	data : 0.11415300369262696
	model : 0.06486454010009765
			 train-loss:  2.2059018169895985 	 ± 0.24089171034996915
	data : 0.11412692070007324
	model : 0.06485462188720703
			 train-loss:  2.206165778570335 	 ± 0.24024369931914308
	data : 0.11422367095947265
	model : 0.06487207412719727
			 train-loss:  2.2047430515289306 	 ± 0.24033041446663034
	data : 0.11434335708618164
	model : 0.06494679450988769
			 train-loss:  2.204214226475078 	 ± 0.23977059307145848
	data : 0.11444544792175293
	model : 0.06498141288757324
			 train-loss:  2.202659618068527 	 ± 0.24002395848402755
	data : 0.11441655158996582
	model : 0.06501212120056152
			 train-loss:  2.200740041628561 	 ± 0.2407640183207083
	data : 0.11450057029724121
	model : 0.0650606632232666
			 train-loss:  2.2003860732783442 	 ± 0.24015661869600827
	data : 0.11440162658691407
	model : 0.06504659652709961
			 train-loss:  2.200443614495767 	 ± 0.23950793907511356
	data : 0.11439023017883301
	model : 0.06498990058898926
			 train-loss:  2.199303480886644 	 ± 0.23936609273910814
	data : 0.11421666145324708
	model : 0.06496319770812989
			 train-loss:  2.2014374044489733 	 ± 0.24049263001757765
	data : 0.11428699493408204
	model : 0.06493997573852539
			 train-loss:  2.2015352870555636 	 ± 0.23985590401797388
	data : 0.11422719955444335
	model : 0.06489992141723633
			 train-loss:  2.20107656181174 	 ± 0.2393031952552639
	data : 0.11424627304077148
	model : 0.06492829322814941
			 train-loss:  2.2004419226395457 	 ± 0.2388320374198628
	data : 0.11439247131347656
	model : 0.06497778892517089
			 train-loss:  2.199124560930342 	 ± 0.23889711891352566
	data : 0.11442699432373046
	model : 0.06509418487548828
			 train-loss:  2.1986536495387554 	 ± 0.23836304240099188
	data : 0.1143765926361084
	model : 0.06510400772094727
			 train-loss:  2.1981386631881636 	 ± 0.23785178618904737
	data : 0.11436409950256347
	model : 0.06510391235351562
			 train-loss:  2.195797368423226 	 ± 0.23945733850584483
	data : 0.1143061637878418
	model : 0.06505122184753417
			 train-loss:  2.195795175356743 	 ± 0.23884255808784782
	data : 0.11406474113464356
	model : 0.06503100395202636
			 train-loss:  2.1953563081974887 	 ± 0.2383112997200616
	data : 0.1140791893005371
	model : 0.06491732597351074
			 train-loss:  2.195914013131621 	 ± 0.2378338763317547
	data : 0.1141789436340332
	model : 0.06491694450378419
			 train-loss:  2.1961458355489403 	 ± 0.2372548382083573
	data : 0.11425180435180664
	model : 0.06493887901306153
			 train-loss:  2.1949059537906743 	 ± 0.23730019255219348
	data : 0.1144639015197754
	model : 0.06495447158813476
			 train-loss:  2.195889076590538 	 ± 0.23711213388577215
	data : 0.11459288597106934
	model : 0.06493978500366211
			 train-loss:  2.195397289831247 	 ± 0.2366237997757247
	data : 0.11467046737670898
	model : 0.06492009162902831
			 train-loss:  2.1964218232891346 	 ± 0.23648387632178094
	data : 0.11447882652282715
	model : 0.0648155689239502
			 train-loss:  2.197105679605982 	 ± 0.23610082637313803
	data : 0.11441898345947266
	model : 0.06475491523742676
			 train-loss:  2.197607048001944 	 ± 0.23562974225753436
	data : 0.11426491737365722
	model : 0.06476426124572754
			 train-loss:  2.199961908270673 	 ± 0.23744850874074278
	data : 0.1142033576965332
	model : 0.0648005485534668
			 train-loss:  2.1986151893162034 	 ± 0.23765499136517582
	data : 0.11417474746704101
	model : 0.06485404968261718
			 train-loss:  2.1976934970864925 	 ± 0.2374490378997662
	data : 0.11426172256469727
	model : 0.06494350433349609
			 train-loss:  2.1966592107827845 	 ± 0.2373445089765459
	data : 0.1144787311553955
	model : 0.06501269340515137
			 train-loss:  2.196531126944072 	 ± 0.2367832240434173
	data : 0.11446747779846192
	model : 0.06499552726745605
			 train-loss:  2.1970428364617485 	 ± 0.2363345907312885
	data : 0.11452932357788086
	model : 0.06489858627319336
			 train-loss:  2.1979422987354873 	 ± 0.23613391220253263
	data : 0.1143765926361084
	model : 0.0648122787475586
			 train-loss:  2.2001852168227143 	 ± 0.2378185928148027
	data : 0.11437969207763672
	model : 0.06478958129882813
			 train-loss:  2.2012166361293883 	 ± 0.23773448451265747
	data : 0.11420760154724122
	model : 0.06474895477294922
			 train-loss:  2.2001371222121695 	 ± 0.2377010799459967
	data : 0.11418452262878417
	model : 0.06477131843566894
			 train-loss:  2.1990094256955524 	 ± 0.23772073531912424
	data : 0.11426258087158203
	model : 0.06488609313964844
			 train-loss:  2.1998375858421677 	 ± 0.23748048319723442
	data : 0.11434197425842285
	model : 0.06493611335754394
			 train-loss:  2.1984295965889085 	 ± 0.2378345888853419
	data : 0.11437249183654785
	model : 0.06500482559204102
			 train-loss:  2.1978520526798495 	 ± 0.23744093942931108
	data : 0.11434035301208496
	model : 0.0649637222290039
			 train-loss:  2.197903245551401 	 ± 0.2368994223926488
	data : 0.11430249214172364
	model : 0.06484746932983398
			 train-loss:  2.197417889941822 	 ± 0.23646951006734404
	data : 0.11421208381652832
	model : 0.06475434303283692
			 train-loss:  2.196337870882647 	 ± 0.23647711178239753
	data : 0.11433224678039551
	model : 0.06488194465637206
			 train-loss:  2.1949710126395696 	 ± 0.2368172728302855
	data : 0.11434588432312012
	model : 0.06478838920593262
			 train-loss:  2.193471947058434 	 ± 0.23733901471484645
	data : 0.11444501876831055
	model : 0.06480388641357422
			 train-loss:  2.193089690591608 	 ± 0.23687743731838556
	data : 0.11501131057739258
	model : 0.06478190422058105
			 train-loss:  2.1921566179063583 	 ± 0.23676266251445469
	data : 0.11507983207702636
	model : 0.06468634605407715
			 train-loss:  2.1907367358165506 	 ± 0.23719640627672955
	data : 0.11512789726257325
	model : 0.06444392204284669
			 train-loss:  2.192564234334467 	 ± 0.2382626030731823
	data : 0.11517267227172852
	model : 0.06426215171813965
			 train-loss:  2.192733441528521 	 ± 0.23775319164350275
	data : 0.11508035659790039
	model : 0.06401267051696777
			 train-loss:  2.1926037869599186 	 ± 0.23724158978857182
	data : 0.11462411880493165
	model : 0.06395602226257324
			 train-loss:  2.191946223507757 	 ± 0.23693433263873645
	data : 0.11470046043395996
	model : 0.06396636962890626
			 train-loss:  2.191944583669885 	 ± 0.2364209328393739
	data : 0.11476755142211914
	model : 0.06388487815856933
			 train-loss:  2.1924207714097252 	 ± 0.23602184586427102
	data : 0.11483926773071289
	model : 0.06392974853515625
			 train-loss:  2.1918785776703142 	 ± 0.23565956552213652
	data : 0.11502585411071778
	model : 0.0640439510345459
			 train-loss:  2.1917077496520476 	 ± 0.23516993722941779
	data : 0.11511259078979492
	model : 0.06400704383850098
			 train-loss:  2.191800425915008 	 ± 0.23467332438804195
	data : 0.11497035026550292
	model : 0.0638805866241455
			 train-loss:  2.1916440634404197 	 ± 0.23418787466509877
	data : 0.11483397483825683
	model : 0.06382579803466797
			 train-loss:  2.193613131841024 	 ± 0.2356429033303729
	data : 0.11490225791931152
	model : 0.06382174491882324
			 train-loss:  2.1936543268315933 	 ± 0.23514818930756637
	data : 0.11482610702514648
	model : 0.06380224227905273
			 train-loss:  2.195628755760991 	 ± 0.23662443754392185
	data : 0.11491270065307617
	model : 0.06383752822875977
			 train-loss:  2.196597820520401 	 ± 0.23660572640265287
	data : 0.1150439739227295
	model : 0.0639157772064209
			 train-loss:  2.1953731429032763 	 ± 0.23687536523358627
	data : 0.11502876281738281
	model : 0.06396198272705078
			 train-loss:  2.1951677547998667 	 ± 0.23640694958842615
	data : 0.1147876262664795
	model : 0.06384754180908203
			 train-loss:  2.1946200083312672 	 ± 0.23607384304808757
	data : 0.11477303504943848
	model : 0.06380515098571778
			 train-loss:  2.1963350211010604 	 ± 0.23710162911432922
	data : 0.11469588279724122
	model : 0.06379504203796386
			 train-loss:  2.196968301948236 	 ± 0.2368239420558197
	data : 0.11465387344360352
	model : 0.06382646560668945
			 train-loss:  2.196940077514183 	 ± 0.23634251530721193
	data : 0.11479644775390625
	model : 0.06382927894592286
			 train-loss:  2.1960943251968876 	 ± 0.23623632811800202
	data : 0.11498475074768066
	model : 0.06392960548400879
			 train-loss:  2.1955224991806093 	 ± 0.23593078934251263
	data : 0.11500310897827148
	model : 0.06396613121032715
			 train-loss:  2.196679681659224 	 ± 0.23616070693451305
	data : 0.11486029624938965
	model : 0.06388139724731445
			 train-loss:  2.1954196214675905 	 ± 0.2365251406336288
	data : 0.11490345001220703
	model : 0.06384592056274414
			 train-loss:  2.196801529462594 	 ± 0.2370625962692735
	data : 0.11485562324523926
	model : 0.06387834548950196
			 train-loss:  2.1954285649080125 	 ± 0.23758957655027207
	data : 0.11473665237426758
	model : 0.06392407417297363
			 train-loss:  2.1974200095112617 	 ± 0.239217649174825
	data : 0.11472253799438477
	model : 0.06390838623046875
			 train-loss:  2.1973814734323756 	 ± 0.23874707073414586
	data : 0.11486082077026367
	model : 0.06398611068725586
			 train-loss:  2.1974825835695455 	 ± 0.23828392819927824
	data : 0.11485042572021484
	model : 0.06397876739501954
			 train-loss:  2.19910577358678 	 ± 0.23922645496757394
	data : 0.11442656517028808
	model : 0.05547113418579101
#epoch  87    val-loss:  2.4364926689549495  train-loss:  2.19910577358678  lr:  1.9073486328125e-08
			 train-loss:  2.2299537658691406 	 ± 0.0
	data : 5.605437994003296
	model : 0.07152080535888672
			 train-loss:  2.354609489440918 	 ± 0.12465572357177734
	data : 2.867759585380554
	model : 0.0681772232055664
			 train-loss:  2.2954142093658447 	 ± 0.1317859197554945
	data : 1.9499239921569824
	model : 0.0670022964477539
			 train-loss:  2.257264733314514 	 ± 0.13187795135398625
	data : 1.4910341501235962
	model : 0.06645005941390991
			 train-loss:  2.217611885070801 	 ± 0.14213665515276255
	data : 1.2156218528747558
	model : 0.06608672142028808
			 train-loss:  2.203842123349508 	 ± 0.1333556234446779
	data : 0.11739158630371094
	model : 0.06468691825866699
			 train-loss:  2.218777997153146 	 ± 0.12876984314748607
	data : 0.11413426399230957
	model : 0.06459941864013671
			 train-loss:  2.220843642950058 	 ± 0.12057707779303554
	data : 0.11401352882385254
	model : 0.06459007263183594
			 train-loss:  2.2347869078318277 	 ± 0.12032756554663239
	data : 0.11386671066284179
	model : 0.06460709571838379
			 train-loss:  2.204939126968384 	 ± 0.14508225573649858
	data : 0.113978910446167
	model : 0.06463351249694824
			 train-loss:  2.2214936559850518 	 ± 0.14790487628015134
	data : 0.11392631530761718
	model : 0.06469316482543945
			 train-loss:  2.2061363657315574 	 ± 0.15048978280398492
	data : 0.11402354240417481
	model : 0.06476912498474122
			 train-loss:  2.214955751712506 	 ± 0.14777842600880414
	data : 0.11393370628356933
	model : 0.06477575302124024
			 train-loss:  2.202015519142151 	 ± 0.1498513196878686
	data : 0.11397910118103027
	model : 0.06474976539611817
			 train-loss:  2.1836260239283245 	 ± 0.1602897988762446
	data : 0.11390666961669922
	model : 0.06484975814819335
			 train-loss:  2.167195148766041 	 ± 0.1677397486473569
	data : 0.11379795074462891
	model : 0.0648888111114502
			 train-loss:  2.161655447062324 	 ± 0.16423318942557608
	data : 0.11380672454833984
	model : 0.0649174690246582
			 train-loss:  2.1727069947454662 	 ± 0.1659831016744106
	data : 0.1139756679534912
	model : 0.06494030952453614
			 train-loss:  2.1649199724197388 	 ± 0.16489951004652748
	data : 0.11409449577331543
	model : 0.06495079994201661
			 train-loss:  2.1581622540950773 	 ± 0.16340111497495774
	data : 0.11407656669616699
	model : 0.0648421287536621
			 train-loss:  2.158474405606588 	 ± 0.1594692703192098
	data : 0.11401300430297852
	model : 0.0647862434387207
			 train-loss:  2.179326279596849 	 ± 0.18277125687757823
	data : 0.11395816802978516
	model : 0.06473069190979004
			 train-loss:  2.182694564694944 	 ± 0.1794506176242232
	data : 0.11389532089233398
	model : 0.06477012634277343
			 train-loss:  2.182834600408872 	 ± 0.17567357045242768
	data : 0.11377334594726562
	model : 0.06478724479675294
			 train-loss:  2.1844356298446654 	 ± 0.17230285635295362
	data : 0.11384162902832032
	model : 0.06481466293334961
			 train-loss:  2.1924398082953234 	 ± 0.17363205182225333
	data : 0.11399378776550292
	model : 0.06479511260986329
			 train-loss:  2.191907842954 	 ± 0.1704078964742967
	data : 0.11396937370300293
	model : 0.06482572555541992
			 train-loss:  2.1898547453539714 	 ± 0.16767695135677016
	data : 0.11379680633544922
	model : 0.06475911140441895
			 train-loss:  2.1813116196928353 	 ± 0.1708497522284065
	data : 0.11377873420715331
	model : 0.06477375030517578
			 train-loss:  2.1855685750643414 	 ± 0.16953518310556984
	data : 0.11377100944519043
	model : 0.06476669311523438
			 train-loss:  2.1737327498774373 	 ± 0.17893466406487252
	data : 0.11371064186096191
	model : 0.06484308242797851
			 train-loss:  2.166793428361416 	 ± 0.18030486194933246
	data : 0.11372256278991699
	model : 0.06485514640808106
			 train-loss:  2.167739268505212 	 ± 0.17763255363847377
	data : 0.11399931907653808
	model : 0.06492404937744141
			 train-loss:  2.1700961239197674 	 ± 0.1755237661780875
	data : 0.11401400566101075
	model : 0.06484298706054688
			 train-loss:  2.1631816795894077 	 ± 0.17763409404461208
	data : 0.11374173164367676
	model : 0.06480507850646973
			 train-loss:  2.1594970491197376 	 ± 0.17650085693734865
	data : 0.11378026008605957
	model : 0.06479716300964355
			 train-loss:  2.1564786885235763 	 ± 0.17503876612349303
	data : 0.11380839347839355
	model : 0.06479015350341796
			 train-loss:  2.1532063421450163 	 ± 0.1738634429023023
	data : 0.1137721061706543
	model : 0.0647505760192871
			 train-loss:  2.147222665640024 	 ± 0.17553909458720268
	data : 0.11381096839904785
	model : 0.06484618186950683
			 train-loss:  2.143278482556343 	 ± 0.17507235936317508
	data : 0.11406145095825196
	model : 0.06487822532653809
			 train-loss:  2.143301693404593 	 ± 0.17292421320807128
	data : 0.11411838531494141
	model : 0.06486754417419434
			 train-loss:  2.1460536008789424 	 ± 0.1717594400964159
	data : 0.11412501335144043
	model : 0.06481151580810547
			 train-loss:  2.156205934147502 	 ± 0.18205538434525495
	data : 0.11413969993591308
	model : 0.06480550765991211
			 train-loss:  2.1579905341972005 	 ± 0.1803547416163215
	data : 0.11402716636657714
	model : 0.06476917266845703
			 train-loss:  2.158509129948086 	 ± 0.1783727151848175
	data : 0.11397786140441894
	model : 0.06477999687194824
			 train-loss:  2.1591660017552585 	 ± 0.17647824803088158
	data : 0.11404585838317871
	model : 0.0647690773010254
			 train-loss:  2.1593399377579385 	 ± 0.1745947113491321
	data : 0.11401185989379883
	model : 0.0648270606994629
			 train-loss:  2.1662790402770042 	 ± 0.17919639494876582
	data : 0.1139650821685791
	model : 0.06480722427368164
			 train-loss:  2.172703195591362 	 ± 0.1828577681228972
	data : 0.11398720741271973
	model : 0.06474719047546387
			 train-loss:  2.1746067786216736 	 ± 0.18150972944980298
	data : 0.11399941444396973
	model : 0.06470909118652343
			 train-loss:  2.1735903164919685 	 ± 0.17986507700152016
	data : 0.114129638671875
	model : 0.06466937065124512
			 train-loss:  2.169149169555077 	 ± 0.18092875897924565
	data : 0.11419367790222168
	model : 0.06469483375549316
			 train-loss:  2.173481954718536 	 ± 0.18191692516731747
	data : 0.11427407264709473
	model : 0.06472291946411132
			 train-loss:  2.171155090685244 	 ± 0.18101899848686767
	data : 0.11442794799804687
	model : 0.06480379104614258
			 train-loss:  2.170765499635176 	 ± 0.17938866854025365
	data : 0.11447062492370605
	model : 0.06484518051147461
			 train-loss:  2.1924693882465363 	 ± 0.23982051462153015
	data : 0.114231538772583
	model : 0.06478796005249024
			 train-loss:  2.190134274332147 	 ± 0.23834894209919857
	data : 0.11413717269897461
	model : 0.06475796699523925
			 train-loss:  2.1876335102936317 	 ± 0.23703839238269322
	data : 0.11404876708984375
	model : 0.06480970382690429
			 train-loss:  2.182218278868724 	 ± 0.2386120459429621
	data : 0.11402735710144044
	model : 0.06481504440307617
			 train-loss:  2.184952022631963 	 ± 0.23754517030960964
	data : 0.11399297714233399
	model : 0.06483945846557618
			 train-loss:  2.1843246885987577 	 ± 0.23564014179328135
	data : 0.1141097068786621
	model : 0.0649418830871582
			 train-loss:  2.188025084234053 	 ± 0.23551212266051083
	data : 0.11420269012451172
	model : 0.06491665840148926
			 train-loss:  2.184277224162268 	 ± 0.2354918853700323
	data : 0.11418561935424805
	model : 0.06488122940063476
			 train-loss:  2.1850217916071415 	 ± 0.23371959132536638
	data : 0.11407222747802734
	model : 0.06477079391479493
			 train-loss:  2.187804864003108 	 ± 0.2329810648692133
	data : 0.11401133537292481
	model : 0.06470155715942383
			 train-loss:  2.186143539168618 	 ± 0.23159695525409835
	data : 0.11395139694213867
	model : 0.06470932960510253
			 train-loss:  2.185438675666923 	 ± 0.22993343853990908
	data : 0.11391177177429199
	model : 0.06474385261535645
			 train-loss:  2.185767222853268 	 ± 0.22825233270189751
	data : 0.11397089958190917
	model : 0.06473298072814941
			 train-loss:  2.185261494871499 	 ± 0.22663066716692462
	data : 0.1140815258026123
	model : 0.06478343009948731
			 train-loss:  2.19360157762255 	 ± 0.23542972583761132
	data : 0.11422548294067383
	model : 0.064793062210083
			 train-loss:  2.194865085709263 	 ± 0.23400479339027777
	data : 0.11412701606750489
	model : 0.06468615531921387
			 train-loss:  2.1991530093881817 	 ± 0.2351661938397679
	data : 0.11433048248291015
	model : 0.06466207504272461
			 train-loss:  2.199427709187547 	 ± 0.23356154349724925
	data : 0.11428780555725097
	model : 0.06467962265014648
			 train-loss:  2.2018211403408565 	 ± 0.23287765119082374
	data : 0.11432781219482421
	model : 0.06470122337341308
			 train-loss:  2.2044916089375812 	 ± 0.23245780449814354
	data : 0.11426200866699218
	model : 0.06474118232727051
			 train-loss:  2.2074403323625265 	 ± 0.23233111136675158
	data : 0.11436948776245118
	model : 0.06482834815979004
			 train-loss:  2.2031612876173736 	 ± 0.23381255879086305
	data : 0.1142054557800293
	model : 0.06483588218688965
			 train-loss:  2.2088718674121757 	 ± 0.23765197749607053
	data : 0.11436634063720703
	model : 0.06474175453186035
			 train-loss:  2.2071243554730957 	 ± 0.23664687068361642
	data : 0.11418557167053223
	model : 0.06474323272705078
			 train-loss:  2.2069998368620873 	 ± 0.23516578093478746
	data : 0.11423192024230958
	model : 0.06475529670715333
			 train-loss:  2.2087218246342224 	 ± 0.23421659094125813
	data : 0.1142228126525879
	model : 0.06479444503784179
			 train-loss:  2.210558130973723 	 ± 0.23336999024742938
	data : 0.11436986923217773
	model : 0.06485099792480468
			 train-loss:  2.2075555353279572 	 ± 0.23354799651670016
	data : 0.11424427032470703
	model : 0.06498494148254394
			 train-loss:  2.204710846855527 	 ± 0.23359576700987236
	data : 0.11439881324768067
	model : 0.06501669883728027
			 train-loss:  2.2041821171255673 	 ± 0.23226816505327724
	data : 0.11419377326965333
	model : 0.06500020027160644
			 train-loss:  2.2059608725614326 	 ± 0.23149542085849717
	data : 0.11415891647338867
	model : 0.06496524810791016
			 train-loss:  2.2084802211016075 	 ± 0.23134390729356683
	data : 0.1139035701751709
	model : 0.06496424674987793
			 train-loss:  2.2093192393129524 	 ± 0.23015878295253234
	data : 0.11391615867614746
	model : 0.06495022773742676
			 train-loss:  2.210533873418744 	 ± 0.2291455693603241
	data : 0.11384444236755371
	model : 0.06497697830200196
			 train-loss:  2.208023634221819 	 ± 0.22909624381781993
	data : 0.1140625
	model : 0.06505556106567383
			 train-loss:  2.206868644599076 	 ± 0.22809732529116844
	data : 0.11412258148193359
	model : 0.06508831977844239
			 train-loss:  2.2046412916287133 	 ± 0.22784715061420271
	data : 0.114650297164917
	model : 0.06503238677978515
			 train-loss:  2.2018394277941797 	 ± 0.22820680549024577
	data : 0.11471824645996094
	model : 0.065032958984375
			 train-loss:  2.207625349785419 	 ± 0.23374702605530512
	data : 0.11472225189208984
	model : 0.0649369239807129
			 train-loss:  2.203412273055629 	 ± 0.23607422616235038
	data : 0.11456117630004883
	model : 0.0648740291595459
			 train-loss:  2.207387341807286 	 ± 0.23801600763170314
	data : 0.11449761390686035
	model : 0.0648465633392334
			 train-loss:  2.2118197335410366 	 ± 0.24073555581323794
	data : 0.11400136947631836
	model : 0.06487574577331542
			 train-loss:  2.2140680004139335 	 ± 0.24052557196451743
	data : 0.11399016380310059
	model : 0.06488375663757324
			 train-loss:  2.2102226452393965 	 ± 0.24231649489494117
	data : 0.11399579048156738
	model : 0.06498203277587891
			 train-loss:  2.2058372890949247 	 ± 0.24501839597071065
	data : 0.11421294212341308
	model : 0.0650148868560791
			 train-loss:  2.2029091261401037 	 ± 0.24555453978950673
	data : 0.11424994468688965
	model : 0.06501684188842774
			 train-loss:  2.2021539994314607 	 ± 0.2444656959361251
	data : 0.11465835571289062
	model : 0.06496491432189941
			 train-loss:  2.199640591167709 	 ± 0.2445968215547394
	data : 0.11497015953063965
	model : 0.0649418830871582
			 train-loss:  2.202241315291478 	 ± 0.2448448643030628
	data : 0.11498174667358399
	model : 0.06485648155212402
			 train-loss:  2.2013719490596224 	 ± 0.24383737941788858
	data : 0.11475410461425781
	model : 0.06486496925354004
			 train-loss:  2.200025641693259 	 ± 0.24307626933326051
	data : 0.11515598297119141
	model : 0.06489348411560059
			 train-loss:  2.1984350948690254 	 ± 0.24249129753693668
	data : 0.11482768058776856
	model : 0.06496129035949708
			 train-loss:  2.1993993322054544 	 ± 0.24157203850139541
	data : 0.11452698707580566
	model : 0.06498532295227051
			 train-loss:  2.1975216821793024 	 ± 0.2412517886501687
	data : 0.11474599838256835
	model : 0.06508989334106445
			 train-loss:  2.1992692817341197 	 ± 0.24084478388747804
	data : 0.11491260528564454
	model : 0.0650559425354004
			 train-loss:  2.2004974511292605 	 ± 0.24010321835609272
	data : 0.11465616226196289
	model : 0.06499238014221191
			 train-loss:  2.2009363153151105 	 ± 0.23907364155847738
	data : 0.11458640098571778
	model : 0.06494345664978027
			 train-loss:  2.199900226255434 	 ± 0.2382658783270111
	data : 0.11437268257141113
	model : 0.06486024856567382
			 train-loss:  2.2035183300051773 	 ± 0.2403162230256078
	data : 0.11408624649047852
	model : 0.06481308937072754
			 train-loss:  2.2010086443113246 	 ± 0.240764881715058
	data : 0.11437945365905762
	model : 0.06483874320983887
			 train-loss:  2.2023901682475517 	 ± 0.2401822150707476
	data : 0.11428341865539551
	model : 0.06486706733703614
			 train-loss:  2.204639278925382 	 ± 0.24037725927542633
	data : 0.11439895629882812
	model : 0.06490488052368164
			 train-loss:  2.20339697599411 	 ± 0.2397334423947765
	data : 0.11472339630126953
	model : 0.06498899459838867
			 train-loss:  2.204158876122547 	 ± 0.23886745784081803
	data : 0.11483478546142578
	model : 0.06503486633300781
			 train-loss:  2.203531707326571 	 ± 0.23796846311716602
	data : 0.11452617645263671
	model : 0.0650251865386963
			 train-loss:  2.204624433162784 	 ± 0.2372852023020921
	data : 0.11451096534729004
	model : 0.06504850387573242
			 train-loss:  2.20430698648828 	 ± 0.23633651919187304
	data : 0.11436090469360352
	model : 0.0649932861328125
			 train-loss:  2.2039352112669284 	 ± 0.23540965882512993
	data : 0.11412315368652344
	model : 0.06491622924804688
			 train-loss:  2.2048312935136978 	 ± 0.2346690328308979
	data : 0.11412434577941895
	model : 0.06488103866577148
			 train-loss:  2.2019636478424074 	 ± 0.23589976494338377
	data : 0.1140824794769287
	model : 0.06488046646118165
			 train-loss:  2.2038112102992953 	 ± 0.2358680322495827
	data : 0.11449332237243652
	model : 0.06487803459167481
			 train-loss:  2.201994765461899 	 ± 0.2358206972143
	data : 0.1147270679473877
	model : 0.06498641967773437
			 train-loss:  2.203686303459108 	 ± 0.23566994370005065
	data : 0.11478657722473144
	model : 0.06504812240600585
			 train-loss:  2.2036296609760253 	 ± 0.23475559184065772
	data : 0.1147892951965332
	model : 0.06502838134765625
			 train-loss:  2.2033341692044184 	 ± 0.23387502445340452
	data : 0.11522955894470215
	model : 0.06495356559753418
			 train-loss:  2.2011698557220343 	 ± 0.23428389095082305
	data : 0.11470575332641601
	model : 0.06491260528564453
			 train-loss:  2.2024212488622377 	 ± 0.23383383042640002
	data : 0.11454644203186035
	model : 0.0648007869720459
			 train-loss:  2.201500807489668 	 ± 0.2331930049110203
	data : 0.11461396217346191
	model : 0.06477952003479004
			 train-loss:  2.2039773010495884 	 ± 0.23407019455939118
	data : 0.11467180252075196
	model : 0.06477737426757812
			 train-loss:  2.2075276966448185 	 ± 0.23679553203915377
	data : 0.11425814628601075
	model : 0.06482748985290528
			 train-loss:  2.207111755714697 	 ± 0.2359728477814401
	data : 0.11433463096618653
	model : 0.06486854553222657
			 train-loss:  2.2061965857109014 	 ± 0.2353521684614799
	data : 0.11435055732727051
	model : 0.06496481895446778
			 train-loss:  2.204531399236209 	 ± 0.23530648445897903
	data : 0.11437110900878907
	model : 0.06497960090637207
			 train-loss:  2.2058879908897895 	 ± 0.23499950962980212
	data : 0.11423430442810059
	model : 0.06492705345153808
			 train-loss:  2.202656343153545 	 ± 0.23723819239837318
	data : 0.11418418884277344
	model : 0.06529850959777832
			 train-loss:  2.201155965209853 	 ± 0.23706108042526858
	data : 0.11389241218566895
	model : 0.06533503532409668
			 train-loss:  2.2022929426649926 	 ± 0.23661037339968216
	data : 0.11393489837646484
	model : 0.06528182029724121
			 train-loss:  2.2014078860516313 	 ± 0.23601737478027712
	data : 0.11379084587097169
	model : 0.06529874801635742
			 train-loss:  2.2015791601604886 	 ± 0.23520535994518296
	data : 0.1138735294342041
	model : 0.06538138389587403
			 train-loss:  2.2007363911332756 	 ± 0.23461097741759418
	data : 0.11398091316223144
	model : 0.06501474380493164
			 train-loss:  2.200773325684952 	 ± 0.23380655764958166
	data : 0.11426677703857421
	model : 0.06496520042419433
			 train-loss:  2.201157942921126 	 ± 0.23305628085900423
	data : 0.11418304443359376
	model : 0.06493077278137208
			 train-loss:  2.2008792468019434 	 ± 0.23229217155267448
	data : 0.11409664154052734
	model : 0.06481847763061524
			 train-loss:  2.1994246428444884 	 ± 0.2321866852305646
	data : 0.11399631500244141
	model : 0.06480569839477539
			 train-loss:  2.2008595848083496 	 ± 0.2320733766014738
	data : 0.11399459838867188
	model : 0.06480193138122559
			 train-loss:  2.204813374588821 	 ± 0.2363180998771181
	data : 0.1140815258026123
	model : 0.06482424736022949
			 train-loss:  2.2076542048077834 	 ± 0.23811226888516687
	data : 0.11414642333984375
	model : 0.06486773490905762
			 train-loss:  2.2071569729474634 	 ± 0.2374120077324356
	data : 0.11429572105407715
	model : 0.06498241424560547
			 train-loss:  2.2079364107800767 	 ± 0.23683625037055095
	data : 0.1145087718963623
	model : 0.06497063636779785
			 train-loss:  2.21112812565219 	 ± 0.23937070450953074
	data : 0.11450886726379395
	model : 0.06491947174072266
			 train-loss:  2.2096053812748346 	 ± 0.23935422177484245
	data : 0.11430349349975585
	model : 0.06481738090515136
			 train-loss:  2.207857875307654 	 ± 0.23958698926809632
	data : 0.11426987648010253
	model : 0.06483602523803711
			 train-loss:  2.2131988572168955 	 ± 0.2480266475171099
	data : 0.11432228088378907
	model : 0.06483826637268067
			 train-loss:  2.2121429795739034 	 ± 0.24760142931235923
	data : 0.11419858932495117
	model : 0.06484932899475097
			 train-loss:  2.2125217504799366 	 ± 0.2468726669566126
	data : 0.11418981552124023
	model : 0.06489176750183105
			 train-loss:  2.2098786660603116 	 ± 0.24836527462620722
	data : 0.11437397003173828
	model : 0.06502866744995117
			 train-loss:  2.2097978054741283 	 ± 0.24759965430467012
	data : 0.11440277099609375
	model : 0.06502494812011719
			 train-loss:  2.2097782978982283 	 ± 0.24683910256145639
	data : 0.11439051628112792
	model : 0.06502485275268555
			 train-loss:  2.207487247339109 	 ± 0.24781766223720586
	data : 0.11432600021362305
	model : 0.06503939628601074
			 train-loss:  2.2059715617786755 	 ± 0.24782685036976987
	data : 0.11412339210510254
	model : 0.06497483253479004
			 train-loss:  2.2073828556451454 	 ± 0.2477434110819915
	data : 0.11412711143493652
	model : 0.06487360000610351
			 train-loss:  2.207547146403147 	 ± 0.24700962042161154
	data : 0.11406569480895996
	model : 0.06488213539123536
			 train-loss:  2.211629799434117 	 ± 0.25186135232690654
	data : 0.11402029991149902
	model : 0.06484532356262207
			 train-loss:  2.2131661084982066 	 ± 0.25190337850292505
	data : 0.1141082763671875
	model : 0.06482596397399902
			 train-loss:  2.2116360713453855 	 ± 0.25194776615969317
	data : 0.1142878532409668
	model : 0.06492066383361816
			 train-loss:  2.2092445238291867 	 ± 0.2531378622549436
	data : 0.11422414779663086
	model : 0.06502251625061035
			 train-loss:  2.2102873554063396 	 ± 0.25276904158254565
	data : 0.11426339149475098
	model : 0.0649789810180664
			 train-loss:  2.2098616595902194 	 ± 0.2520992635573522
	data : 0.1142695426940918
	model : 0.06497979164123535
			 train-loss:  2.2089521206658462 	 ± 0.25165830356411495
	data : 0.11419310569763183
	model : 0.06490101814270019
			 train-loss:  2.2150314896447316 	 ± 0.26344036080466826
	data : 0.11415672302246094
	model : 0.06481513977050782
			 train-loss:  2.213033915920691 	 ± 0.26401667272348167
	data : 0.11412539482116699
	model : 0.06477880477905273
			 train-loss:  2.2123213487829867 	 ± 0.26343947192076367
	data : 0.11418805122375489
	model : 0.0648261547088623
			 train-loss:  2.2119286797019875 	 ± 0.2627503704721906
	data : 0.11423072814941407
	model : 0.06483912467956543
			 train-loss:  2.212011682254642 	 ± 0.2620177431136686
	data : 0.11430082321166993
	model : 0.0649350643157959
			 train-loss:  2.2119198812378777 	 ± 0.2612917890142667
	data : 0.11435980796813965
	model : 0.06500263214111328
			 train-loss:  2.214156128424966 	 ± 0.26229056821302327
	data : 0.1143991470336914
	model : 0.06494693756103516
			 train-loss:  2.215124147278922 	 ± 0.26189300915493824
	data : 0.1141824722290039
	model : 0.06483678817749024
			 train-loss:  2.2148371446328086 	 ± 0.2612051725124807
	data : 0.11406750679016113
	model : 0.06483683586120606
			 train-loss:  2.2147700164629067 	 ± 0.2604959916388307
	data : 0.11407852172851562
	model : 0.06481614112854003
			 train-loss:  2.213954005370269 	 ± 0.260026694054801
	data : 0.11393175125122071
	model : 0.06483683586120606
			 train-loss:  2.2136765539005236 	 ± 0.2593542120389111
	data : 0.11395373344421386
	model : 0.0649139404296875
			 train-loss:  2.2134326345780315 	 ± 0.2586812128745219
	data : 0.11409869194030761
	model : 0.0650209903717041
			 train-loss:  2.2135568299192063 	 ± 0.25799790360085467
	data : 0.11420469284057617
	model : 0.06502304077148438
			 train-loss:  2.2138841354026995 	 ± 0.25735359670903235
	data : 0.11404366493225097
	model : 0.06498875617980956
			 train-loss:  2.215041312418486 	 ± 0.2571679845492989
	data : 0.11400384902954101
	model : 0.06487140655517579
			 train-loss:  2.2158063344306345 	 ± 0.25671056256526437
	data : 0.11393685340881347
	model : 0.06484074592590332
			 train-loss:  2.215072608242432 	 ± 0.2562418930870364
	data : 0.1140169620513916
	model : 0.06483855247497558
			 train-loss:  2.2157058431694545 	 ± 0.25572776618814963
	data : 0.11407008171081542
	model : 0.06485419273376465
			 train-loss:  2.214907701482478 	 ± 0.25530871680525435
	data : 0.11419196128845215
	model : 0.06493182182312011
			 train-loss:  2.2131687848995893 	 ± 0.25580245234850174
	data : 0.11430845260620118
	model : 0.06502747535705566
			 train-loss:  2.2126755848222848 	 ± 0.2552419952828595
	data : 0.11441693305969239
	model : 0.06503310203552246
			 train-loss:  2.211915880290385 	 ± 0.25481541323589596
	data : 0.11418967247009278
	model : 0.0649796962738037
			 train-loss:  2.211096110970083 	 ± 0.2544314235561938
	data : 0.11408834457397461
	model : 0.06490378379821778
			 train-loss:  2.2109667643829805 	 ± 0.2537978697248916
	data : 0.11410441398620605
	model : 0.0649034023284912
			 train-loss:  2.210837255716324 	 ± 0.25316917191862376
	data : 0.11417174339294434
	model : 0.06492815017700196
			 train-loss:  2.2123248778765476 	 ± 0.2534134068947437
	data : 0.11410541534423828
	model : 0.06491913795471191
			 train-loss:  2.2143563350828566 	 ± 0.2544207787133357
	data : 0.11424965858459472
	model : 0.06497879028320312
			 train-loss:  2.214346951451795 	 ± 0.2537933879447117
	data : 0.11425619125366211
	model : 0.06506023406982422
			 train-loss:  2.213471044512356 	 ± 0.25347798206151456
	data : 0.11425714492797852
	model : 0.06503372192382813
			 train-loss:  2.213251028991327 	 ± 0.2528785132127807
	data : 0.1141469955444336
	model : 0.06497840881347657
			 train-loss:  2.2133655339768787 	 ± 0.25226931108774314
	data : 0.11404962539672851
	model : 0.06491436958312988
			 train-loss:  2.2114153202029243 	 ± 0.25321108466863446
	data : 0.11409220695495606
	model : 0.06486186981201172
			 train-loss:  2.2105647571958027 	 ± 0.25289792366031555
	data : 0.11412267684936524
	model : 0.06484613418579102
			 train-loss:  2.2102676892394655 	 ± 0.2523285548352433
	data : 0.11408958435058594
	model : 0.06483936309814453
			 train-loss:  2.210683258942195 	 ± 0.2517987380007378
	data : 0.11418614387512208
	model : 0.06490249633789062
			 train-loss:  2.2094649488892038 	 ± 0.25182100161147175
	data : 0.11432423591613769
	model : 0.06499872207641602
			 train-loss:  2.208866930232858 	 ± 0.25137651884595835
	data : 0.1142880916595459
	model : 0.06502003669738769
			 train-loss:  2.20794926115045 	 ± 0.25114142538393397
	data : 0.11432967185974122
	model : 0.06504368782043457
			 train-loss:  2.208517297406063 	 ± 0.25069107320416245
	data : 0.11425132751464843
	model : 0.06500115394592285
			 train-loss:  2.207145673175191 	 ± 0.25091097363429604
	data : 0.11403126716613769
	model : 0.06494278907775879
			 train-loss:  2.208418384746269 	 ± 0.2510241186236257
	data : 0.11403927803039551
	model : 0.064910888671875
			 train-loss:  2.2065734363371328 	 ± 0.2519086211795526
	data : 0.11416730880737305
	model : 0.06495513916015624
			 train-loss:  2.2080632716143898 	 ± 0.25228657729136483
	data : 0.11427741050720215
	model : 0.06494827270507812
			 train-loss:  2.207075376488847 	 ± 0.2521321851750404
	data : 0.11442527770996094
	model : 0.06499724388122559
			 train-loss:  2.2061620024117556 	 ± 0.2519213817540593
	data : 0.11463871002197265
	model : 0.06516766548156738
			 train-loss:  2.205136134613693 	 ± 0.25181092604473926
	data : 0.11452789306640625
	model : 0.06514630317687989
			 train-loss:  2.2055030873229913 	 ± 0.2513023599325792
	data : 0.11447491645812988
	model : 0.06502070426940917
			 train-loss:  2.2040241769611035 	 ± 0.2517046531044356
	data : 0.11433901786804199
	model : 0.06492114067077637
			 train-loss:  2.205009801047189 	 ± 0.2515731125629956
	data : 0.1143571376800537
	model : 0.06473069190979004
			 train-loss:  2.204991431766086 	 ± 0.25101358920783146
	data : 0.11433486938476563
	model : 0.06439857482910157
			 train-loss:  2.204137497243628 	 ± 0.25078496213394236
	data : 0.11465091705322265
	model : 0.06420807838439942
			 train-loss:  2.2038222577603377 	 ± 0.2502768349064081
	data : 0.1146890640258789
	model : 0.06412329673767089
			 train-loss:  2.203719402614393 	 ± 0.24973218720787077
	data : 0.11487441062927246
	model : 0.06398892402648926
			 train-loss:  2.2025748663073546 	 ± 0.24978489874284235
	data : 0.114982271194458
	model : 0.0639566421508789
			 train-loss:  2.202591633796692 	 ± 0.24924142573587202
	data : 0.11494255065917969
	model : 0.06385011672973633
			 train-loss:  2.2008546059265797 	 ± 0.2500926550617181
	data : 0.11479659080505371
	model : 0.06388130187988281
			 train-loss:  2.2010080177208473 	 ± 0.2495639726918121
	data : 0.11501507759094239
	model : 0.06382827758789063
			 train-loss:  2.201019113155905 	 ± 0.24902790917593845
	data : 0.11489906311035156
	model : 0.06383814811706542
			 train-loss:  2.2016521396799984 	 ± 0.24868302509718207
	data : 0.11486697196960449
	model : 0.06382913589477539
			 train-loss:  2.200861630541213 	 ± 0.2484478052184509
	data : 0.11515307426452637
	model : 0.06392836570739746
			 train-loss:  2.20076246079752 	 ± 0.24792553496698738
	data : 0.11517100334167481
	model : 0.06385369300842285
			 train-loss:  2.2012524836174046 	 ± 0.24751643387275235
	data : 0.11482758522033691
	model : 0.06379766464233398
			 train-loss:  2.2003997033383667 	 ± 0.24734454948714482
	data : 0.11487388610839844
	model : 0.0637364387512207
			 train-loss:  2.2005088000117983 	 ± 0.24683228802739524
	data : 0.11490998268127442
	model : 0.0637420654296875
			 train-loss:  2.1992782284816106 	 ± 0.2470510848785045
	data : 0.11475286483764649
	model : 0.06377363204956055
			 train-loss:  2.198988996600709 	 ± 0.24657871290369923
	data : 0.1147963523864746
	model : 0.0638082504272461
			 train-loss:  2.198609881164614 	 ± 0.24613909923589236
	data : 0.11498942375183105
	model : 0.06387848854064941
			 train-loss:  2.198931172060868 	 ± 0.2456829635124476
	data : 0.11492486000061035
	model : 0.06391706466674804
			 train-loss:  2.1981482554654606 	 ± 0.24548256539976374
	data : 0.11487874984741211
	model : 0.06387934684753419
			 train-loss:  2.1981389259805484 	 ± 0.24498111166281575
	data : 0.11487288475036621
	model : 0.06384024620056153
			 train-loss:  2.1978827627693733 	 ± 0.24451555252765578
	data : 0.11499834060668945
	model : 0.0639035701751709
			 train-loss:  2.197052170873171 	 ± 0.24436757257453814
	data : 0.115028715133667
	model : 0.06396212577819824
			 train-loss:  2.1965641340901776 	 ± 0.24399498481067414
	data : 0.11508278846740723
	model : 0.0640113353729248
			 train-loss:  2.1977464683563355 	 ± 0.24421536559073853
	data : 0.11510505676269531
	model : 0.06402993202209473
			 train-loss:  2.1979643592834472 	 ± 0.24375069609957814
	data : 0.1151247501373291
	model : 0.06400556564331054
			 train-loss:  2.197893497003502 	 ± 0.2432672325866554
	data : 0.1147873878479004
	model : 0.06387052536010743
			 train-loss:  2.1974956043182856 	 ± 0.24286590400282054
	data : 0.11482696533203125
	model : 0.06377410888671875
			 train-loss:  2.197608546306022 	 ± 0.24239208748548313
	data : 0.11493134498596191
	model : 0.06371212005615234
			 train-loss:  2.1970463963005487 	 ± 0.24207965756937255
	data : 0.11497573852539063
	model : 0.06371860504150391
			 train-loss:  2.195930958729164 	 ± 0.24225765972110994
	data : 0.11505494117736817
	model : 0.06376175880432129
			 train-loss:  2.1950575010851026 	 ± 0.24218601812776264
	data : 0.11506214141845703
	model : 0.05544824600219726
#epoch  88    val-loss:  2.402083064380445  train-loss:  2.1950575010851026  lr:  1.9073486328125e-08
			 train-loss:  2.3820502758026123 	 ± 0.0
	data : 5.751308441162109
	model : 0.0711355209350586
			 train-loss:  2.1581791639328003 	 ± 0.223871111869812
	data : 2.940448045730591
	model : 0.0681922435760498
			 train-loss:  2.122025648752848 	 ± 0.1898060496215842
	data : 1.9984827836354573
	model : 0.06704529126485188
			 train-loss:  2.1659764647483826 	 ± 0.1811484889109854
	data : 1.52747243642807
	model : 0.06643933057785034
			 train-loss:  2.131071376800537 	 ± 0.17642358313551515
	data : 1.244856882095337
	model : 0.06608190536499023
			 train-loss:  2.191407640775045 	 ± 0.210095353726157
	data : 0.11742901802062988
	model : 0.06478743553161621
			 train-loss:  2.2413773196084157 	 ± 0.22981764064385143
	data : 0.11430830955505371
	model : 0.06460237503051758
			 train-loss:  2.1994052082300186 	 ± 0.24196226233060028
	data : 0.11424517631530762
	model : 0.06456737518310547
			 train-loss:  2.1763083272510104 	 ± 0.2372938723575913
	data : 0.11412467956542968
	model : 0.06459345817565917
			 train-loss:  2.190203082561493 	 ± 0.22894349018667984
	data : 0.11407704353332519
	model : 0.06464085578918458
			 train-loss:  2.171717177737843 	 ± 0.22598096691273858
	data : 0.11408834457397461
	model : 0.06490159034729004
			 train-loss:  2.186250994602839 	 ± 0.22166490727123672
	data : 0.11393218040466309
	model : 0.0650132179260254
			 train-loss:  2.1863957276711097 	 ± 0.21296934434902845
	data : 0.11393857002258301
	model : 0.06504831314086915
			 train-loss:  2.207329843725477 	 ± 0.21866256037685428
	data : 0.11391921043395996
	model : 0.06497292518615723
			 train-loss:  2.208596698443095 	 ± 0.21130127744238017
	data : 0.11369261741638184
	model : 0.06488289833068847
			 train-loss:  2.2230279222130775 	 ± 0.2120887049696955
	data : 0.1135951042175293
	model : 0.06467061042785645
			 train-loss:  2.2012321528266456 	 ± 0.22346482991966385
	data : 0.11380457878112793
	model : 0.06474418640136718
			 train-loss:  2.196742229991489 	 ± 0.21795639117206522
	data : 0.11377115249633789
	model : 0.06483597755432129
			 train-loss:  2.190933817311337 	 ± 0.21356966953230658
	data : 0.11381263732910156
	model : 0.06497697830200196
			 train-loss:  2.1913270592689513 	 ± 0.20816902195677206
	data : 0.11404542922973633
	model : 0.0650395393371582
			 train-loss:  2.1891969385601224 	 ± 0.20337539159410156
	data : 0.11401753425598145
	model : 0.06501913070678711
			 train-loss:  2.175262673334642 	 ± 0.20870773380647345
	data : 0.11386475563049317
	model : 0.06494169235229492
			 train-loss:  2.1798191018726514 	 ± 0.20523594969344
	data : 0.11386785507202149
	model : 0.06485238075256347
			 train-loss:  2.1749326239029565 	 ± 0.20227680685825652
	data : 0.11386628150939941
	model : 0.06483397483825684
			 train-loss:  2.166862645149231 	 ± 0.202094680036649
	data : 0.11386542320251465
	model : 0.06488666534423829
			 train-loss:  2.173786085385543 	 ± 0.20117095758206838
	data : 0.11395859718322754
	model : 0.06490449905395508
			 train-loss:  2.1784058809280396 	 ± 0.19881091436956486
	data : 0.11407976150512696
	model : 0.06489205360412598
			 train-loss:  2.1670197205884114 	 ± 0.20399644644138426
	data : 0.11413826942443847
	model : 0.06494765281677246
			 train-loss:  2.158880196768662 	 ± 0.20502345577215234
	data : 0.11411848068237304
	model : 0.06490564346313477
			 train-loss:  2.168744806448619 	 ± 0.20845975319678073
	data : 0.11418185234069825
	model : 0.06484971046447754
			 train-loss:  2.1652201414108276 	 ± 0.20597664260475249
	data : 0.11417684555053711
	model : 0.06481542587280273
			 train-loss:  2.161006275564432 	 ± 0.2040857898890923
	data : 0.11409687995910645
	model : 0.06482353210449218
			 train-loss:  2.1646078969493057 	 ± 0.2019998798978437
	data : 0.11417388916015625
	model : 0.06479697227478028
			 train-loss:  2.1631946388412926 	 ± 0.19917265401810508
	data : 0.11434845924377442
	model : 0.06481165885925293
			 train-loss:  2.154693181174142 	 ± 0.2024689294560592
	data : 0.11426506042480469
	model : 0.06485462188720703
			 train-loss:  2.1531055569648743 	 ± 0.19985788371723245
	data : 0.1142350673675537
	model : 0.06488676071166992
			 train-loss:  2.1574242179458207 	 ± 0.19883424862254115
	data : 0.11432089805603027
	model : 0.06486625671386718
			 train-loss:  2.1564486465956034 	 ± 0.19629028633872142
	data : 0.11412734985351562
	model : 0.06482048034667968
			 train-loss:  2.1602101509387674 	 ± 0.19513992345615275
	data : 0.1140207290649414
	model : 0.06479134559631347
			 train-loss:  2.155706825852394 	 ± 0.19472677658663232
	data : 0.11398134231567383
	model : 0.06477823257446289
			 train-loss:  2.155309552099647 	 ± 0.1923538112086105
	data : 0.11403751373291016
	model : 0.06481709480285644
			 train-loss:  2.1470774554070973 	 ± 0.1972244929745536
	data : 0.11403656005859375
	model : 0.06488561630249023
			 train-loss:  2.152372069137041 	 ± 0.1979148575581312
	data : 0.11415576934814453
	model : 0.06492791175842286
			 train-loss:  2.1508784863081845 	 ± 0.19589788389654186
	data : 0.11417813301086426
	model : 0.06497111320495605
			 train-loss:  2.1530417415830825 	 ± 0.19423976670126886
	data : 0.11418743133544922
	model : 0.06493563652038574
			 train-loss:  2.152881111787713 	 ± 0.19211988578041425
	data : 0.11416416168212891
	model : 0.06490964889526367
			 train-loss:  2.1557679303148958 	 ± 0.19107088073336204
	data : 0.11415705680847169
	model : 0.06487832069396973
			 train-loss:  2.1530676558613777 	 ± 0.18997419849432504
	data : 0.11402153968811035
	model : 0.06494278907775879
			 train-loss:  2.1545151326121115 	 ± 0.1882929381275751
	data : 0.11406435966491699
	model : 0.06496996879577636
			 train-loss:  2.1566334700584413 	 ± 0.18698937472763022
	data : 0.11411795616149903
	model : 0.06500186920166015
			 train-loss:  2.154686126054502 	 ± 0.1856584093013234
	data : 0.1141808032989502
	model : 0.06502761840820312
			 train-loss:  2.1553434477402615 	 ± 0.1839244800375849
	data : 0.11411943435668945
	model : 0.06500163078308105
			 train-loss:  2.151195636335409 	 ± 0.1846200742419164
	data : 0.11423335075378419
	model : 0.06491637229919434
			 train-loss:  2.14933842420578 	 ± 0.18340170553781548
	data : 0.11417918205261231
	model : 0.06494612693786621
			 train-loss:  2.143761058287187 	 ± 0.18629116231112514
	data : 0.11421351432800293
	model : 0.06498737335205078
			 train-loss:  2.146505900791713 	 ± 0.1857392112561694
	data : 0.11421260833740235
	model : 0.06495451927185059
			 train-loss:  2.1440528652124238 	 ± 0.18501562502975422
	data : 0.11439380645751954
	model : 0.06499018669128417
			 train-loss:  2.1531527330135476 	 ± 0.19585869517336452
	data : 0.11440963745117187
	model : 0.06503591537475586
			 train-loss:  2.1471087366847668 	 ± 0.19957249821922599
	data : 0.11438703536987305
	model : 0.06501588821411133
			 train-loss:  2.1450342615445455 	 ± 0.19854285613952885
	data : 0.1142848014831543
	model : 0.06494135856628418
			 train-loss:  2.1487157579328193 	 ± 0.1989629442699522
	data : 0.11412091255187988
	model : 0.06486468315124512
			 train-loss:  2.1521044931104107 	 ± 0.19911870914151641
	data : 0.11399073600769043
	model : 0.06484737396240234
			 train-loss:  2.149320222082592 	 ± 0.1987449549153864
	data : 0.11402349472045899
	model : 0.06483306884765624
			 train-loss:  2.1554202307015657 	 ± 0.20304337750616674
	data : 0.1140221118927002
	model : 0.06479711532592773
			 train-loss:  2.171787586578956 	 ± 0.24028595302451497
	data : 0.11421256065368653
	model : 0.06484808921813964
			 train-loss:  2.1763094392689792 	 ± 0.24122934298018828
	data : 0.11440362930297851
	model : 0.06494507789611817
			 train-loss:  2.178445066978682 	 ± 0.2400501706091851
	data : 0.11444940567016601
	model : 0.06494054794311524
			 train-loss:  2.178738280254252 	 ± 0.2382906453538406
	data : 0.11436972618103028
	model : 0.06489224433898926
			 train-loss:  2.181767589804055 	 ± 0.23787289585539215
	data : 0.11424975395202637
	model : 0.06477828025817871
			 train-loss:  2.188750674043383 	 ± 0.24318687830365388
	data : 0.1140012264251709
	model : 0.06473493576049805
			 train-loss:  2.1935269849401124 	 ± 0.24475257351142218
	data : 0.11400866508483887
	model : 0.06471967697143555
			 train-loss:  2.1952586289909153 	 ± 0.24348454710468354
	data : 0.11403112411499024
	model : 0.06472997665405274
			 train-loss:  2.1963133305719453 	 ± 0.24197664687574147
	data : 0.11411385536193848
	model : 0.06477370262145996
			 train-loss:  2.195381359474079 	 ± 0.24046798244630788
	data : 0.11429972648620605
	model : 0.06489176750183105
			 train-loss:  2.194475038846334 	 ± 0.23898668883011243
	data : 0.11439380645751954
	model : 0.06491527557373047
			 train-loss:  2.1927022573195005 	 ± 0.23790509794092313
	data : 0.114231538772583
	model : 0.0648524284362793
			 train-loss:  2.1890298948659526 	 ± 0.23851360654731912
	data : 0.11446089744567871
	model : 0.06486339569091797
			 train-loss:  2.1873237719902625 	 ± 0.2374521710507958
	data : 0.11432185173034667
	model : 0.06487908363342285
			 train-loss:  2.188007034832918 	 ± 0.23602167712965544
	data : 0.11434683799743653
	model : 0.06489858627319336
			 train-loss:  2.1853413000702857 	 ± 0.23573563451100996
	data : 0.11446342468261719
	model : 0.06495933532714844
			 train-loss:  2.1830947561028564 	 ± 0.2351360900864629
	data : 0.1146315097808838
	model : 0.06505799293518066
			 train-loss:  2.183218570744119 	 ± 0.23370059206661314
	data : 0.11449308395385742
	model : 0.06508846282958984
			 train-loss:  2.184858516038182 	 ± 0.23276270217380718
	data : 0.11463818550109864
	model : 0.06509599685668946
			 train-loss:  2.1863963334333327 	 ± 0.23179684835233175
	data : 0.11465964317321778
	model : 0.06503686904907227
			 train-loss:  2.185518137146445 	 ± 0.23056983122788982
	data : 0.11451692581176758
	model : 0.06497445106506347
			 train-loss:  2.1887638056000998 	 ± 0.23117028355453106
	data : 0.11456365585327148
	model : 0.06488327980041504
			 train-loss:  2.1914903939455406 	 ± 0.23122456400192967
	data : 0.11429314613342285
	model : 0.06482172012329102
			 train-loss:  2.1956677342003044 	 ± 0.23318535415709465
	data : 0.1142077922821045
	model : 0.06477599143981934
			 train-loss:  2.194947806636939 	 ± 0.23196995443064555
	data : 0.11403412818908691
	model : 0.06483798027038574
			 train-loss:  2.192329368326399 	 ± 0.2319964950451754
	data : 0.11410460472106934
	model : 0.0648730754852295
			 train-loss:  2.190548363622728 	 ± 0.2313361127341802
	data : 0.11403455734252929
	model : 0.06496057510375977
			 train-loss:  2.1922276603138964 	 ± 0.23063243589806864
	data : 0.11423649787902831
	model : 0.06496567726135254
			 train-loss:  2.1941567992651336 	 ± 0.2301342127937079
	data : 0.1142052173614502
	model : 0.06497311592102051
			 train-loss:  2.19227301947614 	 ± 0.22962655597378273
	data : 0.11434917449951172
	model : 0.06491880416870117
			 train-loss:  2.1929325944498967 	 ± 0.2285042966119508
	data : 0.11417293548583984
	model : 0.06491889953613281
			 train-loss:  2.1916136456032596 	 ± 0.22767428486283778
	data : 0.11412911415100098
	model : 0.06489906311035157
			 train-loss:  2.192586351915733 	 ± 0.22669808929940552
	data : 0.11407690048217774
	model : 0.06496725082397461
			 train-loss:  2.191822705220203 	 ± 0.22566386827694287
	data : 0.11414079666137696
	model : 0.06500062942504883
			 train-loss:  2.1950320679732043 	 ± 0.22675801349460425
	data : 0.11460556983947753
	model : 0.06504154205322266
			 train-loss:  2.1926105320453644 	 ± 0.22690421879266828
	data : 0.1147308349609375
	model : 0.06505365371704101
			 train-loss:  2.1907640152638503 	 ± 0.22653196079135743
	data : 0.1147536277770996
	model : 0.06505746841430664
			 train-loss:  2.189005612158308 	 ± 0.2261104026815074
	data : 0.11468186378479003
	model : 0.06495957374572754
			 train-loss:  2.1912593806831584 	 ± 0.2261584672375643
	data : 0.1146852970123291
	model : 0.06491832733154297
			 train-loss:  2.189889466533294 	 ± 0.2254975486037677
	data : 0.11410245895385743
	model : 0.06491131782531738
			 train-loss:  2.190490240142459 	 ± 0.22450479609355453
	data : 0.11427807807922363
	model : 0.06489081382751465
			 train-loss:  2.1946558963577703 	 ± 0.22748393729139718
	data : 0.11469688415527343
	model : 0.06487970352172852
			 train-loss:  2.1946296680753474 	 ± 0.22641859392729854
	data : 0.11484808921813965
	model : 0.06495432853698731
			 train-loss:  2.1932677858405643 	 ± 0.22580778467373994
	data : 0.11485271453857422
	model : 0.0649724006652832
			 train-loss:  2.190109419166495 	 ± 0.2271534623242774
	data : 0.11487407684326172
	model : 0.06498141288757324
			 train-loss:  2.1883977391503073 	 ± 0.22682365370587035
	data : 0.11472458839416504
	model : 0.06493358612060547
			 train-loss:  2.1901134649912515 	 ± 0.22651550568352785
	data : 0.11417431831359863
	model : 0.06484146118164062
			 train-loss:  2.1888319743531093 	 ± 0.2259058255574011
	data : 0.11398868560791016
	model : 0.06482644081115722
			 train-loss:  2.1893404606169304 	 ± 0.2249683914852742
	data : 0.11403546333312989
	model : 0.06482601165771484
			 train-loss:  2.187512058960764 	 ± 0.22482123549040184
	data : 0.11409010887145996
	model : 0.06483120918273926
			 train-loss:  2.1886941059775973 	 ± 0.22419713329710655
	data : 0.11414437294006348
	model : 0.06489777565002441
			 train-loss:  2.19039765925243 	 ± 0.22397495833133368
	data : 0.11429500579833984
	model : 0.0649754524230957
			 train-loss:  2.1912881574060163 	 ± 0.22322188429205267
	data : 0.11439757347106934
	model : 0.064976167678833
			 train-loss:  2.1926632533639165 	 ± 0.22277112101751814
	data : 0.11425585746765136
	model : 0.06495432853698731
			 train-loss:  2.1895323060139886 	 ± 0.22442520596532034
	data : 0.11410861015319824
	model : 0.06489968299865723
			 train-loss:  2.187699880202611 	 ± 0.22438031806538153
	data : 0.11410932540893555
	model : 0.06486301422119141
			 train-loss:  2.191123988017563 	 ± 0.22657754048761025
	data : 0.11410665512084961
	model : 0.06488051414489746
			 train-loss:  2.1903330693479446 	 ± 0.22581469277900748
	data : 0.11451206207275391
	model : 0.06487584114074707
			 train-loss:  2.188414143352974 	 ± 0.22589143835126327
	data : 0.11467690467834472
	model : 0.06494064331054687
			 train-loss:  2.1880554537619314 	 ± 0.22501390909378272
	data : 0.11525578498840332
	model : 0.06497163772583008
			 train-loss:  2.187056833267212 	 ± 0.22438776109682662
	data : 0.11523361206054687
	model : 0.06499133110046387
			 train-loss:  2.1864619028000605 	 ± 0.22359451690721896
	data : 0.11523780822753907
	model : 0.06498546600341797
			 train-loss:  2.185252390508577 	 ± 0.22312592490704924
	data : 0.11485981941223145
	model : 0.06494898796081543
			 train-loss:  2.1874186657369137 	 ± 0.2235893796717274
	data : 0.11463789939880371
	model : 0.06479358673095703
			 train-loss:  2.1882412914157836 	 ± 0.22291544013343462
	data : 0.11417865753173828
	model : 0.06477570533752441
			 train-loss:  2.1883688083061803 	 ± 0.22206114100160348
	data : 0.11415157318115235
	model : 0.06479368209838868
			 train-loss:  2.187849104859447 	 ± 0.22129130408893366
	data : 0.11406121253967286
	model : 0.06482963562011719
			 train-loss:  2.187423998659307 	 ± 0.22050517300347866
	data : 0.11409225463867187
	model : 0.06488480567932128
			 train-loss:  2.186015148808185 	 ± 0.22027017415488992
	data : 0.11428875923156738
	model : 0.06500487327575684
			 train-loss:  2.1859843624171926 	 ± 0.2194470185859803
	data : 0.11429667472839355
	model : 0.06501479148864746
			 train-loss:  2.1859987806390833 	 ± 0.2186328048328012
	data : 0.11425423622131348
	model : 0.06494560241699218
			 train-loss:  2.186817467212677 	 ± 0.21803512095668287
	data : 0.11412134170532226
	model : 0.06480803489685058
			 train-loss:  2.186958062387731 	 ± 0.21724410231645172
	data : 0.11393647193908692
	model : 0.06480512619018555
			 train-loss:  2.184947781804679 	 ± 0.2177306948430021
	data : 0.11379318237304688
	model : 0.06481833457946777
			 train-loss:  2.184594848173128 	 ± 0.21698569072154586
	data : 0.11383757591247559
	model : 0.06484713554382324
			 train-loss:  2.182082509994507 	 ± 0.21822884898520897
	data : 0.1139418601989746
	model : 0.06491856575012207
			 train-loss:  2.1819035804018063 	 ± 0.21746391672633916
	data : 0.11420502662658691
	model : 0.06500868797302246
			 train-loss:  2.1811647045780234 	 ± 0.21687438803382292
	data : 0.11438422203063965
	model : 0.06495237350463867
			 train-loss:  2.181667051115236 	 ± 0.21619764472932723
	data : 0.11436967849731446
	model : 0.06493310928344727
			 train-loss:  2.1858776725000806 	 ± 0.22125126648427884
	data : 0.11430740356445312
	model : 0.0648658275604248
			 train-loss:  2.1859948815970585 	 ± 0.2204914972083838
	data : 0.1141270637512207
	model : 0.06484284400939941
			 train-loss:  2.189116745778959 	 ± 0.22292753492674308
	data : 0.11407003402709961
	model : 0.06484661102294922
			 train-loss:  2.1889298302786693 	 ± 0.22217946349026857
	data : 0.11397161483764648
	model : 0.06489214897155762
			 train-loss:  2.186821082153836 	 ± 0.2228987611694447
	data : 0.11409544944763184
	model : 0.06488990783691406
			 train-loss:  2.1867946442341646 	 ± 0.2221497523471099
	data : 0.11414484977722168
	model : 0.06493930816650391
			 train-loss:  2.1856793403625487 	 ± 0.22182617194012477
	data : 0.11430501937866211
	model : 0.06497611999511718
			 train-loss:  2.185285674025681 	 ± 0.2211429929996122
	data : 0.11409778594970703
	model : 0.06494841575622559
			 train-loss:  2.183002134687022 	 ± 0.22219334458954038
	data : 0.11404552459716796
	model : 0.0649075984954834
			 train-loss:  2.1828605100220324 	 ± 0.22147291517708106
	data : 0.11401243209838867
	model : 0.06493754386901855
			 train-loss:  2.1816891648552637 	 ± 0.22122763751303953
	data : 0.11402244567871093
	model : 0.06494808197021484
			 train-loss:  2.182297489720006 	 ± 0.22064202685149026
	data : 0.11403384208679199
	model : 0.06491708755493164
			 train-loss:  2.1811619125879727 	 ± 0.22038764054762056
	data : 0.11419806480407715
	model : 0.0649634838104248
			 train-loss:  2.1806578362823292 	 ± 0.21977484616973933
	data : 0.11427440643310546
	model : 0.06497960090637207
			 train-loss:  2.17995428586308 	 ± 0.21925554256628574
	data : 0.11415762901306152
	model : 0.06493573188781739
			 train-loss:  2.1784025700587146 	 ± 0.21943355017995472
	data : 0.11413908004760742
	model : 0.06490297317504883
			 train-loss:  2.176943913847208 	 ± 0.2195186540346183
	data : 0.11391968727111816
	model : 0.06493000984191895
			 train-loss:  2.1767149349177104 	 ± 0.21885502372060547
	data : 0.11392488479614257
	model : 0.06494064331054687
			 train-loss:  2.1776727092118913 	 ± 0.21851669977996363
	data : 0.11393671035766602
	model : 0.064963960647583
			 train-loss:  2.1809074286302907 	 ± 0.22170177817037048
	data : 0.11404452323913575
	model : 0.06499261856079101
			 train-loss:  2.1802369203509353 	 ± 0.22119053968926058
	data : 0.11402502059936523
	model : 0.06498732566833496
			 train-loss:  2.179612080978625 	 ± 0.22066437802131275
	data : 0.11409215927124024
	model : 0.06485219001770019
			 train-loss:  2.1795890223549073 	 ± 0.21999892166095814
	data : 0.11414132118225098
	model : 0.06480026245117188
			 train-loss:  2.1796635032413962 	 ± 0.21934135242273828
	data : 0.11411461830139161
	model : 0.06480212211608886
			 train-loss:  2.1782175387654985 	 ± 0.21948444570633244
	data : 0.114056396484375
	model : 0.06485638618469239
			 train-loss:  2.1785449572568814 	 ± 0.21887526620236142
	data : 0.11402850151062012
	model : 0.06489582061767578
			 train-loss:  2.1795789676554063 	 ± 0.2186441650385452
	data : 0.11419143676757812
	model : 0.06499218940734863
			 train-loss:  2.1807997059403803 	 ± 0.2185841765794587
	data : 0.11421175003051758
	model : 0.06504130363464355
			 train-loss:  2.1807886375937353 	 ± 0.21794787925206574
	data : 0.11425037384033203
	model : 0.06500115394592285
			 train-loss:  2.1821119592368947 	 ± 0.218008961279691
	data : 0.11413884162902832
	model : 0.06485538482666016
			 train-loss:  2.182078617742692 	 ± 0.2173820384085622
	data : 0.1141728401184082
	model : 0.06479096412658691
			 train-loss:  2.182886645453317 	 ± 0.21702195386943512
	data : 0.11407017707824707
	model : 0.06481170654296875
			 train-loss:  2.1833745634013955 	 ± 0.21650077222187672
	data : 0.11390585899353027
	model : 0.06479663848876953
			 train-loss:  2.1827217414554227 	 ± 0.21606196929260751
	data : 0.11384468078613282
	model : 0.06484050750732422
			 train-loss:  2.1823328733444214 	 ± 0.21551630438150873
	data : 0.1139364242553711
	model : 0.0649141788482666
			 train-loss:  2.182921923738618 	 ± 0.21505710395757852
	data : 0.1139946460723877
	model : 0.06494488716125488
			 train-loss:  2.1849951320224337 	 ± 0.21624521369779764
	data : 0.1139721393585205
	model : 0.0648564338684082
			 train-loss:  2.1857573841158198 	 ± 0.2158893790427656
	data : 0.11395907402038574
	model : 0.06478252410888671
			 train-loss:  2.186159515118861 	 ± 0.2153634236166797
	data : 0.11400747299194336
	model : 0.0647240161895752
			 train-loss:  2.1868841270280015 	 ± 0.21499654717176808
	data : 0.11414685249328613
	model : 0.06474289894104004
			 train-loss:  2.1860494548859806 	 ± 0.2147086224089147
	data : 0.11408467292785644
	model : 0.06479043960571289
			 train-loss:  2.1864178837956607 	 ± 0.21418585528340017
	data : 0.11415162086486816
	model : 0.06487641334533692
			 train-loss:  2.186416470876304 	 ± 0.2136093117500602
	data : 0.11425933837890626
	model : 0.06496906280517578
			 train-loss:  2.1858191592170595 	 ± 0.21319309171859924
	data : 0.11429190635681152
	model : 0.0649958610534668
			 train-loss:  2.1847077525676566 	 ± 0.21316781928554301
	data : 0.11403126716613769
	model : 0.0650148868560791
			 train-loss:  2.1849089596006603 	 ± 0.2126210342236547
	data : 0.11383819580078125
	model : 0.06501049995422363
			 train-loss:  2.184933501168301 	 ± 0.21206103541583893
	data : 0.11385130882263184
	model : 0.06500349044799805
			 train-loss:  2.1840725113584107 	 ± 0.21183787665428824
	data : 0.11399049758911133
	model : 0.0650184154510498
			 train-loss:  2.1839297941575446 	 ± 0.2112947014566306
	data : 0.11401820182800293
	model : 0.06505351066589356
			 train-loss:  2.183893621894362 	 ± 0.21074719097322747
	data : 0.11422133445739746
	model : 0.06506013870239258
			 train-loss:  2.185121973150784 	 ± 0.21089486916920447
	data : 0.11453762054443359
	model : 0.065059232711792
			 train-loss:  2.187404246819325 	 ± 0.21274177412919712
	data : 0.11456542015075684
	model : 0.06506514549255371
			 train-loss:  2.1875921165456576 	 ± 0.2122145881114319
	data : 0.11439733505249024
	model : 0.06503825187683106
			 train-loss:  2.1898598664908238 	 ± 0.214042973233106
	data : 0.11429038047790527
	model : 0.06496968269348144
			 train-loss:  2.189027296774315 	 ± 0.2138213354877951
	data : 0.11420483589172363
	model : 0.06488137245178223
			 train-loss:  2.187881460141896 	 ± 0.2138919806676869
	data : 0.11401906013488769
	model : 0.0648111343383789
			 train-loss:  2.187285680770874 	 ± 0.21352205074817104
	data : 0.11404008865356445
	model : 0.0648007869720459
			 train-loss:  2.188310744157478 	 ± 0.2134830038557774
	data : 0.11399760246276855
	model : 0.06478800773620605
			 train-loss:  2.1878203590317526 	 ± 0.21306738406019948
	data : 0.1140782356262207
	model : 0.06489253044128418
			 train-loss:  2.1875294899118356 	 ± 0.21258214009831963
	data : 0.1141580581665039
	model : 0.06496400833129883
			 train-loss:  2.1890415070103666 	 ± 0.21315191458723784
	data : 0.11424460411071777
	model : 0.06501131057739258
			 train-loss:  2.189298148271514 	 ± 0.21266298950432586
	data : 0.11408987045288085
	model : 0.064967679977417
			 train-loss:  2.189081415389348 	 ± 0.21216888345907353
	data : 0.11411747932434083
	model : 0.06493778228759765
			 train-loss:  2.1880399345775734 	 ± 0.21218297015063398
	data : 0.11401782035827637
	model : 0.06489653587341308
			 train-loss:  2.186994557770399 	 ± 0.2122059730139077
	data : 0.11399765014648437
	model : 0.06490263938903809
			 train-loss:  2.1859642712123084 	 ± 0.2122185285500547
	data : 0.11400237083435058
	model : 0.06493911743164063
			 train-loss:  2.1877583100682214 	 ± 0.2132953952326042
	data : 0.11415705680847169
	model : 0.06496429443359375
			 train-loss:  2.1895456206742057 	 ± 0.21435986214457411
	data : 0.11419320106506348
	model : 0.06498918533325196
			 train-loss:  2.1895656310162455 	 ± 0.21385389637093846
	data : 0.11445012092590331
	model : 0.06497855186462402
			 train-loss:  2.188081979191919 	 ± 0.2144421530322287
	data : 0.11443724632263183
	model : 0.06495399475097656
			 train-loss:  2.1883001934702153 	 ± 0.21396423602757056
	data : 0.11434731483459473
	model : 0.06482229232788086
			 train-loss:  2.1889736480491107 	 ± 0.21369328165843562
	data : 0.11434612274169922
	model : 0.06481671333312988
			 train-loss:  2.188177376433655 	 ± 0.21351751181813838
	data : 0.11436538696289063
	model : 0.06485543251037598
			 train-loss:  2.187015839436087 	 ± 0.21370787835673058
	data : 0.11426458358764649
	model : 0.06486215591430664
			 train-loss:  2.1854999152892227 	 ± 0.21438336842011313
	data : 0.11429972648620605
	model : 0.06484675407409668
			 train-loss:  2.185734696584205 	 ± 0.21392143709614314
	data : 0.11439838409423828
	model : 0.06493844985961914
			 train-loss:  2.185846551981839 	 ± 0.21344111710817193
	data : 0.1143404483795166
	model : 0.06546554565429688
			 train-loss:  2.186271057948807 	 ± 0.21305073309081624
	data : 0.11386771202087402
	model : 0.06535954475402832
			 train-loss:  2.186950410808529 	 ± 0.21281012341864355
	data : 0.1137080192565918
	model : 0.06520471572875977
			 train-loss:  2.1866884113961804 	 ± 0.21236831604739087
	data : 0.11388921737670898
	model : 0.06509761810302735
			 train-loss:  2.1870524351085936 	 ± 0.2119634673383212
	data : 0.11389217376708985
	model : 0.06496062278747558
			 train-loss:  2.1880767091115314 	 ± 0.21204677774732936
	data : 0.11403250694274902
	model : 0.0642458438873291
			 train-loss:  2.188970687115087 	 ± 0.2120016513558884
	data : 0.1146115779876709
	model : 0.06409807205200195
			 train-loss:  2.18895582165487 	 ± 0.2115342899995755
	data : 0.11490001678466796
	model : 0.0640571117401123
			 train-loss:  2.1889825327354564 	 ± 0.21107027290673058
	data : 0.11493172645568847
	model : 0.0639571189880371
			 train-loss:  2.1895571827367926 	 ± 0.21078758619978405
	data : 0.1150388240814209
	model : 0.06384859085083008
			 train-loss:  2.1901366347851963 	 ± 0.21051155925170384
	data : 0.11492681503295898
	model : 0.06371784210205078
			 train-loss:  2.1914331654965618 	 ± 0.21097370542497237
	data : 0.11478362083435059
	model : 0.06370429992675782
			 train-loss:  2.1917701075816978 	 ± 0.2105808081649348
	data : 0.11465964317321778
	model : 0.06367812156677247
			 train-loss:  2.191079701476854 	 ± 0.2103914048619287
	data : 0.11459097862243653
	model : 0.06366729736328125
			 train-loss:  2.190266040655283 	 ± 0.2103084276993261
	data : 0.11461877822875977
	model : 0.06373329162597656
			 train-loss:  2.189192732851556 	 ± 0.21050175563574938
	data : 0.11472959518432617
	model : 0.0638117790222168
			 train-loss:  2.188799846475407 	 ± 0.21014163135325248
	data : 0.11486420631408692
	model : 0.0637366771697998
			 train-loss:  2.18774683012741 	 ± 0.2103208626142631
	data : 0.1149064540863037
	model : 0.06369228363037109
			 train-loss:  2.1881381178102575 	 ± 0.20996497471357037
	data : 0.11491956710815429
	model : 0.0637178897857666
			 train-loss:  2.186714478616435 	 ± 0.21067320541420165
	data : 0.11486301422119141
	model : 0.06371383666992188
			 train-loss:  2.186424606045087 	 ± 0.21028160100537646
	data : 0.11492252349853516
	model : 0.06377983093261719
			 train-loss:  2.1858805314139214 	 ± 0.21001408812878028
	data : 0.11506447792053223
	model : 0.06385369300842285
			 train-loss:  2.1868742192087094 	 ± 0.2101466832376591
	data : 0.11505208015441895
	model : 0.06383590698242188
			 train-loss:  2.187120703512749 	 ± 0.209748887890063
	data : 0.11488051414489746
	model : 0.06371855735778809
			 train-loss:  2.1870826003981416 	 ± 0.20931947603801565
	data : 0.11483192443847656
	model : 0.06369857788085938
			 train-loss:  2.1892533779144285 	 ± 0.21162609214657802
	data : 0.11488652229309082
	model : 0.06365652084350586
			 train-loss:  2.1897098950254237 	 ± 0.2113163682959219
	data : 0.11482667922973633
	model : 0.06366243362426757
			 train-loss:  2.1883484380930542 	 ± 0.21196649879139473
	data : 0.11490254402160645
	model : 0.06375112533569335
			 train-loss:  2.187251083793179 	 ± 0.2122405760702946
	data : 0.11512155532836914
	model : 0.06386775970458984
			 train-loss:  2.186995524957956 	 ± 0.2118521918922161
	data : 0.11514420509338379
	model : 0.06380763053894042
			 train-loss:  2.188525438785553 	 ± 0.21280188967287567
	data : 0.11496548652648926
	model : 0.06370444297790527
			 train-loss:  2.191043247264695 	 ± 0.21607653094077686
	data : 0.11483745574951172
	model : 0.06369690895080567
			 train-loss:  2.1902704461226388 	 ± 0.2159946660684313
	data : 0.11474170684814453
	model : 0.06366190910339356
			 train-loss:  2.19091187588311 	 ± 0.2158077262498441
	data : 0.11467385292053223
	model : 0.06364293098449707
			 train-loss:  2.1902099448864853 	 ± 0.2156716751067676
	data : 0.11472849845886231
	model : 0.06372237205505371
			 train-loss:  2.190327629856035 	 ± 0.21525654548180595
	data : 0.11476430892944336
	model : 0.06382408142089843
			 train-loss:  2.1922440915368497 	 ± 0.2170045005606055
	data : 0.11451191902160644
	model : 0.05534963607788086
#epoch  89    val-loss:  2.431829207821896  train-loss:  2.1922440915368497  lr:  1.9073486328125e-08
			 train-loss:  2.075108528137207 	 ± 0.0
	data : 5.309719562530518
	model : 0.08049750328063965
			 train-loss:  2.042752981185913 	 ± 0.032355546951293945
	data : 2.8149776458740234
	model : 0.07998597621917725
			 train-loss:  2.1800034046173096 	 ± 0.19589098603527808
	data : 1.9779308636983235
	model : 0.07477005322774251
			 train-loss:  2.1101465821266174 	 ± 0.20837438842836073
	data : 1.5118457078933716
	model : 0.07222539186477661
			 train-loss:  2.066539931297302 	 ± 0.205771884834916
	data : 1.232315969467163
	model : 0.07071042060852051
			 train-loss:  2.0793828765551248 	 ± 0.19002569134612327
	data : 0.19313602447509765
	model : 0.06750054359436035
			 train-loss:  2.0582066433770314 	 ± 0.18341707351481673
	data : 0.15182938575744628
	model : 0.06453137397766114
			 train-loss:  2.0733751207590103 	 ± 0.17620208268423054
	data : 0.11378130912780762
	model : 0.06460385322570801
			 train-loss:  2.134230891863505 	 ± 0.23921723801276773
	data : 0.11388707160949707
	model : 0.06476359367370606
			 train-loss:  2.157247173786163 	 ± 0.23721328244451456
	data : 0.11394782066345215
	model : 0.06478028297424317
			 train-loss:  2.1795178868553857 	 ± 0.23688503662561128
	data : 0.11394481658935547
	model : 0.06484322547912598
			 train-loss:  2.1649148960908255 	 ± 0.23191384447891894
	data : 0.11395196914672852
	model : 0.06489195823669433
			 train-loss:  2.163505581709055 	 ± 0.22286909182609801
	data : 0.11399812698364258
	model : 0.0649033546447754
			 train-loss:  2.152683504990169 	 ± 0.2182779312660133
	data : 0.11387867927551269
	model : 0.06478033065795899
			 train-loss:  2.1681662162144977 	 ± 0.21868906187073583
	data : 0.11374425888061523
	model : 0.0648033618927002
			 train-loss:  2.1822601929306984 	 ± 0.21866744582970196
	data : 0.11383638381958008
	model : 0.06480321884155274
			 train-loss:  2.1866532143424537 	 ± 0.2128651132067299
	data : 0.11390104293823242
	model : 0.0647963047027588
			 train-loss:  2.186913867791494 	 ± 0.20687049744763952
	data : 0.11390113830566406
	model : 0.06481852531433105
			 train-loss:  2.1897706797248437 	 ± 0.20171742075810106
	data : 0.11396288871765137
	model : 0.06483097076416015
			 train-loss:  2.2179748594760893 	 ± 0.23188242990534363
	data : 0.1139561653137207
	model : 0.06473851203918457
			 train-loss:  2.203390462057931 	 ± 0.23550605877935266
	data : 0.11376333236694336
	model : 0.06479382514953613
			 train-loss:  2.199402549050071 	 ± 0.2308160003247941
	data : 0.11369500160217286
	model : 0.06480708122253417
			 train-loss:  2.188183286915655 	 ± 0.23179486101217173
	data : 0.11369743347167968
	model : 0.06483211517333984
			 train-loss:  2.182073255379995 	 ± 0.22879860654172546
	data : 0.11374588012695312
	model : 0.0648796558380127
			 train-loss:  2.176827621459961 	 ± 0.22564407917463755
	data : 0.11374244689941407
	model : 0.06501445770263672
			 train-loss:  2.171418620989873 	 ± 0.2229089584440396
	data : 0.11393132209777831
	model : 0.0649641513824463
			 train-loss:  2.1925680990572327 	 ± 0.24388092071747083
	data : 0.11398873329162598
	model : 0.06487703323364258
			 train-loss:  2.1804947427340915 	 ± 0.24756690578411752
	data : 0.11392669677734375
	model : 0.06480998992919922
			 train-loss:  2.1740288035622957 	 ± 0.24565541014261746
	data : 0.11389775276184082
	model : 0.06480331420898437
			 train-loss:  2.1634334643681843 	 ± 0.2481745414308976
	data : 0.1139596939086914
	model : 0.06482987403869629
			 train-loss:  2.1552706956863403 	 ± 0.24819897893707812
	data : 0.11396241188049316
	model : 0.06488933563232421
			 train-loss:  2.1663164235651493 	 ± 0.25191248275972306
	data : 0.11399173736572266
	model : 0.0649557113647461
			 train-loss:  2.176243388291561 	 ± 0.25434287850535386
	data : 0.11411619186401367
	model : 0.06498799324035645
			 train-loss:  2.1698686410399044 	 ± 0.2532364078322802
	data : 0.11415023803710937
	model : 0.06493563652038574
			 train-loss:  2.1628268786839078 	 ± 0.25294736306672366
	data : 0.11404099464416503
	model : 0.0648463249206543
			 train-loss:  2.1528937353028192 	 ± 0.25623902372400825
	data : 0.11386919021606445
	model : 0.06479043960571289
			 train-loss:  2.15270379427317 	 ± 0.25275518533464525
	data : 0.11379761695861816
	model : 0.06478385925292969
			 train-loss:  2.1517422450216195 	 ± 0.24947585858374005
	data : 0.11384782791137696
	model : 0.06479129791259766
			 train-loss:  2.1485830270327053 	 ± 0.24702554047410227
	data : 0.11388258934020996
	model : 0.0648160457611084
			 train-loss:  2.159035933017731 	 ± 0.252502165739422
	data : 0.1140479564666748
	model : 0.0648388385772705
			 train-loss:  2.1639239264697565 	 ± 0.2513125260290266
	data : 0.11411890983581544
	model : 0.06485323905944824
			 train-loss:  2.1776891946792603 	 ± 0.263482468369219
	data : 0.11412525177001953
	model : 0.06479706764221191
			 train-loss:  2.1770085955775063 	 ± 0.26043804933724385
	data : 0.1138789176940918
	model : 0.06474132537841797
			 train-loss:  2.179020952094685 	 ± 0.25779946564099954
	data : 0.11384387016296386
	model : 0.06472663879394532
			 train-loss:  2.1776360299852158 	 ± 0.25508440849796893
	data : 0.11389408111572266
	model : 0.06473722457885742
			 train-loss:  2.1738721132278442 	 ± 0.25355679962296834
	data : 0.11402220726013183
	model : 0.06477251052856445
			 train-loss:  2.17717596825133 	 ± 0.25184373529914383
	data : 0.11404914855957031
	model : 0.06485452651977539
			 train-loss:  2.173395335674286 	 ± 0.2505507670436166
	data : 0.11427121162414551
	model : 0.06488752365112305
			 train-loss:  2.172757645042575 	 ± 0.2480203007172874
	data : 0.11428422927856445
	model : 0.06487097740173339
			 train-loss:  2.1672101974487306 	 ± 0.24857940932839148
	data : 0.114048433303833
	model : 0.06479344367980958
			 train-loss:  2.1688643530303358 	 ± 0.2464080596688203
	data : 0.11399993896484376
	model : 0.06473250389099121
			 train-loss:  2.1717529388574452 	 ± 0.24489760900563085
	data : 0.11394538879394531
	model : 0.0646998405456543
			 train-loss:  2.175241132952132 	 ± 0.24387691170131334
	data : 0.11384110450744629
	model : 0.06474995613098145
			 train-loss:  2.171602507432302 	 ± 0.24305604330297043
	data : 0.11387262344360352
	model : 0.06477870941162109
			 train-loss:  2.1845565297386864 	 ± 0.25896657737928064
	data : 0.1140378475189209
	model : 0.0648277759552002
			 train-loss:  2.1850001237222125 	 ± 0.2566650443633785
	data : 0.11409001350402832
	model : 0.06480803489685058
			 train-loss:  2.1788954651146604 	 ± 0.25847273182469915
	data : 0.11420249938964844
	model : 0.06476879119873047
			 train-loss:  2.1770847871385772 	 ± 0.2565992322828351
	data : 0.11424036026000976
	model : 0.06469597816467285
			 train-loss:  2.1856274483567577 	 ± 0.2626020747510095
	data : 0.11420307159423829
	model : 0.06470026969909667
			 train-loss:  2.1848450541496276 	 ± 0.26047386651415705
	data : 0.11421365737915039
	model : 0.0647387981414795
			 train-loss:  2.1903958985062895 	 ± 0.26188376734893676
	data : 0.11425538063049316
	model : 0.06480236053466797
			 train-loss:  2.1848399908311906 	 ± 0.2633626457221429
	data : 0.11426811218261719
	model : 0.06487083435058594
			 train-loss:  2.1845047019776844 	 ± 0.26127744370599126
	data : 0.11439871788024902
	model : 0.0649183750152588
			 train-loss:  2.1784762982279062 	 ± 0.26360723040748957
	data : 0.11442255973815918
	model : 0.06493239402770996
			 train-loss:  2.173139555637653 	 ± 0.2650329912820229
	data : 0.11434755325317383
	model : 0.06490249633789062
			 train-loss:  2.1710696166211907 	 ± 0.2635464066525478
	data : 0.11425848007202148
	model : 0.0648740291595459
			 train-loss:  2.1720524990736547 	 ± 0.2616940981211539
	data : 0.114139986038208
	model : 0.06490020751953125
			 train-loss:  2.171594326986986 	 ± 0.25978982085961116
	data : 0.11402168273925781
	model : 0.06491217613220215
			 train-loss:  2.169668842053068 	 ± 0.25838872589095274
	data : 0.11405835151672364
	model : 0.06491012573242187
			 train-loss:  2.1700573801994323 	 ± 0.25655675420558494
	data : 0.11418123245239258
	model : 0.0649104118347168
			 train-loss:  2.1742386700401846 	 ± 0.25713445808921553
	data : 0.11417274475097657
	model : 0.06490378379821778
			 train-loss:  2.178250268101692 	 ± 0.25757022233103716
	data : 0.11436924934387208
	model : 0.06482806205749511
			 train-loss:  2.1895752263395756 	 ± 0.2732543797635058
	data : 0.11431655883789063
	model : 0.06472969055175781
			 train-loss:  2.193739364276061 	 ± 0.2737238624850667
	data : 0.11431736946105957
	model : 0.06464638710021972
			 train-loss:  2.18842587629954 	 ± 0.2757081965045011
	data : 0.1142209529876709
	model : 0.06467342376708984
			 train-loss:  2.1909594394658742 	 ± 0.27476577807034674
	data : 0.11429796218872071
	model : 0.06468820571899414
			 train-loss:  2.1925995922707893 	 ± 0.27334997701803593
	data : 0.1142049789428711
	model : 0.06478056907653809
			 train-loss:  2.1944789107029257 	 ± 0.272092282203243
	data : 0.11435728073120117
	model : 0.06489424705505371
			 train-loss:  2.1924044225789325 	 ± 0.27098476195456334
	data : 0.11436934471130371
	model : 0.0650094985961914
			 train-loss:  2.1866478219628336 	 ± 0.2741035744897084
	data : 0.11445274353027343
	model : 0.06503920555114746
			 train-loss:  2.1846112454379045 	 ± 0.27301468093719394
	data : 0.11440773010253906
	model : 0.0650747299194336
			 train-loss:  2.180557732175036 	 ± 0.27378629724422265
	data : 0.11435799598693848
	model : 0.06500921249389649
			 train-loss:  2.183145745691047 	 ± 0.2731392274752374
	data : 0.11429743766784668
	model : 0.06495046615600586
			 train-loss:  2.181279569864273 	 ± 0.2720403264047062
	data : 0.11412806510925293
	model : 0.0649540901184082
			 train-loss:  2.183610011549557 	 ± 0.2712774992363571
	data : 0.11411843299865723
	model : 0.06492557525634765
			 train-loss:  2.183825660583585 	 ± 0.26970302106706096
	data : 0.11405143737792969
	model : 0.06491627693176269
			 train-loss:  2.1857386334189055 	 ± 0.2687347108914009
	data : 0.11407208442687988
	model : 0.06492466926574707
			 train-loss:  2.185210798274387 	 ± 0.2672487996102602
	data : 0.1140101432800293
	model : 0.06496624946594239
			 train-loss:  2.191095715158441 	 ± 0.2714167738228049
	data : 0.11409125328063965
	model : 0.06487188339233399
			 train-loss:  2.1897779054111903 	 ± 0.2701908613773825
	data : 0.11399621963500976
	model : 0.06478919982910156
			 train-loss:  2.188970745264829 	 ± 0.2688112820579071
	data : 0.1140439510345459
	model : 0.06475620269775391
			 train-loss:  2.187096965053807 	 ± 0.2679432434545045
	data : 0.11411290168762207
	model : 0.06481008529663086
			 train-loss:  2.189345020119862 	 ± 0.2673696932364775
	data : 0.11431741714477539
	model : 0.06482920646667481
			 train-loss:  2.1893408006810127 	 ± 0.26594371448466836
	data : 0.11435127258300781
	model : 0.06489429473876954
			 train-loss:  2.1904031715895 	 ± 0.26474075213778003
	data : 0.11442275047302246
	model : 0.0649646282196045
			 train-loss:  2.1901633826394877 	 ± 0.26336865487431066
	data : 0.11445331573486328
	model : 0.0650259017944336
			 train-loss:  2.190361373203317 	 ± 0.2620147488648246
	data : 0.11446280479431152
	model : 0.06498031616210938
			 train-loss:  2.188096741024329 	 ± 0.2616269691631597
	data : 0.1142463207244873
	model : 0.06494512557983398
			 train-loss:  2.187489467437821 	 ± 0.26037167825027374
	data : 0.1142397403717041
	model : 0.06492414474487304
			 train-loss:  2.1863045299053194 	 ± 0.25933468789414327
	data : 0.11416063308715821
	model : 0.06494216918945313
			 train-loss:  2.185772748276739 	 ± 0.25810244782099767
	data : 0.11421003341674804
	model : 0.06490225791931152
			 train-loss:  2.1848591180408703 	 ± 0.256998197870236
	data : 0.11411609649658203
	model : 0.06490697860717773
			 train-loss:  2.186192314601639 	 ± 0.2561017891534325
	data : 0.11429772377014161
	model : 0.06494150161743165
			 train-loss:  2.185808783540359 	 ± 0.2548972778232426
	data : 0.1143803596496582
	model : 0.06496992111206054
			 train-loss:  2.1855651480810985 	 ± 0.25369274451559687
	data : 0.11440186500549317
	model : 0.06501708030700684
			 train-loss:  2.1885951993600377 	 ± 0.25439509794074455
	data : 0.11433143615722656
	model : 0.06503214836120605
			 train-loss:  2.1941455424388994 	 ± 0.25957177044764457
	data : 0.11437401771545411
	model : 0.06498994827270507
			 train-loss:  2.1930476040751845 	 ± 0.2586167500009015
	data : 0.11467676162719727
	model : 0.06498408317565918
			 train-loss:  2.195614293080951 	 ± 0.2588059383331696
	data : 0.11468977928161621
	model : 0.0649336814880371
			 train-loss:  2.195354311032729 	 ± 0.2576411600229699
	data : 0.11459121704101563
	model : 0.06486120223999023
			 train-loss:  2.195625733684849 	 ± 0.25649378618476165
	data : 0.11472148895263672
	model : 0.06484150886535645
			 train-loss:  2.198129380387919 	 ± 0.2567049578899913
	data : 0.11477351188659668
	model : 0.06488189697265626
			 train-loss:  2.200456183568566 	 ± 0.2567501542760336
	data : 0.11435675621032715
	model : 0.06488957405090331
			 train-loss:  2.1982759174547697 	 ± 0.2566701049879981
	data : 0.11428728103637695
	model : 0.06494331359863281
			 train-loss:  2.20041968718819 	 ± 0.2565747309331221
	data : 0.11449990272521973
	model : 0.06498236656188965
			 train-loss:  2.2057994747984 	 ± 0.2618996575775659
	data : 0.11444463729858398
	model : 0.0649630069732666
			 train-loss:  2.2077015342875423 	 ± 0.2615814355227615
	data : 0.11426877975463867
	model : 0.06490120887756348
			 train-loss:  2.210479344351817 	 ± 0.2621979683103012
	data : 0.11438961029052734
	model : 0.06483273506164551
			 train-loss:  2.210653641644646 	 ± 0.2611008368064573
	data : 0.11436958312988281
	model : 0.06481847763061524
			 train-loss:  2.2092583100001018 	 ± 0.2604557934728734
	data : 0.1143259048461914
	model : 0.06482858657836914
			 train-loss:  2.208418653031026 	 ± 0.2595403341061458
	data : 0.11471171379089355
	model : 0.06489567756652832
			 train-loss:  2.206498264289293 	 ± 0.2593362281099373
	data : 0.11495084762573242
	model : 0.06504282951354981
			 train-loss:  2.2095053438248673 	 ± 0.2604067523261632
	data : 0.11482768058776856
	model : 0.06509585380554199
			 train-loss:  2.206449563464811 	 ± 0.2615594710784102
	data : 0.11483159065246581
	model : 0.06510567665100098
			 train-loss:  2.204905367851257 	 ± 0.26107802095743904
	data : 0.11480913162231446
	model : 0.06506166458129883
			 train-loss:  2.207344739210038 	 ± 0.2614662186022748
	data : 0.11426138877868652
	model : 0.06491785049438477
			 train-loss:  2.2072857317962047 	 ± 0.260435631931494
	data : 0.11410636901855468
	model : 0.06481623649597168
			 train-loss:  2.20610870141536 	 ± 0.25975520785526146
	data : 0.11425666809082032
	model : 0.06486473083496094
			 train-loss:  2.207666420197302 	 ± 0.2593459344843351
	data : 0.11422133445739746
	model : 0.06489200592041015
			 train-loss:  2.207894669129298 	 ± 0.25835953106476045
	data : 0.11442198753356933
	model : 0.06495728492736816
			 train-loss:  2.2088003185869174 	 ± 0.2575785979500407
	data : 0.11462421417236328
	model : 0.06510276794433593
			 train-loss:  2.211949981523283 	 ± 0.2591209692103734
	data : 0.11466307640075683
	model : 0.06511778831481933
			 train-loss:  2.210382085993774 	 ± 0.2587727422544256
	data : 0.11459732055664062
	model : 0.06503515243530274
			 train-loss:  2.2102693470556343 	 ± 0.2578086426280825
	data : 0.11458659172058105
	model : 0.06493778228759765
			 train-loss:  2.2109341541926066 	 ± 0.25696728287694653
	data : 0.11430826187133789
	model : 0.06485586166381836
			 train-loss:  2.210573838914142 	 ± 0.25605503384915307
	data : 0.11412043571472168
	model : 0.06480669975280762
			 train-loss:  2.210268324309022 	 ± 0.2551436925099044
	data : 0.11418700218200684
	model : 0.06481513977050782
			 train-loss:  2.208344540734222 	 ± 0.2552128645121638
	data : 0.11415529251098633
	model : 0.064837646484375
			 train-loss:  2.2093354935268703 	 ± 0.25455948888971247
	data : 0.11421990394592285
	model : 0.06488823890686035
			 train-loss:  2.2106393797057016 	 ± 0.25411412592634736
	data : 0.11432647705078125
	model : 0.0649078369140625
			 train-loss:  2.210246928194736 	 ± 0.25325398316420916
	data : 0.11437897682189942
	model : 0.06488142013549805
			 train-loss:  2.2090963931150838 	 ± 0.25273019795174384
	data : 0.11421451568603516
	model : 0.0648066520690918
			 train-loss:  2.208413902696196 	 ± 0.25197625756734715
	data : 0.11424226760864258
	model : 0.06481122970581055
			 train-loss:  2.209703746769163 	 ± 0.2515731031887215
	data : 0.11422443389892578
	model : 0.06482162475585937
			 train-loss:  2.2092538438994307 	 ± 0.25076223323736235
	data : 0.11417570114135742
	model : 0.06482453346252441
			 train-loss:  2.208943881400644 	 ± 0.2499298545352385
	data : 0.11426334381103516
	model : 0.06485161781311036
			 train-loss:  2.2076792343944107 	 ± 0.24954659515985608
	data : 0.11430854797363281
	model : 0.06495723724365235
			 train-loss:  2.2077729685886487 	 ± 0.24870469999426958
	data : 0.11419072151184081
	model : 0.06496367454528809
			 train-loss:  2.20752828553219 	 ± 0.24788658863684113
	data : 0.11408419609069824
	model : 0.06488323211669922
			 train-loss:  2.207333485285441 	 ± 0.2470703608498079
	data : 0.11410484313964844
	model : 0.06488723754882812
			 train-loss:  2.2065145022032278 	 ± 0.24645508692513837
	data : 0.11403474807739258
	model : 0.06490535736083984
			 train-loss:  2.2066632713142194 	 ± 0.2456498439950419
	data : 0.11412115097045898
	model : 0.06488251686096191
			 train-loss:  2.2092008528366587 	 ± 0.24683642006587111
	data : 0.11420249938964844
	model : 0.06488313674926757
			 train-loss:  2.208166631785306 	 ± 0.24636605144979576
	data : 0.11442704200744629
	model : 0.06495723724365235
			 train-loss:  2.2050389374456096 	 ± 0.2486184695226839
	data : 0.1143620491027832
	model : 0.06500873565673829
			 train-loss:  2.2053330716414328 	 ± 0.24784738837250922
	data : 0.11442160606384277
	model : 0.06498985290527344
			 train-loss:  2.2040647997218334 	 ± 0.2475641186019768
	data : 0.11428899765014648
	model : 0.06490039825439453
			 train-loss:  2.2033004451401625 	 ± 0.24696521918057301
	data : 0.11430201530456544
	model : 0.06485457420349121
			 train-loss:  2.206272449883275 	 ± 0.24900563741099094
	data : 0.1141439437866211
	model : 0.06483712196350097
			 train-loss:  2.207399608939886 	 ± 0.24863284413869924
	data : 0.1141892910003662
	model : 0.06478891372680665
			 train-loss:  2.2084322427370533 	 ± 0.24820342433662837
	data : 0.11413912773132324
	model : 0.06480674743652344
			 train-loss:  2.206823426264304 	 ± 0.24827681568614784
	data : 0.11426177024841308
	model : 0.06491193771362305
			 train-loss:  2.2042431509567915 	 ± 0.24968335186043322
	data : 0.11422133445739746
	model : 0.0649843692779541
			 train-loss:  2.202322626259269 	 ± 0.25012567856604606
	data : 0.11421871185302734
	model : 0.06498937606811524
			 train-loss:  2.2019835710525513 	 ± 0.24940436933324733
	data : 0.11424431800842286
	model : 0.06495747566223145
			 train-loss:  2.201246118689158 	 ± 0.24883238994864182
	data : 0.1142198085784912
	model : 0.06496739387512207
			 train-loss:  2.199778164218286 	 ± 0.24880616202178676
	data : 0.11425609588623047
	model : 0.06493253707885742
			 train-loss:  2.197680285289174 	 ± 0.2495415999152952
	data : 0.11431751251220704
	model : 0.0649219036102295
			 train-loss:  2.1981974120676164 	 ± 0.24889248512280757
	data : 0.11438755989074707
	model : 0.06495413780212403
			 train-loss:  2.1978952597169314 	 ± 0.24819045369337156
	data : 0.11443662643432617
	model : 0.06501150131225586
			 train-loss:  2.19904147044957 	 ± 0.24791454573725358
	data : 0.11445693969726563
	model : 0.06497855186462402
			 train-loss:  2.1977101106976353 	 ± 0.24780514056306696
	data : 0.11428122520446778
	model : 0.06499042510986328
			 train-loss:  2.19531833574262 	 ± 0.24907101945062177
	data : 0.11423606872558593
	model : 0.06498198509216309
			 train-loss:  2.194586320170041 	 ± 0.24854082833319874
	data : 0.11417427062988281
	model : 0.06496005058288574
			 train-loss:  2.1939986862455094 	 ± 0.24795088618121952
	data : 0.1142573356628418
	model : 0.06490797996520996
			 train-loss:  2.1934246590191666 	 ± 0.24736206172423678
	data : 0.1143073558807373
	model : 0.06492052078247071
			 train-loss:  2.19306809417272 	 ± 0.2467076632337741
	data : 0.11450700759887696
	model : 0.06490554809570312
			 train-loss:  2.19455102521382 	 ± 0.246803510298061
	data : 0.11457319259643554
	model : 0.06488385200500488
			 train-loss:  2.1967636373455965 	 ± 0.24787720443905856
	data : 0.11463541984558105
	model : 0.06486191749572753
			 train-loss:  2.200761165221532 	 ± 0.25290753031519925
	data : 0.11440072059631348
	model : 0.06478829383850097
			 train-loss:  2.202656688611152 	 ± 0.2534868400352041
	data : 0.11426029205322266
	model : 0.06474695205688477
			 train-loss:  2.2033221322101553 	 ± 0.25294796894316923
	data : 0.11442952156066895
	model : 0.0647432804107666
			 train-loss:  2.2008009419415164 	 ± 0.25453861741620615
	data : 0.11433157920837403
	model : 0.06480979919433594
			 train-loss:  2.200676358912302 	 ± 0.25385158857621
	data : 0.11438221931457519
	model : 0.06486363410949707
			 train-loss:  2.1997935288661234 	 ± 0.2534476451933589
	data : 0.11453018188476563
	model : 0.06501879692077636
			 train-loss:  2.1981356143951416 	 ± 0.2537693075479611
	data : 0.1146815299987793
	model : 0.06511392593383789
			 train-loss:  2.1982525108969786 	 ± 0.2530948916245777
	data : 0.11471033096313477
	model : 0.06514549255371094
			 train-loss:  2.197333668140655 	 ± 0.25273340535144173
	data : 0.11473650932312011
	model : 0.06507158279418945
			 train-loss:  2.1978795553641346 	 ± 0.2521750151081802
	data : 0.11453824043273926
	model : 0.06496639251708984
			 train-loss:  2.197870878169411 	 ± 0.25151054944983836
	data : 0.11450104713439942
	model : 0.0648867130279541
			 train-loss:  2.1998944856733553 	 ± 0.25239733097252837
	data : 0.11447005271911621
	model : 0.0648238182067871
			 train-loss:  2.198282076045871 	 ± 0.2527235506321152
	data : 0.11415295600891114
	model : 0.06483888626098633
			 train-loss:  2.200197828248375 	 ± 0.25346188082432336
	data : 0.11425848007202148
	model : 0.06487317085266113
			 train-loss:  2.198762004522933 	 ± 0.2535934990464464
	data : 0.11446032524108887
	model : 0.0649756908416748
			 train-loss:  2.199447835408724 	 ± 0.25312273748722314
	data : 0.1144906997680664
	model : 0.06496167182922363
			 train-loss:  2.1991754472255707 	 ± 0.2525048412413151
	data : 0.11448497772216797
	model : 0.06492671966552735
			 train-loss:  2.198626152149917 	 ± 0.2519805245823554
	data : 0.11447477340698242
	model : 0.06486744880676269
			 train-loss:  2.1975987806464685 	 ± 0.2517567060329232
	data : 0.11438517570495606
	model : 0.06480307579040527
			 train-loss:  2.19622991552305 	 ± 0.2518609737547425
	data : 0.11435251235961914
	model : 0.06479372978210449
			 train-loss:  2.196461672782898 	 ± 0.2512518038228873
	data : 0.11431422233581542
	model : 0.0648777961730957
			 train-loss:  2.1969898719692704 	 ± 0.25073731431675295
	data : 0.11431622505187988
	model : 0.0649336814880371
			 train-loss:  2.195889686593915 	 ± 0.25060179389498877
	data : 0.11444025039672852
	model : 0.06506171226501464
			 train-loss:  2.1964064877608727 	 ± 0.250091671494917
	data : 0.11441726684570312
	model : 0.06512360572814942
			 train-loss:  2.1945548314674226 	 ± 0.2508690076011236
	data : 0.11442856788635254
	model : 0.06513619422912598
			 train-loss:  2.1955493252451825 	 ± 0.2506591657166713
	data : 0.11437201499938965
	model : 0.06506590843200684
			 train-loss:  2.1966693725400757 	 ± 0.25056374636365514
	data : 0.11434884071350097
	model : 0.06500358581542968
			 train-loss:  2.1982808239794007 	 ± 0.2510255588298579
	data : 0.11423630714416504
	model : 0.06491379737854004
			 train-loss:  2.1980463908268857 	 ± 0.2504441186518315
	data : 0.11434531211853027
	model : 0.06490201950073242
			 train-loss:  2.1976720819062594 	 ± 0.24990256574812636
	data : 0.11434249877929688
	model : 0.06486239433288574
			 train-loss:  2.1981662193934124 	 ± 0.2494091762123691
	data : 0.11443147659301758
	model : 0.06489205360412598
			 train-loss:  2.198594818748004 	 ± 0.2488949648055206
	data : 0.11436505317687988
	model : 0.064945650100708
			 train-loss:  2.1982357254568137 	 ± 0.24836203547687058
	data : 0.11431660652160644
	model : 0.06494050025939942
			 train-loss:  2.1981990259018303 	 ± 0.24777891629668783
	data : 0.11412587165832519
	model : 0.06491799354553222
			 train-loss:  2.1976367669684866 	 ± 0.2473354775602034
	data : 0.11407704353332519
	model : 0.06495394706726074
			 train-loss:  2.1968636601470237 	 ± 0.24701864479679264
	data : 0.11410684585571289
	model : 0.06495141983032227
			 train-loss:  2.197135651553119 	 ± 0.24647844675567976
	data : 0.11423735618591309
	model : 0.06494779586791992
			 train-loss:  2.197943097435384 	 ± 0.24619603689555525
	data : 0.1144871711730957
	model : 0.06497311592102051
			 train-loss:  2.1985760216319234 	 ± 0.24580760430465048
	data : 0.11454854011535645
	model : 0.06494903564453125
			 train-loss:  2.1977792330528505 	 ± 0.24552776570875598
	data : 0.11455669403076171
	model : 0.0649491786956787
			 train-loss:  2.197132805260745 	 ± 0.24515582670537664
	data : 0.11438236236572266
	model : 0.06489124298095703
			 train-loss:  2.195992685011609 	 ± 0.24518441986575037
	data : 0.11431727409362794
	model : 0.06521930694580078
			 train-loss:  2.1964148320593275 	 ± 0.24471206274450566
	data : 0.1141702651977539
	model : 0.06511874198913574
			 train-loss:  2.1959615731987716 	 ± 0.24425614432918483
	data : 0.11424436569213867
	model : 0.06512980461120606
			 train-loss:  2.196049081959895 	 ± 0.24371382335092365
	data : 0.11439228057861328
	model : 0.06535501480102539
			 train-loss:  2.19855538421207 	 ± 0.24604778192172805
	data : 0.11452393531799317
	model : 0.06526370048522949
			 train-loss:  2.198751900575857 	 ± 0.24552052103531577
	data : 0.11464180946350097
	model : 0.0647824764251709
			 train-loss:  2.1984955933650685 	 ± 0.24500943013839702
	data : 0.11482524871826172
	model : 0.0646895408630371
			 train-loss:  2.2002605571035754 	 ± 0.24591352151634174
	data : 0.11497440338134765
	model : 0.06453161239624024
			 train-loss:  2.202396747326747 	 ± 0.247487006116296
	data : 0.11482367515563965
	model : 0.0640326976776123
			 train-loss:  2.202990882811339 	 ± 0.24711202104731708
	data : 0.1148834228515625
	model : 0.06402120590209961
			 train-loss:  2.204563564552373 	 ± 0.24772740767619988
	data : 0.1148993968963623
	model : 0.06395888328552246
			 train-loss:  2.2037427677162764 	 ± 0.24750752301507128
	data : 0.11476664543151856
	model : 0.06396322250366211
			 train-loss:  2.203106375211298 	 ± 0.24716596558478282
	data : 0.1146432876586914
	model : 0.06396074295043945
			 train-loss:  2.201786254715716 	 ± 0.24745907773682405
	data : 0.11471395492553711
	model : 0.06403498649597168
			 train-loss:  2.200645073423994 	 ± 0.24754828502914245
	data : 0.11469564437866211
	model : 0.06395893096923828
			 train-loss:  2.2001495957374573 	 ± 0.24714000853003978
	data : 0.11462550163269043
	model : 0.06396265029907226
			 train-loss:  2.200950089386244 	 ± 0.24692447513309795
	data : 0.11466464996337891
	model : 0.06396050453186035
			 train-loss:  2.199726011071886 	 ± 0.24712471744226197
	data : 0.11475172042846679
	model : 0.06397504806518554
			 train-loss:  2.20160523268967 	 ± 0.24830543917675607
	data : 0.1149604320526123
	model : 0.06392817497253418
			 train-loss:  2.2017450725038845 	 ± 0.2477970268484035
	data : 0.11494679450988769
	model : 0.06395235061645507
			 train-loss:  2.20202199997249 	 ± 0.2473196031033991
	data : 0.11504011154174805
	model : 0.06396732330322266
			 train-loss:  2.2010589926696023 	 ± 0.24726044822648988
	data : 0.1149686336517334
	model : 0.06384768486022949
			 train-loss:  2.2004076109992132 	 ± 0.2469591338642879
	data : 0.11489558219909668
	model : 0.06380062103271485
			 train-loss:  2.200780388761739 	 ± 0.2465210492358049
	data : 0.11466403007507324
	model : 0.06382575035095214
			 train-loss:  2.1993820419116896 	 ± 0.24698519674143543
	data : 0.11474418640136719
	model : 0.06386246681213378
			 train-loss:  2.198902800315764 	 ± 0.2465968021581443
	data : 0.11472039222717285
	model : 0.06387195587158204
			 train-loss:  2.1986541627389697 	 ± 0.2461280082613613
	data : 0.11491122245788574
	model : 0.06399450302124024
			 train-loss:  2.1983568798149786 	 ± 0.2456757119793074
	data : 0.11497845649719238
	model : 0.06404247283935546
			 train-loss:  2.1979931937642845 	 ± 0.2452487757406216
	data : 0.1149223804473877
	model : 0.06400976181030274
			 train-loss:  2.198491045475006 	 ± 0.24488383037555964
	data : 0.11498432159423828
	model : 0.06396546363830566
			 train-loss:  2.1987642895178015 	 ± 0.24443371138490289
	data : 0.11500849723815917
	model : 0.06395153999328614
			 train-loss:  2.198560332022016 	 ± 0.24396964139998284
	data : 0.11505608558654785
	model : 0.06393389701843262
			 train-loss:  2.197823074495368 	 ± 0.24376812462432634
	data : 0.11502685546875
	model : 0.0639603614807129
			 train-loss:  2.1970865505886827 	 ± 0.24356969111923324
	data : 0.11519832611083984
	model : 0.06402535438537597
			 train-loss:  2.1971690752926993 	 ± 0.24309519228493343
	data : 0.11520256996154785
	model : 0.06407051086425782
			 train-loss:  2.196481066290289 	 ± 0.2428685600949307
	data : 0.11474924087524414
	model : 0.05557255744934082
#epoch  90    val-loss:  2.4295221253445276  train-loss:  2.196481066290289  lr:  1.9073486328125e-08
			 train-loss:  2.2013819217681885 	 ± 0.0
	data : 4.84060001373291
	model : 0.07265186309814453
			 train-loss:  2.131606936454773 	 ± 0.06977498531341553
	data : 3.025609612464905
	model : 0.06853604316711426
			 train-loss:  2.042836308479309 	 ± 0.137862786250102
	data : 2.055077314376831
	model : 0.06725152333577473
			 train-loss:  2.0392980873584747 	 ± 0.11954985529301465
	data : 1.5699129700660706
	model : 0.06661456823348999
			 train-loss:  2.1137192964553835 	 ± 0.1832697459327309
	data : 1.2786838054656982
	model : 0.06622014045715333
			 train-loss:  2.128321627775828 	 ± 0.17045812842210448
	data : 0.33329434394836427
	model : 0.06457405090332032
			 train-loss:  2.17332170690809 	 ± 0.1924971763767727
	data : 0.1138984203338623
	model : 0.06461114883422851
			 train-loss:  2.1416962295770645 	 ± 0.19855594492184786
	data : 0.11388850212097168
	model : 0.0646294116973877
			 train-loss:  2.1870273086759777 	 ± 0.226899143241623
	data : 0.11393990516662597
	model : 0.06465191841125488
			 train-loss:  2.193667638301849 	 ± 0.21617526426783049
	data : 0.11404604911804199
	model : 0.0646894931793213
			 train-loss:  2.1680079265074297 	 ± 0.22151210470590918
	data : 0.11408796310424804
	model : 0.06481170654296875
			 train-loss:  2.177225261926651 	 ± 0.21427364937117124
	data : 0.11430249214172364
	model : 0.06485209465026856
			 train-loss:  2.1790343523025513 	 ± 0.20596282708373725
	data : 0.11421647071838378
	model : 0.06483211517333984
			 train-loss:  2.200393872601645 	 ± 0.2128887397834807
	data : 0.11411290168762207
	model : 0.06482725143432617
			 train-loss:  2.188337953885396 	 ± 0.21055879691143572
	data : 0.11385846138000488
	model : 0.06482176780700684
			 train-loss:  2.1799097433686256 	 ± 0.20646934399497865
	data : 0.11384115219116211
	model : 0.06478791236877442
			 train-loss:  2.181208477300756 	 ± 0.20037204002852357
	data : 0.11385436058044433
	model : 0.06489462852478027
			 train-loss:  2.1916834579573736 	 ± 0.1994587318043177
	data : 0.11404256820678711
	model : 0.06492528915405274
			 train-loss:  2.1981298484300313 	 ± 0.19605588320101128
	data : 0.11394901275634765
	model : 0.06495685577392578
			 train-loss:  2.204860347509384 	 ± 0.19333056305232338
	data : 0.11420936584472656
	model : 0.0649256706237793
			 train-loss:  2.2104786975043162 	 ± 0.19033701839507874
	data : 0.11420445442199707
	model : 0.06487340927124023
			 train-loss:  2.201884589411996 	 ± 0.19008544831984575
	data : 0.11398544311523437
	model : 0.0647202968597412
			 train-loss:  2.195729260859282 	 ± 0.1881356901395471
	data : 0.11392221450805665
	model : 0.06475396156311035
			 train-loss:  2.1866878966490426 	 ± 0.18920995955356684
	data : 0.11404352188110352
	model : 0.0647538185119629
			 train-loss:  2.173814287185669 	 ± 0.19582111259326238
	data : 0.11411337852478028
	model : 0.06479072570800781
			 train-loss:  2.173376743610089 	 ± 0.19203086112943527
	data : 0.11418890953063965
	model : 0.06484456062316894
			 train-loss:  2.185966129656191 	 ± 0.19907508133899304
	data : 0.11437463760375977
	model : 0.06490745544433593
			 train-loss:  2.1911124927656993 	 ± 0.19730837688883002
	data : 0.11434726715087891
	model : 0.06485176086425781
			 train-loss:  2.1918562034080766 	 ± 0.19391660118294654
	data : 0.1141664981842041
	model : 0.06478309631347656
			 train-loss:  2.196042617162069 	 ± 0.19198554377175506
	data : 0.11389966011047363
	model : 0.06476268768310547
			 train-loss:  2.1962163294515302 	 ± 0.18886601618543264
	data : 0.1139592170715332
	model : 0.06477551460266114
			 train-loss:  2.2084177881479263 	 ± 0.19791618520599039
	data : 0.11396613121032714
	model : 0.06475954055786133
			 train-loss:  2.2041557124166777 	 ± 0.19638001730417579
	data : 0.11398162841796874
	model : 0.06481614112854003
			 train-loss:  2.208794544724857 	 ± 0.19529711331987606
	data : 0.11421194076538085
	model : 0.06487951278686524
			 train-loss:  2.2171519143240794 	 ± 0.19855974413561447
	data : 0.1143423080444336
	model : 0.06489882469177247
			 train-loss:  2.2147903641064963 	 ± 0.1962804072815523
	data : 0.11422271728515625
	model : 0.06491451263427735
			 train-loss:  2.2074351987323246 	 ± 0.19857567716695712
	data : 0.11399078369140625
	model : 0.06489458084106445
			 train-loss:  2.2227041627231396 	 ± 0.21684287892071624
	data : 0.11397819519042969
	model : 0.0648615837097168
			 train-loss:  2.2200039564034877 	 ± 0.21469101998738482
	data : 0.11385717391967773
	model : 0.06484088897705079
			 train-loss:  2.220333030819893 	 ± 0.21200035735944608
	data : 0.11389102935791015
	model : 0.06481103897094727
			 train-loss:  2.2170862308362635 	 ± 0.21040347218690736
	data : 0.11398863792419434
	model : 0.06478033065795899
			 train-loss:  2.217848383245014 	 ± 0.20794085322165676
	data : 0.1141026496887207
	model : 0.0648007869720459
			 train-loss:  2.217003531234209 	 ± 0.2055816372627908
	data : 0.11406149864196777
	model : 0.06485657691955567
			 train-loss:  2.2089151685888115 	 ± 0.21003903850243683
	data : 0.11391205787658691
	model : 0.06484875679016114
			 train-loss:  2.2151983790927465 	 ± 0.2118327113743826
	data : 0.11376967430114746
	model : 0.06484289169311523
			 train-loss:  2.21262556573619 	 ± 0.21022717966963722
	data : 0.11377649307250977
	model : 0.06485261917114257
			 train-loss:  2.213229681583161 	 ± 0.20801905191404138
	data : 0.11381330490112304
	model : 0.06485128402709961
			 train-loss:  2.2132560114065805 	 ± 0.2058408610673163
	data : 0.11388692855834961
	model : 0.064811372756958
			 train-loss:  2.2122358205367108 	 ± 0.2038521883856563
	data : 0.11405510902404785
	model : 0.06482911109924316
			 train-loss:  2.2118589639663697 	 ± 0.20182061202534426
	data : 0.11422195434570312
	model : 0.06490263938903809
			 train-loss:  2.2094464582555435 	 ± 0.2005589953114079
	data : 0.11407656669616699
	model : 0.06484875679016114
			 train-loss:  2.2111474779935985 	 ± 0.1989923132525961
	data : 0.11397590637207031
	model : 0.06483068466186523
			 train-loss:  2.2106082979238257 	 ± 0.19714443173485405
	data : 0.11390924453735352
	model : 0.0648188591003418
			 train-loss:  2.209439304139879 	 ± 0.19549581718947653
	data : 0.113930082321167
	model : 0.06481347084045411
			 train-loss:  2.2091300097378817 	 ± 0.19372376323348545
	data : 0.11396088600158691
	model : 0.06477184295654297
			 train-loss:  2.2066398688725064 	 ± 0.19287244920007948
	data : 0.11398835182189941
	model : 0.06482315063476562
			 train-loss:  2.202435280147352 	 ± 0.19374507381474126
	data : 0.1140516757965088
	model : 0.06490287780761719
			 train-loss:  2.1991571968999404 	 ± 0.1936555540666061
	data : 0.11413807868957519
	model : 0.06489372253417969
			 train-loss:  2.2073871806516485 	 ± 0.20197854134245158
	data : 0.11404790878295898
	model : 0.06483225822448731
			 train-loss:  2.208372950553894 	 ± 0.20043138918963654
	data : 0.11378512382507325
	model : 0.06482820510864258
			 train-loss:  2.2056964147286338 	 ± 0.19985995816237878
	data : 0.11387090682983399
	model : 0.06482424736022949
			 train-loss:  2.2017269538294886 	 ± 0.20065118632934673
	data : 0.11393494606018066
	model : 0.06479392051696778
			 train-loss:  2.202398589679173 	 ± 0.1991225867789224
	data : 0.11398930549621582
	model : 0.06481804847717285
			 train-loss:  2.2056870367377996 	 ± 0.19927757290976986
	data : 0.1141366958618164
	model : 0.06490626335144042
			 train-loss:  2.2022342883623565 	 ± 0.19965865443645095
	data : 0.11429848670959472
	model : 0.06490645408630372
			 train-loss:  2.2034794290860495 	 ± 0.19839445325743282
	data : 0.1142648696899414
	model : 0.06487822532653809
			 train-loss:  2.201646034397296 	 ± 0.19747085663178124
	data : 0.11404581069946289
	model : 0.06480541229248046
			 train-loss:  2.209051680915496 	 ± 0.2051726307166044
	data : 0.11396408081054688
	model : 0.06478476524353027
			 train-loss:  2.2100477132244385 	 ± 0.20384598519481092
	data : 0.11380858421325683
	model : 0.06476883888244629
			 train-loss:  2.207882160799844 	 ± 0.2031825591326919
	data : 0.11387190818786622
	model : 0.06480188369750976
			 train-loss:  2.210413508012261 	 ± 0.202855218802963
	data : 0.11396417617797852
	model : 0.06483855247497558
			 train-loss:  2.2061881638235517 	 ± 0.20456370539945057
	data : 0.11403603553771972
	model : 0.06490120887756348
			 train-loss:  2.203212236704892 	 ± 0.2047210655719676
	data : 0.1139453411102295
	model : 0.06490135192871094
			 train-loss:  2.197766442556639 	 ± 0.20858880812423175
	data : 0.11396231651306152
	model : 0.06483573913574218
			 train-loss:  2.2001349131266275 	 ± 0.2081928944670907
	data : 0.11390066146850586
	model : 0.06480417251586915
			 train-loss:  2.196902640556034 	 ± 0.20870441002409315
	data : 0.11383414268493652
	model : 0.0648148536682129
			 train-loss:  2.1980026867482567 	 ± 0.20756641411678028
	data : 0.11393961906433106
	model : 0.06483821868896485
			 train-loss:  2.1955662002930274 	 ± 0.20733684561949753
	data : 0.11411185264587402
	model : 0.06482982635498047
			 train-loss:  2.193193832530251 	 ± 0.20708308118979404
	data : 0.11416683197021485
	model : 0.0649137020111084
			 train-loss:  2.1948965594172476 	 ± 0.20634050259923525
	data : 0.11421694755554199
	model : 0.06491165161132813
			 train-loss:  2.1955160550129267 	 ± 0.20513768642366195
	data : 0.11422982215881347
	model : 0.06484637260437012
			 train-loss:  2.193619770247762 	 ± 0.2045960637370695
	data : 0.11408658027648926
	model : 0.06475300788879394
			 train-loss:  2.194348129881434 	 ± 0.20346675161670658
	data : 0.11393857002258301
	model : 0.06472783088684082
			 train-loss:  2.1914309234846208 	 ± 0.20399072246759326
	data : 0.11394782066345215
	model : 0.06472592353820801
			 train-loss:  2.197198315227733 	 ± 0.20956320100520823
	data : 0.1138946533203125
	model : 0.06478490829467773
			 train-loss:  2.195328116416931 	 ± 0.20905352235505187
	data : 0.11396641731262207
	model : 0.06484618186950683
			 train-loss:  2.195439788116806 	 ± 0.2078511727940659
	data : 0.11414074897766113
	model : 0.06494388580322266
			 train-loss:  2.193452084606344 	 ± 0.2074967733235468
	data : 0.1141754150390625
	model : 0.0649951457977295
			 train-loss:  2.1935998134398726 	 ± 0.20633242205369412
	data : 0.11413235664367676
	model : 0.06492757797241211
			 train-loss:  2.1939946757422555 	 ± 0.20521674113600846
	data : 0.11415262222290039
	model : 0.06488161087036133
			 train-loss:  2.197886566539387 	 ± 0.20739897268635918
	data : 0.11397342681884766
	model : 0.06490964889526367
			 train-loss:  2.197408590627753 	 ± 0.2063191137837721
	data : 0.11391634941101074
	model : 0.06489019393920899
			 train-loss:  2.2005682145395586 	 ± 0.20743268154797762
	data : 0.11390137672424316
	model : 0.0649024486541748
			 train-loss:  2.200655937194824 	 ± 0.20632810030770768
	data : 0.11442947387695312
	model : 0.06497993469238281
			 train-loss:  2.2007313100915207 	 ± 0.20524059103470382
	data : 0.11442580223083496
	model : 0.06499528884887695
			 train-loss:  2.203826648493608 	 ± 0.2063858474984774
	data : 0.11452035903930664
	model : 0.06493501663208008
			 train-loss:  2.2028185977149257 	 ± 0.20555667123956084
	data : 0.11436004638671875
	model : 0.06488227844238281
			 train-loss:  2.2002090033219783 	 ± 0.20611393647347914
	data : 0.11447806358337402
	model : 0.06485347747802735
			 train-loss:  2.2024320990148216 	 ± 0.20624782469152436
	data : 0.11406049728393555
	model : 0.06482520103454589
			 train-loss:  2.199781413078308 	 ± 0.20690183903376733
	data : 0.11397442817687989
	model : 0.06484436988830566
			 train-loss:  2.200771763773248 	 ± 0.2061130883473196
	data : 0.11398568153381347
	model : 0.06489572525024415
			 train-loss:  2.199890134381313 	 ± 0.20529153262526983
	data : 0.11424622535705567
	model : 0.06495423316955566
			 train-loss:  2.1991331623595896 	 ± 0.20443553761886293
	data : 0.11405410766601562
	model : 0.0649759292602539
			 train-loss:  2.2019990269954386 	 ± 0.20551881246066223
	data : 0.11396870613098145
	model : 0.06496238708496094
			 train-loss:  2.202042711348761 	 ± 0.20453829529685838
	data : 0.11399669647216797
	model : 0.06486315727233886
			 train-loss:  2.201481299580268 	 ± 0.20365247368971445
	data : 0.11393675804138184
	model : 0.06478557586669922
			 train-loss:  2.1991778333610466 	 ± 0.20408123598426944
	data : 0.11380224227905274
	model : 0.06482162475585937
			 train-loss:  2.200583835442861 	 ± 0.20365419876181853
	data : 0.11395955085754395
	model : 0.06480093002319336
			 train-loss:  2.198047239846046 	 ± 0.2044246414852171
	data : 0.11391582489013671
	model : 0.06484155654907227
			 train-loss:  2.200161170959473 	 ± 0.20468663590592762
	data : 0.11393461227416993
	model : 0.0649099349975586
			 train-loss:  2.1997251682453327 	 ± 0.20381384330202706
	data : 0.11389107704162597
	model : 0.06496362686157227
			 train-loss:  2.20066622538226 	 ± 0.2031440116104728
	data : 0.11391448974609375
	model : 0.06486659049987793
			 train-loss:  2.1997625363611544 	 ± 0.20246914743322084
	data : 0.1138885498046875
	model : 0.0648768424987793
			 train-loss:  2.195948228501437 	 ± 0.2056166142049889
	data : 0.11390337944030762
	model : 0.06489653587341308
			 train-loss:  2.196782014680945 	 ± 0.2049141486463157
	data : 0.1140380859375
	model : 0.06491532325744628
			 train-loss:  2.197728527003321 	 ± 0.20428131106531455
	data : 0.11409487724304199
	model : 0.06490049362182618
			 train-loss:  2.1960914338755813 	 ± 0.20416921574456648
	data : 0.11414337158203125
	model : 0.06497740745544434
			 train-loss:  2.196487372204409 	 ± 0.20334735582689303
	data : 0.11419105529785156
	model : 0.0649294376373291
			 train-loss:  2.199589611101551 	 ± 0.20527612004935736
	data : 0.11418499946594238
	model : 0.0648777961730957
			 train-loss:  2.1995551347732545 	 ± 0.2044193594773063
	data : 0.11401515007019043
	model : 0.06482653617858887
			 train-loss:  2.199341669555538 	 ± 0.20358632871767077
	data : 0.113983154296875
	model : 0.06484098434448242
			 train-loss:  2.200369770409631 	 ± 0.20306539947653962
	data : 0.113885498046875
	model : 0.06479992866516113
			 train-loss:  2.1990434464400375 	 ± 0.2027681497209268
	data : 0.11377997398376465
	model : 0.06484618186950683
			 train-loss:  2.1992089940655615 	 ± 0.20195722696966634
	data : 0.11381268501281738
	model : 0.0648653507232666
			 train-loss:  2.1997007255554197 	 ± 0.20122229234227942
	data : 0.11391968727111816
	model : 0.06492347717285156
			 train-loss:  2.2027913918570867 	 ± 0.20337916742201534
	data : 0.11396641731262207
	model : 0.06488499641418458
			 train-loss:  2.2017195863047925 	 ± 0.20293382372744384
	data : 0.11400713920593261
	model : 0.06485147476196289
			 train-loss:  2.1991953887045383 	 ± 0.20413130873651816
	data : 0.11400032043457031
	model : 0.06484551429748535
			 train-loss:  2.2008088725481847 	 ± 0.20415630623168693
	data : 0.11402592658996583
	model : 0.06479606628417969
			 train-loss:  2.2015197552167454 	 ± 0.2035297870974726
	data : 0.11401748657226562
	model : 0.06476945877075195
			 train-loss:  2.199880443456519 	 ± 0.20361117905178922
	data : 0.11403908729553222
	model : 0.06484107971191407
			 train-loss:  2.1985146818738994 	 ± 0.20343990534860076
	data : 0.1140169620513916
	model : 0.06492028236389161
			 train-loss:  2.1989353904150484 	 ± 0.20273128048804728
	data : 0.11415295600891114
	model : 0.06493730545043945
			 train-loss:  2.1958794940763444 	 ± 0.20502506359536882
	data : 0.11423215866088868
	model : 0.0649846076965332
			 train-loss:  2.1977826533494174 	 ± 0.20544890927325848
	data : 0.11412367820739747
	model : 0.06498374938964843
			 train-loss:  2.1989743893637375 	 ± 0.20515999572455448
	data : 0.11406536102294922
	model : 0.06485586166381836
			 train-loss:  2.2017078269137085 	 ± 0.20688050079428194
	data : 0.1140599250793457
	model : 0.06479506492614746
			 train-loss:  2.2006929551345715 	 ± 0.20647156019844345
	data : 0.11392912864685059
	model : 0.0647881031036377
			 train-loss:  2.201985159366251 	 ± 0.20628679587921858
	data : 0.1139460563659668
	model : 0.06479768753051758
			 train-loss:  2.2043190504823413 	 ± 0.2073823110047971
	data : 0.11406569480895996
	model : 0.06479759216308593
			 train-loss:  2.2052697653466082 	 ± 0.20695155461923048
	data : 0.11407675743103027
	model : 0.06488666534423829
			 train-loss:  2.206080712902714 	 ± 0.20644626499695376
	data : 0.11408700942993164
	model : 0.06491174697875976
			 train-loss:  2.2051440410680705 	 ± 0.20602573197811885
	data : 0.11404142379760743
	model : 0.06483583450317383
			 train-loss:  2.2048292317324214 	 ± 0.20534362948305857
	data : 0.11366186141967774
	model : 0.06477627754211426
			 train-loss:  2.2036669180311006 	 ± 0.2051091081813018
	data : 0.11363658905029297
	model : 0.06477794647216797
			 train-loss:  2.2033940235229386 	 ± 0.20443188515193714
	data : 0.1136542797088623
	model : 0.06476616859436035
			 train-loss:  2.2061007955447347 	 ± 0.2063438378363183
	data : 0.11370129585266113
	model : 0.06476788520812989
			 train-loss:  2.205906480550766 	 ± 0.2056590435328962
	data : 0.11372241973876954
	model : 0.06485490798950196
			 train-loss:  2.205996018128107 	 ± 0.20497064504263032
	data : 0.113987398147583
	model : 0.06489729881286621
			 train-loss:  2.2065382583936057 	 ± 0.2043934650251511
	data : 0.11406531333923339
	model : 0.0649146556854248
			 train-loss:  2.205745898335185 	 ± 0.20394655403501774
	data : 0.11396026611328125
	model : 0.06487832069396973
			 train-loss:  2.203482510227906 	 ± 0.20516849820294913
	data : 0.11392955780029297
	model : 0.06494603157043458
			 train-loss:  2.2044846419415443 	 ± 0.20486980369787455
	data : 0.11399245262145996
	model : 0.06493701934814453
			 train-loss:  2.2055351594825843 	 ± 0.2046165736204665
	data : 0.11403264999389648
	model : 0.06493754386901855
			 train-loss:  2.2035248925608975 	 ± 0.2054754659342562
	data : 0.11400017738342286
	model : 0.06496682167053222
			 train-loss:  2.202072825951454 	 ± 0.20561211566206256
	data : 0.11413340568542481
	model : 0.06503252983093262
			 train-loss:  2.200027000372577 	 ± 0.20654294662691028
	data : 0.11414904594421386
	model : 0.0650019645690918
			 train-loss:  2.2004334813431847 	 ± 0.20595127937592575
	data : 0.11426281929016113
	model : 0.06497416496276856
			 train-loss:  2.2022443829842335 	 ± 0.20656065030781612
	data : 0.11412572860717773
	model : 0.06490077972412109
			 train-loss:  2.2029275320470334 	 ± 0.2060942399645589
	data : 0.11400551795959472
	model : 0.06482892036437989
			 train-loss:  2.2007678628708263 	 ± 0.20726139110578623
	data : 0.11382331848144531
	model : 0.06482219696044922
			 train-loss:  2.197757640002686 	 ± 0.2101214123741502
	data : 0.11395530700683594
	model : 0.06479039192199706
			 train-loss:  2.196866658567651 	 ± 0.20978261658531283
	data : 0.11383905410766601
	model : 0.0648608684539795
			 train-loss:  2.198050709759317 	 ± 0.20968767845399616
	data : 0.11387062072753906
	model : 0.06493940353393554
			 train-loss:  2.198531975890651 	 ± 0.20914212722427816
	data : 0.11391220092773438
	model : 0.06500325202941895
			 train-loss:  2.197878036154322 	 ± 0.20868036072390814
	data : 0.11402802467346192
	model : 0.06493916511535644
			 train-loss:  2.1969415413405367 	 ± 0.20840421123870767
	data : 0.11371169090270997
	model : 0.06489195823669433
			 train-loss:  2.196568440823328 	 ± 0.2078389680576446
	data : 0.11364254951477051
	model : 0.06486406326293945
			 train-loss:  2.1978216213587474 	 ± 0.2078587751346528
	data : 0.11371684074401855
	model : 0.06485934257507324
			 train-loss:  2.1953437047846176 	 ± 0.2097350559008534
	data : 0.11380043029785156
	model : 0.06485891342163086
			 train-loss:  2.194825090162935 	 ± 0.20923019059799958
	data : 0.11394610404968261
	model : 0.06498222351074219
			 train-loss:  2.19482956792033 	 ± 0.20862108488644898
	data : 0.1141545295715332
	model : 0.06501603126525879
			 train-loss:  2.194899105612253 	 ± 0.20801925904392077
	data : 0.1142080307006836
	model : 0.06498656272888184
			 train-loss:  2.194334227463295 	 ± 0.207553666590564
	data : 0.11422605514526367
	model : 0.06497626304626465
			 train-loss:  2.1949422386714392 	 ± 0.20711515015914028
	data : 0.11403942108154297
	model : 0.06489667892456055
			 train-loss:  2.1958571821451187 	 ± 0.2068802806728043
	data : 0.11376509666442872
	model : 0.06479272842407227
			 train-loss:  2.197043141402767 	 ± 0.2068941507474248
	data : 0.11370849609375
	model : 0.06482787132263183
			 train-loss:  2.1981019009365124 	 ± 0.20679246364402729
	data : 0.1136469841003418
	model : 0.06482267379760742
			 train-loss:  2.198645597063629 	 ± 0.20634156310971727
	data : 0.11372966766357422
	model : 0.06483139991760253
			 train-loss:  2.196536370118459 	 ± 0.2076936318707771
	data : 0.11384210586547852
	model : 0.06489472389221192
			 train-loss:  2.195420499006029 	 ± 0.2076594592519634
	data : 0.11401596069335937
	model : 0.06494174003601075
			 train-loss:  2.1954724428417918 	 ± 0.20708935955307978
	data : 0.11398091316223144
	model : 0.06488609313964844
			 train-loss:  2.1928694248199463 	 ± 0.20948706645380927
	data : 0.1140246868133545
	model : 0.0649439811706543
			 train-loss:  2.1915640209032143 	 ± 0.20966204478317302
	data : 0.11392111778259277
	model : 0.06495494842529297
			 train-loss:  2.191668629001927 	 ± 0.2090994374975385
	data : 0.11400184631347657
	model : 0.06495208740234375
			 train-loss:  2.1930683159059092 	 ± 0.20940378456450887
	data : 0.11400671005249023
	model : 0.06494145393371582
			 train-loss:  2.1919402030700033 	 ± 0.20940908308767525
	data : 0.1140531063079834
	model : 0.06496572494506836
			 train-loss:  2.1894835614143533 	 ± 0.2115359765752556
	data : 0.11404232978820801
	model : 0.06504039764404297
			 train-loss:  2.189695732934134 	 ± 0.21099567160983207
	data : 0.11406593322753907
	model : 0.06502699851989746
			 train-loss:  2.190130409441496 	 ± 0.21052451722262697
	data : 0.11391649246215821
	model : 0.06493563652038574
			 train-loss:  2.190812688847487 	 ± 0.2101831902319651
	data : 0.11378631591796876
	model : 0.06489181518554688
			 train-loss:  2.1899697445333004 	 ± 0.20995856991316594
	data : 0.11376833915710449
	model : 0.06491713523864746
			 train-loss:  2.1877430090632464 	 ± 0.21167474385100984
	data : 0.11384501457214355
	model : 0.06487388610839843
			 train-loss:  2.1894792809928814 	 ± 0.2125019128533247
	data : 0.11384902000427247
	model : 0.06485905647277831
			 train-loss:  2.189845359019744 	 ± 0.21201765688277766
	data : 0.11385946273803711
	model : 0.06495194435119629
			 train-loss:  2.1888021382750296 	 ± 0.21197726969260447
	data : 0.11406488418579101
	model : 0.06497087478637695
			 train-loss:  2.1883415504155423 	 ± 0.21153687462087525
	data : 0.11397457122802734
	model : 0.06489572525024415
			 train-loss:  2.188572828817849 	 ± 0.21102698300485773
	data : 0.11383209228515626
	model : 0.06485743522644043
			 train-loss:  2.1870223445508947 	 ± 0.21162372032367363
	data : 0.11377382278442383
	model : 0.06487431526184081
			 train-loss:  2.185465074777603 	 ± 0.21223399527132542
	data : 0.11392979621887207
	model : 0.06489367485046386
			 train-loss:  2.185449052212843 	 ± 0.21170551298660892
	data : 0.11388654708862304
	model : 0.06493873596191406
			 train-loss:  2.186057912241114 	 ± 0.21135718511229884
	data : 0.1140594482421875
	model : 0.06503276824951172
			 train-loss:  2.1875430703750385 	 ± 0.21188995163138763
	data : 0.11411137580871582
	model : 0.06496701240539551
			 train-loss:  2.187198864478691 	 ± 0.21142686091977725
	data : 0.11396446228027343
	model : 0.06490902900695801
			 train-loss:  2.1888573658175585 	 ± 0.21223663716699157
	data : 0.11391754150390625
	model : 0.06485285758972167
			 train-loss:  2.1897801056648922 	 ± 0.21213268271795901
	data : 0.11382694244384765
	model : 0.06479787826538086
			 train-loss:  2.1911406897116397 	 ± 0.21251876944693132
	data : 0.11385307312011719
	model : 0.06482720375061035
			 train-loss:  2.1921120973733754 	 ± 0.21246746493925683
	data : 0.11379942893981934
	model : 0.06485261917114257
			 train-loss:  2.1920387642235277 	 ± 0.2119611987421334
	data : 0.1140334129333496
	model : 0.06490898132324219
			 train-loss:  2.191251091730027 	 ± 0.211762316057456
	data : 0.11406979560852051
	model : 0.06490955352783204
			 train-loss:  2.191508172247647 	 ± 0.21129275921854437
	data : 0.11408343315124511
	model : 0.06489472389221192
			 train-loss:  2.193753374072741 	 ± 0.21330184882235376
	data : 0.1137455940246582
	model : 0.06470985412597656
			 train-loss:  2.1918420914752943 	 ± 0.21461246663019903
	data : 0.11382560729980469
	model : 0.06470599174499511
			 train-loss:  2.1917033529727257 	 ± 0.21412002253460213
	data : 0.11380772590637207
	model : 0.0647590160369873
			 train-loss:  2.191747422551 	 ± 0.2136224614098606
	data : 0.11379694938659668
	model : 0.06479873657226562
			 train-loss:  2.19234530572538 	 ± 0.21330761765235262
	data : 0.11393060684204101
	model : 0.06485276222229004
			 train-loss:  2.1950298707056706 	 ± 0.2164420239648012
	data : 0.11415715217590332
	model : 0.06493954658508301
			 train-loss:  2.19779300252232 	 ± 0.21974764458202123
	data : 0.11418957710266113
	model : 0.06485090255737305
			 train-loss:  2.1977351375910787 	 ± 0.21924702825387418
	data : 0.11401033401489258
	model : 0.06468772888183594
			 train-loss:  2.1983945673162286 	 ± 0.21896573791507923
	data : 0.11390166282653809
	model : 0.06458673477172852
			 train-loss:  2.1976525751174307 	 ± 0.21874680732059734
	data : 0.11380767822265625
	model : 0.06511344909667968
			 train-loss:  2.196177056243828 	 ± 0.21935308497968378
	data : 0.1133664608001709
	model : 0.06508102416992187
			 train-loss:  2.1955245353715838 	 ± 0.2190765482963262
	data : 0.11339011192321777
	model : 0.06513357162475586
			 train-loss:  2.193938724696636 	 ± 0.21986603215743425
	data : 0.1135983943939209
	model : 0.06516523361206054
			 train-loss:  2.1958098740047878 	 ± 0.22115716499271798
	data : 0.11366844177246094
	model : 0.06508173942565917
			 train-loss:  2.1952079291892264 	 ± 0.2208519854433963
	data : 0.11366324424743653
	model : 0.06432452201843261
			 train-loss:  2.1940212381043622 	 ± 0.22108593412337868
	data : 0.1142815113067627
	model : 0.06423063278198242
			 train-loss:  2.192921923963647 	 ± 0.22122146441880788
	data : 0.11439943313598633
	model : 0.06408863067626953
			 train-loss:  2.192384157638883 	 ± 0.22088722256272073
	data : 0.11448287963867188
	model : 0.0639676570892334
			 train-loss:  2.1929848173390263 	 ± 0.2205938597883822
	data : 0.11453285217285156
	model : 0.06393203735351563
			 train-loss:  2.1923038371197587 	 ± 0.22035801168140243
	data : 0.11465225219726563
	model : 0.06398658752441407
			 train-loss:  2.19219646679944 	 ± 0.21988864486253154
	data : 0.11460318565368652
	model : 0.06392064094543456
			 train-loss:  2.1920970437864375 	 ± 0.21942149929711197
	data : 0.11461949348449707
	model : 0.06388845443725585
			 train-loss:  2.192806157291445 	 ± 0.21921953670520677
	data : 0.11463804244995117
	model : 0.06386547088623047
			 train-loss:  2.192499876022339 	 ± 0.21880278254606436
	data : 0.11473674774169922
	model : 0.06388263702392578
			 train-loss:  2.191968817832106 	 ± 0.2184904443891698
	data : 0.11479926109313965
	model : 0.06387734413146973
			 train-loss:  2.190503684780266 	 ± 0.2191877024890918
	data : 0.11493387222290039
	model : 0.06392593383789062
			 train-loss:  2.19045003522344 	 ± 0.21872829877082378
	data : 0.1148414134979248
	model : 0.0639200210571289
			 train-loss:  2.189305880578492 	 ± 0.21898277568853264
	data : 0.11488556861877441
	model : 0.0638913631439209
			 train-loss:  2.1884106134374934 	 ± 0.21896394511205045
	data : 0.11467099189758301
	model : 0.06379122734069824
			 train-loss:  2.191106082492844 	 ± 0.2224634785865518
	data : 0.11463508605957032
	model : 0.06373777389526367
			 train-loss:  2.191693118288497 	 ± 0.22219033829711524
	data : 0.11467676162719727
	model : 0.06370363235473633
			 train-loss:  2.1926044451356423 	 ± 0.22218543767013363
	data : 0.11469030380249023
	model : 0.06374964714050294
			 train-loss:  2.1922004638148134 	 ± 0.2218190824747993
	data : 0.11468253135681153
	model : 0.06378827095031739
			 train-loss:  2.192275257986419 	 ± 0.22136901065093034
	data : 0.11484637260437011
	model : 0.06385617256164551
			 train-loss:  2.1909967743284335 	 ± 0.22182311008348887
	data : 0.11479697227478028
	model : 0.06385278701782227
			 train-loss:  2.1907451312068984 	 ± 0.22140880157827494
	data : 0.1146383285522461
	model : 0.06385273933410644
			 train-loss:  2.192845511340326 	 ± 0.2234140842080748
	data : 0.11467628479003907
	model : 0.06378769874572754
			 train-loss:  2.1929788718740624 	 ± 0.22297490098529943
	data : 0.11466302871704101
	model : 0.06381664276123047
			 train-loss:  2.1931932816505433 	 ± 0.22255422296608274
	data : 0.11474695205688476
	model : 0.06383085250854492
			 train-loss:  2.192005933993366 	 ± 0.22290244183262564
	data : 0.11475753784179688
	model : 0.0638847827911377
			 train-loss:  2.1928745143943362 	 ± 0.22288493961731165
	data : 0.11481132507324218
	model : 0.0638493537902832
			 train-loss:  2.192238451935086 	 ± 0.22267306705408088
	data : 0.11474823951721191
	model : 0.06382017135620117
			 train-loss:  2.1932245460082225 	 ± 0.2227871124955177
	data : 0.1146810531616211
	model : 0.06379237174987792
			 train-loss:  2.1931060814389998 	 ± 0.22235786150586795
	data : 0.11453275680541992
	model : 0.06378483772277832
			 train-loss:  2.195791972335428 	 ± 0.2260297642094444
	data : 0.11432337760925293
	model : 0.055341148376464845
#epoch  91    val-loss:  2.414769310700266  train-loss:  2.195791972335428  lr:  1.9073486328125e-08
			 train-loss:  2.103998899459839 	 ± 0.0
	data : 5.857075452804565
	model : 0.07410407066345215
			 train-loss:  2.3434919118881226 	 ± 0.2394930124282837
	data : 2.9906296730041504
	model : 0.06935238838195801
			 train-loss:  2.2675236066182456 	 ± 0.22311499758152484
	data : 2.0320183436075845
	model : 0.06762893994649251
			 train-loss:  2.166502445936203 	 ± 0.2606742245140359
	data : 1.5524464845657349
	model : 0.06688505411148071
			 train-loss:  2.204118275642395 	 ± 0.24499110930549325
	data : 1.2647650718688965
	model : 0.06634039878845215
			 train-loss:  2.210968315601349 	 ± 0.2241691733255856
	data : 0.11602435111999512
	model : 0.06436114311218262
			 train-loss:  2.1794959477015903 	 ± 0.22139567912547242
	data : 0.11386785507202149
	model : 0.06443929672241211
			 train-loss:  2.1785895824432373 	 ± 0.20711057766172
	data : 0.11375904083251953
	model : 0.06457490921020508
			 train-loss:  2.1891307565901013 	 ± 0.19752881882775036
	data : 0.11390395164489746
	model : 0.06462516784667968
			 train-loss:  2.1935415983200075 	 ± 0.18785891132345603
	data : 0.11391329765319824
	model : 0.06479763984680176
			 train-loss:  2.2272192998365923 	 ± 0.20838564323579614
	data : 0.1140716552734375
	model : 0.06549038887023925
			 train-loss:  2.211616039276123 	 ± 0.20611632821263892
	data : 0.11354708671569824
	model : 0.06560678482055664
			 train-loss:  2.2069319578317494 	 ± 0.1986938165641601
	data : 0.11324543952941894
	model : 0.065525484085083
			 train-loss:  2.2016783952713013 	 ± 0.1924008484896921
	data : 0.11305313110351563
	model : 0.06544661521911621
			 train-loss:  2.237235959370931 	 ± 0.2285847308374725
	data : 0.11319928169250489
	model : 0.06542096138000489
			 train-loss:  2.236706256866455 	 ± 0.22133572180047212
	data : 0.11317710876464844
	model : 0.06488351821899414
			 train-loss:  2.2508457408231846 	 ± 0.2220508195282713
	data : 0.11384706497192383
	model : 0.06475596427917481
			 train-loss:  2.247438563240899 	 ± 0.2162513892802695
	data : 0.1141049861907959
	model : 0.0648371696472168
			 train-loss:  2.2219991307509575 	 ± 0.2365424493640749
	data : 0.11413993835449218
	model : 0.06493120193481446
			 train-loss:  2.21534743309021 	 ± 0.23236903921828306
	data : 0.11385517120361328
	model : 0.06488828659057617
			 train-loss:  2.21286332039606 	 ± 0.2270409183189127
	data : 0.11379942893981934
	model : 0.06482419967651368
			 train-loss:  2.204241069880399 	 ± 0.22531248105687735
	data : 0.11370296478271484
	model : 0.06479558944702149
			 train-loss:  2.1945604915204258 	 ± 0.2249893515273671
	data : 0.11368918418884277
	model : 0.0648036003112793
			 train-loss:  2.1988408317168555 	 ± 0.22120674296995885
	data : 0.11379756927490234
	model : 0.06477046012878418
			 train-loss:  2.1923503637313844 	 ± 0.21905742261346117
	data : 0.11399555206298828
	model : 0.06481633186340333
			 train-loss:  2.1817494539114146 	 ± 0.22124650348463126
	data : 0.11408162117004395
	model : 0.06482086181640626
			 train-loss:  2.178599993387858 	 ± 0.21770380891412106
	data : 0.11401605606079102
	model : 0.06479911804199219
			 train-loss:  2.1892017892428806 	 ± 0.2207646251578272
	data : 0.113887357711792
	model : 0.0647435188293457
			 train-loss:  2.180974578035289 	 ± 0.2212502405722158
	data : 0.11374998092651367
	model : 0.06477341651916504
			 train-loss:  2.188183081150055 	 ± 0.22096800594731683
	data : 0.11367259025573731
	model : 0.06475062370300293
			 train-loss:  2.1837611698335215 	 ± 0.21871990914017644
	data : 0.1136777400970459
	model : 0.06480879783630371
			 train-loss:  2.208719115704298 	 ± 0.2562290374276249
	data : 0.1138338565826416
	model : 0.06483001708984375
			 train-loss:  2.1990941112691704 	 ± 0.2581246362111799
	data : 0.11392688751220703
	model : 0.06484746932983398
			 train-loss:  2.1960114345831028 	 ± 0.2549161953893237
	data : 0.1139744758605957
	model : 0.06479778289794921
			 train-loss:  2.1883064712796894 	 ± 0.25523340319084864
	data : 0.11389555931091308
	model : 0.06474361419677735
			 train-loss:  2.196396188603507 	 ± 0.2561738774137637
	data : 0.11376328468322754
	model : 0.06466107368469239
			 train-loss:  2.196293937193381 	 ± 0.25268910090072577
	data : 0.11369867324829101
	model : 0.06469054222106933
			 train-loss:  2.198784385856829 	 ± 0.24980183755746577
	data : 0.11377577781677246
	model : 0.06473712921142578
			 train-loss:  2.1893640756607056 	 ± 0.25332417721378636
	data : 0.11380238533020019
	model : 0.06477303504943847
			 train-loss:  2.19593508541584 	 ± 0.25348127917244045
	data : 0.11389894485473633
	model : 0.06482586860656739
			 train-loss:  2.19345769940353 	 ± 0.2508607503941233
	data : 0.11392855644226074
	model : 0.06485228538513184
			 train-loss:  2.1944438815116882 	 ± 0.24793674786648065
	data : 0.11390256881713867
	model : 0.06475582122802734
			 train-loss:  2.1994791446730146 	 ± 0.24720011656254165
	data : 0.11378355026245117
	model : 0.06472673416137695
			 train-loss:  2.2128528275273065 	 ± 0.2596340957503243
	data : 0.11371240615844727
	model : 0.06468400955200196
			 train-loss:  2.2145038260353935 	 ± 0.2569665381576985
	data : 0.11373362541198731
	model : 0.06469192504882812
			 train-loss:  2.2081563239512234 	 ± 0.2577002386570962
	data : 0.11376934051513672
	model : 0.06471977233886719
			 train-loss:  2.2083678220180754 	 ± 0.2549480425806075
	data : 0.11389632225036621
	model : 0.06478438377380372
			 train-loss:  2.20853670189778 	 ± 0.2522810126951101
	data : 0.11394634246826171
	model : 0.0647202968597412
			 train-loss:  2.2057424783706665 	 ± 0.25044278033977735
	data : 0.11384096145629882
	model : 0.06470093727111817
			 train-loss:  2.205933725833893 	 ± 0.247929317959472
	data : 0.1137761116027832
	model : 0.06467766761779785
			 train-loss:  2.201071112763648 	 ± 0.24788288269385084
	data : 0.11380276679992676
	model : 0.0646881103515625
			 train-loss:  2.1966828360007358 	 ± 0.24748005329183564
	data : 0.11373491287231445
	model : 0.06469311714172363
			 train-loss:  2.197661091696541 	 ± 0.24523569874998777
	data : 0.11367111206054688
	model : 0.06474890708923339
			 train-loss:  2.202824228339725 	 ± 0.24584488408430652
	data : 0.11395349502563476
	model : 0.06476631164550781
			 train-loss:  2.2032413677735763 	 ± 0.24361896386125056
	data : 0.11379261016845703
	model : 0.06472525596618653
			 train-loss:  2.1989285200834274 	 ± 0.2435434470412368
	data : 0.11370334625244141
	model : 0.06470766067504882
			 train-loss:  2.198761553095098 	 ± 0.24140088154876044
	data : 0.11362028121948242
	model : 0.06472063064575195
			 train-loss:  2.195602390272864 	 ± 0.24049642924818437
	data : 0.11373968124389648
	model : 0.06475300788879394
			 train-loss:  2.1909181586766646 	 ± 0.24110341320436254
	data : 0.11379337310791016
	model : 0.06481800079345704
			 train-loss:  2.1906814257303875 	 ± 0.23909269074551473
	data : 0.11400318145751953
	model : 0.06490187644958496
			 train-loss:  2.1934395305445937 	 ± 0.23808529351941973
	data : 0.11409306526184082
	model : 0.0649073600769043
			 train-loss:  2.1910456765082573 	 ± 0.2368963948681605
	data : 0.11410632133483886
	model : 0.064886474609375
			 train-loss:  2.189605576651437 	 ± 0.2352821515519365
	data : 0.11413211822509765
	model : 0.06485910415649414
			 train-loss:  2.1876728385686874 	 ± 0.2339402956714183
	data : 0.11386613845825196
	model : 0.0648773193359375
			 train-loss:  2.186839067018949 	 ± 0.2322295910407933
	data : 0.11380243301391602
	model : 0.06486291885375976
			 train-loss:  2.181968123623819 	 ± 0.2337854758780113
	data : 0.11390137672424316
	model : 0.06488265991210937
			 train-loss:  2.1788573407415135 	 ± 0.2334064540872585
	data : 0.11402740478515624
	model : 0.06492471694946289
			 train-loss:  2.182368881562177 	 ± 0.2334600370422121
	data : 0.11400771141052246
	model : 0.06493902206420898
			 train-loss:  2.186097528623498 	 ± 0.23379279889689178
	data : 0.11409735679626465
	model : 0.06489057540893554
			 train-loss:  2.1861698968069896 	 ± 0.2321176216539692
	data : 0.11404728889465332
	model : 0.0648162841796875
			 train-loss:  2.1965411313822574 	 ± 0.24627040496394118
	data : 0.11381764411926269
	model : 0.06469740867614746
			 train-loss:  2.1950779888365 	 ± 0.24486477776941284
	data : 0.11377458572387696
	model : 0.06468830108642579
			 train-loss:  2.1917995426752794 	 ± 0.24476780341455281
	data : 0.11395654678344727
	model : 0.06474714279174805
			 train-loss:  2.190788223936751 	 ± 0.24326184990092117
	data : 0.11415143013000488
	model : 0.06477975845336914
			 train-loss:  2.191310469309489 	 ± 0.24167642146603172
	data : 0.11417913436889648
	model : 0.06486482620239258
			 train-loss:  2.1901105736431323 	 ± 0.24030595971114646
	data : 0.11443452835083008
	model : 0.06499075889587402
			 train-loss:  2.192187928534173 	 ± 0.23942632370248781
	data : 0.11441802978515625
	model : 0.06498374938964843
			 train-loss:  2.1920825762626452 	 ± 0.23788838490014907
	data : 0.11413660049438476
	model : 0.06491994857788086
			 train-loss:  2.1941789373566833 	 ± 0.23710194494780226
	data : 0.11408319473266601
	model : 0.0648651123046875
			 train-loss:  2.192786255478859 	 ± 0.2359403337848747
	data : 0.11410908699035645
	model : 0.06487255096435547
			 train-loss:  2.1907920955139915 	 ± 0.23515679391874947
	data : 0.11403770446777343
	model : 0.06492581367492675
			 train-loss:  2.18994917811417 	 ± 0.233841601108326
	data : 0.1141134262084961
	model : 0.06494436264038086
			 train-loss:  2.187244455498385 	 ± 0.23371553094970515
	data : 0.11420416831970215
	model : 0.06506242752075195
			 train-loss:  2.188191757315681 	 ± 0.23248044821171202
	data : 0.11411490440368652
	model : 0.06510367393493652
			 train-loss:  2.187725771174711 	 ± 0.23114832876893485
	data : 0.11429004669189453
	model : 0.06511497497558594
			 train-loss:  2.184127523455509 	 ± 0.23218269513866377
	data : 0.11432065963745117
	model : 0.06507744789123535
			 train-loss:  2.1844029193637016 	 ± 0.23085858222600694
	data : 0.11417441368103028
	model : 0.0649878978729248
			 train-loss:  2.1926167891784147 	 ± 0.2419911948634295
	data : 0.11424484252929687
	model : 0.06477675437927247
			 train-loss:  2.192951629670818 	 ± 0.24064835382863853
	data : 0.11421289443969726
	model : 0.06479744911193848
			 train-loss:  2.19147225883272 	 ± 0.23971430397760976
	data : 0.11405820846557617
	model : 0.06479086875915527
			 train-loss:  2.1926307324524763 	 ± 0.23864675164576646
	data : 0.11399869918823242
	model : 0.0647963047027588
			 train-loss:  2.1908420337283094 	 ± 0.237958767352607
	data : 0.11406488418579101
	model : 0.06488499641418458
			 train-loss:  2.1959749485856745 	 ± 0.2417424614893356
	data : 0.11395101547241211
	model : 0.06499481201171875
			 train-loss:  2.197761313712343 	 ± 0.2410694797938983
	data : 0.11402177810668945
	model : 0.06495127677917481
			 train-loss:  2.1943844883065475 	 ± 0.24202198217223844
	data : 0.11398639678955078
	model : 0.06487932205200195
			 train-loss:  2.1946951039135456 	 ± 0.2407771856952993
	data : 0.11396670341491699
	model : 0.0648679256439209
			 train-loss:  2.1913805229147685 	 ± 0.24172439999919484
	data : 0.11400327682495118
	model : 0.06491031646728515
			 train-loss:  2.1894729891601874 	 ± 0.24122065939120552
	data : 0.1141077995300293
	model : 0.06494488716125488
			 train-loss:  2.1876120639569834 	 ± 0.24070528268427166
	data : 0.11406803131103516
	model : 0.06499338150024414
			 train-loss:  2.1883758997917173 	 ± 0.23961928940311542
	data : 0.11407713890075684
	model : 0.06504850387573242
			 train-loss:  2.1905200599443795 	 ± 0.23939226592145885
	data : 0.11410355567932129
	model : 0.06503477096557617
			 train-loss:  2.18922398370855 	 ± 0.23857172627799575
	data : 0.11410079002380372
	model : 0.06495532989501954
			 train-loss:  2.1888884294380264 	 ± 0.23743497292071664
	data : 0.11388659477233887
	model : 0.0648867130279541
			 train-loss:  2.1943340530762305 	 ± 0.2426679658849606
	data : 0.11367287635803222
	model : 0.0647737979888916
			 train-loss:  2.2006708372206916 	 ± 0.2500060173162761
	data : 0.11366891860961914
	model : 0.06475930213928223
			 train-loss:  2.1972023428611034 	 ± 0.25134946836384914
	data : 0.1138768196105957
	model : 0.06476011276245117
			 train-loss:  2.198813052935021 	 ± 0.25072121004594006
	data : 0.11386871337890625
	model : 0.06484050750732422
			 train-loss:  2.195598582426707 	 ± 0.25176316156474343
	data : 0.11404352188110352
	model : 0.06490349769592285
			 train-loss:  2.192526689363182 	 ± 0.2526308047791495
	data : 0.11438298225402832
	model : 0.06504111289978028
			 train-loss:  2.191550411961295 	 ± 0.2516863333533859
	data : 0.11455860137939453
	model : 0.06503663063049317
			 train-loss:  2.1914104609876066 	 ± 0.25055434561941947
	data : 0.11421403884887696
	model : 0.0649726390838623
			 train-loss:  2.1922524305326596 	 ± 0.24959097781142062
	data : 0.11408038139343261
	model : 0.06485819816589355
			 train-loss:  2.192127631828848 	 ± 0.24848764874766446
	data : 0.11398143768310547
	model : 0.06483330726623535
			 train-loss:  2.193611018490373 	 ± 0.24789741484940855
	data : 0.11383662223815919
	model : 0.06477885246276856
			 train-loss:  2.1958469484163365 	 ± 0.24796911678810946
	data : 0.11375217437744141
	model : 0.0648129940032959
			 train-loss:  2.193607050797035 	 ± 0.24806365973863473
	data : 0.11379632949829102
	model : 0.06490540504455566
			 train-loss:  2.190443846914503 	 ± 0.2493397557188476
	data : 0.11395487785339356
	model : 0.06500239372253418
			 train-loss:  2.189137279987335 	 ± 0.24868288928030577
	data : 0.11395831108093261
	model : 0.06498398780822753
			 train-loss:  2.1870813760436882 	 ± 0.24864079367320652
	data : 0.11393308639526367
	model : 0.06495394706726074
			 train-loss:  2.188625461856524 	 ± 0.24817489564447437
	data : 0.11397027969360352
	model : 0.06492066383361816
			 train-loss:  2.190320660260098 	 ± 0.2478439183082454
	data : 0.11390395164489746
	model : 0.06486577987670898
			 train-loss:  2.1913264002956327 	 ± 0.24707388395396887
	data : 0.11384463310241699
	model : 0.06482386589050293
			 train-loss:  2.195150636075958 	 ± 0.24966662230960227
	data : 0.11388578414916992
	model : 0.064837646484375
			 train-loss:  2.1946291452453983 	 ± 0.24872511651706566
	data : 0.11392407417297364
	model : 0.06491227149963379
			 train-loss:  2.199021897315979 	 ± 0.2525114011112802
	data : 0.11380085945129395
	model : 0.06494011878967285
			 train-loss:  2.194615281763531 	 ± 0.2562874218323594
	data : 0.11387701034545898
	model : 0.06496629714965821
			 train-loss:  2.195531932387765 	 ± 0.25548370397576614
	data : 0.11398868560791016
	model : 0.06494817733764649
			 train-loss:  2.1960545917972922 	 ± 0.25455191818521844
	data : 0.11396961212158203
	model : 0.0649454116821289
			 train-loss:  2.197555805361548 	 ± 0.2541315513889376
	data : 0.11398992538452149
	model : 0.06488547325134278
			 train-loss:  2.1982641155903155 	 ± 0.2532800308528925
	data : 0.11406745910644531
	model : 0.06488456726074218
			 train-loss:  2.196093992422555 	 ± 0.25352179385007334
	data : 0.11415181159973145
	model : 0.06492366790771484
			 train-loss:  2.198473607048844 	 ± 0.25402396960467116
	data : 0.1141141414642334
	model : 0.06493449211120605
			 train-loss:  2.1981889663782335 	 ± 0.2530883196646022
	data : 0.11413507461547852
	model : 0.06494169235229492
			 train-loss:  2.197076372246244 	 ± 0.25246845549652513
	data : 0.11402769088745117
	model : 0.0648890495300293
			 train-loss:  2.198157070301197 	 ± 0.25184255042594
	data : 0.11372637748718262
	model : 0.06480975151062011
			 train-loss:  2.1990038209101734 	 ± 0.25110775680183717
	data : 0.11366996765136719
	model : 0.06474714279174805
			 train-loss:  2.2007219304133505 	 ± 0.2509906526493813
	data : 0.11353678703308105
	model : 0.06474652290344238
			 train-loss:  2.201958910278652 	 ± 0.2504983813571344
	data : 0.11354022026062012
	model : 0.06474785804748535
			 train-loss:  2.2020585296822968 	 ± 0.24959842500439602
	data : 0.11358432769775391
	model : 0.06482658386230469
			 train-loss:  2.203714573383331 	 ± 0.24947060560436368
	data : 0.11384925842285157
	model : 0.06488432884216308
			 train-loss:  2.2044742766847003 	 ± 0.24874685288774703
	data : 0.11387267112731933
	model : 0.0649024486541748
			 train-loss:  2.2043845015512384 	 ± 0.24787172853582767
	data : 0.11391572952270508
	model : 0.06484918594360352
			 train-loss:  2.203099355831013 	 ± 0.2474778128155041
	data : 0.11378026008605957
	model : 0.06486773490905762
			 train-loss:  2.200917329225275 	 ± 0.24799357010099846
	data : 0.11377291679382324
	model : 0.06487975120544434
			 train-loss:  2.2034705071613705 	 ± 0.24902883893068645
	data : 0.1138228416442871
	model : 0.06489219665527343
			 train-loss:  2.2027963946943414 	 ± 0.24830725294710962
	data : 0.1137784481048584
	model : 0.06492452621459961
			 train-loss:  2.203386061045588 	 ± 0.24756377983042854
	data : 0.11386117935180665
	model : 0.0649940013885498
			 train-loss:  2.2024730097603156 	 ± 0.24697422247507592
	data : 0.11405353546142578
	model : 0.0649487018585205
			 train-loss:  2.2003623479164687 	 ± 0.24747973629497938
	data : 0.11396760940551758
	model : 0.064886474609375
			 train-loss:  2.2030867862701418 	 ± 0.24888526238253497
	data : 0.11386923789978028
	model : 0.06486339569091797
			 train-loss:  2.2032883151477534 	 ± 0.24807204918025713
	data : 0.11390786170959473
	model : 0.06483683586120606
			 train-loss:  2.2017248861099543 	 ± 0.24799993090187056
	data : 0.11395740509033203
	model : 0.06487312316894531
			 train-loss:  2.199235521110834 	 ± 0.24908615701049516
	data : 0.11387796401977539
	model : 0.06496634483337402
			 train-loss:  2.2007943198278355 	 ± 0.24902369091044224
	data : 0.1140512466430664
	model : 0.06501250267028809
			 train-loss:  2.19973734348051 	 ± 0.24856541333950485
	data : 0.11404671669006347
	model : 0.06508946418762207
			 train-loss:  2.203516898246912 	 ± 0.2521961316725633
	data : 0.114019775390625
	model : 0.06504268646240234
			 train-loss:  2.2034900302340272 	 ± 0.25139190023520414
	data : 0.11387548446655274
	model : 0.06495614051818847
			 train-loss:  2.2035253387463243 	 ± 0.2505954840067731
	data : 0.11393308639526367
	model : 0.06487202644348145
			 train-loss:  2.205965912566995 	 ± 0.2516828387333639
	data : 0.11384172439575195
	model : 0.064813232421875
			 train-loss:  2.2042831242084504 	 ± 0.25179079060017995
	data : 0.11384620666503906
	model : 0.0648036003112793
			 train-loss:  2.204754465114996 	 ± 0.2510784101690384
	data : 0.11391720771789551
	model : 0.06483726501464844
			 train-loss:  2.203965112015053 	 ± 0.25050258598974456
	data : 0.1139909267425537
	model : 0.06486849784851074
			 train-loss:  2.203691387469052 	 ± 0.249757291486858
	data : 0.11399378776550292
	model : 0.06488757133483887
			 train-loss:  2.201886343519862 	 ± 0.25005885433633374
	data : 0.11400890350341797
	model : 0.06494898796081543
			 train-loss:  2.1999277656728573 	 ± 0.2505585217942526
	data : 0.11404504776000976
	model : 0.0648876667022705
			 train-loss:  2.2012029275836715 	 ± 0.2503391277830292
	data : 0.11406402587890625
	model : 0.0648679256439209
			 train-loss:  2.2015850822368783 	 ± 0.24963704525281913
	data : 0.11394834518432617
	model : 0.06486668586730956
			 train-loss:  2.1990155222870054 	 ± 0.251098288710741
	data : 0.11398954391479492
	model : 0.06488537788391113
			 train-loss:  2.1992945727511977 	 ± 0.2503804177946256
	data : 0.11403336524963378
	model : 0.06488547325134278
			 train-loss:  2.199601338891422 	 ± 0.2496747697557561
	data : 0.11409487724304199
	model : 0.06488533020019531
			 train-loss:  2.1990180698751707 	 ± 0.24905978878641075
	data : 0.11397995948791503
	model : 0.06494178771972656
			 train-loss:  2.198964389257653 	 ± 0.2483357144774183
	data : 0.11408748626708984
	model : 0.06492090225219727
			 train-loss:  2.1986381607937675 	 ± 0.24765390078420588
	data : 0.11392602920532227
	model : 0.06486353874206544
			 train-loss:  2.1978771823576126 	 ± 0.24714398932142995
	data : 0.11386823654174805
	model : 0.06481585502624512
			 train-loss:  2.198479246412005 	 ± 0.24656478608402846
	data : 0.11380329132080078
	model : 0.06482796669006348
			 train-loss:  2.1975850625471636 	 ± 0.24614771178819778
	data : 0.11389355659484864
	model : 0.06483578681945801
			 train-loss:  2.199832560652393 	 ± 0.2472557514043338
	data : 0.11397833824157715
	model : 0.06487116813659669
			 train-loss:  2.2006830673539235 	 ± 0.24681974044632793
	data : 0.11428389549255372
	model : 0.0649031639099121
			 train-loss:  2.1979668207008745 	 ± 0.2487829022119335
	data : 0.1143526554107666
	model : 0.06492838859558106
			 train-loss:  2.197093441751268 	 ± 0.24836590418112126
	data : 0.11437325477600098
	model : 0.06493582725524902
			 train-loss:  2.1974645372253754 	 ± 0.24772889607279294
	data : 0.1143296241760254
	model : 0.06489248275756836
			 train-loss:  2.1978234222957065 	 ± 0.24709456254584128
	data : 0.1142848014831543
	model : 0.06490111351013184
			 train-loss:  2.196460936890274 	 ± 0.24710310339888206
	data : 0.11406922340393066
	model : 0.06490402221679688
			 train-loss:  2.1954843265854795 	 ± 0.24678459367634528
	data : 0.11408934593200684
	model : 0.06492042541503906
			 train-loss:  2.2016486251676404 	 ± 0.25993301138900854
	data : 0.11415672302246094
	model : 0.06489434242248535
			 train-loss:  2.203575870683116 	 ± 0.26055528728397626
	data : 0.11430754661560058
	model : 0.06490240097045899
			 train-loss:  2.204993877181395 	 ± 0.26057630885018157
	data : 0.11423172950744628
	model : 0.06486401557922364
			 train-loss:  2.2053471163232277 	 ± 0.25992725120533755
	data : 0.11413531303405762
	model : 0.06482396125793458
			 train-loss:  2.2046001417927013 	 ± 0.2594409424654531
	data : 0.11398935317993164
	model : 0.06482257843017578
			 train-loss:  2.2034195692915666 	 ± 0.25926581066784515
	data : 0.11388664245605469
	model : 0.0648991584777832
			 train-loss:  2.202141009700236 	 ± 0.25918608310828406
	data : 0.11371331214904785
	model : 0.06495299339294433
			 train-loss:  2.2019947015990815 	 ± 0.2585181459959082
	data : 0.11377692222595215
	model : 0.0650247573852539
			 train-loss:  2.201024605820216 	 ± 0.25819768086025335
	data : 0.11387743949890136
	model : 0.06511354446411133
			 train-loss:  2.1994346392523383 	 ± 0.2584768960456148
	data : 0.1140294075012207
	model : 0.06513090133666992
			 train-loss:  2.198968856762617 	 ± 0.2578948967742272
	data : 0.11402850151062012
	model : 0.06508011817932129
			 train-loss:  2.1982010128546734 	 ± 0.2574595329625962
	data : 0.11399741172790527
	model : 0.06500511169433594
			 train-loss:  2.199583093527005 	 ± 0.2575331546699518
	data : 0.11403870582580566
	model : 0.06492242813110352
			 train-loss:  2.1992025941309303 	 ± 0.2569375042326062
	data : 0.11404471397399903
	model : 0.06488161087036133
			 train-loss:  2.201518567962263 	 ± 0.25835471245591385
	data : 0.11403236389160157
	model : 0.0648529052734375
			 train-loss:  2.2030023169517516 	 ± 0.2585566134575461
	data : 0.11410384178161621
	model : 0.06484751701354981
			 train-loss:  2.2038747173043625 	 ± 0.25820756030577796
	data : 0.11418318748474121
	model : 0.06488866806030273
			 train-loss:  2.2049891689036154 	 ± 0.25805180004321243
	data : 0.11423873901367188
	model : 0.06495184898376465
			 train-loss:  2.2038943468056287 	 ± 0.2578852898121888
	data : 0.11420445442199707
	model : 0.06494317054748536
			 train-loss:  2.202217408839394 	 ± 0.25835959285463694
	data : 0.11411209106445312
	model : 0.06497187614440918
			 train-loss:  2.2016515470132596 	 ± 0.25785536982653084
	data : 0.11409611701965332
	model : 0.06496200561523438
			 train-loss:  2.2012605950670334 	 ± 0.2572896434230037
	data : 0.11406536102294922
	model : 0.06497416496276856
			 train-loss:  2.2015957849613135 	 ± 0.2567125011918987
	data : 0.11395201683044434
	model : 0.06499176025390625
			 train-loss:  2.2022884149964037 	 ± 0.2562884712253619
	data : 0.11399273872375489
	model : 0.06500186920166015
			 train-loss:  2.2026611041794553 	 ± 0.25573109831352514
	data : 0.11405482292175292
	model : 0.06500935554504395
			 train-loss:  2.201641958100455 	 ± 0.2555465777356527
	data : 0.11404776573181152
	model : 0.06509571075439453
			 train-loss:  2.203065498180299 	 ± 0.2557735589235159
	data : 0.11439533233642578
	model : 0.06507248878479004
			 train-loss:  2.2023536754104325 	 ± 0.255379012338482
	data : 0.1143075942993164
	model : 0.06501293182373047
			 train-loss:  2.2040147255284124 	 ± 0.2559241618779142
	data : 0.1142364501953125
	model : 0.06493134498596191
			 train-loss:  2.202286624462805 	 ± 0.2565681275046864
	data : 0.1142606258392334
	model : 0.06488089561462403
			 train-loss:  2.202713228935419 	 ± 0.25604682611514196
	data : 0.11423773765563965
	model : 0.06485309600830078
			 train-loss:  2.200936744058574 	 ± 0.2567780694143099
	data : 0.11397795677185059
	model : 0.06485509872436523
			 train-loss:  2.2007362397585046 	 ± 0.25620267902314203
	data : 0.11403794288635254
	model : 0.06488895416259766
			 train-loss:  2.1997918798289167 	 ± 0.2559926496640591
	data : 0.11415657997131348
	model : 0.06493244171142579
			 train-loss:  2.199280209737281 	 ± 0.255519229206329
	data : 0.11418046951293945
	model : 0.0649604320526123
			 train-loss:  2.200851956280795 	 ± 0.2559967152433333
	data : 0.11415443420410157
	model : 0.06484518051147461
			 train-loss:  2.2009851468634283 	 ± 0.2554245203180826
	data : 0.11410956382751465
	model : 0.06472253799438477
			 train-loss:  2.1993425880466497 	 ± 0.25601574560026735
	data : 0.11413002014160156
	model : 0.06463780403137206
			 train-loss:  2.1996004977033814 	 ± 0.25546997728804416
	data : 0.11415915489196778
	model : 0.06457300186157226
			 train-loss:  2.201733444418226 	 ± 0.25688144746136765
	data : 0.11420722007751465
	model : 0.06444344520568848
			 train-loss:  2.2022033299340142 	 ± 0.2564064258167089
	data : 0.11435952186584472
	model : 0.06433119773864746
			 train-loss:  2.20212755372039 	 ± 0.2558410509941714
	data : 0.11448402404785156
	model : 0.06423406600952149
			 train-loss:  2.201251035219772 	 ± 0.25561676248895854
	data : 0.11473288536071777
	model : 0.06407737731933594
			 train-loss:  2.2001807799464776 	 ± 0.2555647999033929
	data : 0.11469759941101074
	model : 0.06390771865844727
			 train-loss:  2.2015255189878973 	 ± 0.2558133174889144
	data : 0.11477947235107422
	model : 0.06375808715820312
			 train-loss:  2.2002746711606567 	 ± 0.25595747252215945
	data : 0.11487398147583008
	model : 0.06376209259033203
			 train-loss:  2.200125055911737 	 ± 0.2554129300772723
	data : 0.1149259090423584
	model : 0.06378774642944336
			 train-loss:  2.20190101091204 	 ± 0.25628724888830573
	data : 0.11488490104675293
	model : 0.06379046440124511
			 train-loss:  2.2028255846367375 	 ± 0.25612413802545275
	data : 0.11498522758483887
	model : 0.06385664939880371
			 train-loss:  2.2025192155797257 	 ± 0.25561906005793517
	data : 0.1149897575378418
	model : 0.06390714645385742
			 train-loss:  2.2022248171745464 	 ± 0.2551143615967569
	data : 0.1147526741027832
	model : 0.06383557319641113
			 train-loss:  2.2020172916226466 	 ± 0.2545931683097362
	data : 0.11468596458435058
	model : 0.06376099586486816
			 train-loss:  2.201130555148869 	 ± 0.2544204319528314
	data : 0.114631986618042
	model : 0.06378931999206543
			 train-loss:  2.2017874327026496 	 ± 0.2540866881875877
	data : 0.11476325988769531
	model : 0.06378226280212403
			 train-loss:  2.2007116792591046 	 ± 0.2540971149472792
	data : 0.11477422714233398
	model : 0.06387672424316407
			 train-loss:  2.2004348526398343 	 ± 0.2536033060512638
	data : 0.11499462127685547
	model : 0.06393122673034668
			 train-loss:  2.1992675372673762 	 ± 0.2537218973009774
	data : 0.11491742134094238
	model : 0.06397366523742676
			 train-loss:  2.1989032458667914 	 ± 0.2532602856293091
	data : 0.11495723724365234
	model : 0.06394076347351074
			 train-loss:  2.2012600344394953 	 ± 0.25538401664308563
	data : 0.11489534378051758
	model : 0.06394586563110352
			 train-loss:  2.201845080637541 	 ± 0.2550232746301476
	data : 0.11482605934143067
	model : 0.06386356353759766
			 train-loss:  2.2029194301488446 	 ± 0.2550549849449179
	data : 0.11480827331542968
	model : 0.06385183334350586
			 train-loss:  2.202951023733713 	 ± 0.2545365329758565
	data : 0.11484627723693848
	model : 0.06385345458984375
			 train-loss:  2.202716642545785 	 ± 0.2540473528572329
	data : 0.11468162536621093
	model : 0.06382904052734376
			 train-loss:  2.2011557274287745 	 ± 0.25471870751688835
	data : 0.11462631225585937
	model : 0.06380195617675781
			 train-loss:  2.202204078555586 	 ± 0.2547422484499181
	data : 0.11469593048095703
	model : 0.06381516456604004
			 train-loss:  2.2019707798957824 	 ± 0.25425890609896123
	data : 0.11468505859375
	model : 0.0638972282409668
			 train-loss:  2.201804016215868 	 ± 0.25376560786885954
	data : 0.11467947959899902
	model : 0.0639183521270752
			 train-loss:  2.2006352827662514 	 ± 0.2539375721663357
	data : 0.11485629081726074
	model : 0.06400752067565918
			 train-loss:  2.2019196593243144 	 ± 0.25425404022539916
	data : 0.11481385231018067
	model : 0.06396799087524414
			 train-loss:  2.201298692094998 	 ± 0.2539452017902257
	data : 0.1146148681640625
	model : 0.06386733055114746
			 train-loss:  2.202802417792526 	 ± 0.25457732273870876
	data : 0.11460165977478028
	model : 0.06377825736999512
			 train-loss:  2.1993061136454344 	 ± 0.26014151357814297
	data : 0.11441445350646973
	model : 0.05534682273864746
#epoch  92    val-loss:  2.414590283444053  train-loss:  2.1993061136454344  lr:  1.9073486328125e-08
			 train-loss:  2.2989654541015625 	 ± 0.0
	data : 5.3755152225494385
	model : 0.08199596405029297
			 train-loss:  2.342848300933838 	 ± 0.04388284683227539
	data : 2.77308189868927
	model : 0.07435834407806396
			 train-loss:  2.3178063233693442 	 ± 0.05037860767645183
	data : 1.8867034117380779
	model : 0.07105429967244466
			 train-loss:  2.2964637875556946 	 ± 0.05718404137899072
	data : 1.4434692859649658
	model : 0.06938982009887695
			 train-loss:  2.2529908180236817 	 ± 0.10087421866102657
	data : 1.1775149822235107
	model : 0.06841254234313965
			 train-loss:  2.2323198318481445 	 ± 0.10303456542704523
	data : 0.12533893585205078
	model : 0.06494283676147461
			 train-loss:  2.277475663593837 	 ± 0.1460610410281279
	data : 0.11404361724853515
	model : 0.06454586982727051
			 train-loss:  2.279951423406601 	 ± 0.1367845201384147
	data : 0.11401453018188476
	model : 0.06459932327270508
			 train-loss:  2.314526902304755 	 ± 0.1618481555006333
	data : 0.11416611671447754
	model : 0.06464381217956543
			 train-loss:  2.299208164215088 	 ± 0.1602726318652062
	data : 0.1142728328704834
	model : 0.06470365524291992
			 train-loss:  2.2735929272391577 	 ± 0.1729552242204571
	data : 0.1141636848449707
	model : 0.06468572616577148
			 train-loss:  2.2938035329182944 	 ± 0.17864453957737544
	data : 0.11418285369873046
	model : 0.06518173217773438
			 train-loss:  2.2878057039701023 	 ± 0.17288911403557214
	data : 0.11420178413391113
	model : 0.06513910293579102
			 train-loss:  2.287842239652361 	 ± 0.16660017100828817
	data : 0.11416440010070801
	model : 0.06521635055541992
			 train-loss:  2.2999966939290366 	 ± 0.1672527223615544
	data : 0.11423969268798828
	model : 0.06525063514709473
			 train-loss:  2.2998462170362473 	 ± 0.16194280075290687
	data : 0.1142298698425293
	model : 0.06530671119689942
			 train-loss:  2.2906486567328956 	 ± 0.1613577324340367
	data : 0.11412849426269531
	model : 0.06483383178710937
			 train-loss:  2.279236992200216 	 ± 0.16371835328166626
	data : 0.11409478187561035
	model : 0.06481304168701171
			 train-loss:  2.2599405740436755 	 ± 0.1791516390010563
	data : 0.11376194953918457
	model : 0.06476068496704102
			 train-loss:  2.274280917644501 	 ± 0.18546646032674322
	data : 0.11364641189575195
	model : 0.06473383903503419
			 train-loss:  2.2804994923727855 	 ± 0.1831208084475483
	data : 0.11370711326599121
	model : 0.06474442481994629
			 train-loss:  2.2818847569552334 	 ± 0.17902315731303914
	data : 0.11375546455383301
	model : 0.06474089622497559
			 train-loss:  2.2786207406417183 	 ± 0.1757561580166331
	data : 0.11384921073913574
	model : 0.06487360000610351
			 train-loss:  2.2659601122140884 	 ± 0.18245503299699203
	data : 0.11413168907165527
	model : 0.06490840911865234
			 train-loss:  2.2976349496841433 	 ± 0.23672205937216995
	data : 0.114129638671875
	model : 0.0648836612701416
			 train-loss:  2.2857113755666294 	 ± 0.23965880295370945
	data : 0.11398406028747558
	model : 0.06482944488525391
			 train-loss:  2.307937988528499 	 ± 0.2610625410340061
	data : 0.1139042854309082
	model : 0.0647928237915039
			 train-loss:  2.2939501021589552 	 ± 0.2664628256400708
	data : 0.11392149925231934
	model : 0.06476836204528809
			 train-loss:  2.2866674743849655 	 ± 0.26464902081876895
	data : 0.11404905319213868
	model : 0.06479787826538086
			 train-loss:  2.29335705836614 	 ± 0.2626827609387092
	data : 0.11406712532043457
	model : 0.06482162475585937
			 train-loss:  2.2958619094664052 	 ± 0.25877515874780965
	data : 0.11415514945983887
	model : 0.06486830711364747
			 train-loss:  2.3010960333049297 	 ± 0.256361497101245
	data : 0.11432371139526368
	model : 0.0648951530456543
			 train-loss:  2.29247003251856 	 ± 0.2571200451654359
	data : 0.11415581703186035
	model : 0.06487665176391602
			 train-loss:  2.2940670567400314 	 ± 0.2534767263567553
	data : 0.11411776542663574
	model : 0.06481389999389649
			 train-loss:  2.301325842312404 	 ± 0.2533893891823545
	data : 0.11404099464416503
	model : 0.06482057571411133
			 train-loss:  2.2934619221422405 	 ± 0.25413996319914384
	data : 0.11398830413818359
	model : 0.06482234001159667
			 train-loss:  2.2931703006899036 	 ± 0.2506882218681436
	data : 0.11385631561279297
	model : 0.06484251022338867
			 train-loss:  2.286752609830154 	 ± 0.2504290112924062
	data : 0.1139981746673584
	model : 0.0648928165435791
			 train-loss:  2.2886908451716104 	 ± 0.24748611570628012
	data : 0.11402130126953125
	model : 0.06492438316345214
			 train-loss:  2.2984010607004164 	 ± 0.2517844041431409
	data : 0.1141279697418213
	model : 0.06489334106445313
			 train-loss:  2.2945935871542953 	 ± 0.2498580225466541
	data : 0.11418476104736328
	model : 0.06483602523803711
			 train-loss:  2.2887821112360274 	 ± 0.24965441968240262
	data : 0.11418166160583496
	model : 0.06478533744812012
			 train-loss:  2.2870779619660486 	 ± 0.24698143600714306
	data : 0.11428589820861816
	model : 0.06474485397338867
			 train-loss:  2.281000638549978 	 ± 0.24738962375443757
	data : 0.11419496536254883
	model : 0.06477136611938476
			 train-loss:  2.276411090956794 	 ± 0.24651248139227436
	data : 0.11426582336425781
	model : 0.06481623649597168
			 train-loss:  2.281608610049538 	 ± 0.24629857986788853
	data : 0.11440668106079102
	model : 0.06490254402160645
			 train-loss:  2.2891640688510653 	 ± 0.24899437056859472
	data : 0.1145167350769043
	model : 0.06496500968933105
			 train-loss:  2.281880716482798 	 ± 0.2513956779668203
	data : 0.11445269584655762
	model : 0.06493988037109374
			 train-loss:  2.2817870597450103 	 ± 0.24881803866894162
	data : 0.11432924270629882
	model : 0.06482887268066406
			 train-loss:  2.2865851163864135 	 ± 0.2485965689000744
	data : 0.11414179801940919
	model : 0.06477723121643067
			 train-loss:  2.292934342926624 	 ± 0.2502081498756778
	data : 0.11409139633178711
	model : 0.0647669792175293
			 train-loss:  2.2915658125510583 	 ± 0.2479832848355025
	data : 0.11402726173400879
	model : 0.06479859352111816
			 train-loss:  2.279018116447161 	 ± 0.261768096569988
	data : 0.11407499313354492
	model : 0.06488003730773925
			 train-loss:  2.276780426502228 	 ± 0.2598441554207111
	data : 0.1142643928527832
	model : 0.06499800682067872
			 train-loss:  2.2758187315680765 	 ± 0.2575680676659727
	data : 0.11441607475280761
	model : 0.06503777503967285
			 train-loss:  2.2704707980155945 	 ± 0.25832085643816016
	data : 0.1142798900604248
	model : 0.06497502326965332
			 train-loss:  2.268637046479342 	 ± 0.2564123185399471
	data : 0.11430950164794922
	model : 0.06490516662597656
			 train-loss:  2.260560152859523 	 ± 0.26140422214849407
	data : 0.11417112350463868
	model : 0.06489744186401367
			 train-loss:  2.2632046614663075 	 ± 0.2599607926663406
	data : 0.11423134803771973
	model : 0.06488285064697266
			 train-loss:  2.262536718448003 	 ± 0.2578364007524603
	data : 0.11414160728454589
	model : 0.06487870216369629
			 train-loss:  2.2582746314220743 	 ± 0.25783658203826676
	data : 0.11424603462219238
	model : 0.06499185562133789
			 train-loss:  2.255548321431683 	 ± 0.2566336854319183
	data : 0.11437997817993165
	model : 0.06504678726196289
			 train-loss:  2.2528463412844943 	 ± 0.25547618558512164
	data : 0.11440987586975097
	model : 0.0650320053100586
			 train-loss:  2.256943864747882 	 ± 0.2555504228852118
	data : 0.11431822776794434
	model : 0.06496930122375488
			 train-loss:  2.2540305962929357 	 ± 0.2546458073002186
	data : 0.11426091194152832
	model : 0.06495680809020996
			 train-loss:  2.2503973697171067 	 ± 0.2544012907661482
	data : 0.11428346633911132
	model : 0.06490912437438964
			 train-loss:  2.2468312836405055 	 ± 0.2541522503502652
	data : 0.11408004760742188
	model : 0.06491012573242187
			 train-loss:  2.243369111243416 	 ± 0.25386328729858004
	data : 0.11417427062988281
	model : 0.0649073600769043
			 train-loss:  2.2432444320208784 	 ± 0.2520190815210042
	data : 0.11422820091247558
	model : 0.06496949195861816
			 train-loss:  2.2441344925335476 	 ± 0.25032167782866216
	data : 0.11437487602233887
	model : 0.06502618789672851
			 train-loss:  2.2489215437795074 	 ± 0.25175882125712723
	data : 0.11436567306518555
	model : 0.06501893997192383
			 train-loss:  2.247806073890792 	 ± 0.2501810042994021
	data : 0.11443867683410644
	model : 0.06501927375793456
			 train-loss:  2.250158625106289 	 ± 0.24926213937212546
	data : 0.1142270565032959
	model : 0.06495771408081055
			 train-loss:  2.251017452897252 	 ± 0.24768092666780706
	data : 0.11408677101135253
	model : 0.06486425399780274
			 train-loss:  2.2569182189305623 	 ± 0.2512061022870796
	data : 0.11396713256835937
	model : 0.06479101181030274
			 train-loss:  2.261237315441433 	 ± 0.25233564548325355
	data : 0.11386594772338868
	model : 0.06479525566101074
			 train-loss:  2.258804505521601 	 ± 0.2515872873684954
	data : 0.11385202407836914
	model : 0.06478910446166992
			 train-loss:  2.2584159878584056 	 ± 0.24999259335854623
	data : 0.11409869194030761
	model : 0.06485509872436523
			 train-loss:  2.2554438612129113 	 ± 0.24978835089043483
	data : 0.11416211128234863
	model : 0.06497855186462402
			 train-loss:  2.257402662932873 	 ± 0.24883208779742957
	data : 0.11431879997253418
	model : 0.06502132415771485
			 train-loss:  2.258165846636266 	 ± 0.24738551174919315
	data : 0.11429939270019532
	model : 0.06499757766723632
			 train-loss:  2.2527863354217716 	 ± 0.2505939451704558
	data : 0.11419529914855957
	model : 0.06493396759033203
			 train-loss:  2.251892660037581 	 ± 0.24921119726341345
	data : 0.11405825614929199
	model : 0.06493611335754394
			 train-loss:  2.2544723734969185 	 ± 0.24883572805512014
	data : 0.11407809257507324
	model : 0.06486964225769043
			 train-loss:  2.253679501309114 	 ± 0.24747437127891095
	data : 0.11406550407409669
	model : 0.06488356590270997
			 train-loss:  2.2514300304789874 	 ± 0.24690390860815917
	data : 0.11420578956604004
	model : 0.06496248245239258
			 train-loss:  2.2533024193226607 	 ± 0.24609415874238133
	data : 0.11441397666931152
	model : 0.065020751953125
			 train-loss:  2.2536297020587055 	 ± 0.24471094281272246
	data : 0.1144178867340088
	model : 0.0649960994720459
			 train-loss:  2.256457252448864 	 ± 0.24477369437107324
	data : 0.11436634063720703
	model : 0.06497678756713868
			 train-loss:  2.250874490208096 	 ± 0.24904283245675737
	data : 0.11428327560424804
	model : 0.06496186256408691
			 train-loss:  2.2521803903055715 	 ± 0.24798034611962833
	data : 0.1142611026763916
	model : 0.06486625671386718
			 train-loss:  2.252161577991817 	 ± 0.2466290098174155
	data : 0.11428346633911132
	model : 0.06484646797180176
			 train-loss:  2.2534585973267913 	 ± 0.24561472824162872
	data : 0.11439366340637207
	model : 0.0648338794708252
			 train-loss:  2.2528692204901515 	 ± 0.24437088082466715
	data : 0.11451029777526855
	model : 0.06489877700805664
			 train-loss:  2.249780141679864 	 ± 0.244919399795653
	data : 0.11455821990966797
	model : 0.06494145393371582
			 train-loss:  2.2477599245806537 	 ± 0.24443482697020988
	data : 0.11460366249084472
	model : 0.06499557495117188
			 train-loss:  2.2471393961267374 	 ± 0.24324758424640971
	data : 0.11453919410705567
	model : 0.06501078605651855
			 train-loss:  2.248263807929292 	 ± 0.24225658973207093
	data : 0.11440715789794922
	model : 0.06497316360473633
			 train-loss:  2.245854875054022 	 ± 0.24220680100669023
	data : 0.11422219276428222
	model : 0.06486964225769043
			 train-loss:  2.2492457067966463 	 ± 0.24334290488963767
	data : 0.11417927742004394
	model : 0.06482858657836914
			 train-loss:  2.248295858354852 	 ± 0.24232147205773016
	data : 0.11406307220458985
	model : 0.06483974456787109
			 train-loss:  2.2464535668784498 	 ± 0.24184046467486897
	data : 0.11409611701965332
	model : 0.0648355484008789
			 train-loss:  2.2455736722760986 	 ± 0.24082762948646305
	data : 0.11423616409301758
	model : 0.06493153572082519
			 train-loss:  2.2440645889594006 	 ± 0.2401558656344092
	data : 0.11423702239990234
	model : 0.06502132415771485
			 train-loss:  2.2423305023284184 	 ± 0.23966286629389252
	data : 0.11422390937805176
	model : 0.06504817008972168
			 train-loss:  2.242614486307468 	 ± 0.2385474517769327
	data : 0.11419825553894043
	model : 0.06498355865478515
			 train-loss:  2.240365998767247 	 ± 0.23855600846959146
	data : 0.11411724090576172
	model : 0.0649794578552246
			 train-loss:  2.236859671495579 	 ± 0.24020309674796805
	data : 0.11393594741821289
	model : 0.06495194435119629
			 train-loss:  2.235620410070507 	 ± 0.239445308181056
	data : 0.1140169620513916
	model : 0.0649538516998291
			 train-loss:  2.2330717986280266 	 ± 0.23983502190796624
	data : 0.1141209602355957
	model : 0.06493730545043945
			 train-loss:  2.233609411093566 	 ± 0.23881881202323293
	data : 0.1142575740814209
	model : 0.06500124931335449
			 train-loss:  2.2342397325805257 	 ± 0.2378429942494404
	data : 0.11434121131896972
	model : 0.06497931480407715
			 train-loss:  2.2330171312906044 	 ± 0.23714149608797866
	data : 0.11441922187805176
	model : 0.06497879028320312
			 train-loss:  2.233221849851441 	 ± 0.23610913987802234
	data : 0.11450858116149902
	model : 0.06493067741394043
			 train-loss:  2.232403076213339 	 ± 0.235242830559601
	data : 0.11430249214172364
	model : 0.06484451293945312
			 train-loss:  2.2319044501617036 	 ± 0.23428768547608414
	data : 0.11427407264709473
	model : 0.06478533744812012
			 train-loss:  2.2289044989479914 	 ± 0.23551121875927827
	data : 0.11430950164794922
	model : 0.0648496150970459
			 train-loss:  2.229066229472726 	 ± 0.23451769144999465
	data : 0.11436972618103028
	model : 0.06483340263366699
			 train-loss:  2.229481866379746 	 ± 0.23357388533533047
	data : 0.11442174911499023
	model : 0.06489396095275879
			 train-loss:  2.226935734351476 	 ± 0.23425108762407165
	data : 0.11454029083251953
	model : 0.06502609252929688
			 train-loss:  2.226642275644728 	 ± 0.23330324819660914
	data : 0.11448698043823242
	model : 0.06510696411132813
			 train-loss:  2.2259809189155453 	 ± 0.2324589840657268
	data : 0.11434049606323242
	model : 0.06507368087768554
			 train-loss:  2.2235598951820434 	 ± 0.2330513635105631
	data : 0.1142496109008789
	model : 0.06507406234741211
			 train-loss:  2.2207853572983898 	 ± 0.23414053879429864
	data : 0.11396384239196777
	model : 0.06496520042419433
			 train-loss:  2.2183348598480226 	 ± 0.23479316398301947
	data : 0.1138382911682129
	model : 0.06493525505065918
			 train-loss:  2.216834274549333 	 ± 0.2344606084693876
	data : 0.11387138366699219
	model : 0.06489119529724122
			 train-loss:  2.215759468829538 	 ± 0.23384713898375623
	data : 0.11398053169250488
	model : 0.06497383117675781
			 train-loss:  2.2152647264301777 	 ± 0.23299860014971588
	data : 0.11410021781921387
	model : 0.0649806022644043
			 train-loss:  2.21437200280123 	 ± 0.23231340484018237
	data : 0.11425600051879883
	model : 0.06508860588073731
			 train-loss:  2.2145213750692516 	 ± 0.2314243854632762
	data : 0.1144378662109375
	model : 0.06506052017211914
			 train-loss:  2.2145074706041177 	 ± 0.23053944864195242
	data : 0.1144261360168457
	model : 0.06503233909606934
			 train-loss:  2.213000962228486 	 ± 0.2303109008354995
	data : 0.11419839859008789
	model : 0.06485981941223144
			 train-loss:  2.2118678899635946 	 ± 0.22981244357923247
	data : 0.11404099464416503
	model : 0.06485424041748047
			 train-loss:  2.2172402179063257 	 ± 0.23718823831097027
	data : 0.11409115791320801
	model : 0.06481609344482422
			 train-loss:  2.21799152692159 	 ± 0.23646811810469967
	data : 0.1140974998474121
	model : 0.06482234001159667
			 train-loss:  2.2174843234174393 	 ± 0.23567083971085
	data : 0.11404995918273926
	model : 0.06483187675476074
			 train-loss:  2.2165953695339007 	 ± 0.23503789119933297
	data : 0.11422128677368164
	model : 0.0649322509765625
			 train-loss:  2.215994551561881 	 ± 0.2342903212813799
	data : 0.11429347991943359
	model : 0.06491193771362305
			 train-loss:  2.2158673478545046 	 ± 0.2334508116192282
	data : 0.11414070129394531
	model : 0.06489348411560059
			 train-loss:  2.2173443828310284 	 ± 0.23326647370906567
	data : 0.11407699584960937
	model : 0.06481337547302246
			 train-loss:  2.2169518555309757 	 ± 0.23248421252524762
	data : 0.11405735015869141
	model : 0.06485271453857422
			 train-loss:  2.216840624809265 	 ± 0.2316679249352518
	data : 0.11419992446899414
	model : 0.06481289863586426
			 train-loss:  2.215262886527535 	 ± 0.23162078320983304
	data : 0.11421136856079102
	model : 0.06483516693115235
			 train-loss:  2.215819193257226 	 ± 0.23091099063375264
	data : 0.11435670852661133
	model : 0.06487894058227539
			 train-loss:  2.2140544102109714 	 ± 0.23108579439545157
	data : 0.11439199447631836
	model : 0.06498603820800782
			 train-loss:  2.2140615035409796 	 ± 0.2302930607485595
	data : 0.11457223892211914
	model : 0.06494503021240235
			 train-loss:  2.215877948164129 	 ± 0.2305554922666905
	data : 0.11441020965576172
	model : 0.06492094993591309
			 train-loss:  2.2144865232544975 	 ± 0.23039373964796822
	data : 0.11430015563964843
	model : 0.06481266021728516
			 train-loss:  2.214380265882351 	 ± 0.22962294334952107
	data : 0.11416287422180176
	model : 0.06478848457336425
			 train-loss:  2.214889950752258 	 ± 0.22894080419708046
	data : 0.1141897201538086
	model : 0.06472244262695312
			 train-loss:  2.213068076316884 	 ± 0.22926984958503907
	data : 0.11407480239868165
	model : 0.06473803520202637
			 train-loss:  2.21270278566762 	 ± 0.22855851066487298
	data : 0.11407914161682128
	model : 0.06477952003479004
			 train-loss:  2.212067897023718 	 ± 0.22794479633800738
	data : 0.11425266265869141
	model : 0.06490850448608398
			 train-loss:  2.2120044154006164 	 ± 0.2272048673634496
	data : 0.11436362266540527
	model : 0.06491756439208984
			 train-loss:  2.2129514740359397 	 ± 0.22677551002589316
	data : 0.11424427032470703
	model : 0.06493024826049805
			 train-loss:  2.215269345503587 	 ± 0.22788201488283127
	data : 0.11422915458679199
	model : 0.06488499641418458
			 train-loss:  2.2166373851192986 	 ± 0.22779685264263652
	data : 0.11414361000061035
	model : 0.06489057540893554
			 train-loss:  2.216307707979709 	 ± 0.2271124023798347
	data : 0.11411852836608886
	model : 0.06484460830688477
			 train-loss:  2.2149786004480325 	 ± 0.22701266873427345
	data : 0.11415481567382812
	model : 0.06489448547363282
			 train-loss:  2.2144048109650614 	 ± 0.22641777278357053
	data : 0.1142970085144043
	model : 0.06495466232299804
			 train-loss:  2.212427556144525 	 ± 0.22709495259606738
	data : 0.11436080932617188
	model : 0.06502184867858887
			 train-loss:  2.2109547787242465 	 ± 0.22716291928129706
	data : 0.11455693244934081
	model : 0.06506648063659667
			 train-loss:  2.2104210802382487 	 ± 0.2265668824961218
	data : 0.11449108123779297
	model : 0.06510353088378906
			 train-loss:  2.209259377020161 	 ± 0.22636149460205682
	data : 0.11446208953857422
	model : 0.06501131057739258
			 train-loss:  2.209770996642835 	 ± 0.22576959807710054
	data : 0.11434941291809082
	model : 0.06497893333435059
			 train-loss:  2.20862904873239 	 ± 0.2255659973013419
	data : 0.11433463096618653
	model : 0.06500306129455566
			 train-loss:  2.207843223017847 	 ± 0.22511742941444587
	data : 0.11440362930297851
	model : 0.06497197151184082
			 train-loss:  2.2052132338285446 	 ± 0.22700510108582203
	data : 0.11450605392456055
	model : 0.06501388549804688
			 train-loss:  2.207395527489792 	 ± 0.22809314113908924
	data : 0.11443748474121093
	model : 0.06506495475769043
			 train-loss:  2.2076861472690807 	 ± 0.22745266898114475
	data : 0.11447405815124512
	model : 0.06506338119506835
			 train-loss:  2.2073423799715544 	 ± 0.22683091574226702
	data : 0.11445178985595703
	model : 0.06501874923706055
			 train-loss:  2.206898103619731 	 ± 0.2262451667931998
	data : 0.11418700218200684
	model : 0.06501173973083496
			 train-loss:  2.2053314709249947 	 ± 0.22652404671529242
	data : 0.1141230583190918
	model : 0.06495423316955566
			 train-loss:  2.2049699055737464 	 ± 0.2259222362216379
	data : 0.11415410041809082
	model : 0.06495556831359864
			 train-loss:  2.2053653492246355 	 ± 0.22533620248179523
	data : 0.1142007827758789
	model : 0.06495990753173828
			 train-loss:  2.2054412534291092 	 ± 0.22469737449748026
	data : 0.11421027183532714
	model : 0.06496229171752929
			 train-loss:  2.2057079938845447 	 ± 0.2240896796132861
	data : 0.11428794860839844
	model : 0.06496601104736328
			 train-loss:  2.2051595791002336 	 ± 0.22357840999603779
	data : 0.11433553695678711
	model : 0.06498918533325196
			 train-loss:  2.2042748282075593 	 ± 0.2232652733950607
	data : 0.1144566535949707
	model : 0.06497769355773926
			 train-loss:  2.203997555706236 	 ± 0.22267513091865374
	data : 0.11435947418212891
	model : 0.06496586799621581
			 train-loss:  2.201974989959548 	 ± 0.22371098987816443
	data : 0.11432423591613769
	model : 0.06491823196411133
			 train-loss:  2.200716101861262 	 ± 0.22373751205002032
	data : 0.11435098648071289
	model : 0.0649223804473877
			 train-loss:  2.199845958277176 	 ± 0.22343395444044045
	data : 0.114406156539917
	model : 0.0649071216583252
			 train-loss:  2.200344796413961 	 ± 0.22292812838967993
	data : 0.11440825462341309
	model : 0.06493868827819824
			 train-loss:  2.201669526100159 	 ± 0.2230498183619268
	data : 0.11460447311401367
	model : 0.06492733955383301
			 train-loss:  2.2038233068681534 	 ± 0.22437003929073712
	data : 0.11466093063354492
	model : 0.06496458053588867
			 train-loss:  2.203744460554684 	 ± 0.2237718989195971
	data : 0.11462616920471191
	model : 0.0649836540222168
			 train-loss:  2.2032681329453245 	 ± 0.22327100229704158
	data : 0.11452317237854004
	model : 0.06496272087097169
			 train-loss:  2.2032114784553567 	 ± 0.2226809098181335
	data : 0.11431741714477539
	model : 0.06489672660827636
			 train-loss:  2.2024110762696516 	 ± 0.22236655808898567
	data : 0.1141664981842041
	model : 0.06491832733154297
			 train-loss:  2.202422102708467 	 ± 0.22178373483255137
	data : 0.11408677101135253
	model : 0.06493277549743652
			 train-loss:  2.202840090418855 	 ± 0.22128083452380373
	data : 0.11433048248291015
	model : 0.06492862701416016
			 train-loss:  2.2014736493016773 	 ± 0.22151748681031783
	data : 0.11441378593444824
	model : 0.0649594783782959
			 train-loss:  2.200266661103239 	 ± 0.22158119322889533
	data : 0.1145902156829834
	model : 0.06502504348754883
			 train-loss:  2.199406681305323 	 ± 0.22133665553082166
	data : 0.11464004516601563
	model : 0.06503081321716309
			 train-loss:  2.1987006858903535 	 ± 0.22099131270888647
	data : 0.11467370986938477
	model : 0.06501059532165528
			 train-loss:  2.197974500317259 	 ± 0.2206640334109666
	data : 0.11442642211914063
	model : 0.06498041152954101
			 train-loss:  2.1986100300393923 	 ± 0.2202867704866497
	data : 0.11434807777404785
	model : 0.06490392684936523
			 train-loss:  2.1979486307307106 	 ± 0.21992959237765017
	data : 0.11418304443359376
	model : 0.06489028930664062
			 train-loss:  2.1978907322883607 	 ± 0.21938059980107188
	data : 0.11431384086608887
	model : 0.06493086814880371
			 train-loss:  2.1965158365259123 	 ± 0.21969632043548395
	data : 0.11427969932556152
	model : 0.06501154899597168
			 train-loss:  2.1964935914124593 	 ± 0.21915206989680236
	data : 0.11435074806213379
	model : 0.06501541137695313
			 train-loss:  2.1967179246723947 	 ± 0.2186348695064772
	data : 0.11435432434082031
	model : 0.06504545211791993
			 train-loss:  2.1964329095447765 	 ± 0.21813614315724497
	data : 0.11445956230163574
	model : 0.06501216888427734
			 train-loss:  2.198837415183463 	 ± 0.220296888267558
	data : 0.11424493789672852
	model : 0.06490764617919922
			 train-loss:  2.1965768105775405 	 ± 0.22213228326536846
	data : 0.11421751976013184
	model : 0.06482672691345215
			 train-loss:  2.1962139871385364 	 ± 0.22165626204615935
	data : 0.11419453620910644
	model : 0.06485962867736816
			 train-loss:  2.19668872654438 	 ± 0.2212282588641598
	data : 0.1141162395477295
	model : 0.0649339199066162
			 train-loss:  2.1963413993707683 	 ± 0.2207552102866828
	data : 0.11410756111145019
	model : 0.06497302055358886
			 train-loss:  2.195750541914077 	 ± 0.22039456920848674
	data : 0.11423354148864746
	model : 0.0650172233581543
			 train-loss:  2.1967352984640836 	 ± 0.220334303414902
	data : 0.11433396339416504
	model : 0.06503138542175294
			 train-loss:  2.1954472795972286 	 ± 0.2206088313032538
	data : 0.11421618461608887
	model : 0.06499147415161133
			 train-loss:  2.1962330419692635 	 ± 0.2203875235879194
	data : 0.11437134742736817
	model : 0.06490230560302734
			 train-loss:  2.19795617656173 	 ± 0.22130551940960655
	data : 0.11424498558044434
	model : 0.0648686408996582
			 train-loss:  2.197038674909015 	 ± 0.22119783936690204
	data : 0.11412296295166016
	model : 0.06487793922424316
			 train-loss:  2.1973844865957894 	 ± 0.22074345813875956
	data : 0.11406965255737304
	model : 0.06492390632629394
			 train-loss:  2.1976753379892093 	 ± 0.2202757254798563
	data : 0.1141120433807373
	model : 0.06492781639099121
			 train-loss:  2.1974215868416183 	 ± 0.21980171196129944
	data : 0.11410174369812012
	model : 0.06495609283447265
			 train-loss:  2.1978907759331134 	 ± 0.2194086971710104
	data : 0.11425814628601075
	model : 0.06491351127624512
			 train-loss:  2.197945873303847 	 ± 0.2189109915774151
	data : 0.11425061225891113
	model : 0.06483726501464844
			 train-loss:  2.1990074431734388 	 ± 0.21898197467647817
	data : 0.11417622566223144
	model : 0.06470575332641601
			 train-loss:  2.199894775141467 	 ± 0.218886057978643
	data : 0.11431102752685547
	model : 0.06463470458984374
			 train-loss:  2.1987924436817257 	 ± 0.21901145691135165
	data : 0.11429300308227539
	model : 0.06461381912231445
			 train-loss:  2.2010293528437614 	 ± 0.22106045170489505
	data : 0.11433806419372558
	model : 0.06460580825805665
			 train-loss:  2.19932017326355 	 ± 0.22204707489670714
	data : 0.11440467834472656
	model : 0.06448159217834473
			 train-loss:  2.1984111249974343 	 ± 0.2219744876089851
	data : 0.1146845817565918
	model : 0.0643651008605957
			 train-loss:  2.197593873818015 	 ± 0.22182551295992292
	data : 0.11458663940429688
	model : 0.06425185203552246
			 train-loss:  2.1979430840726484 	 ± 0.22140104361374302
	data : 0.11468539237976075
	model : 0.06404657363891601
			 train-loss:  2.1975407943975456 	 ± 0.2210006033934586
	data : 0.11470470428466797
	model : 0.063877534866333
			 train-loss:  2.1982997282691623 	 ± 0.22081850676316483
	data : 0.11503939628601074
	model : 0.06382699012756347
			 train-loss:  2.198015613473339 	 ± 0.2203821522692237
	data : 0.11502161026000976
	model : 0.06385722160339355
			 train-loss:  2.197476612082843 	 ± 0.2200592139524944
	data : 0.11512622833251954
	model : 0.06382250785827637
			 train-loss:  2.197611405614108 	 ± 0.21959607411063106
	data : 0.11516833305358887
	model : 0.06382017135620117
			 train-loss:  2.1970458682785687 	 ± 0.2192963245173759
	data : 0.11508135795593262
	model : 0.06385459899902343
			 train-loss:  2.1968757162702843 	 ± 0.2188447180358712
	data : 0.11491212844848633
	model : 0.06390137672424316
			 train-loss:  2.1965985692153542 	 ± 0.21842189586146876
	data : 0.1148310661315918
	model : 0.06384296417236328
			 train-loss:  2.196281504530444 	 ± 0.2180150214351256
	data : 0.11492133140563965
	model : 0.06387324333190918
			 train-loss:  2.1954548248723778 	 ± 0.21792844468725367
	data : 0.11486883163452148
	model : 0.063946533203125
			 train-loss:  2.1958141177269206 	 ± 0.21754267644008793
	data : 0.11501111984252929
	model : 0.06392669677734375
			 train-loss:  2.1975384414196015 	 ± 0.21871956056602962
	data : 0.11514129638671874
	model : 0.06388111114501953
			 train-loss:  2.1959354630149748 	 ± 0.21967347762345862
	data : 0.11521997451782226
	model : 0.06388773918151855
			 train-loss:  2.194885578529894 	 ± 0.21982418895285863
	data : 0.11514887809753419
	model : 0.0638317584991455
			 train-loss:  2.1948935519520636 	 ± 0.21937144457085153
	data : 0.1152003288269043
	model : 0.06376409530639648
			 train-loss:  2.193952506682912 	 ± 0.21941238448420444
	data : 0.11522679328918457
	model : 0.06380820274353027
			 train-loss:  2.193859355303706 	 ± 0.21896898085474154
	data : 0.11503748893737793
	model : 0.06386947631835938
			 train-loss:  2.193804647864365 	 ± 0.2185251464904869
	data : 0.11495528221130372
	model : 0.06385469436645508
			 train-loss:  2.1933375466690372 	 ± 0.2182053616257124
	data : 0.11493425369262696
	model : 0.06386771202087402
			 train-loss:  2.1938130307582115 	 ± 0.21789316791794103
	data : 0.11479206085205078
	model : 0.06383166313171387
			 train-loss:  2.1958440556583634 	 ± 0.2197948489321419
	data : 0.11470513343811035
	model : 0.06375970840454101
			 train-loss:  2.195201642036438 	 ± 0.21958892876186148
	data : 0.11481699943542481
	model : 0.06374444961547851
			 train-loss:  2.1946727355637874 	 ± 0.2193105660780341
	data : 0.11481676101684571
	model : 0.06384696960449218
			 train-loss:  2.195130404025789 	 ± 0.2189950623973762
	data : 0.11497421264648437
	model : 0.06386985778808593
			 train-loss:  2.1970373622984756 	 ± 0.22064829973427277
	data : 0.11511964797973633
	model : 0.06388735771179199
			 train-loss:  2.1963708588457482 	 ± 0.22046855942424098
	data : 0.11511297225952148
	model : 0.06391382217407227
			 train-loss:  2.195244319298688 	 ± 0.22076712078691282
	data : 0.11499123573303223
	model : 0.06386280059814453
			 train-loss:  2.195677049923688 	 ± 0.22044384446018317
	data : 0.11483793258666992
	model : 0.05533933639526367
#epoch  93    val-loss:  2.436517263713636  train-loss:  2.195677049923688  lr:  1.9073486328125e-08
			 train-loss:  2.3438291549682617 	 ± 0.0
	data : 5.633715867996216
	model : 0.07196259498596191
			 train-loss:  2.136377215385437 	 ± 0.2074519395828247
	data : 2.8799095153808594
	model : 0.06823813915252686
			 train-loss:  2.0987137953440347 	 ± 0.1775610821929949
	data : 1.9580796559651692
	model : 0.06707882881164551
			 train-loss:  2.087019205093384 	 ± 0.15510075356216838
	data : 1.497070848941803
	model : 0.06646430492401123
			 train-loss:  2.0581044912338258 	 ± 0.15029716469302565
	data : 1.2204773902893067
	model : 0.06612086296081543
			 train-loss:  2.0798651973406472 	 ± 0.145574745109306
	data : 0.11678776741027833
	model : 0.06468682289123535
			 train-loss:  2.09672326701028 	 ± 0.14096007946895378
	data : 0.11432991027832032
	model : 0.06476421356201172
			 train-loss:  2.0681967586278915 	 ± 0.15192879176390636
	data : 0.11426253318786621
	model : 0.06480488777160645
			 train-loss:  2.1100320153766208 	 ± 0.18579332874503163
	data : 0.11428728103637695
	model : 0.06486573219299316
			 train-loss:  2.1105501532554625 	 ± 0.176265881858336
	data : 0.11425175666809081
	model : 0.06488080024719238
			 train-loss:  2.089280139316212 	 ± 0.18102287532502107
	data : 0.11407837867736817
	model : 0.06493577957153321
			 train-loss:  2.081018418073654 	 ± 0.17546887005529022
	data : 0.11416411399841309
	model : 0.06493802070617676
			 train-loss:  2.0929193038206835 	 ± 0.17355255046011137
	data : 0.1141538143157959
	model : 0.06507539749145508
			 train-loss:  2.1007550188473294 	 ± 0.16960897366797753
	data : 0.1139404296875
	model : 0.06498894691467286
			 train-loss:  2.092911156018575 	 ± 0.16646548269594139
	data : 0.11401481628417968
	model : 0.0650660514831543
			 train-loss:  2.087721459567547 	 ± 0.1624279189669234
	data : 0.11387844085693359
	model : 0.06507987976074218
			 train-loss:  2.0834825810264137 	 ± 0.15848781407761828
	data : 0.11395750045776368
	model : 0.06511578559875489
			 train-loss:  2.099285476737552 	 ± 0.16723742695192467
	data : 0.11410031318664551
	model : 0.06498351097106933
			 train-loss:  2.1015860720684656 	 ± 0.16306933336835755
	data : 0.11442956924438477
	model : 0.06511354446411133
			 train-loss:  2.1026393353939055 	 ± 0.15900661942184083
	data : 0.1144106388092041
	model : 0.06507620811462403
			 train-loss:  2.1078061773663475 	 ± 0.15688554030701146
	data : 0.11467161178588867
	model : 0.06502819061279297
			 train-loss:  2.1053877797993747 	 ± 0.1536786201969075
	data : 0.11465263366699219
	model : 0.06497311592102051
			 train-loss:  2.096861937771673 	 ± 0.15552963282003593
	data : 0.11445751190185546
	model : 0.06492700576782226
			 train-loss:  2.1011760383844376 	 ± 0.15365427398596354
	data : 0.11435985565185547
	model : 0.06485586166381836
			 train-loss:  2.125023350715637 	 ± 0.19056209960983925
	data : 0.11434884071350097
	model : 0.06481218338012695
			 train-loss:  2.146133995973147 	 ± 0.21461292690519934
	data : 0.11423683166503906
	model : 0.06480922698974609
			 train-loss:  2.144930605535154 	 ± 0.21069048903147503
	data : 0.11417851448059083
	model : 0.06479940414428711
			 train-loss:  2.156249757323946 	 ± 0.21509168772885243
	data : 0.11424021720886231
	model : 0.0648111343383789
			 train-loss:  2.154378969093849 	 ± 0.21158238248036948
	data : 0.11426434516906739
	model : 0.06488847732543945
			 train-loss:  2.15605978568395 	 ± 0.2082229500913486
	data : 0.11433906555175781
	model : 0.06487622261047363
			 train-loss:  2.167920570219717 	 ± 0.21489191907981234
	data : 0.11430459022521973
	model : 0.0648895263671875
			 train-loss:  2.177481424063444 	 ± 0.21810356558520053
	data : 0.11434259414672851
	model : 0.06494150161743165
			 train-loss:  2.174457488637982 	 ± 0.21545368271951654
	data : 0.11439480781555175
	model : 0.06497368812561036
			 train-loss:  2.172080934047699 	 ± 0.21270019077294308
	data : 0.1143721103668213
	model : 0.06493401527404785
			 train-loss:  2.1839460406984603 	 ± 0.2207607443563644
	data : 0.11441841125488281
	model : 0.06497917175292969
			 train-loss:  2.1896200014485254 	 ± 0.22024607104924807
	data : 0.11442880630493164
	model : 0.06495518684387207
			 train-loss:  2.206399772618268 	 ± 0.2394441098795622
	data : 0.1143425464630127
	model : 0.06484885215759277
			 train-loss:  2.2052313120741593 	 ± 0.23637940270236604
	data : 0.11435461044311523
	model : 0.06478157043457031
			 train-loss:  2.2072968819202523 	 ± 0.23367638737225987
	data : 0.11438493728637696
	model : 0.06477460861206055
			 train-loss:  2.212560811638832 	 ± 0.23306691518139275
	data : 0.11434121131896972
	model : 0.06474566459655762
			 train-loss:  2.2156891270381647 	 ± 0.2310557478901771
	data : 0.11438446044921875
	model : 0.06477532386779786
			 train-loss:  2.207557737827301 	 ± 0.23415068747014584
	data : 0.1145634651184082
	model : 0.06489286422729493
			 train-loss:  2.2061873286269433 	 ± 0.23158235127763666
	data : 0.11461853981018066
	model : 0.06495752334594726
			 train-loss:  2.1985093463550913 	 ± 0.23440653517611337
	data : 0.11465134620666503
	model : 0.06491813659667969
			 train-loss:  2.2143027782440186 	 ± 0.25436277575793453
	data : 0.11457915306091308
	model : 0.06483755111694336
			 train-loss:  2.213389438131581 	 ± 0.25165736496711855
	data : 0.11444740295410157
	model : 0.06479010581970215
			 train-loss:  2.2103319066636105 	 ± 0.24982790689055903
	data : 0.11418676376342773
	model : 0.06476078033447266
			 train-loss:  2.210519631703695 	 ± 0.2472151857390684
	data : 0.1142354965209961
	model : 0.06473288536071778
			 train-loss:  2.2073575282583433 	 ± 0.2456583871402207
	data : 0.11409168243408203
	model : 0.06478557586669922
			 train-loss:  2.2102918004989625 	 ± 0.24405526020639104
	data : 0.11415200233459473
	model : 0.06486082077026367
			 train-loss:  2.2071622726964018 	 ± 0.24266183440033492
	data : 0.11426100730895997
	model : 0.06493558883666992
			 train-loss:  2.2101331765835104 	 ± 0.24125195604600352
	data : 0.11437759399414063
	model : 0.06490950584411621
			 train-loss:  2.2102006471382 	 ± 0.2389656513355224
	data : 0.11411700248718262
	model : 0.06483254432678223
			 train-loss:  2.2063377963172064 	 ± 0.23840708026821023
	data : 0.11424641609191895
	model : 0.06477251052856445
			 train-loss:  2.206846662001176 	 ± 0.236259395361544
	data : 0.11426362991333008
	model : 0.06476874351501465
			 train-loss:  2.2032823094299863 	 ± 0.2356278770498289
	data : 0.11424016952514648
	model : 0.06471962928771972
			 train-loss:  2.2023278161099085 	 ± 0.23366101914024004
	data : 0.11429028511047364
	model : 0.06476359367370606
			 train-loss:  2.199811836768841 	 ± 0.23241547885280917
	data : 0.11458492279052734
	model : 0.06487221717834472
			 train-loss:  2.196180751768209 	 ± 0.23209078039372896
	data : 0.11451406478881836
	model : 0.06491203308105468
			 train-loss:  2.197517200311025 	 ± 0.23037738841400074
	data : 0.11439690589904786
	model : 0.064884614944458
			 train-loss:  2.1985519635872763 	 ± 0.22862179219257195
	data : 0.11438975334167481
	model : 0.06485977172851562
			 train-loss:  2.194665705004046 	 ± 0.2287928674316071
	data : 0.11429367065429688
	model : 0.06483020782470703
			 train-loss:  2.197100317667401 	 ± 0.22777791707919162
	data : 0.11423025131225586
	model : 0.0648409366607666
			 train-loss:  2.197768796235323 	 ± 0.22605367390432196
	data : 0.11422891616821289
	model : 0.06487221717834472
			 train-loss:  2.1980697301717904 	 ± 0.224320978744401
	data : 0.11434922218322754
	model : 0.0649503231048584
			 train-loss:  2.194712201754252 	 ± 0.22425481837548134
	data : 0.11441192626953126
	model : 0.06497416496276856
			 train-loss:  2.192558526992798 	 ± 0.2232616223211361
	data : 0.11454291343688965
	model : 0.06499371528625489
			 train-loss:  2.1922265501583325 	 ± 0.22163057147917534
	data : 0.11435227394104004
	model : 0.0649350643157959
			 train-loss:  2.1993912130162334 	 ± 0.22781314128445324
	data : 0.11437807083129883
	model : 0.0648460865020752
			 train-loss:  2.197675585746765 	 ± 0.22662856953776747
	data : 0.11432147026062012
	model : 0.06485147476196289
			 train-loss:  2.196431025652818 	 ± 0.22526772111103935
	data : 0.11418499946594238
	model : 0.06488194465637206
			 train-loss:  2.1916244294908314 	 ± 0.22733474427660857
	data : 0.11422567367553711
	model : 0.06489543914794922
			 train-loss:  2.2012444097701818 	 ± 0.24007560341512874
	data : 0.11431012153625489
	model : 0.0649503231048584
			 train-loss:  2.2035257816314697 	 ± 0.23924332070417234
	data : 0.11433706283569336
	model : 0.06501154899597168
			 train-loss:  2.2060090192159016 	 ± 0.2386011741662778
	data : 0.11425065994262695
	model : 0.06492743492126465
			 train-loss:  2.2067744951499138 	 ± 0.2371189174767457
	data : 0.1142094612121582
	model : 0.06480264663696289
			 train-loss:  2.2067204357741717 	 ± 0.23557462379337868
	data : 0.114097261428833
	model : 0.06478629112243653
			 train-loss:  2.203292426390526 	 ± 0.23598468163566075
	data : 0.11427631378173828
	model : 0.06478080749511719
			 train-loss:  2.199256616302683 	 ± 0.23717987761811807
	data : 0.11424951553344727
	model : 0.06479134559631347
			 train-loss:  2.199635371565819 	 ± 0.23571688241618913
	data : 0.11434831619262695
	model : 0.06486649513244629
			 train-loss:  2.208705386997741 	 ± 0.24790664955701572
	data : 0.114520263671875
	model : 0.06495671272277832
			 train-loss:  2.2058916077381228 	 ± 0.247688369358825
	data : 0.1145674705505371
	model : 0.06494474411010742
			 train-loss:  2.2048813104629517 	 ± 0.24636167481972057
	data : 0.11437711715698243
	model : 0.0649726390838623
			 train-loss:  2.200348533335186 	 ± 0.24834824338997363
	data : 0.11426572799682617
	model : 0.06496543884277343
			 train-loss:  2.1968429551405064 	 ± 0.24896490231995128
	data : 0.11412873268127441
	model : 0.06496281623840332
			 train-loss:  2.2045341328132984 	 ± 0.25747017718946924
	data : 0.11411123275756836
	model : 0.06496400833129883
			 train-loss:  2.2066096560708406 	 ± 0.2567087812846422
	data : 0.11406211853027344
	model : 0.06499462127685547
			 train-loss:  2.206226713278077 	 ± 0.2552710318850444
	data : 0.11415939331054688
	model : 0.06495208740234375
			 train-loss:  2.206201560041878 	 ± 0.2538329834536847
	data : 0.11425490379333496
	model : 0.06497082710266114
			 train-loss:  2.208227050304413 	 ± 0.2531410946720317
	data : 0.11436600685119629
	model : 0.06495800018310546
			 train-loss:  2.2060773385750068 	 ± 0.252571072941768
	data : 0.11444211006164551
	model : 0.06501584053039551
			 train-loss:  2.2048422422098075 	 ± 0.2514708156463919
	data : 0.11430001258850098
	model : 0.0648869514465332
			 train-loss:  2.205326061094961 	 ± 0.2501582152648297
	data : 0.11417999267578124
	model : 0.06485133171081543
			 train-loss:  2.2046159163434456 	 ± 0.24891825491201147
	data : 0.11419663429260254
	model : 0.06485085487365723
			 train-loss:  2.2033304001155654 	 ± 0.2479181797581126
	data : 0.11420230865478516
	model : 0.06489739418029786
			 train-loss:  2.205077212303877 	 ± 0.2472105547384818
	data : 0.11407866477966308
	model : 0.06488685607910157
			 train-loss:  2.203953294409919 	 ± 0.2461793923584677
	data : 0.11419415473937988
	model : 0.06497926712036133
			 train-loss:  2.2058060643624287 	 ± 0.245598980956476
	data : 0.11430115699768066
	model : 0.06503801345825196
			 train-loss:  2.203666241482051 	 ± 0.24527189922680795
	data : 0.11421246528625488
	model : 0.0649979591369629
			 train-loss:  2.2002618980407713 	 ± 0.24638199483155032
	data : 0.1142209529876709
	model : 0.06493959426879883
			 train-loss:  2.2025668408611034 	 ± 0.24624039666468864
	data : 0.11419696807861328
	model : 0.06490631103515625
			 train-loss:  2.209245200250663 	 ± 0.2540561504918304
	data : 0.11421790122985839
	model : 0.06492223739624023
			 train-loss:  2.2057549131726755 	 ± 0.2552654624114318
	data : 0.11414461135864258
	model : 0.0649557113647461
			 train-loss:  2.2048929711947074 	 ± 0.2541858309983878
	data : 0.11435694694519043
	model : 0.06499619483947754
			 train-loss:  2.205653573217846 	 ± 0.2530914158829926
	data : 0.11433491706848145
	model : 0.0650064468383789
			 train-loss:  2.204186935469789 	 ± 0.2523426789161853
	data : 0.1143960952758789
	model : 0.06498441696166993
			 train-loss:  2.202695912289842 	 ± 0.2516294318582541
	data : 0.11433792114257812
	model : 0.06497330665588379
			 train-loss:  2.207355793979433 	 ± 0.2550579350685628
	data : 0.1142688274383545
	model : 0.06485319137573242
			 train-loss:  2.2087312127472063 	 ± 0.2542873008985063
	data : 0.11394548416137695
	model : 0.06483349800109864
			 train-loss:  2.2082334377548913 	 ± 0.25318215332632893
	data : 0.11402063369750977
	model : 0.06484718322753906
			 train-loss:  2.206246185947109 	 ± 0.2528994328226255
	data : 0.11401848793029785
	model : 0.06490049362182618
			 train-loss:  2.2056451301489557 	 ± 0.2518475118148286
	data : 0.11409502029418946
	model : 0.06493868827819824
			 train-loss:  2.2112651641389967 	 ± 0.2576884988012978
	data : 0.11417350769042969
	model : 0.06500768661499023
			 train-loss:  2.209298258287865 	 ± 0.2574063751815935
	data : 0.11443076133728028
	model : 0.06504793167114258
			 train-loss:  2.208199419145999 	 ± 0.25655317988228915
	data : 0.11437797546386719
	model : 0.0650705337524414
			 train-loss:  2.2096487540623237 	 ± 0.2559173500886304
	data : 0.11427364349365235
	model : 0.06499853134155273
			 train-loss:  2.208142982588874 	 ± 0.255336889783357
	data : 0.11438989639282227
	model : 0.06495428085327148
			 train-loss:  2.2063032863503795 	 ± 0.2550301852223827
	data : 0.11439061164855957
	model : 0.064998197555542
			 train-loss:  2.2033499479293823 	 ± 0.2559747213184409
	data : 0.11437993049621582
	model : 0.06498565673828124
			 train-loss:  2.2027772814035416 	 ± 0.25498246637392824
	data : 0.1144136905670166
	model : 0.06497955322265625
			 train-loss:  2.2011782068851566 	 ± 0.25453011625597066
	data : 0.11458282470703125
	model : 0.06508388519287109
			 train-loss:  2.199168794467801 	 ± 0.25444668806289306
	data : 0.11453037261962891
	model : 0.06513986587524415
			 train-loss:  2.1978593269983926 	 ± 0.2538226630489851
	data : 0.11448698043823242
	model : 0.06513032913208008
			 train-loss:  2.2005594959182124 	 ± 0.25456465271239104
	data : 0.11448793411254883
	model : 0.06506128311157226
			 train-loss:  2.199733361244202 	 ± 0.2537111880174904
	data : 0.11438336372375488
	model : 0.0649874210357666
			 train-loss:  2.1975649727715387 	 ± 0.25386263510966595
	data : 0.11432867050170899
	model : 0.0648916244506836
			 train-loss:  2.1978852317089173 	 ± 0.2528867535181852
	data : 0.11434168815612793
	model : 0.06486287117004394
			 train-loss:  2.1973920203745365 	 ± 0.25195829230679867
	data : 0.11447353363037109
	model : 0.06479988098144532
			 train-loss:  2.1988705590713855 	 ± 0.2515366429462866
	data : 0.11443901062011719
	model : 0.0648796558380127
			 train-loss:  2.1984694040738617 	 ± 0.25060874795774
	data : 0.11449804306030273
	model : 0.06495952606201172
			 train-loss:  2.196026738363368 	 ± 0.25119908254714346
	data : 0.11451687812805175
	model : 0.06497902870178222
			 train-loss:  2.1939300614776034 	 ± 0.251393763688506
	data : 0.11451716423034668
	model : 0.06495261192321777
			 train-loss:  2.1947278680657982 	 ± 0.2506145699614648
	data : 0.11433730125427247
	model : 0.06488451957702637
			 train-loss:  2.192376418789821 	 ± 0.2511460680523632
	data : 0.11431946754455566
	model : 0.06483798027038574
			 train-loss:  2.190714300120318 	 ± 0.2509528315530015
	data : 0.11430349349975585
	model : 0.06478548049926758
			 train-loss:  2.1886248571031235 	 ± 0.251204367686147
	data : 0.1142880916595459
	model : 0.06478028297424317
			 train-loss:  2.1893751290592833 	 ± 0.2504387736833967
	data : 0.11426949501037598
	model : 0.06478581428527833
			 train-loss:  2.187712665917217 	 ± 0.2502872909782536
	data : 0.11439738273620606
	model : 0.06488447189331055
			 train-loss:  2.1877722723020923 	 ± 0.24938633488226147
	data : 0.11433372497558594
	model : 0.06489777565002441
			 train-loss:  2.1862711242267063 	 ± 0.24912353031472237
	data : 0.11434311866760254
	model : 0.06490106582641601
			 train-loss:  2.1865150438132863 	 ± 0.24825531835691259
	data : 0.11426091194152832
	model : 0.06491045951843262
			 train-loss:  2.186003785737803 	 ± 0.24745411557744124
	data : 0.1141751766204834
	model : 0.06487879753112794
			 train-loss:  2.186201148933464 	 ± 0.24659858873150708
	data : 0.11409282684326172
	model : 0.06487293243408203
			 train-loss:  2.184722630514039 	 ± 0.24637606760483569
	data : 0.11413025856018066
	model : 0.06490864753723144
			 train-loss:  2.1862473882477858 	 ± 0.24620585199737594
	data : 0.11414580345153809
	model : 0.06494503021240235
			 train-loss:  2.188496919527446 	 ± 0.24685196194405856
	data : 0.11425323486328125
	model : 0.06496982574462891
			 train-loss:  2.189085443003648 	 ± 0.24611365217491302
	data : 0.11434307098388671
	model : 0.0650252342224121
			 train-loss:  2.1891244746543266 	 ± 0.24528123438426905
	data : 0.11439847946166992
	model : 0.06503429412841796
			 train-loss:  2.1900228205943266 	 ± 0.24470093166931584
	data : 0.1142693042755127
	model : 0.06497211456298828
			 train-loss:  2.1877342716852826 	 ± 0.24547858687406207
	data : 0.11423487663269043
	model : 0.06491403579711914
			 train-loss:  2.19184240284345 	 ± 0.24978426041724644
	data : 0.11424484252929687
	model : 0.06494727134704589
			 train-loss:  2.1939316893878735 	 ± 0.2502815110292967
	data : 0.1142204761505127
	model : 0.06491708755493164
			 train-loss:  2.195010473525602 	 ± 0.24981655544666748
	data : 0.1142798900604248
	model : 0.06491994857788086
			 train-loss:  2.1945137002251367 	 ± 0.2490799480247627
	data : 0.11444501876831055
	model : 0.06498847007751465
			 train-loss:  2.1948696890184958 	 ± 0.24831446447022737
	data : 0.11454315185546875
	model : 0.06503586769104004
			 train-loss:  2.1945037719530935 	 ± 0.24755922555327584
	data : 0.11445116996765137
	model : 0.06498508453369141
			 train-loss:  2.1927716268855297 	 ± 0.24771610191097715
	data : 0.11447615623474121
	model : 0.06501798629760742
			 train-loss:  2.191375382338898 	 ± 0.24754992093391695
	data : 0.1142282485961914
	model : 0.0650291919708252
			 train-loss:  2.1913768555383264 	 ± 0.24677023490168837
	data : 0.11423044204711914
	model : 0.0649533748626709
			 train-loss:  2.194095942378044 	 ± 0.24837573862737056
	data : 0.11429152488708497
	model : 0.06492815017700196
			 train-loss:  2.194363719928339 	 ± 0.24762635048605988
	data : 0.11446003913879395
	model : 0.06492853164672852
			 train-loss:  2.195481507866471 	 ± 0.24726799169198074
	data : 0.11439599990844726
	model : 0.06492824554443359
			 train-loss:  2.195375781849118 	 ± 0.24651200694902714
	data : 0.11460022926330567
	model : 0.06490874290466309
			 train-loss:  2.193642011502894 	 ± 0.2467541354622034
	data : 0.1145111083984375
	model : 0.06498661041259765
			 train-loss:  2.1953352465774074 	 ± 0.24695907145232307
	data : 0.11433801651000977
	model : 0.06498570442199707
			 train-loss:  2.1961966465754683 	 ± 0.24646259819367833
	data : 0.11409120559692383
	model : 0.06494660377502441
			 train-loss:  2.1950505993323413 	 ± 0.2461668241993463
	data : 0.11396985054016114
	model : 0.06482977867126465
			 train-loss:  2.198899040619532 	 ± 0.25042115702427575
	data : 0.11391782760620117
	model : 0.06481122970581055
			 train-loss:  2.1988544308927636 	 ± 0.24967983628755924
	data : 0.11403632164001465
	model : 0.06477265357971192
			 train-loss:  2.20056688785553 	 ± 0.24993780954530123
	data : 0.11411871910095214
	model : 0.06479668617248535
			 train-loss:  2.2021759434750208 	 ± 0.25008745186998355
	data : 0.11415061950683594
	model : 0.06482400894165039
			 train-loss:  2.2026523018992226 	 ± 0.2494371866649846
	data : 0.11421170234680175
	model : 0.06494059562683105
			 train-loss:  2.2021083335655964 	 ± 0.24881752003919583
	data : 0.11422810554504395
	model : 0.06494383811950684
			 train-loss:  2.2029954839026793 	 ± 0.24837574348307362
	data : 0.1141357421875
	model : 0.06488609313964844
			 train-loss:  2.2009381328310287 	 ± 0.24914750933578017
	data : 0.11414952278137207
	model : 0.06483550071716308
			 train-loss:  2.2008717066862364 	 ± 0.24844024969166145
	data : 0.11429314613342285
	model : 0.0648613452911377
			 train-loss:  2.200377801717338 	 ± 0.24782408370577994
	data : 0.11442618370056153
	model : 0.0648773193359375
			 train-loss:  2.1997435635395264 	 ± 0.24727098105580403
	data : 0.11451377868652343
	model : 0.06490545272827149
			 train-loss:  2.199605167245066 	 ± 0.24658622577465075
	data : 0.11467909812927246
	model : 0.06499004364013672
			 train-loss:  2.198031318187714 	 ± 0.2468002140096604
	data : 0.11475152969360351
	model : 0.06510882377624512
			 train-loss:  2.200226926013251 	 ± 0.24787405926972628
	data : 0.11479668617248535
	model : 0.06511869430541992
			 train-loss:  2.198916737849896 	 ± 0.24781981693579877
	data : 0.11473517417907715
	model : 0.06510505676269532
			 train-loss:  2.198649392101934 	 ± 0.2471681019960409
	data : 0.11467318534851074
	model : 0.06510848999023437
			 train-loss:  2.1972009507210357 	 ± 0.24727308649883772
	data : 0.11444206237792968
	model : 0.06505241394042968
			 train-loss:  2.195279661384789 	 ± 0.2479771767919532
	data : 0.11428370475769042
	model : 0.06495676040649415
			 train-loss:  2.1946193671995595 	 ± 0.24747268992390323
	data : 0.11419763565063476
	model : 0.06499137878417968
			 train-loss:  2.1947080375039003 	 ± 0.24681307385712822
	data : 0.1141775131225586
	model : 0.06498494148254394
			 train-loss:  2.194568088714113 	 ± 0.24616322020336776
	data : 0.1141653060913086
	model : 0.0649960994720459
			 train-loss:  2.197155939838874 	 ± 0.24806198112672276
	data : 0.11429710388183593
	model : 0.06503095626831054
			 train-loss:  2.1965074187830873 	 ± 0.24756891753727253
	data : 0.11438498497009278
	model : 0.06518397331237794
			 train-loss:  2.1961571196611014 	 ± 0.24696718759153272
	data : 0.11432228088378907
	model : 0.06511812210083008
			 train-loss:  2.1965191066265106 	 ± 0.2463740013842372
	data : 0.11431198120117188
	model : 0.06507573127746583
			 train-loss:  2.196921649992157 	 ± 0.24579819348254736
	data : 0.11419506072998047
	model : 0.06495456695556641
			 train-loss:  2.1969953517323915 	 ± 0.24516601258876802
	data : 0.11415905952453613
	model : 0.06490082740783691
			 train-loss:  2.1952756790014414 	 ± 0.24570683166975255
	data : 0.11417560577392578
	model : 0.0647916316986084
			 train-loss:  2.194848827561554 	 ± 0.24515170185125404
	data : 0.11432404518127441
	model : 0.0648120403289795
			 train-loss:  2.194367426301017 	 ± 0.2446215576943934
	data : 0.114314603805542
	model : 0.06488842964172363
			 train-loss:  2.194991343551212 	 ± 0.2441601369548308
	data : 0.11448225975036622
	model : 0.06498522758483886
			 train-loss:  2.1944110471399587 	 ± 0.2436827426573659
	data : 0.11446003913879395
	model : 0.06501555442810059
			 train-loss:  2.19380191385746 	 ± 0.24322460874978205
	data : 0.11441030502319335
	model : 0.06499323844909669
			 train-loss:  2.194256398808304 	 ± 0.24270393931981615
	data : 0.11411566734313965
	model : 0.06492233276367188
			 train-loss:  2.19501396157954 	 ± 0.2423405587628536
	data : 0.11428489685058593
	model : 0.06483983993530273
			 train-loss:  2.1952259487706454 	 ± 0.24176169849188964
	data : 0.11418395042419434
	model : 0.06486711502075196
			 train-loss:  2.194970152541703 	 ± 0.24119595387823167
	data : 0.11428122520446778
	model : 0.06489005088806152
			 train-loss:  2.1960625212366987 	 ± 0.24111228141417074
	data : 0.11427249908447265
	model : 0.06489663124084473
			 train-loss:  2.197899286608094 	 ± 0.24195977518992118
	data : 0.11451683044433594
	model : 0.0649569034576416
			 train-loss:  2.196665614699396 	 ± 0.24202320123056276
	data : 0.11439194679260253
	model : 0.06500282287597656
			 train-loss:  2.197880332859663 	 ± 0.24207241700522927
	data : 0.11446747779846192
	model : 0.06496424674987793
			 train-loss:  2.1977007337734458 	 ± 0.241506492736356
	data : 0.11429800987243652
	model : 0.06492433547973633
			 train-loss:  2.197411014352526 	 ± 0.24096719487303367
	data : 0.11417078971862793
	model : 0.0649118423461914
			 train-loss:  2.1956907861040666 	 ± 0.24168456184330045
	data : 0.11412510871887208
	model : 0.06487011909484863
			 train-loss:  2.194212939941658 	 ± 0.24206761867361265
	data : 0.11419377326965333
	model : 0.06490845680236816
			 train-loss:  2.1925727996467983 	 ± 0.2426765805381774
	data : 0.11430215835571289
	model : 0.06495294570922852
			 train-loss:  2.1922104704045804 	 ± 0.24216665750930816
	data : 0.11447882652282715
	model : 0.06500687599182128
			 train-loss:  2.1926134597423466 	 ± 0.2416747353285966
	data : 0.11464500427246094
	model : 0.0650202751159668
			 train-loss:  2.1932828062110477 	 ± 0.24131432141498957
	data : 0.11457357406616211
	model : 0.06511383056640625
			 train-loss:  2.1923710338531004 	 ± 0.24113028790370972
	data : 0.1144378662109375
	model : 0.06511158943176269
			 train-loss:  2.190705955028534 	 ± 0.24182375974906264
	data : 0.11486334800720215
	model : 0.0650674819946289
			 train-loss:  2.192609947566028 	 ± 0.24290326367566287
	data : 0.11483845710754395
	model : 0.06500515937805176
			 train-loss:  2.193274676257914 	 ± 0.24255014508929587
	data : 0.1146165370941162
	model : 0.06489906311035157
			 train-loss:  2.19426063229056 	 ± 0.2424422307297435
	data : 0.11475906372070313
	model : 0.06479530334472657
			 train-loss:  2.1934355277198927 	 ± 0.24220636794872588
	data : 0.11481494903564453
	model : 0.06471467018127441
			 train-loss:  2.1934825117812564 	 ± 0.24166370810191834
	data : 0.11433815956115723
	model : 0.06468138694763184
			 train-loss:  2.195019799151591 	 ± 0.24221402273440912
	data : 0.11441397666931152
	model : 0.06464161872863769
			 train-loss:  2.195368816057841 	 ± 0.2417316153130728
	data : 0.1146876335144043
	model : 0.06459231376647949
			 train-loss:  2.194875219754413 	 ± 0.2413098297737831
	data : 0.11477751731872558
	model : 0.06447796821594239
			 train-loss:  2.1954897946723233 	 ± 0.24095491888842469
	data : 0.11476855278015137
	model : 0.06430025100708008
			 train-loss:  2.1945317424180213 	 ± 0.2408588428069083
	data : 0.11474876403808594
	model : 0.06411809921264648
			 train-loss:  2.193517355419142 	 ± 0.240819970149284
	data : 0.11492729187011719
	model : 0.06406593322753906
			 train-loss:  2.1929531175157297 	 ± 0.24044752967132865
	data : 0.11507606506347656
	model : 0.06404476165771485
			 train-loss:  2.192750973618908 	 ± 0.23994610097382132
	data : 0.11500058174133301
	model : 0.06405229568481445
			 train-loss:  2.1912322167692513 	 ± 0.24053855570421617
	data : 0.11514978408813477
	model : 0.06407108306884765
			 train-loss:  2.1908233042950283 	 ± 0.24010262045119674
	data : 0.11526288986206054
	model : 0.06412835121154785
			 train-loss:  2.1893993886108074 	 ± 0.24057289916351426
	data : 0.115195894241333
	model : 0.06405529975891114
			 train-loss:  2.1929893366833952 	 ± 0.2462615885834392
	data : 0.11492891311645508
	model : 0.06392860412597656
			 train-loss:  2.194214250576698 	 ± 0.2464556713650441
	data : 0.11493473052978516
	model : 0.06384987831115722
			 train-loss:  2.195878589706582 	 ± 0.24726066243445147
	data : 0.11500334739685059
	model : 0.06386280059814453
			 train-loss:  2.195745676004586 	 ± 0.24674914464175599
	data : 0.11501331329345703
	model : 0.06383037567138672
			 train-loss:  2.195426095479702 	 ± 0.2462817455426244
	data : 0.11497893333435058
	model : 0.06382508277893066
			 train-loss:  2.196104529996713 	 ± 0.2459918202289311
	data : 0.11520524024963379
	model : 0.0639225959777832
			 train-loss:  2.1958244593806286 	 ± 0.24551927434679247
	data : 0.11520729064941407
	model : 0.06395273208618164
			 train-loss:  2.194751117840286 	 ± 0.245577424257984
	data : 0.11500692367553711
	model : 0.06385645866394044
			 train-loss:  2.1939080865294844 	 ± 0.2454222462454552
	data : 0.11492247581481933
	model : 0.06382379531860352
			 train-loss:  2.197362122477078 	 ± 0.2507674314999861
	data : 0.11480388641357422
	model : 0.0638197898864746
			 train-loss:  2.1967673832056476 	 ± 0.25042751542333913
	data : 0.11469469070434571
	model : 0.06384096145629883
			 train-loss:  2.1952140840088448 	 ± 0.2510978428231249
	data : 0.11475639343261719
	model : 0.0638455867767334
			 train-loss:  2.195998123782849 	 ± 0.250890581166437
	data : 0.11489014625549317
	model : 0.06392140388488769
			 train-loss:  2.1959067203344835 	 ± 0.2503883632442868
	data : 0.11493148803710937
	model : 0.06394667625427246
			 train-loss:  2.196723464502388 	 ± 0.25021586999502954
	data : 0.1149674415588379
	model : 0.06389374732971191
			 train-loss:  2.198548487186432 	 ± 0.25137003781775835
	data : 0.11493172645568847
	model : 0.06386446952819824
			 train-loss:  2.1988599732577563 	 ± 0.2509171400771442
	data : 0.11506047248840331
	model : 0.06388626098632813
			 train-loss:  2.1985303603467488 	 ± 0.2504732362222923
	data : 0.11499557495117188
	model : 0.06389765739440918
			 train-loss:  2.1978771154117207 	 ± 0.2501927378044413
	data : 0.11512832641601563
	model : 0.06393313407897949
			 train-loss:  2.1967211867880634 	 ± 0.25037574715869393
	data : 0.1151881217956543
	model : 0.06400771141052246
			 train-loss:  2.1965471333148434 	 ± 0.24989972838037403
	data : 0.11525835990905761
	model : 0.0640411376953125
			 train-loss:  2.1964522935450077 	 ± 0.24941576341865826
	data : 0.11469202041625977
	model : 0.05554180145263672
#epoch  94    val-loss:  2.4047795345908716  train-loss:  2.1964522935450077  lr:  1.9073486328125e-08
			 train-loss:  2.439688205718994 	 ± 0.0
	data : 5.466190576553345
	model : 0.10493302345275879
			 train-loss:  2.3548660278320312 	 ± 0.08482217788696289
	data : 2.7833194732666016
	model : 0.08864951133728027
			 train-loss:  2.314866781234741 	 ± 0.08942267067710899
	data : 1.8929860591888428
	model : 0.08060773213704427
			 train-loss:  2.277714252471924 	 ± 0.10068883604295928
	data : 1.4482383131980896
	model : 0.07663202285766602
			 train-loss:  2.2687480449676514 	 ± 0.09182681999823056
	data : 1.1814301013946533
	model : 0.07420477867126465
			 train-loss:  2.2499962647755942 	 ± 0.09372806602620347
	data : 0.11094069480895996
	model : 0.06612000465393067
			 train-loss:  2.2699668407440186 	 ± 0.09961375848425752
	data : 0.11383042335510254
	model : 0.06454010009765625
			 train-loss:  2.2611369490623474 	 ± 0.09606407850601756
	data : 0.11413698196411133
	model : 0.06457419395446777
			 train-loss:  2.2753230730692544 	 ± 0.09906012633531407
	data : 0.11412105560302735
	model : 0.06460652351379395
			 train-loss:  2.254658579826355 	 ± 0.1125824555403022
	data : 0.11402769088745117
	model : 0.064705228805542
			 train-loss:  2.2666550332849678 	 ± 0.11384947457424717
	data : 0.11415247917175293
	model : 0.06477570533752441
			 train-loss:  2.286031504472097 	 ± 0.1265364965062844
	data : 0.11411619186401367
	model : 0.06480746269226074
			 train-loss:  2.295148574388944 	 ± 0.12560765831029136
	data : 0.11402130126953125
	model : 0.06519761085510253
			 train-loss:  2.2807071719850813 	 ± 0.13176319032848502
	data : 0.11385636329650879
	model : 0.06516213417053222
			 train-loss:  2.274910736083984 	 ± 0.12912971697230868
	data : 0.11389322280883789
	model : 0.06512885093688965
			 train-loss:  2.256384052336216 	 ± 0.14415581406194358
	data : 0.1137850284576416
	model : 0.06513290405273438
			 train-loss:  2.2414833728004906 	 ± 0.1520229486964472
	data : 0.11365909576416015
	model : 0.06517729759216309
			 train-loss:  2.216657771004571 	 ± 0.17973400441921752
	data : 0.11382913589477539
	model : 0.06487383842468261
			 train-loss:  2.2133489784441496 	 ± 0.1755025678638519
	data : 0.11405067443847657
	model : 0.06495003700256348
			 train-loss:  2.2061460494995115 	 ± 0.17391623064772935
	data : 0.1140669345855713
	model : 0.06500058174133301
			 train-loss:  2.1929688964571272 	 ± 0.1796643428361028
	data : 0.11417584419250489
	model : 0.06496710777282715
			 train-loss:  2.1828326853838833 	 ± 0.18157542382699535
	data : 0.11412429809570312
	model : 0.06491208076477051
			 train-loss:  2.181462795838066 	 ± 0.17770047076141338
	data : 0.11401252746582032
	model : 0.06489772796630859
			 train-loss:  2.197948465744654 	 ± 0.19108272581043817
	data : 0.11401371955871582
	model : 0.0648829460144043
			 train-loss:  2.1951168060302733 	 ± 0.18773530009226916
	data : 0.11399564743041993
	model : 0.06487832069396973
			 train-loss:  2.1838626403075 	 ± 0.1924977654709003
	data : 0.11406846046447754
	model : 0.06496329307556152
			 train-loss:  2.19371231396993 	 ± 0.1954619802488364
	data : 0.11413240432739258
	model : 0.0650507926940918
			 train-loss:  2.183265596628189 	 ± 0.19946810032724605
	data : 0.11416473388671874
	model : 0.06507954597473145
			 train-loss:  2.1841029257609925 	 ± 0.19604889914743573
	data : 0.11405205726623535
	model : 0.064984130859375
			 train-loss:  2.184516894817352 	 ± 0.1927666159790982
	data : 0.11413674354553223
	model : 0.06491327285766602
			 train-loss:  2.18072404784541 	 ± 0.19076651300975123
	data : 0.11401805877685547
	model : 0.06492137908935547
			 train-loss:  2.196858424693346 	 ± 0.20814532996832547
	data : 0.11409978866577149
	model : 0.06489911079406738
			 train-loss:  2.1905179637851138 	 ± 0.20808186281149607
	data : 0.11415333747863769
	model : 0.06488299369812012
			 train-loss:  2.1960856669089375 	 ± 0.20747907539475796
	data : 0.11429734230041504
	model : 0.06497650146484375
			 train-loss:  2.1945923362459454 	 ± 0.20467891315107684
	data : 0.11425909996032715
	model : 0.06495614051818847
			 train-loss:  2.1968942317697735 	 ± 0.20227507419683158
	data : 0.11422138214111328
	model : 0.06486434936523437
			 train-loss:  2.196301450600495 	 ± 0.19955460229014127
	data : 0.11422247886657715
	model : 0.06483249664306641
			 train-loss:  2.1937790513038635 	 ± 0.19750823609789742
	data : 0.114190673828125
	model : 0.06482791900634766
			 train-loss:  2.1979298377648377 	 ± 0.19663154051361992
	data : 0.11420121192932128
	model : 0.0648043155670166
			 train-loss:  2.197117802500725 	 ± 0.19422430410784516
	data : 0.11428689956665039
	model : 0.06485710144042969
			 train-loss:  2.2045195131767086 	 ± 0.1974700433331515
	data : 0.11437478065490722
	model : 0.06490674018859863
			 train-loss:  2.2177505918911526 	 ± 0.21270519197169144
	data : 0.11430349349975585
	model : 0.06492381095886231
			 train-loss:  2.210975865985072 	 ± 0.21475333424615442
	data : 0.11428627967834473
	model : 0.06487727165222168
			 train-loss:  2.219566445459019 	 ± 0.21964551440301897
	data : 0.11423006057739257
	model : 0.06480541229248046
			 train-loss:  2.2190072774887084 	 ± 0.21722296646256262
	data : 0.11410679817199706
	model : 0.06476669311523438
			 train-loss:  2.2144183723822883 	 ± 0.21704296988918237
	data : 0.11393661499023437
	model : 0.06478219032287598
			 train-loss:  2.2053254513030356 	 ± 0.22340254288813846
	data : 0.11404376029968262
	model : 0.06480154991149903
			 train-loss:  2.201480264465014 	 ± 0.22262939691531053
	data : 0.11408534049987792
	model : 0.06484198570251465
			 train-loss:  2.1985893152198015 	 ± 0.22125439083351145
	data : 0.11412434577941895
	model : 0.06495480537414551
			 train-loss:  2.1972178649902343 	 ± 0.21924095936827784
	data : 0.11421451568603516
	model : 0.06503453254699706
			 train-loss:  2.1977818152483772 	 ± 0.21711752103524412
	data : 0.11436681747436524
	model : 0.06501765251159668
			 train-loss:  2.1979255905518165 	 ± 0.21502216948997058
	data : 0.11426286697387696
	model : 0.0649439811706543
			 train-loss:  2.1922435220682397 	 ± 0.2168894893788383
	data : 0.11425094604492188
	model : 0.06488656997680664
			 train-loss:  2.1956188722893044 	 ± 0.21627239412226743
	data : 0.11413140296936035
	model : 0.06483621597290039
			 train-loss:  2.191248835216869 	 ± 0.21669002461116593
	data : 0.11407194137573243
	model : 0.06481871604919434
			 train-loss:  2.1880760682480678 	 ± 0.21603181969922777
	data : 0.11403288841247558
	model : 0.06485772132873535
			 train-loss:  2.1846025282876536 	 ± 0.21570036277473134
	data : 0.11412243843078614
	model : 0.06498856544494629
			 train-loss:  2.178546539668379 	 ± 0.21866627045733963
	data : 0.11413030624389649
	model : 0.06507687568664551
			 train-loss:  2.1805798239627125 	 ± 0.2173575427073846
	data : 0.11423792839050292
	model : 0.0650639533996582
			 train-loss:  2.1764554738998414 	 ± 0.2178543134573071
	data : 0.11413331031799316
	model : 0.06498537063598633
			 train-loss:  2.173222819312674 	 ± 0.21750738888168472
	data : 0.11412053108215332
	model : 0.06493239402770996
			 train-loss:  2.1701891403044424 	 ± 0.21704332298291648
	data : 0.11401524543762206
	model : 0.06487593650817872
			 train-loss:  2.1771367939691695 	 ± 0.2221548883090456
	data : 0.11412067413330078
	model : 0.06481370925903321
			 train-loss:  2.1729697939008474 	 ± 0.22288019658548097
	data : 0.11418070793151855
	model : 0.06486706733703614
			 train-loss:  2.1692467817893397 	 ± 0.22315562895459365
	data : 0.11436877250671387
	model : 0.0649538516998291
			 train-loss:  2.164696843335123 	 ± 0.22447614370271046
	data : 0.11453833580017089
	model : 0.06495299339294433
			 train-loss:  2.1656638561789667 	 ± 0.22293311537786598
	data : 0.11459956169128419
	model : 0.06493306159973145
			 train-loss:  2.1675388865611134 	 ± 0.22181942709278976
	data : 0.11449060440063477
	model : 0.06497335433959961
			 train-loss:  2.1690978675648784 	 ± 0.22058111324730184
	data : 0.11444501876831055
	model : 0.06495504379272461
			 train-loss:  2.172406027998243 	 ± 0.22071717710730795
	data : 0.11444439888000488
	model : 0.06490263938903809
			 train-loss:  2.1700427683306414 	 ± 0.22004744975848703
	data : 0.11429920196533203
	model : 0.06492395401000976
			 train-loss:  2.168697319096989 	 ± 0.2188078940747764
	data : 0.11429376602172851
	model : 0.06498498916625976
			 train-loss:  2.1679937921158254 	 ± 0.21738602313579983
	data : 0.11430315971374512
	model : 0.06495175361633301
			 train-loss:  2.17075732108709 	 ± 0.21719941752479416
	data : 0.11411781311035156
	model : 0.06484198570251465
			 train-loss:  2.167542533874512 	 ± 0.21751174347984348
	data : 0.11404166221618653
	model : 0.06483492851257325
			 train-loss:  2.164222257701974 	 ± 0.21798086662733906
	data : 0.11401948928833008
	model : 0.06486082077026367
			 train-loss:  2.166165279103564 	 ± 0.21722222950234163
	data : 0.11403894424438477
	model : 0.06483898162841797
			 train-loss:  2.163775998812455 	 ± 0.21684123584940151
	data : 0.11419544219970704
	model : 0.06483292579650879
			 train-loss:  2.165264589877068 	 ± 0.21586516767274141
	data : 0.1143805980682373
	model : 0.06493887901306153
			 train-loss:  2.1792926147580145 	 ± 0.24811560121136422
	data : 0.11437106132507324
	model : 0.06495013236999511
			 train-loss:  2.1777187409224332 	 ± 0.24698077088413528
	data : 0.11425566673278809
	model : 0.06483454704284668
			 train-loss:  2.1777667432296566 	 ± 0.24547055113094512
	data : 0.11418418884277344
	model : 0.06473741531372071
			 train-loss:  2.1766894337642624 	 ± 0.2441822821733824
	data : 0.11393971443176269
	model : 0.0647507667541504
			 train-loss:  2.1742611967381977 	 ± 0.24373050991617867
	data : 0.1139864444732666
	model : 0.06477532386779786
			 train-loss:  2.1777441094903387 	 ± 0.24438629563538222
	data : 0.11408014297485351
	model : 0.06484885215759277
			 train-loss:  2.1831565055736277 	 ± 0.24803262154828434
	data : 0.11430296897888184
	model : 0.06491928100585938
			 train-loss:  2.1856377412532937 	 ± 0.24767421031197007
	data : 0.11428976058959961
	model : 0.06500916481018067
			 train-loss:  2.1867353821342643 	 ± 0.24647567704458534
	data : 0.11428499221801758
	model : 0.0649416446685791
			 train-loss:  2.1841717811112993 	 ± 0.24626411149137808
	data : 0.11407318115234374
	model : 0.0648453712463379
			 train-loss:  2.18187978665034 	 ± 0.24584488251803135
	data : 0.11408758163452148
	model : 0.0648256778717041
			 train-loss:  2.1783205965063073 	 ± 0.24681093729282233
	data : 0.11436309814453124
	model : 0.06482887268066406
			 train-loss:  2.1748857031697812 	 ± 0.24764324148308992
	data : 0.11485862731933594
	model : 0.06481757164001464
			 train-loss:  2.1762009615539224 	 ± 0.2466310894579427
	data : 0.1150777816772461
	model : 0.0649033546447754
			 train-loss:  2.175877279423653 	 ± 0.24533557281493115
	data : 0.11522231101989747
	model : 0.06500825881958008
			 train-loss:  2.1745962619781496 	 ± 0.24435675493823877
	data : 0.11518502235412598
	model : 0.06499857902526855
			 train-loss:  2.177491620182991 	 ± 0.24471337643251412
	data : 0.11482515335083007
	model : 0.0649942398071289
			 train-loss:  2.176301208968015 	 ± 0.2437279401052576
	data : 0.1143110752105713
	model : 0.06496024131774902
			 train-loss:  2.185398522688418 	 ± 0.25850528218547286
	data : 0.11424551010131836
	model : 0.06488142013549805
			 train-loss:  2.1854451858636104 	 ± 0.25719680110924514
	data : 0.11413884162902832
	model : 0.06485872268676758
			 train-loss:  2.1852415299415586 	 ± 0.25591560846018047
	data : 0.11417350769042969
	model : 0.06487541198730469
			 train-loss:  2.186463464604746 	 ± 0.25493855639080776
	data : 0.11419796943664551
	model : 0.06489887237548828
			 train-loss:  2.1872761600157795 	 ± 0.25381722276592
	data : 0.11427783966064453
	model : 0.06495814323425293
			 train-loss:  2.1873731728896355 	 ± 0.2525839954419171
	data : 0.11434521675109863
	model : 0.06506085395812988
			 train-loss:  2.1907071264890523 	 ± 0.2536337839441197
	data : 0.11454768180847168
	model : 0.06510100364685059
			 train-loss:  2.188492687543233 	 ± 0.2534312879604793
	data : 0.11439285278320313
	model : 0.06505794525146484
			 train-loss:  2.188641751712223 	 ± 0.2522376494409954
	data : 0.11437172889709472
	model : 0.06500287055969238
			 train-loss:  2.1894373838032517 	 ± 0.25118980427172233
	data : 0.11425199508666992
	model : 0.06497430801391602
			 train-loss:  2.18969937938231 	 ± 0.250038871509326
	data : 0.11410841941833497
	model : 0.0649479866027832
			 train-loss:  2.1887889028689185 	 ± 0.249069052543068
	data : 0.11392269134521485
	model : 0.06492605209350585
			 train-loss:  2.193237600543282 	 ± 0.25224718851928457
	data : 0.11416349411010743
	model : 0.06494579315185547
			 train-loss:  2.192473496402706 	 ± 0.2512362178368268
	data : 0.11412911415100098
	model : 0.06498322486877442
			 train-loss:  2.192154406436852 	 ± 0.25013470532332793
	data : 0.11428780555725097
	model : 0.06502552032470703
			 train-loss:  2.201644592580542 	 ± 0.26851592906364635
	data : 0.11418671607971191
	model : 0.0649336338043213
			 train-loss:  2.1995853509819296 	 ± 0.2682303389815619
	data : 0.11427578926086426
	model : 0.06481518745422363
			 train-loss:  2.197004678974981 	 ± 0.2684792527179111
	data : 0.11409730911254883
	model : 0.06485276222229004
			 train-loss:  2.196192038470301 	 ± 0.26746151965451787
	data : 0.11434760093688964
	model : 0.0648622989654541
			 train-loss:  2.1960683113489394 	 ± 0.26631940285023664
	data : 0.11435832977294921
	model : 0.06483631134033203
			 train-loss:  2.1972343355922375 	 ± 0.2654882878742092
	data : 0.11450591087341308
	model : 0.06489367485046386
			 train-loss:  2.19737013648538 	 ± 0.26437455316089753
	data : 0.11453452110290527
	model : 0.06499214172363281
			 train-loss:  2.195774585008621 	 ± 0.26384541594144845
	data : 0.11461896896362304
	model : 0.06491436958312988
			 train-loss:  2.1954745911369637 	 ± 0.26277343363287203
	data : 0.11434421539306641
	model : 0.06488256454467774
			 train-loss:  2.192749229610943 	 ± 0.26340583492118935
	data : 0.11424474716186524
	model : 0.06486706733703614
			 train-loss:  2.196894654413549 	 ± 0.26629882111111924
	data : 0.11418008804321289
	model : 0.06487011909484863
			 train-loss:  2.1953632783505226 	 ± 0.2657660913630266
	data : 0.11421360969543456
	model : 0.06490178108215332
			 train-loss:  2.1953203897476197 	 ± 0.2647013231652491
	data : 0.11428108215332031
	model : 0.06501235961914062
			 train-loss:  2.1997787507753523 	 ± 0.26831945089456544
	data : 0.1144026279449463
	model : 0.06500840187072754
			 train-loss:  2.2020411519553718 	 ± 0.2684648230784842
	data : 0.11443195343017579
	model : 0.06502151489257812
			 train-loss:  2.200947287492454 	 ± 0.26769805491967774
	data : 0.11458239555358887
	model : 0.06497697830200196
			 train-loss:  2.201184791187907 	 ± 0.2666719848904775
	data : 0.11434602737426758
	model : 0.06487536430358887
			 train-loss:  2.2050945492891163 	 ± 0.2693303511391411
	data : 0.11421537399291992
	model : 0.06475605964660644
			 train-loss:  2.209783125469703 	 ± 0.2735742376664079
	data : 0.11422004699707031
	model : 0.06478519439697265
			 train-loss:  2.2111601495381557 	 ± 0.272991343648184
	data : 0.11423811912536622
	model : 0.06475028991699219
			 train-loss:  2.2096708080822363 	 ± 0.2725008899695069
	data : 0.11419968605041504
	model : 0.06482305526733398
			 train-loss:  2.2116477249273614 	 ± 0.2724378294645954
	data : 0.114436674118042
	model : 0.0648923397064209
			 train-loss:  2.2136931693112407 	 ± 0.27245772242441163
	data : 0.11449394226074219
	model : 0.06488571166992188
			 train-loss:  2.2111890614032745 	 ± 0.27300898006196894
	data : 0.11435656547546387
	model : 0.06486949920654297
			 train-loss:  2.2087035387971974 	 ± 0.27355080694911
	data : 0.11436190605163574
	model : 0.06486330032348633
			 train-loss:  2.207776126654252 	 ± 0.27277395415753836
	data : 0.11439008712768554
	model : 0.06486144065856933
			 train-loss:  2.206576954546592 	 ± 0.27215580769247466
	data : 0.11445393562316894
	model : 0.06485238075256347
			 train-loss:  2.2064365812710354 	 ± 0.27118713071892175
	data : 0.11440110206604004
	model : 0.06490330696105957
			 train-loss:  2.206950957048024 	 ± 0.27029229304513613
	data : 0.11456713676452637
	model : 0.0649200439453125
			 train-loss:  2.2081214928291213 	 ± 0.2696972802521558
	data : 0.11444568634033203
	model : 0.064935302734375
			 train-loss:  2.208449737175361 	 ± 0.26878109115061305
	data : 0.11437058448791504
	model : 0.06490683555603027
			 train-loss:  2.209389734599325 	 ± 0.26808196444022186
	data : 0.11408305168151855
	model : 0.06482982635498047
			 train-loss:  2.207782305520156 	 ± 0.2678513952963196
	data : 0.114070463180542
	model : 0.06488428115844727
			 train-loss:  2.2089050248877644 	 ± 0.26727465740468553
	data : 0.11404900550842285
	model : 0.06488575935363769
			 train-loss:  2.2075544271339367 	 ± 0.26686345899281155
	data : 0.11420035362243652
	model : 0.06489667892456055
			 train-loss:  2.2049249684488452 	 ± 0.26786429704139747
	data : 0.11418070793151855
	model : 0.06490797996520996
			 train-loss:  2.2058781393422375 	 ± 0.2672156293091375
	data : 0.11430063247680664
	model : 0.06498560905456544
			 train-loss:  2.21087215423584 	 ± 0.27321100293662237
	data : 0.11444320678710937
	model : 0.06490821838378906
			 train-loss:  2.2107627849705174 	 ± 0.27230812249275754
	data : 0.11425671577453614
	model : 0.06490039825439453
			 train-loss:  2.213111673530779 	 ± 0.2729413531372393
	data : 0.11410632133483886
	model : 0.06485495567321778
			 train-loss:  2.2172503907696095 	 ± 0.2767917608778609
	data : 0.11404995918273926
	model : 0.06483073234558105
			 train-loss:  2.2174573919989844 	 ± 0.27590350386208484
	data : 0.11404080390930176
	model : 0.06481714248657226
			 train-loss:  2.2160435253574002 	 ± 0.27557118477282005
	data : 0.11387648582458496
	model : 0.0648571491241455
			 train-loss:  2.2158172000677157 	 ± 0.27470097549597255
	data : 0.11401081085205078
	model : 0.06487593650817872
			 train-loss:  2.215652301053333 	 ± 0.27383247965087354
	data : 0.11419119834899902
	model : 0.06493048667907715
			 train-loss:  2.214044035235538 	 ± 0.27370737416226604
	data : 0.11414761543273926
	model : 0.06487302780151367
			 train-loss:  2.2135277334249244 	 ± 0.27292247297164274
	data : 0.11423711776733399
	model : 0.06486043930053711
			 train-loss:  2.213036189973354 	 ± 0.27213884563908974
	data : 0.1142500877380371
	model : 0.06485037803649903
			 train-loss:  2.2135409035297653 	 ± 0.2713674848181723
	data : 0.11433644294738769
	model : 0.06485767364501953
			 train-loss:  2.2112123826403676 	 ± 0.27213725493806096
	data : 0.11434545516967773
	model : 0.06487135887145996
			 train-loss:  2.2119526548619652 	 ± 0.271464756904317
	data : 0.11451492309570313
	model : 0.0649752140045166
			 train-loss:  2.210994260340202 	 ± 0.2709123193087117
	data : 0.11460094451904297
	model : 0.06499829292297363
			 train-loss:  2.2104781100244235 	 ± 0.27017099619143325
	data : 0.11473007202148437
	model : 0.06502227783203125
			 train-loss:  2.2101194391767662 	 ± 0.2693953980972021
	data : 0.1146317958831787
	model : 0.06497597694396973
			 train-loss:  2.211629311481636 	 ± 0.26929117877811726
	data : 0.11442375183105469
	model : 0.06492719650268555
			 train-loss:  2.211570145828383 	 ± 0.2684896093995304
	data : 0.11439862251281738
	model : 0.06485095024108886
			 train-loss:  2.2109347271496023 	 ± 0.26782074820477836
	data : 0.11421117782592774
	model : 0.06493964195251464
			 train-loss:  2.209923579412348 	 ± 0.2673552187069609
	data : 0.1141519546508789
	model : 0.06490945816040039
			 train-loss:  2.211355087352775 	 ± 0.26722495160974985
	data : 0.11423125267028808
	model : 0.06494703292846679
			 train-loss:  2.2107366829417474 	 ± 0.2665696900414279
	data : 0.11443276405334472
	model : 0.06498541831970214
			 train-loss:  2.213628870214341 	 ± 0.2684909462081208
	data : 0.11446166038513184
	model : 0.06503305435180665
			 train-loss:  2.211856195981475 	 ± 0.26873169435736216
	data : 0.1146270751953125
	model : 0.06499905586242676
			 train-loss:  2.21013834135873 	 ± 0.2689191975748523
	data : 0.11457533836364746
	model : 0.06506886482238769
			 train-loss:  2.2109838873147964 	 ± 0.26838732390659226
	data : 0.11456131935119629
	model : 0.06506571769714356
			 train-loss:  2.211548698824004 	 ± 0.26773296898508653
	data : 0.11434345245361328
	model : 0.06504340171813965
			 train-loss:  2.2102814229686607 	 ± 0.26751168320084245
	data : 0.11425390243530273
	model : 0.06504898071289063
			 train-loss:  2.2098578940556703 	 ± 0.26682323587400314
	data : 0.11424522399902344
	model : 0.06502413749694824
			 train-loss:  2.2091666764683193 	 ± 0.2662416878753706
	data : 0.11435990333557129
	model : 0.06496515274047851
			 train-loss:  2.2095877254865446 	 ± 0.26556528264656987
	data : 0.11430888175964356
	model : 0.06495327949523926
			 train-loss:  2.207875084746015 	 ± 0.26583513278667315
	data : 0.11459479331970215
	model : 0.06500658988952637
			 train-loss:  2.206897768817964 	 ± 0.26543547012704743
	data : 0.11468515396118165
	model : 0.06502704620361328
			 train-loss:  2.2071505273165912 	 ± 0.26473527742648584
	data : 0.11462678909301757
	model : 0.06500110626220704
			 train-loss:  2.2061273877685132 	 ± 0.264383328539252
	data : 0.11439952850341797
	model : 0.06496367454528809
			 train-loss:  2.2060261343115117 	 ± 0.26367525946172726
	data : 0.11437950134277344
	model : 0.06492505073547364
			 train-loss:  2.2055359849317826 	 ± 0.26305425059000037
	data : 0.11412644386291504
	model : 0.06487917900085449
			 train-loss:  2.206270861498853 	 ± 0.26254610049702065
	data : 0.11409616470336914
	model : 0.06486649513244629
			 train-loss:  2.204725638268486 	 ± 0.26270636435577777
	data : 0.11406469345092773
	model : 0.06492204666137695
			 train-loss:  2.208701008871982 	 ± 0.2676532719007019
	data : 0.11422419548034668
	model : 0.06494736671447754
			 train-loss:  2.2078512981924088 	 ± 0.26720850665236895
	data : 0.114231538772583
	model : 0.0650069236755371
			 train-loss:  2.208386745924751 	 ± 0.2666144585973859
	data : 0.11428542137145996
	model : 0.06496338844299317
			 train-loss:  2.207661834405494 	 ± 0.2661124908861189
	data : 0.11417346000671387
	model : 0.06486039161682129
			 train-loss:  2.207105035019904 	 ± 0.26553843891800016
	data : 0.11424651145935058
	model : 0.06476778984069824
			 train-loss:  2.20818504125644 	 ± 0.2652835343837379
	data : 0.11419596672058105
	model : 0.06476640701293945
			 train-loss:  2.207024282338668 	 ± 0.26510192593648124
	data : 0.11418633460998535
	model : 0.06475543975830078
			 train-loss:  2.207377031974986 	 ± 0.2644743343242444
	data : 0.11433358192443847
	model : 0.06481389999389649
			 train-loss:  2.2070739955613106 	 ± 0.2638399101894763
	data : 0.11455259323120118
	model : 0.06490588188171387
			 train-loss:  2.2076260168947766 	 ± 0.26329076653111816
	data : 0.11455783843994141
	model : 0.06495599746704102
			 train-loss:  2.2054262554645536 	 ± 0.26445863328972696
	data : 0.11450724601745606
	model : 0.06499428749084472
			 train-loss:  2.2035138725641357 	 ± 0.2651826885560997
	data : 0.11447205543518066
	model : 0.06496210098266601
			 train-loss:  2.2017789016855827 	 ± 0.2656666430176008
	data : 0.11434764862060547
	model : 0.06491475105285645
			 train-loss:  2.2013143342116783 	 ± 0.26509372455664276
	data : 0.11407198905944824
	model : 0.06494946479797363
			 train-loss:  2.2006190851622938 	 ± 0.26462865192937235
	data : 0.11405844688415527
	model : 0.06495656967163085
			 train-loss:  2.200568744612903 	 ± 0.26398340632538836
	data : 0.11422734260559082
	model : 0.06490840911865234
			 train-loss:  2.2005003956915106 	 ± 0.26334370871729135
	data : 0.11436581611633301
	model : 0.06496477127075195
			 train-loss:  2.20059743595584 	 ± 0.26271053475230327
	data : 0.1144223690032959
	model : 0.06506047248840333
			 train-loss:  2.2007939299711814 	 ± 0.26209350556576366
	data : 0.11456198692321777
	model : 0.0650421142578125
			 train-loss:  2.198718718934858 	 ± 0.2631731049468025
	data : 0.11460604667663574
	model : 0.0650334358215332
			 train-loss:  2.1992964676448277 	 ± 0.2626785794540878
	data : 0.11435379981994628
	model : 0.06498122215270996
			 train-loss:  2.2000458206610656 	 ± 0.26228027566501716
	data : 0.11431045532226562
	model : 0.06491103172302246
			 train-loss:  2.200332417802991 	 ± 0.26169407430090236
	data : 0.1141125202178955
	model : 0.0648653507232666
			 train-loss:  2.199175131712721 	 ± 0.2616222512371087
	data : 0.11421446800231934
	model : 0.06485652923583984
			 train-loss:  2.1994096074148874 	 ± 0.26103270057877315
	data : 0.11418976783752441
	model : 0.06485404968261718
			 train-loss:  2.199436446123345 	 ± 0.26042523621901065
	data : 0.11428647041320801
	model : 0.06494507789611817
			 train-loss:  2.1997572238798493 	 ± 0.25986427090335823
	data : 0.11429662704467773
	model : 0.06501946449279786
			 train-loss:  2.200401604999595 	 ± 0.25943772423704264
	data : 0.1144822120666504
	model : 0.06499695777893066
			 train-loss:  2.20037801550069 	 ± 0.25884223288512653
	data : 0.11431946754455566
	model : 0.06494836807250977
			 train-loss:  2.2001779493131592 	 ± 0.25826748617236
	data : 0.11424489021301269
	model : 0.06495504379272461
			 train-loss:  2.199223184585571 	 ± 0.25806692571999773
	data : 0.11432032585144043
	model : 0.06488971710205078
			 train-loss:  2.1998930365791147 	 ± 0.2576740221126816
	data : 0.11435518264770508
	model : 0.0647885799407959
			 train-loss:  2.200083672463357 	 ± 0.25710863973131187
	data : 0.11448502540588379
	model : 0.06474242210388184
			 train-loss:  2.199725703808224 	 ± 0.25658695550178023
	data : 0.11460590362548828
	model : 0.0647153377532959
			 train-loss:  2.1996288555009023 	 ± 0.2560176611175625
	data : 0.1146653175354004
	model : 0.06462430953979492
			 train-loss:  2.2020610915289986 	 ± 0.25802880547012963
	data : 0.11455163955688477
	model : 0.06449418067932129
			 train-loss:  2.20257385009158 	 ± 0.2575721743691761
	data : 0.11447834968566895
	model : 0.06436257362365723
			 train-loss:  2.202605497469461 	 ± 0.25700464894016706
	data : 0.11438283920288086
	model : 0.06420164108276367
			 train-loss:  2.20215991819114 	 ± 0.25652828164266617
	data : 0.11448426246643066
	model : 0.06409759521484375
			 train-loss:  2.20231611864015 	 ± 0.25597842955565947
	data : 0.11455450057983399
	model : 0.06399388313293457
			 train-loss:  2.20247554468072 	 ± 0.2554327420255693
	data : 0.11489195823669433
	model : 0.06397924423217774
			 train-loss:  2.201419123323449 	 ± 0.25538230552723157
	data : 0.11492929458618165
	model : 0.06404576301574708
			 train-loss:  2.2009332293066484 	 ± 0.25493830281651203
	data : 0.11499190330505371
	model : 0.06406760215759277
			 train-loss:  2.200902714749774 	 ± 0.2543910612478412
	data : 0.1148374080657959
	model : 0.06396827697753907
			 train-loss:  2.1997149051764073 	 ± 0.2544935966890339
	data : 0.11484928131103515
	model : 0.0639577865600586
			 train-loss:  2.2002328497298222 	 ± 0.254075108806186
	data : 0.11474385261535644
	model : 0.06395702362060547
			 train-loss:  2.1995043643450334 	 ± 0.2537820693219921
	data : 0.11482272148132325
	model : 0.06390032768249512
			 train-loss:  2.201917805249178 	 ± 0.2559457261155716
	data : 0.11478071212768555
	model : 0.06391048431396484
			 train-loss:  2.201984565798976 	 ± 0.2554095268765554
	data : 0.11494321823120117
	model : 0.06402349472045898
			 train-loss:  2.202353303901321 	 ± 0.2549381121677242
	data : 0.11496362686157227
	model : 0.06401300430297852
			 train-loss:  2.201494602362315 	 ± 0.25475255850125805
	data : 0.11482777595520019
	model : 0.0639183521270752
			 train-loss:  2.2008732097277504 	 ± 0.2544056742208607
	data : 0.11486034393310547
	model : 0.06392321586608887
			 train-loss:  2.201535029844804 	 ± 0.25408730623121056
	data : 0.1150273323059082
	model : 0.06391744613647461
			 train-loss:  2.2018224614147295 	 ± 0.2536033753500462
	data : 0.11501655578613282
	model : 0.06387109756469726
			 train-loss:  2.200985425319828 	 ± 0.25341929724814394
	data : 0.11488995552062989
	model : 0.06389389038085938
			 train-loss:  2.201535493986947 	 ± 0.2530475068636171
	data : 0.11500639915466308
	model : 0.06400179862976074
			 train-loss:  2.2014306081019765 	 ± 0.2525379953063071
	data : 0.11497783660888672
	model : 0.063991117477417
			 train-loss:  2.2021369099134374 	 ± 0.25226961574545503
	data : 0.1146240234375
	model : 0.06389555931091309
			 train-loss:  2.2013102202646193 	 ± 0.2520955175858574
	data : 0.11456265449523925
	model : 0.06388168334960938
			 train-loss:  2.20072281312751 	 ± 0.25175879758362774
	data : 0.11470232009887696
	model : 0.06394834518432617
			 train-loss:  2.200599685668945 	 ± 0.25126228750949303
	data : 0.11473641395568848
	model : 0.06392302513122558
			 train-loss:  2.2011769007876576 	 ± 0.2509272934511489
	data : 0.11473026275634765
	model : 0.0639042854309082
			 train-loss:  2.203360373065585 	 ± 0.2528068487874837
	data : 0.11495013236999511
	model : 0.06398248672485352
			 train-loss:  2.2027561975562056 	 ± 0.25248896216755395
	data : 0.11490101814270019
	model : 0.06397519111633301
			 train-loss:  2.201795110083002 	 ± 0.2524547136679737
	data : 0.11482362747192383
	model : 0.06388249397277831
			 train-loss:  2.2004323230070226 	 ± 0.2528936012003875
	data : 0.11470623016357422
	model : 0.0639216423034668
			 train-loss:  2.198315965011716 	 ± 0.254651696005353
	data : 0.1145434856414795
	model : 0.05554203987121582
#epoch  95    val-loss:  2.439383707548443  train-loss:  2.198315965011716  lr:  1.9073486328125e-08
			 train-loss:  2.2381629943847656 	 ± 0.0
	data : 5.561061382293701
	model : 0.0762629508972168
			 train-loss:  2.1513808965682983 	 ± 0.08678209781646729
	data : 2.8437451124191284
	model : 0.07057452201843262
			 train-loss:  2.1390711466471353 	 ± 0.07296447688563527
	data : 1.933848222096761
	model : 0.06861662864685059
			 train-loss:  2.2163907885551453 	 ± 0.14808052620025505
	data : 1.4788225889205933
	model : 0.06753522157669067
			 train-loss:  2.2307342052459718 	 ± 0.13551829471057494
	data : 1.2057655811309815
	model : 0.06697244644165039
			 train-loss:  2.190081318219503 	 ± 0.15351751133226713
	data : 0.11635208129882812
	model : 0.06468143463134765
			 train-loss:  2.1683856759752547 	 ± 0.15174000093878326
	data : 0.11393747329711915
	model : 0.06467504501342773
			 train-loss:  2.1630828976631165 	 ± 0.14263146779053695
	data : 0.11396141052246093
	model : 0.06470904350280762
			 train-loss:  2.138163063261244 	 ± 0.15182656457252994
	data : 0.11413512229919434
	model : 0.064854097366333
			 train-loss:  2.1440276145935058 	 ± 0.14510586411260418
	data : 0.11421041488647461
	model : 0.06484298706054688
			 train-loss:  2.131162925200029 	 ± 0.14421011931762054
	data : 0.11397261619567871
	model : 0.06478323936462402
			 train-loss:  2.1284040609995523 	 ± 0.13837354017024184
	data : 0.11379442214965821
	model : 0.06502256393432618
			 train-loss:  2.1497068405151367 	 ± 0.15205286332389184
	data : 0.11369819641113281
	model : 0.06503496170043946
			 train-loss:  2.1574667521885464 	 ± 0.14916920064513586
	data : 0.11365900039672852
	model : 0.06501994132995606
			 train-loss:  2.1569196859995525 	 ± 0.14412567548097338
	data : 0.11366147994995117
	model : 0.06506934165954589
			 train-loss:  2.15695521235466 	 ± 0.13954915305680837
	data : 0.11382918357849121
	model : 0.06513071060180664
			 train-loss:  2.1814979665419636 	 ± 0.167230341833322
	data : 0.11397161483764648
	model : 0.06491293907165527
			 train-loss:  2.1851886510849 	 ± 0.16322953428594877
	data : 0.11390805244445801
	model : 0.06485981941223144
			 train-loss:  2.190260560888993 	 ± 0.16032656731554804
	data : 0.11382951736450195
	model : 0.06482667922973633
			 train-loss:  2.1942893385887148 	 ± 0.1572506525364817
	data : 0.11394686698913574
	model : 0.06479744911193848
			 train-loss:  2.1979959351675853 	 ± 0.15435359451077837
	data : 0.11401948928833008
	model : 0.06482291221618652
			 train-loss:  2.1898454319347036 	 ± 0.15536125930887815
	data : 0.11402740478515624
	model : 0.06487970352172852
			 train-loss:  2.204143306483393 	 ± 0.16608769858297942
	data : 0.11417183876037598
	model : 0.06494460105895997
			 train-loss:  2.1949701259533563 	 ± 0.16843731775819606
	data : 0.11430010795593262
	model : 0.06497635841369628
			 train-loss:  2.187599091529846 	 ± 0.16893861718498301
	data : 0.11425886154174805
	model : 0.06498451232910156
			 train-loss:  2.179361811051002 	 ± 0.1707011246308321
	data : 0.11415834426879883
	model : 0.06492533683776855
			 train-loss:  2.172430409325494 	 ± 0.1711981570385863
	data : 0.11401972770690919
	model : 0.06484413146972656
			 train-loss:  2.162042464528765 	 ± 0.17656619018793931
	data : 0.11403903961181641
	model : 0.06481409072875977
			 train-loss:  2.188046537596604 	 ± 0.22143741555144172
	data : 0.11402287483215331
	model : 0.06478743553161621
			 train-loss:  2.192210642496745 	 ± 0.21886730688903816
	data : 0.11403717994689941
	model : 0.06479935646057129
			 train-loss:  2.1892640436849287 	 ± 0.2159122891050873
	data : 0.11421294212341308
	model : 0.06485614776611329
			 train-loss:  2.1825001016259193 	 ± 0.2158230263914726
	data : 0.11430211067199707
	model : 0.06487641334533692
			 train-loss:  2.1805648370222612 	 ± 0.21280959636739546
	data : 0.11418423652648926
	model : 0.0648808479309082
			 train-loss:  2.172676384449005 	 ± 0.21449811212307476
	data : 0.1139000415802002
	model : 0.06484761238098144
			 train-loss:  2.175519388062613 	 ± 0.21206059378726536
	data : 0.11379404067993164
	model : 0.06483612060546876
			 train-loss:  2.173241671588686 	 ± 0.20952832019218534
	data : 0.11374387741088868
	model : 0.06484045982360839
			 train-loss:  2.1723256336676107 	 ± 0.20675053029651622
	data : 0.11374211311340332
	model : 0.06487064361572266
			 train-loss:  2.1730244504778007 	 ± 0.20405627053903835
	data : 0.11387443542480469
	model : 0.06489710807800293
			 train-loss:  2.1712658191338563 	 ± 0.2017147036419883
	data : 0.11407237052917481
	model : 0.06502141952514648
			 train-loss:  2.172880467772484 	 ± 0.19943238864979637
	data : 0.11415047645568847
	model : 0.06501803398132325
			 train-loss:  2.167162299156189 	 ± 0.2002775469018858
	data : 0.114017915725708
	model : 0.06498537063598633
			 train-loss:  2.168189000515711 	 ± 0.19798810177804638
	data : 0.11396417617797852
	model : 0.06491641998291016
			 train-loss:  2.1640185450398643 	 ± 0.19753017602195622
	data : 0.11380252838134766
	model : 0.06490006446838378
			 train-loss:  2.1669806214896115 	 ± 0.19623626424496798
	data : 0.11376519203186035
	model : 0.06482605934143067
			 train-loss:  2.1685956875483194 	 ± 0.19433912225755273
	data : 0.11371750831604004
	model : 0.06484723091125488
			 train-loss:  2.1660106052523074 	 ± 0.19299579496885264
	data : 0.11383986473083496
	model : 0.06486277580261231
			 train-loss:  2.1587575192147113 	 ± 0.19716696812724419
	data : 0.11395735740661621
	model : 0.06487431526184081
			 train-loss:  2.1632221092780433 	 ± 0.19748861208862678
	data : 0.1140127182006836
	model : 0.0648108959197998
			 train-loss:  2.1659946490307243 	 ± 0.19640461467296963
	data : 0.1140897274017334
	model : 0.06478805541992187
			 train-loss:  2.171897168159485 	 ± 0.1987722920397728
	data : 0.11404051780700683
	model : 0.06472887992858886
			 train-loss:  2.1782149146584904 	 ± 0.20182023170715233
	data : 0.11399741172790527
	model : 0.06471419334411621
			 train-loss:  2.18042444724303 	 ± 0.20049212803714753
	data : 0.11400604248046875
	model : 0.06472039222717285
			 train-loss:  2.1827130497626537 	 ± 0.1992762358081638
	data : 0.11408791542053223
	model : 0.06477727890014648
			 train-loss:  2.179109412210959 	 ± 0.19915797178053501
	data : 0.11407299041748047
	model : 0.06479544639587402
			 train-loss:  2.1745132207870483 	 ± 0.20020860278625288
	data : 0.11407675743103027
	model : 0.06484422683715821
			 train-loss:  2.172094389796257 	 ± 0.199222235213068
	data : 0.11400389671325684
	model : 0.06481261253356933
			 train-loss:  2.1729340866992346 	 ± 0.19756689270474953
	data : 0.11392555236816407
	model : 0.0647916316986084
			 train-loss:  2.171690811370981 	 ± 0.1960811227875757
	data : 0.11389303207397461
	model : 0.06478581428527833
			 train-loss:  2.165187021433297 	 ± 0.20062279885105683
	data : 0.1138850212097168
	model : 0.06481046676635742
			 train-loss:  2.1636368532975516 	 ± 0.1992999261135843
	data : 0.11400117874145507
	model : 0.06486244201660156
			 train-loss:  2.1622021413240278 	 ± 0.1979717388031263
	data : 0.11407170295715333
	model : 0.06494841575622559
			 train-loss:  2.1602645439486348 	 ± 0.19695095475052196
	data : 0.11411452293395996
	model : 0.06497869491577149
			 train-loss:  2.159901495963808 	 ± 0.19540251081143703
	data : 0.11412959098815918
	model : 0.06495885848999024
			 train-loss:  2.1691000405699015 	 ± 0.20716220291235846
	data : 0.1141087532043457
	model : 0.06485676765441895
			 train-loss:  2.1700585970511805 	 ± 0.20570545586376393
	data : 0.11409387588500977
	model : 0.06478676795959473
			 train-loss:  2.1678400346727082 	 ± 0.20492323798652823
	data : 0.11413402557373047
	model : 0.0647202491760254
			 train-loss:  2.1685614888347797 	 ± 0.20347264448516708
	data : 0.11407880783081055
	model : 0.06473255157470703
			 train-loss:  2.1672987534719357 	 ± 0.20223528039658292
	data : 0.1141624927520752
	model : 0.06479778289794921
			 train-loss:  2.165233019469441 	 ± 0.20148583425664318
	data : 0.11414289474487305
	model : 0.06486196517944336
			 train-loss:  2.163806406089238 	 ± 0.20039216898472725
	data : 0.11413283348083496
	model : 0.06488981246948242
			 train-loss:  2.161084729181209 	 ± 0.20027470210501666
	data : 0.1141118049621582
	model : 0.06489772796630859
			 train-loss:  2.164513452185525 	 ± 0.20096656343551808
	data : 0.11406126022338867
	model : 0.06479191780090332
			 train-loss:  2.167575770861482 	 ± 0.20126973609112017
	data : 0.1139519214630127
	model : 0.06472158432006836
			 train-loss:  2.169918566136747 	 ± 0.2009048412773109
	data : 0.11400079727172852
	model : 0.06475176811218261
			 train-loss:  2.177429854075114 	 ± 0.20976087389969975
	data : 0.11405220031738281
	model : 0.06472973823547364
			 train-loss:  2.174511358926171 	 ± 0.2099035560449363
	data : 0.11411919593811035
	model : 0.06477694511413574
			 train-loss:  2.1753413909441464 	 ± 0.20866159671627887
	data : 0.1141876220703125
	model : 0.06488919258117676
			 train-loss:  2.17487496137619 	 ± 0.20736010465794594
	data : 0.11424732208251953
	model : 0.06494174003601075
			 train-loss:  2.1764121131051946 	 ± 0.20649027335604475
	data : 0.1141092300415039
	model : 0.06494550704956055
			 train-loss:  2.1771561607718466 	 ± 0.2053021919653093
	data : 0.11391363143920899
	model : 0.06491589546203613
			 train-loss:  2.178134501716237 	 ± 0.2042185207185816
	data : 0.11378140449523926
	model : 0.06490774154663086
			 train-loss:  2.1785433830284493 	 ± 0.20300282276691894
	data : 0.11388492584228516
	model : 0.06491036415100097
			 train-loss:  2.1766030630433417 	 ± 0.20253976222570194
	data : 0.11387062072753906
	model : 0.06492371559143066
			 train-loss:  2.1815137025855837 	 ± 0.2062413331737405
	data : 0.11401395797729492
	model : 0.06489181518554688
			 train-loss:  2.1801709918414844 	 ± 0.20539355216054725
	data : 0.11403050422668456
	model : 0.06496548652648926
			 train-loss:  2.178742837074191 	 ± 0.20461998595875433
	data : 0.11409873962402343
	model : 0.06491456031799317
			 train-loss:  2.176354438409038 	 ± 0.20464277273195913
	data : 0.11394038200378417
	model : 0.06485152244567871
			 train-loss:  2.175235577604987 	 ± 0.20374415678450744
	data : 0.11391510963439941
	model : 0.06481022834777832
			 train-loss:  2.1745617416467558 	 ± 0.202694881208487
	data : 0.1137998104095459
	model : 0.06481318473815918
			 train-loss:  2.1703053911527 	 ± 0.20552635528981072
	data : 0.11382379531860351
	model : 0.06484918594360352
			 train-loss:  2.1709834782631843 	 ± 0.20449517644232176
	data : 0.11384315490722656
	model : 0.064923095703125
			 train-loss:  2.1781322839467423 	 ± 0.21450949667279545
	data : 0.11389288902282715
	model : 0.06494617462158203
			 train-loss:  2.1790929648184005 	 ± 0.21355199365837277
	data : 0.11386938095092773
	model : 0.06496586799621581
			 train-loss:  2.1797107090341283 	 ± 0.2124965641269301
	data : 0.11390681266784668
	model : 0.06490764617919922
			 train-loss:  2.1779474823098433 	 ± 0.2120653654572213
	data : 0.11376252174377441
	model : 0.06479172706604004
			 train-loss:  2.1782617333034673 	 ± 0.21098020148383545
	data : 0.11369547843933106
	model : 0.06475610733032226
			 train-loss:  2.178735837493975 	 ± 0.20994125498728938
	data : 0.11372690200805664
	model : 0.06479411125183106
			 train-loss:  2.1808133234783096 	 ± 0.2098671704489043
	data : 0.11389307975769043
	model : 0.0647977352142334
			 train-loss:  2.1791028482745394 	 ± 0.20948999765929008
	data : 0.11384539604187012
	model : 0.06490969657897949
			 train-loss:  2.1792299544811247 	 ± 0.20844375253502795
	data : 0.1140838623046875
	model : 0.06497268676757813
			 train-loss:  2.1807640531275534 	 ± 0.2079758584981865
	data : 0.11406049728393555
	model : 0.06492218971252442
			 train-loss:  2.1796705944865358 	 ± 0.20724541042160216
	data : 0.11402359008789062
	model : 0.06485662460327149
			 train-loss:  2.1800423374453795 	 ± 0.20627108161063662
	data : 0.1138570785522461
	model : 0.06483378410339355
			 train-loss:  2.179931414815096 	 ± 0.20528008509095197
	data : 0.1139261245727539
	model : 0.0648310661315918
			 train-loss:  2.18052537668319 	 ± 0.20438999745553735
	data : 0.11383090019226075
	model : 0.06484546661376953
			 train-loss:  2.179380179576154 	 ± 0.20376179659736793
	data : 0.11385159492492676
	model : 0.06490201950073242
			 train-loss:  2.1824456921247677 	 ± 0.20524854166145648
	data : 0.1138223648071289
	model : 0.06495423316955566
			 train-loss:  2.1819429298241935 	 ± 0.2043622904467153
	data : 0.11372866630554199
	model : 0.06499176025390625
			 train-loss:  2.1844232990107404 	 ± 0.20504933862741515
	data : 0.11370563507080078
	model : 0.0648575782775879
			 train-loss:  2.1847027269276706 	 ± 0.2041360149803737
	data : 0.11373100280761719
	model : 0.06481285095214843
			 train-loss:  2.1856096317102245 	 ± 0.2034368844895986
	data : 0.1137472152709961
	model : 0.06479597091674805
			 train-loss:  2.184594185224601 	 ± 0.2028090201744332
	data : 0.11382126808166504
	model : 0.06482954025268554
			 train-loss:  2.1851035419818574 	 ± 0.20198158545522069
	data : 0.11394624710083008
	model : 0.06479468345642089
			 train-loss:  2.183055483458335 	 ± 0.20226883035681317
	data : 0.11392555236816407
	model : 0.06489486694335937
			 train-loss:  2.182429648482281 	 ± 0.20149830672988595
	data : 0.11388187408447266
	model : 0.06489934921264648
			 train-loss:  2.1850700327034653 	 ± 0.2026161183548059
	data : 0.11377320289611817
	model : 0.06483664512634277
			 train-loss:  2.1839305019786215 	 ± 0.2021213449791869
	data : 0.11369113922119141
	model : 0.06484642028808593
			 train-loss:  2.184878576610048 	 ± 0.2015241693525366
	data : 0.11373429298400879
	model : 0.06486787796020507
			 train-loss:  2.183957990478067 	 ± 0.20092465279515073
	data : 0.1138160228729248
	model : 0.06486344337463379
			 train-loss:  2.1851971914370854 	 ± 0.20054184633058808
	data : 0.11392545700073242
	model : 0.06490945816040039
			 train-loss:  2.1839025089563417 	 ± 0.20021439581217323
	data : 0.11398377418518066
	model : 0.0650324821472168
			 train-loss:  2.1836105555784506 	 ± 0.19941801767612888
	data : 0.11408052444458008
	model : 0.06493058204650878
			 train-loss:  2.183629552523295 	 ± 0.19860583179111047
	data : 0.11383776664733887
	model : 0.06487665176391602
			 train-loss:  2.1817718427027426 	 ± 0.19887347980225412
	data : 0.11379194259643555
	model : 0.06489415168762207
			 train-loss:  2.182022036552429 	 ± 0.19809598104012238
	data : 0.11379103660583496
	model : 0.06488595008850098
			 train-loss:  2.180483557875194 	 ± 0.19805665275201564
	data : 0.11390137672424316
	model : 0.0648693561553955
			 train-loss:  2.1797914495618325 	 ± 0.19742827524127401
	data : 0.11391277313232422
	model : 0.0649726390838623
			 train-loss:  2.179127431474626 	 ± 0.1967978802657904
	data : 0.11407928466796875
	model : 0.06501798629760742
			 train-loss:  2.1851936966873877 	 ± 0.20770056126780984
	data : 0.11416902542114257
	model : 0.06496253013610839
			 train-loss:  2.1892994908186107 	 ± 0.21209032044856824
	data : 0.11408867835998535
	model : 0.06487102508544922
			 train-loss:  2.1896758434426693 	 ± 0.21132283603539692
	data : 0.11393098831176758
	model : 0.06478567123413086
			 train-loss:  2.189841094342145 	 ± 0.21052934515181626
	data : 0.11386585235595703
	model : 0.06473207473754883
			 train-loss:  2.18860159511853 	 ± 0.2102192945439269
	data : 0.1139486312866211
	model : 0.06474537849426269
			 train-loss:  2.1884966510445323 	 ± 0.20943692220892654
	data : 0.11393933296203614
	model : 0.06478915214538575
			 train-loss:  2.187856290075514 	 ± 0.20879141624490255
	data : 0.11397099494934082
	model : 0.06487798690795898
			 train-loss:  2.1869480636189964 	 ± 0.20828987159107393
	data : 0.11414599418640137
	model : 0.06490373611450195
			 train-loss:  2.184213472978912 	 ± 0.20996428332313216
	data : 0.11406221389770507
	model : 0.06491303443908691
			 train-loss:  2.1844774588294653 	 ± 0.20922497718055505
	data : 0.11411466598510742
	model : 0.0648930549621582
			 train-loss:  2.1822958318449612 	 ± 0.2100404076666168
	data : 0.11391005516052247
	model : 0.06491789817810059
			 train-loss:  2.181262154238565 	 ± 0.20964343930554377
	data : 0.11393036842346191
	model : 0.06491689682006836
			 train-loss:  2.179295834074629 	 ± 0.21019030657625373
	data : 0.11387166976928711
	model : 0.06494307518005371
			 train-loss:  2.1794424711818428 	 ± 0.20945612979546926
	data : 0.11398539543151856
	model : 0.06496706008911132
			 train-loss:  2.176580370722951 	 ± 0.2114906237076578
	data : 0.1138758659362793
	model : 0.06496639251708984
			 train-loss:  2.17518569694625 	 ± 0.21141386489634714
	data : 0.11401510238647461
	model : 0.0648965835571289
			 train-loss:  2.1747936906485723 	 ± 0.21073609929781492
	data : 0.11393098831176758
	model : 0.06482067108154296
			 train-loss:  2.1751216388728523 	 ± 0.21005028505347753
	data : 0.11391825675964355
	model : 0.06478290557861328
			 train-loss:  2.1759210476258986 	 ± 0.20955734417579794
	data : 0.11380462646484375
	model : 0.06476645469665528
			 train-loss:  2.1794724367760323 	 ± 0.21324066222511814
	data : 0.1137995719909668
	model : 0.0647392749786377
			 train-loss:  2.1824379191302614 	 ± 0.21556420459939388
	data : 0.11373052597045899
	model : 0.06475315093994141
			 train-loss:  2.1820971171061196 	 ± 0.21488472697270675
	data : 0.11387014389038086
	model : 0.06478686332702636
			 train-loss:  2.179492411234521 	 ± 0.21653480582319995
	data : 0.11382470130920411
	model : 0.06473760604858399
			 train-loss:  2.178810757241751 	 ± 0.21598383130544088
	data : 0.11391224861145019
	model : 0.06470794677734375
			 train-loss:  2.178191561324924 	 ± 0.21541215658514193
	data : 0.11390857696533203
	model : 0.06474623680114747
			 train-loss:  2.1768524700945076 	 ± 0.21534956984302467
	data : 0.11400790214538574
	model : 0.06476993560791015
			 train-loss:  2.175857275532138 	 ± 0.21500875410957832
	data : 0.11386637687683106
	model : 0.06480522155761718
			 train-loss:  2.176192733721855 	 ± 0.21435920461067878
	data : 0.11398515701293946
	model : 0.06488032341003418
			 train-loss:  2.1760376555145164 	 ± 0.213684220199065
	data : 0.11392412185668946
	model : 0.06486649513244629
			 train-loss:  2.177904564368574 	 ± 0.21428754600838956
	data : 0.11379022598266601
	model : 0.06478877067565918
			 train-loss:  2.1772200184048347 	 ± 0.21378585541544354
	data : 0.11376819610595704
	model : 0.06479501724243164
			 train-loss:  2.1770850859582422 	 ± 0.2131235191186616
	data : 0.11377978324890137
	model : 0.06477627754211426
			 train-loss:  2.177289667336837 	 ± 0.21247637283988272
	data : 0.11370882987976075
	model : 0.0647963523864746
			 train-loss:  2.1773040920128057 	 ± 0.21181964548196539
	data : 0.11378722190856934
	model : 0.06483030319213867
			 train-loss:  2.1761038917705324 	 ± 0.2117207097922434
	data : 0.11390666961669922
	model : 0.06490435600280761
			 train-loss:  2.1754177488931794 	 ± 0.21125593688705877
	data : 0.1139078140258789
	model : 0.06486425399780274
			 train-loss:  2.1763820893836745 	 ± 0.21097654816594785
	data : 0.11397032737731934
	model : 0.06486039161682129
			 train-loss:  2.1785899702324927 	 ± 0.21224348385773312
	data : 0.11383838653564453
	model : 0.06477742195129395
			 train-loss:  2.1795786640601245 	 ± 0.21199014062093524
	data : 0.1137300968170166
	model : 0.06473665237426758
			 train-loss:  2.1824740767478943 	 ± 0.21464470884977246
	data : 0.11378655433654786
	model : 0.06471452713012696
			 train-loss:  2.183694608112764 	 ± 0.21459264348986196
	data : 0.11368384361267089
	model : 0.064752197265625
			 train-loss:  2.1840000054415536 	 ± 0.213997388964916
	data : 0.1137608528137207
	model : 0.06475596427917481
			 train-loss:  2.1835946994915343 	 ± 0.21343617909978962
	data : 0.11390500068664551
	model : 0.06480393409729004
			 train-loss:  2.1842499361481775 	 ± 0.2129872394787264
	data : 0.11397972106933593
	model : 0.06481084823608399
			 train-loss:  2.1846938450212425 	 ± 0.2124505601824579
	data : 0.11372594833374024
	model : 0.0647812843322754
			 train-loss:  2.1826112277206335 	 ± 0.21360288994253873
	data : 0.11374239921569824
	model : 0.06474499702453614
			 train-loss:  2.1823322180339266 	 ± 0.21302351690451732
	data : 0.11361117362976074
	model : 0.0647921085357666
			 train-loss:  2.179537818513133 	 ± 0.2156100657201664
	data : 0.11360273361206055
	model : 0.06483597755432129
			 train-loss:  2.1787429778589367 	 ± 0.21525856516670194
	data : 0.11356267929077149
	model : 0.06489419937133789
			 train-loss:  2.1801037594173733 	 ± 0.21541515603292627
	data : 0.11377530097961426
	model : 0.0648888111114502
			 train-loss:  2.181416468247355 	 ± 0.21552536128312466
	data : 0.11375856399536133
	model : 0.06487894058227539
			 train-loss:  2.1814179943667518 	 ± 0.21492584687392527
	data : 0.11379475593566894
	model : 0.06480703353881836
			 train-loss:  2.180482081286815 	 ± 0.21469880540287958
	data : 0.11388936042785644
	model : 0.0647970199584961
			 train-loss:  2.1823484026468716 	 ± 0.21557540622932106
	data : 0.1140446662902832
	model : 0.06474952697753907
			 train-loss:  2.182698529274737 	 ± 0.2150374791080803
	data : 0.11403532028198242
	model : 0.06480093002319336
			 train-loss:  2.182926360031833 	 ± 0.21447448788760567
	data : 0.1141653060913086
	model : 0.0648336410522461
			 train-loss:  2.1848016500473024 	 ± 0.21540133746973
	data : 0.1141883373260498
	model : 0.06481060981750489
			 train-loss:  2.1836002956154528 	 ± 0.21544207503719717
	data : 0.11409649848937989
	model : 0.06478958129882813
			 train-loss:  2.184688084265765 	 ± 0.21537680574560888
	data : 0.11382160186767579
	model : 0.06472687721252442
			 train-loss:  2.185910595858351 	 ± 0.21545279335513673
	data : 0.11379151344299317
	model : 0.06476941108703613
			 train-loss:  2.1853833255313693 	 ± 0.2150036391017073
	data : 0.11369891166687011
	model : 0.06479768753051758
			 train-loss:  2.183773196370978 	 ± 0.21557655861652794
	data : 0.1137939453125
	model : 0.06485800743103028
			 train-loss:  2.1841663939790577 	 ± 0.21507978053361
	data : 0.11381354331970214
	model : 0.06485214233398437
			 train-loss:  2.1835437466700873 	 ± 0.2146914688686528
	data : 0.11404910087585449
	model : 0.0649721622467041
			 train-loss:  2.1857541138644048 	 ± 0.2163138108914417
	data : 0.11407990455627441
	model : 0.06489048004150391
			 train-loss:  2.1854756861617886 	 ± 0.21579025088551462
	data : 0.1141125202178955
	model : 0.06479692459106445
			 train-loss:  2.1850479162656344 	 ± 0.2153186819168883
	data : 0.11403617858886719
	model : 0.06478557586669922
			 train-loss:  2.185497850787883 	 ± 0.21486058104716368
	data : 0.11401982307434082
	model : 0.06479649543762207
			 train-loss:  2.1852854290589465 	 ± 0.21433518829372789
	data : 0.11403851509094239
	model : 0.06476187705993652
			 train-loss:  2.1852699759030583 	 ± 0.21379336271118657
	data : 0.1141214370727539
	model : 0.06479763984680176
			 train-loss:  2.185174129716116 	 ± 0.21325978154067834
	data : 0.11420140266418458
	model : 0.06485309600830078
			 train-loss:  2.1866521298885346 	 ± 0.2137452880605515
	data : 0.11425085067749023
	model : 0.06482644081115722
			 train-loss:  2.1870089478753694 	 ± 0.21327262655139076
	data : 0.1141169548034668
	model : 0.06476941108703613
			 train-loss:  2.1865184354309988 	 ± 0.2128576989616593
	data : 0.1139176368713379
	model : 0.06471433639526367
			 train-loss:  2.186029433029626 	 ± 0.21244648463778712
	data : 0.11379170417785645
	model : 0.06472082138061523
			 train-loss:  2.1872192364113006 	 ± 0.212602068330146
	data : 0.11367583274841309
	model : 0.06473898887634277
			 train-loss:  2.1864801546422448 	 ± 0.21234544203216074
	data : 0.11361451148986816
	model : 0.06476268768310547
			 train-loss:  2.186230313430712 	 ± 0.21185961540069204
	data : 0.11383647918701172
	model : 0.06482625007629395
			 train-loss:  2.1849434352727326 	 ± 0.2121528004333113
	data : 0.1139719009399414
	model : 0.0649444580078125
			 train-loss:  2.1867405588810263 	 ± 0.21321576030890546
	data : 0.11390476226806641
	model : 0.06487102508544922
			 train-loss:  2.1852918749220636 	 ± 0.21372873089795535
	data : 0.11383814811706543
	model : 0.0648007869720459
			 train-loss:  2.1842152629579816 	 ± 0.21378656937880178
	data : 0.11389236450195313
	model : 0.06479482650756836
			 train-loss:  2.183948314585392 	 ± 0.2133144444435516
	data : 0.11377735137939453
	model : 0.06479759216308593
			 train-loss:  2.1824401752004086 	 ± 0.2139353439217104
	data : 0.11385798454284668
	model : 0.06473765373229981
			 train-loss:  2.1819991203540927 	 ± 0.21352914745232235
	data : 0.11406149864196777
	model : 0.06483049392700195
			 train-loss:  2.182688843424075 	 ± 0.2132673566121768
	data : 0.11436753273010254
	model : 0.0648965835571289
			 train-loss:  2.183536115912504 	 ± 0.21313151114602477
	data : 0.11435418128967285
	model : 0.06487407684326171
			 train-loss:  2.18343253047378 	 ± 0.21264300333003278
	data : 0.11434874534606934
	model : 0.06482992172241211
			 train-loss:  2.183975554831017 	 ± 0.2123025352536084
	data : 0.11436276435852051
	model : 0.0647824764251709
			 train-loss:  2.1845479274014816 	 ± 0.21198279163282882
	data : 0.1142608642578125
	model : 0.06471953392028809
			 train-loss:  2.1835352400122168 	 ± 0.21202613120227626
	data : 0.11405806541442871
	model : 0.0647578239440918
			 train-loss:  2.182617899504575 	 ± 0.2119788445066655
	data : 0.11409554481506348
	model : 0.06473746299743652
			 train-loss:  2.182938393424539 	 ± 0.21155212640395538
	data : 0.11416683197021485
	model : 0.06474785804748535
			 train-loss:  2.1853513792828396 	 ± 0.21410156210666348
	data : 0.11416702270507813
	model : 0.0646977424621582
			 train-loss:  2.185075047839383 	 ± 0.2136606477758096
	data : 0.11415495872497558
	model : 0.0646557331085205
			 train-loss:  2.1850901820829938 	 ± 0.21318331301382729
	data : 0.11407852172851562
	model : 0.064457368850708
			 train-loss:  2.1844560771518284 	 ± 0.21292065582239186
	data : 0.1140854835510254
	model : 0.06437759399414063
			 train-loss:  2.183717424890636 	 ± 0.2127377940853518
	data : 0.11414031982421875
	model : 0.06422882080078125
			 train-loss:  2.182911224827367 	 ± 0.21261441153659152
	data : 0.11420931816101074
	model : 0.06414351463317872
			 train-loss:  2.182298006718619 	 ± 0.212348725483224
	data : 0.1143078327178955
	model : 0.06400151252746582
			 train-loss:  2.1836557200902416 	 ± 0.21287405881441232
	data : 0.11453685760498047
	model : 0.06397247314453125
			 train-loss:  2.185762284113013 	 ± 0.21478955918360182
	data : 0.11458959579467773
	model : 0.06381459236145019
			 train-loss:  2.1850033126352155 	 ± 0.21463300472916566
	data : 0.11452302932739258
	model : 0.06378192901611328
			 train-loss:  2.184799537576478 	 ± 0.21419232657368917
	data : 0.11452269554138184
	model : 0.06373310089111328
			 train-loss:  2.184574371755379 	 ± 0.21375970686636372
	data : 0.11458139419555664
	model : 0.06376204490661622
			 train-loss:  2.1848002325775275 	 ± 0.21333032648103617
	data : 0.11456522941589356
	model : 0.06375913619995117
			 train-loss:  2.1844750323194138 	 ± 0.21293406508062393
	data : 0.11460690498352051
	model : 0.06386938095092773
			 train-loss:  2.1848241151389427 	 ± 0.21254983031640096
	data : 0.11475424766540528
	model : 0.06381196975708008
			 train-loss:  2.1843748112771078 	 ± 0.2122132195530968
	data : 0.11471285820007324
	model : 0.06373152732849122
			 train-loss:  2.1854996310562647 	 ± 0.21247373297080446
	data : 0.11469779014587403
	model : 0.06368608474731445
			 train-loss:  2.1858593749201947 	 ± 0.21210138271221277
	data : 0.11475367546081543
	model : 0.06365714073181153
			 train-loss:  2.1860178381204607 	 ± 0.21167322022281457
	data : 0.1146998405456543
	model : 0.06361575126647949
			 train-loss:  2.187925165619593 	 ± 0.21329025427959575
	data : 0.11466789245605469
	model : 0.06367473602294922
			 train-loss:  2.1882514401900868 	 ± 0.21290937450100295
	data : 0.11469130516052246
	model : 0.0637448787689209
			 train-loss:  2.1888949400112954 	 ± 0.21270652824032604
	data : 0.1146841049194336
	model : 0.06370811462402344
			 train-loss:  2.1880290371472717 	 ± 0.21269894000536588
	data : 0.11474695205688476
	model : 0.06374139785766601
			 train-loss:  2.1879263595658904 	 ± 0.21227047606457924
	data : 0.11474266052246093
	model : 0.0637472152709961
			 train-loss:  2.186835019084496 	 ± 0.21252621068768662
	data : 0.11482539176940917
	model : 0.06378440856933594
			 train-loss:  2.1887188310082624 	 ± 0.21414368576445988
	data : 0.11484065055847167
	model : 0.06381254196166992
			 train-loss:  2.1887650456159347 	 ± 0.2137127425904177
	data : 0.11491250991821289
	model : 0.06385979652404786
			 train-loss:  2.1886243030249353 	 ± 0.2132946848784821
	data : 0.11476278305053711
	model : 0.06379046440124511
			 train-loss:  2.1882452120780944 	 ± 0.2129517031713284
	data : 0.11455006599426269
	model : 0.06373605728149415
			 train-loss:  2.188988476160513 	 ± 0.21285174941775398
	data : 0.11451683044433594
	model : 0.06374073028564453
			 train-loss:  2.1897041111711473 	 ± 0.2127313506358588
	data : 0.11453084945678711
	model : 0.06375999450683593
			 train-loss:  2.1902289668561914 	 ± 0.21247393914840898
	data : 0.11444520950317383
	model : 0.0637669563293457
			 train-loss:  2.1903695623705706 	 ± 0.21206706246957446
	data : 0.11444292068481446
	model : 0.0638211727142334
			 train-loss:  2.1892856798919977 	 ± 0.21235459964503173
	data : 0.11469516754150391
	model : 0.06387968063354492
			 train-loss:  2.1874990579672158 	 ± 0.21385109365771926
	data : 0.11433382034301758
	model : 0.055379295349121095
#epoch  96    val-loss:  2.467513598893818  train-loss:  2.1874990579672158  lr:  1.9073486328125e-08
			 train-loss:  2.402062177658081 	 ± 0.0
	data : 4.767148971557617
	model : 0.10111188888549805
			 train-loss:  2.3048653602600098 	 ± 0.09719681739807129
	data : 2.8051687479019165
	model : 0.08270525932312012
			 train-loss:  2.1475780407587686 	 ± 0.2361710170476336
	data : 1.9079349835713704
	model : 0.07675449053446452
			 train-loss:  2.0791424810886383 	 ± 0.23639551487301355
	data : 1.4595885276794434
	model : 0.07374030351638794
			 train-loss:  2.098583197593689 	 ± 0.21498380728459698
	data : 1.1905242919921875
	model : 0.07185149192810059
			 train-loss:  2.121505916118622 	 ± 0.2028356139464101
	data : 0.25972929000854494
	model : 0.06451807022094727
			 train-loss:  2.127017855644226 	 ± 0.18827401753755738
	data : 0.11372866630554199
	model : 0.06455135345458984
			 train-loss:  2.1042500436306 	 ± 0.18613121800756752
	data : 0.11376585960388183
	model : 0.06454601287841796
			 train-loss:  2.128738456302219 	 ± 0.18866071018526637
	data : 0.1137082576751709
	model : 0.06457352638244629
			 train-loss:  2.1209931135177613 	 ± 0.18048127388869717
	data : 0.11367769241333008
	model : 0.06473674774169921
			 train-loss:  2.1248170896009966 	 ± 0.17250649776542976
	data : 0.11385993957519532
	model : 0.06486563682556153
			 train-loss:  2.143113613128662 	 ± 0.17595740382888786
	data : 0.11404166221618653
	model : 0.06494250297546386
			 train-loss:  2.151388223354633 	 ± 0.17146726235002047
	data : 0.11406230926513672
	model : 0.06484031677246094
			 train-loss:  2.1523931537355696 	 ± 0.16526971156143008
	data : 0.11380929946899414
	model : 0.06476807594299316
			 train-loss:  2.1617358843485515 	 ± 0.16344770316501714
	data : 0.11364121437072754
	model : 0.06470637321472168
			 train-loss:  2.1683747619390488 	 ± 0.16033270136024558
	data : 0.11361150741577149
	model : 0.06462998390197754
			 train-loss:  2.1911031077889835 	 ± 0.18016566523256475
	data : 0.11359648704528809
	model : 0.06462798118591309
			 train-loss:  2.183381862110562 	 ± 0.17796025368427654
	data : 0.11366233825683594
	model : 0.06478462219238282
			 train-loss:  2.1869633825201737 	 ± 0.17387900613156337
	data : 0.11391539573669433
	model : 0.06485962867736816
			 train-loss:  2.1780152916908264 	 ± 0.17390661745589206
	data : 0.11404328346252442
	model : 0.06478314399719239
			 train-loss:  2.1624895561309088 	 ± 0.18336933677600695
	data : 0.11407718658447266
	model : 0.06478853225708008
			 train-loss:  2.153906058181416 	 ± 0.1834206666480758
	data : 0.11409640312194824
	model : 0.06479225158691407
			 train-loss:  2.1640951063321983 	 ± 0.18564580023148525
	data : 0.11406230926513672
	model : 0.06467852592468262
			 train-loss:  2.1567561328411102 	 ± 0.18511385752118145
	data : 0.11390848159790039
	model : 0.06472058296203613
			 train-loss:  2.1615359210968017 	 ± 0.1828791066660686
	data : 0.11394844055175782
	model : 0.06480998992919922
			 train-loss:  2.1583868723649244 	 ± 0.1800176189581829
	data : 0.11397309303283691
	model : 0.0648355484008789
			 train-loss:  2.1595465960326017 	 ± 0.17675145583008198
	data : 0.11404690742492676
	model : 0.06483888626098633
			 train-loss:  2.159894142832075 	 ± 0.17357587873931155
	data : 0.1140899658203125
	model : 0.06492447853088379
			 train-loss:  2.162599571819963 	 ± 0.1711566844174421
	data : 0.11411628723144532
	model : 0.06485066413879395
			 train-loss:  2.169211721420288 	 ± 0.17200586835295684
	data : 0.11391806602478027
	model : 0.06477060317993164
			 train-loss:  2.174876889874858 	 ± 0.17203038991663863
	data : 0.11377935409545899
	model : 0.06473040580749512
			 train-loss:  2.1648991219699383 	 ± 0.1782017389300789
	data : 0.11368870735168457
	model : 0.06471776962280273
			 train-loss:  2.160574851614056 	 ± 0.17717770466035462
	data : 0.11375675201416016
	model : 0.06472902297973633
			 train-loss:  2.155481121119331 	 ± 0.17698832641161247
	data : 0.11386609077453613
	model : 0.06481585502624512
			 train-loss:  2.16144129208156 	 ± 0.17786982643200075
	data : 0.11410312652587891
	model : 0.06489477157592774
			 train-loss:  2.1678804953893027 	 ± 0.17947163485753306
	data : 0.11413154602050782
	model : 0.06489019393920899
			 train-loss:  2.168035919601853 	 ± 0.17703218630621276
	data : 0.11402764320373535
	model : 0.064825439453125
			 train-loss:  2.1621116964440596 	 ± 0.17836539824945088
	data : 0.11381330490112304
	model : 0.06481719017028809
			 train-loss:  2.164493053387373 	 ± 0.17667472625277242
	data : 0.113822603225708
	model : 0.06474947929382324
			 train-loss:  2.1629821240901945 	 ± 0.17470730754717034
	data : 0.113775634765625
	model : 0.0647660732269287
			 train-loss:  2.1600505026375374 	 ± 0.17355680559736458
	data : 0.11384997367858887
	model : 0.06485657691955567
			 train-loss:  2.1646809861773537 	 ± 0.1740226181337163
	data : 0.11397304534912109
	model : 0.06491451263427735
			 train-loss:  2.1704127123189525 	 ± 0.17595285814528344
	data : 0.1140449047088623
	model : 0.06488595008850098
			 train-loss:  2.175358674742959 	 ± 0.17693974900714127
	data : 0.11389222145080566
	model : 0.0648310661315918
			 train-loss:  2.1808519840240477 	 ± 0.17871684960925027
	data : 0.11368727684020996
	model : 0.06475706100463867
			 train-loss:  2.1803796135860942 	 ± 0.1767920014710816
	data : 0.11355047225952149
	model : 0.06466941833496094
			 train-loss:  2.1820027625307126 	 ± 0.1752472401997279
	data : 0.11349349021911621
	model : 0.06467008590698242
			 train-loss:  2.177858717739582 	 ± 0.17572394924290405
	data : 0.1135554313659668
	model : 0.06465296745300293
			 train-loss:  2.181248385079053 	 ± 0.1754999633042745
	data : 0.11376843452453614
	model : 0.06473913192749023
			 train-loss:  2.178517506122589 	 ± 0.1747846093661386
	data : 0.11390271186828613
	model : 0.06482701301574707
			 train-loss:  2.1725651937372543 	 ± 0.1781071247119925
	data : 0.11396236419677734
	model : 0.06485562324523926
			 train-loss:  2.1689413556685815 	 ± 0.17827464454505595
	data : 0.11402401924133301
	model : 0.06482634544372559
			 train-loss:  2.173669266250898 	 ± 0.17984591894772484
	data : 0.11405682563781738
	model : 0.06482987403869629
			 train-loss:  2.1768628138082997 	 ± 0.1796833739320158
	data : 0.1138582706451416
	model : 0.06478581428527833
			 train-loss:  2.174509044127031 	 ± 0.17888059455890032
	data : 0.11402330398559571
	model : 0.0647843360900879
			 train-loss:  2.1770498795168742 	 ± 0.1782749026898944
	data : 0.11404423713684082
	model : 0.06477890014648438
			 train-loss:  2.1749355792999268 	 ± 0.17741109852787704
	data : 0.11402354240417481
	model : 0.06480979919433594
			 train-loss:  2.1741683894190293 	 ± 0.17597039453807722
	data : 0.11402482986450195
	model : 0.06475658416748047
			 train-loss:  2.1781858306820108 	 ± 0.17713511737812543
	data : 0.11407108306884765
	model : 0.06476421356201172
			 train-loss:  2.183331294854482 	 ± 0.18004436842960045
	data : 0.11391692161560059
	model : 0.06464991569519044
			 train-loss:  2.1881828425360506 	 ± 0.1824741518075396
	data : 0.11395883560180664
	model : 0.06464452743530273
			 train-loss:  2.1904069608257664 	 ± 0.18182826922521012
	data : 0.1139498233795166
	model : 0.06464357376098633
			 train-loss:  2.1872733907094077 	 ± 0.18205913239821298
	data : 0.11391997337341309
	model : 0.06472830772399903
			 train-loss:  2.186009945347905 	 ± 0.18090935673113995
	data : 0.11391453742980957
	model : 0.06473946571350098
			 train-loss:  2.1847264051437376 	 ± 0.17980579268840036
	data : 0.11389288902282715
	model : 0.06484379768371581
			 train-loss:  2.182181092825803 	 ± 0.1796145392118101
	data : 0.11394290924072266
	model : 0.06484355926513671
			 train-loss:  2.1847166854943803 	 ± 0.1794552833260751
	data : 0.11392216682434082
	model : 0.0648050308227539
			 train-loss:  2.187236735049416 	 ± 0.17932122320014818
	data : 0.1137202262878418
	model : 0.06472620964050294
			 train-loss:  2.1869416703348574 	 ± 0.17803367921842925
	data : 0.11380410194396973
	model : 0.06469841003417968
			 train-loss:  2.1917653032711573 	 ± 0.1812419489525602
	data : 0.11395235061645508
	model : 0.06467342376708984
			 train-loss:  2.192941794932728 	 ± 0.18023006515454446
	data : 0.1139195442199707
	model : 0.06469478607177734
			 train-loss:  2.19401743180222 	 ± 0.17920343731811
	data : 0.11390390396118164
	model : 0.06474180221557617
			 train-loss:  2.1917717440487587 	 ± 0.17898899425260106
	data : 0.11403975486755372
	model : 0.0647819995880127
			 train-loss:  2.1954697289982357 	 ± 0.18056137069033984
	data : 0.11404953002929688
	model : 0.06481842994689942
			 train-loss:  2.1993095954259236 	 ± 0.18236997640493674
	data : 0.11392369270324706
	model : 0.06475672721862794
			 train-loss:  2.19734272988219 	 ± 0.18196520106978487
	data : 0.1139571189880371
	model : 0.06472616195678711
			 train-loss:  2.194115439018646 	 ± 0.18295597271107045
	data : 0.11400423049926758
	model : 0.06476664543151855
			 train-loss:  2.194685589044522 	 ± 0.1818482302156046
	data : 0.11405320167541504
	model : 0.06482758522033691
			 train-loss:  2.191030386128003 	 ± 0.18355464131309535
	data : 0.1139686107635498
	model : 0.06484708786010743
			 train-loss:  2.1894350424408913 	 ± 0.18295413937693497
	data : 0.11397972106933593
	model : 0.06489925384521485
			 train-loss:  2.1882178915871515 	 ± 0.18214690873819536
	data : 0.1138418197631836
	model : 0.06486868858337402
			 train-loss:  2.195677851758352 	 ± 0.19308191063620378
	data : 0.11370525360107422
	model : 0.06481575965881348
			 train-loss:  2.1980831895966126 	 ± 0.19314731004413443
	data : 0.11370329856872559
	model : 0.06485376358032227
			 train-loss:  2.196517077230272 	 ± 0.19252361018167122
	data : 0.11376008987426758
	model : 0.06486659049987793
			 train-loss:  2.193558596162235 	 ± 0.1932989832036183
	data : 0.11375150680541993
	model : 0.06493606567382812
			 train-loss:  2.19520199714705 	 ± 0.19276823179926342
	data : 0.11393117904663086
	model : 0.06500520706176757
			 train-loss:  2.1979128670418397 	 ± 0.19329890737824656
	data : 0.11397600173950195
	model : 0.06501588821411133
			 train-loss:  2.1954838741909373 	 ± 0.19352821994850294
	data : 0.11390113830566406
	model : 0.06495709419250488
			 train-loss:  2.199136538451977 	 ± 0.19546468199143488
	data : 0.11388740539550782
	model : 0.06491708755493164
			 train-loss:  2.205923875172933 	 ± 0.20465085517485276
	data : 0.11392326354980468
	model : 0.06485319137573242
			 train-loss:  2.2069076627165405 	 ± 0.2037371750105495
	data : 0.11368036270141602
	model : 0.06487245559692383
			 train-loss:  2.2053737795871236 	 ± 0.20315451765694648
	data : 0.11370286941528321
	model : 0.06488199234008789
			 train-loss:  2.2044630768478557 	 ± 0.20224806207219734
	data : 0.11384210586547852
	model : 0.06492929458618164
			 train-loss:  2.204722483107384 	 ± 0.20118495180139104
	data : 0.11386780738830567
	model : 0.06492929458618164
			 train-loss:  2.2064584857539127 	 ± 0.20082981959020893
	data : 0.11382822990417481
	model : 0.0648721694946289
			 train-loss:  2.2089355612794557 	 ± 0.20123467846161397
	data : 0.11378741264343262
	model : 0.06477470397949218
			 train-loss:  2.2102483154572163 	 ± 0.20060746887184983
	data : 0.11378097534179688
	model : 0.06476454734802246
			 train-loss:  2.2113055380023257 	 ± 0.1998527679669359
	data : 0.11368913650512695
	model : 0.06473569869995117
			 train-loss:  2.2131109020926734 	 ± 0.1996424263247139
	data : 0.11363253593444825
	model : 0.06473002433776856
			 train-loss:  2.213588161468506 	 ± 0.1986984581711715
	data : 0.11360793113708496
	model : 0.06481313705444336
			 train-loss:  2.2107998474989787 	 ± 0.19966883833894372
	data : 0.11387453079223633
	model : 0.06487932205200195
			 train-loss:  2.2097473331526216 	 ± 0.1989690231424478
	data : 0.11401181221008301
	model : 0.06486849784851074
			 train-loss:  2.2097339676421823 	 ± 0.1980008443182317
	data : 0.11392374038696289
	model : 0.0647740364074707
			 train-loss:  2.2098733003322897 	 ± 0.19705169169029998
	data : 0.11389632225036621
	model : 0.06482820510864258
			 train-loss:  2.209108134678432 	 ± 0.1962662870783804
	data : 0.11395635604858398
	model : 0.06479225158691407
			 train-loss:  2.2079594157776743 	 ± 0.19569263708004347
	data : 0.11384978294372558
	model : 0.06479415893554688
			 train-loss:  2.2099384459379676 	 ± 0.195838864188614
	data : 0.11388726234436035
	model : 0.06482815742492676
			 train-loss:  2.2079089515739017 	 ± 0.19605728324874278
	data : 0.11409182548522949
	model : 0.06492848396301269
			 train-loss:  2.2080597997805396 	 ± 0.19516216197796998
	data : 0.11411981582641602
	model : 0.0649289608001709
			 train-loss:  2.2073572212999517 	 ± 0.19441146229989492
	data : 0.11427350044250488
	model : 0.06495776176452636
			 train-loss:  2.2082124841105832 	 ± 0.19374151855744728
	data : 0.11420154571533203
	model : 0.06490874290466309
			 train-loss:  2.205819287470409 	 ± 0.19451574605487681
	data : 0.11392755508422851
	model : 0.06487517356872559
			 train-loss:  2.2049266026083347 	 ± 0.19388344834886667
	data : 0.11388297080993652
	model : 0.06484642028808593
			 train-loss:  2.2032367200182197 	 ± 0.19386526820501748
	data : 0.11391239166259766
	model : 0.06480813026428223
			 train-loss:  2.1998373218204663 	 ± 0.19640341287941607
	data : 0.11379489898681641
	model : 0.06484718322753906
			 train-loss:  2.19830185174942 	 ± 0.1962470264121697
	data : 0.11393847465515136
	model : 0.0649134635925293
			 train-loss:  2.200059664555085 	 ± 0.19632155869747328
	data : 0.11401810646057128
	model : 0.06493916511535644
			 train-loss:  2.200696177401785 	 ± 0.19560912138465522
	data : 0.11392374038696289
	model : 0.06491675376892089
			 train-loss:  2.2009821659376643 	 ± 0.19481027259856226
	data : 0.11384921073913574
	model : 0.06484837532043457
			 train-loss:  2.2001776178677876 	 ± 0.19419529355270784
	data : 0.11370682716369629
	model : 0.06478452682495117
			 train-loss:  2.1982307581862144 	 ± 0.19456355178786156
	data : 0.11357250213623046
	model : 0.06480274200439454
			 train-loss:  2.1985070422047475 	 ± 0.19378835175081463
	data : 0.11371021270751953
	model : 0.06483616828918456
			 train-loss:  2.197762476719492 	 ± 0.19317412584400764
	data : 0.11375546455383301
	model : 0.06486740112304687
			 train-loss:  2.1983743934862074 	 ± 0.19251327703513016
	data : 0.11383037567138672
	model : 0.06494698524475098
			 train-loss:  2.1991266355514525 	 ± 0.19192456483627135
	data : 0.11395630836486817
	model : 0.06495556831359864
			 train-loss:  2.2011882265408835 	 ± 0.1925460118710925
	data : 0.11404118537902833
	model : 0.06486067771911622
			 train-loss:  2.2011980569268776 	 ± 0.19178649031180844
	data : 0.11382436752319336
	model : 0.06477398872375488
			 train-loss:  2.2011871626600623 	 ± 0.19103589482619127
	data : 0.11380720138549805
	model : 0.06481552124023438
			 train-loss:  2.2011889625889385 	 ± 0.19029400614292688
	data : 0.11380109786987305
	model : 0.06481633186340333
			 train-loss:  2.2025422618939325 	 ± 0.19018283164073463
	data : 0.11387114524841309
	model : 0.06483831405639648
			 train-loss:  2.2005423107220015 	 ± 0.1908229046394137
	data : 0.11394290924072266
	model : 0.06492304801940918
			 train-loss:  2.1976304740616768 	 ± 0.19299804101859466
	data : 0.11413774490356446
	model : 0.06498775482177735
			 train-loss:  2.1963382376764056 	 ± 0.19284347366927823
	data : 0.11416788101196289
	model : 0.06494536399841308
			 train-loss:  2.1957883852631297 	 ± 0.1922271815906742
	data : 0.11412186622619629
	model : 0.06488170623779296
			 train-loss:  2.196736404630873 	 ± 0.19182806714504325
	data : 0.11415190696716308
	model : 0.06487493515014649
			 train-loss:  2.200218479422962 	 ± 0.19535682178254424
	data : 0.11394720077514649
	model : 0.06483607292175293
			 train-loss:  2.2003632374923594 	 ± 0.1946498554822849
	data : 0.11380600929260254
	model : 0.06480674743652344
			 train-loss:  2.1989440866138623 	 ± 0.19465335325659616
	data : 0.11387763023376465
	model : 0.06481099128723145
			 train-loss:  2.2005825265706016 	 ± 0.19490458568826335
	data : 0.11398162841796874
	model : 0.06486659049987793
			 train-loss:  2.1991177490779332 	 ± 0.19497356429208743
	data : 0.11379766464233398
	model : 0.06483974456787109
			 train-loss:  2.198114883815143 	 ± 0.1946429717706942
	data : 0.11388311386108399
	model : 0.06487064361572266
			 train-loss:  2.198364237664451 	 ± 0.1939789974395436
	data : 0.11393547058105469
	model : 0.06485037803649903
			 train-loss:  2.198281019717663 	 ± 0.1933021029413079
	data : 0.11372804641723633
	model : 0.0648165225982666
			 train-loss:  2.2005693945619793 	 ± 0.19456377168200772
	data : 0.11364927291870117
	model : 0.06478490829467773
			 train-loss:  2.2004169085930134 	 ± 0.19390033560304878
	data : 0.11374177932739257
	model : 0.06482524871826172
			 train-loss:  2.20132986650075 	 ± 0.19354761811776278
	data : 0.11386599540710449
	model : 0.06484227180480957
			 train-loss:  2.2013743977968385 	 ± 0.1928889199546767
	data : 0.11390419006347656
	model : 0.06489086151123047
			 train-loss:  2.2023365014308207 	 ± 0.1925897510627622
	data : 0.11394262313842773
	model : 0.0648622989654541
			 train-loss:  2.2018402774861996 	 ± 0.19203729805142672
	data : 0.11393308639526367
	model : 0.06481866836547852
			 train-loss:  2.201881915728251 	 ± 0.19139677812947511
	data : 0.11382951736450195
	model : 0.06479034423828126
			 train-loss:  2.199716599571784 	 ± 0.19259650814080942
	data : 0.11373672485351563
	model : 0.0648190975189209
			 train-loss:  2.1985854945684733 	 ± 0.19246446035873827
	data : 0.1137629508972168
	model : 0.06481785774230957
			 train-loss:  2.1991856160506704 	 ± 0.19197708814053535
	data : 0.11390542984008789
	model : 0.0648684024810791
			 train-loss:  2.1982025496371382 	 ± 0.19173874170270427
	data : 0.11393671035766602
	model : 0.06494274139404296
			 train-loss:  2.196687742971605 	 ± 0.19204149046417254
	data : 0.11400308609008789
	model : 0.06497387886047364
			 train-loss:  2.195476996592986 	 ± 0.1920175507055338
	data : 0.11406154632568359
	model : 0.06488890647888183
			 train-loss:  2.1952436759973026 	 ± 0.19142723606820142
	data : 0.11413917541503907
	model : 0.06489377021789551
			 train-loss:  2.1939012347897395 	 ± 0.19156042782550206
	data : 0.11403260231018067
	model : 0.06489062309265137
			 train-loss:  2.1936487169385708 	 ± 0.19098346461672502
	data : 0.11398043632507324
	model : 0.06490092277526856
			 train-loss:  2.1917257234454155 	 ± 0.19192364032475662
	data : 0.11391363143920899
	model : 0.06491913795471191
			 train-loss:  2.1920708454913975 	 ± 0.1913764727869922
	data : 0.11386647224426269
	model : 0.06496434211730957
			 train-loss:  2.1899424607371105 	 ± 0.1926868128829217
	data : 0.11376099586486817
	model : 0.06492714881896973
			 train-loss:  2.1897102360345104 	 ± 0.19211757813716573
	data : 0.11375784873962402
	model : 0.06496009826660157
			 train-loss:  2.189190093337036 	 ± 0.19164604703900653
	data : 0.11367621421813964
	model : 0.06490502357482911
			 train-loss:  2.188225508458687 	 ± 0.19146331680925
	data : 0.11368746757507324
	model : 0.06487374305725098
			 train-loss:  2.189606754894716 	 ± 0.19170853665564536
	data : 0.1136974811553955
	model : 0.06486706733703614
			 train-loss:  2.1912288758569134 	 ± 0.19227293641906548
	data : 0.11370081901550293
	model : 0.06489582061767578
			 train-loss:  2.191803541211855 	 ± 0.1918436325104622
	data : 0.11368727684020996
	model : 0.06489028930664062
			 train-loss:  2.190268102482226 	 ± 0.19230776484762713
	data : 0.11384010314941406
	model : 0.06494998931884766
			 train-loss:  2.191728277066175 	 ± 0.19267864483915637
	data : 0.11385498046875
	model : 0.0649219036102295
			 train-loss:  2.1931088521466617 	 ± 0.1929558821232714
	data : 0.11374225616455078
	model : 0.06487994194030762
			 train-loss:  2.1997971874336866 	 ± 0.21134093837631687
	data : 0.11350135803222657
	model : 0.06482815742492676
			 train-loss:  2.2002472043726486 	 ± 0.21081187279212443
	data : 0.11353387832641601
	model : 0.06480703353881836
			 train-loss:  2.198465903600057 	 ± 0.21150689956998908
	data : 0.11352224349975586
	model : 0.06483902931213378
			 train-loss:  2.1957036120550972 	 ± 0.21402617439896487
	data : 0.11359868049621583
	model : 0.06494593620300293
			 train-loss:  2.195960758084601 	 ± 0.21344438816988212
	data : 0.11392159461975097
	model : 0.06498303413391113
			 train-loss:  2.1943881531893195 	 ± 0.21386064928071638
	data : 0.11416387557983398
	model : 0.06503357887268066
			 train-loss:  2.1935433437315264 	 ± 0.21355504397076952
	data : 0.11410269737243653
	model : 0.0649827003479004
			 train-loss:  2.1924826319657225 	 ± 0.21342737763277253
	data : 0.1140976905822754
	model : 0.06494560241699218
			 train-loss:  2.1920180062452954 	 ± 0.21292445853360287
	data : 0.11403484344482422
	model : 0.06488313674926757
			 train-loss:  2.1904930436150147 	 ± 0.21331886304248798
	data : 0.11373543739318848
	model : 0.064915132522583
			 train-loss:  2.191943273439512 	 ± 0.2136248659355775
	data : 0.11383676528930664
	model : 0.06493186950683594
			 train-loss:  2.1898719691187956 	 ± 0.2148651750815202
	data : 0.1139714241027832
	model : 0.06502804756164551
			 train-loss:  2.19062867242357 	 ± 0.21452487361152844
	data : 0.11407475471496582
	model : 0.06501421928405762
			 train-loss:  2.1906465130883293 	 ± 0.21394442789189497
	data : 0.11404013633728027
	model : 0.06497383117675781
			 train-loss:  2.1913817851774153 	 ± 0.21360277775476094
	data : 0.11411786079406738
	model : 0.0648676872253418
			 train-loss:  2.1907848699845096 	 ± 0.2131863734305587
	data : 0.11396074295043945
	model : 0.06483697891235352
			 train-loss:  2.1920802276185216 	 ± 0.21335524310680448
	data : 0.11382937431335449
	model : 0.06478228569030761
			 train-loss:  2.193298719547413 	 ± 0.2134449311014521
	data : 0.11374835968017578
	model : 0.0647810459136963
			 train-loss:  2.1941893916381034 	 ± 0.21323435198912324
	data : 0.11383237838745117
	model : 0.06481823921203614
			 train-loss:  2.196166406751303 	 ± 0.2144142339548053
	data : 0.11380505561828613
	model : 0.06488280296325684
			 train-loss:  2.1958177611231804 	 ± 0.21390940925940452
	data : 0.1137627124786377
	model : 0.06486020088195801
			 train-loss:  2.1939790409463673 	 ± 0.21487038532854097
	data : 0.11384768486022949
	model : 0.06484122276306152
			 train-loss:  2.19329516174867 	 ± 0.21452636391623808
	data : 0.1138145923614502
	model : 0.06483135223388672
			 train-loss:  2.1939799797840607 	 ± 0.21418808114554805
	data : 0.11387786865234376
	model : 0.064815092086792
			 train-loss:  2.1951694026285287 	 ± 0.2142856548217153
	data : 0.11390032768249511
	model : 0.06483654975891114
			 train-loss:  2.196769012412444 	 ± 0.21491107258335787
	data : 0.11403374671936035
	model : 0.06487197875976562
			 train-loss:  2.1953333092458323 	 ± 0.2153127204649175
	data : 0.11406373977661133
	model : 0.0648590087890625
			 train-loss:  2.195654245477226 	 ± 0.21481852563401643
	data : 0.11404876708984375
	model : 0.06484599113464355
			 train-loss:  2.1963331431150435 	 ± 0.21449471662332148
	data : 0.1138545036315918
	model : 0.06475229263305664
			 train-loss:  2.196334523941154 	 ± 0.21396048326866113
	data : 0.11382555961608887
	model : 0.06474733352661133
			 train-loss:  2.1969120697219773 	 ± 0.21358722956904405
	data : 0.11381001472473144
	model : 0.06476063728332519
			 train-loss:  2.196367499276335 	 ± 0.21320103780037244
	data : 0.1137650489807129
	model : 0.06483297348022461
			 train-loss:  2.1966059283882964 	 ± 0.21270497333535468
	data : 0.1138197898864746
	model : 0.06484785079956054
			 train-loss:  2.197503986009737 	 ± 0.2125728902516383
	data : 0.11387877464294434
	model : 0.06493325233459472
			 train-loss:  2.19839584422343 	 ± 0.2124404326787059
	data : 0.11419024467468261
	model : 0.06488113403320313
			 train-loss:  2.1975428384283315 	 ± 0.21228001101134703
	data : 0.11410360336303711
	model : 0.06483182907104493
			 train-loss:  2.1981844747295747 	 ± 0.2119702250125878
	data : 0.11411423683166504
	model : 0.06475648880004883
			 train-loss:  2.196083752732528 	 ± 0.21362187265177146
	data : 0.1140068531036377
	model : 0.06477718353271485
			 train-loss:  2.1969320870581126 	 ± 0.2134652419697585
	data : 0.11406750679016113
	model : 0.0648280143737793
			 train-loss:  2.195862100022664 	 ± 0.21352253608484806
	data : 0.11421098709106445
	model : 0.06490249633789062
			 train-loss:  2.196510358801428 	 ± 0.2132263771960708
	data : 0.11450986862182617
	model : 0.06492900848388672
			 train-loss:  2.196535699244396 	 ± 0.21272557690705873
	data : 0.11491079330444336
	model : 0.06497354507446289
			 train-loss:  2.1960783918327262 	 ± 0.21233289201574218
	data : 0.11503772735595703
	model : 0.06494174003601075
			 train-loss:  2.194564322538154 	 ± 0.2129932698683446
	data : 0.11489715576171874
	model : 0.06485972404479981
			 train-loss:  2.195122712188297 	 ± 0.2126573332874896
	data : 0.11456899642944336
	model : 0.06478443145751953
			 train-loss:  2.194900151221983 	 ± 0.2121919862731746
	data : 0.11440434455871581
	model : 0.06476774215698242
			 train-loss:  2.194589508782833 	 ± 0.21175419842518897
	data : 0.11408514976501465
	model : 0.06477456092834473
			 train-loss:  2.196216068311369 	 ± 0.21263079298751109
	data : 0.11409659385681152
	model : 0.06476593017578125
			 train-loss:  2.1951283693313597 	 ± 0.21275676770721966
	data : 0.1142648696899414
	model : 0.06479620933532715
			 train-loss:  2.194252236396479 	 ± 0.21267227172830652
	data : 0.11432323455810547
	model : 0.06477060317993164
			 train-loss:  2.1927196657335437 	 ± 0.2134123649049821
	data : 0.11428189277648926
	model : 0.06468443870544434
			 train-loss:  2.1933073217024184 	 ± 0.21311327019282997
	data : 0.11428532600402833
	model : 0.06454696655273437
			 train-loss:  2.19325109145471 	 ± 0.21263869678703268
	data : 0.11426997184753418
	model : 0.0646202564239502
			 train-loss:  2.1928196038140193 	 ± 0.2122639006834602
	data : 0.11428303718566894
	model : 0.0644864559173584
			 train-loss:  2.1928956941165754 	 ± 0.21179684500256013
	data : 0.1143519401550293
	model : 0.06437864303588867
			 train-loss:  2.1916932791865347 	 ± 0.212101491789129
	data : 0.11448307037353515
	model : 0.06428699493408203
			 train-loss:  2.1925782848868454 	 ± 0.21205547763073013
	data : 0.11457681655883789
	model : 0.06413640975952148
			 train-loss:  2.195461592299449 	 ± 0.21602460532431572
	data : 0.11454358100891113
	model : 0.06379690170288085
			 train-loss:  2.195664460244386 	 ± 0.21557633520650923
	data : 0.11466689109802246
	model : 0.06370816230773926
			 train-loss:  2.195194443066915 	 ± 0.21522728572394476
	data : 0.11454148292541504
	model : 0.06367344856262207
			 train-loss:  2.1952024878099046 	 ± 0.21476296769991216
	data : 0.11451640129089355
	model : 0.06366105079650879
			 train-loss:  2.1954486538923863 	 ± 0.21433440604228982
	data : 0.11450753211975098
	model : 0.06372632980346679
			 train-loss:  2.195182173170595 	 ± 0.21391461351537777
	data : 0.11471481323242187
	model : 0.06382274627685547
			 train-loss:  2.1941028879043905 	 ± 0.21409651380991238
	data : 0.11461839675903321
	model : 0.06380853652954102
			 train-loss:  2.1930648588528068 	 ± 0.21423422886741195
	data : 0.11474413871765136
	model : 0.06378788948059082
			 train-loss:  2.1923219730079424 	 ± 0.21408618137831334
	data : 0.11478729248046875
	model : 0.06382865905761718
			 train-loss:  2.1911057273880776 	 ± 0.21445489253895336
	data : 0.11492252349853516
	model : 0.06388115882873535
			 train-loss:  2.19133694401346 	 ± 0.2140354973701385
	data : 0.11490387916564941
	model : 0.06386117935180664
			 train-loss:  2.1915678789218265 	 ± 0.21361896042169778
	data : 0.11504158973693848
	model : 0.06391539573669433
			 train-loss:  2.1904081996545752 	 ± 0.2139310096610973
	data : 0.11501274108886719
	model : 0.06397299766540528
			 train-loss:  2.191178847442974 	 ± 0.21382349880874943
	data : 0.11494803428649902
	model : 0.06384568214416504
			 train-loss:  2.191030061294022 	 ± 0.21339563190181834
	data : 0.11467008590698242
	model : 0.06379756927490235
			 train-loss:  2.1929178780219596 	 ± 0.2149815849087734
	data : 0.11459889411926269
	model : 0.06380639076232911
			 train-loss:  2.1953163589750018 	 ± 0.21778912313225038
	data : 0.11476798057556152
	model : 0.06377706527709961
			 train-loss:  2.1955565558216437 	 ± 0.21737852674535235
	data : 0.11483550071716309
	model : 0.06378870010375977
			 train-loss:  2.196849567687463 	 ± 0.21788390689554232
	data : 0.11493887901306152
	model : 0.06388545036315918
			 train-loss:  2.196769051494137 	 ± 0.2174478630899931
	data : 0.11506104469299316
	model : 0.06378850936889649
			 train-loss:  2.196426252284682 	 ± 0.21707791710855118
	data : 0.11510133743286133
	model : 0.06374502182006836
			 train-loss:  2.1976462531089784 	 ± 0.21749699517145496
	data : 0.11479172706604004
	model : 0.0637521743774414
			 train-loss:  2.1963510622541267 	 ± 0.218027192887108
	data : 0.11477775573730468
	model : 0.06374006271362305
			 train-loss:  2.1960333486398063 	 ± 0.2176523809031765
	data : 0.11473674774169922
	model : 0.06373858451843262
			 train-loss:  2.1953919607660044 	 ± 0.21746030202070157
	data : 0.11477451324462891
	model : 0.06386294364929199
			 train-loss:  2.1945579587005253 	 ± 0.21743684691498
	data : 0.11478161811828613
	model : 0.06385922431945801
			 train-loss:  2.1941922599194097 	 ± 0.21708833282459092
	data : 0.11489081382751465
	model : 0.0638247013092041
			 train-loss:  2.193134273402393 	 ± 0.21732161329464608
	data : 0.11454887390136718
	model : 0.05536994934082031
#epoch  97    val-loss:  2.4102995270176937  train-loss:  2.193134273402393  lr:  1.9073486328125e-08
			 train-loss:  2.1482346057891846 	 ± 0.0
	data : 5.4523444175720215
	model : 0.07773303985595703
			 train-loss:  2.3705952167510986 	 ± 0.22236061096191406
	data : 2.7928905487060547
	model : 0.0712505578994751
			 train-loss:  2.329325040181478 	 ± 0.19070732153331837
	data : 1.9000476996103923
	model : 0.06981229782104492
			 train-loss:  2.3447543382644653 	 ± 0.1673055635142191
	data : 1.4534045457839966
	model : 0.06853246688842773
			 train-loss:  2.2253148317337037 	 ± 0.2818795915767862
	data : 1.1854939460754395
	model : 0.06773033142089843
			 train-loss:  2.257921278476715 	 ± 0.2674496624954636
	data : 0.11779642105102539
	model : 0.06513862609863282
			 train-loss:  2.2197020394461497 	 ± 0.26471703533093477
	data : 0.11386971473693848
	model : 0.06509785652160645
			 train-loss:  2.201927274465561 	 ± 0.2520462578296007
	data : 0.11370716094970704
	model : 0.06465249061584473
			 train-loss:  2.1850847668117948 	 ± 0.24235941475880654
	data : 0.1137880802154541
	model : 0.06471123695373535
			 train-loss:  2.1469497442245484 	 ± 0.25681276615381915
	data : 0.11383109092712403
	model : 0.0647726058959961
			 train-loss:  2.139098037372936 	 ± 0.24611700684633064
	data : 0.1138537883758545
	model : 0.06481542587280273
			 train-loss:  2.1522355874379477 	 ± 0.2396337415228316
	data : 0.11440730094909668
	model : 0.06513390541076661
			 train-loss:  2.153073916068444 	 ± 0.23025096624169397
	data : 0.11414055824279785
	model : 0.0652036190032959
			 train-loss:  2.1664510113852367 	 ± 0.22705724459403356
	data : 0.11414470672607421
	model : 0.06516146659851074
			 train-loss:  2.1628811995188397 	 ± 0.2197644253038232
	data : 0.11406002044677735
	model : 0.06508336067199708
			 train-loss:  2.1786861270666122 	 ± 0.22141547707117243
	data : 0.11392855644226074
	model : 0.06503438949584961
			 train-loss:  2.217687789131613 	 ± 0.2654789544011673
	data : 0.11342372894287109
	model : 0.06483230590820313
			 train-loss:  2.2215435637368097 	 ± 0.25848851040848886
	data : 0.11378326416015624
	model : 0.06480789184570312
			 train-loss:  2.2463126182556152 	 ± 0.2726587083321713
	data : 0.1137990951538086
	model : 0.06482834815979004
			 train-loss:  2.252785849571228 	 ± 0.26724854427581607
	data : 0.11385650634765625
	model : 0.06490898132324219
			 train-loss:  2.242805446897234 	 ± 0.26459953857674756
	data : 0.11389627456665039
	model : 0.06490578651428222
			 train-loss:  2.2338635379617866 	 ± 0.2617434297061055
	data : 0.11399178504943848
	model : 0.0648381233215332
			 train-loss:  2.2403221855992856 	 ± 0.25777636543797183
	data : 0.11385545730590821
	model : 0.06479625701904297
			 train-loss:  2.2524600128332772 	 ± 0.2589758254871968
	data : 0.11383719444274902
	model : 0.064794921875
			 train-loss:  2.2403690958023073 	 ± 0.2605653577337159
	data : 0.1138824462890625
	model : 0.06482701301574707
			 train-loss:  2.227947335976821 	 ± 0.26294579124192036
	data : 0.11397075653076172
	model : 0.06486043930053711
			 train-loss:  2.228347486919827 	 ± 0.258038550250368
	data : 0.11389837265014649
	model : 0.06491365432739257
			 train-loss:  2.2264443891389027 	 ± 0.25358171328652457
	data : 0.11396713256835937
	model : 0.06492900848388672
			 train-loss:  2.240221829249941 	 ± 0.25961744321716834
	data : 0.11398873329162598
	model : 0.0648427963256836
			 train-loss:  2.239345995585124 	 ± 0.2552973855111786
	data : 0.11386566162109375
	model : 0.06477642059326172
			 train-loss:  2.2391161303366385 	 ± 0.25114908763620797
	data : 0.11385021209716797
	model : 0.06475687026977539
			 train-loss:  2.241642065346241 	 ± 0.2475934858623382
	data : 0.11378540992736816
	model : 0.06468920707702637
			 train-loss:  2.2436050284992564 	 ± 0.24406594368648674
	data : 0.11385297775268555
	model : 0.06470003128051757
			 train-loss:  2.2414369302637436 	 ± 0.2407723008963764
	data : 0.11389222145080566
	model : 0.06475563049316406
			 train-loss:  2.236389126096453 	 ± 0.23912613640433517
	data : 0.11389870643615722
	model : 0.0647702693939209
			 train-loss:  2.228835850954056 	 ± 0.23997866197290313
	data : 0.11389026641845704
	model : 0.06473584175109863
			 train-loss:  2.2250376682023743 	 ± 0.23780795018383502
	data : 0.11386137008666992
	model : 0.06477146148681641
			 train-loss:  2.2277910866235433 	 ± 0.23525497418503402
	data : 0.11382670402526855
	model : 0.06477241516113282
			 train-loss:  2.2263042896221847 	 ± 0.23240009528240102
	data : 0.11379852294921874
	model : 0.0648078441619873
			 train-loss:  2.223373535275459 	 ± 0.23020543586258005
	data : 0.11384806632995606
	model : 0.06485638618469239
			 train-loss:  2.227032984175333 	 ± 0.22855558520575286
	data : 0.11385908126831054
	model : 0.0649035930633545
			 train-loss:  2.231090111391885 	 ± 0.22730766233671384
	data : 0.1140223503112793
	model : 0.06487741470336914
			 train-loss:  2.2312237190645794 	 ± 0.22465067041746198
	data : 0.11398425102233886
	model : 0.06481213569641113
			 train-loss:  2.227739401838996 	 ± 0.22325538182404292
	data : 0.11388359069824219
	model : 0.06481919288635254
			 train-loss:  2.222106268670824 	 ± 0.22390078246027165
	data : 0.1138805866241455
	model : 0.06483097076416015
			 train-loss:  2.219006043413411 	 ± 0.2224280933364443
	data : 0.11389412879943847
	model : 0.06484475135803222
			 train-loss:  2.21617073962029 	 ± 0.2208877645664563
	data : 0.11373653411865234
	model : 0.06487851142883301
			 train-loss:  2.210628919303417 	 ± 0.22185213599295195
	data : 0.1138150691986084
	model : 0.06495122909545899
			 train-loss:  2.2126238662369397 	 ± 0.22001123777178716
	data : 0.11388883590698243
	model : 0.06491827964782715
			 train-loss:  2.213384473323822 	 ± 0.21786508092130447
	data : 0.11376333236694336
	model : 0.06482691764831543
			 train-loss:  2.2179093664767695 	 ± 0.21807850952993002
	data : 0.1137960433959961
	model : 0.06479220390319824
			 train-loss:  2.2120339366105886 	 ± 0.22000956645759648
	data : 0.11374611854553222
	model : 0.06474556922912597
			 train-loss:  2.2082486197633564 	 ± 0.21962698102300626
	data : 0.1139338493347168
	model : 0.06474461555480956
			 train-loss:  2.2104926815739385 	 ± 0.21819635525109016
	data : 0.11391725540161132
	model : 0.06476445198059082
			 train-loss:  2.213403914191506 	 ± 0.21725948615433865
	data : 0.1141127586364746
	model : 0.06485867500305176
			 train-loss:  2.211056206907545 	 ± 0.2160137547148076
	data : 0.11413178443908692
	model : 0.06489315032958984
			 train-loss:  2.209763242487322 	 ± 0.2143290228881071
	data : 0.1142467975616455
	model : 0.0649327278137207
			 train-loss:  2.210482720671029 	 ± 0.21254274864513212
	data : 0.11396956443786621
	model : 0.06484365463256836
			 train-loss:  2.2098929033441057 	 ± 0.21078170975157462
	data : 0.11385231018066407
	model : 0.06479463577270508
			 train-loss:  2.205557811260223 	 ± 0.21165357401235266
	data : 0.11371965408325195
	model : 0.06476793289184571
			 train-loss:  2.202314103235964 	 ± 0.2114099169597592
	data : 0.11362876892089843
	model : 0.06481895446777344
			 train-loss:  2.199236008428758 	 ± 0.2110716330533259
	data : 0.1136622428894043
	model : 0.06483087539672852
			 train-loss:  2.2002735667758517 	 ± 0.20954907903633663
	data : 0.11372938156127929
	model : 0.06494417190551757
			 train-loss:  2.2045537903904915 	 ± 0.2106629768393754
	data : 0.1138524055480957
	model : 0.06499824523925782
			 train-loss:  2.2001998828007623 	 ± 0.21191827288623558
	data : 0.11393804550170898
	model : 0.06495442390441894
			 train-loss:  2.192673045577425 	 ± 0.21888666640029764
	data : 0.11382627487182617
	model : 0.06492242813110352
			 train-loss:  2.1913273316710744 	 ± 0.21752195301409366
	data : 0.11381807327270507
	model : 0.06493873596191406
			 train-loss:  2.189077396603192 	 ± 0.21670059385577617
	data : 0.11386909484863281
	model : 0.06495990753173828
			 train-loss:  2.185851278512374 	 ± 0.21676326488542955
	data : 0.11398587226867676
	model : 0.06499571800231933
			 train-loss:  2.188108571938106 	 ± 0.2160246762292664
	data : 0.11411914825439454
	model : 0.06501836776733398
			 train-loss:  2.1959714671255837 	 ± 0.22435940425838444
	data : 0.11434793472290039
	model : 0.06499218940734863
			 train-loss:  2.1929074029127755 	 ± 0.22428686554854804
	data : 0.11431970596313476
	model : 0.06493921279907226
			 train-loss:  2.189954488244775 	 ± 0.22415020128752772
	data : 0.11424803733825684
	model : 0.06483941078186035
			 train-loss:  2.1894506683220736 	 ± 0.2226721333511675
	data : 0.1141439437866211
	model : 0.06477937698364258
			 train-loss:  2.1902602815628054 	 ± 0.2212922930528295
	data : 0.11396026611328125
	model : 0.06481513977050782
			 train-loss:  2.186721433150141 	 ± 0.22195763513633837
	data : 0.11398701667785645
	model : 0.0648420810699463
			 train-loss:  2.1850984638387505 	 ± 0.2209650885162105
	data : 0.11407465934753418
	model : 0.06489849090576172
			 train-loss:  2.1870097670799646 	 ± 0.22018376111461116
	data : 0.11409978866577149
	model : 0.06496949195861816
			 train-loss:  2.19161531744124 	 ± 0.22253465317971613
	data : 0.11427536010742187
	model : 0.06497273445129395
			 train-loss:  2.189434017241001 	 ± 0.22198769907815905
	data : 0.11426873207092285
	model : 0.06492247581481933
			 train-loss:  2.1874100411379778 	 ± 0.22135464726141804
	data : 0.11409082412719726
	model : 0.06485476493835449
			 train-loss:  2.182501535590102 	 ± 0.22439231784472283
	data : 0.11402349472045899
	model : 0.06489052772521972
			 train-loss:  2.1783789152122406 	 ± 0.22613919311301947
	data : 0.11402983665466308
	model : 0.06495194435119629
			 train-loss:  2.1764818685395375 	 ± 0.22545251620929802
	data : 0.11390972137451172
	model : 0.06503310203552246
			 train-loss:  2.1751236214357266 	 ± 0.22446785271631167
	data : 0.11402969360351563
	model : 0.06508064270019531
			 train-loss:  2.1727938832238665 	 ± 0.2241902944436237
	data : 0.11412153244018555
	model : 0.06510758399963379
			 train-loss:  2.1693678203670457 	 ± 0.2251511317834962
	data : 0.11410775184631347
	model : 0.0650331974029541
			 train-loss:  2.1721958220005035 	 ± 0.22541687064574542
	data : 0.11404428482055665
	model : 0.06490578651428222
			 train-loss:  2.1721027968974598 	 ± 0.2241486050467831
	data : 0.1139915943145752
	model : 0.06479406356811523
			 train-loss:  2.168535754415724 	 ± 0.22542573845715883
	data : 0.11396598815917969
	model : 0.06484322547912598
			 train-loss:  2.171816933286059 	 ± 0.2263344642689266
	data : 0.11398482322692871
	model : 0.06489372253417969
			 train-loss:  2.173287067724311 	 ± 0.22553746743873712
	data : 0.11407999992370606
	model : 0.06491169929504395
			 train-loss:  2.1694327323667464 	 ± 0.22734760504874174
	data : 0.11426777839660644
	model : 0.064990234375
			 train-loss:  2.1716692802753856 	 ± 0.2271613350802034
	data : 0.11434807777404785
	model : 0.06507821083068847
			 train-loss:  2.172713932238127 	 ± 0.22618946113655375
	data : 0.11429424285888672
	model : 0.06502580642700195
			 train-loss:  2.1784789760907493 	 ± 0.23191837497627013
	data : 0.11430692672729492
	model : 0.06494512557983398
			 train-loss:  2.178612143723006 	 ± 0.2307235117323013
	data : 0.11412625312805176
	model : 0.06484675407409668
			 train-loss:  2.1772289787020003 	 ± 0.22994720403132665
	data : 0.11389698982238769
	model : 0.06480231285095214
			 train-loss:  2.175046192275153 	 ± 0.22980109886438668
	data : 0.11387972831726074
	model : 0.06483221054077148
			 train-loss:  2.182185946702957 	 ± 0.2394307683330027
	data : 0.11399011611938477
	model : 0.06482930183410644
			 train-loss:  2.1828792767949623 	 ± 0.23834338360382332
	data : 0.11395630836486817
	model : 0.0648836612701416
			 train-loss:  2.190889565383687 	 ± 0.2504621101330591
	data : 0.1140326976776123
	model : 0.0649488925933838
			 train-loss:  2.193861541238803 	 ± 0.2510441325718908
	data : 0.11420221328735351
	model : 0.06494131088256835
			 train-loss:  2.192693913212189 	 ± 0.25011515387085376
	data : 0.1140963077545166
	model : 0.06489286422729493
			 train-loss:  2.1955496776671635 	 ± 0.250619161771932
	data : 0.11383953094482421
	model : 0.06479339599609375
			 train-loss:  2.193714772755245 	 ± 0.2501418385606488
	data : 0.11392126083374024
	model : 0.06474990844726562
			 train-loss:  2.1966774318819846 	 ± 0.2508317429446864
	data : 0.11396918296813965
	model : 0.0647958755493164
			 train-loss:  2.1991168161233268 	 ± 0.25093966648119237
	data : 0.11388506889343261
	model : 0.06487808227539063
			 train-loss:  2.1975528920462373 	 ± 0.25031411454599817
	data : 0.11400632858276367
	model : 0.06493182182312011
			 train-loss:  2.1989944252100857 	 ± 0.24962782172546963
	data : 0.11423730850219727
	model : 0.06503562927246094
			 train-loss:  2.198333378310676 	 ± 0.24859752548549244
	data : 0.11421394348144531
	model : 0.06506609916687012
			 train-loss:  2.2018302710992947 	 ± 0.2502124581564277
	data : 0.11406002044677735
	model : 0.06499190330505371
			 train-loss:  2.2001816793880633 	 ± 0.24971310760621682
	data : 0.11416282653808593
	model : 0.06493139266967773
			 train-loss:  2.2041840480085004 	 ± 0.25222963731715714
	data : 0.11408524513244629
	model : 0.06486997604370118
			 train-loss:  2.204557782670726 	 ± 0.25116229339330054
	data : 0.11404647827148437
	model : 0.06490192413330079
			 train-loss:  2.2027505081275414 	 ± 0.2508272328405867
	data : 0.11399245262145996
	model : 0.06493449211120605
			 train-loss:  2.2000984931603456 	 ± 0.25138102740891577
	data : 0.11416687965393066
	model : 0.06502881050109863
			 train-loss:  2.1966396307541154 	 ± 0.2530941490041357
	data : 0.1142082691192627
	model : 0.06509623527526856
			 train-loss:  2.1988851683480397 	 ± 0.25320617075891705
	data : 0.11427822113037109
	model : 0.06511564254760742
			 train-loss:  2.200729082028071 	 ± 0.25294997325744717
	data : 0.1142667293548584
	model : 0.06506619453430176
			 train-loss:  2.199426152489402 	 ± 0.2523065857224696
	data : 0.11434826850891114
	model : 0.06503715515136718
			 train-loss:  2.197799291767058 	 ± 0.251906865846213
	data : 0.11424064636230469
	model : 0.06496047973632812
			 train-loss:  2.1959411351661373 	 ± 0.2517188767975528
	data : 0.11410965919494628
	model : 0.06492576599121094
			 train-loss:  2.197749508004035 	 ± 0.2515027674040135
	data : 0.1141359806060791
	model : 0.06492528915405274
			 train-loss:  2.193959700584412 	 ± 0.2540247631043739
	data : 0.11420712471008301
	model : 0.06499991416931153
			 train-loss:  2.190632695243472 	 ± 0.2557343724416351
	data : 0.11413145065307617
	model : 0.06503763198852539
			 train-loss:  2.1915174281503274 	 ± 0.2549190751293642
	data : 0.11419930458068847
	model : 0.06507639884948731
			 train-loss:  2.191541653126478 	 ± 0.25392149173723305
	data : 0.11415743827819824
	model : 0.06502161026000977
			 train-loss:  2.1932647228240967 	 ± 0.2536855082715626
	data : 0.11400876045227051
	model : 0.06491889953613281
			 train-loss:  2.1906805891257064 	 ± 0.25440659920549064
	data : 0.11396961212158203
	model : 0.06489911079406738
			 train-loss:  2.191374007982152 	 ± 0.25355701338000725
	data : 0.11399059295654297
	model : 0.06490445137023926
			 train-loss:  2.1900417885997077 	 ± 0.2530545488829424
	data : 0.1139791488647461
	model : 0.06492462158203124
			 train-loss:  2.193995516103013 	 ± 0.2561611706600678
	data : 0.11411361694335938
	model : 0.06495275497436523
			 train-loss:  2.192267779983691 	 ± 0.2559802127235657
	data : 0.11422419548034668
	model : 0.06503963470458984
			 train-loss:  2.1914598950633297 	 ± 0.25520178556251766
	data : 0.11419868469238281
	model : 0.06498618125915527
			 train-loss:  2.1906784343368866 	 ± 0.2544238811041259
	data : 0.11411385536193848
	model : 0.06495699882507325
			 train-loss:  2.1900657885266046 	 ± 0.2535942902313333
	data : 0.11417069435119628
	model : 0.06490273475646972
			 train-loss:  2.189795602059019 	 ± 0.25269358951262016
	data : 0.11407742500305176
	model : 0.06492629051208496
			 train-loss:  2.192215680218429 	 ± 0.2533829195701086
	data : 0.11411075592041016
	model : 0.06491961479187011
			 train-loss:  2.193543850524085 	 ± 0.25296148484126846
	data : 0.11413226127624512
	model : 0.06491155624389648
			 train-loss:  2.1914248551037296 	 ± 0.25330674350261523
	data : 0.11424045562744141
	model : 0.06492900848388672
			 train-loss:  2.191644488925665 	 ± 0.2524267154900992
	data : 0.11420831680297852
	model : 0.06498045921325683
			 train-loss:  2.1890755191549554 	 ± 0.2533985034730195
	data : 0.11416559219360352
	model : 0.06498336791992188
			 train-loss:  2.1873422298166485 	 ± 0.2533663493130332
	data : 0.11398072242736816
	model : 0.06496720314025879
			 train-loss:  2.188693301431064 	 ± 0.2530111519984402
	data : 0.1140315055847168
	model : 0.06496715545654297
			 train-loss:  2.189686904214833 	 ± 0.2524268949968633
	data : 0.11402974128723145
	model : 0.06498918533325196
			 train-loss:  2.18893026332466 	 ± 0.25173291050595753
	data : 0.11406159400939941
	model : 0.064998197555542
			 train-loss:  2.1895932071917765 	 ± 0.2510097446687081
	data : 0.11424045562744141
	model : 0.06499671936035156
			 train-loss:  2.189794860430212 	 ± 0.250178040322995
	data : 0.11426353454589844
	model : 0.06505293846130371
			 train-loss:  2.1877862882614134 	 ± 0.25054522840308713
	data : 0.11427669525146485
	model : 0.06508259773254395
			 train-loss:  2.186840583157066 	 ± 0.24998270050481508
	data : 0.11412439346313477
	model : 0.06499600410461426
			 train-loss:  2.185591121253214 	 ± 0.24963164394332013
	data : 0.11408085823059082
	model : 0.06495661735534668
			 train-loss:  2.185843974936242 	 ± 0.24883404496705586
	data : 0.11398940086364746
	model : 0.06493558883666992
			 train-loss:  2.1859421521038205 	 ± 0.24802779934759397
	data : 0.11414647102355957
	model : 0.06490511894226074
			 train-loss:  2.1853967935808245 	 ± 0.2473190294695847
	data : 0.11404976844787598
	model : 0.06496644020080566
			 train-loss:  2.186996498933205 	 ± 0.24732824739657197
	data : 0.11425323486328125
	model : 0.06501121520996093
			 train-loss:  2.1856985843864973 	 ± 0.24707171207264547
	data : 0.11426706314086914
	model : 0.06503777503967285
			 train-loss:  2.1838852981977825 	 ± 0.24733437047206985
	data : 0.11430778503417968
	model : 0.06506199836730957
			 train-loss:  2.1845508986299143 	 ± 0.2466972732544563
	data : 0.11423053741455078
	model : 0.06502256393432618
			 train-loss:  2.1862142100930213 	 ± 0.2468178734936841
	data : 0.11426458358764649
	model : 0.06488685607910157
			 train-loss:  2.1850430928402065 	 ± 0.24649569145086409
	data : 0.1141202449798584
	model : 0.06488165855407715
			 train-loss:  2.18420836881355 	 ± 0.24596187191223243
	data : 0.11415305137634277
	model : 0.06489033699035644
			 train-loss:  2.185722144103489 	 ± 0.2459620280445391
	data : 0.11418390274047852
	model : 0.06486940383911133
			 train-loss:  2.1878184959655855 	 ± 0.24666732211827472
	data : 0.11432204246520997
	model : 0.06487135887145996
			 train-loss:  2.1891500249053495 	 ± 0.2465091856312216
	data : 0.11436114311218262
	model : 0.0649195671081543
			 train-loss:  2.188739701207862 	 ± 0.2458220778994395
	data : 0.11442828178405762
	model : 0.06491518020629883
			 train-loss:  2.1908631217693855 	 ± 0.246607231409995
	data : 0.11436643600463867
	model : 0.06483263969421386
			 train-loss:  2.1910789275453206 	 ± 0.24588800159586505
	data : 0.1141467571258545
	model : 0.06471405029296876
			 train-loss:  2.191986438790722 	 ± 0.24544146623552318
	data : 0.11395883560180664
	model : 0.06475090980529785
			 train-loss:  2.1922693483969744 	 ± 0.2447461499196223
	data : 0.11398100852966309
	model : 0.06476740837097168
			 train-loss:  2.189775850340637 	 ± 0.24618562523667017
	data : 0.11401004791259765
	model : 0.06477913856506348
			 train-loss:  2.1916498846785966 	 ± 0.2466891697920658
	data : 0.11412653923034669
	model : 0.06482167243957519
			 train-loss:  2.1901203318138345 	 ± 0.24679177422712179
	data : 0.11427240371704102
	model : 0.06496138572692871
			 train-loss:  2.1925727923711142 	 ± 0.2481867516039798
	data : 0.11444835662841797
	model : 0.06494550704956055
			 train-loss:  2.19049553326198 	 ± 0.2489889428813976
	data : 0.11435637474060059
	model : 0.06489691734313965
			 train-loss:  2.1894034960053186 	 ± 0.24870050662145646
	data : 0.11422758102416992
	model : 0.06482400894165039
			 train-loss:  2.189025158262522 	 ± 0.248047754548567
	data : 0.11418008804321289
	model : 0.06485095024108886
			 train-loss:  2.1891462816281266 	 ± 0.24735525890703913
	data : 0.11417970657348633
	model : 0.06484708786010743
			 train-loss:  2.1896920204162598 	 ± 0.246770793355469
	data : 0.1140662670135498
	model : 0.06486248970031738
			 train-loss:  2.1913514653841655 	 ± 0.2470838661833383
	data : 0.11412372589111328
	model : 0.06489043235778809
			 train-loss:  2.1918179145834062 	 ± 0.24647982710648733
	data : 0.11427884101867676
	model : 0.06495270729064942
			 train-loss:  2.1919025667421113 	 ± 0.24580439039956992
	data : 0.1142995834350586
	model : 0.06494498252868652
			 train-loss:  2.191455101054874 	 ± 0.2452061919196884
	data : 0.11423764228820801
	model : 0.06488056182861328
			 train-loss:  2.191047814877137 	 ± 0.24460102373488982
	data : 0.1141885757446289
	model : 0.06478919982910156
			 train-loss:  2.1930288108619482 	 ± 0.2454146220930956
	data : 0.11419668197631835
	model : 0.06474747657775878
			 train-loss:  2.1946225948231195 	 ± 0.2457121399693623
	data : 0.11412692070007324
	model : 0.06472668647766114
			 train-loss:  2.1974842752364867 	 ± 0.24814268400844477
	data : 0.1141169548034668
	model : 0.06471118927001954
			 train-loss:  2.1977882562799658 	 ± 0.24751675857671587
	data : 0.1142040729522705
	model : 0.06484274864196778
			 train-loss:  2.1985134435078453 	 ± 0.24706125411602062
	data : 0.11426525115966797
	model : 0.06492762565612793
			 train-loss:  2.198653869879873 	 ± 0.2464177976585663
	data : 0.11430258750915527
	model : 0.06495976448059082
			 train-loss:  2.197603494709075 	 ± 0.2461979709518035
	data : 0.11433067321777343
	model : 0.06492691040039063
			 train-loss:  2.195765825609366 	 ± 0.24686587085018075
	data : 0.11413664817810058
	model : 0.06487150192260742
			 train-loss:  2.1949152217627805 	 ± 0.24650742335611883
	data : 0.1142035961151123
	model : 0.06477365493774415
			 train-loss:  2.194781171906855 	 ± 0.24587832665341106
	data : 0.11430573463439941
	model : 0.06476259231567383
			 train-loss:  2.194304911295573 	 ± 0.2453367560325776
	data : 0.11426348686218261
	model : 0.06481690406799316
			 train-loss:  2.1937813028997306 	 ± 0.24481930829407714
	data : 0.11427531242370606
	model : 0.06488542556762696
			 train-loss:  2.1939466144832864 	 ± 0.24420811580465668
	data : 0.11441507339477539
	model : 0.06495332717895508
			 train-loss:  2.193590019688462 	 ± 0.24364206183567572
	data : 0.11439633369445801
	model : 0.06494956016540528
			 train-loss:  2.19456016837652 	 ± 0.24341222411511995
	data : 0.11441388130187988
	model : 0.06494226455688476
			 train-loss:  2.19568190574646 	 ± 0.24331803045627928
	data : 0.11438097953796386
	model : 0.0648190975189209
			 train-loss:  2.194509957560259 	 ± 0.243277230466774
	data : 0.11439261436462403
	model : 0.06475186347961426
			 train-loss:  2.195911196788939 	 ± 0.243486097031159
	data : 0.11431007385253907
	model : 0.06476664543151855
			 train-loss:  2.1953488353437978 	 ± 0.2430171091311631
	data : 0.11434216499328613
	model : 0.06477475166320801
			 train-loss:  2.1959617576178383 	 ± 0.24257798838855074
	data : 0.11432738304138183
	model : 0.06479244232177735
			 train-loss:  2.1976201272592313 	 ± 0.24314208785959585
	data : 0.11433796882629395
	model : 0.06493096351623535
			 train-loss:  2.1979540418652657 	 ± 0.2425983331152687
	data : 0.11438651084899902
	model : 0.06501555442810059
			 train-loss:  2.197454603973794 	 ± 0.242117774845346
	data : 0.11458721160888671
	model : 0.06512312889099121
			 train-loss:  2.1973758620711474 	 ± 0.24153771660657572
	data : 0.11442646980285645
	model : 0.06511449813842773
			 train-loss:  2.1976319232055443 	 ± 0.24098748005871332
	data : 0.11424989700317383
	model : 0.06507182121276855
			 train-loss:  2.1979641908691043 	 ± 0.24046099905457444
	data : 0.11421504020690917
	model : 0.06497540473937988
			 train-loss:  2.198112208131365 	 ± 0.23990009894117476
	data : 0.11410083770751953
	model : 0.06494979858398438
			 train-loss:  2.198310223952779 	 ± 0.23935091156877766
	data : 0.11391558647155761
	model : 0.06484951972961425
			 train-loss:  2.1998832108269277 	 ± 0.2398842347546031
	data : 0.11393566131591797
	model : 0.06488590240478516
			 train-loss:  2.1990452185969485 	 ± 0.2396353938825495
	data : 0.11408848762512207
	model : 0.06498179435729981
			 train-loss:  2.1982119222019993 	 ± 0.23938802465297623
	data : 0.11418652534484863
	model : 0.06504364013671875
			 train-loss:  2.1989119201898575 	 ± 0.23905369122079825
	data : 0.11438460350036621
	model : 0.06504712104797364
			 train-loss:  2.1990975207447456 	 ± 0.23851783845944832
	data : 0.11440701484680176
	model : 0.0650421142578125
			 train-loss:  2.1985053266953987 	 ± 0.23812999187552486
	data : 0.11443548202514649
	model : 0.06492381095886231
			 train-loss:  2.1972666717555427 	 ± 0.23828854780144018
	data : 0.11435794830322266
	model : 0.06485238075256347
			 train-loss:  2.1977832279422067 	 ± 0.23786922968189628
	data : 0.11438570022583008
	model : 0.06482186317443847
			 train-loss:  2.1970395734407244 	 ± 0.23758663506768357
	data : 0.11422963142395019
	model : 0.06481075286865234
			 train-loss:  2.197374034035313 	 ± 0.2371030650866534
	data : 0.11433272361755371
	model : 0.06473093032836914
			 train-loss:  2.196853660681857 	 ± 0.23669786746355567
	data : 0.11444358825683594
	model : 0.06476097106933594
			 train-loss:  2.197215656616858 	 ± 0.23623079198047098
	data : 0.11458992958068848
	model : 0.06470494270324707
			 train-loss:  2.1952109861373903 	 ± 0.23760714330743898
	data : 0.11454615592956544
	model : 0.06457500457763672
			 train-loss:  2.1956572970457837 	 ± 0.23717538374253722
	data : 0.1147397518157959
	model : 0.06434946060180664
			 train-loss:  2.1945928603016855 	 ± 0.2371927898378406
	data : 0.11479272842407226
	model : 0.06422295570373535
			 train-loss:  2.193483891194327 	 ± 0.23726110249714605
	data : 0.11479582786560058
	model : 0.06409811973571777
			 train-loss:  2.1983440411663473 	 ± 0.24785603019420485
	data : 0.11492528915405273
	model : 0.06393346786499024
			 train-loss:  2.199419191609258 	 ± 0.24785121664544213
	data : 0.11504182815551758
	model : 0.06388883590698242
			 train-loss:  2.1991673062890125 	 ± 0.24734366074239997
	data : 0.1149604320526123
	model : 0.06395268440246582
			 train-loss:  2.1975584708411118 	 ± 0.24801833321059802
	data : 0.11501078605651856
	model : 0.0639571189880371
			 train-loss:  2.198948199145272 	 ± 0.24838913200690035
	data : 0.11495018005371094
	model : 0.06390137672424316
			 train-loss:  2.1975137983631883 	 ± 0.2488230226969049
	data : 0.11489205360412598
	model : 0.06393837928771973
			 train-loss:  2.1970784664154053 	 ± 0.248382333798457
	data : 0.1148369312286377
	model : 0.06394977569580078
			 train-loss:  2.19736301494857 	 ± 0.24789392258421547
	data : 0.11489858627319335
	model : 0.06389389038085938
			 train-loss:  2.197489248549385 	 ± 0.24737798790808685
	data : 0.11488580703735352
	model : 0.0638875961303711
			 train-loss:  2.1977993480297697 	 ± 0.24690389572030005
	data : 0.11504034996032715
	model : 0.0639193058013916
			 train-loss:  2.197292048562022 	 ± 0.24651108392444107
	data : 0.11498780250549316
	model : 0.06394233703613281
			 train-loss:  2.197347867488861 	 ± 0.245998496645947
	data : 0.11490988731384277
	model : 0.06388392448425292
			 train-loss:  2.1981700752780644 	 ± 0.24581783060302026
	data : 0.11476325988769531
	model : 0.06385130882263183
			 train-loss:  2.196818768978119 	 ± 0.24620475801878047
	data : 0.11468334197998047
	model : 0.06388726234436035
			 train-loss:  2.196617609679454 	 ± 0.24571756883681742
	data : 0.11467833518981933
	model : 0.06392936706542969
			 train-loss:  2.196606524166514 	 ± 0.2452135931570528
	data : 0.1146812915802002
	model : 0.0639317512512207
			 train-loss:  2.19701007774898 	 ± 0.24479382271001632
	data : 0.11487278938293458
	model : 0.06399240493774414
			 train-loss:  2.196507185939851 	 ± 0.24442254932707574
	data : 0.1148526668548584
	model : 0.06402907371520997
			 train-loss:  2.1963860375678492 	 ± 0.24393466573948275
	data : 0.11486468315124512
	model : 0.06394281387329101
			 train-loss:  2.196263129192014 	 ± 0.24345002870170807
	data : 0.1147700309753418
	model : 0.06390233039855957
			 train-loss:  2.195477095473722 	 ± 0.24327580904701462
	data : 0.11476645469665528
	model : 0.06394143104553222
			 train-loss:  2.195301320552826 	 ± 0.24280461301073095
	data : 0.11484417915344239
	model : 0.06394004821777344
			 train-loss:  2.194271441949791 	 ± 0.24286697134520666
	data : 0.11498222351074219
	model : 0.063991117477417
			 train-loss:  2.1940575828627935 	 ± 0.24240829294774982
	data : 0.11513710021972656
	model : 0.06402387619018554
			 train-loss:  2.1944863391016782 	 ± 0.2420244742372688
	data : 0.1151092529296875
	model : 0.06400761604309083
			 train-loss:  2.1957814355534833 	 ± 0.24242438633873847
	data : 0.1150660514831543
	model : 0.0638840675354004
			 train-loss:  2.1961394225849826 	 ± 0.24201583715937353
	data : 0.11477899551391602
	model : 0.06386146545410157
			 train-loss:  2.2000814378261566 	 ± 0.24961057335189904
	data : 0.11453857421875
	model : 0.055440759658813475
#epoch  98    val-loss:  2.403963415246261  train-loss:  2.2000814378261566  lr:  1.9073486328125e-08
			 train-loss:  2.2455906867980957 	 ± 0.0
	data : 5.6382222175598145
	model : 0.07462811470031738
			 train-loss:  2.352785587310791 	 ± 0.10719490051269531
	data : 2.8807719945907593
	model : 0.0696113109588623
			 train-loss:  2.4488898118336997 	 ± 0.1616556269963519
	data : 1.9585875670115154
	model : 0.06781578063964844
			 train-loss:  2.3640552759170532 	 ± 0.2029534472662783
	data : 1.4975113272666931
	model : 0.06706184148788452
			 train-loss:  2.3568717002868653 	 ± 0.18209474578128643
	data : 1.2208106994628907
	model : 0.06656994819641113
			 train-loss:  2.3154392639795938 	 ± 0.19030321662916413
	data : 0.11609930992126465
	model : 0.0646897792816162
			 train-loss:  2.2381142377853394 	 ± 0.25868253901701044
	data : 0.11435041427612305
	model : 0.06477465629577636
			 train-loss:  2.2006781846284866 	 ± 0.2614618146594071
	data : 0.11432576179504395
	model : 0.06489348411560059
			 train-loss:  2.200061493449741 	 ± 0.24651473392446258
	data : 0.11412382125854492
	model : 0.064827299118042
			 train-loss:  2.185073173046112 	 ± 0.23814787501542034
	data : 0.11399698257446289
	model : 0.06487526893615722
			 train-loss:  2.186390085653825 	 ± 0.22710327490801535
	data : 0.11390566825866699
	model : 0.06486334800720214
			 train-loss:  2.155814051628113 	 ± 0.23992027517894404
	data : 0.11377644538879395
	model : 0.06490545272827149
			 train-loss:  2.1389620029009304 	 ± 0.23778522023789603
	data : 0.11381936073303223
	model : 0.06494703292846679
			 train-loss:  2.1461994222232272 	 ± 0.23061668040339006
	data : 0.11406178474426269
	model : 0.06505770683288574
			 train-loss:  2.1604511976242065 	 ± 0.22908957452791276
	data : 0.11418561935424805
	model : 0.06505846977233887
			 train-loss:  2.1494269892573357 	 ± 0.22588692666196208
	data : 0.11415390968322754
	model : 0.06494283676147461
			 train-loss:  2.1353479273178997 	 ± 0.2262630294115395
	data : 0.11411733627319336
	model : 0.0648686408996582
			 train-loss:  2.184220274289449 	 ± 0.29825392066693557
	data : 0.11414580345153809
	model : 0.06483926773071289
			 train-loss:  2.1885533207341243 	 ± 0.29088054802505253
	data : 0.11409163475036621
	model : 0.06481733322143554
			 train-loss:  2.192331516742706 	 ± 0.28399320255050675
	data : 0.11427435874938965
	model : 0.0648496150970459
			 train-loss:  2.2008622941516696 	 ± 0.27976247837546214
	data : 0.11433563232421876
	model : 0.06494455337524414
			 train-loss:  2.1983752359043467 	 ± 0.2735678088097766
	data : 0.11440534591674804
	model : 0.06504802703857422
			 train-loss:  2.186589028524316 	 ± 0.2732061208400364
	data : 0.11426506042480469
	model : 0.06505546569824219
			 train-loss:  2.1840120206276574 	 ± 0.26773916551038096
	data : 0.11418499946594238
	model : 0.06501760482788085
			 train-loss:  2.2133595514297486 	 ± 0.2991447005356766
	data : 0.11388421058654785
	model : 0.06490764617919922
			 train-loss:  2.22295232461049 	 ± 0.29723097983448965
	data : 0.11372818946838378
	model : 0.06489329338073731
			 train-loss:  2.234139835392987 	 ± 0.2972008448444326
	data : 0.11366381645202636
	model : 0.06480002403259277
			 train-loss:  2.2291310727596283 	 ± 0.2930036288363353
	data : 0.11380906105041504
	model : 0.06481328010559081
			 train-loss:  2.2305340150306963 	 ± 0.2880032183065068
	data : 0.11385817527770996
	model : 0.06486496925354004
			 train-loss:  2.230286117394765 	 ± 0.283165630165411
	data : 0.11410102844238282
	model : 0.06494054794311524
			 train-loss:  2.2262605582514117 	 ± 0.2794322579790109
	data : 0.11417570114135742
	model : 0.06487197875976562
			 train-loss:  2.2192347832024097 	 ± 0.27779942256597023
	data : 0.11421055793762207
	model : 0.06487827301025391
			 train-loss:  2.2204894333174736 	 ± 0.2736500149679041
	data : 0.11419577598571777
	model : 0.06487054824829101
			 train-loss:  2.2361755546401527 	 ± 0.28425630274175007
	data : 0.11421470642089844
	model : 0.06485190391540527
			 train-loss:  2.221829778807504 	 ± 0.2923871934019906
	data : 0.11410040855407715
	model : 0.06490936279296874
			 train-loss:  2.2172950870460935 	 ± 0.28954319392314243
	data : 0.11422309875488282
	model : 0.06500649452209473
			 train-loss:  2.208864740423254 	 ± 0.2900482547247867
	data : 0.11430897712707519
	model : 0.0650148868560791
			 train-loss:  2.212873051041051 	 ± 0.2872430296660321
	data : 0.1142423152923584
	model : 0.06497416496276856
			 train-loss:  2.2194014390309653 	 ± 0.2863782609215785
	data : 0.11405372619628906
	model : 0.06493430137634278
			 train-loss:  2.223934179544449 	 ± 0.2841891612046393
	data : 0.11409311294555664
	model : 0.06482858657836914
			 train-loss:  2.224830697222454 	 ± 0.280759306425294
	data : 0.1139826774597168
	model : 0.06480398178100585
			 train-loss:  2.2205227670215426 	 ± 0.2787649053091801
	data : 0.11392955780029297
	model : 0.06480379104614258
			 train-loss:  2.219874858856201 	 ± 0.27553638065783154
	data : 0.11406211853027344
	model : 0.06486945152282715
			 train-loss:  2.2154462987726387 	 ± 0.27393093651293327
	data : 0.1143500804901123
	model : 0.06492538452148437
			 train-loss:  2.2191554175482855 	 ± 0.27198524976392036
	data : 0.11436104774475098
	model : 0.06499757766723632
			 train-loss:  2.216078343598739 	 ± 0.26980340987466034
	data : 0.11446576118469239
	model : 0.06495165824890137
			 train-loss:  2.213743793203476 	 ± 0.2673869476959972
	data : 0.1144066333770752
	model : 0.06486344337463379
			 train-loss:  2.2162928034861884 	 ± 0.2651634678769389
	data : 0.11430392265319825
	model : 0.06481633186340333
			 train-loss:  2.2151530275539475 	 ± 0.2625625431099733
	data : 0.11414151191711426
	model : 0.06476478576660157
			 train-loss:  2.2186824655532837 	 ± 0.26109518754748384
	data : 0.11413745880126953
	model : 0.0647475242614746
			 train-loss:  2.2192229803870704 	 ± 0.25855100936212766
	data : 0.11401443481445313
	model : 0.06479225158691407
			 train-loss:  2.2145322882212124 	 ± 0.25823478893222374
	data : 0.11413841247558594
	model : 0.06488666534423829
			 train-loss:  2.212199678960836 	 ± 0.25633948234100923
	data : 0.11425089836120605
	model : 0.06489315032958984
			 train-loss:  2.2167459196514554 	 ± 0.2561025181086152
	data : 0.11436738967895507
	model : 0.06492919921875
			 train-loss:  2.216714334487915 	 ± 0.253763739480929
	data : 0.11427254676818847
	model : 0.06490812301635743
			 train-loss:  2.2159738540649414 	 ± 0.251547735806849
	data : 0.11420960426330566
	model : 0.0648198127746582
			 train-loss:  2.212022195782578 	 ± 0.2510789264029086
	data : 0.11411781311035156
	model : 0.06484651565551758
			 train-loss:  2.2070412676909874 	 ± 0.25172975521763963
	data : 0.11407756805419922
	model : 0.06490836143493653
			 train-loss:  2.207224583221694 	 ± 0.24959123990758922
	data : 0.11405296325683593
	model : 0.06493611335754394
			 train-loss:  2.2096635222435 	 ± 0.24821055761363317
	data : 0.11422195434570312
	model : 0.06498799324035645
			 train-loss:  2.209566487640631 	 ± 0.24616878509839632
	data : 0.11424942016601562
	model : 0.06508860588073731
			 train-loss:  2.2040593239568893 	 ± 0.24793492362308653
	data : 0.11435408592224121
	model : 0.06502699851989746
			 train-loss:  2.2085855347769603 	 ± 0.2485279720592792
	data : 0.1142542839050293
	model : 0.06496167182922363
			 train-loss:  2.207202158868313 	 ± 0.24682305742756466
	data : 0.11423468589782715
	model : 0.06487689018249512
			 train-loss:  2.2056794570042535 	 ± 0.2452198150673091
	data : 0.11419615745544434
	model : 0.06480731964111328
			 train-loss:  2.2007529627193105 	 ± 0.24657499426807056
	data : 0.11432895660400391
	model : 0.06478390693664551
			 train-loss:  2.199187456671871 	 ± 0.24505821766125677
	data : 0.11433215141296386
	model : 0.06482434272766113
			 train-loss:  2.197024741593529 	 ± 0.24389295078331405
	data : 0.11443881988525391
	model : 0.0648576259613037
			 train-loss:  2.1954361113949097 	 ± 0.24247330269423426
	data : 0.11456160545349121
	model : 0.06497912406921387
			 train-loss:  2.19419344493321 	 ± 0.24095632254261484
	data : 0.11452240943908691
	model : 0.06504006385803222
			 train-loss:  2.201615595481765 	 ± 0.24718087693229657
	data : 0.11442465782165527
	model : 0.06505546569824219
			 train-loss:  2.1995785501268177 	 ± 0.24605774839642364
	data : 0.11430730819702148
	model : 0.06502408981323242
			 train-loss:  2.2007330084500247 	 ± 0.24456287477852356
	data : 0.11416168212890625
	model : 0.06493053436279297
			 train-loss:  2.201055143330548 	 ± 0.24292039486485062
	data : 0.11407427787780762
	model : 0.06484375
			 train-loss:  2.196488954226176 	 ± 0.24447171643240745
	data : 0.11407985687255859
	model : 0.0648411750793457
			 train-loss:  2.194056863847532 	 ± 0.24376966513742
	data : 0.11415772438049317
	model : 0.06482973098754882
			 train-loss:  2.193070066439641 	 ± 0.24233431532703842
	data : 0.114292573928833
	model : 0.06485958099365234
			 train-loss:  2.195512048709087 	 ± 0.24172752497380612
	data : 0.11443405151367188
	model : 0.06491584777832031
			 train-loss:  2.196047141582151 	 ± 0.2402392174000971
	data : 0.11437754631042481
	model : 0.06497778892517089
			 train-loss:  2.1924206003546716 	 ± 0.2408992275994105
	data : 0.11432695388793945
	model : 0.06495194435119629
			 train-loss:  2.195207479559345 	 ± 0.2407017317428548
	data : 0.11425213813781739
	model : 0.06486096382141113
			 train-loss:  2.1920885298310258 	 ± 0.240870767376263
	data : 0.11402325630187989
	model : 0.06488447189331055
			 train-loss:  2.1926747962652917 	 ± 0.23947419455449437
	data : 0.11407980918884278
	model : 0.06489763259887696
			 train-loss:  2.1950117079984572 	 ± 0.23899467121311274
	data : 0.11413922309875488
	model : 0.06489400863647461
			 train-loss:  2.2000809206682095 	 ± 0.24208472092528316
	data : 0.11415886878967285
	model : 0.06491227149963379
			 train-loss:  2.2017340757126034 	 ± 0.24115525458178855
	data : 0.11421918869018555
	model : 0.06502175331115723
			 train-loss:  2.2004354972948974 	 ± 0.24006753424010255
	data : 0.11441192626953126
	model : 0.064996337890625
			 train-loss:  2.200672186233781 	 ± 0.23870982594993148
	data : 0.11427421569824218
	model : 0.06495857238769531
			 train-loss:  2.197969798291667 	 ± 0.2387148608381381
	data : 0.11410813331604004
	model : 0.06484417915344239
			 train-loss:  2.1951787458525764 	 ± 0.23884080089143733
	data : 0.11408400535583496
	model : 0.06484799385070801
			 train-loss:  2.192858888552739 	 ± 0.23854227557851335
	data : 0.11403307914733887
	model : 0.06484718322753906
			 train-loss:  2.192641793385796 	 ± 0.23725134680613566
	data : 0.11401886940002441
	model : 0.06483912467956543
			 train-loss:  2.18950496181365 	 ± 0.23788275602673764
	data : 0.11413154602050782
	model : 0.06494855880737305
			 train-loss:  2.187444840339904 	 ± 0.23744663563153884
	data : 0.11441235542297364
	model : 0.06504111289978028
			 train-loss:  2.185613593302275 	 ± 0.2368599741231268
	data : 0.11445927619934082
	model : 0.06507477760314942
			 train-loss:  2.185174056639274 	 ± 0.2356620420232219
	data : 0.11447005271911621
	model : 0.06500682830810547
			 train-loss:  2.183703519634365 	 ± 0.23488647001170684
	data : 0.11430296897888184
	model : 0.06495094299316406
			 train-loss:  2.1838725890432085 	 ± 0.2336909292816859
	data : 0.11425495147705078
	model : 0.06484909057617187
			 train-loss:  2.1855624938252 	 ± 0.23310874260096656
	data : 0.11407995223999023
	model : 0.06484718322753906
			 train-loss:  2.1830812215805055 	 ± 0.23325051661574167
	data : 0.11411776542663574
	model : 0.06483235359191894
			 train-loss:  2.1842740738745965 	 ± 0.2323992724585536
	data : 0.11414585113525391
	model : 0.06491341590881347
			 train-loss:  2.1831896608951045 	 ± 0.2315139062909723
	data : 0.11444745063781739
	model : 0.06498775482177735
			 train-loss:  2.1832038430334295 	 ± 0.2303873557776103
	data : 0.1143735408782959
	model : 0.06502742767333984
			 train-loss:  2.1821475395789514 	 ± 0.229527536776455
	data : 0.11446261405944824
	model : 0.06497354507446289
			 train-loss:  2.180658401761736 	 ± 0.2289361736323972
	data : 0.11432795524597168
	model : 0.064884614944458
			 train-loss:  2.1804357362243363 	 ± 0.22786515050761452
	data : 0.11433019638061523
	model : 0.0648390769958496
			 train-loss:  2.1806591158715363 	 ± 0.22680952094957468
	data : 0.11403470039367676
	model : 0.06483211517333984
			 train-loss:  2.1843809993178755 	 ± 0.22901626002989028
	data : 0.11395635604858398
	model : 0.06486873626708985
			 train-loss:  2.184395903841071 	 ± 0.22796335875221008
	data : 0.11385011672973633
	model : 0.06494660377502441
			 train-loss:  2.1866346229206433 	 ± 0.22812530784112506
	data : 0.11405358314514161
	model : 0.06504688262939454
			 train-loss:  2.1862696419965038 	 ± 0.22712765144718525
	data : 0.11395325660705566
	model : 0.06505537033081055
			 train-loss:  2.187361038156918 	 ± 0.2264035981699215
	data : 0.11401324272155762
	model : 0.06497492790222167
			 train-loss:  2.1869155601062604 	 ± 0.2254488853549186
	data : 0.11401348114013672
	model : 0.06488599777221679
			 train-loss:  2.184794891298863 	 ± 0.225587089538721
	data : 0.11402997970581055
	model : 0.06484980583190918
			 train-loss:  2.1855861031490824 	 ± 0.22476294871946614
	data : 0.11392149925231934
	model : 0.06483626365661621
			 train-loss:  2.1854333723413535 	 ± 0.22379803927825956
	data : 0.11405129432678222
	model : 0.0648284912109375
			 train-loss:  2.1849342971785455 	 ± 0.22290440433762654
	data : 0.1141054630279541
	model : 0.06491832733154297
			 train-loss:  2.184646225581735 	 ± 0.2219797553497336
	data : 0.11430773735046387
	model : 0.06500077247619629
			 train-loss:  2.1832773855754306 	 ± 0.22154465772267704
	data : 0.11429920196533203
	model : 0.06508350372314453
			 train-loss:  2.1839010963837304 	 ± 0.22072451429234977
	data : 0.11421456336975097
	model : 0.06510205268859863
			 train-loss:  2.1882872532221898 	 ± 0.2250006153133022
	data : 0.11401433944702148
	model : 0.06506357192993165
			 train-loss:  2.1870419207166454 	 ± 0.22449491937311863
	data : 0.11391515731811523
	model : 0.06493053436279297
			 train-loss:  2.1869768020583362 	 ± 0.22358163290852806
	data : 0.1136998176574707
	model : 0.06488614082336426
			 train-loss:  2.18807124226324 	 ± 0.22300883579483322
	data : 0.11378173828125
	model : 0.06481785774230957
			 train-loss:  2.1859778156280516 	 ± 0.22333494859820754
	data : 0.11387577056884765
	model : 0.06480994224548339
			 train-loss:  2.1893006544264537 	 ± 0.22552781506968028
	data : 0.1141726016998291
	model : 0.06485052108764648
			 train-loss:  2.187554217699006 	 ± 0.2254919202875927
	data : 0.11419091224670411
	model : 0.06501832008361816
			 train-loss:  2.1872231708839536 	 ± 0.224640346304754
	data : 0.11429691314697266
	model : 0.06502065658569336
			 train-loss:  2.186327480530554 	 ± 0.22399729097786705
	data : 0.11414961814880371
	model : 0.06494884490966797
			 train-loss:  2.185247917358692 	 ± 0.22347073718303231
	data : 0.11407465934753418
	model : 0.06487717628479003
			 train-loss:  2.186477802181972 	 ± 0.22305738164044384
	data : 0.11384921073913574
	model : 0.06488285064697266
			 train-loss:  2.1839809959585015 	 ± 0.22404090287952008
	data : 0.1139908790588379
	model : 0.06483020782470703
			 train-loss:  2.1831894476610914 	 ± 0.22338225006436294
	data : 0.11402945518493653
	model : 0.06484708786010743
			 train-loss:  2.1850162904653976 	 ± 0.2235421940349897
	data : 0.11404876708984375
	model : 0.06492471694946289
			 train-loss:  2.1911628935072156 	 ± 0.2338024247196309
	data : 0.11417655944824219
	model : 0.06493406295776367
			 train-loss:  2.190840307404013 	 ± 0.2329714233640763
	data : 0.11424622535705567
	model : 0.06486964225769043
			 train-loss:  2.192625251129596 	 ± 0.23305108990452214
	data : 0.11403694152832031
	model : 0.06474928855895996
			 train-loss:  2.1936578197755674 	 ± 0.23251947991996055
	data : 0.11390976905822754
	model : 0.06473126411437988
			 train-loss:  2.1916736570193613 	 ± 0.23285111539852701
	data : 0.11392650604248047
	model : 0.06476573944091797
			 train-loss:  2.1901880460126057 	 ± 0.23267818409644414
	data : 0.1138601303100586
	model : 0.06479048728942871
			 train-loss:  2.1919519723729883 	 ± 0.23278911682023745
	data : 0.1139833927154541
	model : 0.06484379768371581
			 train-loss:  2.1923100469817576 	 ± 0.23200695335613608
	data : 0.11412858963012695
	model : 0.0649385929107666
			 train-loss:  2.193553243483697 	 ± 0.23166846692192128
	data : 0.11421723365783691
	model : 0.06489129066467285
			 train-loss:  2.1940906966725984 	 ± 0.2309521047666842
	data : 0.1141465187072754
	model : 0.06479024887084961
			 train-loss:  2.193390653051179 	 ± 0.23030759688352503
	data : 0.11408624649047852
	model : 0.06475772857666015
			 train-loss:  2.191733418262168 	 ± 0.2303834251034089
	data : 0.11384201049804688
	model : 0.0648085594177246
			 train-loss:  2.189650576941821 	 ± 0.23097367613224004
	data : 0.11393990516662597
	model : 0.06486358642578124
			 train-loss:  2.188834837159595 	 ± 0.2304044104436487
	data : 0.11397776603698731
	model : 0.0649193286895752
			 train-loss:  2.190185502871571 	 ± 0.23021708291487317
	data : 0.11408143043518067
	model : 0.06498398780822753
			 train-loss:  2.1888271673520405 	 ± 0.23004671003128999
	data : 0.11415457725524902
	model : 0.06505141258239747
			 train-loss:  2.188037936261158 	 ± 0.22948735960332584
	data : 0.1142946720123291
	model : 0.06497578620910645
			 train-loss:  2.1865739249869396 	 ± 0.22943760429659696
	data : 0.11408371925354004
	model : 0.06482410430908203
			 train-loss:  2.1877459131814296 	 ± 0.22914260186374275
	data : 0.11402420997619629
	model : 0.0647885799407959
			 train-loss:  2.1871568018739875 	 ± 0.22851363363582225
	data : 0.11397852897644042
	model : 0.06476283073425293
			 train-loss:  2.1885525695739254 	 ± 0.22843293353613112
	data : 0.1139291763305664
	model : 0.06472530364990234
			 train-loss:  2.189263090873376 	 ± 0.22787136269244146
	data : 0.11394748687744141
	model : 0.06474180221557617
			 train-loss:  2.189541787098927 	 ± 0.22717116881071928
	data : 0.11399345397949219
	model : 0.06486821174621582
			 train-loss:  2.1894847011264367 	 ± 0.22645226127244278
	data : 0.11401495933532715
	model : 0.06492958068847657
			 train-loss:  2.188366112469127 	 ± 0.22617648636262416
	data : 0.11402645111083984
	model : 0.06488213539123536
			 train-loss:  2.1922739155590536 	 ± 0.23079028638148644
	data : 0.1140334129333496
	model : 0.06478605270385743
			 train-loss:  2.191397866106922 	 ± 0.23033913484684157
	data : 0.11393342018127442
	model : 0.06482253074645997
			 train-loss:  2.189004722200794 	 ± 0.23162616286873958
	data : 0.11408724784851074
	model : 0.0648186206817627
			 train-loss:  2.189281465817083 	 ± 0.23094142363830036
	data : 0.11407194137573243
	model : 0.06479287147521973
			 train-loss:  2.1891497729755027 	 ± 0.23024239640773767
	data : 0.11421728134155273
	model : 0.06487836837768554
			 train-loss:  2.188826787110531 	 ± 0.22958089505353585
	data : 0.11432280540466308
	model : 0.06496152877807618
			 train-loss:  2.1895446037671653 	 ± 0.22907398589971628
	data : 0.11440563201904297
	model : 0.06492409706115723
			 train-loss:  2.1886793517780876 	 ± 0.22865902089893633
	data : 0.11436395645141602
	model : 0.06488456726074218
			 train-loss:  2.191414971436773 	 ± 0.23070217077866234
	data : 0.11427726745605468
	model : 0.06477861404418946
			 train-loss:  2.1904214761666294 	 ± 0.23037877770775422
	data : 0.11414275169372559
	model : 0.06473908424377442
			 train-loss:  2.1907388371579786 	 ± 0.2297372419035748
	data : 0.11413159370422363
	model : 0.06476173400878907
			 train-loss:  2.1913006954025804 	 ± 0.22918162321617772
	data : 0.1141505241394043
	model : 0.06478848457336425
			 train-loss:  2.1923631547495375 	 ± 0.22893639130871302
	data : 0.11420841217041015
	model : 0.0648193359375
			 train-loss:  2.1949454835384565 	 ± 0.2307723576418026
	data : 0.11436653137207031
	model : 0.06492295265197753
			 train-loss:  2.193977415561676 	 ± 0.23046028006964056
	data : 0.11439542770385742
	model : 0.06495161056518554
			 train-loss:  2.1931266941343037 	 ± 0.23007471041948954
	data : 0.11431589126586914
	model : 0.06493916511535644
			 train-loss:  2.1940561797131193 	 ± 0.22974942654427968
	data : 0.1142080307006836
	model : 0.06483912467956543
			 train-loss:  2.1944149544010054 	 ± 0.22914893494568903
	data : 0.11398429870605468
	model : 0.06488194465637206
			 train-loss:  2.1942978581685697 	 ± 0.22850966200214787
	data : 0.11389851570129395
	model : 0.06492733955383301
			 train-loss:  2.1930586925432003 	 ± 0.22846942248388175
	data : 0.11388130187988281
	model : 0.06499409675598145
			 train-loss:  2.195557545953327 	 ± 0.23027377319111553
	data : 0.11386103630065918
	model : 0.06503605842590332
			 train-loss:  2.1954870638926387 	 ± 0.22963872370484997
	data : 0.11401252746582032
	model : 0.06515507698059082
			 train-loss:  2.1947764649495975 	 ± 0.2292064409831648
	data : 0.11413636207580566
	model : 0.06515645980834961
			 train-loss:  2.196740324379968 	 ± 0.23010962654826175
	data : 0.1141209602355957
	model : 0.06508021354675293
			 train-loss:  2.196035529608312 	 ± 0.22968145054865194
	data : 0.11402077674865722
	model : 0.06492180824279785
			 train-loss:  2.1948121715236355 	 ± 0.22966016115382595
	data : 0.11388621330261231
	model : 0.06489181518554688
			 train-loss:  2.1951921639903897 	 ± 0.22910027018891352
	data : 0.11383323669433594
	model : 0.06486258506774903
			 train-loss:  2.193106794102307 	 ± 0.23025013741211664
	data : 0.1138272762298584
	model : 0.06484508514404297
			 train-loss:  2.1931484240166683 	 ± 0.2296376591692246
	data : 0.11392421722412109
	model : 0.06488590240478516
			 train-loss:  2.1953048151006143 	 ± 0.2309299585441516
	data : 0.11402225494384766
	model : 0.06496329307556152
			 train-loss:  2.1963243559787147 	 ± 0.23074754016260332
	data : 0.11422085762023926
	model : 0.0649184226989746
			 train-loss:  2.194713658687332 	 ± 0.23121112938461524
	data : 0.11408348083496093
	model : 0.06484346389770508
			 train-loss:  2.193105290954312 	 ± 0.23167702507176974
	data : 0.11412515640258789
	model : 0.06479115486145019
			 train-loss:  2.19159779338639 	 ± 0.2320182502462091
	data : 0.11405625343322753
	model : 0.06481599807739258
			 train-loss:  2.1922600975970634 	 ± 0.23160233233276362
	data : 0.11413178443908692
	model : 0.06483292579650879
			 train-loss:  2.192229631008246 	 ± 0.23100810664699128
	data : 0.11408591270446777
	model : 0.06491236686706543
			 train-loss:  2.1935678483272087 	 ± 0.23117458168742896
	data : 0.11425690650939942
	model : 0.06508855819702149
			 train-loss:  2.192175274573002 	 ± 0.23140982024063297
	data : 0.11416831016540527
	model : 0.06515111923217773
			 train-loss:  2.1925523184766673 	 ± 0.2308853692702738
	data : 0.1141395092010498
	model : 0.06511669158935547
			 train-loss:  2.192697435168166 	 ± 0.23031357695355714
	data : 0.11391558647155761
	model : 0.0650712490081787
			 train-loss:  2.1912014174461363 	 ± 0.2307043513815828
	data : 0.11391777992248535
	model : 0.06496801376342773
			 train-loss:  2.191087198494679 	 ± 0.23013541327644585
	data : 0.11372947692871094
	model : 0.0649022102355957
			 train-loss:  2.190598782926503 	 ± 0.22966947403034016
	data : 0.11379532814025879
	model : 0.06490020751953125
			 train-loss:  2.1892021072321923 	 ± 0.2299614459215127
	data : 0.11377549171447754
	model : 0.06495103836059571
			 train-loss:  2.1880862333026587 	 ± 0.22994740760096843
	data : 0.1140047550201416
	model : 0.06498727798461915
			 train-loss:  2.1877302815274495 	 ± 0.22944220758299486
	data : 0.11401443481445313
	model : 0.06504793167114258
			 train-loss:  2.186935993652899 	 ± 0.22916698641221053
	data : 0.11416211128234863
	model : 0.06501359939575195
			 train-loss:  2.1864158744397373 	 ± 0.22873462317264148
	data : 0.11409983634948731
	model : 0.06495938301086426
			 train-loss:  2.187374596985487 	 ± 0.2286006462650219
	data : 0.11409940719604492
	model : 0.06481523513793945
			 train-loss:  2.188528215485897 	 ± 0.22865920009256005
	data : 0.11394639015197754
	model : 0.0647674560546875
			 train-loss:  2.188597865899404 	 ± 0.2281163460852636
	data : 0.11395049095153809
	model : 0.06477828025817871
			 train-loss:  2.1892255940143532 	 ± 0.2277568772224744
	data : 0.11429944038391113
	model : 0.06483964920043946
			 train-loss:  2.1877985917172342 	 ± 0.22816261096374674
	data : 0.11449017524719238
	model : 0.064892578125
			 train-loss:  2.1882264395834694 	 ± 0.2277116155571153
	data : 0.11455845832824707
	model : 0.06499791145324707
			 train-loss:  2.1887546928129464 	 ± 0.22730973593017284
	data : 0.11467180252075196
	model : 0.06503958702087402
			 train-loss:  2.188368837777958 	 ± 0.22685072842525988
	data : 0.11470861434936523
	model : 0.06502633094787598
			 train-loss:  2.188780473890128 	 ± 0.2264054702686427
	data : 0.11459755897521973
	model : 0.0649484634399414
			 train-loss:  2.187846084893574 	 ± 0.22630025242618376
	data : 0.11437840461730957
	model : 0.06487879753112794
			 train-loss:  2.1878604670183375 	 ± 0.22578071795523713
	data : 0.11438145637512206
	model : 0.0648104190826416
			 train-loss:  2.1865725054588494 	 ± 0.22606589619844528
	data : 0.11437492370605469
	model : 0.06479482650756836
			 train-loss:  2.186121734163978 	 ± 0.2256501496424745
	data : 0.11435718536376953
	model : 0.06477870941162109
			 train-loss:  2.184390815135041 	 ± 0.22659816777583244
	data : 0.11411604881286622
	model : 0.06479053497314453
			 train-loss:  2.183426256652351 	 ± 0.22654149843266072
	data : 0.11438684463500977
	model : 0.06480536460876465
			 train-loss:  2.1859698488038752 	 ± 0.22918817687992163
	data : 0.11452054977416992
	model : 0.06478829383850097
			 train-loss:  2.186012947133609 	 ± 0.22867692956817262
	data : 0.11442179679870605
	model : 0.06478486061096192
			 train-loss:  2.186223037507799 	 ± 0.22818985751732002
	data : 0.11430902481079101
	model : 0.06464405059814453
			 train-loss:  2.1859263529819724 	 ± 0.22772794080676542
	data : 0.11428470611572265
	model : 0.0643740177154541
			 train-loss:  2.185410578345412 	 ± 0.22735803947438712
	data : 0.11408209800720215
	model : 0.06420512199401855
			 train-loss:  2.1850504917010927 	 ± 0.2269237616249682
	data : 0.1140047550201416
	model : 0.06408758163452148
			 train-loss:  2.184735398105138 	 ± 0.2264777339797246
	data : 0.11422543525695801
	model : 0.06394782066345214
			 train-loss:  2.1855298560598624 	 ± 0.22630442109716417
	data : 0.11442713737487793
	model : 0.06390972137451172
			 train-loss:  2.187592603427507 	 ± 0.2279706556986472
	data : 0.11466841697692871
	model : 0.06394991874694825
			 train-loss:  2.189169075982324 	 ± 0.22873719624842292
	data : 0.11467947959899902
	model : 0.06389622688293457
			 train-loss:  2.1883777303245444 	 ± 0.22856385858280448
	data : 0.11475644111633301
	model : 0.06388540267944336
			 train-loss:  2.189563434348147 	 ± 0.22879195080557876
	data : 0.11470050811767578
	model : 0.06384339332580566
			 train-loss:  2.1892093232337464 	 ± 0.2283688928198804
	data : 0.11478476524353028
	model : 0.06386280059814453
			 train-loss:  2.189564034090204 	 ± 0.22794941175578595
	data : 0.11477446556091309
	model : 0.06393184661865234
			 train-loss:  2.1897904208943815 	 ± 0.22749458258096394
	data : 0.11481261253356934
	model : 0.06399288177490234
			 train-loss:  2.190079415545744 	 ± 0.227059740970121
	data : 0.11478638648986816
	model : 0.06397862434387207
			 train-loss:  2.189820175889147 	 ± 0.22661951548503437
	data : 0.11468386650085449
	model : 0.06394033432006836
			 train-loss:  2.1927586535612744 	 ± 0.23066447792139447
	data : 0.11447300910949706
	model : 0.06390881538391113
			 train-loss:  2.192110049774043 	 ± 0.23040463107183737
	data : 0.11434316635131836
	model : 0.06388583183288574
			 train-loss:  2.1925093772982764 	 ± 0.23001165106074056
	data : 0.11451954841613769
	model : 0.06391215324401855
			 train-loss:  2.1930436872160484 	 ± 0.22968833224551014
	data : 0.1145258903503418
	model : 0.06396827697753907
			 train-loss:  2.191454561518841 	 ± 0.23055187429361967
	data : 0.11463580131530762
	model : 0.06401648521423339
			 train-loss:  2.1915369281963426 	 ± 0.2300844764851832
	data : 0.11483311653137207
	model : 0.06399178504943848
			 train-loss:  2.192215136880797 	 ± 0.2298616096604755
	data : 0.11479768753051758
	model : 0.06390676498413086
			 train-loss:  2.1916423138336616 	 ± 0.22957170179387698
	data : 0.11467347145080567
	model : 0.0638537883758545
			 train-loss:  2.1916662940094547 	 ± 0.2291086980767041
	data : 0.11465640068054199
	model : 0.06381778717041016
			 train-loss:  2.191884651241532 	 ± 0.22867403380592252
	data : 0.11471176147460938
	model : 0.06383333206176758
			 train-loss:  2.19186789560318 	 ± 0.22821638063300068
	data : 0.11460614204406738
	model : 0.06389684677124023
			 train-loss:  2.192345600679101 	 ± 0.22788652014107463
	data : 0.11460738182067871
	model : 0.06396670341491699
			 train-loss:  2.192023596593312 	 ± 0.2274911228720254
	data : 0.11463346481323242
	model : 0.06394257545471191
			 train-loss:  2.1918213428715942 	 ± 0.22706379114046152
	data : 0.11466746330261231
	model : 0.0639155387878418
			 train-loss:  2.1916115176020643 	 ± 0.2266409492408591
	data : 0.11464977264404297
	model : 0.0639195442199707
			 train-loss:  2.192182930310567 	 ± 0.22637936805147235
	data : 0.11466116905212402
	model : 0.063897705078125
			 train-loss:  2.1979746003635228 	 ± 0.24413318082773425
	data : 0.11446633338928222
	model : 0.055495262145996094
#epoch  99    val-loss:  2.4009527658161365  train-loss:  2.1979746003635228  lr:  1.9073486328125e-08
