model ba4c19b045af244f92ef5415cc764437d466e1160cc3db553a461431 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=5.0, disp=1, domain='global', epoch=13, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
			 train-loss:  2.254342555999756 	 ± 0.0
	data : 5.814225196838379
	model : 1.9269652366638184
			 train-loss:  2.0998889803886414 	 ± 0.1544535756111145
	data : 2.922666907310486
	model : 0.9966955184936523
			 train-loss:  1.9911175568898518 	 ± 0.19891300398066367
	data : 1.98750901222229
	model : 0.686953624089559
			 train-loss:  2.103538930416107 	 ± 0.25998169751166733
	data : 1.5196661949157715
	model : 0.5324722528457642
			 train-loss:  2.036795806884766 	 ± 0.2681249048255647
	data : 1.2387098789215087
	model : 0.43987860679626467
			 train-loss:  2.0564425388971963 	 ± 0.24867470227241284
	data : 0.09873471260070801
	model : 0.06830110549926757
			 train-loss:  2.1207126208714078 	 ± 0.27890645376297146
	data : 0.11539511680603028
	model : 0.06793704032897949
			 train-loss:  2.1531797647476196 	 ± 0.2746707423937932
	data : 0.11562891006469726
	model : 0.06837162971496583
			 train-loss:  2.1706545088026257 	 ± 0.2636366469977602
	data : 0.1153106689453125
	model : 0.06861891746520996
			 train-loss:  2.197457695007324 	 ± 0.2627157220128118
	data : 0.1153261661529541
	model : 0.06765632629394532
			 train-loss:  2.1622235016389326 	 ± 0.27415238429856403
	data : 0.11623630523681641
	model : 0.067840576171875
			 train-loss:  2.157746066649755 	 ± 0.2629006605737435
	data : 0.11622276306152343
	model : 0.06892156600952148
			 train-loss:  2.147592773804298 	 ± 0.25502383409516893
	data : 0.1154200553894043
	model : 0.06835222244262695
			 train-loss:  2.1253948296819414 	 ± 0.25845188762942595
	data : 0.11599302291870117
	model : 0.06825804710388184
			 train-loss:  2.101057489713033 	 ± 0.2657752909996282
	data : 0.11596012115478516
	model : 0.06987223625183106
			 train-loss:  2.1051394417881966 	 ± 0.25782098191472885
	data : 0.11480450630187988
	model : 0.06979241371154785
			 train-loss:  2.0957695526235245 	 ± 0.252915555692623
	data : 0.11485247611999512
	model : 0.06880226135253906
			 train-loss:  2.096531238820818 	 ± 0.24580980249488785
	data : 0.11574320793151856
	model : 0.06934146881103516
			 train-loss:  2.075735192549856 	 ± 0.2550037045034036
	data : 0.11514263153076172
	model : 0.06932473182678223
			 train-loss:  2.0788577795028687 	 ± 0.248919274627819
	data : 0.11512689590454102
	model : 0.06866931915283203
			 train-loss:  2.0847522644769576 	 ± 0.24434645445807665
	data : 0.1154250144958496
	model : 0.06779861450195312
			 train-loss:  2.103200218894265 	 ± 0.25325517765320643
	data : 0.1163632869720459
	model : 0.06846065521240234
			 train-loss:  2.090587263521941 	 ± 0.2546555996235019
	data : 0.11573147773742676
	model : 0.06829581260681153
			 train-loss:  2.102310578028361 	 ± 0.2555551669499822
	data : 0.11591858863830566
	model : 0.06817040443420411
			 train-loss:  2.1233998584747313 	 ± 0.27086951030807405
	data : 0.11580495834350586
	model : 0.06725106239318848
			 train-loss:  2.122863384393545 	 ± 0.26562295163005717
	data : 0.11657028198242188
	model : 0.067156982421875
			 train-loss:  2.109767105844286 	 ± 0.269075656892037
	data : 0.1163668155670166
	model : 0.06700096130371094
			 train-loss:  2.116875925234386 	 ± 0.2667965278260032
	data : 0.11682567596435547
	model : 0.06626296043395996
			 train-loss:  2.1081052730823386 	 ± 0.266232553690226
	data : 0.11743416786193847
	model : 0.0663069248199463
			 train-loss:  2.1049124161402384 	 ± 0.2623218429418347
	data : 0.11656656265258789
	model : 0.06720232963562012
			 train-loss:  2.1084079434794765 	 ± 0.25876542373322015
	data : 0.11574583053588867
	model : 0.06796145439147949
			 train-loss:  2.110937364399433 	 ± 0.25507919521099315
	data : 0.11518630981445313
	model : 0.06816840171813965
			 train-loss:  2.115310221007376 	 ± 0.25239971704664105
	data : 0.11474399566650391
	model : 0.06901483535766602
			 train-loss:  2.1209720303030575 	 ± 0.2507783350515746
	data : 0.11438369750976562
	model : 0.06938834190368652
			 train-loss:  2.1145790679114205 	 ± 0.24996500071294586
	data : 0.11491990089416504
	model : 0.06932129859924316
			 train-loss:  2.113833874464035 	 ± 0.2465082402316108
	data : 0.11514816284179688
	model : 0.06860275268554687
			 train-loss:  2.104838509817381 	 ± 0.24907223208600476
	data : 0.11578083038330078
	model : 0.06872148513793945
			 train-loss:  2.103758670781788 	 ± 0.24586087267402373
	data : 0.11558475494384765
	model : 0.06787829399108887
			 train-loss:  2.0986573298772178 	 ± 0.24471724674751894
	data : 0.11596841812133789
	model : 0.06711201667785645
			 train-loss:  2.101323077082634 	 ± 0.2422117043056717
	data : 0.11694183349609374
	model : 0.0662912368774414
			 train-loss:  2.098792436646252 	 ± 0.23977444580862978
	data : 0.11754417419433594
	model : 0.06718301773071289
			 train-loss:  2.1067261355263844 	 ± 0.24228829839065086
	data : 0.11674966812133789
	model : 0.06726574897766113
			 train-loss:  2.1081172699152035 	 ± 0.23962408015049966
	data : 0.11677684783935546
	model : 0.0679776668548584
			 train-loss:  2.100727742368525 	 ± 0.241790667019197
	data : 0.1161531925201416
	model : 0.0675461769104004
			 train-loss:  2.1018403900994196 	 ± 0.23920289775188988
	data : 0.11638169288635254
	model : 0.06840825080871582
			 train-loss:  2.1025291059328164 	 ± 0.23663368522583933
	data : 0.11568517684936523
	model : 0.06854948997497559
			 train-loss:  2.1127048959123327 	 ± 0.24406404000619597
	data : 0.11583352088928223
	model : 0.06758589744567871
			 train-loss:  2.103963832060496 	 ± 0.24883199634147735
	data : 0.11664037704467774
	model : 0.06864147186279297
			 train-loss:  2.10178149233059 	 ± 0.24674348600083543
	data : 0.11549787521362305
	model : 0.06858615875244141
			 train-loss:  2.092650113105774 	 ± 0.2524884614531729
	data : 0.11550202369689941
	model : 0.06859116554260254
			 train-loss:  2.088682546335108 	 ± 0.2515700581817776
	data : 0.11535344123840333
	model : 0.06795411109924317
			 train-loss:  2.0885900327792535 	 ± 0.2491402486042671
	data : 0.11574573516845703
	model : 0.06809649467468262
			 train-loss:  2.0789298358953223 	 ± 0.25642215993677103
	data : 0.11626005172729492
	model : 0.06644291877746582
			 train-loss:  2.0824574607389943 	 ± 0.25533160348462985
	data : 0.11802139282226562
	model : 0.0685694694519043
			 train-loss:  2.084144936908375 	 ± 0.2533034686639636
	data : 0.11596579551696777
	model : 0.06776547431945801
			 train-loss:  2.084431360874857 	 ± 0.2510406297937849
	data : 0.11674795150756836
	model : 0.06817455291748047
			 train-loss:  2.080018668843989 	 ± 0.2510103242955522
	data : 0.11633687019348145
	model : 0.06903724670410157
			 train-loss:  2.0759361180765876 	 ± 0.2507387105901893
	data : 0.11515235900878906
	model : 0.0697016716003418
			 train-loss:  2.0740198220236827 	 ± 0.24903272174329194
	data : 0.11460866928100585
	model : 0.0685415267944336
			 train-loss:  2.077931100130081 	 ± 0.24876949688349512
	data : 0.11589589118957519
	model : 0.06936321258544922
			 train-loss:  2.0840294419742023 	 ± 0.2512033440855914
	data : 0.11509757041931153
	model : 0.0694047451019287
			 train-loss:  2.0863527732510723 	 ± 0.24982913717574992
	data : 0.11497516632080078
	model : 0.06933794021606446
			 train-loss:  2.0856509416822404 	 ± 0.24790003846074926
	data : 0.11481599807739258
	model : 0.06955342292785645
			 train-loss:  2.096712762489915 	 ± 0.2611572859949532
	data : 0.11469378471374511
	model : 0.06953282356262207
			 train-loss:  2.0927898957179143 	 ± 0.2610339780430948
	data : 0.11448850631713867
	model : 0.06941466331481934
			 train-loss:  2.0958596215103613 	 ± 0.2602284412686191
	data : 0.1147127628326416
	model : 0.07032260894775391
			 train-loss:  2.091711328990424 	 ± 0.2604685441663954
	data : 0.11380562782287598
	model : 0.07027087211608887
			 train-loss:  2.1050307435147904 	 ± 0.28059296936858996
	data : 0.11372122764587403
	model : 0.07017836570739747
			 train-loss:  2.107978495998659 	 ± 0.2796108610447504
	data : 0.11373167037963867
	model : 0.07022209167480468
			 train-loss:  2.1127364873886108 	 ± 0.28040577708950387
	data : 0.11409201622009277
	model : 0.07038512229919433
			 train-loss:  2.11057560376718 	 ± 0.27901044948243897
	data : 0.11383070945739746
	model : 0.06948666572570801
			 train-loss:  2.1087450683116913 	 ± 0.277495109865394
	data : 0.11477584838867187
	model : 0.06949901580810547
			 train-loss:  2.105704670083033 	 ± 0.2767928152613603
	data : 0.1149787425994873
	model : 0.06892290115356445
			 train-loss:  2.1041429622753247 	 ± 0.2752398530205947
	data : 0.11538186073303222
	model : 0.068882417678833
			 train-loss:  2.1054716364542645 	 ± 0.27363757312129927
	data : 0.1155397891998291
	model : 0.06802892684936523
			 train-loss:  2.105770838888068 	 ± 0.27184371435789184
	data : 0.11661086082458497
	model : 0.06785116195678711
			 train-loss:  2.107085262026106 	 ± 0.27031571082696626
	data : 0.11671967506408691
	model : 0.06768250465393066
			 train-loss:  2.1112636511142435 	 ± 0.27106847694024305
	data : 0.1167076587677002
	model : 0.06782007217407227
			 train-loss:  2.110445351540288 	 ± 0.269444328801277
	data : 0.11655821800231933
	model : 0.06770782470703125
			 train-loss:  2.106961666047573 	 ± 0.2695394077838915
	data : 0.1163525104522705
	model : 0.0684166431427002
			 train-loss:  2.1070808319397916 	 ± 0.267872537571469
	data : 0.11548776626586914
	model : 0.0686887264251709
			 train-loss:  2.108604263968584 	 ± 0.26658697525146086
	data : 0.11559662818908692
	model : 0.069096040725708
			 train-loss:  2.109033485493028 	 ± 0.26500466778403003
	data : 0.11530346870422363
	model : 0.06973390579223633
			 train-loss:  2.1094691228298914 	 ± 0.26345243274947705
	data : 0.11475849151611328
	model : 0.06927857398986817
			 train-loss:  2.1077362256891585 	 ± 0.2623792589782619
	data : 0.11507344245910645
	model : 0.06891779899597168
			 train-loss:  2.1035324016282724 	 ± 0.2637129284120015
	data : 0.11533594131469727
	model : 0.06883611679077148
			 train-loss:  2.099177150890745 	 ± 0.2652855299795726
	data : 0.11512813568115235
	model : 0.06777186393737793
			 train-loss:  2.099546815861355 	 ± 0.2637964541039871
	data : 0.11631903648376465
	model : 0.06668672561645508
			 train-loss:  2.101368442010344 	 ± 0.262866292024394
	data : 0.11730971336364746
	model : 0.06726741790771484
			 train-loss:  2.09991755882899 	 ± 0.26175995646189004
	data : 0.11677474975585937
	model : 0.06781644821166992
			 train-loss:  2.1003474992710154 	 ± 0.2603496939169724
	data : 0.11639461517333985
	model : 0.0678898811340332
			 train-loss:  2.098555732032527 	 ± 0.2594944156799841
	data : 0.11629047393798828
	model : 0.06878619194030762
			 train-loss:  2.097583364414912 	 ± 0.25826397369997317
	data : 0.11530942916870117
	model : 0.06964941024780273
			 train-loss:  2.0997819405920963 	 ± 0.25776004388490414
	data : 0.11475653648376465
	model : 0.06880908012390137
			 train-loss:  2.096794731993424 	 ± 0.25803036701859766
	data : 0.1155477523803711
	model : 0.06873931884765624
			 train-loss:  2.1001617846389613 	 ± 0.2587723926650872
	data : 0.11575455665588379
	model : 0.06792254447937011
			 train-loss:  2.097463648343824 	 ± 0.2587888784887269
	data : 0.11647005081176758
	model : 0.06784963607788086
			 train-loss:  2.0962126546976516 	 ± 0.2577597770358864
	data : 0.11629962921142578
	model : 0.0678316593170166
			 train-loss:  2.0988049458975744 	 ± 0.2577354223398923
	data : 0.11608567237854003
	model : 0.06865410804748535
			 train-loss:  2.103627519607544 	 ± 0.2608941106101406
	data : 0.11561746597290039
	model : 0.06838755607604981
			 train-loss:  2.0996785317317093 	 ± 0.262585737022731
	data : 0.11565046310424805
	model : 0.06909313201904296
			 train-loss:  2.0993430836527955 	 ± 0.2613171282529995
	data : 0.11508831977844239
	model : 0.06909127235412597
			 train-loss:  2.097114505119694 	 ± 0.2610177269241511
	data : 0.11521587371826172
	model : 0.06823687553405762
			 train-loss:  2.0970260340433855 	 ± 0.2597613546493199
	data : 0.11599063873291016
	model : 0.06820864677429199
			 train-loss:  2.0954487403233846 	 ± 0.2590213700364607
	data : 0.11589818000793457
	model : 0.06773190498352051
			 train-loss:  2.100000987637718 	 ± 0.2619828933667655
	data : 0.11649880409240723
	model : 0.06781277656555176
			 train-loss:  2.101277662214832 	 ± 0.26108687570191474
	data : 0.11634645462036133
	model : 0.06770415306091308
			 train-loss:  2.0997082182654627 	 ± 0.26038191989051485
	data : 0.11645650863647461
	model : 0.06772141456604004
			 train-loss:  2.100264145693648 	 ± 0.2592491376095161
	data : 0.11641039848327636
	model : 0.06797151565551758
			 train-loss:  2.103226953203028 	 ± 0.2599152577118791
	data : 0.11620626449584961
	model : 0.06855716705322265
			 train-loss:  2.1016826769253156 	 ± 0.25924825153322906
	data : 0.11563720703125
	model : 0.06773300170898437
			 train-loss:  2.1025145085794583 	 ± 0.2582370528844195
	data : 0.11671438217163085
	model : 0.06790752410888672
			 train-loss:  2.100493926917557 	 ± 0.2579796476370314
	data : 0.11657743453979492
	model : 0.06882300376892089
			 train-loss:  2.0998331998523914 	 ± 0.25694168070682455
	data : 0.11581082344055176
	model : 0.06783065795898438
			 train-loss:  2.0978535911311273 	 ± 0.2566937820924044
	data : 0.11675505638122559
	model : 0.06800436973571777
			 train-loss:  2.094871927951944 	 ± 0.2575772741309688
	data : 0.11654744148254395
	model : 0.06799516677856446
			 train-loss:  2.095920261154827 	 ± 0.25672256652748615
	data : 0.11649918556213379
	model : 0.06811981201171875
			 train-loss:  2.0949407341116566 	 ± 0.25585192020610287
	data : 0.11653676033020019
	model : 0.06813759803771972
			 train-loss:  2.097491097049553 	 ± 0.2562764771644136
	data : 0.11641693115234375
	model : 0.06789655685424804
			 train-loss:  2.096164541443189 	 ± 0.25561637147296884
	data : 0.11642656326293946
	model : 0.06794800758361816
			 train-loss:  2.0975547792497746 	 ± 0.25501306425499615
	data : 0.11643028259277344
	model : 0.0687790870666504
			 train-loss:  2.0982360224254797 	 ± 0.25407631075559506
	data : 0.11560449600219727
	model : 0.06857681274414062
			 train-loss:  2.0999537590073376 	 ± 0.2537516732135756
	data : 0.11582045555114746
	model : 0.06866745948791504
			 train-loss:  2.0992857861903405 	 ± 0.25283496447828013
	data : 0.1159541130065918
	model : 0.06966724395751953
			 train-loss:  2.099462338447571 	 ± 0.2518292681071313
	data : 0.1150740146636963
	model : 0.06968522071838379
			 train-loss:  2.098537416685195 	 ± 0.2510410285290337
	data : 0.11504073143005371
	model : 0.06984663009643555
			 train-loss:  2.096189301783644 	 ± 0.25143604364173777
	data : 0.11486201286315918
	model : 0.06911625862121581
			 train-loss:  2.095402012579143 	 ± 0.2506090477629522
	data : 0.11564054489135742
	model : 0.06918425559997558
			 train-loss:  2.094109343003857 	 ± 0.25006383681698663
	data : 0.11564836502075196
	model : 0.06924295425415039
			 train-loss:  2.0958849888581494 	 ± 0.24991525392236394
	data : 0.1155426025390625
	model : 0.06926007270812988
			 train-loss:  2.094491982278023 	 ± 0.24946566710295473
	data : 0.11549086570739746
	model : 0.06997652053833008
			 train-loss:  2.0923218113003355 	 ± 0.2497571203217254
	data : 0.11470904350280761
	model : 0.07117061614990235
			 train-loss:  2.094736099243164 	 ± 0.25035775754924644
	data : 0.11375918388366699
	model : 0.07178430557250977
			 train-loss:  2.0937025351310843 	 ± 0.24970648943944343
	data : 0.11319355964660645
	model : 0.07158794403076171
			 train-loss:  2.0969781275148747 	 ± 0.25165295165266216
	data : 0.11349735260009766
	model : 0.07145776748657226
			 train-loss:  2.0964887107119843 	 ± 0.25079052717896655
	data : 0.11355977058410645
	model : 0.07049365043640136
			 train-loss:  2.0943494043210995 	 ± 0.2511159420971329
	data : 0.11442890167236328
	model : 0.07022762298583984
			 train-loss:  2.0944334087164505 	 ± 0.25020637942545976
	data : 0.11440529823303222
	model : 0.06946053504943847
			 train-loss:  2.093648437973407 	 ± 0.2494752132983931
	data : 0.11512169837951661
	model : 0.0693554401397705
			 train-loss:  2.0933393061161043 	 ± 0.24860935006178325
	data : 0.11519074440002441
	model : 0.06937551498413086
			 train-loss:  2.093379444264351 	 ± 0.24772664318525048
	data : 0.11509876251220703
	model : 0.06876616477966309
			 train-loss:  2.092517889721293 	 ± 0.24706472477747876
	data : 0.11585984230041504
	model : 0.068534517288208
			 train-loss:  2.092216483362905 	 ± 0.24622554364716476
	data : 0.11596112251281739
	model : 0.06829848289489746
			 train-loss:  2.0915063180857234 	 ± 0.2455160223869472
	data : 0.1161999225616455
	model : 0.06846346855163574
			 train-loss:  2.09102591728342 	 ± 0.2447358558227861
	data : 0.11603889465332032
	model : 0.0684091567993164
			 train-loss:  2.0898202739349783 	 ± 0.24432798350750068
	data : 0.11632080078125
	model : 0.06908564567565918
			 train-loss:  2.0916679294741884 	 ± 0.2445168429763114
	data : 0.11552681922912597
	model : 0.06913609504699707
			 train-loss:  2.093830430829847 	 ± 0.24509578294947737
	data : 0.1154362678527832
	model : 0.06940393447875977
			 train-loss:  2.0949446194923964 	 ± 0.24464771619975298
	data : 0.11522159576416016
	model : 0.06949577331542969
			 train-loss:  2.095898577372233 	 ± 0.24410875373758184
	data : 0.11530585289001465
	model : 0.0695075511932373
			 train-loss:  2.0947517227652845 	 ± 0.24370421679093313
	data : 0.11515083312988281
	model : 0.06949291229248047
			 train-loss:  2.091926980959742 	 ± 0.2453688330286073
	data : 0.11525168418884277
	model : 0.06911773681640625
			 train-loss:  2.0958293279012046 	 ± 0.24925301444981646
	data : 0.11561970710754395
	model : 0.0689927577972412
			 train-loss:  2.0947873290483052 	 ± 0.2487765347398876
	data : 0.11562891006469726
	model : 0.06966381072998047
			 train-loss:  2.0945093270271053 	 ± 0.2479967284960027
	data : 0.11489510536193848
	model : 0.06950831413269043
			 train-loss:  2.09776154313332 	 ± 0.2504946153599629
	data : 0.11519265174865723
	model : 0.0685610294342041
			 train-loss:  2.099996394412533 	 ± 0.2512509458359018
	data : 0.11592764854431152
	model : 0.06895270347595214
			 train-loss:  2.0991776578034025 	 ± 0.25066459926030926
	data : 0.11557331085205078
	model : 0.06889801025390625
			 train-loss:  2.0983090025823823 	 ± 0.25011354946843856
	data : 0.11542119979858398
	model : 0.06738252639770508
			 train-loss:  2.0969453118741512 	 ± 0.24992297299030886
	data : 0.11701087951660157
	model : 0.06764421463012696
			 train-loss:  2.099264373690445 	 ± 0.25086653918221424
	data : 0.11695494651794433
	model : 0.06785883903503417
			 train-loss:  2.1011495420962203 	 ± 0.251232383104113
	data : 0.1166259765625
	model : 0.06789326667785645
			 train-loss:  2.1053896762110704 	 ± 0.256208978273623
	data : 0.11674065589904785
	model : 0.06790590286254883
			 train-loss:  2.104070039784036 	 ± 0.25598170417917676
	data : 0.11670641899108887
	model : 0.06855573654174804
			 train-loss:  2.105551593953913 	 ± 0.25590912882994865
	data : 0.115826416015625
	model : 0.06857333183288575
			 train-loss:  2.1113201207425223 	 ± 0.2656793010034439
	data : 0.11572022438049316
	model : 0.06939339637756348
			 train-loss:  2.109749092313344 	 ± 0.26565491338389974
	data : 0.1153292179107666
	model : 0.06852512359619141
			 train-loss:  2.112902406425703 	 ± 0.2679794852514238
	data : 0.11591849327087403
	model : 0.06855592727661133
			 train-loss:  2.112462047289109 	 ± 0.26724642824785916
	data : 0.11585555076599122
	model : 0.06872143745422363
			 train-loss:  2.111386263370514 	 ± 0.2668260060156443
	data : 0.11561813354492187
	model : 0.06773366928100585
			 train-loss:  2.110958617333083 	 ± 0.2661030922681652
	data : 0.11616768836975097
	model : 0.06761283874511718
			 train-loss:  2.1085839049760686 	 ± 0.26713943539697776
	data : 0.11635322570800781
	model : 0.06860108375549316
			 train-loss:  2.1097077565386115 	 ± 0.2667737160458574
	data : 0.11576337814331054
	model : 0.06859302520751953
			 train-loss:  2.109673809731144 	 ± 0.2660063950405528
	data : 0.11568622589111328
	model : 0.06860823631286621
			 train-loss:  2.1086337212153845 	 ± 0.2655998742437589
	data : 0.11576328277587891
	model : 0.06954379081726074
			 train-loss:  2.1096571656790646 	 ± 0.2651900842456082
	data : 0.11500191688537598
	model : 0.06889700889587402
			 train-loss:  2.108526059463199 	 ± 0.26486531409493647
	data : 0.11516084671020507
	model : 0.06792349815368652
			 train-loss:  2.1061802064434865 	 ± 0.26595779503315925
	data : 0.11590633392333985
	model : 0.06719856262207032
			 train-loss:  2.108535439608483 	 ± 0.2670688622608139
	data : 0.11689066886901855
	model : 0.06720809936523438
			 train-loss:  2.1094379127025604 	 ± 0.26659953262764313
	data : 0.11706185340881348
	model : 0.06719932556152344
			 train-loss:  2.107884563135179 	 ± 0.2666776154840934
	data : 0.11707072257995606
	model : 0.06769657135009766
			 train-loss:  2.106859110213898 	 ± 0.266301575930424
	data : 0.11683459281921386
	model : 0.06845569610595703
			 train-loss:  2.1061750331211613 	 ± 0.2657332803998822
	data : 0.11604123115539551
	model : 0.06875090599060059
			 train-loss:  2.1053062839352568 	 ± 0.26527065106569914
	data : 0.11581802368164062
	model : 0.06882767677307129
			 train-loss:  2.103824612256643 	 ± 0.2653150816006691
	data : 0.1158745288848877
	model : 0.06809730529785156
			 train-loss:  2.1033710350272474 	 ± 0.26467281857130937
	data : 0.11655960083007813
	model : 0.06753506660461425
			 train-loss:  2.1031928247308986 	 ± 0.2639753776804529
	data : 0.11706061363220215
	model : 0.06760683059692382
			 train-loss:  2.102824440027805 	 ± 0.26332057084328897
	data : 0.11674470901489258
	model : 0.06796956062316895
			 train-loss:  2.1028919503802346 	 ± 0.2626246629842415
	data : 0.11645069122314453
	model : 0.06799578666687012
			 train-loss:  2.1039578971109894 	 ± 0.26234224670914064
	data : 0.1161947250366211
	model : 0.06881160736083984
			 train-loss:  2.1031377015937687 	 ± 0.2618987191965535
	data : 0.11564269065856933
	model : 0.06949553489685059
			 train-loss:  2.103073164820671 	 ± 0.26121732361443634
	data : 0.11469182968139649
	model : 0.06943020820617676
			 train-loss:  2.1024814269703285 	 ± 0.26066870368804507
	data : 0.1148618221282959
	model : 0.06959080696105957
			 train-loss:  2.100602739250537 	 ± 0.2613027210045999
	data : 0.11441979408264161
	model : 0.06963272094726562
			 train-loss:  2.1014348341868474 	 ± 0.2608894110859733
	data : 0.11469402313232421
	model : 0.06961569786071778
			 train-loss:  2.103904915707452 	 ± 0.26249909940731553
	data : 0.11476635932922363
	model : 0.06965479850769044
			 train-loss:  2.1032457484811697 	 ± 0.2619945876380554
	data : 0.11482782363891601
	model : 0.0692026138305664
			 train-loss:  2.1016994004297738 	 ± 0.2622318750433427
	data : 0.11518373489379882
	model : 0.06917672157287598
			 train-loss:  2.102964392858534 	 ± 0.26217711854161085
	data : 0.11521372795104981
	model : 0.06921095848083496
			 train-loss:  2.10341827750206 	 ± 0.26159922309136174
	data : 0.11487689018249511
	model : 0.07002787590026856
			 train-loss:  2.1023877277896177 	 ± 0.26135434115102113
	data : 0.11412453651428223
	model : 0.07059674263000489
			 train-loss:  2.1030162708594067 	 ± 0.26085887183395445
	data : 0.11400737762451171
	model : 0.07054228782653808
			 train-loss:  2.1015231303980784 	 ± 0.26107948026055583
	data : 0.11409602165222169
	model : 0.0705115795135498
			 train-loss:  2.100980837555493 	 ± 0.2605533797240251
	data : 0.11405386924743652
	model : 0.0703084945678711
			 train-loss:  2.101836997125207 	 ± 0.26020460464399675
	data : 0.11412539482116699
	model : 0.06927900314331055
			 train-loss:  2.1050919751519137 	 ± 0.2637227996548889
	data : 0.11477346420288086
	model : 0.06885037422180176
			 train-loss:  2.104905216590218 	 ± 0.2630986718340027
	data : 0.11513667106628418
	model : 0.06859941482543945
			 train-loss:  2.1066204991478186 	 ± 0.26362312586178394
	data : 0.1156148910522461
	model : 0.06843881607055664
			 train-loss:  2.1074083952242106 	 ± 0.2632370652390745
	data : 0.11592354774475097
	model : 0.068233060836792
			 train-loss:  2.106035666238694 	 ± 0.26335834656025386
	data : 0.11604037284851074
	model : 0.06820039749145508
			 train-loss:  2.105892298910855 	 ± 0.2627417477098322
	data : 0.11606545448303222
	model : 0.06800522804260253
			 train-loss:  2.1068794232494428 	 ± 0.26251323588756875
	data : 0.11604366302490235
	model : 0.06871085166931153
			 train-loss:  2.1069081725089203 	 ± 0.2618966171846625
	data : 0.11546058654785156
	model : 0.06817626953125
			 train-loss:  2.1110274981115467 	 ± 0.26811133597669884
	data : 0.11621108055114746
	model : 0.06813764572143555
			 train-loss:  2.110005693102992 	 ± 0.26790442331894465
	data : 0.11626453399658203
	model : 0.0681380271911621
			 train-loss:  2.109130812463937 	 ± 0.2675912239093767
	data : 0.1161426067352295
	model : 0.06809864044189454
			 train-loss:  2.107418267408274 	 ± 0.2681577389272325
	data : 0.11616058349609375
	model : 0.06815128326416016
			 train-loss:  2.106403394029775 	 ± 0.2679593628851273
	data : 0.11586441993713378
	model : 0.06870446205139161
			 train-loss:  2.1065540373597513 	 ± 0.2673561357119429
	data : 0.11534204483032226
	model : 0.06899819374084473
			 train-loss:  2.1067289672114633 	 ± 0.26676037726381036
	data : 0.11533503532409668
	model : 0.06838607788085938
			 train-loss:  2.106758663017825 	 ± 0.2661565271797918
	data : 0.11625080108642578
	model : 0.06850552558898926
			 train-loss:  2.107442862815685 	 ± 0.2657511196918325
	data : 0.11607666015625
	model : 0.06774544715881348
			 train-loss:  2.1077910597548892 	 ± 0.26520534520880507
	data : 0.11685371398925781
	model : 0.06710572242736816
			 train-loss:  2.1071641546274935 	 ± 0.26477825789139586
	data : 0.11713323593139649
	model : 0.06634125709533692
			 train-loss:  2.1062048122617933 	 ± 0.264579085548598
	data : 0.1179013729095459
	model : 0.06654009819030762
			 train-loss:  2.10657206626065 	 ± 0.2640505552446208
	data : 0.11779193878173828
	model : 0.06627683639526367
			 train-loss:  2.104588423006335 	 ± 0.265150563364145
	data : 0.11824836730957031
	model : 0.06660594940185546
			 train-loss:  2.103753083630612 	 ± 0.2648676377518295
	data : 0.11778998374938965
	model : 0.06660990715026856
			 train-loss:  2.1028731155603735 	 ± 0.26462249109082686
	data : 0.11766576766967773
	model : 0.06639218330383301
			 train-loss:  2.1013143430585446 	 ± 0.26509813978699187
	data : 0.11730847358703614
	model : 0.06599040031433105
			 train-loss:  2.1003902190691464 	 ± 0.264894725056212
	data : 0.11723732948303223
	model : 0.06533899307250976
			 train-loss:  2.10041980445385 	 ± 0.26432359720852466
	data : 0.11776156425476074
	model : 0.06517171859741211
			 train-loss:  2.100977602434772 	 ± 0.26389257279959577
	data : 0.11805868148803711
	model : 0.0650747299194336
			 train-loss:  2.101231004947271 	 ± 0.26335650317702464
	data : 0.11828818321228027
	model : 0.06517457962036133
			 train-loss:  2.1014163438310014 	 ± 0.26281086566524137
	data : 0.11853604316711426
	model : 0.06529707908630371
			 train-loss:  2.101449569908239 	 ± 0.26225396652048205
	data : 0.1184854507446289
	model : 0.06562633514404297
			 train-loss:  2.101163921979912 	 ± 0.2617368915508379
	data : 0.11785316467285156
	model : 0.06598353385925293
			 train-loss:  2.1003410300286878 	 ± 0.26149348757908425
	data : 0.11737513542175293
	model : 0.0666360855102539
			 train-loss:  2.1000096723125568 	 ± 0.2609959233495146
	data : 0.11672687530517578
	model : 0.06733565330505371
			 train-loss:  2.099188530941804 	 ± 0.26076079968956856
	data : 0.11626453399658203
	model : 0.06817398071289063
			 train-loss:  2.0977096918707567 	 ± 0.2612258110672503
	data : 0.11586847305297851
	model : 0.06878166198730469
			 train-loss:  2.096586259436016 	 ± 0.26126827504763683
	data : 0.11572499275207519
	model : 0.06901421546936035
			 train-loss:  2.0961462955906556 	 ± 0.26081994770939737
	data : 0.11549124717712403
	model : 0.06855430603027343
			 train-loss:  2.0956976213416114 	 ± 0.26037888497964
	data : 0.11592888832092285
	model : 0.06860747337341308
			 train-loss:  2.095419633145235 	 ± 0.25988323593611895
	data : 0.11564388275146484
	model : 0.06811413764953614
			 train-loss:  2.09452507554031 	 ± 0.2597321764744887
	data : 0.11609649658203125
	model : 0.0676382064819336
			 train-loss:  2.0927142406764783 	 ± 0.26075725698175284
	data : 0.1164712905883789
	model : 0.06682829856872559
			 train-loss:  2.0931902888321106 	 ± 0.2603385332800729
	data : 0.11750316619873047
	model : 0.06689167022705078
			 train-loss:  2.0937143688699806 	 ± 0.2599462909545057
	data : 0.11706218719482422
	model : 0.06656365394592285
			 train-loss:  2.094075424671173 	 ± 0.2594884310443967
	data : 0.11706109046936035
	model : 0.06673679351806641
			 train-loss:  2.0926048575169536 	 ± 0.26001273787446844
	data : 0.11641788482666016
	model : 0.06690406799316406
			 train-loss:  2.0945808215746804 	 ± 0.2613778041970869
	data : 0.11610889434814453
	model : 0.06750264167785644
			 train-loss:  2.0958389048519814 	 ± 0.26162412432220666
	data : 0.11541032791137695
	model : 0.0678795337677002
			 train-loss:  2.0960250576650066 	 ± 0.26112539613612734
	data : 0.11547050476074219
	model : 0.06811399459838867
			 train-loss:  2.095934134838628 	 ± 0.2606169111623017
	data : 0.11563963890075683
	model : 0.06828327178955078
			 train-loss:  2.1006839657202363 	 ± 0.2709407409643891
	data : 0.11489276885986328
	model : 0.2750601291656494
#epoch  13    val-loss:  2.4674409941623083  train-loss:  2.1006839657202363  lr:  0.0025
			 train-loss:  2.103740930557251 	 ± 0.0
	data : 5.581552982330322
	model : 0.07246613502502441
			 train-loss:  1.9759300351142883 	 ± 0.12781089544296265
	data : 2.8566672801971436
	model : 0.07162129878997803
			 train-loss:  1.9686750173568726 	 ± 0.10486032246728083
	data : 1.944181203842163
	model : 0.07082700729370117
			 train-loss:  2.0966653525829315 	 ± 0.2395649038461048
	data : 1.4870004653930664
	model : 0.06970083713531494
			 train-loss:  2.063238191604614 	 ± 0.2244606310427646
	data : 1.212965440750122
	model : 0.06952261924743652
			 train-loss:  2.0483394463857016 	 ± 0.20759417650531642
	data : 0.11969947814941406
	model : 0.06879200935363769
			 train-loss:  2.060920476913452 	 ± 0.19464983598298138
	data : 0.11626548767089843
	model : 0.06830849647521972
			 train-loss:  2.0243459045886993 	 ± 0.20619501497937912
	data : 0.11548905372619629
	model : 0.06882762908935547
			 train-loss:  2.041515906651815 	 ± 0.2003766784832404
	data : 0.11525683403015137
	model : 0.0695570945739746
			 train-loss:  2.04008469581604 	 ± 0.1901424919161581
	data : 0.11487350463867188
	model : 0.06968932151794434
			 train-loss:  2.0619419271295722 	 ± 0.19402270579335426
	data : 0.11473312377929687
	model : 0.06982302665710449
			 train-loss:  2.070711096127828 	 ± 0.18802559508538455
	data : 0.1148406982421875
	model : 0.06950216293334961
			 train-loss:  2.101598702944242 	 ± 0.20995867663798046
	data : 0.11530418395996093
	model : 0.06901316642761231
			 train-loss:  2.106511422566005 	 ± 0.2030951453322469
	data : 0.11551766395568848
	model : 0.06902742385864258
			 train-loss:  2.091017421086629 	 ± 0.20459397025855552
	data : 0.11564073562622071
	model : 0.06859335899353028
			 train-loss:  2.08441973477602 	 ± 0.1997384948985092
	data : 0.11628866195678711
	model : 0.06865506172180176
			 train-loss:  2.081169878735262 	 ± 0.19421034790914452
	data : 0.11609935760498047
	model : 0.06827797889709472
			 train-loss:  2.0723675820562573 	 ± 0.19219625640985954
	data : 0.1161801815032959
	model : 0.06778955459594727
			 train-loss:  2.0936448762291358 	 ± 0.20771191692069513
	data : 0.11645321846008301
	model : 0.06763272285461426
			 train-loss:  2.077392911911011 	 ± 0.2144887618246717
	data : 0.11629700660705566
	model : 0.06795120239257812
			 train-loss:  2.0727958168302263 	 ± 0.21032679311218339
	data : 0.1161313533782959
	model : 0.06733369827270508
			 train-loss:  2.076342750679363 	 ± 0.20613288517067296
	data : 0.11699876785278321
	model : 0.068214750289917
			 train-loss:  2.0650732206261675 	 ± 0.20841639209179966
	data : 0.11609930992126465
	model : 0.06843152046203613
			 train-loss:  2.0800236761569977 	 ± 0.2162599637289124
	data : 0.11599440574645996
	model : 0.06754512786865234
			 train-loss:  2.0890760040283203 	 ± 0.21648165832907754
	data : 0.11677026748657227
	model : 0.06677517890930176
			 train-loss:  2.101904740700355 	 ± 0.2217571805110049
	data : 0.11717524528503417
	model : 0.06736664772033692
			 train-loss:  2.1046184963650174 	 ± 0.21805132615440923
	data : 0.1165693759918213
	model : 0.06711692810058593
			 train-loss:  2.122742031301771 	 ± 0.2339161897134942
	data : 0.11734247207641602
	model : 0.06746931076049804
			 train-loss:  2.1113364614289383 	 ± 0.23763929564979333
	data : 0.11709661483764648
	model : 0.06833114624023437
			 train-loss:  2.1039995471636455 	 ± 0.2369622319263494
	data : 0.11645741462707519
	model : 0.06838431358337402
			 train-loss:  2.0949236846739248 	 ± 0.23835039746494188
	data : 0.11648569107055665
	model : 0.06847090721130371
			 train-loss:  2.0972581692039967 	 ± 0.23495641155592203
	data : 0.11653804779052734
	model : 0.06846756935119629
			 train-loss:  2.104688301230922 	 ± 0.23515584367029996
	data : 0.11652846336364746
	model : 0.06839909553527831
			 train-loss:  2.104594886302948 	 ± 0.23167248229348833
	data : 0.11656413078308106
	model : 0.06756672859191895
			 train-loss:  2.1014115129198347 	 ± 0.22909212298134693
	data : 0.11712350845336914
	model : 0.06829390525817872
			 train-loss:  2.1072592271698847 	 ± 0.2285217403030483
	data : 0.11643671989440918
	model : 0.06747722625732422
			 train-loss:  2.106637548755955 	 ± 0.2254433161354732
	data : 0.1169729232788086
	model : 0.06771883964538575
			 train-loss:  2.1068250944739892 	 ± 0.22246010517743764
	data : 0.11668558120727539
	model : 0.06765851974487305
			 train-loss:  2.1099880169599485 	 ± 0.2204534361901473
	data : 0.11683902740478516
	model : 0.06846528053283692
			 train-loss:  2.10632187128067 	 ± 0.21888103830932498
	data : 0.11642351150512695
	model : 0.06854619979858398
			 train-loss:  2.109651809785424 	 ± 0.21721864232899102
	data : 0.11614336967468261
	model : 0.06897211074829102
			 train-loss:  2.1146198511123657 	 ± 0.21696186366730386
	data : 0.11579022407531739
	model : 0.06821641921997071
			 train-loss:  2.1153894635133965 	 ± 0.21448221109502857
	data : 0.11628823280334473
	model : 0.06806273460388183
			 train-loss:  2.124488938938488 	 ± 0.22026693871380915
	data : 0.11636362075805665
	model : 0.06745409965515137
			 train-loss:  2.1193878995047677 	 ± 0.220418383147352
	data : 0.11676654815673829
	model : 0.0670811653137207
			 train-loss:  2.1181833614473757 	 ± 0.21815905899619323
	data : 0.11737489700317383
	model : 0.06681709289550782
			 train-loss:  2.119847016131624 	 ± 0.2161204904200933
	data : 0.11768646240234375
	model : 0.06783299446105957
			 train-loss:  2.121533624827862 	 ± 0.21416974613892645
	data : 0.11681132316589356
	model : 0.06786112785339356
			 train-loss:  2.117937922477722 	 ± 0.21343191014109503
	data : 0.11683330535888672
	model : 0.06849365234375
			 train-loss:  2.117758252620697 	 ± 0.21129055455170156
	data : 0.11642322540283204
	model : 0.06901845932006836
			 train-loss:  2.1151220144010057 	 ± 0.2100376618708677
	data : 0.11589636802673339
	model : 0.06964378356933594
			 train-loss:  2.11343436057751 	 ± 0.20835713380784168
	data : 0.11527581214904785
	model : 0.06854100227355957
			 train-loss:  2.1056323073944956 	 ± 0.2139133785276715
	data : 0.11616148948669433
	model : 0.06857976913452149
			 train-loss:  2.1102869356120073 	 ± 0.2146155184024797
	data : 0.11597623825073242
	model : 0.06776823997497558
			 train-loss:  2.104108463634144 	 ± 0.217448245143495
	data : 0.11641492843627929
	model : 0.06789188385009766
			 train-loss:  2.098949890051569 	 ± 0.21886750628080048
	data : 0.11648821830749512
	model : 0.06799077987670898
			 train-loss:  2.103250212836684 	 ± 0.21931296704423806
	data : 0.1163219928741455
	model : 0.06805915832519531
			 train-loss:  2.1033796750265976 	 ± 0.2174163147908579
	data : 0.116522216796875
	model : 0.06849331855773926
			 train-loss:  2.1120611914133622 	 ± 0.2254773960402058
	data : 0.11616125106811523
	model : 0.07029404640197753
			 train-loss:  2.1107334514458973 	 ± 0.22382299383256316
	data : 0.11451945304870606
	model : 0.07000813484191895
			 train-loss:  2.109718801545315 	 ± 0.22211988997490315
	data : 0.1146212100982666
	model : 0.06972589492797851
			 train-loss:  2.1078821862897565 	 ± 0.22078778496402648
	data : 0.11507973670959473
	model : 0.06968398094177246
			 train-loss:  2.1037246291599576 	 ± 0.22146143596525888
	data : 0.11501736640930176
	model : 0.06931881904602051
			 train-loss:  2.1023058239370584 	 ± 0.22001285507021703
	data : 0.11531434059143067
	model : 0.06821603775024414
			 train-loss:  2.103495852763836 	 ± 0.2185213691632965
	data : 0.11622328758239746
	model : 0.06818113327026368
			 train-loss:  2.103804535938032 	 ± 0.21687386519520854
	data : 0.11603689193725586
	model : 0.06836271286010742
			 train-loss:  2.10087583314127 	 ± 0.21656031753019378
	data : 0.11553792953491211
	model : 0.06931018829345703
			 train-loss:  2.093684466446147 	 ± 0.22287585060217543
	data : 0.1147587776184082
	model : 0.06943306922912598
			 train-loss:  2.0903598778489707 	 ± 0.22294692947293415
	data : 0.11487970352172852
	model : 0.0697542667388916
			 train-loss:  2.088913253375462 	 ± 0.22167466028817914
	data : 0.11499466896057128
	model : 0.06954631805419922
			 train-loss:  2.091342808495105 	 ± 0.2210446535484167
	data : 0.11522259712219238
	model : 0.06928586959838867
			 train-loss:  2.086590098010169 	 ± 0.22312750554998503
	data : 0.11550722122192383
	model : 0.06893882751464844
			 train-loss:  2.089703376979044 	 ± 0.22316304746350055
	data : 0.11581277847290039
	model : 0.06810936927795411
			 train-loss:  2.0931767837421313 	 ± 0.22362795368763735
	data : 0.11654505729675294
	model : 0.06804609298706055
			 train-loss:  2.09051118850708 	 ± 0.22331249023973998
	data : 0.11650452613830567
	model : 0.06837353706359864
			 train-loss:  2.087632921181227 	 ± 0.2232344864721041
	data : 0.11634507179260253
	model : 0.06841464042663574
			 train-loss:  2.0860212994860365 	 ± 0.22222475679884174
	data : 0.11613507270812988
	model : 0.06897635459899902
			 train-loss:  2.0839004134520507 	 ± 0.22157859662158735
	data : 0.11532931327819824
	model : 0.06979970932006836
			 train-loss:  2.0800921630255784 	 ± 0.22272585994350927
	data : 0.11442699432373046
	model : 0.06925559043884277
			 train-loss:  2.081460277736187 	 ± 0.22166323752492273
	data : 0.11510419845581055
	model : 0.06916847229003906
			 train-loss:  2.086438598456206 	 ± 0.2247458234496378
	data : 0.11523542404174805
	model : 0.06908273696899414
			 train-loss:  2.083109736442566 	 ± 0.2253714451732695
	data : 0.11571779251098632
	model : 0.06865949630737304
			 train-loss:  2.0831904468766176 	 ± 0.22401086397997044
	data : 0.11636576652526856
	model : 0.06861400604248047
			 train-loss:  2.082019614321845 	 ± 0.22292881456062857
	data : 0.11645240783691406
	model : 0.06825699806213378
			 train-loss:  2.084704114409054 	 ± 0.22297518287353474
	data : 0.1166614055633545
	model : 0.06829509735107422
			 train-loss:  2.0848387327305105 	 ± 0.22167849932934508
	data : 0.11668930053710938
	model : 0.06775841712951661
			 train-loss:  2.083350594016327 	 ± 0.22083243821473406
	data : 0.11726737022399902
	model : 0.06762375831604003
			 train-loss:  2.0809261148626153 	 ± 0.22073556652662607
	data : 0.11709179878234863
	model : 0.06762223243713379
			 train-loss:  2.076651389679212 	 ± 0.22312502269305093
	data : 0.11694245338439942
	model : 0.06765990257263184
			 train-loss:  2.0800523585743376 	 ± 0.22418973668666012
	data : 0.1168752670288086
	model : 0.06769776344299316
			 train-loss:  2.076308160037785 	 ± 0.22576631940491562
	data : 0.11658825874328613
	model : 0.06870923042297364
			 train-loss:  2.0788534788981727 	 ± 0.22584499447888504
	data : 0.11557965278625489
	model : 0.06917409896850586
			 train-loss:  2.0796831487327494 	 ± 0.22476841130166209
	data : 0.11548366546630859
	model : 0.06917281150817871
			 train-loss:  2.076464390501063 	 ± 0.22571420117384228
	data : 0.11545243263244628
	model : 0.07008066177368164
			 train-loss:  2.075902051674692 	 ± 0.22458927532953174
	data : 0.11452255249023438
	model : 0.07016701698303222
			 train-loss:  2.074890621006489 	 ± 0.2236338672861857
	data : 0.11453590393066407
	model : 0.07007350921630859
			 train-loss:  2.07527585127919 	 ± 0.22251014470449623
	data : 0.11451783180236816
	model : 0.06905078887939453
			 train-loss:  2.076495591475039 	 ± 0.22169769040272
	data : 0.11548643112182617
	model : 0.06820554733276367
			 train-loss:  2.071245166990492 	 ± 0.22661633756182784
	data : 0.11640791893005371
	model : 0.0682002067565918
			 train-loss:  2.0724281239509583 	 ± 0.22578740916025003
	data : 0.1162198543548584
	model : 0.06753401756286621
			 train-loss:  2.076922690514291 	 ± 0.22911856095344868
	data : 0.11681785583496093
	model : 0.06748137474060059
			 train-loss:  2.0738379312496558 	 ± 0.2300907315266209
	data : 0.11678524017333984
	model : 0.06818113327026368
			 train-loss:  2.0735840531228815 	 ± 0.22898541772631228
	data : 0.11613979339599609
	model : 0.06896462440490722
			 train-loss:  2.0734004894128213 	 ± 0.2278894820152867
	data : 0.11553850173950195
	model : 0.06802639961242676
			 train-loss:  2.0711582672028315 	 ± 0.22795147824055975
	data : 0.1166494369506836
	model : 0.06852097511291504
			 train-loss:  2.0717923427527807 	 ± 0.22696670568596933
	data : 0.11630682945251465
	model : 0.06803703308105469
			 train-loss:  2.071961646882173 	 ± 0.2259103487668167
	data : 0.11665802001953125
	model : 0.06825838088989258
			 train-loss:  2.068383095441041 	 ± 0.22788852569011112
	data : 0.11640539169311523
	model : 0.06758246421813965
			 train-loss:  2.0689261812682544 	 ± 0.2269109576608281
	data : 0.11703195571899414
	model : 0.06843523979187012
			 train-loss:  2.0699441172859885 	 ± 0.22612706560514798
	data : 0.11639699935913086
	model : 0.06914319992065429
			 train-loss:  2.06992185652793 	 ± 0.22510629177977381
	data : 0.11563916206359863
	model : 0.06936378479003906
			 train-loss:  2.0709235646895 	 ± 0.22434746720200407
	data : 0.11551346778869628
	model : 0.06898117065429688
			 train-loss:  2.070380875494628 	 ± 0.22342640242185427
	data : 0.11577105522155762
	model : 0.06890859603881835
			 train-loss:  2.0695403155527616 	 ± 0.22262368996773485
	data : 0.11573591232299804
	model : 0.0690767765045166
			 train-loss:  2.0676096314969272 	 ± 0.22261015051038363
	data : 0.11557302474975586
	model : 0.0675872802734375
			 train-loss:  2.0705445470481085 	 ± 0.2238719663042154
	data : 0.11707148551940919
	model : 0.06712274551391602
			 train-loss:  2.0723190491016092 	 ± 0.2237309995933252
	data : 0.11757960319519042
	model : 0.06644377708435059
			 train-loss:  2.0730685925079606 	 ± 0.22292844795741265
	data : 0.11804509162902832
	model : 0.06633663177490234
			 train-loss:  2.076752027543653 	 ± 0.22556697039094992
	data : 0.11814718246459961
	model : 0.0655524730682373
			 train-loss:  2.078601378202438 	 ± 0.22552925743421687
	data : 0.11874313354492187
	model : 0.06643815040588379
			 train-loss:  2.07578457781106 	 ± 0.2267051182366703
	data : 0.11796741485595703
	model : 0.06715173721313476
			 train-loss:  2.077168070879139 	 ± 0.22628640886669613
	data : 0.11744732856750488
	model : 0.0678903579711914
			 train-loss:  2.076822181058124 	 ± 0.22539704899512955
	data : 0.11694598197937012
	model : 0.06867294311523438
			 train-loss:  2.076105945533322 	 ± 0.22462684516979428
	data : 0.11592860221862793
	model : 0.06899666786193848
			 train-loss:  2.0753672313690186 	 ± 0.22387770867779241
	data : 0.1154454231262207
	model : 0.06846990585327148
			 train-loss:  2.0772474285156006 	 ± 0.22397618975964056
	data : 0.11574444770812989
	model : 0.06848306655883789
			 train-loss:  2.082945746699656 	 ± 0.23208114001057245
	data : 0.1156393051147461
	model : 0.06785907745361328
			 train-loss:  2.0863783564418554 	 ± 0.2343870264950647
	data : 0.11627430915832519
	model : 0.06816558837890625
			 train-loss:  2.085684109103772 	 ± 0.23360886364721326
	data : 0.11630387306213379
	model : 0.06898007392883301
			 train-loss:  2.084020201059488 	 ± 0.23347474423582237
	data : 0.11561222076416015
	model : 0.06950411796569825
			 train-loss:  2.0831770805912164 	 ± 0.23278049020367672
	data : 0.11517214775085449
	model : 0.06863632202148437
			 train-loss:  2.0845239397251243 	 ± 0.23240888269487878
	data : 0.11589269638061524
	model : 0.06960020065307618
			 train-loss:  2.084446837131242 	 ± 0.23153521115820114
	data : 0.1147840976715088
	model : 0.06952242851257324
			 train-loss:  2.0870182389643657 	 ± 0.23256805552940732
	data : 0.1146615982055664
	model : 0.06904964447021485
			 train-loss:  2.0852498010352805 	 ± 0.23260764665262904
	data : 0.11522603034973145
	model : 0.06901779174804687
			 train-loss:  2.0845481481622246 	 ± 0.23189424200453054
	data : 0.11514129638671874
	model : 0.06966314315795899
			 train-loss:  2.0847988050349437 	 ± 0.23106485322198267
	data : 0.11435122489929199
	model : 0.06959357261657714
			 train-loss:  2.0882811554964036 	 ± 0.23380642014828798
	data : 0.11468009948730469
	model : 0.06921844482421875
			 train-loss:  2.0906983785492055 	 ± 0.23468808028162438
	data : 0.11526885032653808
	model : 0.0684537410736084
			 train-loss:  2.089074618475778 	 ± 0.2346306973874687
	data : 0.11603050231933594
	model : 0.06818680763244629
			 train-loss:  2.090243588102625 	 ± 0.2342059694578578
	data : 0.11644387245178223
	model : 0.06787471771240235
			 train-loss:  2.0903218816703473 	 ± 0.2333816952584019
	data : 0.11686973571777344
	model : 0.06687889099121094
			 train-loss:  2.0899256259411363 	 ± 0.2326121753084987
	data : 0.11759910583496094
	model : 0.06710515022277833
			 train-loss:  2.0869554670320616 	 ± 0.23450841102348136
	data : 0.1171915054321289
	model : 0.0676389217376709
			 train-loss:  2.0851796816135275 	 ± 0.23466788473563482
	data : 0.11683001518249511
	model : 0.06803998947143555
			 train-loss:  2.084964284341629 	 ± 0.2338772295503
	data : 0.11651625633239746
	model : 0.06882338523864746
			 train-loss:  2.0854139206360798 	 ± 0.23314368250737194
	data : 0.11610355377197265
	model : 0.06975598335266113
			 train-loss:  2.084669055165471 	 ± 0.23253013953463075
	data : 0.1154407024383545
	model : 0.06956753730773926
			 train-loss:  2.0837169997644103 	 ± 0.23203776999907938
	data : 0.1159358024597168
	model : 0.07017502784729004
			 train-loss:  2.0835145783424376 	 ± 0.2312762166439738
	data : 0.11547970771789551
	model : 0.07007908821105957
			 train-loss:  2.0835683085271066 	 ± 0.230510068545079
	data : 0.11550979614257813
	model : 0.06963210105895996
			 train-loss:  2.083991164439603 	 ± 0.22980931208344094
	data : 0.11576304435729981
	model : 0.0687406063079834
			 train-loss:  2.0833890656240626 	 ± 0.22917732185408882
	data : 0.11635351181030273
	model : 0.06901011466979981
			 train-loss:  2.0832239234602294 	 ± 0.22844116073981804
	data : 0.11589298248291016
	model : 0.06863117218017578
			 train-loss:  2.0848626505944035 	 ± 0.22860936081545613
	data : 0.11602950096130371
	model : 0.06876072883605958
			 train-loss:  2.0836880826033077 	 ± 0.22834418109316346
	data : 0.11608171463012695
	model : 0.06802005767822265
			 train-loss:  2.085340180214803 	 ± 0.22854922201001784
	data : 0.11676163673400879
	model : 0.06796684265136718
			 train-loss:  2.085783409921429 	 ± 0.2278924971812661
	data : 0.11670107841491699
	model : 0.06798810958862304
			 train-loss:  2.089303824136842 	 ± 0.2314443757790847
	data : 0.1166308879852295
	model : 0.06794939041137696
			 train-loss:  2.0898792259395123 	 ± 0.2308340341691616
	data : 0.11649322509765625
	model : 0.06782536506652832
			 train-loss:  2.0934188936067666 	 ± 0.23443138053176313
	data : 0.11654829978942871
	model : 0.06779012680053711
			 train-loss:  2.0936468552660057 	 ± 0.23372460595392194
	data : 0.11655364036560059
	model : 0.06897697448730469
			 train-loss:  2.093309586033499 	 ± 0.23304609602240878
	data : 0.11586012840270996
	model : 0.06807532310485839
			 train-loss:  2.0962229567330057 	 ± 0.23529305385008795
	data : 0.11663565635681153
	model : 0.0681727409362793
			 train-loss:  2.094004309538639 	 ± 0.2362933807402298
	data : 0.11646814346313476
	model : 0.06834049224853515
			 train-loss:  2.096738573298397 	 ± 0.23818434398725233
	data : 0.11631464958190918
	model : 0.069392728805542
			 train-loss:  2.095896673059749 	 ± 0.23771775405050027
	data : 0.11534652709960938
	model : 0.06837606430053711
			 train-loss:  2.095648942249162 	 ± 0.2370308250616891
	data : 0.11621484756469727
	model : 0.06947464942932129
			 train-loss:  2.095885340984051 	 ± 0.23634837252191718
	data : 0.11532754898071289
	model : 0.06921982765197754
			 train-loss:  2.095672076589921 	 ± 0.23566851319594798
	data : 0.1155998706817627
	model : 0.06902899742126464
			 train-loss:  2.098020014707108 	 ± 0.23696420220476275
	data : 0.1156280517578125
	model : 0.0688044548034668
			 train-loss:  2.0963695534439974 	 ± 0.23725803599629738
	data : 0.11576852798461915
	model : 0.06979923248291016
			 train-loss:  2.097199340776212 	 ± 0.23682149830082122
	data : 0.11502056121826172
	model : 0.06981992721557617
			 train-loss:  2.0975795079921853 	 ± 0.23619293169597894
	data : 0.11504435539245605
	model : 0.07002167701721192
			 train-loss:  2.0953025790623254 	 ± 0.2374245229123058
	data : 0.11469154357910157
	model : 0.07021212577819824
			 train-loss:  2.09428569809957 	 ± 0.23713092564971303
	data : 0.1146928310394287
	model : 0.0703232765197754
			 train-loss:  2.0934273166171575 	 ± 0.2367341680208999
	data : 0.1145937442779541
	model : 0.06963958740234374
			 train-loss:  2.092387129751484 	 ± 0.2364735283791146
	data : 0.11501874923706054
	model : 0.06938157081604004
			 train-loss:  2.09238919199512 	 ± 0.2358120642734765
	data : 0.11539125442504883
	model : 0.06916203498840331
			 train-loss:  2.090925523969862 	 ± 0.23597007449991214
	data : 0.11595706939697266
	model : 0.06899700164794922
			 train-loss:  2.088953446288135 	 ± 0.23680007950596235
	data : 0.11592903137207031
	model : 0.06936540603637695
			 train-loss:  2.0896780910072748 	 ± 0.236349787670191
	data : 0.11569375991821289
	model : 0.06962509155273437
			 train-loss:  2.0877760427245677 	 ± 0.2370957758115561
	data : 0.11549172401428223
	model : 0.07017922401428223
			 train-loss:  2.0877964166195495 	 ± 0.23645077667277356
	data : 0.11509647369384765
	model : 0.0704376220703125
			 train-loss:  2.087337515160844 	 ± 0.2358930007495527
	data : 0.11501679420471192
	model : 0.07049312591552734
			 train-loss:  2.089394902029345 	 ± 0.2369164703862439
	data : 0.11512222290039062
	model : 0.0699418067932129
			 train-loss:  2.090302427184773 	 ± 0.2366061000932062
	data : 0.11532502174377442
	model : 0.06965250968933105
			 train-loss:  2.091023879482391 	 ± 0.23618213289567394
	data : 0.11545920372009277
	model : 0.06858501434326172
			 train-loss:  2.0902101804339694 	 ± 0.23582055253332795
	data : 0.11614842414855957
	model : 0.06852488517761231
			 train-loss:  2.0888637988190903 	 ± 0.23592636623196395
	data : 0.11608490943908692
	model : 0.06848058700561524
			 train-loss:  2.088122427151465 	 ± 0.2355297437592175
	data : 0.11619672775268555
	model : 0.0686572551727295
			 train-loss:  2.088200503960252 	 ± 0.23491806250368868
	data : 0.11627235412597656
	model : 0.0691418170928955
			 train-loss:  2.0870433057528084 	 ± 0.23485668901048412
	data : 0.11584181785583496
	model : 0.06965694427490235
			 train-loss:  2.087413069513655 	 ± 0.2343069236844483
	data : 0.11544575691223144
	model : 0.06984634399414062
			 train-loss:  2.090008958792075 	 ± 0.2364857157817941
	data : 0.11520476341247558
	model : 0.06997108459472656
			 train-loss:  2.090382599708985 	 ± 0.23593936315551428
	data : 0.1149515151977539
	model : 0.06995158195495606
			 train-loss:  2.090531383674157 	 ± 0.23534898836526547
	data : 0.11501240730285645
	model : 0.07020573616027832
			 train-loss:  2.090853258214816 	 ± 0.23479738711413214
	data : 0.11480607986450195
	model : 0.06935906410217285
			 train-loss:  2.091870485837735 	 ± 0.2346436866032349
	data : 0.11543869972229004
	model : 0.06822237968444825
			 train-loss:  2.091391800045967 	 ± 0.23415373206892137
	data : 0.11648659706115723
	model : 0.06797752380371094
			 train-loss:  2.0909657282615774 	 ± 0.23364824352762106
	data : 0.11680536270141602
	model : 0.06805272102355957
			 train-loss:  2.091099799859642 	 ± 0.23307693956822284
	data : 0.11686334609985352
	model : 0.0678861141204834
			 train-loss:  2.0923542077905437 	 ± 0.23318470048832898
	data : 0.11712775230407715
	model : 0.06869940757751465
			 train-loss:  2.093218781784469 	 ± 0.23293840390646606
	data : 0.11640477180480957
	model : 0.06955699920654297
			 train-loss:  2.09339957993205 	 ± 0.23238391504166492
	data : 0.11559019088745118
	model : 0.06891980171203613
			 train-loss:  2.0946428607968453 	 ± 0.23250164549702682
	data : 0.11612358093261718
	model : 0.06906933784484863
			 train-loss:  2.096766156850806 	 ± 0.2339328914483966
	data : 0.11594290733337402
	model : 0.06820793151855468
			 train-loss:  2.097050316631794 	 ± 0.23340568385900734
	data : 0.11667828559875489
	model : 0.06811590194702148
			 train-loss:  2.0966360506258512 	 ± 0.23292326664711535
	data : 0.11686396598815918
	model : 0.06785993576049805
			 train-loss:  2.0959206200781324 	 ± 0.23259809562077127
	data : 0.11709532737731934
	model : 0.06828494071960449
			 train-loss:  2.0969198365912054 	 ± 0.2324976089438717
	data : 0.11660995483398437
	model : 0.06775007247924805
			 train-loss:  2.0955755778078764 	 ± 0.23276908038130154
	data : 0.11686196327209472
	model : 0.06845831871032715
			 train-loss:  2.0952849332155754 	 ± 0.23226058716131082
	data : 0.11645016670227051
	model : 0.06775836944580078
			 train-loss:  2.094078036112206 	 ± 0.23238579331093767
	data : 0.11723113059997559
	model : 0.06805448532104492
			 train-loss:  2.092558906799139 	 ± 0.23290736095449924
	data : 0.11683135032653809
	model : 0.06749391555786133
			 train-loss:  2.0929360064091505 	 ± 0.2324333765386766
	data : 0.11752219200134277
	model : 0.06780004501342773
			 train-loss:  2.092998975432963 	 ± 0.23189904396434582
	data : 0.11742153167724609
	model : 0.06740589141845703
			 train-loss:  2.093632052250958 	 ± 0.2315544276385822
	data : 0.1175774097442627
	model : 0.06754131317138672
			 train-loss:  2.0912440084431267 	 ± 0.2337002879981915
	data : 0.11737284660339356
	model : 0.06783199310302734
			 train-loss:  2.092552055012096 	 ± 0.2339706749963166
	data : 0.11728320121765137
	model : 0.06840882301330567
			 train-loss:  2.092696338757131 	 ± 0.23345053879626007
	data : 0.11652669906616211
	model : 0.06811366081237794
			 train-loss:  2.093506676656706 	 ± 0.23323546359704544
	data : 0.11668438911437988
	model : 0.06855335235595703
			 train-loss:  2.0945748128163975 	 ± 0.2332554905102095
	data : 0.1162717342376709
	model : 0.06861286163330078
			 train-loss:  2.0970545964581624 	 ± 0.23566189837142118
	data : 0.11609029769897461
	model : 0.06838059425354004
			 train-loss:  2.096925835079617 	 ± 0.23514551904430234
	data : 0.11618108749389648
	model : 0.06858434677124023
			 train-loss:  2.0958311721286944 	 ± 0.23519857263036942
	data : 0.11600208282470703
	model : 0.06816611289978028
			 train-loss:  2.095267950700768 	 ± 0.23483263543838234
	data : 0.11613693237304687
	model : 0.06739411354064942
			 train-loss:  2.0964911428459905 	 ± 0.2350407067215665
	data : 0.11675472259521484
	model : 0.06758217811584473
			 train-loss:  2.095855074678446 	 ± 0.23472353507027974
	data : 0.11639981269836426
	model : 0.06739435195922852
			 train-loss:  2.093541648076928 	 ± 0.2368146716373237
	data : 0.11658673286437989
	model : 0.06720151901245117
			 train-loss:  2.0930357547033402 	 ± 0.23642604860352226
	data : 0.11680941581726074
	model : 0.06766486167907715
			 train-loss:  2.0938938928061517 	 ± 0.23627621241594518
	data : 0.1166184425354004
	model : 0.06826395988464355
			 train-loss:  2.092418939770547 	 ± 0.23683657428711885
	data : 0.11601967811584472
	model : 0.06851530075073242
			 train-loss:  2.0906211035883326 	 ± 0.2379179721391056
	data : 0.11598086357116699
	model : 0.06846079826354981
			 train-loss:  2.0906043397619367 	 ± 0.23741136251322237
	data : 0.11579322814941406
	model : 0.06843857765197754
			 train-loss:  2.0913207864357255 	 ± 0.2371622828496699
	data : 0.1156428337097168
	model : 0.06838111877441407
			 train-loss:  2.091715964586926 	 ± 0.23673926343478865
	data : 0.11558609008789063
	model : 0.06831598281860352
			 train-loss:  2.0904269599113143 	 ± 0.23707335780328756
	data : 0.11562418937683105
	model : 0.06856207847595215
			 train-loss:  2.09243224056196 	 ± 0.23859096493676402
	data : 0.11552538871765136
	model : 0.06865839958190918
			 train-loss:  2.092189382513364 	 ± 0.23812298203818813
	data : 0.11575436592102051
	model : 0.06891870498657227
			 train-loss:  2.092380525660218 	 ± 0.2376468868593524
	data : 0.11570076942443848
	model : 0.06914029121398926
			 train-loss:  2.091418172209716 	 ± 0.2376254756535285
	data : 0.11558599472045898
	model : 0.0691411018371582
			 train-loss:  2.0919151605402 	 ± 0.23726202862825163
	data : 0.11559014320373535
	model : 0.06889047622680664
			 train-loss:  2.0926210846080155 	 ± 0.23703091359727407
	data : 0.11570839881896973
	model : 0.06881299018859863
			 train-loss:  2.0922914821274428 	 ± 0.236602706210325
	data : 0.11578426361083985
	model : 0.06879134178161621
			 train-loss:  2.0913745673691353 	 ± 0.2365570876100599
	data : 0.11583156585693359
	model : 0.06868610382080079
			 train-loss:  2.0924343671875927 	 ± 0.23666220910539557
	data : 0.11596493721008301
	model : 0.06829137802124023
			 train-loss:  2.091657153060359 	 ± 0.2365002362288263
	data : 0.11630182266235352
	model : 0.06812448501586914
			 train-loss:  2.0924627091511185 	 ± 0.23636553490329232
	data : 0.11604704856872558
	model : 0.06795449256896972
			 train-loss:  2.092852216720581 	 ± 0.23597238979651725
	data : 0.11595020294189454
	model : 0.0676912784576416
			 train-loss:  2.0929347433416967 	 ± 0.23550547108311634
	data : 0.11598644256591797
	model : 0.06757616996765137
			 train-loss:  2.0932816238630387 	 ± 0.23510197399686314
	data : 0.11601319313049316
	model : 0.06801433563232422
			 train-loss:  2.0946798371703257 	 ± 0.235684382554134
	data : 0.11562504768371581
	model : 0.06794705390930175
			 train-loss:  2.0948145962136935 	 ± 0.23522974552305612
	data : 0.11605486869812012
	model : 0.06775197982788086
			 train-loss:  2.094677198634428 	 ± 0.23477826970983076
	data : 0.11639194488525391
	model : 0.06728286743164062
			 train-loss:  2.0954310595989227 	 ± 0.23462829812879485
	data : 0.11592459678649902
	model : 0.05813755989074707
#epoch  14    val-loss:  2.451656542326275  train-loss:  2.0954310595989227  lr:  0.0025
			 train-loss:  2.076019763946533 	 ± 0.0
	data : 5.473679304122925
	model : 0.08478879928588867
			 train-loss:  2.0586496591567993 	 ± 0.017370104789733887
	data : 2.804680585861206
	model : 0.08054304122924805
			 train-loss:  2.16251269976298 	 ± 0.1475676435905239
	data : 1.9087144533793132
	model : 0.07627749443054199
			 train-loss:  2.1878512501716614 	 ± 0.1351232159422827
	data : 1.4606136679649353
	model : 0.07462364435195923
			 train-loss:  2.1144564390182494 	 ± 0.19014157884652103
	data : 1.1915835380554198
	model : 0.07309222221374512
			 train-loss:  2.1120686729749045 	 ± 0.1736568180980193
	data : 0.1203498363494873
	model : 0.07034845352172851
			 train-loss:  2.0810016904558455 	 ± 0.17787505797643108
	data : 0.11602954864501953
	model : 0.06907854080200196
			 train-loss:  2.0711717903614044 	 ± 0.16840719334375454
	data : 0.11542057991027832
	model : 0.06938467025756836
			 train-loss:  2.063125822279188 	 ± 0.16039845161719007
	data : 0.11520123481750488
	model : 0.06925730705261231
			 train-loss:  2.057188892364502 	 ± 0.15320613951268383
	data : 0.11529803276062012
	model : 0.06975421905517579
			 train-loss:  2.1100668690421363 	 ± 0.22203399775936233
	data : 0.11495966911315918
	model : 0.06927838325500488
			 train-loss:  2.1344687740008035 	 ± 0.2274660870752332
	data : 0.11543488502502441
	model : 0.0689852237701416
			 train-loss:  2.146821480530959 	 ± 0.22269223142412917
	data : 0.11589488983154297
	model : 0.06823205947875977
			 train-loss:  2.156786016055516 	 ± 0.21757838117932538
	data : 0.11646137237548829
	model : 0.06825823783874511
			 train-loss:  2.1388118187586467 	 ± 0.22069737934868267
	data : 0.11648678779602051
	model : 0.06827902793884277
			 train-loss:  2.133300967514515 	 ± 0.21475257191635574
	data : 0.11644606590270996
	model : 0.06758298873901367
			 train-loss:  2.1314485283458935 	 ± 0.2084723215811816
	data : 0.11709518432617187
	model : 0.06783552169799804
			 train-loss:  2.0981346435017056 	 ± 0.24477148268862964
	data : 0.1170656681060791
	model : 0.06874094009399415
			 train-loss:  2.108577947867544 	 ± 0.24232805391334386
	data : 0.11650505065917968
	model : 0.0679743766784668
			 train-loss:  2.0935259640216826 	 ± 0.24513552081243223
	data : 0.1171645164489746
	model : 0.0671755313873291
			 train-loss:  2.106237678300767 	 ± 0.24588957034013073
	data : 0.11794819831848144
	model : 0.0680783748626709
			 train-loss:  2.0900033549828962 	 ± 0.25149163188693
	data : 0.11706771850585937
	model : 0.06813745498657227
			 train-loss:  2.0827831392702847 	 ± 0.24828415055258804
	data : 0.11699576377868652
	model : 0.06897811889648438
			 train-loss:  2.090468148390452 	 ± 0.24583499006252457
	data : 0.11617879867553711
	model : 0.07022924423217773
			 train-loss:  2.102450532913208 	 ± 0.24791794928546065
	data : 0.11508641242980958
	model : 0.0710799217224121
			 train-loss:  2.1080990387843204 	 ± 0.24473858828759287
	data : 0.11430792808532715
	model : 0.07106852531433105
			 train-loss:  2.1002355372464216 	 ± 0.24348772273715194
	data : 0.11416373252868653
	model : 0.07102756500244141
			 train-loss:  2.0870779837880815 	 ± 0.24868288451182533
	data : 0.11415543556213378
	model : 0.0702021598815918
			 train-loss:  2.0870638140316666 	 ± 0.24435764633932847
	data : 0.11503748893737793
	model : 0.0695760726928711
			 train-loss:  2.0895464579264322 	 ± 0.24062220619514332
	data : 0.11554555892944336
	model : 0.06970844268798829
			 train-loss:  2.0906034900296118 	 ± 0.2367801817197923
	data : 0.11538071632385254
	model : 0.06950325965881347
			 train-loss:  2.090201713144779 	 ± 0.23306186285652103
	data : 0.11563024520874024
	model : 0.06863446235656738
			 train-loss:  2.0847949909441397 	 ± 0.2315324634907544
	data : 0.11634430885314942
	model : 0.06864900588989258
			 train-loss:  2.086692550603081 	 ± 0.2283624773762928
	data : 0.11622295379638672
	model : 0.0689206600189209
			 train-loss:  2.099689381463187 	 ± 0.23749239237501396
	data : 0.11603446006774902
	model : 0.06935458183288574
			 train-loss:  2.0924193461736045 	 ± 0.2380877275163918
	data : 0.11556096076965332
	model : 0.06961593627929688
			 train-loss:  2.0845362979012565 	 ± 0.23956386719758915
	data : 0.11532740592956543
	model : 0.07054281234741211
			 train-loss:  2.0943430881751213 	 ± 0.24380108089573074
	data : 0.11449966430664063
	model : 0.07052130699157715
			 train-loss:  2.0852534740399093 	 ± 0.24709207231850575
	data : 0.11459059715270996
	model : 0.07021293640136719
			 train-loss:  2.087520036101341 	 ± 0.24439411764022048
	data : 0.1149704933166504
	model : 0.06945648193359374
			 train-loss:  2.096393925387685 	 ± 0.24783367178236962
	data : 0.11576166152954101
	model : 0.06840200424194336
			 train-loss:  2.0832399725914 	 ± 0.25894632060708406
	data : 0.11673774719238281
	model : 0.06867990493774415
			 train-loss:  2.0815274022346317 	 ± 0.25615815810955866
	data : 0.11644248962402344
	model : 0.06882696151733399
			 train-loss:  2.0851368714462626 	 ± 0.25433427215066806
	data : 0.1163914680480957
	model : 0.06937308311462402
			 train-loss:  2.0988333834542168 	 ± 0.2673997241385774
	data : 0.11594820022583008
	model : 0.06865320205688477
			 train-loss:  2.104212400705918 	 ± 0.2669273872183632
	data : 0.1163823127746582
	model : 0.06952815055847168
			 train-loss:  2.106610671002814 	 ± 0.26457295010067183
	data : 0.11541099548339843
	model : 0.06818242073059082
			 train-loss:  2.1049946074684462 	 ± 0.26203680031282495
	data : 0.11663641929626464
	model : 0.06723365783691407
			 train-loss:  2.1062969309943065 	 ± 0.2595060758989871
	data : 0.11734418869018555
	model : 0.06693158149719239
			 train-loss:  2.1010302257537843 	 ± 0.2595297793709956
	data : 0.11741099357604981
	model : 0.06773104667663574
			 train-loss:  2.1028116871328915 	 ± 0.2572813372312791
	data : 0.11689419746398926
	model : 0.06782803535461426
			 train-loss:  2.1059704973147464 	 ± 0.2557921287845942
	data : 0.11704888343811035
	model : 0.06850838661193848
			 train-loss:  2.1084098006194494 	 ± 0.25397736626951695
	data : 0.11603751182556152
	model : 0.06832394599914551
			 train-loss:  2.1081050678535744 	 ± 0.2516245147850311
	data : 0.11632113456726074
	model : 0.06833767890930176
			 train-loss:  2.1121701327237217 	 ± 0.2511096436519997
	data : 0.11629438400268555
	model : 0.06841259002685547
			 train-loss:  2.1161451126847948 	 ± 0.2505974417251616
	data : 0.11626873016357422
	model : 0.0677462100982666
			 train-loss:  2.1234988706153737 	 ± 0.25441244842082533
	data : 0.11688723564147949
	model : 0.06787171363830566
			 train-loss:  2.1236807847845145 	 ± 0.2522134412739085
	data : 0.1172095775604248
	model : 0.06887784004211425
			 train-loss:  2.1261747772410766 	 ± 0.2507871942797866
	data : 0.1161409854888916
	model : 0.06914811134338379
			 train-loss:  2.120684732993444 	 ± 0.25223852685985976
	data : 0.11608123779296875
	model : 0.0685089111328125
			 train-loss:  2.1128851644328384 	 ± 0.2573543271911168
	data : 0.11677236557006836
	model : 0.06913743019104004
			 train-loss:  2.1126048584138193 	 ± 0.255279839822937
	data : 0.11637191772460938
	model : 0.06931958198547364
			 train-loss:  2.112839515247042 	 ± 0.253252445412254
	data : 0.1162111759185791
	model : 0.06898179054260253
			 train-loss:  2.1102250907570124 	 ± 0.25212156226921895
	data : 0.11636285781860352
	model : 0.06869688034057617
			 train-loss:  2.1108618387809166 	 ± 0.2502265042279685
	data : 0.11647701263427734
	model : 0.06930809020996094
			 train-loss:  2.103951239224636 	 ± 0.25449711610840536
	data : 0.11565074920654297
	model : 0.06938204765319825
			 train-loss:  2.104492609180621 	 ± 0.2526290308761343
	data : 0.11554045677185058
	model : 0.06895098686218262
			 train-loss:  2.102717985125149 	 ± 0.2511849503988431
	data : 0.11591534614562989
	model : 0.06876668930053711
			 train-loss:  2.0981561446535415 	 ± 0.2521796615037672
	data : 0.11627907752990722
	model : 0.0687777042388916
			 train-loss:  2.096234895501818 	 ± 0.2508800118285959
	data : 0.11625876426696777
	model : 0.0687671184539795
			 train-loss:  2.0920448001001923 	 ± 0.25156166924726886
	data : 0.11641411781311035
	model : 0.06880550384521485
			 train-loss:  2.086874438656701 	 ± 0.2535790928696921
	data : 0.11637353897094727
	model : 0.06929931640625
			 train-loss:  2.0867138986718166 	 ± 0.25183994481895616
	data : 0.11593875885009766
	model : 0.06985750198364257
			 train-loss:  2.092848194612039 	 ± 0.2555645617675276
	data : 0.11550893783569335
	model : 0.06977910995483398
			 train-loss:  2.0871808751424155 	 ± 0.2584940457718775
	data : 0.1153337001800537
	model : 0.07002382278442383
			 train-loss:  2.083022876789695 	 ± 0.2593002956891488
	data : 0.1151517391204834
	model : 0.06996693611145019
			 train-loss:  2.085396265054678 	 ± 0.2584406044824328
	data : 0.11520366668701172
	model : 0.06982579231262206
			 train-loss:  2.087495825229547 	 ± 0.2574386760926149
	data : 0.11536450386047363
	model : 0.06886768341064453
			 train-loss:  2.0933529666707487 	 ± 0.2609820459485539
	data : 0.11618270874023437
	model : 0.06881861686706543
			 train-loss:  2.0961789220571516 	 ± 0.2605592616612943
	data : 0.11641831398010254
	model : 0.06867480278015137
			 train-loss:  2.0943636776488503 	 ± 0.2594543808341171
	data : 0.11636919975280761
	model : 0.0686264991760254
			 train-loss:  2.0910023145559355 	 ± 0.25963597900297897
	data : 0.11632213592529297
	model : 0.0686708927154541
			 train-loss:  2.09684823076409 	 ± 0.2634406786282455
	data : 0.11638398170471191
	model : 0.07031927108764649
			 train-loss:  2.0929499495597113 	 ± 0.26426521976484335
	data : 0.11474127769470215
	model : 0.0696195125579834
			 train-loss:  2.093512605218326 	 ± 0.2627567277209796
	data : 0.11548519134521484
	model : 0.06879029273986817
			 train-loss:  2.0902146211890287 	 ± 0.26298823905992996
	data : 0.11620535850524902
	model : 0.0680990219116211
			 train-loss:  2.0896845137935944 	 ± 0.2615186536420958
	data : 0.11687312126159669
	model : 0.06902856826782226
			 train-loss:  2.090286135673523 	 ± 0.26008904987950593
	data : 0.11580100059509277
	model : 0.0681567668914795
			 train-loss:  2.0914641230293873 	 ± 0.25885972358678455
	data : 0.11670560836791992
	model : 0.06933813095092774
			 train-loss:  2.0928421126471624 	 ± 0.2577456446391458
	data : 0.11570377349853515
	model : 0.07038464546203613
			 train-loss:  2.0932769067994843 	 ± 0.2563587340168342
	data : 0.11497912406921387
	model : 0.07110056877136231
			 train-loss:  2.0959244681441267 	 ± 0.25620953675992225
	data : 0.1142507553100586
	model : 0.07026047706604004
			 train-loss:  2.0943117718542776 	 ± 0.25529738957069026
	data : 0.11509404182434083
	model : 0.07077598571777344
			 train-loss:  2.090734077260849 	 ± 0.25626895624549134
	data : 0.11474447250366211
	model : 0.06978654861450195
			 train-loss:  2.088101069550765 	 ± 0.2561916306049994
	data : 0.11562533378601074
	model : 0.06956720352172852
			 train-loss:  2.087818782776594 	 ± 0.2548686576771209
	data : 0.11564688682556153
	model : 0.06872749328613281
			 train-loss:  2.0888504748491896 	 ± 0.25375291825989976
	data : 0.11633520126342774
	model : 0.06780538558959961
			 train-loss:  2.088850214773295 	 ± 0.25245494086959785
	data : 0.11727509498596192
	model : 0.06751885414123535
			 train-loss:  2.088990650995813 	 ± 0.2511805272296898
	data : 0.11750144958496093
	model : 0.06839594841003419
			 train-loss:  2.0922215688228607 	 ± 0.2519805247579648
	data : 0.11671295166015624
	model : 0.06831727027893067
			 train-loss:  2.0899054130705275 	 ± 0.2517975125146971
	data : 0.11694669723510742
	model : 0.0690737247467041
			 train-loss:  2.086726863010257 	 ± 0.25258824285142584
	data : 0.11617054939270019
	model : 0.06985006332397461
			 train-loss:  2.0847547158454227 	 ± 0.2521470010898208
	data : 0.11538815498352051
	model : 0.06920585632324219
			 train-loss:  2.08704175055027 	 ± 0.2520030288183796
	data : 0.1159705638885498
	model : 0.06963229179382324
			 train-loss:  2.084757543745495 	 ± 0.2518796190064362
	data : 0.11536865234375
	model : 0.06929917335510254
			 train-loss:  2.0845110168996848 	 ± 0.2507014196171316
	data : 0.11552133560180664
	model : 0.06937189102172851
			 train-loss:  2.083128598248847 	 ± 0.24993275497479922
	data : 0.11548438072204589
	model : 0.07056603431701661
			 train-loss:  2.0822573513896376 	 ± 0.24893615685528103
	data : 0.1141937255859375
	model : 0.07102088928222657
			 train-loss:  2.086932074039354 	 ± 0.25250903501803895
	data : 0.113734769821167
	model : 0.07016096115112305
			 train-loss:  2.0890856992114673 	 ± 0.252362283362548
	data : 0.1146629810333252
	model : 0.06962614059448242
			 train-loss:  2.0895242014446773 	 ± 0.2512650375083426
	data : 0.11523571014404296
	model : 0.06938252449035645
			 train-loss:  2.08979334043605 	 ± 0.25015687469219633
	data : 0.11563615798950196
	model : 0.06823310852050782
			 train-loss:  2.0884840393488386 	 ± 0.24943269317149375
	data : 0.11685266494750976
	model : 0.06818137168884278
			 train-loss:  2.092437968965162 	 ± 0.2518680217631587
	data : 0.11684651374816894
	model : 0.06837630271911621
			 train-loss:  2.09062776669212 	 ± 0.2515142716178218
	data : 0.11658153533935547
	model : 0.06865372657775878
			 train-loss:  2.0908046909447373 	 ± 0.25043499890339305
	data : 0.11643490791320801
	model : 0.06909823417663574
			 train-loss:  2.0894637362569823 	 ± 0.24978035662084647
	data : 0.11628837585449218
	model : 0.06920561790466309
			 train-loss:  2.086537334878566 	 ± 0.250725872913748
	data : 0.11636214256286621
	model : 0.06930704116821289
			 train-loss:  2.0866114051402116 	 ± 0.24967147683727797
	data : 0.1163743019104004
	model : 0.06943659782409668
			 train-loss:  2.0875612517197926 	 ± 0.24884481866081548
	data : 0.11634025573730469
	model : 0.06995916366577148
			 train-loss:  2.0840142571236475 	 ± 0.2508420175661768
	data : 0.11596002578735351
	model : 0.06999039649963379
			 train-loss:  2.083304795085407 	 ± 0.2499337306048221
	data : 0.11575198173522949
	model : 0.07010278701782227
			 train-loss:  2.0833946834734784 	 ± 0.248917646452287
	data : 0.11547727584838867
	model : 0.06981902122497559
			 train-loss:  2.085729580733084 	 ± 0.24926067078421615
	data : 0.11558837890625
	model : 0.06915912628173829
			 train-loss:  2.088215912818909 	 ± 0.24980068855326087
	data : 0.11609053611755371
	model : 0.06841478347778321
			 train-loss:  2.087739194196368 	 ± 0.24886452225949635
	data : 0.1165764331817627
	model : 0.06750283241271973
			 train-loss:  2.0874485415736523 	 ± 0.24790427393510023
	data : 0.11731867790222168
	model : 0.06661338806152343
			 train-loss:  2.0854159239679575 	 ± 0.24799416275839123
	data : 0.11822876930236817
	model : 0.06673412322998047
			 train-loss:  2.0896645849065263 	 ± 0.2516642512213811
	data : 0.11822171211242676
	model : 0.06710829734802246
			 train-loss:  2.0883215051430923 	 ± 0.25115812190640663
	data : 0.11793990135192871
	model : 0.06790533065795898
			 train-loss:  2.088164889175473 	 ± 0.2502040389856672
	data : 0.11714873313903809
	model : 0.06856060028076172
			 train-loss:  2.0886206852667257 	 ± 0.2493090819564711
	data : 0.11652703285217285
	model : 0.06931977272033692
			 train-loss:  2.09068450981513 	 ± 0.24949934760267742
	data : 0.1157991886138916
	model : 0.0703056812286377
			 train-loss:  2.089964993854067 	 ± 0.2487051010879238
	data : 0.1148148536682129
	model : 0.07033495903015137
			 train-loss:  2.0936793283179953 	 ± 0.2514850878957833
	data : 0.11485881805419922
	model : 0.07095537185668946
			 train-loss:  2.0900164702359367 	 ± 0.25414748861851305
	data : 0.11420001983642578
	model : 0.0707179069519043
			 train-loss:  2.089039175179753 	 ± 0.25347460144447315
	data : 0.11446332931518555
	model : 0.07072901725769043
			 train-loss:  2.087769143823264 	 ± 0.2529916532578677
	data : 0.11464009284973145
	model : 0.06997838020324706
			 train-loss:  2.089933500015478 	 ± 0.25335896077622466
	data : 0.11539530754089355
	model : 0.07012410163879394
			 train-loss:  2.091361047540392 	 ± 0.2530128940086984
	data : 0.11502256393432617
	model : 0.06920127868652344
			 train-loss:  2.0955837601465537 	 ± 0.257017306747906
	data : 0.11593523025512695
	model : 0.06856274604797363
			 train-loss:  2.097160000196645 	 ± 0.2567937277807538
	data : 0.11645336151123047
	model : 0.06828160285949707
			 train-loss:  2.0988524493637617 	 ± 0.25668778933007025
	data : 0.11660895347595215
	model : 0.06882991790771484
			 train-loss:  2.098206321398417 	 ± 0.25591162762657416
	data : 0.11606268882751465
	model : 0.06896181106567383
			 train-loss:  2.09838228225708 	 ± 0.25503638817422075
	data : 0.1161336898803711
	model : 0.06936082839965821
			 train-loss:  2.1012357326403057 	 ± 0.25647352968710047
	data : 0.11582036018371582
	model : 0.07017536163330078
			 train-loss:  2.1018770714195405 	 ± 0.2557171279511727
	data : 0.11507740020751953
	model : 0.07037115097045898
			 train-loss:  2.1037778564401575 	 ± 0.25589162816244
	data : 0.11491351127624512
	model : 0.06880874633789062
			 train-loss:  2.1048371599824636 	 ± 0.25535687400512175
	data : 0.11632604598999023
	model : 0.06858344078063965
			 train-loss:  2.105472526550293 	 ± 0.25462240433952704
	data : 0.11650872230529785
	model : 0.06808185577392578
			 train-loss:  2.104759992353174 	 ± 0.25392788302733044
	data : 0.11673212051391602
	model : 0.06752557754516601
			 train-loss:  2.102401314597381 	 ± 0.25474542365413577
	data : 0.11727023124694824
	model : 0.06765880584716796
			 train-loss:  2.102332476696937 	 ± 0.25391297586525463
	data : 0.11711125373840332
	model : 0.06863622665405274
			 train-loss:  2.102443298736176 	 ± 0.25309095269980375
	data : 0.11631708145141602
	model : 0.06881628036499024
			 train-loss:  2.101335693174793 	 ± 0.25264737827752215
	data : 0.11624579429626465
	model : 0.0690910816192627
			 train-loss:  2.101224491229424 	 ± 0.25184011459920436
	data : 0.1162682056427002
	model : 0.06950626373291016
			 train-loss:  2.102134201936661 	 ± 0.2512937998077664
	data : 0.11586618423461914
	model : 0.06991400718688964
			 train-loss:  2.1014145914512343 	 ± 0.2506595301725662
	data : 0.11545243263244628
	model : 0.07039222717285157
			 train-loss:  2.101120851324789 	 ± 0.24989732757819982
	data : 0.11511850357055664
	model : 0.07038354873657227
			 train-loss:  2.100237788259983 	 ± 0.24936390752474707
	data : 0.11506004333496093
	model : 0.07063336372375488
			 train-loss:  2.100168216302528 	 ± 0.24858983685732294
	data : 0.11479477882385254
	model : 0.07097463607788086
			 train-loss:  2.100987427028609 	 ± 0.24803929668570832
	data : 0.11441969871520996
	model : 0.0705803394317627
			 train-loss:  2.099946110526477 	 ± 0.24763220923212143
	data : 0.1147918701171875
	model : 0.06994695663452148
			 train-loss:  2.1010995529046874 	 ± 0.2473148967532402
	data : 0.11527385711669921
	model : 0.06898860931396485
			 train-loss:  2.100728089159185 	 ± 0.24661020426917213
	data : 0.11618132591247558
	model : 0.0680459976196289
			 train-loss:  2.098666405821421 	 ± 0.24728842802095777
	data : 0.1169980525970459
	model : 0.06798195838928223
			 train-loss:  2.104682814575241 	 ± 0.25844558522059186
	data : 0.11705760955810547
	model : 0.06773843765258789
			 train-loss:  2.1046758613416126 	 ± 0.2576752695609831
	data : 0.11724691390991211
	model : 0.06771488189697265
			 train-loss:  2.105532707547295 	 ± 0.25715172317548796
	data : 0.11706457138061524
	model : 0.06835274696350098
			 train-loss:  2.1106034145635717 	 ± 0.26473263134303493
	data : 0.11650018692016602
	model : 0.06913280487060547
			 train-loss:  2.110086713618005 	 ± 0.2640433836115509
	data : 0.11588773727416993
	model : 0.06904888153076172
			 train-loss:  2.1071701430997183 	 ± 0.2660228515217107
	data : 0.11607813835144043
	model : 0.0689852237701416
			 train-loss:  2.10750436162673 	 ± 0.26528909850469207
	data : 0.11611318588256836
	model : 0.06899404525756836
			 train-loss:  2.108142986379821 	 ± 0.26465900580973417
	data : 0.116387939453125
	model : 0.06953096389770508
			 train-loss:  2.11087518623897 	 ± 0.26635133019936535
	data : 0.11598320007324218
	model : 0.06947517395019531
			 train-loss:  2.1125907091931864 	 ± 0.26656138884153385
	data : 0.11607871055603028
	model : 0.06955480575561523
			 train-loss:  2.113951675635947 	 ± 0.2664198301737295
	data : 0.11592016220092774
	model : 0.06960482597351074
			 train-loss:  2.11146275782853 	 ± 0.26772603258947436
	data : 0.11576600074768066
	model : 0.06977572441101074
			 train-loss:  2.113052527997747 	 ± 0.2678183503754104
	data : 0.11545796394348144
	model : 0.06937007904052735
			 train-loss:  2.112809732225206 	 ± 0.2670931285458074
	data : 0.11569385528564453
	model : 0.06972951889038086
			 train-loss:  2.1124566997612377 	 ± 0.26639638957912326
	data : 0.11537971496582031
	model : 0.06967906951904297
			 train-loss:  2.114418809230511 	 ± 0.266971786384877
	data : 0.11556000709533691
	model : 0.06888394355773926
			 train-loss:  2.113805834712878 	 ± 0.2663697504308139
	data : 0.11623649597167969
	model : 0.06799211502075195
			 train-loss:  2.115659943093424 	 ± 0.2668264099289147
	data : 0.11696434020996094
	model : 0.06791844367980956
			 train-loss:  2.1171936615093334 	 ± 0.2669162972789575
	data : 0.11697754859924317
	model : 0.06749997138977051
			 train-loss:  2.116703780748511 	 ± 0.26628119106364323
	data : 0.1172335147857666
	model : 0.06753716468811036
			 train-loss:  2.118943047396002 	 ± 0.26731846289758693
	data : 0.1171271800994873
	model : 0.06847434043884278
			 train-loss:  2.1190624579470208 	 ± 0.2666115621992317
	data : 0.11646537780761719
	model : 0.06934118270874023
			 train-loss:  2.1193975746316256 	 ± 0.26594500242508473
	data : 0.1158067226409912
	model : 0.06952624320983887
			 train-loss:  2.1176426191078987 	 ± 0.2663392451600537
	data : 0.11567301750183105
	model : 0.06996822357177734
			 train-loss:  2.1192023286020567 	 ± 0.2665096800100268
	data : 0.11532936096191407
	model : 0.06998186111450196
			 train-loss:  2.119470665231347 	 ± 0.2658406063466763
	data : 0.1153573989868164
	model : 0.06994838714599609
			 train-loss:  2.118796813055641 	 ± 0.2653153568000875
	data : 0.1155280590057373
	model : 0.06993961334228516
			 train-loss:  2.117705324261459 	 ± 0.2650647507431416
	data : 0.11548504829406739
	model : 0.07062368392944336
			 train-loss:  2.117214228556706 	 ± 0.264472693900511
	data : 0.11471266746520996
	model : 0.07043123245239258
			 train-loss:  2.118104001697229 	 ± 0.2640896064404753
	data : 0.1149019718170166
	model : 0.06987662315368652
			 train-loss:  2.1169168767590207 	 ± 0.26394224611636063
	data : 0.11536321640014649
	model : 0.06979889869689941
			 train-loss:  2.1155538378339824 	 ± 0.2639690600721233
	data : 0.11545267105102539
	model : 0.06889715194702148
			 train-loss:  2.1131087739263945 	 ± 0.26554326659520705
	data : 0.11629791259765625
	model : 0.0683415412902832
			 train-loss:  2.1122344332933425 	 ± 0.26516559003547485
	data : 0.11696672439575195
	model : 0.06837711334228516
			 train-loss:  2.1108857156032355 	 ± 0.2651919741320812
	data : 0.1169158935546875
	model : 0.06840457916259765
			 train-loss:  2.110908550201076 	 ± 0.26453494203499595
	data : 0.11702094078063965
	model : 0.06764349937438965
			 train-loss:  2.111761759654642 	 ± 0.26416105301363846
	data : 0.11767053604125977
	model : 0.06845202445983886
			 train-loss:  2.11125973685115 	 ± 0.26360986203088094
	data : 0.11695160865783691
	model : 0.06833424568176269
			 train-loss:  2.1103209338537074 	 ± 0.2633077640152143
	data : 0.11720719337463378
	model : 0.06863603591918946
			 train-loss:  2.110898165448198 	 ± 0.2627978799124856
	data : 0.11703691482543946
	model : 0.06922106742858887
			 train-loss:  2.1083238850469175 	 ± 0.2647531581774121
	data : 0.11636314392089844
	model : 0.07017836570739747
			 train-loss:  2.108164368913724 	 ± 0.2641259367210043
	data : 0.11559181213378907
	model : 0.06951627731323243
			 train-loss:  2.1079142846559225 	 ± 0.2635179828721624
	data : 0.11621599197387696
	model : 0.06878151893615722
			 train-loss:  2.1080518552235197 	 ± 0.26289733332288895
	data : 0.11675314903259278
	model : 0.0684748649597168
			 train-loss:  2.106686245773641 	 ± 0.26301915389359964
	data : 0.11688227653503418
	model : 0.06774024963378907
			 train-loss:  2.106591118394204 	 ± 0.26240173076215684
	data : 0.11756057739257812
	model : 0.0679142951965332
			 train-loss:  2.1069251858572446 	 ± 0.2618302243443877
	data : 0.11716642379760742
	model : 0.06861414909362792
			 train-loss:  2.1074557343376017 	 ± 0.26133249167052813
	data : 0.11664094924926757
	model : 0.06959853172302247
			 train-loss:  2.1085909072742908 	 ± 0.26125234110876494
	data : 0.11573233604431152
	model : 0.06934099197387696
			 train-loss:  2.1099255234003067 	 ± 0.26138048637855427
	data : 0.11594772338867188
	model : 0.06979870796203613
			 train-loss:  2.1084002844199605 	 ± 0.2617392089744456
	data : 0.11555256843566894
	model : 0.06947851181030273
			 train-loss:  2.1078415458355475 	 ± 0.2612678784216962
	data : 0.11595087051391602
	model : 0.06951689720153809
			 train-loss:  2.1077722093286035 	 ± 0.260672704198612
	data : 0.11582345962524414
	model : 0.06929192543029786
			 train-loss:  2.10695353475484 	 ± 0.2603616213010704
	data : 0.11605963706970215
	model : 0.06952366828918458
			 train-loss:  2.1077243456473718 	 ± 0.26002336991751723
	data : 0.11578340530395508
	model : 0.06896529197692872
			 train-loss:  2.107577690670082 	 ± 0.25944623114193127
	data : 0.11630992889404297
	model : 0.06824421882629395
			 train-loss:  2.1069690539697894 	 ± 0.25902265372209343
	data : 0.11683316230773926
	model : 0.06807589530944824
			 train-loss:  2.1082680267947063 	 ± 0.2591707712713444
	data : 0.11687512397766113
	model : 0.06803922653198242
			 train-loss:  2.108417962392171 	 ± 0.25860393131902715
	data : 0.116845703125
	model : 0.06704854965209961
			 train-loss:  2.1075419683372023 	 ± 0.2583655144855483
	data : 0.11782207489013671
	model : 0.06741681098937988
			 train-loss:  2.106648677246161 	 ± 0.2581453372298405
	data : 0.11723594665527344
	model : 0.06806378364562989
			 train-loss:  2.105189227221305 	 ± 0.2585154685400478
	data : 0.1165543556213379
	model : 0.06787753105163574
			 train-loss:  2.1045296098467565 	 ± 0.2581426233908192
	data : 0.1168294906616211
	model : 0.06711058616638184
			 train-loss:  2.1041201684785924 	 ± 0.2576553421260698
	data : 0.11753253936767578
	model : 0.06762490272521973
			 train-loss:  2.1048500899112588 	 ± 0.25733524723857376
	data : 0.11714153289794922
	model : 0.06769428253173829
			 train-loss:  2.104303712988722 	 ± 0.25691428956681794
	data : 0.11742725372314453
	model : 0.06769986152648926
			 train-loss:  2.1033828304560913 	 ± 0.25674580978716394
	data : 0.11753363609313965
	model : 0.06795897483825683
			 train-loss:  2.10435965580818 	 ± 0.2566301501828689
	data : 0.11720032691955566
	model : 0.06794929504394531
			 train-loss:  2.1035352899673136 	 ± 0.2563938460059199
	data : 0.11722965240478515
	model : 0.06830859184265137
			 train-loss:  2.1038856455835244 	 ± 0.2559064287590307
	data : 0.11688799858093261
	model : 0.06812992095947265
			 train-loss:  2.1049775131644077 	 ± 0.25591626059732786
	data : 0.11683106422424316
	model : 0.06798653602600098
			 train-loss:  2.103547882633049 	 ± 0.256324681036582
	data : 0.11689109802246093
	model : 0.06757950782775879
			 train-loss:  2.1032503489171113 	 ± 0.25582905654691246
	data : 0.11721920967102051
	model : 0.06737923622131348
			 train-loss:  2.101755038400491 	 ± 0.2563400026545851
	data : 0.11713900566101074
	model : 0.06680197715759277
			 train-loss:  2.100363596841013 	 ± 0.2567142513559658
	data : 0.1173548698425293
	model : 0.06701045036315918
			 train-loss:  2.098926782115432 	 ± 0.2571525090104434
	data : 0.11714529991149902
	model : 0.06691818237304688
			 train-loss:  2.0977943512637918 	 ± 0.25722679551434546
	data : 0.1172119140625
	model : 0.06683869361877441
			 train-loss:  2.096228262440103 	 ± 0.2578574099198145
	data : 0.11727843284606934
	model : 0.06723852157592773
			 train-loss:  2.096963878553741 	 ± 0.2575870537586843
	data : 0.11712479591369629
	model : 0.0677572250366211
			 train-loss:  2.0958603780444074 	 ± 0.25764260160464775
	data : 0.11694140434265136
	model : 0.06822466850280762
			 train-loss:  2.095941601011917 	 ± 0.25712368482834425
	data : 0.11668615341186524
	model : 0.06845426559448242
			 train-loss:  2.09563542229514 	 ± 0.25664988095412783
	data : 0.11648292541503906
	model : 0.06893062591552734
			 train-loss:  2.0951790546317657 	 ± 0.2562348101394678
	data : 0.11622829437255859
	model : 0.06894927024841309
			 train-loss:  2.096771876811981 	 ± 0.256954054954284
	data : 0.1162567138671875
	model : 0.06827197074890137
			 train-loss:  2.0955191874409103 	 ± 0.2572054522643525
	data : 0.11681275367736817
	model : 0.06759057044982911
			 train-loss:  2.094929249513717 	 ± 0.25686471334051564
	data : 0.11746673583984375
	model : 0.06767816543579101
			 train-loss:  2.0941746889838115 	 ± 0.2566362632574049
	data : 0.1175011157989502
	model : 0.06774706840515136
			 train-loss:  2.0947260401380343 	 ± 0.25628066785960035
	data : 0.11722111701965332
	model : 0.06785826683044434
			 train-loss:  2.0948134024937946 	 ± 0.25578145266040075
	data : 0.11713428497314453
	model : 0.06774182319641113
			 train-loss:  2.0952669247053564 	 ± 0.2553840978687727
	data : 0.11611599922180176
	model : 0.05933127403259277
#epoch  15    val-loss:  2.474964267329166  train-loss:  2.0952669247053564  lr:  0.0025
			 train-loss:  2.096034526824951 	 ± 0.0
	data : 5.754895448684692
	model : 0.07829642295837402
			 train-loss:  2.037813127040863 	 ± 0.058221399784088135
	data : 2.9404038190841675
	model : 0.07392024993896484
			 train-loss:  1.969421664873759 	 ± 0.10777107739674223
	data : 1.9988778432210286
	model : 0.07156133651733398
			 train-loss:  1.91757932305336 	 ± 0.12951385662820106
	data : 1.528669834136963
	model : 0.07106798887252808
			 train-loss:  1.8797253847122193 	 ± 0.13838624873414163
	data : 1.2459763050079347
	model : 0.07074675559997559
			 train-loss:  1.8564664125442505 	 ± 0.13661574017589684
	data : 0.11802845001220703
	model : 0.06900162696838379
			 train-loss:  1.8700669492994035 	 ± 0.13079542149262624
	data : 0.1159276008605957
	model : 0.06825590133666992
			 train-loss:  1.9023900926113129 	 ± 0.1492732767542138
	data : 0.11654915809631347
	model : 0.06895008087158203
			 train-loss:  1.9171192116207547 	 ± 0.14677279092102052
	data : 0.11592440605163574
	model : 0.06912670135498047
			 train-loss:  1.9663775444030762 	 ± 0.2030405798743601
	data : 0.11595911979675293
	model : 0.06945757865905762
			 train-loss:  1.9771705323999578 	 ± 0.19657719178856375
	data : 0.11584296226501464
	model : 0.0693939208984375
			 train-loss:  1.9816959301630657 	 ± 0.18880584677893952
	data : 0.11601996421813965
	model : 0.07015957832336425
			 train-loss:  1.9681128446872418 	 ± 0.18740203723088952
	data : 0.11531291007995606
	model : 0.06913208961486816
			 train-loss:  1.9918714335986547 	 ± 0.1998727176806949
	data : 0.11613845825195312
	model : 0.0680466651916504
			 train-loss:  1.992759680747986 	 ± 0.1931239899222909
	data : 0.11695713996887207
	model : 0.06756024360656739
			 train-loss:  2.0130482390522957 	 ± 0.20283048276035026
	data : 0.11740231513977051
	model : 0.06675229072570801
			 train-loss:  1.9947085310431087 	 ± 0.21000407360632678
	data : 0.11805491447448731
	model : 0.06681246757507324
			 train-loss:  2.001204616493649 	 ± 0.20583731689027515
	data : 0.11806507110595703
	model : 0.06774487495422363
			 train-loss:  2.0160110561471236 	 ± 0.20996477179769138
	data : 0.11728787422180176
	model : 0.06863832473754883
			 train-loss:  2.007441884279251 	 ± 0.20802915312468925
	data : 0.1164273738861084
	model : 0.06888408660888672
			 train-loss:  2.0019504172461375 	 ± 0.20449568388381972
	data : 0.116206693649292
	model : 0.06959390640258789
			 train-loss:  2.0215747627344998 	 ± 0.2191005663274774
	data : 0.11569547653198242
	model : 0.06887521743774414
			 train-loss:  2.0261273954225625 	 ± 0.21534591693295926
	data : 0.11651954650878907
	model : 0.06900544166564941
			 train-loss:  2.0079056123892465 	 ± 0.22820689577197403
	data : 0.11645069122314453
	model : 0.0692716121673584
			 train-loss:  2.035160493850708 	 ± 0.26042875668835974
	data : 0.11640486717224122
	model : 0.06933398246765136
			 train-loss:  2.062491242702191 	 ± 0.28963563476474236
	data : 0.11625595092773437
	model : 0.06958994865417481
			 train-loss:  2.0648161835140653 	 ± 0.28446853497873675
	data : 0.11588525772094727
	model : 0.07042303085327148
			 train-loss:  2.0669006194387163 	 ± 0.2795524550631486
	data : 0.11500415802001954
	model : 0.0693936824798584
			 train-loss:  2.065995980953348 	 ± 0.274732008652707
	data : 0.11601471900939941
	model : 0.06928725242614746
			 train-loss:  2.0692101160685223 	 ± 0.270668327104149
	data : 0.11595487594604492
	model : 0.06964693069458008
			 train-loss:  2.069656741234564 	 ± 0.2662781597876597
	data : 0.11572651863098145
	model : 0.06963253021240234
			 train-loss:  2.0656764693558216 	 ± 0.2630198206345301
	data : 0.11570792198181153
	model : 0.0686060905456543
			 train-loss:  2.063881993293762 	 ± 0.2592028634517319
	data : 0.11656866073608399
	model : 0.06966333389282227
			 train-loss:  2.05997670398039 	 ± 0.2563461611397511
	data : 0.11569175720214844
	model : 0.0696218490600586
			 train-loss:  2.0627053805759976 	 ± 0.25315801963276574
	data : 0.11578073501586914
	model : 0.06838736534118653
			 train-loss:  2.082795911365085 	 ± 0.2764701852602136
	data : 0.11668753623962402
	model : 0.06802773475646973
			 train-loss:  2.0710028667707703 	 ± 0.28173864542369825
	data : 0.11705126762390136
	model : 0.06968474388122559
			 train-loss:  2.067292338923404 	 ± 0.2789215326273691
	data : 0.11541056632995605
	model : 0.06939287185668945
			 train-loss:  2.0627308717140784 	 ± 0.2767545594444521
	data : 0.11555776596069336
	model : 0.0691378116607666
			 train-loss:  2.061013415455818 	 ± 0.27348362949712895
	data : 0.11594228744506836
	model : 0.07058730125427246
			 train-loss:  2.059112612794085 	 ± 0.2703952494243463
	data : 0.11460785865783692
	model : 0.07085094451904297
			 train-loss:  2.0620918614523753 	 ± 0.26783708472304196
	data : 0.11434130668640137
	model : 0.0697166919708252
			 train-loss:  2.057999469513117 	 ± 0.2660297177518354
	data : 0.11543188095092774
	model : 0.0699544906616211
			 train-loss:  2.052354484796524 	 ± 0.26558161107030237
	data : 0.11532201766967773
	model : 0.07031550407409667
			 train-loss:  2.056352300114102 	 ± 0.26394963569183394
	data : 0.11493434906005859
	model : 0.06978387832641601
			 train-loss:  2.0488081496694814 	 ± 0.2659247958605821
	data : 0.11561241149902343
	model : 0.07006034851074219
			 train-loss:  2.0431454536762645 	 ± 0.26586922191828916
	data : 0.11542243957519531
	model : 0.07039189338684082
			 train-loss:  2.0503369520107904 	 ± 0.2676649741762135
	data : 0.11518373489379882
	model : 0.07028989791870117
			 train-loss:  2.0604838692412084 	 ± 0.2740884557391805
	data : 0.1151766300201416
	model : 0.07031707763671875
			 train-loss:  2.063870949745178 	 ± 0.27236764738268227
	data : 0.11517863273620606
	model : 0.07014060020446777
			 train-loss:  2.071688553866218 	 ± 0.27529128420893983
	data : 0.11533999443054199
	model : 0.06986355781555176
			 train-loss:  2.078267675179702 	 ± 0.27665033601304334
	data : 0.11549563407897949
	model : 0.06969904899597168
			 train-loss:  2.0769639555013404 	 ± 0.27418921883687963
	data : 0.1154029369354248
	model : 0.06952857971191406
			 train-loss:  2.087572111023797 	 ± 0.28240353683418556
	data : 0.11548476219177246
	model : 0.06910166740417481
			 train-loss:  2.0983923001722857 	 ± 0.29090179836687885
	data : 0.11565275192260742
	model : 0.06913228034973144
			 train-loss:  2.096730879374913 	 ± 0.28855594527017314
	data : 0.11562128067016601
	model : 0.06995210647583008
			 train-loss:  2.0933068844310023 	 ± 0.28715898345246077
	data : 0.11487059593200684
	model : 0.06967334747314453
			 train-loss:  2.102967521239971 	 ± 0.29386773093334273
	data : 0.11542739868164062
	model : 0.06910924911499024
			 train-loss:  2.09943137330524 	 ± 0.29260860563856766
	data : 0.11590847969055176
	model : 0.06953907012939453
			 train-loss:  2.097888457775116 	 ± 0.29040188395366373
	data : 0.11565256118774414
	model : 0.06958017349243165
			 train-loss:  2.097356909611186 	 ± 0.2880411332550864
	data : 0.1156966209411621
	model : 0.06898298263549804
			 train-loss:  2.0979754540228073 	 ± 0.2857496181192577
	data : 0.1164179801940918
	model : 0.06964054107666015
			 train-loss:  2.094100666424585 	 ± 0.2851098631336759
	data : 0.11597843170166015
	model : 0.07046089172363282
			 train-loss:  2.100364001467824 	 ± 0.28720892190772995
	data : 0.11525483131408691
	model : 0.07021746635437012
			 train-loss:  2.1085336006604707 	 ± 0.29238914871811017
	data : 0.11527438163757324
	model : 0.06944637298583985
			 train-loss:  2.1069802244504294 	 ± 0.2904357646303835
	data : 0.1158034324645996
	model : 0.06937899589538574
			 train-loss:  2.110433576711968 	 ± 0.2896222138103483
	data : 0.11572976112365722
	model : 0.0692901611328125
			 train-loss:  2.1129256434300365 	 ± 0.28820752729407667
	data : 0.11566691398620606
	model : 0.06933517456054687
			 train-loss:  2.112218924190687 	 ± 0.28617079087177877
	data : 0.11576862335205078
	model : 0.06926450729370118
			 train-loss:  2.1134109173502242 	 ± 0.2842918390160411
	data : 0.11601405143737793
	model : 0.07002043724060059
			 train-loss:  2.1138298091754106 	 ± 0.28230443982463554
	data : 0.11512975692749024
	model : 0.0700800895690918
			 train-loss:  2.1111968060334525 	 ± 0.2812136781391289
	data : 0.11510915756225586
	model : 0.06986021995544434
			 train-loss:  2.1120942128847724 	 ± 0.27938470608815
	data : 0.11524028778076172
	model : 0.06962747573852539
			 train-loss:  2.111682985280011 	 ± 0.27751279356662506
	data : 0.1152724266052246
	model : 0.06887383460998535
			 train-loss:  2.1080257892608643 	 ± 0.2774459623783327
	data : 0.11600337028503419
	model : 0.06884245872497559
			 train-loss:  2.105658835486362 	 ± 0.27637583423606
	data : 0.11645045280456542
	model : 0.06873602867126465
			 train-loss:  2.1073878275883664 	 ± 0.2749887313365923
	data : 0.11649608612060547
	model : 0.06885104179382324
			 train-loss:  2.1061218824142065 	 ± 0.27344603134827106
	data : 0.11631999015808106
	model : 0.06807503700256348
			 train-loss:  2.106300622602052 	 ± 0.2717144341567967
	data : 0.11703329086303711
	model : 0.06830534934997559
			 train-loss:  2.104696388542652 	 ± 0.2703871046185518
	data : 0.11677703857421876
	model : 0.0683527946472168
			 train-loss:  2.0978436999850802 	 ± 0.27561449714032266
	data : 0.1167633056640625
	model : 0.06855554580688476
			 train-loss:  2.093564847620522 	 ± 0.2766224204333473
	data : 0.11679635047912598
	model : 0.06871566772460938
			 train-loss:  2.094235420227051 	 ± 0.2750180157014708
	data : 0.11673436164855958
	model : 0.06962909698486328
			 train-loss:  2.103845556577047 	 ± 0.2870538890047484
	data : 0.11603856086730957
	model : 0.06973214149475097
			 train-loss:  2.1012736685135787 	 ± 0.28633223933082985
	data : 0.11586174964904786
	model : 0.0691108226776123
			 train-loss:  2.105090254961058 	 ± 0.2868291515330726
	data : 0.11626615524291992
	model : 0.0680356502532959
			 train-loss:  2.101189779139113 	 ± 0.2874607766751667
	data : 0.11714558601379395
	model : 0.06716852188110352
			 train-loss:  2.1030193282799288 	 ± 0.28633178235204554
	data : 0.11799440383911133
	model : 0.06732115745544434
			 train-loss:  2.1077721185898515 	 ± 0.2881883640928804
	data : 0.1178591251373291
	model : 0.06693487167358399
			 train-loss:  2.1073278970188563 	 ± 0.2866134851915916
	data : 0.11831555366516114
	model : 0.06761584281921387
			 train-loss:  2.1112920150652035 	 ± 0.28750452742869187
	data : 0.11769251823425293
	model : 0.06837091445922852
			 train-loss:  2.1124507590480475 	 ± 0.28615130974461916
	data : 0.11692304611206054
	model : 0.06888904571533203
			 train-loss:  2.11345402784245 	 ± 0.2847713411591386
	data : 0.11657495498657226
	model : 0.06863970756530761
			 train-loss:  2.109195946378911 	 ± 0.2862135810259964
	data : 0.11670861244201661
	model : 0.06936440467834473
			 train-loss:  2.1071010664889687 	 ± 0.2854267647216032
	data : 0.11599411964416503
	model : 0.06902923583984374
			 train-loss:  2.107582281033198 	 ± 0.2839750118642639
	data : 0.11624441146850586
	model : 0.06823091506958008
			 train-loss:  2.104155713749915 	 ± 0.2844953767740539
	data : 0.11704273223876953
	model : 0.06828370094299316
			 train-loss:  2.1023048344923527 	 ± 0.2836265563567183
	data : 0.11701908111572265
	model : 0.06875348091125488
			 train-loss:  2.10194115325658 	 ± 0.2822134289029658
	data : 0.11676898002624511
	model : 0.06897416114807128
			 train-loss:  2.1049764955043795 	 ± 0.2824182929843671
	data : 0.11655731201171875
	model : 0.06902275085449219
			 train-loss:  2.103878388310423 	 ± 0.28123117229399824
	data : 0.11652178764343261
	model : 0.06898860931396485
			 train-loss:  2.100598521092359 	 ± 0.2817837486561596
	data : 0.11659455299377441
	model : 0.07003860473632813
			 train-loss:  2.1028993534810336 	 ± 0.2813736974981853
	data : 0.11548247337341308
	model : 0.06893329620361328
			 train-loss:  2.1038275349598665 	 ± 0.2801760752829329
	data : 0.11640763282775879
	model : 0.0691225528717041
			 train-loss:  2.1005285603659494 	 ± 0.2808609680989149
	data : 0.11638669967651367
	model : 0.06846532821655274
			 train-loss:  2.100929838306499 	 ± 0.27956325362008494
	data : 0.11718506813049316
	model : 0.06932077407836915
			 train-loss:  2.103972506300311 	 ± 0.2800116360264601
	data : 0.11634330749511719
	model : 0.06758933067321778
			 train-loss:  2.104266071761096 	 ± 0.2787288133308397
	data : 0.11786208152770997
	model : 0.06736927032470703
			 train-loss:  2.1036226640053846 	 ± 0.2775278553775852
	data : 0.11806893348693848
	model : 0.06801981925964355
			 train-loss:  2.101059241728349 	 ± 0.277556781079437
	data : 0.1172605037689209
	model : 0.06870288848876953
			 train-loss:  2.104152252008249 	 ± 0.27820149383820314
	data : 0.11644258499145507
	model : 0.06869497299194335
			 train-loss:  2.1046148048979894 	 ± 0.27699960983466426
	data : 0.11635990142822265
	model : 0.06959943771362305
			 train-loss:  2.108430883525747 	 ± 0.2787126892423942
	data : 0.11563501358032227
	model : 0.07042164802551269
			 train-loss:  2.108658181993585 	 ± 0.277498091958786
	data : 0.11491580009460449
	model : 0.06954917907714844
			 train-loss:  2.1073568209357885 	 ± 0.2766381107378447
	data : 0.11594686508178711
	model : 0.06902389526367188
			 train-loss:  2.106407482048561 	 ± 0.2756311987373278
	data : 0.11648726463317871
	model : 0.06864500045776367
			 train-loss:  2.103467634600452 	 ± 0.27627119496489944
	data : 0.11704068183898926
	model : 0.06796517372131347
			 train-loss:  2.102956964807995 	 ± 0.275153514000258
	data : 0.11756367683410644
	model : 0.06810574531555176
			 train-loss:  2.1032751638348364 	 ± 0.2740167694169889
	data : 0.11757540702819824
	model : 0.06823153495788574
			 train-loss:  2.1034684509038923 	 ± 0.27288079051619873
	data : 0.1174774169921875
	model : 0.06821846961975098
			 train-loss:  2.103549534624273 	 ± 0.271752296099565
	data : 0.11738181114196777
	model : 0.06905550956726074
			 train-loss:  2.1002554786009866 	 ± 0.2730511593711564
	data : 0.11660680770874024
	model : 0.06964998245239258
			 train-loss:  2.10365427509556 	 ± 0.2745179473810319
	data : 0.11616020202636719
	model : 0.06947112083435059
			 train-loss:  2.1019941289578714 	 ± 0.2740280262018409
	data : 0.11618280410766602
	model : 0.06973133087158204
			 train-loss:  2.0995092182159425 	 ± 0.2743288194192294
	data : 0.11598672866821289
	model : 0.07053961753845214
			 train-loss:  2.097550740317693 	 ± 0.2741139970746291
	data : 0.11543927192687989
	model : 0.0695570468902588
			 train-loss:  2.0980664594905583 	 ± 0.2730940379066138
	data : 0.11621527671813965
	model : 0.06939420700073243
			 train-loss:  2.095490929670632 	 ± 0.2735692425465253
	data : 0.11623635292053222
	model : 0.06952757835388183
			 train-loss:  2.093118573344031 	 ± 0.2738254333667904
	data : 0.11594014167785645
	model : 0.06919102668762207
			 train-loss:  2.0917217364678016 	 ± 0.2732312110057629
	data : 0.11612191200256347
	model : 0.06911125183105468
			 train-loss:  2.0917062086003426 	 ± 0.2721864035307932
	data : 0.11613693237304687
	model : 0.06975355148315429
			 train-loss:  2.0912548411976206 	 ± 0.27120264362381585
	data : 0.11551580429077149
	model : 0.06997852325439453
			 train-loss:  2.08947779988884 	 ± 0.2709514706111397
	data : 0.11535444259643554
	model : 0.06997380256652833
			 train-loss:  2.092061879919536 	 ± 0.27157859266254475
	data : 0.1155005931854248
	model : 0.0698465347290039
			 train-loss:  2.091307858184532 	 ± 0.27071162645718205
	data : 0.11564688682556153
	model : 0.06978583335876465
			 train-loss:  2.0948951936819973 	 ± 0.27291617347494435
	data : 0.11565766334533692
	model : 0.06929364204406738
			 train-loss:  2.0934845600685064 	 ± 0.2724154715394218
	data : 0.11594939231872559
	model : 0.0692328929901123
			 train-loss:  2.0959158323813174 	 ± 0.2729143689757088
	data : 0.11616902351379395
	model : 0.06925401687622071
			 train-loss:  2.092334499462045 	 ± 0.2751661129648571
	data : 0.11613979339599609
	model : 0.06933398246765136
			 train-loss:  2.090001745734896 	 ± 0.275557544302961
	data : 0.11613411903381347
	model : 0.06943402290344239
			 train-loss:  2.0921035819019833 	 ± 0.2757025858057072
	data : 0.11605749130249024
	model : 0.06990170478820801
			 train-loss:  2.08787073551769 	 ± 0.27929002133706676
	data : 0.11590785980224609
	model : 0.06981310844421387
			 train-loss:  2.088223447332849 	 ± 0.2783435048280338
	data : 0.11589608192443848
	model : 0.0698164939880371
			 train-loss:  2.088611723648177 	 ± 0.27741420938889333
	data : 0.11573495864868164
	model : 0.06976714134216308
			 train-loss:  2.0916466219671843 	 ± 0.27884444019744264
	data : 0.11567568778991699
	model : 0.06976675987243652
			 train-loss:  2.091369379056643 	 ± 0.2779079054153951
	data : 0.11580567359924317
	model : 0.06882786750793457
			 train-loss:  2.0924239710074706 	 ± 0.2772540110427548
	data : 0.11659793853759766
	model : 0.06900725364685059
			 train-loss:  2.0917497889415637 	 ± 0.2764366307914981
	data : 0.11635360717773438
	model : 0.06843070983886719
			 train-loss:  2.0923063675029168 	 ± 0.27559062221225994
	data : 0.11687545776367188
	model : 0.068536376953125
			 train-loss:  2.091055150826772 	 ± 0.275094751634342
	data : 0.11683568954467774
	model : 0.0677417278289795
			 train-loss:  2.091138729985976 	 ± 0.27418423952201504
	data : 0.11743149757385254
	model : 0.06843852996826172
			 train-loss:  2.091641942921438 	 ± 0.27335077904841776
	data : 0.1168940544128418
	model : 0.06821808815002442
			 train-loss:  2.089676930234323 	 ± 0.27353097070378346
	data : 0.11712493896484374
	model : 0.06873235702514649
			 train-loss:  2.0888026906298354 	 ± 0.27285580467172976
	data : 0.11687498092651367
	model : 0.06873025894165039
			 train-loss:  2.0920746895574753 	 ± 0.2749885200255148
	data : 0.11679744720458984
	model : 0.0694511890411377
			 train-loss:  2.0950156221022973 	 ± 0.2765403388917544
	data : 0.11610541343688965
	model : 0.06908330917358399
			 train-loss:  2.095597239816265 	 ± 0.27575393284957506
	data : 0.1164238452911377
	model : 0.06930732727050781
			 train-loss:  2.0951141119003296 	 ± 0.2749465583885396
	data : 0.11618108749389648
	model : 0.06937370300292969
			 train-loss:  2.0947172806697822 	 ± 0.2741259691015753
	data : 0.11606397628784179
	model : 0.0698817253112793
			 train-loss:  2.0959550186991693 	 ± 0.2737133129129851
	data : 0.11566057205200195
	model : 0.06967148780822754
			 train-loss:  2.095859047789011 	 ± 0.27286464788845805
	data : 0.1160386085510254
	model : 0.07007374763488769
			 train-loss:  2.0960519475701416 	 ± 0.2720321809047792
	data : 0.11560750007629395
	model : 0.06914491653442383
			 train-loss:  2.094136971637515 	 ± 0.27228952603222584
	data : 0.11652336120605469
	model : 0.06977019309997559
			 train-loss:  2.0928414645718365 	 ± 0.27196152729806905
	data : 0.1157982349395752
	model : 0.06903529167175293
			 train-loss:  2.090932682788733 	 ± 0.27223581090799837
	data : 0.11641016006469726
	model : 0.06905689239501953
			 train-loss:  2.091877996203411 	 ± 0.27168607575734627
	data : 0.1163701057434082
	model : 0.06848196983337403
			 train-loss:  2.089243005849644 	 ± 0.2729906495121456
	data : 0.11698188781738281
	model : 0.06940841674804688
			 train-loss:  2.086962557974316 	 ± 0.2737677373416143
	data : 0.11617703437805176
	model : 0.06892633438110352
			 train-loss:  2.089304313151794 	 ± 0.27463898384526525
	data : 0.11673455238342285
	model : 0.06904058456420899
			 train-loss:  2.090911159795873 	 ± 0.2746256293946633
	data : 0.11670660972595215
	model : 0.06848349571228027
			 train-loss:  2.0917908266970984 	 ± 0.2740615560608736
	data : 0.11702752113342285
	model : 0.06832094192504883
			 train-loss:  2.0932676903037137 	 ± 0.2739452948330314
	data : 0.11707496643066406
	model : 0.06806755065917969
			 train-loss:  2.0936459364918614 	 ± 0.27319743882846337
	data : 0.11735749244689941
	model : 0.06788272857666015
			 train-loss:  2.093192330722151 	 ± 0.2724765850314101
	data : 0.1174461841583252
	model : 0.06799302101135254
			 train-loss:  2.0949517127445767 	 ± 0.27268634884629295
	data : 0.11728930473327637
	model : 0.06879396438598633
			 train-loss:  2.0938400558450003 	 ± 0.27230794843012684
	data : 0.11666221618652343
	model : 0.07007112503051757
			 train-loss:  2.0959234507070423 	 ± 0.27294068419625656
	data : 0.11550226211547851
	model : 0.07043495178222656
			 train-loss:  2.0973339201359265 	 ± 0.2728190330943402
	data : 0.11514124870300294
	model : 0.07061386108398438
			 train-loss:  2.096002787185115 	 ± 0.27263494707157865
	data : 0.1151397705078125
	model : 0.07060108184814454
			 train-loss:  2.0955431746111977 	 ± 0.2719461041771431
	data : 0.11520586013793946
	model : 0.07049264907836914
			 train-loss:  2.093945715308848 	 ± 0.272039393995018
	data : 0.11535367965698243
	model : 0.07019658088684082
			 train-loss:  2.0958197732548136 	 ± 0.2724600842316463
	data : 0.11571741104125977
	model : 0.07005796432495118
			 train-loss:  2.0957466761271157 	 ± 0.27171642749070085
	data : 0.1159660816192627
	model : 0.06990504264831543
			 train-loss:  2.096075169418169 	 ± 0.2710134961550469
	data : 0.11605238914489746
	model : 0.06988945007324218
			 train-loss:  2.0968933582305906 	 ± 0.27050780550357884
	data : 0.11598882675170899
	model : 0.06900687217712402
			 train-loss:  2.096479956821729 	 ± 0.26983824474704626
	data : 0.1166731834411621
	model : 0.06856832504272461
			 train-loss:  2.098797511289464 	 ± 0.27096553772946347
	data : 0.11702747344970703
	model : 0.06824312210083008
			 train-loss:  2.0986363304422255 	 ± 0.2702529120848155
	data : 0.11720142364501954
	model : 0.06836833953857421
			 train-loss:  2.0981263761167175 	 ± 0.26962768645312957
	data : 0.11671724319458007
	model : 0.06845345497131347
			 train-loss:  2.096981918811798 	 ± 0.26937708034105795
	data : 0.11679863929748535
	model : 0.06930065155029297
			 train-loss:  2.0959398808903718 	 ± 0.26905465113726085
	data : 0.11619009971618652
	model : 0.06950831413269043
			 train-loss:  2.095351134737333 	 ± 0.2684763988620083
	data : 0.1159522533416748
	model : 0.06890377998352051
			 train-loss:  2.0990972370681367 	 ± 0.2727645462840363
	data : 0.11651229858398438
	model : 0.06880559921264648
			 train-loss:  2.0988515381960524 	 ± 0.27208204828871463
	data : 0.11707367897033691
	model : 0.06871986389160156
			 train-loss:  2.0982918042403003 	 ± 0.27149546506747446
	data : 0.11682443618774414
	model : 0.06886210441589355
			 train-loss:  2.0961933537405364 	 ± 0.27238281633583644
	data : 0.116729736328125
	model : 0.06910481452941894
			 train-loss:  2.0957963321414694 	 ± 0.27174746033750286
	data : 0.11656684875488281
	model : 0.07010402679443359
			 train-loss:  2.098575132061737 	 ± 0.27385196689243235
	data : 0.11570868492126465
	model : 0.07003235816955566
			 train-loss:  2.099312380929688 	 ± 0.27335994784332984
	data : 0.11565709114074707
	model : 0.06995620727539062
			 train-loss:  2.099025720357895 	 ± 0.2727056755219521
	data : 0.1159358024597168
	model : 0.06989059448242188
			 train-loss:  2.1005091240156943 	 ± 0.27283418183359404
	data : 0.11587982177734375
	model : 0.06984667778015137
			 train-loss:  2.099223048380106 	 ± 0.2727680983827741
	data : 0.11580166816711426
	model : 0.06975736618041992
			 train-loss:  2.09709237479224 	 ± 0.27377537245389405
	data : 0.11587753295898437
	model : 0.06971673965454102
			 train-loss:  2.095427333724265 	 ± 0.2741319528169207
	data : 0.11611413955688477
	model : 0.06985301971435547
			 train-loss:  2.0954703301918216 	 ± 0.27346321047201466
	data : 0.1160961627960205
	model : 0.07020130157470703
			 train-loss:  2.0950520808256945 	 ± 0.2728643776942258
	data : 0.11588864326477051
	model : 0.06987338066101074
			 train-loss:  2.0935133793503766 	 ± 0.27309889936641984
	data : 0.11629815101623535
	model : 0.06986165046691895
			 train-loss:  2.0947761352245626 	 ± 0.2730467158264888
	data : 0.11642427444458008
	model : 0.06984071731567383
			 train-loss:  2.094494518480803 	 ± 0.2724229890203071
	data : 0.11625127792358399
	model : 0.0701179027557373
			 train-loss:  2.094189623423985 	 ± 0.27180933103549715
	data : 0.11616759300231934
	model : 0.06923952102661132
			 train-loss:  2.0936923462067734 	 ± 0.2712602045077386
	data : 0.11680803298950196
	model : 0.06933565139770508
			 train-loss:  2.0923280305457563 	 ± 0.2713443560369266
	data : 0.1166928768157959
	model : 0.06938047409057617
			 train-loss:  2.0934297786632055 	 ± 0.27118153567358305
	data : 0.11654024124145508
	model : 0.06860551834106446
			 train-loss:  2.095276240433488 	 ± 0.27188598677340947
	data : 0.11726245880126954
	model : 0.0683053970336914
			 train-loss:  2.094478582226953 	 ± 0.2715038230648953
	data : 0.11716108322143555
	model : 0.06903614997863769
			 train-loss:  2.094988862121547 	 ± 0.2709779304834381
	data : 0.11647653579711914
	model : 0.06903929710388183
			 train-loss:  2.0940670307880174 	 ± 0.2706920891185758
	data : 0.11646919250488282
	model : 0.06904444694519044
			 train-loss:  2.0946779251098633 	 ± 0.270220409169249
	data : 0.11655206680297851
	model : 0.06994729042053223
			 train-loss:  2.093547435655986 	 ± 0.27011896364220306
	data : 0.11577301025390625
	model : 0.07032084465026855
			 train-loss:  2.09231048876589 	 ± 0.2701252988024837
	data : 0.11549854278564453
	model : 0.07025909423828125
			 train-loss:  2.0912743763686303 	 ± 0.2699512602798398
	data : 0.11557149887084961
	model : 0.07028570175170898
			 train-loss:  2.091384266410862 	 ± 0.2693475298758021
	data : 0.11561455726623535
	model : 0.07019004821777344
			 train-loss:  2.090240358237194 	 ± 0.26928285655797696
	data : 0.1156015396118164
	model : 0.06935205459594726
			 train-loss:  2.090135617447751 	 ± 0.2686856590713163
	data : 0.11640238761901855
	model : 0.06804256439208985
			 train-loss:  2.0887901565763687 	 ± 0.26884313173542707
	data : 0.11741538047790527
	model : 0.06717267036437988
			 train-loss:  2.089001011531965 	 ± 0.2682663319241338
	data : 0.11808571815490723
	model : 0.0665562629699707
			 train-loss:  2.0868985006987786 	 ± 0.26953447846422063
	data : 0.11831507682800294
	model : 0.06587233543395996
			 train-loss:  2.087260814612372 	 ± 0.26899813835942504
	data : 0.11869320869445801
	model : 0.06645264625549316
			 train-loss:  2.0880991772272703 	 ± 0.26870851520221556
	data : 0.1179990291595459
	model : 0.06703414916992187
			 train-loss:  2.0886893173922663 	 ± 0.26827241254020334
	data : 0.11764469146728515
	model : 0.0676048755645752
			 train-loss:  2.08995124220332 	 ± 0.2683743523199327
	data : 0.11711974143981933
	model : 0.06783933639526367
			 train-loss:  2.09050160389522 	 ± 0.2679259426135923
	data : 0.1169731616973877
	model : 0.06775164604187012
			 train-loss:  2.0907729690167014 	 ± 0.2673823251631473
	data : 0.11707587242126465
	model : 0.06740102767944336
			 train-loss:  2.093735590449765 	 ± 0.27061568903939365
	data : 0.11744227409362792
	model : 0.06732187271118165
			 train-loss:  2.0935864727547826 	 ± 0.2700489312046352
	data : 0.1174776554107666
	model : 0.06675620079040527
			 train-loss:  2.0936973099991425 	 ± 0.269481542816142
	data : 0.11829290390014649
	model : 0.06671633720397949
			 train-loss:  2.0934358563604234 	 ± 0.2689424095076259
	data : 0.11856732368469239
	model : 0.06695079803466797
			 train-loss:  2.0920338540517984 	 ± 0.26924331313599853
	data : 0.11840581893920898
	model : 0.06672053337097168
			 train-loss:  2.0925125197885426 	 ± 0.26878091225482115
	data : 0.11850919723510742
	model : 0.06695713996887206
			 train-loss:  2.0928311089674634 	 ± 0.2682655844943256
	data : 0.11826605796813965
	model : 0.0674828052520752
			 train-loss:  2.0926947761868044 	 ± 0.2677167696430163
	data : 0.1174344539642334
	model : 0.06753487586975097
			 train-loss:  2.092704529604636 	 ± 0.26716310612514177
	data : 0.11731457710266113
	model : 0.06712126731872559
			 train-loss:  2.092674466317573 	 ± 0.2666132312615726
	data : 0.11770811080932617
	model : 0.06753773689270019
			 train-loss:  2.091710460967705 	 ± 0.2664903644134481
	data : 0.11747674942016602
	model : 0.06721086502075195
			 train-loss:  2.091500829190624 	 ± 0.26596610921328884
	data : 0.11774888038635253
	model : 0.06644735336303711
			 train-loss:  2.0911290054398823 	 ± 0.265488776565587
	data : 0.11842193603515624
	model : 0.06621346473693848
			 train-loss:  2.0918549651559064 	 ± 0.2651953530694857
	data : 0.11857686042785645
	model : 0.06640496253967285
			 train-loss:  2.0910892880732015 	 ± 0.2649335744848727
	data : 0.11832404136657715
	model : 0.06617178916931152
			 train-loss:  2.0919201230428306 	 ± 0.2647245800072225
	data : 0.11842904090881348
	model : 0.06656484603881836
			 train-loss:  2.091814157485962 	 ± 0.2641998917355412
	data : 0.1182478904724121
	model : 0.06761379241943359
			 train-loss:  2.091706066017607 	 ± 0.2636786107728566
	data : 0.11757206916809082
	model : 0.06763944625854493
			 train-loss:  2.0918870606119673 	 ± 0.2631705413746746
	data : 0.11770119667053222
	model : 0.068320894241333
			 train-loss:  2.092318891065394 	 ± 0.2627393694357417
	data : 0.11715126037597656
	model : 0.0688586711883545
			 train-loss:  2.093122517968726 	 ± 0.26253302328653666
	data : 0.11671218872070313
	model : 0.06855878829956055
			 train-loss:  2.0943396530899347 	 ± 0.2627348090409244
	data : 0.11699385643005371
	model : 0.06794471740722656
			 train-loss:  2.0921584870666265 	 ± 0.26452427453217314
	data : 0.11648392677307129
	model : 0.05950145721435547
#epoch  16    val-loss:  2.513652067435415  train-loss:  2.0921584870666265  lr:  0.0025
			 train-loss:  2.073499917984009 	 ± 0.0
	data : 5.485372066497803
	model : 0.07842230796813965
			 train-loss:  2.0466710329055786 	 ± 0.026828885078430176
	data : 2.8039075136184692
	model : 0.07241535186767578
			 train-loss:  2.1101695696512857 	 ± 0.09243369364428895
	data : 1.9088718891143799
	model : 0.07146501541137695
			 train-loss:  2.1468265652656555 	 ± 0.10217238752696571
	data : 1.4606483578681946
	model : 0.07100725173950195
			 train-loss:  2.0870681524276735 	 ± 0.15045141753412367
	data : 1.1916593551635741
	model : 0.07100520133972169
			 train-loss:  2.0447677175203958 	 ± 0.1667622804026141
	data : 0.11749758720397949
	model : 0.06923718452453613
			 train-loss:  2.038938675607954 	 ± 0.15505068923898718
	data : 0.11610207557678223
	model : 0.06980609893798828
			 train-loss:  2.0314977020025253 	 ± 0.14636667421812977
	data : 0.11558513641357422
	model : 0.06993603706359863
			 train-loss:  2.014007820023431 	 ± 0.14659473104510515
	data : 0.11546096801757813
	model : 0.0698974609375
			 train-loss:  1.993636167049408 	 ± 0.15190803750037105
	data : 0.11554665565490722
	model : 0.06879687309265137
			 train-loss:  1.9591861421411687 	 ± 0.18123540250979323
	data : 0.11667718887329101
	model : 0.06890039443969727
			 train-loss:  1.9615123569965363 	 ± 0.17369112209244567
	data : 0.11676020622253418
	model : 0.06904478073120117
			 train-loss:  1.9715232940820546 	 ± 0.17044227831694156
	data : 0.1165799617767334
	model : 0.06900601387023926
			 train-loss:  1.9850099171910967 	 ± 0.17128948170889785
	data : 0.11676554679870606
	model : 0.06831703186035157
			 train-loss:  1.9981940507888794 	 ± 0.17267767683355753
	data : 0.11732301712036133
	model : 0.06927294731140136
			 train-loss:  2.0103799775242805 	 ± 0.17372804459226884
	data : 0.11644949913024902
	model : 0.06929984092712402
			 train-loss:  2.0214878040201523 	 ± 0.17429915659399473
	data : 0.11641936302185059
	model : 0.0692601203918457
			 train-loss:  2.007666005028619 	 ± 0.17871799990057377
	data : 0.11649003028869628
	model : 0.06909704208374023
			 train-loss:  1.988294896326567 	 ± 0.19238862154433792
	data : 0.1165349006652832
	model : 0.0696824073791504
			 train-loss:  1.983199453353882 	 ± 0.18882801661144938
	data : 0.11610202789306641
	model : 0.06963777542114258
			 train-loss:  2.0001049722943987 	 ± 0.19918344701024085
	data : 0.11609292030334473
	model : 0.06960926055908204
			 train-loss:  1.9990406036376953 	 ± 0.1946650208662628
	data : 0.11626229286193848
	model : 0.0696366310119629
			 train-loss:  1.9925149109052576 	 ± 0.1928308775211089
	data : 0.11622705459594726
	model : 0.06891474723815919
			 train-loss:  1.9794678688049316 	 ± 0.19887082891763122
	data : 0.1169931411743164
	model : 0.06976547241210937
			 train-loss:  1.9791290140151978 	 ± 0.19485989344207014
	data : 0.11607685089111328
	model : 0.06977605819702148
			 train-loss:  2.005566335641421 	 ± 0.2323430185842449
	data : 0.1161149024963379
	model : 0.06983466148376465
			 train-loss:  1.9965327536618267 	 ± 0.23260619788723533
	data : 0.11590142250061035
	model : 0.06972575187683105
			 train-loss:  1.9986202333654677 	 ± 0.2286721524869109
	data : 0.11617469787597656
	model : 0.06975278854370118
			 train-loss:  2.0105093142081953 	 ± 0.2333358605390347
	data : 0.1158332347869873
	model : 0.06918511390686036
			 train-loss:  2.016893708705902 	 ± 0.23197591251322547
	data : 0.11648597717285156
	model : 0.06818857192993164
			 train-loss:  2.0226049538581603 	 ± 0.23033774348567546
	data : 0.11726646423339844
	model : 0.06743803024291992
			 train-loss:  2.0325065068900585 	 ± 0.2333168570260331
	data : 0.11807074546813964
	model : 0.06759157180786132
			 train-loss:  2.0453195174535117 	 ± 0.24091637715835457
	data : 0.11785292625427246
	model : 0.06844568252563477
			 train-loss:  2.037033635027268 	 ± 0.24207284919730188
	data : 0.11730914115905762
	model : 0.06752305030822754
			 train-loss:  2.0363107340676443 	 ± 0.23862683742780869
	data : 0.1182546615600586
	model : 0.06856598854064941
			 train-loss:  2.0517654750082226 	 ± 0.25242967260587956
	data : 0.11744627952575684
	model : 0.06890459060668945
			 train-loss:  2.0462121351345166 	 ± 0.2512146143745274
	data : 0.11702117919921876
	model : 0.06898212432861328
			 train-loss:  2.0474343644945243 	 ± 0.24799858324654622
	data : 0.11712136268615722
	model : 0.06888055801391602
			 train-loss:  2.0494434191630435 	 ± 0.24511154418863368
	data : 0.1171882152557373
	model : 0.06963624954223632
			 train-loss:  2.0515476197004316 	 ± 0.24238472745479103
	data : 0.11645054817199707
	model : 0.06972265243530273
			 train-loss:  2.0558227184342175 	 ± 0.240932520337049
	data : 0.11617779731750488
	model : 0.07005014419555664
			 train-loss:  2.058065808954693 	 ± 0.2384798992851075
	data : 0.11590185165405273
	model : 0.06994194984436035
			 train-loss:  2.056237204130306 	 ± 0.23598830844649732
	data : 0.11580948829650879
	model : 0.07018661499023438
			 train-loss:  2.0633770606734534 	 ± 0.2379429000193886
	data : 0.11562824249267578
	model : 0.07036070823669434
			 train-loss:  2.068364980485704 	 ± 0.2375991649204289
	data : 0.11533575057983399
	model : 0.0703038215637207
			 train-loss:  2.0669248933377475 	 ± 0.2352008491484419
	data : 0.11547150611877441
	model : 0.0710968017578125
			 train-loss:  2.067756876032403 	 ± 0.2327536703679477
	data : 0.11451954841613769
	model : 0.07055726051330566
			 train-loss:  2.069919059673945 	 ± 0.23079290979827152
	data : 0.11501641273498535
	model : 0.06967430114746094
			 train-loss:  2.0769472170849235 	 ± 0.2335578702145748
	data : 0.11581120491027833
	model : 0.06957306861877441
			 train-loss:  2.064896352291107 	 ± 0.24611833929239835
	data : 0.11593914031982422
	model : 0.06954588890075683
			 train-loss:  2.0663952897576725 	 ± 0.24392385589868176
	data : 0.11598563194274902
	model : 0.06811327934265136
			 train-loss:  2.0623608988064985 	 ± 0.2432791209677816
	data : 0.11752114295959473
	model : 0.06868667602539062
			 train-loss:  2.060391086452412 	 ± 0.24139139591903191
	data : 0.11693902015686035
	model : 0.0694958209991455
			 train-loss:  2.0592008851192616 	 ± 0.23930276688774346
	data : 0.11635198593139648
	model : 0.06952810287475586
			 train-loss:  2.065136170387268 	 ± 0.24109522729216995
	data : 0.11638846397399902
	model : 0.06945557594299316
			 train-loss:  2.0635459763663158 	 ± 0.23922376041746352
	data : 0.1163632869720459
	model : 0.06921296119689942
			 train-loss:  2.0692504832619116 	 ± 0.24092805099338813
	data : 0.11639394760131835
	model : 0.06918387413024903
			 train-loss:  2.071247779089829 	 ± 0.2393175942155759
	data : 0.1165949821472168
	model : 0.06817731857299805
			 train-loss:  2.073281183081158 	 ± 0.23778561361372422
	data : 0.11728739738464355
	model : 0.06816458702087402
			 train-loss:  2.0741475025812783 	 ± 0.2358896171236216
	data : 0.11738762855529786
	model : 0.06813592910766601
			 train-loss:  2.0747767276451237 	 ± 0.23399887103924383
	data : 0.1174849510192871
	model : 0.06885356903076172
			 train-loss:  2.0763650594219083 	 ± 0.23243538789726523
	data : 0.11686983108520507
	model : 0.06854166984558105
			 train-loss:  2.0820512468852694 	 ± 0.23488993122971544
	data : 0.11708078384399415
	model : 0.06852340698242188
			 train-loss:  2.081182327121496 	 ± 0.23314965934856824
	data : 0.11708083152770996
	model : 0.06854419708251953
			 train-loss:  2.0815434859349176 	 ± 0.23136728996730016
	data : 0.11697249412536621
	model : 0.06850876808166503
			 train-loss:  2.082606554031372 	 ± 0.22976772442756968
	data : 0.11708583831787109
	model : 0.06872749328613281
			 train-loss:  2.0831151257699996 	 ± 0.22808401841526582
	data : 0.117069673538208
	model : 0.06811809539794922
			 train-loss:  2.079033488736433 	 ± 0.22885254930925467
	data : 0.11748361587524414
	model : 0.06909961700439453
			 train-loss:  2.0878927863162495 	 ± 0.238645301008615
	data : 0.11669259071350098
	model : 0.0689704418182373
			 train-loss:  2.090189376899174 	 ± 0.23770131331348596
	data : 0.11674323081970214
	model : 0.06885156631469727
			 train-loss:  2.0906550699556377 	 ± 0.23605358214180494
	data : 0.11683378219604493
	model : 0.06872940063476562
			 train-loss:  2.092957130736775 	 ± 0.2352098002602842
	data : 0.11693406105041504
	model : 0.06870470046997071
			 train-loss:  2.092739848241414 	 ± 0.23360049477380945
	data : 0.11691389083862305
	model : 0.06867733001708984
			 train-loss:  2.0925712279371314 	 ± 0.23202121724389338
	data : 0.11679120063781738
	model : 0.06867194175720215
			 train-loss:  2.089765748977661 	 ± 0.231729353665428
	data : 0.11685390472412109
	model : 0.06891059875488281
			 train-loss:  2.0865580376825834 	 ± 0.2318698775361102
	data : 0.11663813591003418
	model : 0.0690394401550293
			 train-loss:  2.086858148698683 	 ± 0.2303741653909178
	data : 0.11663641929626464
	model : 0.07003574371337891
			 train-loss:  2.0862850898351426 	 ± 0.22894787412767936
	data : 0.11590209007263183
	model : 0.07000303268432617
			 train-loss:  2.0845791659777677 	 ± 0.22799257630208358
	data : 0.11604475975036621
	model : 0.07007393836975098
			 train-loss:  2.084632894396782 	 ± 0.22656364495644132
	data : 0.11602058410644531
	model : 0.06965270042419433
			 train-loss:  2.0912824854438687 	 ± 0.23288351809863386
	data : 0.11628680229187012
	model : 0.06949477195739746
			 train-loss:  2.0882714594282756 	 ± 0.2330401302566545
	data : 0.11619148254394532
	model : 0.06968326568603515
			 train-loss:  2.0933726221682076 	 ± 0.2361931101243067
	data : 0.11608676910400391
	model : 0.06970095634460449
			 train-loss:  2.0900104542573295 	 ± 0.23677267257070117
	data : 0.11606278419494628
	model : 0.06970372200012206
			 train-loss:  2.0863421762690826 	 ± 0.2377647575101008
	data : 0.11586623191833496
	model : 0.07012457847595215
			 train-loss:  2.085820972919464 	 ± 0.23642719946441781
	data : 0.11565241813659669
	model : 0.07036199569702148
			 train-loss:  2.0804231331266205 	 ± 0.24033533024069997
	data : 0.11543512344360352
	model : 0.06935076713562012
			 train-loss:  2.0841786590489475 	 ± 0.24151964925540814
	data : 0.11622118949890137
	model : 0.06969699859619141
			 train-loss:  2.0846124499031666 	 ± 0.24019343769752158
	data : 0.11594247817993164
	model : 0.0696601390838623
			 train-loss:  2.081696531507704 	 ± 0.24043415950608332
	data : 0.1161262035369873
	model : 0.06943168640136718
			 train-loss:  2.0774613998748443 	 ± 0.24246153607578694
	data : 0.11605958938598633
	model : 0.06926288604736328
			 train-loss:  2.0769863543303115 	 ± 0.2411827868544347
	data : 0.1163060188293457
	model : 0.06982212066650391
			 train-loss:  2.0803309461121917 	 ± 0.2420181835156452
	data : 0.1158970832824707
	model : 0.06929302215576172
			 train-loss:  2.0787995939559125 	 ± 0.24117996374700415
	data : 0.11640744209289551
	model : 0.06938018798828124
			 train-loss:  2.0814729125876177 	 ± 0.2413032656246561
	data : 0.11646337509155273
	model : 0.06960840225219726
			 train-loss:  2.0908325351774693 	 ± 0.2567936385652927
	data : 0.1163722038269043
	model : 0.0695688247680664
			 train-loss:  2.0859647473109137 	 ± 0.259880554836624
	data : 0.11625866889953614
	model : 0.06986308097839355
			 train-loss:  2.0887200576918468 	 ± 0.2599714192609366
	data : 0.11610994338989258
	model : 0.07006855010986328
			 train-loss:  2.0871105133885086 	 ± 0.259145408377675
	data : 0.11586885452270508
	model : 0.06980757713317871
			 train-loss:  2.086769913434982 	 ± 0.2578686954263846
	data : 0.11607789993286133
	model : 0.0698124885559082
			 train-loss:  2.0894978695576736 	 ± 0.2580349970243418
	data : 0.1161527156829834
	model : 0.06978912353515625
			 train-loss:  2.0866141073844013 	 ± 0.2583974049980989
	data : 0.1162449836730957
	model : 0.06893038749694824
			 train-loss:  2.0855068801676184 	 ± 0.2573830242676178
	data : 0.11681852340698243
	model : 0.06870093345642089
			 train-loss:  2.0848214523150372 	 ± 0.2562370597247444
	data : 0.11706690788269043
	model : 0.06875009536743164
			 train-loss:  2.0866302251815796 	 ± 0.2556802191747541
	data : 0.11695828437805175
	model : 0.06824121475219727
			 train-loss:  2.08471372554887 	 ± 0.2552279684943293
	data : 0.11751818656921387
	model : 0.06829657554626464
			 train-loss:  2.0843980234360027 	 ± 0.254053308079658
	data : 0.11755905151367188
	model : 0.06911177635192871
			 train-loss:  2.085169638748522 	 ± 0.25300033390196947
	data : 0.11698703765869141
	model : 0.06934223175048829
			 train-loss:  2.0919486721721263 	 ± 0.2615054505004776
	data : 0.11669225692749023
	model : 0.06949019432067871
			 train-loss:  2.093731553988023 	 ± 0.260978720729151
	data : 0.11661028861999512
	model : 0.06988210678100586
			 train-loss:  2.093415536322035 	 ± 0.25982162231981487
	data : 0.116245698928833
	model : 0.06981778144836426
			 train-loss:  2.096174138997282 	 ± 0.260286821683749
	data : 0.11619863510131836
	model : 0.06972680091857911
			 train-loss:  2.0988060214878184 	 ± 0.26062517347340247
	data : 0.11616544723510742
	model : 0.06965031623840331
			 train-loss:  2.1016700340990435 	 ± 0.26125951409622983
	data : 0.11634960174560546
	model : 0.06970291137695313
			 train-loss:  2.097278279843538 	 ± 0.2643137799100557
	data : 0.11629033088684082
	model : 0.06965136528015137
			 train-loss:  2.096924132314222 	 ± 0.2631994318574973
	data : 0.11624798774719239
	model : 0.06884288787841797
			 train-loss:  2.0937607461570673 	 ± 0.26427763687926864
	data : 0.1171147346496582
	model : 0.06898789405822754
			 train-loss:  2.0972478925171547 	 ± 0.2658449257090549
	data : 0.11693072319030762
	model : 0.06891374588012696
			 train-loss:  2.099201383710909 	 ± 0.26557472102362506
	data : 0.11689014434814453
	model : 0.06813273429870606
			 train-loss:  2.1016593227783837 	 ± 0.2658215891916805
	data : 0.11748642921447754
	model : 0.06843023300170899
			 train-loss:  2.0983992085969154 	 ± 0.26711896683250536
	data : 0.11734352111816407
	model : 0.06932110786437988
			 train-loss:  2.101067190287543 	 ± 0.26763590719275715
	data : 0.11651401519775391
	model : 0.06836304664611817
			 train-loss:  2.100174063589515 	 ± 0.2667282245733352
	data : 0.11732039451599122
	model : 0.0676987648010254
			 train-loss:  2.0997401100973927 	 ± 0.26569412321066505
	data : 0.11803011894226074
	model : 0.06765923500061036
			 train-loss:  2.1003759393692016 	 ± 0.26472391420272706
	data : 0.11807985305786133
	model : 0.0673912525177002
			 train-loss:  2.1006926969876365 	 ± 0.26369511203832163
	data : 0.11816220283508301
	model : 0.06734981536865234
			 train-loss:  2.0990821949140295 	 ± 0.2632762805080445
	data : 0.1181950569152832
	model : 0.06814789772033691
			 train-loss:  2.0997251821681857 	 ± 0.26234593015796237
	data : 0.1176342487335205
	model : 0.06895699501037597
			 train-loss:  2.0994083909101264 	 ± 0.2613516837440861
	data : 0.11677021980285644
	model : 0.07012434005737304
			 train-loss:  2.1044389330423794 	 ± 0.2665404312506716
	data : 0.11575479507446289
	model : 0.06942148208618164
			 train-loss:  2.1029474635160605 	 ± 0.26606515174139683
	data : 0.11647424697875977
	model : 0.06861157417297363
			 train-loss:  2.100849138064818 	 ± 0.2661412427365155
	data : 0.11723737716674805
	model : 0.06866421699523925
			 train-loss:  2.1012648075146783 	 ± 0.26518183020048475
	data : 0.11690359115600586
	model : 0.06862087249755859
			 train-loss:  2.102565961987225 	 ± 0.26461629933223196
	data : 0.11705431938171387
	model : 0.06838226318359375
			 train-loss:  2.1017231473216302 	 ± 0.2638148803158148
	data : 0.11719651222229004
	model : 0.06913690567016602
			 train-loss:  2.1003359766567455 	 ± 0.26333687866876426
	data : 0.11636052131652833
	model : 0.0700289249420166
			 train-loss:  2.1012737698798634 	 ± 0.2626018664122026
	data : 0.11555628776550293
	model : 0.06965899467468262
			 train-loss:  2.1004434625307717 	 ± 0.26182910684416366
	data : 0.11609435081481934
	model : 0.06966695785522461
			 train-loss:  2.0983870183821205 	 ± 0.2620016799840106
	data : 0.11607122421264648
	model : 0.06867418289184571
			 train-loss:  2.0975689445223127 	 ± 0.26124238701547514
	data : 0.11700215339660644
	model : 0.06877808570861817
			 train-loss:  2.0960881261960833 	 ± 0.260903343324226
	data : 0.11705479621887208
	model : 0.06886329650878906
			 train-loss:  2.0956056126406497 	 ± 0.2600461727865564
	data : 0.11705150604248046
	model : 0.06896748542785644
			 train-loss:  2.0959557011410905 	 ± 0.2591689035943814
	data : 0.1168405532836914
	model : 0.06852397918701172
			 train-loss:  2.0958482929401927 	 ± 0.2582706376134353
	data : 0.1172109603881836
	model : 0.0693655014038086
			 train-loss:  2.098850699950909 	 ± 0.2598880087154656
	data : 0.11648621559143066
	model : 0.0692990779876709
			 train-loss:  2.0963198893690764 	 ± 0.26078321932552584
	data : 0.11656370162963867
	model : 0.06936073303222656
			 train-loss:  2.0959726336861952 	 ± 0.259928556379998
	data : 0.11655750274658203
	model : 0.06965851783752441
			 train-loss:  2.096043330592078 	 ± 0.2590503490875612
	data : 0.11639280319213867
	model : 0.0703099250793457
			 train-loss:  2.0960671261653006 	 ± 0.2581797514417228
	data : 0.11590547561645508
	model : 0.07028694152832031
			 train-loss:  2.0984377495447792 	 ± 0.25893969166591463
	data : 0.11598672866821289
	model : 0.07003154754638671
			 train-loss:  2.098054750076193 	 ± 0.2581234763389872
	data : 0.11620106697082519
	model : 0.06953201293945313
			 train-loss:  2.0967764258384705 	 ± 0.2577520897220404
	data : 0.11648039817810059
	model : 0.06951618194580078
			 train-loss:  2.0952759778577517 	 ± 0.25757352528963495
	data : 0.11652240753173829
	model : 0.06937575340270996
			 train-loss:  2.0965236526031 	 ± 0.25719931739295476
	data : 0.11661763191223144
	model : 0.06947999000549317
			 train-loss:  2.0971308531299715 	 ± 0.2564790122215688
	data : 0.11628713607788085
	model : 0.06912922859191895
			 train-loss:  2.09638456427134 	 ± 0.25582442089625196
	data : 0.11656002998352051
	model : 0.06941494941711426
			 train-loss:  2.0965281466769565 	 ± 0.2550146977130067
	data : 0.11633682250976562
	model : 0.06848978996276855
			 train-loss:  2.0960131664819355 	 ± 0.254288291306916
	data : 0.11709489822387695
	model : 0.06751322746276855
			 train-loss:  2.094415214826476 	 ± 0.2542819252425328
	data : 0.11784467697143555
	model : 0.06728010177612305
			 train-loss:  2.093749015033245 	 ± 0.253625205016769
	data : 0.11828885078430176
	model : 0.06767168045043945
			 train-loss:  2.0943234973812697 	 ± 0.25294072552329405
	data : 0.1180485725402832
	model : 0.06772737503051758
			 train-loss:  2.094500082510489 	 ± 0.25216879032791545
	data : 0.11808309555053711
	model : 0.06871037483215332
			 train-loss:  2.0950651958676203 	 ± 0.25149695196569566
	data : 0.11718134880065918
	model : 0.06875247955322265
			 train-loss:  2.0939633882627255 	 ± 0.251123317286569
	data : 0.11729145050048828
	model : 0.06886987686157227
			 train-loss:  2.0943831754453255 	 ± 0.25041889157294533
	data : 0.11705427169799805
	model : 0.06888275146484375
			 train-loss:  2.095580185034189 	 ± 0.25013650218747346
	data : 0.11705522537231446
	model : 0.06889109611511231
			 train-loss:  2.095429786664997 	 ± 0.24939399410160137
	data : 0.11698946952819825
	model : 0.06880545616149902
			 train-loss:  2.0948422905944644 	 ± 0.2487665214817517
	data : 0.11700282096862794
	model : 0.06968827247619629
			 train-loss:  2.094223755351185 	 ± 0.24815897019693162
	data : 0.11588907241821289
	model : 0.06976442337036133
			 train-loss:  2.094752814489252 	 ± 0.24752358673451652
	data : 0.11591157913208008
	model : 0.06997251510620117
			 train-loss:  2.0959893869377715 	 ± 0.24732485311132932
	data : 0.11562819480895996
	model : 0.06908049583435058
			 train-loss:  2.0943625243597253 	 ± 0.2475207636790336
	data : 0.11641120910644531
	model : 0.07003750801086425
			 train-loss:  2.0942640587084553 	 ± 0.24680772737623424
	data : 0.11553106307983399
	model : 0.07018418312072754
			 train-loss:  2.0972612212444175 	 ± 0.2492348884606874
	data : 0.1155961036682129
	model : 0.07013921737670899
			 train-loss:  2.097155519894191 	 ± 0.24852567979918352
	data : 0.11563539505004883
	model : 0.06991677284240723
			 train-loss:  2.097420294853774 	 ± 0.24784338694868172
	data : 0.11587305068969726
	model : 0.07078418731689454
			 train-loss:  2.097293509601873 	 ± 0.24714799629396147
	data : 0.1151214599609375
	model : 0.06981930732727051
			 train-loss:  2.099081559797351 	 ± 0.24759819241372752
	data : 0.11614665985107422
	model : 0.06970791816711426
			 train-loss:  2.099694625625397 	 ± 0.247041050953366
	data : 0.11622204780578613
	model : 0.06979231834411621
			 train-loss:  2.100209473901325 	 ± 0.24645015033792345
	data : 0.11609692573547363
	model : 0.06973757743835449
			 train-loss:  2.0980733507904556 	 ± 0.24743373512438652
	data : 0.11600565910339355
	model : 0.06963777542114258
			 train-loss:  2.0979564779407376 	 ± 0.2467580454692043
	data : 0.11604962348937989
	model : 0.06962246894836426
			 train-loss:  2.097489328332286 	 ± 0.246163606060993
	data : 0.11619806289672852
	model : 0.06953511238098145
			 train-loss:  2.0976908945518993 	 ± 0.24550891455695378
	data : 0.11610908508300781
	model : 0.06954164505004883
			 train-loss:  2.096871422432564 	 ± 0.24509667621095235
	data : 0.11611838340759277
	model : 0.06987833976745605
			 train-loss:  2.0975822953767675 	 ± 0.24462808273871647
	data : 0.11600356101989746
	model : 0.07014613151550293
			 train-loss:  2.097820904165666 	 ± 0.24399482190231314
	data : 0.11579451560974122
	model : 0.07033538818359375
			 train-loss:  2.0963107467965876 	 ± 0.2442197233685248
	data : 0.11565828323364258
	model : 0.07045178413391114
			 train-loss:  2.0966907071058083 	 ± 0.24362849163627676
	data : 0.11576981544494629
	model : 0.0702336311340332
			 train-loss:  2.096222574459879 	 ± 0.24307173228690512
	data : 0.11595654487609863
	model : 0.0699554443359375
			 train-loss:  2.096561944297471 	 ± 0.24247971065107507
	data : 0.11618180274963379
	model : 0.06985683441162109
			 train-loss:  2.0972704806675515 	 ± 0.24204558520741146
	data : 0.11625370979309083
	model : 0.0696554183959961
			 train-loss:  2.097137077484724 	 ± 0.24142478639605358
	data : 0.11631865501403808
	model : 0.06969265937805176
			 train-loss:  2.097413426207513 	 ± 0.24083235601595127
	data : 0.1162071704864502
	model : 0.06984238624572754
			 train-loss:  2.0956818378888644 	 ± 0.2414217800696146
	data : 0.11616239547729493
	model : 0.06995820999145508
			 train-loss:  2.0963010562925923 	 ± 0.2409603188206883
	data : 0.11600141525268555
	model : 0.06989507675170899
			 train-loss:  2.0987166940863364 	 ± 0.24271560493977215
	data : 0.11601247787475585
	model : 0.06992249488830567
			 train-loss:  2.1015918742526662 	 ± 0.24544218764399853
	data : 0.11606159210205078
	model : 0.06989936828613282
			 train-loss:  2.101814585714484 	 ± 0.2448447781791319
	data : 0.11606316566467285
	model : 0.06971344947814942
			 train-loss:  2.1016009324789047 	 ± 0.24425049532782933
	data : 0.11612901687622071
	model : 0.06968774795532226
			 train-loss:  2.1002684161437686 	 ± 0.24436983639487117
	data : 0.11616063117980957
	model : 0.06973004341125488
			 train-loss:  2.0993632408651974 	 ± 0.24410177779441772
	data : 0.11619644165039063
	model : 0.06979570388793946
			 train-loss:  2.0978312057814574 	 ± 0.24447141546190243
	data : 0.11611595153808593
	model : 0.06978225708007812
			 train-loss:  2.096951970282723 	 ± 0.2441930204351003
	data : 0.11629395484924317
	model : 0.07002506256103516
			 train-loss:  2.095139946588656 	 ± 0.24496769531008167
	data : 0.11611776351928711
	model : 0.07017045021057129
			 train-loss:  2.0939589246962833 	 ± 0.24495673432706025
	data : 0.11599192619323731
	model : 0.06922869682312012
			 train-loss:  2.0955851498433358 	 ± 0.24547651119650124
	data : 0.11694226264953614
	model : 0.0695272445678711
			 train-loss:  2.096666980821353 	 ± 0.2453798613114526
	data : 0.11662945747375489
	model : 0.06950316429138184
			 train-loss:  2.0960142019262724 	 ± 0.24497309469696557
	data : 0.11658234596252441
	model : 0.06954770088195801
			 train-loss:  2.0970681417556034 	 ± 0.24486363858873206
	data : 0.11658878326416015
	model : 0.06952581405639649
			 train-loss:  2.0962136057315846 	 ± 0.2445963779206779
	data : 0.1164625644683838
	model : 0.07035970687866211
			 train-loss:  2.0966150170227267 	 ± 0.24408847182385873
	data : 0.11540160179138184
	model : 0.06994643211364746
			 train-loss:  2.0962319446841318 	 ± 0.24357868836123303
	data : 0.1158947467803955
	model : 0.06991896629333497
			 train-loss:  2.0951582627875784 	 ± 0.24351360661054364
	data : 0.11578898429870606
	model : 0.06979408264160156
			 train-loss:  2.0943643253903055 	 ± 0.2432240940090236
	data : 0.11587400436401367
	model : 0.06907892227172852
			 train-loss:  2.0943023126434395 	 ± 0.2426621257270386
	data : 0.11675801277160644
	model : 0.06924543380737305
			 train-loss:  2.0926739966265067 	 ± 0.2432822506097371
	data : 0.11680331230163574
	model : 0.0684018611907959
			 train-loss:  2.092837070653198 	 ± 0.24273550955146983
	data : 0.11759223937988281
	model : 0.06765079498291016
			 train-loss:  2.092198101897218 	 ± 0.24236437313620474
	data : 0.11829724311828613
	model : 0.06761980056762695
			 train-loss:  2.094463371146809 	 ± 0.24412552825674375
	data : 0.1183006763458252
	model : 0.0682805061340332
			 train-loss:  2.0931562821789567 	 ± 0.24434293231231935
	data : 0.11746797561645508
	model : 0.06812562942504882
			 train-loss:  2.0952372373761357 	 ± 0.24574691615479058
	data : 0.11758174896240234
	model : 0.06897687911987305
			 train-loss:  2.094623519166168 	 ± 0.24536574536299346
	data : 0.11666598320007324
	model : 0.06966152191162109
			 train-loss:  2.0954024254211356 	 ± 0.24509359958415472
	data : 0.11621813774108887
	model : 0.06888914108276367
			 train-loss:  2.094197737905714 	 ± 0.2452121037622461
	data : 0.11670002937316895
	model : 0.06808791160583497
			 train-loss:  2.094556772075923 	 ± 0.2447282618779099
	data : 0.11751213073730468
	model : 0.06806483268737792
			 train-loss:  2.0956597396455674 	 ± 0.24475093168541773
	data : 0.1175114631652832
	model : 0.06804757118225098
			 train-loss:  2.0942813931849966 	 ± 0.24509498050443793
	data : 0.1175168514251709
	model : 0.067230224609375
			 train-loss:  2.093286921884295 	 ± 0.2450198240429497
	data : 0.11820220947265625
	model : 0.06747074127197265
			 train-loss:  2.093443503068841 	 ± 0.24449807406900606
	data : 0.11797981262207032
	model : 0.06798667907714843
			 train-loss:  2.093911590514245 	 ± 0.2440715421477364
	data : 0.11734857559204101
	model : 0.06780524253845215
			 train-loss:  2.0932429278719016 	 ± 0.24375690520867432
	data : 0.11731510162353516
	model : 0.06755027770996094
			 train-loss:  2.092877873023692 	 ± 0.24329680619052227
	data : 0.11731758117675781
	model : 0.06810650825500489
			 train-loss:  2.091822291031862 	 ± 0.24331048879390743
	data : 0.11675662994384765
	model : 0.06854887008666992
			 train-loss:  2.0910773941811094 	 ± 0.24305949678566469
	data : 0.11657671928405762
	model : 0.06789708137512207
			 train-loss:  2.0912499210592044 	 ± 0.24255841298664968
	data : 0.11741347312927246
	model : 0.06763191223144531
			 train-loss:  2.089967534511904 	 ± 0.24284654027292657
	data : 0.11770291328430176
	model : 0.06773605346679687
			 train-loss:  2.0906325698900625 	 ± 0.24255199200532643
	data : 0.11754655838012695
	model : 0.06779313087463379
			 train-loss:  2.0900398587582 	 ± 0.2422166860396921
	data : 0.11756515502929688
	model : 0.06790781021118164
			 train-loss:  2.090852591395378 	 ± 0.24203788310382854
	data : 0.11748437881469727
	model : 0.06871070861816406
			 train-loss:  2.09209300967173 	 ± 0.24229843091017628
	data : 0.11678681373596192
	model : 0.06867618560791015
			 train-loss:  2.091510640688179 	 ± 0.24196625470764693
	data : 0.11715354919433593
	model : 0.06883845329284669
			 train-loss:  2.0920152124554043 	 ± 0.24159541166152756
	data : 0.11720871925354004
	model : 0.06894464492797851
			 train-loss:  2.0923837149729496 	 ± 0.24116825326583738
	data : 0.11703014373779297
	model : 0.06815180778503419
			 train-loss:  2.0943172153161496 	 ± 0.24256319857287395
	data : 0.11772680282592773
	model : 0.06820144653320312
			 train-loss:  2.092871198809244 	 ± 0.24312551697832327
	data : 0.11755037307739258
	model : 0.06873984336853027
			 train-loss:  2.093705553757517 	 ± 0.2429855091326318
	data : 0.11699318885803223
	model : 0.06849079132080078
			 train-loss:  2.0947642864719516 	 ± 0.24306532297685562
	data : 0.11713457107543945
	model : 0.06831245422363282
			 train-loss:  2.0957555349572115 	 ± 0.24307850045468468
	data : 0.11707015037536621
	model : 0.06886887550354004
			 train-loss:  2.0936570639610292 	 ± 0.2448413778422635
	data : 0.11644554138183594
	model : 0.06873130798339844
			 train-loss:  2.092931470073077 	 ± 0.24462233761556745
	data : 0.11655349731445312
	model : 0.068109130859375
			 train-loss:  2.092706985889919 	 ± 0.24416239692849587
	data : 0.11676888465881348
	model : 0.06818213462829589
			 train-loss:  2.0919189735834776 	 ± 0.24400025685881177
	data : 0.11674056053161622
	model : 0.06823315620422363
			 train-loss:  2.094109435719768 	 ± 0.2459993020693914
	data : 0.11697211265563964
	model : 0.06816220283508301
			 train-loss:  2.0925880815468583 	 ± 0.24671081861583938
	data : 0.1169588565826416
	model : 0.06788063049316406
			 train-loss:  2.0912179183214903 	 ± 0.24719869107914932
	data : 0.11614084243774414
	model : 0.05916862487792969
#epoch  17    val-loss:  2.46767837122867  train-loss:  2.0912179183214903  lr:  0.00125
			 train-loss:  2.222132444381714 	 ± 0.0
	data : 5.494163990020752
	model : 0.07160520553588867
			 train-loss:  2.1697176694869995 	 ± 0.052414774894714355
	data : 2.814636707305908
	model : 0.07173192501068115
			 train-loss:  2.148495356241862 	 ± 0.052271524215311206
	data : 1.9162193934122722
	model : 0.07020107905069987
			 train-loss:  2.0657106041908264 	 ± 0.1503634919097658
	data : 1.4664955735206604
	model : 0.06909030675888062
			 train-loss:  2.028488779067993 	 ± 0.15371792615481675
	data : 1.1969907760620118
	model : 0.06848840713500977
			 train-loss:  2.058019757270813 	 ± 0.15508511873331102
	data : 0.12208046913146972
	model : 0.06800141334533691
			 train-loss:  2.02154849256788 	 ± 0.16910469984787138
	data : 0.1184476375579834
	model : 0.06738820075988769
			 train-loss:  2.0351473838090897 	 ± 0.1622231748941581
	data : 0.11798629760742188
	model : 0.06790213584899903
			 train-loss:  2.035315844747755 	 ± 0.15294621824399238
	data : 0.11762266159057617
	model : 0.06868853569030761
			 train-loss:  2.0307228803634643 	 ± 0.14575029675490658
	data : 0.11703982353210449
	model : 0.06848278045654296
			 train-loss:  2.019327001138167 	 ± 0.14356397275537464
	data : 0.11711006164550782
	model : 0.06874370574951172
			 train-loss:  2.043294976154963 	 ± 0.15878339073822312
	data : 0.11677350997924804
	model : 0.06882009506225586
			 train-loss:  2.03474720624777 	 ± 0.15540122360477404
	data : 0.11675782203674316
	model : 0.06867818832397461
			 train-loss:  2.015132818903242 	 ± 0.16560793343147365
	data : 0.11704654693603515
	model : 0.06864371299743652
			 train-loss:  2.017278210322062 	 ± 0.16019371492614742
	data : 0.11693077087402344
	model : 0.06935901641845703
			 train-loss:  2.002745032310486 	 ± 0.16500408663035376
	data : 0.11625237464904785
	model : 0.06923723220825195
			 train-loss:  2.011130964054781 	 ± 0.16355421213259028
	data : 0.11644186973571777
	model : 0.06905498504638671
			 train-loss:  2.0179305738872952 	 ± 0.16139969047417727
	data : 0.11663012504577637
	model : 0.06936526298522949
			 train-loss:  2.0042633571122823 	 ± 0.16745472664655886
	data : 0.11639857292175293
	model : 0.06961112022399903
			 train-loss:  2.020229786634445 	 ± 0.17743348546567478
	data : 0.11641607284545899
	model : 0.07004294395446778
			 train-loss:  2.019964269229344 	 ± 0.17316142277363342
	data : 0.11609320640563965
	model : 0.0699953556060791
			 train-loss:  2.021395580335097 	 ± 0.16930726801523263
	data : 0.1163170337677002
	model : 0.07019104957580566
			 train-loss:  2.0251539738281914 	 ± 0.16652150089867115
	data : 0.1162841796875
	model : 0.0701174259185791
			 train-loss:  2.018040969967842 	 ± 0.16654638728418558
	data : 0.11629424095153809
	model : 0.06998643875122071
			 train-loss:  2.020465235710144 	 ± 0.16361308229496976
	data : 0.11639919281005859
	model : 0.0701378345489502
			 train-loss:  2.024172008037567 	 ± 0.16150281280692808
	data : 0.11628718376159668
	model : 0.07076144218444824
			 train-loss:  2.0234390232298107 	 ± 0.15852786685763073
	data : 0.11550741195678711
	model : 0.0713655948638916
			 train-loss:  2.0145682437079295 	 ± 0.16235207275415353
	data : 0.11481642723083496
	model : 0.07162585258483886
			 train-loss:  2.0102124337492318 	 ± 0.16118479607150268
	data : 0.11457986831665039
	model : 0.07148356437683105
			 train-loss:  2.0061295509338377 	 ± 0.1599935891389645
	data : 0.1147082805633545
	model : 0.07050628662109375
			 train-loss:  2.0044577314007666 	 ± 0.15765804040959627
	data : 0.11561923027038574
	model : 0.06993465423583985
			 train-loss:  2.0047418661415577 	 ± 0.15518314538494987
	data : 0.11611008644104004
	model : 0.06955246925354004
			 train-loss:  2.012570139133569 	 ± 0.15910083686975662
	data : 0.11654915809631347
	model : 0.06904425621032714
			 train-loss:  2.0201753833714653 	 ± 0.16271842386789237
	data : 0.11689934730529786
	model : 0.0691213607788086
			 train-loss:  2.012221874509539 	 ± 0.16694781791015334
	data : 0.11679162979125976
	model : 0.06884627342224121
			 train-loss:  2.0091335078080497 	 ± 0.16562365117824562
	data : 0.11705284118652344
	model : 0.06880254745483398
			 train-loss:  2.0227252373824247 	 ± 0.18259319339510652
	data : 0.11720843315124511
	model : 0.06860594749450684
			 train-loss:  2.0253841782871045 	 ± 0.18089910835768885
	data : 0.11739816665649414
	model : 0.06884350776672363
			 train-loss:  2.024020384519528 	 ± 0.17876262320382905
	data : 0.11723251342773437
	model : 0.06881461143493653
			 train-loss:  2.024296796321869 	 ± 0.1765223875440188
	data : 0.11721644401550294
	model : 0.06948933601379395
			 train-loss:  2.0381194207726456 	 ± 0.19504550093093986
	data : 0.11660518646240234
	model : 0.06861820220947265
			 train-loss:  2.037912119002569 	 ± 0.19271411382559644
	data : 0.11722660064697266
	model : 0.06788911819458007
			 train-loss:  2.02950806673183 	 ± 0.19809446799917982
	data : 0.11774721145629882
	model : 0.06797637939453124
			 train-loss:  2.0315900363705377 	 ± 0.19630577050319267
	data : 0.11798639297485351
	model : 0.06797943115234376
			 train-loss:  2.025424673822191 	 ± 0.19837367658277597
	data : 0.11792216300964356
	model : 0.06828427314758301
			 train-loss:  2.0202866559443264 	 ± 0.1992099395989058
	data : 0.117791748046875
	model : 0.06864047050476074
			 train-loss:  2.0312039167323013 	 ± 0.2105298783212191
	data : 0.11751842498779297
	model : 0.06897892951965331
			 train-loss:  2.028399703403314 	 ± 0.20921048531032121
	data : 0.11713600158691406
	model : 0.06798005104064941
			 train-loss:  2.026760909022117 	 ± 0.2073757284944468
	data : 0.1179387092590332
	model : 0.06787958145141601
			 train-loss:  2.0294321298599245 	 ± 0.20614129748956878
	data : 0.11804676055908203
	model : 0.06715121269226074
			 train-loss:  2.033486370946847 	 ± 0.2061137011101204
	data : 0.11866059303283691
	model : 0.06734819412231445
			 train-loss:  2.036085390127622 	 ± 0.20496433759750274
	data : 0.11847772598266601
	model : 0.06730561256408692
			 train-loss:  2.042515511782664 	 ± 0.2082492428974877
	data : 0.11853113174438476
	model : 0.06864590644836426
			 train-loss:  2.041269264839314 	 ± 0.20651139606538257
	data : 0.11734914779663086
	model : 0.06871724128723145
			 train-loss:  2.036718492074446 	 ± 0.20733999194791197
	data : 0.11721906661987305
	model : 0.06923065185546876
			 train-loss:  2.0379069426230023 	 ± 0.20566934331454426
	data : 0.11672554016113282
	model : 0.06959977149963378
			 train-loss:  2.0394328623487237 	 ± 0.2041768051946105
	data : 0.11642422676086425
	model : 0.07014799118041992
			 train-loss:  2.040729146579216 	 ± 0.20264546966560443
	data : 0.11609816551208496
	model : 0.06992273330688477
			 train-loss:  2.040909805540311 	 ± 0.20092550672380724
	data : 0.11605048179626465
	model : 0.06910943984985352
			 train-loss:  2.0440668523311616 	 ± 0.20071436915831214
	data : 0.11695265769958496
	model : 0.0683809757232666
			 train-loss:  2.041201876812294 	 ± 0.2002955631341963
	data : 0.1174851894378662
	model : 0.0685183048248291
			 train-loss:  2.0403550382583373 	 ± 0.19878377278018305
	data : 0.117439603805542
	model : 0.06850724220275879
			 train-loss:  2.040844128245399 	 ± 0.1972374133952383
	data : 0.11737289428710937
	model : 0.06804847717285156
			 train-loss:  2.045383622869849 	 ± 0.19897986300624892
	data : 0.11780109405517578
	model : 0.06874284744262696
			 train-loss:  2.0471845828569855 	 ± 0.19796829105258168
	data : 0.11705670356750489
	model : 0.06868600845336914
			 train-loss:  2.043397038271933 	 ± 0.19882175839096272
	data : 0.11717643737792968
	model : 0.06849799156188965
			 train-loss:  2.0466722854927406 	 ± 0.19911827733775395
	data : 0.11724286079406739
	model : 0.06853756904602051
			 train-loss:  2.0422603715868557 	 ± 0.20092083564387753
	data : 0.11714954376220703
	model : 0.06820721626281738
			 train-loss:  2.043597544448963 	 ± 0.19976413018906786
	data : 0.11753830909729004
	model : 0.06845083236694335
			 train-loss:  2.0461573447499957 	 ± 0.1994686793072493
	data : 0.11742138862609863
	model : 0.06948776245117187
			 train-loss:  2.050990751091863 	 ± 0.20214522082754255
	data : 0.11648020744323731
	model : 0.06873493194580078
			 train-loss:  2.0473549796475305 	 ± 0.20306080407544402
	data : 0.11697392463684082
	model : 0.06860008239746093
			 train-loss:  2.0472168661143684 	 ± 0.2016685858190432
	data : 0.11700358390808105
	model : 0.06913938522338867
			 train-loss:  2.049312823527568 	 ± 0.20110025533241166
	data : 0.11653151512145996
	model : 0.06818113327026368
			 train-loss:  2.0494302876790367 	 ± 0.19975764373698818
	data : 0.11742887496948243
	model : 0.06816654205322266
			 train-loss:  2.054719965708883 	 ± 0.20365811364334982
	data : 0.11743397712707519
	model : 0.0687469482421875
			 train-loss:  2.0574511893383867 	 ± 0.20372750925456704
	data : 0.11713628768920899
	model : 0.06883349418640136
			 train-loss:  2.0624437698951135 	 ± 0.20710402028252792
	data : 0.11705865859985351
	model : 0.06801414489746094
			 train-loss:  2.063148447229892 	 ± 0.2058831465361079
	data : 0.11774640083312989
	model : 0.06926884651184081
			 train-loss:  2.0604511320590975 	 ± 0.2059922010018362
	data : 0.11673603057861329
	model : 0.0684096336364746
			 train-loss:  2.058999822463518 	 ± 0.20512783641090246
	data : 0.1176304817199707
	model : 0.06881966590881347
			 train-loss:  2.0560305438390594 	 ± 0.20561720684026466
	data : 0.1174008846282959
	model : 0.0687936782836914
			 train-loss:  2.0553532968084496 	 ± 0.20446678804918256
	data : 0.11751503944396972
	model : 0.06947159767150879
			 train-loss:  2.05788863272894 	 ± 0.2045543620571507
	data : 0.11704225540161133
	model : 0.06914348602294922
			 train-loss:  2.059476709365845 	 ± 0.20386777436669434
	data : 0.11722736358642578
	model : 0.06990203857421876
			 train-loss:  2.0606863055118296 	 ± 0.20298560291134993
	data : 0.11632142066955567
	model : 0.06971616744995117
			 train-loss:  2.0595502812286903 	 ± 0.2020904324984996
	data : 0.1163184642791748
	model : 0.06885862350463867
			 train-loss:  2.060636280612512 	 ± 0.20119406841281784
	data : 0.11708278656005859
	model : 0.06897430419921875
			 train-loss:  2.06034340349476 	 ± 0.20007943602220998
	data : 0.11688766479492188
	model : 0.06878695487976075
			 train-loss:  2.062132836712731 	 ± 0.19967966219903846
	data : 0.11714205741882325
	model : 0.06820173263549804
			 train-loss:  2.065273438181196 	 ± 0.2008021862169109
	data : 0.11776952743530274
	model : 0.06796259880065918
			 train-loss:  2.068517056496247 	 ± 0.20209071492636627
	data : 0.11803021430969238
	model : 0.06882662773132324
			 train-loss:  2.0687495623865435 	 ± 0.201013640423575
	data : 0.11728434562683106
	model : 0.06936020851135254
			 train-loss:  2.0678811973713813 	 ± 0.20011685287051645
	data : 0.11676101684570313
	model : 0.06878600120544434
			 train-loss:  2.0696789076453763 	 ± 0.19982241031713457
	data : 0.11723427772521973
	model : 0.06941633224487305
			 train-loss:  2.0693259624143443 	 ± 0.19880870908665702
	data : 0.11657247543334961
	model : 0.06888957023620605
			 train-loss:  2.0709962095181966 	 ± 0.19845715714257345
	data : 0.11683897972106934
	model : 0.06811294555664063
			 train-loss:  2.0767068437167575 	 ± 0.2052965008232972
	data : 0.1174534797668457
	model : 0.06764154434204102
			 train-loss:  2.0750350193543867 	 ± 0.20492642376362682
	data : 0.11787724494934082
	model : 0.0675356388092041
			 train-loss:  2.0713830065727232 	 ± 0.20711174110457836
	data : 0.11791133880615234
	model : 0.06665892601013183
			 train-loss:  2.0712406753313424 	 ± 0.20608879990267842
	data : 0.11892743110656738
	model : 0.06726341247558594
			 train-loss:  2.0711129903793335 	 ± 0.20508008708432895
	data : 0.11851768493652344
	model : 0.06803207397460938
			 train-loss:  2.0740345658607855 	 ± 0.2062041345129341
	data : 0.1176997184753418
	model : 0.06706295013427735
			 train-loss:  2.0745962720650892 	 ± 0.20528954057345183
	data : 0.11860613822937012
	model : 0.06714458465576172
			 train-loss:  2.07556791305542 	 ± 0.20454977622688458
	data : 0.1184420108795166
	model : 0.06802163124084473
			 train-loss:  2.075515294974705 	 ± 0.20358334636990566
	data : 0.11759510040283203
	model : 0.06770429611206055
			 train-loss:  2.0742263292597833 	 ± 0.20306388972376277
	data : 0.11802997589111328
	model : 0.0678835391998291
			 train-loss:  2.073666516277525 	 ± 0.20220452781932194
	data : 0.11791572570800782
	model : 0.06888298988342285
			 train-loss:  2.074161018800298 	 ± 0.20134044187161085
	data : 0.11696252822875977
	model : 0.0695688247680664
			 train-loss:  2.0794871666214685 	 ± 0.20799409178567815
	data : 0.1164320945739746
	model : 0.06944475173950196
			 train-loss:  2.0771001459241987 	 ± 0.20856309369871037
	data : 0.11645007133483887
	model : 0.06997394561767578
			 train-loss:  2.0789706132241657 	 ± 0.2085630214974248
	data : 0.11607065200805664
	model : 0.06962571144104004
			 train-loss:  2.0797435963048345 	 ± 0.20779920951811964
	data : 0.11652450561523438
	model : 0.06966123580932618
			 train-loss:  2.0792952985094306 	 ± 0.20694067935789295
	data : 0.11653194427490235
	model : 0.06929469108581543
			 train-loss:  2.0781504060911096 	 ± 0.20640127679562606
	data : 0.11680126190185547
	model : 0.06947536468505859
			 train-loss:  2.078082416591973 	 ± 0.2055109837936309
	data : 0.11677818298339844
	model : 0.06927142143249512
			 train-loss:  2.079923598175375 	 ± 0.20558943971732038
	data : 0.11681804656982422
	model : 0.06862411499023438
			 train-loss:  2.084380950968144 	 ± 0.21031732678592524
	data : 0.11752405166625976
	model : 0.0677567481994629
			 train-loss:  2.0819120547350716 	 ± 0.21114197077783256
	data : 0.11819868087768555
	model : 0.06814088821411132
			 train-loss:  2.083879025777181 	 ± 0.2113523884258863
	data : 0.11791181564331055
	model : 0.06809215545654297
			 train-loss:  2.083402661252613 	 ± 0.21054189774602627
	data : 0.11792325973510742
	model : 0.06735291481018066
			 train-loss:  2.083144015953189 	 ± 0.20969654723993927
	data : 0.11852855682373047
	model : 0.0684194564819336
			 train-loss:  2.0849051669361147 	 ± 0.20974637639354826
	data : 0.11751422882080079
	model : 0.0691004753112793
			 train-loss:  2.0867709190614763 	 ± 0.209921229488726
	data : 0.11688179969787597
	model : 0.06902461051940918
			 train-loss:  2.085780830383301 	 ± 0.20937034504097907
	data : 0.11684088706970215
	model : 0.06893119812011719
			 train-loss:  2.084943337099893 	 ± 0.20874796157294734
	data : 0.11684679985046387
	model : 0.06921796798706055
			 train-loss:  2.0852454750556646 	 ± 0.2079521527370425
	data : 0.11663060188293457
	model : 0.06899676322937012
			 train-loss:  2.085733608342707 	 ± 0.20721127895252459
	data : 0.11680779457092286
	model : 0.06823549270629883
			 train-loss:  2.087768738584001 	 ± 0.20768682573299652
	data : 0.11755671501159667
	model : 0.06802706718444824
			 train-loss:  2.0883871014301594 	 ± 0.20700566430144388
	data : 0.11768465042114258
	model : 0.06789121627807618
			 train-loss:  2.0868204841176974 	 ± 0.20698621505584228
	data : 0.11780967712402343
	model : 0.06831517219543456
			 train-loss:  2.0863157203703215 	 ± 0.20628160326036066
	data : 0.11736998558044434
	model : 0.06825218200683594
			 train-loss:  2.0878369216632127 	 ± 0.20624648915469482
	data : 0.11728663444519043
	model : 0.06874585151672363
			 train-loss:  2.090361767740392 	 ± 0.20752836951071343
	data : 0.11698236465454101
	model : 0.06911029815673828
			 train-loss:  2.0899696509043375 	 ± 0.2068081357268342
	data : 0.11688942909240722
	model : 0.06851015090942383
			 train-loss:  2.091150254011154 	 ± 0.20650251526801924
	data : 0.11734991073608399
	model : 0.06870136260986329
			 train-loss:  2.092935111400855 	 ± 0.20679768202954393
	data : 0.117254638671875
	model : 0.06842951774597168
			 train-loss:  2.0903700641963794 	 ± 0.20822289727879045
	data : 0.11743855476379395
	model : 0.06866035461425782
			 train-loss:  2.089469305902934 	 ± 0.2077422059822311
	data : 0.11721930503845215
	model : 0.06865739822387695
			 train-loss:  2.0890892727034434 	 ± 0.20704742506028478
	data : 0.11717453002929687
	model : 0.06855649948120117
			 train-loss:  2.0903877501792096 	 ± 0.2068831783127436
	data : 0.11731338500976562
	model : 0.06856093406677247
			 train-loss:  2.090044335580208 	 ± 0.2061937561617574
	data : 0.11718206405639649
	model : 0.06817183494567872
			 train-loss:  2.0906028680868083 	 ± 0.2055793021048748
	data : 0.11764111518859863
	model : 0.06808404922485352
			 train-loss:  2.09067105087969 	 ± 0.2048658640075616
	data : 0.11765942573547364
	model : 0.06812267303466797
			 train-loss:  2.0899208611455458 	 ± 0.20435658735189585
	data : 0.11768054962158203
	model : 0.06814265251159668
			 train-loss:  2.0918372951141775 	 ± 0.20495882663173448
	data : 0.11779694557189942
	model : 0.06818499565124511
			 train-loss:  2.092047866509885 	 ± 0.20427634413196755
	data : 0.11767158508300782
	model : 0.06894588470458984
			 train-loss:  2.0950703347051465 	 ± 0.20685686938055434
	data : 0.11706352233886719
	model : 0.06921548843383789
			 train-loss:  2.095682420986611 	 ± 0.20629598378585332
	data : 0.11675887107849121
	model : 0.06934180259704589
			 train-loss:  2.096955189704895 	 ± 0.2061933165696489
	data : 0.11669788360595704
	model : 0.06939187049865722
			 train-loss:  2.094059184687027 	 ± 0.20854771573350936
	data : 0.11662721633911133
	model : 0.06975693702697754
			 train-loss:  2.0935493346891905 	 ± 0.2079549691006816
	data : 0.11643214225769043
	model : 0.06882128715515137
			 train-loss:  2.091829253178017 	 ± 0.20835628086791458
	data : 0.11722788810729981
	model : 0.0688161849975586
			 train-loss:  2.0925573497623593 	 ± 0.207873881322778
	data : 0.11734404563903808
	model : 0.06848101615905762
			 train-loss:  2.091419784484371 	 ± 0.20768257214433775
	data : 0.11755909919738769
	model : 0.06843333244323731
			 train-loss:  2.0909391244252524 	 ± 0.20710232628307965
	data : 0.11767888069152832
	model : 0.06794767379760742
			 train-loss:  2.0931259856861866 	 ± 0.20824079472808218
	data : 0.1179234504699707
	model : 0.0689432144165039
			 train-loss:  2.0915618750113474 	 ± 0.2085038677624685
	data : 0.11693458557128907
	model : 0.06968984603881836
			 train-loss:  2.093839742102713 	 ± 0.20981004158876243
	data : 0.11608600616455078
	model : 0.06928548812866211
			 train-loss:  2.093474894016981 	 ± 0.20920394860700353
	data : 0.11643295288085938
	model : 0.07018465995788574
			 train-loss:  2.094872462083094 	 ± 0.20930113050097682
	data : 0.11538372039794922
	model : 0.06945466995239258
			 train-loss:  2.0954126627356917 	 ± 0.2087666936359279
	data : 0.11647181510925293
	model : 0.06988954544067383
			 train-loss:  2.0970446105383655 	 ± 0.2091592594786589
	data : 0.11588582992553711
	model : 0.06997251510620117
			 train-loss:  2.095847889417555 	 ± 0.209079605049033
	data : 0.11581993103027344
	model : 0.0702406883239746
			 train-loss:  2.0948196664001 	 ± 0.2088605611207406
	data : 0.11547374725341797
	model : 0.07013916969299316
			 train-loss:  2.092602295329772 	 ± 0.21016947357783972
	data : 0.1157191276550293
	model : 0.07069816589355468
			 train-loss:  2.092267930864574 	 ± 0.20958355877217158
	data : 0.11508159637451172
	model : 0.07100768089294433
			 train-loss:  2.091093496907325 	 ± 0.2095093087751144
	data : 0.11512789726257325
	model : 0.07019414901733398
			 train-loss:  2.0896586807521844 	 ± 0.20971476563537456
	data : 0.11593103408813477
	model : 0.06955528259277344
			 train-loss:  2.0876316968132467 	 ± 0.21075089460325327
	data : 0.11657419204711914
	model : 0.06972618103027343
			 train-loss:  2.090412434081585 	 ± 0.21323865080657575
	data : 0.11630573272705078
	model : 0.07009930610656738
			 train-loss:  2.0892957067766855 	 ± 0.21311876546244798
	data : 0.11598358154296876
	model : 0.06942715644836425
			 train-loss:  2.090572989055876 	 ± 0.21316115149808093
	data : 0.11659231185913085
	model : 0.06846590042114258
			 train-loss:  2.0923059308665923 	 ± 0.21376640180895268
	data : 0.11745543479919433
	model : 0.07016730308532715
			 train-loss:  2.09169974735805 	 ± 0.2133046926019195
	data : 0.11572942733764649
	model : 0.06937851905822753
			 train-loss:  2.0949969427152113 	 ± 0.21712412917247226
	data : 0.11648168563842773
	model : 0.06828279495239258
			 train-loss:  2.0951131844924666 	 ± 0.2165154073280748
	data : 0.11723012924194336
	model : 0.0673604965209961
			 train-loss:  2.095746063114552 	 ± 0.21607047840761442
	data : 0.11804141998291015
	model : 0.06819443702697754
			 train-loss:  2.0959085285996593 	 ± 0.21547698668242557
	data : 0.11741957664489747
	model : 0.06734981536865234
			 train-loss:  2.0954026089774236 	 ± 0.2149841886904769
	data : 0.1183743953704834
	model : 0.06787877082824707
			 train-loss:  2.0972167589387842 	 ± 0.21576667356842685
	data : 0.11797256469726562
	model : 0.06877350807189941
			 train-loss:  2.0978902146056457 	 ± 0.21536376320995315
	data : 0.11727371215820312
	model : 0.06962442398071289
			 train-loss:  2.0970668962093 	 ± 0.21506154627776528
	data : 0.11641402244567871
	model : 0.06970634460449218
			 train-loss:  2.0969621279965276 	 ± 0.21448102640323016
	data : 0.11624150276184082
	model : 0.06968259811401367
			 train-loss:  2.095729378107432 	 ± 0.21455318764913225
	data : 0.11612257957458497
	model : 0.06981701850891113
			 train-loss:  2.095979323951147 	 ± 0.21400265932006013
	data : 0.11590256690979003
	model : 0.0696523666381836
			 train-loss:  2.0965095968807446 	 ± 0.21355218317399746
	data : 0.11612014770507813
	model : 0.06961760520935059
			 train-loss:  2.095654954935642 	 ± 0.21330387907545886
	data : 0.11616969108581543
	model : 0.06908588409423828
			 train-loss:  2.0967411080365457 	 ± 0.2132594681514375
	data : 0.11679630279541016
	model : 0.07008910179138184
			 train-loss:  2.095147288473029 	 ± 0.21382315994265608
	data : 0.11585865020751954
	model : 0.0702364444732666
			 train-loss:  2.0941130715514977 	 ± 0.21373861392420448
	data : 0.11578712463378907
	model : 0.07044143676757812
			 train-loss:  2.096071912596623 	 ± 0.21489331001694723
	data : 0.11561589241027832
	model : 0.07085356712341309
			 train-loss:  2.096346247381497 	 ± 0.21436957433450518
	data : 0.11505560874938965
	model : 0.07141809463500977
			 train-loss:  2.0967302420704637 	 ± 0.2138828995311677
	data : 0.1146127700805664
	model : 0.07053661346435547
			 train-loss:  2.096261864442092 	 ± 0.21343350166958133
	data : 0.11551342010498047
	model : 0.07042250633239747
			 train-loss:  2.097659870069854 	 ± 0.2137815563422516
	data : 0.11549839973449708
	model : 0.07047543525695801
			 train-loss:  2.0963624582677927 	 ± 0.2140104753052011
	data : 0.11533408164978028
	model : 0.06975803375244141
			 train-loss:  2.094380539475065 	 ± 0.2152742074211687
	data : 0.11620416641235351
	model : 0.06953105926513672
			 train-loss:  2.0936024183004944 	 ± 0.21501160115131993
	data : 0.11620612144470215
	model : 0.06866292953491211
			 train-loss:  2.091985182762146 	 ± 0.21568336373725944
	data : 0.11726956367492676
	model : 0.06870613098144532
			 train-loss:  2.0923100193934654 	 ± 0.21519520856727697
	data : 0.11726970672607422
	model : 0.06866135597229003
			 train-loss:  2.093635088146323 	 ± 0.2154823488645299
	data : 0.1174656867980957
	model : 0.06892681121826172
			 train-loss:  2.0939664746740183 	 ± 0.21500254296745383
	data : 0.11718144416809081
	model : 0.06917357444763184
			 train-loss:  2.093502170314976 	 ± 0.2145769265978692
	data : 0.11700553894042968
	model : 0.07010269165039062
			 train-loss:  2.093859930154754 	 ± 0.2141139102474277
	data : 0.11595315933227539
	model : 0.06983213424682617
			 train-loss:  2.0936307056436263 	 ± 0.213618797500674
	data : 0.11622233390808105
	model : 0.06977767944335937
			 train-loss:  2.091561017980898 	 ± 0.21516264653010528
	data : 0.11622204780578613
	model : 0.06962127685546875
			 train-loss:  2.090597279369831 	 ± 0.21509219531209503
	data : 0.1163750171661377
	model : 0.06937975883483886
			 train-loss:  2.089956658308586 	 ± 0.21477581973337462
	data : 0.11649799346923828
	model : 0.06888179779052735
			 train-loss:  2.0887341749100456 	 ± 0.21499147846802794
	data : 0.11705622673034669
	model : 0.06902580261230469
			 train-loss:  2.0875584734559625 	 ± 0.21515704699408242
	data : 0.11681780815124512
	model : 0.06873555183410644
			 train-loss:  2.085394521929183 	 ± 0.21693833352751093
	data : 0.1170964241027832
	model : 0.06885695457458496
			 train-loss:  2.0865621813026394 	 ± 0.21709522770194428
	data : 0.11717939376831055
	model : 0.06915373802185058
			 train-loss:  2.0871593863050513 	 ± 0.21676270419457427
	data : 0.11706361770629883
	model : 0.06958956718444824
			 train-loss:  2.0896123542342075 	 ± 0.21921491538022764
	data : 0.11660313606262207
	model : 0.06962351799011231
			 train-loss:  2.090844792348367 	 ± 0.2194521946589976
	data : 0.11662650108337402
	model : 0.069744873046875
			 train-loss:  2.0931019585253456 	 ± 0.22144482440772975
	data : 0.1163719654083252
	model : 0.0695101261138916
			 train-loss:  2.0941779788480988 	 ± 0.22150420550344588
	data : 0.1163947582244873
	model : 0.06936144828796387
			 train-loss:  2.096426059121955 	 ± 0.22347666223088103
	data : 0.1163444995880127
	model : 0.06964921951293945
			 train-loss:  2.0946558675982736 	 ± 0.22450181265269886
	data : 0.11607165336608886
	model : 0.0699458122253418
			 train-loss:  2.093595107216641 	 ± 0.22454521046092651
	data : 0.1159212589263916
	model : 0.07003870010375976
			 train-loss:  2.0929501711785257 	 ± 0.224243963643208
	data : 0.1159515380859375
	model : 0.06965947151184082
			 train-loss:  2.091735823272055 	 ± 0.2244710017573252
	data : 0.1163294792175293
	model : 0.0696824073791504
			 train-loss:  2.090862781341587 	 ± 0.22434852067577526
	data : 0.11627907752990722
	model : 0.0685920238494873
			 train-loss:  2.090962748527527 	 ± 0.22385441325087566
	data : 0.11738996505737305
	model : 0.06748595237731933
			 train-loss:  2.0908940001926593 	 ± 0.22336099158264652
	data : 0.11829733848571777
	model : 0.06750445365905762
			 train-loss:  2.091903967479252 	 ± 0.223385048530669
	data : 0.11830921173095703
	model : 0.0676088809967041
			 train-loss:  2.090079601919442 	 ± 0.22458304187902653
	data : 0.11816120147705078
	model : 0.06746382713317871
			 train-loss:  2.0894410964182892 	 ± 0.22429945291099745
	data : 0.11829605102539062
	model : 0.0678853988647461
			 train-loss:  2.0891802414603857 	 ± 0.2238461229464087
	data : 0.11779723167419434
	model : 0.06863327026367187
			 train-loss:  2.0880379460074683 	 ± 0.2240318859226351
	data : 0.11715030670166016
	model : 0.06860084533691406
			 train-loss:  2.087765619672578 	 ± 0.22358685070444778
	data : 0.11694917678833008
	model : 0.06853065490722657
			 train-loss:  2.088193651944271 	 ± 0.22320177175480196
	data : 0.11695647239685059
	model : 0.06841740608215333
			 train-loss:  2.0894176409794736 	 ± 0.2235065945936512
	data : 0.11712627410888672
	model : 0.06852827072143555
			 train-loss:  2.089389097944219 	 ± 0.22303096905561534
	data : 0.11679039001464844
	model : 0.06842527389526368
			 train-loss:  2.087387750714512 	 ± 0.22466264787079743
	data : 0.1166142463684082
	model : 0.06809172630310059
			 train-loss:  2.08824193729127 	 ± 0.22457188516435042
	data : 0.11724958419799805
	model : 0.06783385276794433
			 train-loss:  2.087437467915671 	 ± 0.22444155081968234
	data : 0.11749262809753418
	model : 0.06751294136047363
			 train-loss:  2.086423877891636 	 ± 0.2245167090140542
	data : 0.11751847267150879
	model : 0.06713461875915527
			 train-loss:  2.0852222353219987 	 ± 0.22481731060277724
	data : 0.11804046630859374
	model : 0.066532564163208
			 train-loss:  2.086302466412303 	 ± 0.22497368223600298
	data : 0.11870231628417968
	model : 0.06633663177490234
			 train-loss:  2.086473592056716 	 ± 0.22452409639000256
	data : 0.11859326362609864
	model : 0.06698246002197265
			 train-loss:  2.0868295740198204 	 ± 0.22413006023648616
	data : 0.11816720962524414
	model : 0.0674407958984375
			 train-loss:  2.0877303236820657 	 ± 0.2241106060134814
	data : 0.11811552047729493
	model : 0.06805410385131835
			 train-loss:  2.0893206703419587 	 ± 0.22502819120382836
	data : 0.11735944747924805
	model : 0.06827983856201172
			 train-loss:  2.0906502328267913 	 ± 0.22553256527450083
	data : 0.11721434593200683
	model : 0.06819581985473633
			 train-loss:  2.0931668281555176 	 ± 0.22851036536702343
	data : 0.11731572151184082
	model : 0.06803393363952637
			 train-loss:  2.0918576895229277 	 ± 0.22897544447138546
	data : 0.11728224754333497
	model : 0.06767597198486328
			 train-loss:  2.090911641178361 	 ± 0.2290003379075281
	data : 0.11759228706359863
	model : 0.06760520935058593
			 train-loss:  2.091849561691284 	 ± 0.22902059769803895
	data : 0.11788573265075683
	model : 0.06798481941223145
			 train-loss:  2.090626700940835 	 ± 0.22938028555332984
	data : 0.11749496459960937
	model : 0.06814146041870117
			 train-loss:  2.090073737360182 	 ± 0.22909227925772985
	data : 0.11735162734985352
	model : 0.06823844909667968
			 train-loss:  2.0890431333436323 	 ± 0.22922366609934391
	data : 0.1172861099243164
	model : 0.06841921806335449
			 train-loss:  2.088430908721263 	 ± 0.22897915663599608
	data : 0.11690688133239746
	model : 0.06805925369262696
			 train-loss:  2.089104201746922 	 ± 0.22878152197357748
	data : 0.11709165573120117
	model : 0.06795682907104492
			 train-loss:  2.0901558361947536 	 ± 0.2289509581825569
	data : 0.11621956825256348
	model : 0.05956687927246094
#epoch  18    val-loss:  2.4630060258664583  train-loss:  2.0901558361947536  lr:  0.00125
			 train-loss:  1.9104121923446655 	 ± 0.0
	data : 5.565549373626709
	model : 0.07832479476928711
			 train-loss:  2.2873502373695374 	 ± 0.3769380450248718
	data : 2.8505613803863525
	model : 0.07467007637023926
			 train-loss:  2.1868441899617515 	 ± 0.3390050997490359
	data : 1.9386641184488933
	model : 0.07367793718973796
			 train-loss:  2.1775542497634888 	 ± 0.2940276385113349
	data : 1.4827910661697388
	model : 0.07225137948989868
			 train-loss:  2.181786632537842 	 ± 0.2631225076321605
	data : 1.2097636699676513
	model : 0.07198963165283204
			 train-loss:  2.1093000968297324 	 ± 0.2897689326233557
	data : 0.11968765258789063
	model : 0.07008318901062012
			 train-loss:  2.1210335322788785 	 ± 0.2698090576671167
	data : 0.1159508228302002
	model : 0.06981987953186035
			 train-loss:  2.0861975997686386 	 ± 0.26868588910905083
	data : 0.11613836288452148
	model : 0.06883463859558106
			 train-loss:  2.1169047488106623 	 ± 0.26779505933028147
	data : 0.11696276664733887
	model : 0.06924972534179688
			 train-loss:  2.112102544307709 	 ± 0.2544608514768662
	data : 0.1167680263519287
	model : 0.06910700798034668
			 train-loss:  2.1240433671257715 	 ± 0.2455397406820473
	data : 0.11670637130737305
	model : 0.07011914253234863
			 train-loss:  2.144335558017095 	 ± 0.24453040783487898
	data : 0.11566967964172363
	model : 0.06919808387756347
			 train-loss:  2.1177463072996874 	 ± 0.2523476862538851
	data : 0.11649560928344727
	model : 0.06898646354675293
			 train-loss:  2.1176245297704424 	 ± 0.2431687114018176
	data : 0.11674270629882813
	model : 0.06818180084228516
			 train-loss:  2.1066959381103514 	 ± 0.23845551299404785
	data : 0.11736512184143066
	model : 0.06842603683471679
			 train-loss:  2.1029466837644577 	 ± 0.23133973030951513
	data : 0.11743464469909667
	model : 0.06701221466064453
			 train-loss:  2.120267896091237 	 ± 0.23488367195330268
	data : 0.11889510154724121
	model : 0.06803679466247559
			 train-loss:  2.10473730829027 	 ± 0.23707742653828187
	data : 0.11800155639648438
	model : 0.06898531913757325
			 train-loss:  2.119282923246685 	 ± 0.23886368408050107
	data : 0.11700987815856934
	model : 0.06967062950134277
			 train-loss:  2.1100826382637026 	 ± 0.2362442088718318
	data : 0.11649384498596191
	model : 0.06921324729919434
			 train-loss:  2.116516737710862 	 ± 0.23233939988341035
	data : 0.11664767265319824
	model : 0.06961088180541992
			 train-loss:  2.113951639695601 	 ± 0.22730169836568395
	data : 0.11624026298522949
	model : 0.06861772537231445
			 train-loss:  2.1245469632356064 	 ± 0.2277925611930575
	data : 0.11721816062927246
	model : 0.06845345497131347
			 train-loss:  2.106777166326841 	 ± 0.23872578656145246
	data : 0.11742105484008789
	model : 0.06864933967590332
			 train-loss:  2.1044365549087525 	 ± 0.2341834413575401
	data : 0.11726527214050293
	model : 0.0688654899597168
			 train-loss:  2.103838466680967 	 ± 0.22965522790820847
	data : 0.11716551780700683
	model : 0.06911468505859375
			 train-loss:  2.1078783097090543 	 ± 0.22630170693717594
	data : 0.11689577102661133
	model : 0.06929512023925781
			 train-loss:  2.1212771236896515 	 ± 0.23287487601053727
	data : 0.11674141883850098
	model : 0.06933245658874512
			 train-loss:  2.129897688997203 	 ± 0.2333269799264185
	data : 0.1165924072265625
	model : 0.06932907104492188
			 train-loss:  2.130354901154836 	 ± 0.22941845135462127
	data : 0.1165191650390625
	model : 0.06923499107360839
			 train-loss:  2.1312119153238114 	 ± 0.22573663178469
	data : 0.11654095649719239
	model : 0.06918559074401856
			 train-loss:  2.135424103587866 	 ± 0.22341584157592084
	data : 0.11667518615722657
	model : 0.06981916427612304
			 train-loss:  2.130275430101337 	 ± 0.2219242129840124
	data : 0.11602444648742676
	model : 0.0689847469329834
			 train-loss:  2.13755306075601 	 ± 0.2225974514215233
	data : 0.11698665618896484
	model : 0.06873393058776855
			 train-loss:  2.134255123138428 	 ± 0.22023559917300356
	data : 0.11720771789550781
	model : 0.06818943023681641
			 train-loss:  2.1256729894214206 	 ± 0.2230117693780308
	data : 0.11777429580688477
	model : 0.06748824119567871
			 train-loss:  2.140781151281821 	 ± 0.23792292103627183
	data : 0.11841120719909667
	model : 0.06684961318969726
			 train-loss:  2.1530979746266414 	 ± 0.24643600227027881
	data : 0.11907873153686524
	model : 0.06782937049865723
			 train-loss:  2.1574312417935104 	 ± 0.2447182859157121
	data : 0.11811909675598145
	model : 0.06704950332641602
			 train-loss:  2.1539601027965545 	 ± 0.24261031904372363
	data : 0.11872682571411133
	model : 0.06764092445373535
			 train-loss:  2.1499574765926455 	 ± 0.24096680994845743
	data : 0.11835803985595703
	model : 0.0686953067779541
			 train-loss:  2.1500654561179027 	 ± 0.23808187973084646
	data : 0.11753783226013184
	model : 0.06947417259216308
			 train-loss:  2.147142781767734 	 ± 0.23605833526926978
	data : 0.11677155494689942
	model : 0.0693321704864502
			 train-loss:  2.1422125670042904 	 ± 0.2355892513889055
	data : 0.11698040962219239
	model : 0.07019629478454589
			 train-loss:  2.1386474000083076 	 ± 0.23415415716923865
	data : 0.11632132530212402
	model : 0.070135498046875
			 train-loss:  2.1316623091697693 	 ± 0.23628768445755904
	data : 0.11630392074584961
	model : 0.06990818977355957
			 train-loss:  2.13327793618466 	 ± 0.23401715585039506
	data : 0.1163912296295166
	model : 0.06972184181213378
			 train-loss:  2.1296911785999932 	 ± 0.23286854438777183
	data : 0.11658859252929688
	model : 0.06976089477539063
			 train-loss:  2.128267005998261 	 ± 0.2306911937018441
	data : 0.11654829978942871
	model : 0.06985507011413575
			 train-loss:  2.128913779258728 	 ± 0.22841750322911425
	data : 0.11624679565429688
	model : 0.06991801261901856
			 train-loss:  2.1275406865512623 	 ± 0.2263753397679816
	data : 0.11610221862792969
	model : 0.06998357772827149
			 train-loss:  2.126064896583557 	 ± 0.2244356793500623
	data : 0.11604342460632325
	model : 0.07007675170898438
			 train-loss:  2.130957198592852 	 ± 0.2250901398653293
	data : 0.11598124504089355
	model : 0.06997523307800294
			 train-loss:  2.127135460023527 	 ± 0.22472521240455554
	data : 0.11598787307739258
	model : 0.06983318328857421
			 train-loss:  2.1236097314141014 	 ± 0.2241750980692574
	data : 0.11607675552368164
	model : 0.06953396797180175
			 train-loss:  2.12751599081925 	 ± 0.2240453318014819
	data : 0.11632671356201171
	model : 0.0692986011505127
			 train-loss:  2.124246026340284 	 ± 0.22341544947255326
	data : 0.11638355255126953
	model : 0.06930274963378906
			 train-loss:  2.119421089517659 	 ± 0.2244567435983901
	data : 0.11640095710754395
	model : 0.06944808959960938
			 train-loss:  2.1137864003747198 	 ± 0.22664598218142368
	data : 0.11632328033447266
	model : 0.06960515975952149
			 train-loss:  2.1097777605056764 	 ± 0.22684872413287646
	data : 0.11629776954650879
	model : 0.07002105712890624
			 train-loss:  2.117942813967095 	 ± 0.23370241190786226
	data : 0.11594953536987304
	model : 0.0693674087524414
			 train-loss:  2.115106684546317 	 ± 0.23286597509845514
	data : 0.11665496826171876
	model : 0.06954693794250488
			 train-loss:  2.109593841764662 	 ± 0.23505338175972498
	data : 0.11649417877197266
	model : 0.06861400604248047
			 train-loss:  2.105515332892537 	 ± 0.23544588840308917
	data : 0.11723065376281738
	model : 0.06854066848754883
			 train-loss:  2.1099341264137856 	 ± 0.2362870519562668
	data : 0.1173856258392334
	model : 0.06805968284606934
			 train-loss:  2.107445285175786 	 ± 0.23534712559417645
	data : 0.11770362854003906
	model : 0.06879663467407227
			 train-loss:  2.105663577122475 	 ± 0.23403225173523956
	data : 0.117008638381958
	model : 0.06872639656066895
			 train-loss:  2.1106409746057846 	 ± 0.2358506479053787
	data : 0.11707024574279785
	model : 0.06863923072814941
			 train-loss:  2.112077775208846 	 ± 0.2344349376039823
	data : 0.11712937355041504
	model : 0.0686617374420166
			 train-loss:  2.1126173189708166 	 ± 0.2327975241330283
	data : 0.11705145835876465
	model : 0.06907892227172852
			 train-loss:  2.1132528244609565 	 ± 0.2312134349347124
	data : 0.11667909622192382
	model : 0.0698920726776123
			 train-loss:  2.1070851501491337 	 ± 0.23541030558657278
	data : 0.11591253280639649
	model : 0.06996078491210937
			 train-loss:  2.1098869157164066 	 ± 0.23499798558399848
	data : 0.11578106880187988
	model : 0.07081437110900879
			 train-loss:  2.109400170880395 	 ± 0.23344180739114695
	data : 0.11501064300537109
	model : 0.07047829627990723
			 train-loss:  2.1095267375310263 	 ± 0.2318828622822306
	data : 0.115317964553833
	model : 0.07041807174682617
			 train-loss:  2.108654814331155 	 ± 0.2304759966350501
	data : 0.11556739807128906
	model : 0.06954836845397949
			 train-loss:  2.1175457455895166 	 ± 0.24173750957567663
	data : 0.11641836166381836
	model : 0.06939067840576171
			 train-loss:  2.1178626876610975 	 ± 0.2401990128033944
	data : 0.11654963493347167
	model : 0.0694735050201416
			 train-loss:  2.1127906871747366 	 ± 0.24284111445848514
	data : 0.1164588451385498
	model : 0.06903996467590331
			 train-loss:  2.114982250332832 	 ± 0.2421034742884619
	data : 0.11694426536560058
	model : 0.06895136833190918
			 train-loss:  2.119537303477158 	 ± 0.24402938803693477
	data : 0.11691288948059082
	model : 0.06812105178833008
			 train-loss:  2.1184754313492196 	 ± 0.24272505413678527
	data : 0.11772012710571289
	model : 0.06893596649169922
			 train-loss:  2.120194774076163 	 ± 0.24176027509868284
	data : 0.11695008277893067
	model : 0.068080472946167
			 train-loss:  2.1174397738206956 	 ± 0.24162407355092036
	data : 0.11781077384948731
	model : 0.0697361946105957
			 train-loss:  2.1160307379329906 	 ± 0.24054545451959036
	data : 0.11620430946350098
	model : 0.06971850395202636
			 train-loss:  2.1218351292055706 	 ± 0.2450572011289554
	data : 0.11626911163330078
	model : 0.07060565948486328
			 train-loss:  2.1231832915338975 	 ± 0.24396531654027692
	data : 0.11541476249694824
	model : 0.06987080574035645
			 train-loss:  2.1233215928077698 	 ± 0.2425786194453594
	data : 0.1161233901977539
	model : 0.07082443237304688
			 train-loss:  2.121840008189169 	 ± 0.24161204835677774
	data : 0.11525125503540039
	model : 0.07008323669433594
			 train-loss:  2.1222008970048694 	 ± 0.24029013061525817
	data : 0.11605825424194335
	model : 0.07021751403808593
			 train-loss:  2.124639403689039 	 ± 0.2400833540465051
	data : 0.11589283943176269
	model : 0.0694460391998291
			 train-loss:  2.128529053667317 	 ± 0.2416407835409212
	data : 0.11658220291137696
	model : 0.06934475898742676
			 train-loss:  2.1275371531004548 	 ± 0.24052636375843725
	data : 0.11656780242919922
	model : 0.06871018409729004
			 train-loss:  2.1240782737731934 	 ± 0.24155767973651787
	data : 0.1171224594116211
	model : 0.06870055198669434
			 train-loss:  2.1327099549142936 	 ± 0.254439496430736
	data : 0.11708035469055175
	model : 0.06786766052246093
			 train-loss:  2.1342216903964677 	 ± 0.25353933756433145
	data : 0.1177833080291748
	model : 0.0688560962677002
			 train-loss:  2.132337567732506 	 ± 0.25290370656164196
	data : 0.11688127517700195
	model : 0.06932969093322754
			 train-loss:  2.128960054747912 	 ± 0.25379945910023566
	data : 0.11677050590515137
	model : 0.06982960700988769
			 train-loss:  2.126112787410466 	 ± 0.2540826569648662
	data : 0.11632304191589356
	model : 0.07032985687255859
			 train-loss:  2.126371136903763 	 ± 0.2528221198911016
	data : 0.11583933830261231
	model : 0.07110786437988281
			 train-loss:  2.127450698673135 	 ± 0.2517989436848876
	data : 0.115116548538208
	model : 0.07026309967041015
			 train-loss:  2.1249438524246216 	 ± 0.2518249887390777
	data : 0.11575818061828613
	model : 0.06988387107849121
			 train-loss:  2.1235116419282933 	 ± 0.251016657767807
	data : 0.11595869064331055
	model : 0.06996245384216308
			 train-loss:  2.122436279287705 	 ± 0.2500452217452639
	data : 0.11605391502380372
	model : 0.06988592147827148
			 train-loss:  2.12128955863771 	 ± 0.2491263056346896
	data : 0.11624231338500976
	model : 0.06983003616333008
			 train-loss:  2.118509977493646 	 ± 0.24957893542224924
	data : 0.11638369560241699
	model : 0.07016801834106445
			 train-loss:  2.1190173013188014 	 ± 0.2484648482233003
	data : 0.11620078086853028
	model : 0.07017364501953124
			 train-loss:  2.121602331046705 	 ± 0.24875324516905514
	data : 0.1161808967590332
	model : 0.07013502120971679
			 train-loss:  2.119585742644214 	 ± 0.24849483534640898
	data : 0.11592469215393067
	model : 0.06954126358032227
			 train-loss:  2.1187892057678916 	 ± 0.24750248438353795
	data : 0.1163710117340088
	model : 0.06971683502197265
			 train-loss:  2.115924598934414 	 ± 0.24821012780101603
	data : 0.11627078056335449
	model : 0.06998605728149414
			 train-loss:  2.1153039165905545 	 ± 0.24718607585205568
	data : 0.1161916732788086
	model : 0.0698625087738037
			 train-loss:  2.117988776316685 	 ± 0.2477248241606103
	data : 0.11623640060424804
	model : 0.06977977752685546
			 train-loss:  2.1160162959182474 	 ± 0.2475256007845337
	data : 0.11626124382019043
	model : 0.06964530944824218
			 train-loss:  2.1187803745269775 	 ± 0.24820782558812113
	data : 0.11637887954711915
	model : 0.06914944648742676
			 train-loss:  2.118326927053517 	 ± 0.24718348339043306
	data : 0.11679039001464844
	model : 0.06886472702026367
			 train-loss:  2.1177374297737055 	 ± 0.24620675424969085
	data : 0.11693372726440429
	model : 0.06901154518127442
			 train-loss:  2.1194192736835804 	 ± 0.24583531486308785
	data : 0.11677470207214355
	model : 0.06923832893371581
			 train-loss:  2.1174907724396523 	 ± 0.2456949341877638
	data : 0.11670293807983398
	model : 0.06966891288757324
			 train-loss:  2.1163302630186083 	 ± 0.24499636273149109
	data : 0.11648063659667969
	model : 0.06938667297363281
			 train-loss:  2.115006675404951 	 ± 0.24441232416010344
	data : 0.11678643226623535
	model : 0.069191312789917
			 train-loss:  2.1193804506395684 	 ± 0.24811781999148347
	data : 0.11706624031066895
	model : 0.06917004585266114
			 train-loss:  2.119620782573049 	 ± 0.24712141045263367
	data : 0.11711258888244629
	model : 0.06916332244873047
			 train-loss:  2.1197003491463198 	 ± 0.24612451793837828
	data : 0.11720523834228516
	model : 0.06819148063659668
			 train-loss:  2.118451542854309 	 ± 0.24553215722379765
	data : 0.11798281669616699
	model : 0.06879992485046386
			 train-loss:  2.116151181478349 	 ± 0.24590452903864757
	data : 0.11737451553344727
	model : 0.06905059814453125
			 train-loss:  2.1175657313639724 	 ± 0.24544861605373366
	data : 0.11707682609558105
	model : 0.06905498504638671
			 train-loss:  2.1191050466150045 	 ± 0.2451025995225036
	data : 0.11702966690063477
	model : 0.0689009666442871
			 train-loss:  2.121199914651324 	 ± 0.24529840774436062
	data : 0.11708946228027343
	model : 0.06964540481567383
			 train-loss:  2.1176931262016296 	 ± 0.24757794436266883
	data : 0.11635112762451172
	model : 0.06998682022094727
			 train-loss:  2.118440821880603 	 ± 0.246778474247108
	data : 0.11607980728149414
	model : 0.07014288902282714
			 train-loss:  2.1191345209425148 	 ± 0.24597010842430048
	data : 0.11582007408142089
	model : 0.0698629379272461
			 train-loss:  2.118701125446119 	 ± 0.2450942493567186
	data : 0.11598143577575684
	model : 0.06990833282470703
			 train-loss:  2.1154544638164006 	 ± 0.24703203548976618
	data : 0.11599669456481934
	model : 0.07029280662536622
			 train-loss:  2.117186837726169 	 ± 0.24693104462705576
	data : 0.11583924293518066
	model : 0.06928896903991699
			 train-loss:  2.117710266043158 	 ± 0.24609669353746272
	data : 0.11688809394836426
	model : 0.06907429695129394
			 train-loss:  2.1172460656966607 	 ± 0.24525663736399575
	data : 0.11714367866516114
	model : 0.06932945251464843
			 train-loss:  2.1147932436155235 	 ± 0.24604711079639968
	data : 0.11707034111022949
	model : 0.06943540573120117
			 train-loss:  2.112876753155276 	 ± 0.2461920229848087
	data : 0.11699094772338867
	model : 0.06910533905029297
			 train-loss:  2.114330610207149 	 ± 0.2459093009566286
	data : 0.1170987606048584
	model : 0.07055268287658692
			 train-loss:  2.114201527115301 	 ± 0.24504049047715742
	data : 0.11568360328674317
	model : 0.07081575393676758
			 train-loss:  2.1123591678243288 	 ± 0.24515420768817223
	data : 0.11552438735961915
	model : 0.07076091766357422
			 train-loss:  2.111714493144642 	 ± 0.24441627954179587
	data : 0.11555676460266114
	model : 0.0697300910949707
			 train-loss:  2.1092081227236323 	 ± 0.24540328438386336
	data : 0.11636185646057129
	model : 0.06976833343505859
			 train-loss:  2.1068476644055596 	 ± 0.24619052878672623
	data : 0.11643924713134765
	model : 0.06822171211242675
			 train-loss:  2.1084730772122944 	 ± 0.24612542776790894
	data : 0.11788825988769532
	model : 0.0681060791015625
			 train-loss:  2.1093678912337945 	 ± 0.24552501679033828
	data : 0.11786928176879882
	model : 0.06815309524536133
			 train-loss:  2.1089766138308756 	 ± 0.24474011697869066
	data : 0.1180497646331787
	model : 0.0681490421295166
			 train-loss:  2.1064222859056203 	 ± 0.24588893060788686
	data : 0.11814165115356445
	model : 0.06742558479309083
			 train-loss:  2.1051608761151632 	 ± 0.24555116058596288
	data : 0.11871514320373536
	model : 0.06798605918884278
			 train-loss:  2.104490833566678 	 ± 0.24487427164815087
	data : 0.11822924613952637
	model : 0.06821837425231933
			 train-loss:  2.103644304369625 	 ± 0.24428901155768143
	data : 0.11808452606201172
	model : 0.06814637184143066
			 train-loss:  2.101719142564761 	 ± 0.24464346366995882
	data : 0.11809630393981933
	model : 0.06817798614501953
			 train-loss:  2.1037591262297197 	 ± 0.24514995370293496
	data : 0.11811590194702148
	model : 0.06866555213928223
			 train-loss:  2.1026676254887735 	 ± 0.2447329955673453
	data : 0.11775569915771485
	model : 0.06898202896118164
			 train-loss:  2.1030510862668357 	 ± 0.24399404342156034
	data : 0.11730384826660156
	model : 0.06875476837158204
			 train-loss:  2.101214501508482 	 ± 0.24429510094606652
	data : 0.11746482849121094
	model : 0.06898250579833984
			 train-loss:  2.1023707963243314 	 ± 0.24395140089074568
	data : 0.11719169616699218
	model : 0.06895899772644043
			 train-loss:  2.105182860632363 	 ± 0.2457385040768257
	data : 0.11713624000549316
	model : 0.06860480308532715
			 train-loss:  2.103566175699234 	 ± 0.24581611952028676
	data : 0.11734123229980468
	model : 0.0684122085571289
			 train-loss:  2.105501838352369 	 ± 0.2462716730262592
	data : 0.11762371063232421
	model : 0.06877970695495605
			 train-loss:  2.1046184374962325 	 ± 0.24576614854705434
	data : 0.11742100715637208
	model : 0.06852035522460938
			 train-loss:  2.1049348679056927 	 ± 0.2450442047058003
	data : 0.11739439964294433
	model : 0.0694272518157959
			 train-loss:  2.1059261444138317 	 ± 0.244623573314649
	data : 0.11664543151855469
	model : 0.0700416088104248
			 train-loss:  2.1060210358012807 	 ± 0.24388419070211162
	data : 0.11611661911010743
	model : 0.06944789886474609
			 train-loss:  2.104106865733503 	 ± 0.24438853493081955
	data : 0.11669583320617676
	model : 0.06905803680419922
			 train-loss:  2.103415046623367 	 ± 0.24381871671220076
	data : 0.11682600975036621
	model : 0.06823554039001464
			 train-loss:  2.104196514402117 	 ± 0.24330165967677955
	data : 0.11769680976867676
	model : 0.06854820251464844
			 train-loss:  2.102177388569307 	 ± 0.24398840385373194
	data : 0.11741194725036622
	model : 0.06846089363098144
			 train-loss:  2.1005775185192332 	 ± 0.244157188022982
	data : 0.11738839149475097
	model : 0.06891374588012696
			 train-loss:  2.100452041068272 	 ± 0.2434477286260095
	data : 0.11706910133361817
	model : 0.06835274696350098
			 train-loss:  2.1011877905490786 	 ± 0.24292959709044645
	data : 0.11763491630554199
	model : 0.06907610893249512
			 train-loss:  2.102705481424497 	 ± 0.24304288822300144
	data : 0.11688051223754883
	model : 0.0689056396484375
			 train-loss:  2.1028846118641997 	 ± 0.24235493562045335
	data : 0.11701593399047852
	model : 0.06897478103637696
			 train-loss:  2.1025539752415248 	 ± 0.24170085395151822
	data : 0.11691713333129883
	model : 0.06940054893493652
			 train-loss:  2.1053514196114107 	 ± 0.243837797663787
	data : 0.11638908386230469
	model : 0.06999483108520507
			 train-loss:  2.105976828074051 	 ± 0.24328953313291057
	data : 0.11587653160095215
	model : 0.06992387771606445
			 train-loss:  2.105496426646629 	 ± 0.2426893467419682
	data : 0.11592693328857422
	model : 0.06969962120056153
			 train-loss:  2.1065279371911587 	 ± 0.24240147223890599
	data : 0.11606292724609375
	model : 0.06945815086364746
			 train-loss:  2.1060154702928333 	 ± 0.24182441393284376
	data : 0.11635103225708007
	model : 0.06865954399108887
			 train-loss:  2.1056529975069163 	 ± 0.2412044943388061
	data : 0.11700954437255859
	model : 0.06876811981201172
			 train-loss:  2.1078258530124203 	 ± 0.24231074140981942
	data : 0.11707854270935059
	model : 0.06862754821777343
			 train-loss:  2.1068720661225866 	 ± 0.24199012021544522
	data : 0.1172715187072754
	model : 0.06948261260986328
			 train-loss:  2.1079981845358144 	 ± 0.2418119760709551
	data : 0.11619782447814941
	model : 0.06964788436889649
			 train-loss:  2.1047796152733467 	 ± 0.24507764471537657
	data : 0.11606459617614746
	model : 0.07047004699707031
			 train-loss:  2.1025596293069984 	 ± 0.24627601340355798
	data : 0.11538176536560059
	model : 0.0704646110534668
			 train-loss:  2.101488819734298 	 ± 0.2460504154961343
	data : 0.11520018577575683
	model : 0.07013506889343261
			 train-loss:  2.1027198154875575 	 ± 0.24597185194384058
	data : 0.11573700904846192
	model : 0.06911907196044922
			 train-loss:  2.1038214769312944 	 ± 0.24578487023144638
	data : 0.11686019897460938
	model : 0.06901912689208985
			 train-loss:  2.1062591690766186 	 ± 0.2474173737370678
	data : 0.11684784889221192
	model : 0.06878099441528321
			 train-loss:  2.1079861705839944 	 ± 0.24791437851761677
	data : 0.11699070930480956
	model : 0.06883354187011718
			 train-loss:  2.1085353568196297 	 ± 0.24738438420336745
	data : 0.11691675186157227
	model : 0.06977481842041015
			 train-loss:  2.108002861546729 	 ± 0.24685295611561042
	data : 0.11611790657043457
	model : 0.06994600296020508
			 train-loss:  2.1093589706519214 	 ± 0.24693563946736105
	data : 0.116176176071167
	model : 0.06992464065551758
			 train-loss:  2.1079063256581625 	 ± 0.24713130281014464
	data : 0.1161013126373291
	model : 0.06992712020874023
			 train-loss:  2.1068112746793397 	 ± 0.24697390761344257
	data : 0.11619706153869629
	model : 0.06970076560974121
			 train-loss:  2.1069298280677216 	 ± 0.24635186395532424
	data : 0.11657733917236328
	model : 0.06946811676025391
			 train-loss:  2.1061669220828048 	 ± 0.24596216901884466
	data : 0.11660099029541016
	model : 0.0696303367614746
			 train-loss:  2.105506368617916 	 ± 0.24551939867321826
	data : 0.1164693832397461
	model : 0.06982626914978027
			 train-loss:  2.1041183096170424 	 ± 0.24568636734549487
	data : 0.11633520126342774
	model : 0.07023301124572753
			 train-loss:  2.1036344268428744 	 ± 0.24516996591925297
	data : 0.11589446067810058
	model : 0.07037711143493652
			 train-loss:  2.103938353533792 	 ± 0.24460031264298607
	data : 0.11572966575622559
	model : 0.07016711235046387
			 train-loss:  2.103991391623549 	 ± 0.2439982694598165
	data : 0.11582622528076172
	model : 0.07000017166137695
			 train-loss:  2.1050998153639773 	 ± 0.24391130139678535
	data : 0.11605887413024903
	model : 0.06985459327697754
			 train-loss:  2.1039526887056303 	 ± 0.24386668090738248
	data : 0.11627798080444336
	model : 0.06955347061157227
			 train-loss:  2.103924278495381 	 ± 0.24327439147872343
	data : 0.11640586853027343
	model : 0.06876707077026367
			 train-loss:  2.103947272623219 	 ± 0.24268628517274224
	data : 0.11694698333740235
	model : 0.06877460479736328
			 train-loss:  2.1023274591335883 	 ± 0.24322130219184326
	data : 0.1171457290649414
	model : 0.06859087944030762
			 train-loss:  2.101821228077537 	 ± 0.24274855310291196
	data : 0.1171234130859375
	model : 0.06855211257934571
			 train-loss:  2.0997505988393512 	 ± 0.24401300083303723
	data : 0.1173783302307129
	model : 0.06852536201477051
			 train-loss:  2.0992988585295835 	 ± 0.2435220891902686
	data : 0.1175644874572754
	model : 0.06845464706420898
			 train-loss:  2.098497251294694 	 ± 0.2432259441377441
	data : 0.11771945953369141
	model : 0.06781373023986817
			 train-loss:  2.1001418644273784 	 ± 0.24383298848784202
	data : 0.11809501647949219
	model : 0.06805167198181153
			 train-loss:  2.099606551299585 	 ± 0.2433880414001821
	data : 0.11782383918762207
	model : 0.06814236640930176
			 train-loss:  2.098472638462865 	 ± 0.24338727701204868
	data : 0.11757140159606934
	model : 0.06781148910522461
			 train-loss:  2.099921730933366 	 ± 0.24375108487169456
	data : 0.11807851791381836
	model : 0.06881017684936523
			 train-loss:  2.0993873254494733 	 ± 0.24331559483035967
	data : 0.11705155372619629
	model : 0.06973752975463868
			 train-loss:  2.0991985158089106 	 ± 0.24277282289197358
	data : 0.11627168655395508
	model : 0.0698197364807129
			 train-loss:  2.0995151838755497 	 ± 0.24226303485591066
	data : 0.1160161018371582
	model : 0.06973953247070312
			 train-loss:  2.0981604467738757 	 ± 0.2425418160421706
	data : 0.11597795486450195
	model : 0.06953749656677247
			 train-loss:  2.0967526284817657 	 ± 0.24289170146284816
	data : 0.11609869003295899
	model : 0.06932215690612793
			 train-loss:  2.0981477563445634 	 ± 0.243229890337766
	data : 0.11637320518493652
	model : 0.06875076293945312
			 train-loss:  2.0976886840144613 	 ± 0.24278029265550213
	data : 0.11669731140136719
	model : 0.06855602264404297
			 train-loss:  2.0958685630134175 	 ± 0.24375786918160133
	data : 0.1169665813446045
	model : 0.06850700378417969
			 train-loss:  2.0957633357577854 	 ± 0.24322068073965136
	data : 0.11707577705383301
	model : 0.06825556755065917
			 train-loss:  2.0960812389323142 	 ± 0.24272883027786107
	data : 0.11708650588989258
	model : 0.06771583557128906
			 train-loss:  2.0973718880556755 	 ± 0.24296955102010376
	data : 0.11764292716979981
	model : 0.0675389289855957
			 train-loss:  2.0964943327401815 	 ± 0.2427964049104749
	data : 0.11789307594299317
	model : 0.0670015811920166
			 train-loss:  2.0975383806436865 	 ± 0.24277808479927798
	data : 0.11826944351196289
	model : 0.06667685508728027
			 train-loss:  2.096971823339877 	 ± 0.2424013994234072
	data : 0.11844801902770996
	model : 0.067073392868042
			 train-loss:  2.095910480012109 	 ± 0.2424111307875624
	data : 0.11814026832580567
	model : 0.06719951629638672
			 train-loss:  2.0939280077301223 	 ± 0.24375754802538915
	data : 0.11793842315673828
	model : 0.06785798072814941
			 train-loss:  2.0932646135403874 	 ± 0.2434436932015165
	data : 0.1175499439239502
	model : 0.06767759323120118
			 train-loss:  2.093195842881488 	 ± 0.24292522553004328
	data : 0.11788597106933593
	model : 0.06794490814208984
			 train-loss:  2.092962313712911 	 ± 0.24243413305216507
	data : 0.11777167320251465
	model : 0.06814665794372558
			 train-loss:  2.0924115251686612 	 ± 0.24206725650512864
	data : 0.11754379272460938
	model : 0.06857767105102539
			 train-loss:  2.092726321160039 	 ± 0.24160442999784618
	data : 0.11711478233337402
	model : 0.06771674156188964
			 train-loss:  2.09263144721504 	 ± 0.24110074740493223
	data : 0.11785602569580078
	model : 0.0681696891784668
			 train-loss:  2.0928698723286265 	 ± 0.2406239387813841
	data : 0.11738071441650391
	model : 0.06750950813293458
			 train-loss:  2.0916655202706655 	 ± 0.24084287675209942
	data : 0.1178924560546875
	model : 0.06742377281188965
			 train-loss:  2.0927520480888018 	 ± 0.24093139204799421
	data : 0.11779813766479492
	model : 0.06717538833618164
			 train-loss:  2.0916559853829626 	 ± 0.2410344256617403
	data : 0.1179647445678711
	model : 0.06779990196228028
			 train-loss:  2.0912476920296625 	 ± 0.24062180250664147
	data : 0.11727695465087891
	model : 0.06742048263549805
			 train-loss:  2.0904582313826827 	 ± 0.2404433628874708
	data : 0.11755013465881348
	model : 0.06762065887451171
			 train-loss:  2.089804564203535 	 ± 0.2401693065209356
	data : 0.11740927696228028
	model : 0.06764430999755859
			 train-loss:  2.0900969374470595 	 ± 0.23972434607354337
	data : 0.11764731407165527
	model : 0.06718077659606933
			 train-loss:  2.092273038891163 	 ± 0.24166094388566434
	data : 0.11800274848937989
	model : 0.06664829254150391
			 train-loss:  2.0916418024609165 	 ± 0.24137718898145266
	data : 0.11834878921508789
	model : 0.06732311248779296
			 train-loss:  2.0929443371822556 	 ± 0.24176375990180335
	data : 0.11788291931152343
	model : 0.06731438636779785
			 train-loss:  2.0929317402839662 	 ± 0.24127982976458925
	data : 0.11792898178100586
	model : 0.06708416938781739
			 train-loss:  2.0916426409763167 	 ± 0.24165981136049983
	data : 0.11811690330505371
	model : 0.06763100624084473
			 train-loss:  2.0912962518041094 	 ± 0.24124227836440915
	data : 0.11805319786071777
	model : 0.06791787147521973
			 train-loss:  2.0917536854272774 	 ± 0.24087452286536837
	data : 0.11779561042785644
	model : 0.06746220588684082
			 train-loss:  2.0922913504397775 	 ± 0.24055196242572766
	data : 0.1181985855102539
	model : 0.06792693138122559
			 train-loss:  2.0907712099598905 	 ± 0.2412991383512161
	data : 0.11773886680603027
	model : 0.06820363998413086
			 train-loss:  2.0898978048935533 	 ± 0.24123091629461976
	data : 0.11626267433166504
	model : 0.05933394432067871
#epoch  19    val-loss:  2.499506661766454  train-loss:  2.0898978048935533  lr:  0.00125
			 train-loss:  1.757493257522583 	 ± 0.0
	data : 5.551837205886841
	model : 0.07123279571533203
			 train-loss:  1.8307134509086609 	 ± 0.07322019338607788
	data : 2.8438358306884766
	model : 0.06932413578033447
			 train-loss:  1.9997613827387493 	 ± 0.24643160848071038
	data : 1.935503403345744
	model : 0.06941874821980794
			 train-loss:  1.9887052178382874 	 ± 0.21427346917131668
	data : 1.4806509613990784
	model : 0.06955063343048096
			 train-loss:  2.014081621170044 	 ± 0.1982582736149816
	data : 1.2075230121612548
	model : 0.0695042610168457
			 train-loss:  2.053376873334249 	 ± 0.20118615780888857
	data : 0.12049837112426758
	model : 0.06868019104003906
			 train-loss:  2.0575545174734935 	 ± 0.1865430759930696
	data : 0.11689219474792481
	model : 0.06850066184997558
			 train-loss:  2.042790040373802 	 ± 0.17881403129795978
	data : 0.11692862510681153
	model : 0.06788506507873535
			 train-loss:  2.051479114426507 	 ± 0.17036942307838177
	data : 0.11749129295349121
	model : 0.06722531318664551
			 train-loss:  2.0370012640953066 	 ± 0.1673608067618233
	data : 0.11819524765014648
	model : 0.06740193367004395
			 train-loss:  2.003093285994096 	 ± 0.19225196631693803
	data : 0.11805214881896972
	model : 0.06810536384582519
			 train-loss:  1.997712602217992 	 ± 0.1849303103249556
	data : 0.11750187873840331
	model : 0.06887092590332031
			 train-loss:  1.9882199397453895 	 ± 0.18069266216644023
	data : 0.11671733856201172
	model : 0.06992621421813965
			 train-loss:  2.0490762846810475 	 ± 0.28011272411237803
	data : 0.11604166030883789
	model : 0.07048683166503907
			 train-loss:  2.0387254397074384 	 ± 0.27337194847379853
	data : 0.11551966667175292
	model : 0.07034082412719726
			 train-loss:  2.0432223677635193 	 ± 0.2652636306643113
	data : 0.115576171875
	model : 0.07007327079772949
			 train-loss:  2.032557249069214 	 ± 0.2608555238950007
	data : 0.11593713760375976
	model : 0.07006611824035644
			 train-loss:  2.0416546397738986 	 ± 0.25626598641591636
	data : 0.11619820594787597
	model : 0.06969156265258789
			 train-loss:  2.0342162596551994 	 ± 0.25141947469615034
	data : 0.11634507179260253
	model : 0.0697784423828125
			 train-loss:  2.0422188341617584 	 ± 0.24752363002265315
	data : 0.11624250411987305
	model : 0.07002701759338378
			 train-loss:  2.052819473402841 	 ± 0.24616640136356535
	data : 0.11605591773986816
	model : 0.07009978294372558
			 train-loss:  2.063122635537928 	 ± 0.24509733870237246
	data : 0.115822172164917
	model : 0.07003273963928222
			 train-loss:  2.093211116998092 	 ± 0.2781686834377639
	data : 0.11579174995422363
	model : 0.06993756294250489
			 train-loss:  2.0817919224500656 	 ± 0.27776409929139567
	data : 0.11590909957885742
	model : 0.07004971504211426
			 train-loss:  2.0832975721359253 	 ± 0.27225206446304606
	data : 0.11581401824951172
	model : 0.0698458194732666
			 train-loss:  2.071256555043734 	 ± 0.2736695526549346
	data : 0.11595673561096191
	model : 0.07005462646484376
			 train-loss:  2.0739270934352168 	 ± 0.2688987921663452
	data : 0.11591095924377441
	model : 0.0700000286102295
			 train-loss:  2.089223154953548 	 ± 0.27575597856731454
	data : 0.11598215103149415
	model : 0.06996302604675293
			 train-loss:  2.0806679520113716 	 ± 0.27471549972661835
	data : 0.11593289375305176
	model : 0.06990876197814941
			 train-loss:  2.0986979365348817 	 ± 0.28701971275971017
	data : 0.11605405807495117
	model : 0.06997489929199219
			 train-loss:  2.0957360690639866 	 ± 0.2828180788907496
	data : 0.11597843170166015
	model : 0.0699310302734375
			 train-loss:  2.0832882449030876 	 ± 0.28686216081840893
	data : 0.11612954139709472
	model : 0.06984329223632812
			 train-loss:  2.079048839482394 	 ± 0.2834984794725937
	data : 0.11614227294921875
	model : 0.06985354423522949
			 train-loss:  2.0828096761423 	 ± 0.280132597108896
	data : 0.1161773681640625
	model : 0.0698887825012207
			 train-loss:  2.0803581714630126 	 ± 0.2764714915926004
	data : 0.11633710861206055
	model : 0.0699385166168213
			 train-loss:  2.086952017413245 	 ± 0.2753815682014739
	data : 0.11633133888244629
	model : 0.07043023109436035
			 train-loss:  2.100532009794905 	 ± 0.2835919537690476
	data : 0.11566061973571777
	model : 0.0705528736114502
			 train-loss:  2.0935686167917753 	 ± 0.2830230589912634
	data : 0.11548256874084473
	model : 0.07015972137451172
			 train-loss:  2.093189218105414 	 ± 0.27938078495375257
	data : 0.11597051620483398
	model : 0.06993956565856933
			 train-loss:  2.1023995012044905 	 ± 0.28179891004434515
	data : 0.11611237525939941
	model : 0.0698697566986084
			 train-loss:  2.101898260232879 	 ± 0.27835917601782834
	data : 0.11630945205688477
	model : 0.06922721862792969
			 train-loss:  2.0884622023219155 	 ± 0.28816767948010785
	data : 0.11704201698303222
	model : 0.06917772293090821
			 train-loss:  2.0967733222384783 	 ± 0.2898457736187089
	data : 0.11722211837768555
	model : 0.06874604225158691
			 train-loss:  2.095513557845896 	 ± 0.2866521979049919
	data : 0.1174957275390625
	model : 0.06869840621948242
			 train-loss:  2.1007210546069675 	 ± 0.2855462958139622
	data : 0.11755547523498536
	model : 0.0686429500579834
			 train-loss:  2.1069137816843777 	 ± 0.28546434885550265
	data : 0.11738247871398926
	model : 0.06860847473144531
			 train-loss:  2.117564858274257 	 ± 0.2915039567856911
	data : 0.11731171607971191
	model : 0.06867060661315919
			 train-loss:  2.129853777587414 	 ± 0.300503041307743
	data : 0.11717190742492675
	model : 0.06974425315856933
			 train-loss:  2.1335125237095114 	 ± 0.29849912328258554
	data : 0.11620550155639649
	model : 0.06906018257141114
			 train-loss:  2.132237803936005 	 ± 0.2956337475374052
	data : 0.11676011085510254
	model : 0.06821446418762207
			 train-loss:  2.1362486797220566 	 ± 0.294091749116113
	data : 0.11757302284240723
	model : 0.06823201179504394
			 train-loss:  2.142033047400988 	 ± 0.2941650775975714
	data : 0.11759033203125
	model : 0.06958317756652832
			 train-loss:  2.1416683714344815 	 ± 0.29138858662876843
	data : 0.11628160476684571
	model : 0.06897873878479004
			 train-loss:  2.1383826622256525 	 ± 0.2896672794077511
	data : 0.11685690879821778
	model : 0.06971755027770996
			 train-loss:  2.1383693543347446 	 ± 0.2870218773733029
	data : 0.1161616325378418
	model : 0.06969904899597168
			 train-loss:  2.1471288182905743 	 ± 0.2917713322499549
	data : 0.11613478660583496
	model : 0.06968607902526855
			 train-loss:  2.1604045796812628 	 ± 0.30578875006463174
	data : 0.11606693267822266
	model : 0.06869421005249024
			 train-loss:  2.1624544427312653 	 ± 0.3035359692861131
	data : 0.11703071594238282
	model : 0.0689432144165039
			 train-loss:  2.1612991417868663 	 ± 0.30108122454107156
	data : 0.1168403148651123
	model : 0.06814475059509277
			 train-loss:  2.163864368200302 	 ± 0.2992111542844135
	data : 0.11760673522949219
	model : 0.06984519958496094
			 train-loss:  2.162628312579921 	 ± 0.2969028864104568
	data : 0.1159595012664795
	model : 0.06913194656372071
			 train-loss:  2.1566262533587794 	 ± 0.29820636482086804
	data : 0.11661734580993652
	model : 0.06868371963500977
			 train-loss:  2.152801299852038 	 ± 0.2973593322291692
	data : 0.11714534759521485
	model : 0.06870908737182617
			 train-loss:  2.1472567711025476 	 ± 0.2982913057892039
	data : 0.11707239151000977
	model : 0.06869440078735352
			 train-loss:  2.1436702159734873 	 ± 0.29737530418931196
	data : 0.11691250801086425
	model : 0.06793131828308105
			 train-loss:  2.1380328445723564 	 ± 0.29859318282618397
	data : 0.11782526969909668
	model : 0.06881732940673828
			 train-loss:  2.1409833680337935 	 ± 0.29732430597846954
	data : 0.11713938713073731
	model : 0.0681262493133545
			 train-loss:  2.1392657546436085 	 ± 0.29546468555500105
	data : 0.11770496368408204
	model : 0.0674464225769043
			 train-loss:  2.1308928987254268 	 ± 0.30133252844952835
	data : 0.11825361251831054
	model : 0.06840071678161622
			 train-loss:  2.135653700147356 	 ± 0.3017748038849977
	data : 0.1177253246307373
	model : 0.06822142601013184
			 train-loss:  2.137020900215901 	 ± 0.299860351563959
	data : 0.11778736114501953
	model : 0.06718254089355469
			 train-loss:  2.136653463045756 	 ± 0.297786802295578
	data : 0.1186279296875
	model : 0.06781659126281739
			 train-loss:  2.1354997811252123 	 ± 0.29590210742412687
	data : 0.11804351806640626
	model : 0.06874504089355468
			 train-loss:  2.1305309437416695 	 ± 0.2969463964591352
	data : 0.11730537414550782
	model : 0.06845531463623047
			 train-loss:  2.129987891515096 	 ± 0.2949971015029411
	data : 0.11742372512817383
	model : 0.06773138046264648
			 train-loss:  2.1280540714138434 	 ± 0.2935280584898884
	data : 0.11814770698547364
	model : 0.068703031539917
			 train-loss:  2.127742954662868 	 ± 0.29162841597963424
	data : 0.11724815368652344
	model : 0.06809134483337402
			 train-loss:  2.124008964269589 	 ± 0.29159967735840464
	data : 0.11778573989868164
	model : 0.06799397468566895
			 train-loss:  2.127109962173655 	 ± 0.29103968918156586
	data : 0.11771297454833984
	model : 0.06816320419311524
			 train-loss:  2.126409450173378 	 ± 0.28928198365670177
	data : 0.11757559776306152
	model : 0.06882133483886718
			 train-loss:  2.1234380065658947 	 ± 0.2887166216538714
	data : 0.11708583831787109
	model : 0.06877002716064454
			 train-loss:  2.120649366843991 	 ± 0.2880462337239808
	data : 0.11725735664367676
	model : 0.06949009895324706
			 train-loss:  2.1231605575745363 	 ± 0.2872073903180285
	data : 0.1165802001953125
	model : 0.06872663497924805
			 train-loss:  2.1233018154189702 	 ± 0.28549560452724687
	data : 0.11736235618591309
	model : 0.06877722740173339
			 train-loss:  2.1262996477239273 	 ± 0.2851380954733067
	data : 0.11739873886108398
	model : 0.06809592247009277
			 train-loss:  2.123505170955214 	 ± 0.28464383846340957
	data : 0.11789002418518066
	model : 0.06798062324523926
			 train-loss:  2.1314386636361307 	 ± 0.2924101441909431
	data : 0.11798901557922363
	model : 0.06783227920532227
			 train-loss:  2.1276445429433477 	 ± 0.29288983452251427
	data : 0.11807398796081543
	model : 0.06872444152832032
			 train-loss:  2.1282587145151717 	 ± 0.29129671998699674
	data : 0.1173013687133789
	model : 0.06882662773132324
			 train-loss:  2.132605524857839 	 ± 0.29256211812448707
	data : 0.11722211837768555
	model : 0.0697249412536621
			 train-loss:  2.1318563511083415 	 ± 0.29103698825832924
	data : 0.11637339591979981
	model : 0.06930561065673828
			 train-loss:  2.131112315084623 	 ± 0.2895379517965013
	data : 0.11676268577575684
	model : 0.06952781677246093
			 train-loss:  2.1341706386176487 	 ± 0.2894672888103589
	data : 0.11668376922607422
	model : 0.06936516761779785
			 train-loss:  2.1342177200824657 	 ± 0.28792381025606817
	data : 0.11678900718688964
	model : 0.0690737247467041
			 train-loss:  2.1310424516075535 	 ± 0.2880542078747056
	data : 0.11697311401367187
	model : 0.06817464828491211
			 train-loss:  2.1323025239010653 	 ± 0.28681307624309904
	data : 0.11768631935119629
	model : 0.06857690811157227
			 train-loss:  2.133991257431581 	 ± 0.2858101753462854
	data : 0.1172715663909912
	model : 0.06833038330078126
			 train-loss:  2.131747450147356 	 ± 0.2852056695758202
	data : 0.11748747825622559
	model : 0.06793394088745117
			 train-loss:  2.1302786981216584 	 ± 0.28413384806033626
	data : 0.11786317825317383
	model : 0.06821680068969727
			 train-loss:  2.1296188020706177 	 ± 0.2827858447561658
	data : 0.11787190437316894
	model : 0.06897931098937989
			 train-loss:  2.127711273656033 	 ± 0.2820282605113974
	data : 0.11722521781921387
	model : 0.0691286563873291
			 train-loss:  2.131247760034075 	 ± 0.28288392800323003
	data : 0.11715841293334961
	model : 0.0693079948425293
			 train-loss:  2.1285057901178748 	 ± 0.282866166265322
	data : 0.11688566207885742
	model : 0.0698817253112793
			 train-loss:  2.124474354661428 	 ± 0.2844607429343907
	data : 0.11643528938293457
	model : 0.06966733932495117
			 train-loss:  2.1221220924740747 	 ± 0.2841174295209713
	data : 0.11630992889404297
	model : 0.06962151527404785
			 train-loss:  2.1201059165990577 	 ± 0.283527776529655
	data : 0.11641297340393067
	model : 0.06955852508544921
			 train-loss:  2.121686255820444 	 ± 0.2826684318798618
	data : 0.11640329360961914
	model : 0.06938290596008301
			 train-loss:  2.119793788150505 	 ± 0.28203692601602803
	data : 0.11663823127746582
	model : 0.06914315223693848
			 train-loss:  2.1207107272716836 	 ± 0.2809018734891153
	data : 0.11679153442382813
	model : 0.06934952735900879
			 train-loss:  2.1217259125276047 	 ± 0.27982292977348405
	data : 0.11666874885559082
	model : 0.06923460960388184
			 train-loss:  2.117986022889077 	 ± 0.2813076677970788
	data : 0.11681499481201171
	model : 0.06946611404418945
			 train-loss:  2.1197305992245674 	 ± 0.2806515348722723
	data : 0.11666741371154785
	model : 0.06935515403747558
			 train-loss:  2.1201430896742153 	 ± 0.2794410541276712
	data : 0.11668481826782226
	model : 0.06944246292114258
			 train-loss:  2.1177387718568768 	 ± 0.2793842344927054
	data : 0.11660661697387695
	model : 0.06961660385131836
			 train-loss:  2.121673660692961 	 ± 0.281321717048645
	data : 0.11642050743103027
	model : 0.07022972106933593
			 train-loss:  2.119681385056726 	 ± 0.28092010306434273
	data : 0.11578989028930664
	model : 0.07001662254333496
			 train-loss:  2.119761725776216 	 ± 0.27971835196301265
	data : 0.1159858226776123
	model : 0.0702974796295166
			 train-loss:  2.1200607954445534 	 ± 0.27854936797581575
	data : 0.11585025787353516
	model : 0.07048373222351074
			 train-loss:  2.121639970971757 	 ± 0.2779064654542599
	data : 0.11580424308776856
	model : 0.07027778625488282
			 train-loss:  2.1209508975346885 	 ± 0.2768481666072004
	data : 0.1160048484802246
	model : 0.06994295120239258
			 train-loss:  2.123636840788786 	 ± 0.2772673679088136
	data : 0.11639490127563476
	model : 0.06904950141906738
			 train-loss:  2.121886942230287 	 ± 0.27679879320384376
	data : 0.1172152042388916
	model : 0.06822466850280762
			 train-loss:  2.120003292231056 	 ± 0.2764553084737618
	data : 0.11805691719055175
	model : 0.0682222843170166
			 train-loss:  2.118383647934083 	 ± 0.27592362337170767
	data : 0.11815171241760254
	model : 0.06848382949829102
			 train-loss:  2.1160287284851074 	 ± 0.27606599794773656
	data : 0.11789469718933106
	model : 0.06837949752807618
			 train-loss:  2.1161050645131914 	 ± 0.27496964018691344
	data : 0.11810193061828614
	model : 0.06903891563415528
			 train-loss:  2.1177905338017022 	 ± 0.27453761801046545
	data : 0.1173820972442627
	model : 0.06906213760375976
			 train-loss:  2.11804585903883 	 ± 0.27347824003332394
	data : 0.11738004684448242
	model : 0.06860008239746093
			 train-loss:  2.1166052642718767 	 ± 0.27290331206148527
	data : 0.11775689125061035
	model : 0.0682976245880127
			 train-loss:  2.1168316666896527 	 ± 0.27186381892525563
	data : 0.1178469181060791
	model : 0.06838388442993164
			 train-loss:  2.119656867653359 	 ± 0.27273314231804274
	data : 0.11759252548217773
	model : 0.06877870559692383
			 train-loss:  2.1208250730326683 	 ± 0.27202689712987865
	data : 0.11720404624938965
	model : 0.06968379020690918
			 train-loss:  2.1212823615038303 	 ± 0.2710532326815221
	data : 0.11627936363220215
	model : 0.07000222206115722
			 train-loss:  2.1187459897639145 	 ± 0.27161956041622437
	data : 0.11587157249450683
	model : 0.07002296447753906
			 train-loss:  2.1189006955535326 	 ± 0.270617617810974
	data : 0.11575465202331543
	model : 0.07004694938659668
			 train-loss:  2.1211223277975533 	 ± 0.27085369348386407
	data : 0.11573610305786133
	model : 0.07078709602355956
			 train-loss:  2.121354532067793 	 ± 0.26987695196626
	data : 0.11504182815551758
	model : 0.0708230972290039
			 train-loss:  2.121516273505446 	 ± 0.26890402316759265
	data : 0.11502156257629395
	model : 0.07009897232055665
			 train-loss:  2.121773766956741 	 ± 0.26795207062345805
	data : 0.1156951904296875
	model : 0.06995611190795899
			 train-loss:  2.119749619279589 	 ± 0.2680577815050942
	data : 0.11598172187805175
	model : 0.0699998378753662
			 train-loss:  2.118862454772841 	 ± 0.26731171504413687
	data : 0.11589298248291016
	model : 0.06887202262878418
			 train-loss:  2.1196012899909222 	 ± 0.26651325221220595
	data : 0.11706194877624512
	model : 0.06881065368652343
			 train-loss:  2.118142503958482 	 ± 0.266148058733742
	data : 0.1171173095703125
	model : 0.06958861351013183
			 train-loss:  2.118347976770666 	 ± 0.26523370494259635
	data : 0.11643266677856445
	model : 0.06977219581604004
			 train-loss:  2.118077816634343 	 ± 0.2643374043530481
	data : 0.11619877815246582
	model : 0.06973114013671874
			 train-loss:  2.121521091624482 	 ± 0.2666736079567698
	data : 0.11630396842956543
	model : 0.07001166343688965
			 train-loss:  2.1206396161293495 	 ± 0.2659783464916653
	data : 0.11604366302490235
	model : 0.07010016441345215
			 train-loss:  2.1184544466637276 	 ± 0.2663989442471907
	data : 0.11589484214782715
	model : 0.07008605003356934
			 train-loss:  2.1198805706613015 	 ± 0.2660697395710407
	data : 0.11589069366455078
	model : 0.06987180709838867
			 train-loss:  2.1238643471399943 	 ± 0.2696031399169794
	data : 0.11612696647644043
	model : 0.06897001266479492
			 train-loss:  2.121837385442873 	 ± 0.2698532487224895
	data : 0.11673130989074706
	model : 0.0679931640625
			 train-loss:  2.1206665423355604 	 ± 0.2693486471199287
	data : 0.11750388145446777
	model : 0.06788983345031738
			 train-loss:  2.1219408893896863 	 ± 0.2689263118767967
	data : 0.11761837005615235
	model : 0.06792387962341309
			 train-loss:  2.119402066453711 	 ± 0.2698850165194302
	data : 0.11750283241271972
	model : 0.06831536293029786
			 train-loss:  2.119983140883907 	 ± 0.2691096389528765
	data : 0.11733908653259277
	model : 0.06828012466430664
			 train-loss:  2.116487200443561 	 ± 0.2717537763390671
	data : 0.11743345260620117
	model : 0.06915864944458008
			 train-loss:  2.117055659081526 	 ± 0.27097996736206165
	data : 0.11657834053039551
	model : 0.06878170967102051
			 train-loss:  2.1185197996187815 	 ± 0.27074334165843933
	data : 0.11692619323730469
	model : 0.06784343719482422
			 train-loss:  2.1188949803886175 	 ± 0.2699318036670621
	data : 0.11776938438415527
	model : 0.06816229820251465
			 train-loss:  2.116518411785364 	 ± 0.2707504907074758
	data : 0.11760382652282715
	model : 0.06965122222900391
			 train-loss:  2.1164276000135436 	 ± 0.26991078525984036
	data : 0.11649560928344727
	model : 0.06984353065490723
			 train-loss:  2.117776909728109 	 ± 0.26962057013988533
	data : 0.11649580001831054
	model : 0.06927227973937988
			 train-loss:  2.118397388721536 	 ± 0.26890823297137356
	data : 0.11703372001647949
	model : 0.0702244758605957
			 train-loss:  2.1184319192316474 	 ± 0.26808749969617934
	data : 0.11616744995117187
	model : 0.06958479881286621
			 train-loss:  2.1180431705532654 	 ± 0.26732024020553385
	data : 0.11654872894287109
	model : 0.0683553695678711
			 train-loss:  2.1172994849193527 	 ± 0.26668499114379546
	data : 0.11761155128479003
	model : 0.06831283569335937
			 train-loss:  2.1169722023124464 	 ± 0.2659187692692485
	data : 0.11753363609313965
	model : 0.06921191215515136
			 train-loss:  2.1146995631002246 	 ± 0.2667478562904067
	data : 0.11675243377685547
	model : 0.06918253898620605
			 train-loss:  2.1117637291462463 	 ± 0.2686659619309961
	data : 0.11666250228881836
	model : 0.06856656074523926
			 train-loss:  2.1105494394021878 	 ± 0.2683393244838678
	data : 0.11723847389221191
	model : 0.06844124794006348
			 train-loss:  2.109675105552227 	 ± 0.26779630977950514
	data : 0.11749749183654785
	model : 0.06838250160217285
			 train-loss:  2.107677022385043 	 ± 0.26829201658859325
	data : 0.11759209632873535
	model : 0.06745719909667969
			 train-loss:  2.1057775427151277 	 ± 0.26867287406061585
	data : 0.1183741569519043
	model : 0.06748847961425782
			 train-loss:  2.105936943114489 	 ± 0.26790791692587534
	data : 0.11847000122070313
	model : 0.06835818290710449
			 train-loss:  2.1058648470469885 	 ± 0.26714306187498155
	data : 0.11763496398925781
	model : 0.06910138130187989
			 train-loss:  2.109465292231603 	 ± 0.2706076332253987
	data : 0.11702661514282227
	model : 0.06873540878295899
			 train-loss:  2.109049543149054 	 ± 0.26989848461143673
	data : 0.11735544204711915
	model : 0.06879425048828125
			 train-loss:  2.1087970552819497 	 ± 0.2691602368769702
	data : 0.11729159355163574
	model : 0.0688854694366455
			 train-loss:  2.109216786629661 	 ± 0.2684657501688546
	data : 0.11730761528015136
	model : 0.06878690719604492
			 train-loss:  2.1082706967989604 	 ± 0.26801803871027025
	data : 0.11734175682067871
	model : 0.06905136108398438
			 train-loss:  2.106728833683288 	 ± 0.2680759584714523
	data : 0.11704869270324707
	model : 0.06938786506652832
			 train-loss:  2.1059666583826253 	 ± 0.2675350506744398
	data : 0.1167829990386963
	model : 0.07017536163330078
			 train-loss:  2.105039932037312 	 ± 0.2670958417037296
	data : 0.11608896255493165
	model : 0.06986656188964843
			 train-loss:  2.1045210912175802 	 ± 0.2664615039166567
	data : 0.11641993522644042
	model : 0.06986842155456544
			 train-loss:  2.1067840956352852 	 ± 0.26750745539666193
	data : 0.11645989418029785
	model : 0.06881427764892578
			 train-loss:  2.1094275014374846 	 ± 0.2691992034910216
	data : 0.11752963066101074
	model : 0.06786222457885742
			 train-loss:  2.109290821029541 	 ± 0.2684849258050536
	data : 0.11829090118408203
	model : 0.0678976058959961
			 train-loss:  2.109465887571903 	 ± 0.2677806196444066
	data : 0.11812376976013184
	model : 0.06810035705566406
			 train-loss:  2.109485529717945 	 ± 0.267071401556842
	data : 0.11786184310913086
	model : 0.06710400581359863
			 train-loss:  2.108600610180905 	 ± 0.26664532655297113
	data : 0.11875762939453124
	model : 0.06796860694885254
			 train-loss:  2.1078502551423317 	 ± 0.26614743373982147
	data : 0.11780791282653809
	model : 0.06818342208862305
			 train-loss:  2.1058439860741296 	 ± 0.26689759103458605
	data : 0.11770939826965332
	model : 0.0674670696258545
			 train-loss:  2.1052604561642663 	 ± 0.26632801525897365
	data : 0.11840171813964843
	model : 0.06747379302978515
			 train-loss:  2.1028913069017157 	 ± 0.26767195140690536
	data : 0.11832365989685059
	model : 0.06850137710571289
			 train-loss:  2.102705220075754 	 ± 0.2669973116298188
	data : 0.11743392944335937
	model : 0.06860356330871582
			 train-loss:  2.101925180882824 	 ± 0.26653799405897965
	data : 0.11720314025878906
	model : 0.06938233375549316
			 train-loss:  2.0986920940089346 	 ± 0.269686186487001
	data : 0.11670327186584473
	model : 0.07018108367919922
			 train-loss:  2.0980714938857337 	 ± 0.2691452882560485
	data : 0.11601943969726562
	model : 0.0698246955871582
			 train-loss:  2.1005156765032056 	 ± 0.27066220248789663
	data : 0.11623382568359375
	model : 0.06977157592773438
			 train-loss:  2.099404581785202 	 ± 0.27043928982180787
	data : 0.11625285148620605
	model : 0.06968941688537597
			 train-loss:  2.0995864168328433 	 ± 0.2697779727245751
	data : 0.11631035804748535
	model : 0.06971607208251954
			 train-loss:  2.099669423433814 	 ± 0.2691119500909432
	data : 0.11621894836425781
	model : 0.06972599029541016
			 train-loss:  2.0996522327949263 	 ± 0.2684484056411357
	data : 0.11623735427856445
	model : 0.07010555267333984
			 train-loss:  2.098565446979859 	 ± 0.2682369342618905
	data : 0.11587715148925781
	model : 0.0695467472076416
			 train-loss:  2.097257657748897 	 ± 0.26823306273009667
	data : 0.11635870933532715
	model : 0.0695950984954834
			 train-loss:  2.0969369498271386 	 ± 0.26762061589148234
	data : 0.11644206047058106
	model : 0.06950225830078124
			 train-loss:  2.0953906878181128 	 ± 0.2678942527813837
	data : 0.11632351875305176
	model : 0.06906824111938477
			 train-loss:  2.096227367910055 	 ± 0.2675204709549507
	data : 0.11680412292480469
	model : 0.0681889533996582
			 train-loss:  2.096156507587889 	 ± 0.26688165913162043
	data : 0.11754851341247559
	model : 0.06873030662536621
			 train-loss:  2.0951231996218365 	 ± 0.26666421583749306
	data : 0.11708192825317383
	model : 0.06842250823974609
			 train-loss:  2.0926363445571248 	 ± 0.26846140285484543
	data : 0.11722822189331054
	model : 0.06860299110412597
			 train-loss:  2.091825043255428 	 ± 0.26808664097319934
	data : 0.11713485717773438
	model : 0.06922597885131836
			 train-loss:  2.0928354531946316 	 ± 0.2678609040472246
	data : 0.11662869453430176
	model : 0.07004971504211426
			 train-loss:  2.0954303886288796 	 ± 0.26990454238232314
	data : 0.11619391441345214
	model : 0.07038674354553223
			 train-loss:  2.095982221115467 	 ± 0.26939710285888385
	data : 0.11580934524536132
	model : 0.07056245803833008
			 train-loss:  2.094612334061552 	 ± 0.2695223025187642
	data : 0.1157078742980957
	model : 0.06975202560424805
			 train-loss:  2.092988962401992 	 ± 0.2699569354933771
	data : 0.116550874710083
	model : 0.06890273094177246
			 train-loss:  2.0931504396123626 	 ± 0.26934756035150337
	data : 0.11731271743774414
	model : 0.06811628341674805
			 train-loss:  2.0919642666159155 	 ± 0.26930199731312376
	data : 0.11802158355712891
	model : 0.06773648262023926
			 train-loss:  2.0907296473329717 	 ± 0.26930973163010946
	data : 0.11831893920898437
	model : 0.0677342414855957
			 train-loss:  2.0908276635597196 	 ± 0.26870367572786297
	data : 0.11821861267089843
	model : 0.06830863952636719
			 train-loss:  2.089791036403931 	 ± 0.2685403476350442
	data : 0.1175954818725586
	model : 0.06884827613830566
			 train-loss:  2.0916811740986434 	 ± 0.2694135469134157
	data : 0.11684160232543946
	model : 0.0687417984008789
			 train-loss:  2.091364965374981 	 ± 0.26885297540055864
	data : 0.11672449111938477
	model : 0.0680356502532959
			 train-loss:  2.089734084341261 	 ± 0.2693630608181212
	data : 0.11753883361816406
	model : 0.06722440719604492
			 train-loss:  2.091035820214094 	 ± 0.26947481964035247
	data : 0.11836905479431152
	model : 0.06701164245605469
			 train-loss:  2.0918115335414065 	 ± 0.2691333728878524
	data : 0.11825222969055176
	model : 0.0664560317993164
			 train-loss:  2.0906070664263607 	 ± 0.2691549795523496
	data : 0.1187981128692627
	model : 0.06678671836853027
			 train-loss:  2.0908273208609836 	 ± 0.26858725333229383
	data : 0.1184239387512207
	model : 0.06675329208374023
			 train-loss:  2.089567968638047 	 ± 0.2686794577439985
	data : 0.11822395324707032
	model : 0.06673774719238282
			 train-loss:  2.090213992894986 	 ± 0.26827623062655687
	data : 0.11814026832580567
	model : 0.06634039878845215
			 train-loss:  2.090936065233987 	 ± 0.26792228711008576
	data : 0.11885757446289062
	model : 0.0667231559753418
			 train-loss:  2.091642567528164 	 ± 0.2675632166810659
	data : 0.11852951049804687
	model : 0.06704797744750976
			 train-loss:  2.09035227033827 	 ± 0.26771635811604116
	data : 0.11835346221923829
	model : 0.06698374748229981
			 train-loss:  2.09068484103426 	 ± 0.2671945772795108
	data : 0.11863088607788086
	model : 0.06773142814636231
			 train-loss:  2.090132684525797 	 ± 0.26676220817658225
	data : 0.11807947158813477
	model : 0.06835041046142579
			 train-loss:  2.0891590002719864 	 ± 0.2666187468850943
	data : 0.11737604141235351
	model : 0.06863737106323242
			 train-loss:  2.0881465157540906 	 ± 0.2665142247312748
	data : 0.1174311637878418
	model : 0.06829113960266113
			 train-loss:  2.0871374093339035 	 ± 0.2664113181285599
	data : 0.11765742301940918
	model : 0.06901364326477051
			 train-loss:  2.087529521683852 	 ± 0.2659248165382808
	data : 0.11693820953369141
	model : 0.06869053840637207
			 train-loss:  2.087771568555555 	 ± 0.26539902305163854
	data : 0.11703648567199706
	model : 0.06816091537475585
			 train-loss:  2.0886230394859946 	 ± 0.2651797627961018
	data : 0.11743197441101075
	model : 0.06772866249084472
			 train-loss:  2.0890692443023493 	 ± 0.264724582374706
	data : 0.11738266944885253
	model : 0.06801238059997558
			 train-loss:  2.0902924620714343 	 ± 0.2648688102899477
	data : 0.1171679973602295
	model : 0.06758265495300293
			 train-loss:  2.0920215874302146 	 ± 0.26570409691761854
	data : 0.11755213737487794
	model : 0.06714019775390626
			 train-loss:  2.0929836107463373 	 ± 0.2655907103759314
	data : 0.11795001029968262
	model : 0.06731276512145996
			 train-loss:  2.0962876110424395 	 ± 0.2700708862234826
	data : 0.11765375137329101
	model : 0.06735062599182129
			 train-loss:  2.096304022016064 	 ± 0.2695259618763862
	data : 0.11747546195983886
	model : 0.06707677841186524
			 train-loss:  2.095028606284574 	 ± 0.2697330497446259
	data : 0.11763982772827149
	model : 0.06675996780395507
			 train-loss:  2.094619707107544 	 ± 0.26927036021022765
	data : 0.11792874336242676
	model : 0.06697654724121094
			 train-loss:  2.093054673586234 	 ± 0.26987031838162784
	data : 0.1178196907043457
	model : 0.0673823356628418
			 train-loss:  2.0919974134081887 	 ± 0.26985468019225894
	data : 0.11797075271606446
	model : 0.06740269660949708
			 train-loss:  2.0914930180598623 	 ± 0.2694398424707344
	data : 0.11833949089050293
	model : 0.06778159141540527
			 train-loss:  2.0921001955280154 	 ± 0.2690822970770755
	data : 0.11809611320495605
	model : 0.06786646842956542
			 train-loss:  2.09237194295023 	 ± 0.2685890864640344
	data : 0.1179971694946289
	model : 0.06800503730773926
			 train-loss:  2.0920729911886156 	 ± 0.26810649005938736
	data : 0.11700763702392578
	model : 0.05908017158508301
#epoch  20    val-loss:  2.429256407838119  train-loss:  2.0920729911886156  lr:  0.00125
			 train-loss:  1.867064356803894 	 ± 0.0
	data : 5.696235179901123
	model : 0.07117128372192383
			 train-loss:  1.7722508907318115 	 ± 0.09481346607208252
	data : 2.913985252380371
	model : 0.0719527006149292
			 train-loss:  1.870743751525879 	 ± 0.1593573013300626
	data : 1.9821927547454834
	model : 0.07112693786621094
			 train-loss:  1.9116060137748718 	 ± 0.15509750334456288
	data : 1.5154374837875366
	model : 0.07062423229217529
			 train-loss:  1.9418991088867188 	 ± 0.1513765995152321
	data : 1.2355443000793458
	model : 0.07042865753173828
			 train-loss:  1.9817474683125813 	 ± 0.16442380549054575
	data : 0.11948165893554688
	model : 0.07022294998168946
			 train-loss:  2.0042130265917097 	 ± 0.1618679275746827
	data : 0.11635923385620117
	model : 0.06950488090515136
			 train-loss:  2.0209164023399353 	 ± 0.15773107512428322
	data : 0.11603722572326661
	model : 0.06954464912414551
			 train-loss:  2.0699841711256237 	 ± 0.2034107076004155
	data : 0.11617865562438964
	model : 0.06966381072998047
			 train-loss:  2.0578992486000063 	 ± 0.19634849768180868
	data : 0.11609401702880859
	model : 0.06971201896667481
			 train-loss:  2.0706627910787407 	 ± 0.19151245087597346
	data : 0.1160797119140625
	model : 0.06871547698974609
			 train-loss:  2.0561912059783936 	 ± 0.18953705477371574
	data : 0.11687674522399902
	model : 0.06896324157714843
			 train-loss:  2.0522026465489316 	 ± 0.18262472748862332
	data : 0.11677889823913574
	model : 0.0691147804260254
			 train-loss:  2.039843167577471 	 ± 0.18153610567577927
	data : 0.1167337417602539
	model : 0.06867666244506836
			 train-loss:  2.0303866545359295 	 ± 0.1789142050378721
	data : 0.11711139678955078
	model : 0.06778273582458497
			 train-loss:  2.0206804275512695 	 ± 0.17726480784614002
	data : 0.11791648864746093
	model : 0.06883578300476074
			 train-loss:  2.026138319688685 	 ± 0.17335232555192306
	data : 0.11699728965759278
	model : 0.06885781288146972
			 train-loss:  2.004359073109097 	 ± 0.19090634369359294
	data : 0.11694574356079102
	model : 0.06873698234558105
			 train-loss:  2.001520878390262 	 ± 0.18620434538598263
	data : 0.11701288223266601
	model : 0.06914381980895996
			 train-loss:  2.0035350382328034 	 ± 0.18170177568242424
	data : 0.11673130989074706
	model : 0.06993184089660645
			 train-loss:  2.005557065918332 	 ± 0.1775532000793215
	data : 0.11606340408325196
	model : 0.06993927955627441
			 train-loss:  2.007415560158816 	 ± 0.17367991253707896
	data : 0.11613712310791016
	model : 0.0698659896850586
			 train-loss:  2.032560198203377 	 ± 0.20679159773380154
	data : 0.11623978614807129
	model : 0.06987342834472657
			 train-loss:  2.0166400571664176 	 ± 0.21635700475692507
	data : 0.11615705490112305
	model : 0.06994009017944336
			 train-loss:  2.005607256889343 	 ± 0.2187676478229447
	data : 0.11615161895751953
	model : 0.07002363204956055
			 train-loss:  2.0175179288937497 	 ± 0.22263230839620143
	data : 0.1161041259765625
	model : 0.06993050575256347
			 train-loss:  2.029095654134397 	 ± 0.2263062867127614
	data : 0.11616411209106445
	model : 0.07080473899841308
			 train-loss:  2.0448210537433624 	 ± 0.2367746313750245
	data : 0.11519351005554199
	model : 0.07069511413574218
			 train-loss:  2.0366448698372674 	 ± 0.2366449775194338
	data : 0.11522645950317383
	model : 0.07062258720397949
			 train-loss:  2.0324433048566184 	 ± 0.23376503553635403
	data : 0.115134859085083
	model : 0.07058863639831543
			 train-loss:  2.0310059093659922 	 ± 0.23009845253086803
	data : 0.11509947776794434
	model : 0.07056212425231934
			 train-loss:  2.049675013870001 	 ± 0.24918939915553187
	data : 0.11500406265258789
	model : 0.0696375846862793
			 train-loss:  2.058560382236134 	 ± 0.2504796835060029
	data : 0.11591563224792481
	model : 0.06977992057800293
			 train-loss:  2.05386023311054 	 ± 0.24824139839660037
	data : 0.11593213081359863
	model : 0.06985721588134766
			 train-loss:  2.0591905696051462 	 ± 0.2466356354481535
	data : 0.11601004600524903
	model : 0.06999640464782715
			 train-loss:  2.0659197999371424 	 ± 0.24642306580784432
	data : 0.11588501930236816
	model : 0.06941981315612793
			 train-loss:  2.073460253509315 	 ± 0.24724488501886172
	data : 0.11668105125427246
	model : 0.06885013580322266
			 train-loss:  2.0762797876408228 	 ± 0.24457205415830965
	data : 0.11717453002929687
	model : 0.06881160736083984
			 train-loss:  2.0923079129977102 	 ± 0.26085242890196497
	data : 0.11720609664916992
	model : 0.06868362426757812
			 train-loss:  2.089665171504021 	 ± 0.2580993396813514
	data : 0.1172670841217041
	model : 0.06837372779846192
			 train-loss:  2.0786579120449904 	 ± 0.26426671548615155
	data : 0.11757097244262696
	model : 0.06850752830505372
			 train-loss:  2.074205585888454 	 ± 0.26265350785846475
	data : 0.11730837821960449
	model : 0.06907491683959961
			 train-loss:  2.069118164306463 	 ± 0.2616668827709714
	data : 0.1168304443359375
	model : 0.06908593177795411
			 train-loss:  2.0689324709502133 	 ± 0.2586791718680514
	data : 0.11679372787475586
	model : 0.06828374862670898
			 train-loss:  2.072274586889479 	 ± 0.2567477057039525
	data : 0.11759657859802246
	model : 0.06845149993896485
			 train-loss:  2.0683980454569277 	 ± 0.2552696517321568
	data : 0.1173370361328125
	model : 0.06874933242797851
			 train-loss:  2.0625177393568324 	 ± 0.2556692099063827
	data : 0.11703324317932129
	model : 0.06865506172180176
			 train-loss:  2.0695373912652335 	 ± 0.2575284194334087
	data : 0.11711554527282715
	model : 0.06768808364868165
			 train-loss:  2.063774196469054 	 ± 0.2579955255702114
	data : 0.11805281639099122
	model : 0.06805825233459473
			 train-loss:  2.063833613395691 	 ± 0.2554028785627757
	data : 0.1177546501159668
	model : 0.06803150177001953
			 train-loss:  2.0569935008591296 	 ± 0.25747030000680476
	data : 0.11780872344970703
	model : 0.06789379119873047
			 train-loss:  2.059500691982416 	 ± 0.2556104764954861
	data : 0.11782488822937012
	model : 0.06797161102294921
			 train-loss:  2.0603711582579702 	 ± 0.2532653716431065
	data : 0.11775245666503906
	model : 0.06896867752075195
			 train-loss:  2.0576103969856545 	 ± 0.25171305932652416
	data : 0.11695423126220703
	model : 0.06878471374511719
			 train-loss:  2.054593719135631 	 ± 0.25039746927681794
	data : 0.11690983772277833
	model : 0.06832523345947265
			 train-loss:  2.0541620211941853 	 ± 0.2481723584707339
	data : 0.11745223999023438
	model : 0.06917853355407715
			 train-loss:  2.056834944507532 	 ± 0.24679767966695038
	data : 0.11685791015625
	model : 0.06843180656433105
			 train-loss:  2.062395856298249 	 ± 0.24823696554063823
	data : 0.11755867004394531
	model : 0.06828975677490234
			 train-loss:  2.060897469520569 	 ± 0.24638867012160076
	data : 0.11751155853271485
	model : 0.06811742782592774
			 train-loss:  2.060170215368271 	 ± 0.24439065483171174
	data : 0.11763558387756348
	model : 0.06773672103881836
			 train-loss:  2.0541530652124376 	 ± 0.24681983345847047
	data : 0.11800932884216309
	model : 0.06721673011779786
			 train-loss:  2.048512468414922 	 ± 0.24875338253490278
	data : 0.11840147972106933
	model : 0.06798295974731446
			 train-loss:  2.0504675914370822 	 ± 0.2472509784132853
	data : 0.11764388084411621
	model : 0.067974853515625
			 train-loss:  2.046927450224757 	 ± 0.24691576755055528
	data : 0.11779656410217285
	model : 0.06830382347106934
			 train-loss:  2.046361543582036 	 ± 0.24505087705117448
	data : 0.11764688491821289
	model : 0.0692166805267334
			 train-loss:  2.0509017290491047 	 ± 0.2459267133694485
	data : 0.11678023338317871
	model : 0.06862502098083496
			 train-loss:  2.0526924471356978 	 ± 0.2445176943617427
	data : 0.11720242500305175
	model : 0.06834602355957031
			 train-loss:  2.053117553977405 	 ± 0.24273805273374666
	data : 0.11757593154907227
	model : 0.06764817237854004
			 train-loss:  2.0485933172530024 	 ± 0.2438435901997928
	data : 0.11830992698669433
	model : 0.06814899444580078
			 train-loss:  2.046604754243578 	 ± 0.2426584526906144
	data : 0.11785349845886231
	model : 0.06765956878662109
			 train-loss:  2.0402638257389336 	 ± 0.24671501997101714
	data : 0.1182255744934082
	model : 0.06821870803833008
			 train-loss:  2.042666870686743 	 ± 0.24583105252440782
	data : 0.11786656379699707
	model : 0.06841330528259278
			 train-loss:  2.0424704568026817 	 ± 0.24414716053720703
	data : 0.11769609451293946
	model : 0.06906652450561523
			 train-loss:  2.040386998975599 	 ± 0.24314440739376103
	data : 0.11711153984069825
	model : 0.0689352035522461
			 train-loss:  2.0386528031031292 	 ± 0.24197829866103257
	data : 0.1171999454498291
	model : 0.06869454383850097
			 train-loss:  2.0370443212358573 	 ± 0.2407843377721801
	data : 0.11755790710449218
	model : 0.06847310066223145
			 train-loss:  2.0350636228338463 	 ± 0.2398380883611473
	data : 0.11765866279602051
	model : 0.06857528686523437
			 train-loss:  2.032375075878241 	 ± 0.23946068791743558
	data : 0.11738648414611816
	model : 0.06871771812438965
			 train-loss:  2.0346385949774635 	 ± 0.23877858937640364
	data : 0.11718568801879883
	model : 0.06838645935058593
			 train-loss:  2.0328689485788347 	 ± 0.23780228085953276
	data : 0.11753344535827637
	model : 0.06911797523498535
			 train-loss:  2.0310819811291165 	 ± 0.23686966411039936
	data : 0.11692299842834472
	model : 0.06875071525573731
			 train-loss:  2.031712440455832 	 ± 0.2354892756894162
	data : 0.11726198196411133
	model : 0.0678177833557129
			 train-loss:  2.0292249415294235 	 ± 0.23514772253196192
	data : 0.11812415122985839
	model : 0.06713948249816895
			 train-loss:  2.0334003227097646 	 ± 0.23681889746671406
	data : 0.11869392395019532
	model : 0.06709184646606445
			 train-loss:  2.0320925979053275 	 ± 0.23572662146970585
	data : 0.11859855651855469
	model : 0.06682052612304687
			 train-loss:  2.031359557495561 	 ± 0.23444953891449727
	data : 0.11852374076843261
	model : 0.06738090515136719
			 train-loss:  2.0381763132139183 	 ± 0.24151821481570926
	data : 0.11812701225280761
	model : 0.06807913780212402
			 train-loss:  2.033527284860611 	 ± 0.24402575005803265
	data : 0.11748170852661133
	model : 0.06795520782470703
			 train-loss:  2.0364324617921636 	 ± 0.24417658842874385
	data : 0.1176264762878418
	model : 0.06861648559570313
			 train-loss:  2.038988208770752 	 ± 0.2440103919312775
	data : 0.11733241081237793
	model : 0.0690424919128418
			 train-loss:  2.036153203838474 	 ± 0.2441518516206907
	data : 0.11715002059936523
	model : 0.06829133033752441
			 train-loss:  2.0395007418549578 	 ± 0.24491210151972656
	data : 0.11799750328063965
	model : 0.06842303276062012
			 train-loss:  2.0412639135955484 	 ± 0.24417816786768093
	data : 0.11797151565551758
	model : 0.06878266334533692
			 train-loss:  2.0406380409890033 	 ± 0.24295085985261533
	data : 0.11759109497070312
	model : 0.06876645088195801
			 train-loss:  2.0412040258708752 	 ± 0.2417310801337604
	data : 0.1175260066986084
	model : 0.06860880851745606
			 train-loss:  2.0379351836939654 	 ± 0.24257026767278983
	data : 0.11775832176208496
	model : 0.0694037914276123
			 train-loss:  2.0354098610042297 	 ± 0.2425818414876979
	data : 0.11665925979614258
	model : 0.06917009353637696
			 train-loss:  2.0348784996538747 	 ± 0.2413977390069425
	data : 0.11685261726379395
	model : 0.06870942115783692
			 train-loss:  2.036253712394021 	 ± 0.24056099466303552
	data : 0.11730413436889649
	model : 0.06777558326721192
			 train-loss:  2.0366531348228456 	 ± 0.23938815869028376
	data : 0.11802825927734376
	model : 0.06768951416015626
			 train-loss:  2.0367084682577907 	 ± 0.23820076348443997
	data : 0.11791410446166992
	model : 0.06766996383666993
			 train-loss:  2.0370571028952504 	 ± 0.23705613102591022
	data : 0.11803598403930664
	model : 0.06792821884155273
			 train-loss:  2.0383498414048873 	 ± 0.23626358256981492
	data : 0.117706298828125
	model : 0.06887946128845215
			 train-loss:  2.0346771547427545 	 ± 0.23806107131063878
	data : 0.11680030822753906
	model : 0.06981301307678223
			 train-loss:  2.0327528544834683 	 ± 0.23773606160932678
	data : 0.11602654457092285
	model : 0.06959128379821777
			 train-loss:  2.036884035704271 	 ± 0.24036896914098646
	data : 0.11622319221496583
	model : 0.06943726539611816
			 train-loss:  2.0367992147106992 	 ± 0.23924470680790869
	data : 0.116387939453125
	model : 0.06949357986450196
			 train-loss:  2.0379970956731728 	 ± 0.23845667172553273
	data : 0.1164423942565918
	model : 0.06952013969421386
			 train-loss:  2.039368791317721 	 ± 0.23778798469467444
	data : 0.11647596359252929
	model : 0.06952419281005859
			 train-loss:  2.040869209983132 	 ± 0.23722243606520857
	data : 0.1164665699005127
	model : 0.06894402503967285
			 train-loss:  2.040891505576469 	 ± 0.23615156471661525
	data : 0.11707835197448731
	model : 0.06897926330566406
			 train-loss:  2.0433292474065508 	 ± 0.23649368664350995
	data : 0.11702432632446289
	model : 0.06902251243591309
			 train-loss:  2.042868421141025 	 ± 0.23549543307617993
	data : 0.1169393539428711
	model : 0.06904377937316894
			 train-loss:  2.0459223724248115 	 ± 0.23669713207229814
	data : 0.11692862510681153
	model : 0.06802678108215332
			 train-loss:  2.0432568011076553 	 ± 0.23737808369809657
	data : 0.11784696578979492
	model : 0.06889858245849609
			 train-loss:  2.041933621825843 	 ± 0.23677824083893217
	data : 0.11697111129760743
	model : 0.06888151168823242
			 train-loss:  2.044926976546263 	 ± 0.23795826610613122
	data : 0.11689834594726563
	model : 0.0679318904876709
			 train-loss:  2.0449163156040644 	 ± 0.2369478511047742
	data : 0.11777839660644532
	model : 0.06765961647033691
			 train-loss:  2.0448267109253826 	 ± 0.2359521791759951
	data : 0.11800260543823242
	model : 0.06895971298217773
			 train-loss:  2.044918234149615 	 ± 0.2349691094682874
	data : 0.11695394515991211
	model : 0.06908822059631348
			 train-loss:  2.048373436139635 	 ± 0.23703756647584867
	data : 0.11703968048095703
	model : 0.06929240226745606
			 train-loss:  2.04527945108101 	 ± 0.23850484750542564
	data : 0.11700491905212403
	model : 0.07031035423278809
			 train-loss:  2.0451966737344014 	 ± 0.23753509661170336
	data : 0.11610727310180664
	model : 0.07079596519470215
			 train-loss:  2.0441960942360664 	 ± 0.23683547275424058
	data : 0.11571626663208008
	model : 0.06998448371887207
			 train-loss:  2.045399215698242 	 ± 0.23626638140206838
	data : 0.11633954048156739
	model : 0.06898007392883301
			 train-loss:  2.044046825832791 	 ± 0.23581219771666195
	data : 0.11720356941223145
	model : 0.06975131034851074
			 train-loss:  2.044709567948589 	 ± 0.23499974834689644
	data : 0.11651644706726075
	model : 0.06969070434570312
			 train-loss:  2.042440289631486 	 ± 0.2354728001125888
	data : 0.11665091514587403
	model : 0.0692814826965332
			 train-loss:  2.042104435521503 	 ± 0.23458911445672176
	data : 0.1170379638671875
	model : 0.06978807449340821
			 train-loss:  2.0442402482032778 	 ± 0.23494081786608434
	data : 0.1166612148284912
	model : 0.07070188522338867
			 train-loss:  2.0467075855677366 	 ± 0.23572705253599044
	data : 0.11575856208801269
	model : 0.06985363960266114
			 train-loss:  2.0455695471980353 	 ± 0.23519341303973945
	data : 0.11644363403320312
	model : 0.06994175910949707
			 train-loss:  2.048132382837453 	 ± 0.23615042854577067
	data : 0.11634135246276855
	model : 0.06917424201965332
			 train-loss:  2.0493201987067264 	 ± 0.23566608505350997
	data : 0.1167337417602539
	model : 0.06822218894958496
			 train-loss:  2.04874850820612 	 ± 0.23488487083031293
	data : 0.11755590438842774
	model : 0.06816658973693848
			 train-loss:  2.0494838067713905 	 ± 0.23417562593322724
	data : 0.11757793426513671
	model : 0.06802759170532227
			 train-loss:  2.050215492283341 	 ± 0.23347538250774652
	data : 0.11783599853515625
	model : 0.06801104545593262
			 train-loss:  2.0497874712598496 	 ± 0.23268185886566112
	data : 0.1179654598236084
	model : 0.06896576881408692
			 train-loss:  2.048713555438913 	 ± 0.23218634688942635
	data : 0.11728887557983399
	model : 0.06985912322998047
			 train-loss:  2.0509482775415693 	 ± 0.23285100022097271
	data : 0.11640734672546386
	model : 0.06999845504760742
			 train-loss:  2.0503338068089585 	 ± 0.2321377018409948
	data : 0.11640830039978027
	model : 0.07014851570129395
			 train-loss:  2.0530302835182406 	 ± 0.23352436573174518
	data : 0.11630330085754395
	model : 0.06911845207214355
			 train-loss:  2.0513883570691087 	 ± 0.23352750657828616
	data : 0.11690502166748047
	model : 0.06887331008911132
			 train-loss:  2.0545713620053396 	 ± 0.23580752278624748
	data : 0.11702508926391601
	model : 0.06875171661376953
			 train-loss:  2.0526626940431267 	 ± 0.2361065388271418
	data : 0.117041015625
	model : 0.06868562698364258
			 train-loss:  2.0509321011909067 	 ± 0.23621757281193906
	data : 0.11711759567260742
	model : 0.06872081756591797
			 train-loss:  2.0513458195186796 	 ± 0.23546581111713272
	data : 0.11694059371948243
	model : 0.06972360610961914
			 train-loss:  2.0543383577385463 	 ± 0.23745726452701868
	data : 0.11624884605407715
	model : 0.06975655555725098
			 train-loss:  2.0553121926800517 	 ± 0.23695543827987933
	data : 0.11621484756469727
	model : 0.06994891166687012
			 train-loss:  2.054427069822947 	 ± 0.23641128031580919
	data : 0.11617169380187989
	model : 0.07002968788146972
			 train-loss:  2.0532915947453074 	 ± 0.2360371890498882
	data : 0.11607480049133301
	model : 0.07009592056274414
			 train-loss:  2.0548501336260845 	 ± 0.23603771654320987
	data : 0.1160123348236084
	model : 0.07007398605346679
			 train-loss:  2.05575445196987 	 ± 0.2355291180659214
	data : 0.11599230766296387
	model : 0.07015113830566407
			 train-loss:  2.0549904200937843 	 ± 0.23495330991980096
	data : 0.11586995124816894
	model : 0.06990041732788085
			 train-loss:  2.0549429262838057 	 ± 0.23419491124689965
	data : 0.11600337028503419
	model : 0.06959772109985352
			 train-loss:  2.0534415260339394 	 ± 0.2341902493631418
	data : 0.1162379264831543
	model : 0.06862983703613282
			 train-loss:  2.0532074521301658 	 ± 0.23346153549286777
	data : 0.1171875
	model : 0.06842103004455566
			 train-loss:  2.053490750397308 	 ± 0.2327486309189758
	data : 0.11747856140136718
	model : 0.06855778694152832
			 train-loss:  2.051509118679934 	 ± 0.233348807188332
	data : 0.11741619110107422
	model : 0.06889510154724121
			 train-loss:  2.0512957416474817 	 ± 0.23263400898489567
	data : 0.11716628074645996
	model : 0.06815943717956544
			 train-loss:  2.0523184700782253 	 ± 0.23227095734412434
	data : 0.11778807640075684
	model : 0.06897687911987305
			 train-loss:  2.053034625671528 	 ± 0.23173119702721892
	data : 0.1169851303100586
	model : 0.06907420158386231
			 train-loss:  2.0553304088627633 	 ± 0.23285992428841104
	data : 0.11675605773925782
	model : 0.06882767677307129
			 train-loss:  2.0540636567080894 	 ± 0.23271156288128753
	data : 0.1172935962677002
	model : 0.06816511154174805
			 train-loss:  2.0516696026830963 	 ± 0.2340222777749773
	data : 0.11782903671264648
	model : 0.0684889316558838
			 train-loss:  2.054487214749118 	 ± 0.23610682538333927
	data : 0.11761794090270997
	model : 0.06850347518920899
			 train-loss:  2.0593338562342933 	 ± 0.24354045045626815
	data : 0.11755533218383789
	model : 0.06856961250305176
			 train-loss:  2.060204424318813 	 ± 0.24307503185005397
	data : 0.11738085746765137
	model : 0.06882848739624023
			 train-loss:  2.0592599479404425 	 ± 0.24266378988712978
	data : 0.11689624786376954
	model : 0.06928620338439942
			 train-loss:  2.05998771611382 	 ± 0.2421339270422768
	data : 0.11664290428161621
	model : 0.06987347602844238
			 train-loss:  2.0622041099949886 	 ± 0.24314828445150063
	data : 0.11606087684631347
	model : 0.0698967456817627
			 train-loss:  2.0602715029272924 	 ± 0.2437540581362773
	data : 0.11603899002075195
	model : 0.06988034248352051
			 train-loss:  2.0600332138855335 	 ± 0.24306863635981965
	data : 0.11599144935607911
	model : 0.06902580261230469
			 train-loss:  2.0598021636064026 	 ± 0.24238820856092846
	data : 0.11671276092529297
	model : 0.06815414428710938
			 train-loss:  2.06227445602417 	 ± 0.24388490250665942
	data : 0.11750330924987792
	model : 0.06721439361572265
			 train-loss:  2.062219886617227 	 ± 0.24319213211086788
	data : 0.11820182800292969
	model : 0.06720161437988281
			 train-loss:  2.060875080399594 	 ± 0.24315956048586668
	data : 0.11818056106567383
	model : 0.06710762977600097
			 train-loss:  2.061185434962926 	 ± 0.24251071880581548
	data : 0.11849474906921387
	model : 0.06789259910583496
			 train-loss:  2.0626774300410093 	 ± 0.24265022137616638
	data : 0.1178044319152832
	model : 0.06838383674621581
			 train-loss:  2.065754834810893 	 ± 0.24545310928476813
	data : 0.11740670204162598
	model : 0.06870360374450683
			 train-loss:  2.0667417484093766 	 ± 0.24513198681673465
	data : 0.11708240509033203
	model : 0.06876130104064941
			 train-loss:  2.0658131739595436 	 ± 0.2447766229967249
	data : 0.11714634895324708
	model : 0.06896100044250489
			 train-loss:  2.066089058834347 	 ± 0.24413529038281023
	data : 0.11705613136291504
	model : 0.06902174949645996
			 train-loss:  2.063413739852283 	 ± 0.24614611021264962
	data : 0.11705961227416992
	model : 0.06951498985290527
			 train-loss:  2.0648758044113986 	 ± 0.24627977951980812
	data : 0.11660261154174804
	model : 0.0699319839477539
			 train-loss:  2.0669594855718714 	 ± 0.24724654498464207
	data : 0.11637945175170898
	model : 0.06980547904968262
			 train-loss:  2.067604522016597 	 ± 0.24674144451030838
	data : 0.1164219856262207
	model : 0.06943774223327637
			 train-loss:  2.067178835894199 	 ± 0.24615318308726505
	data : 0.11649107933044434
	model : 0.06915431022644043
			 train-loss:  2.0664700326465426 	 ± 0.24569341009798584
	data : 0.11668128967285156
	model : 0.06918325424194335
			 train-loss:  2.068891753648457 	 ± 0.2472973400133523
	data : 0.11679730415344239
	model : 0.06844220161437989
			 train-loss:  2.0680573978973307 	 ± 0.246917100337027
	data : 0.11733865737915039
	model : 0.06853857040405273
			 train-loss:  2.068300169582168 	 ± 0.2462961016013373
	data : 0.11732821464538574
	model : 0.06886043548583984
			 train-loss:  2.0679229003777775 	 ± 0.24571281556063293
	data : 0.11720175743103027
	model : 0.06821975708007813
			 train-loss:  2.0686678339525595 	 ± 0.24529712229997838
	data : 0.11777739524841309
	model : 0.06809759140014648
			 train-loss:  2.0674806814927322 	 ± 0.24522544899518695
	data : 0.11777215003967285
	model : 0.0681447982788086
			 train-loss:  2.06618071514733 	 ± 0.24527176741406703
	data : 0.11786446571350098
	model : 0.0679135799407959
			 train-loss:  2.065502168563417 	 ± 0.2448328235873788
	data : 0.11810116767883301
	model : 0.0678441047668457
			 train-loss:  2.0655746249237446 	 ± 0.24421589373630784
	data : 0.11809053421020507
	model : 0.0685490608215332
			 train-loss:  2.066454157158358 	 ± 0.243915693321259
	data : 0.11752147674560547
	model : 0.06864814758300782
			 train-loss:  2.0656364822387694 	 ± 0.24357840828714522
	data : 0.11744422912597656
	model : 0.06971936225891114
			 train-loss:  2.0670939286549888 	 ± 0.24384440668513846
	data : 0.11640057563781739
	model : 0.0690378189086914
			 train-loss:  2.06704052368013 	 ± 0.2432412609687581
	data : 0.11692748069763184
	model : 0.06911029815673828
			 train-loss:  2.067394271859982 	 ± 0.24269348818599684
	data : 0.1168135166168213
	model : 0.06839885711669921
			 train-loss:  2.066816851204517 	 ± 0.24223766480634282
	data : 0.11741666793823242
	model : 0.06751313209533691
			 train-loss:  2.066159923483686 	 ± 0.24182821139789487
	data : 0.11820220947265625
	model : 0.0663877010345459
			 train-loss:  2.0684158472181524 	 ± 0.2433932628093005
	data : 0.11912837028503417
	model : 0.06721076965332032
			 train-loss:  2.0685514211654663 	 ± 0.24281244159307366
	data : 0.11836657524108887
	model : 0.06711235046386718
			 train-loss:  2.0704583955498843 	 ± 0.24377694016724324
	data : 0.11849451065063477
	model : 0.06767616271972657
			 train-loss:  2.0717477667274657 	 ± 0.24390295474820245
	data : 0.11798267364501953
	model : 0.06757402420043945
			 train-loss:  2.0718895724841526 	 ± 0.243330176503475
	data : 0.11797146797180176
	model : 0.06886749267578125
			 train-loss:  2.0713072297697384 	 ± 0.24289951942402832
	data : 0.11724286079406739
	model : 0.06898784637451172
			 train-loss:  2.0696244341022565 	 ± 0.24355571001817056
	data : 0.11704602241516113
	model : 0.06913676261901855
			 train-loss:  2.070297896022528 	 ± 0.2431810887498276
	data : 0.11698718070983886
	model : 0.06846790313720703
			 train-loss:  2.070952164792569 	 ± 0.2428000803375844
	data : 0.11764097213745117
	model : 0.06931724548339843
			 train-loss:  2.0711803824402564 	 ± 0.24225777603805548
	data : 0.11671504974365235
	model : 0.06899480819702149
			 train-loss:  2.0706486762673766 	 ± 0.24182205330995468
	data : 0.11686749458312988
	model : 0.06891684532165528
			 train-loss:  2.0718652207730552 	 ± 0.24192581058481272
	data : 0.11710920333862304
	model : 0.06801519393920899
			 train-loss:  2.071144194231121 	 ± 0.24160387852152954
	data : 0.11789951324462891
	model : 0.06908617019653321
			 train-loss:  2.0731966011056073 	 ± 0.24294894700099556
	data : 0.1169191837310791
	model : 0.06907796859741211
			 train-loss:  2.073565448956056 	 ± 0.24245761226585907
	data : 0.11709933280944824
	model : 0.0684056282043457
			 train-loss:  2.0734223256823165 	 ± 0.2419177580731824
	data : 0.11774125099182128
	model : 0.0682070255279541
			 train-loss:  2.073135116615811 	 ± 0.24141004374442815
	data : 0.11796841621398926
	model : 0.06904935836791992
			 train-loss:  2.0743874503892634 	 ± 0.24158981916134878
	data : 0.11714463233947754
	model : 0.06811528205871582
			 train-loss:  2.0749824808112214 	 ± 0.2412136718235771
	data : 0.11781511306762696
	model : 0.06738872528076172
			 train-loss:  2.0760685290230647 	 ± 0.2412253053108736
	data : 0.11832518577575683
	model : 0.06730647087097168
			 train-loss:  2.0753818170159266 	 ± 0.2409113438897223
	data : 0.1180800437927246
	model : 0.06714153289794922
			 train-loss:  2.0748641926811655 	 ± 0.24050603670369117
	data : 0.11815738677978516
	model : 0.06643452644348144
			 train-loss:  2.0744751341510237 	 ± 0.24004961123268892
	data : 0.1185868263244629
	model : 0.06697893142700195
			 train-loss:  2.0743062345221572 	 ± 0.23953848885852058
	data : 0.11808128356933593
	model : 0.06760234832763672
			 train-loss:  2.075200378894806 	 ± 0.23939987312463185
	data : 0.11737256050109864
	model : 0.06736712455749512
			 train-loss:  2.0753927401133945 	 ± 0.23889894247015458
	data : 0.11757402420043946
	model : 0.06748638153076172
			 train-loss:  2.07720303792378 	 ± 0.23996610142119812
	data : 0.11739039421081543
	model : 0.06787848472595215
			 train-loss:  2.077140903779877 	 ± 0.2394524692525353
	data : 0.11705317497253417
	model : 0.06759443283081054
			 train-loss:  2.0789941574773216 	 ± 0.24060902523192337
	data : 0.11721844673156738
	model : 0.066868257522583
			 train-loss:  2.079205284220107 	 ± 0.2401182657122405
	data : 0.118125581741333
	model : 0.06696958541870117
			 train-loss:  2.0800849426600894 	 ± 0.2399881578131279
	data : 0.1181396484375
	model : 0.06650652885437011
			 train-loss:  2.079540391511555 	 ± 0.23962738654927315
	data : 0.11855568885803222
	model : 0.0661341667175293
			 train-loss:  2.080818135197423 	 ± 0.23993113916143885
	data : 0.119044828414917
	model : 0.06659531593322754
			 train-loss:  2.0798204199539567 	 ± 0.23992290289082502
	data : 0.11888608932495118
	model : 0.06735906600952149
			 train-loss:  2.0786165565252306 	 ± 0.24014481699812512
	data : 0.11813936233520508
	model : 0.0681943416595459
			 train-loss:  2.079267875782187 	 ± 0.2398584009125909
	data : 0.11764249801635743
	model : 0.06897559165954589
			 train-loss:  2.08009034542998 	 ± 0.23970261305991733
	data : 0.11690177917480468
	model : 0.06948237419128418
			 train-loss:  2.0809377348472062 	 ± 0.2395718376463999
	data : 0.11643104553222657
	model : 0.06936125755310059
			 train-loss:  2.0835623057162174 	 ± 0.24255579937545613
	data : 0.116267728805542
	model : 0.0687103271484375
			 train-loss:  2.0857636675542714 	 ± 0.2444904954177639
	data : 0.116664457321167
	model : 0.06835088729858399
			 train-loss:  2.088512379948686 	 ± 0.2477573206866663
	data : 0.11662397384643555
	model : 0.06723933219909668
			 train-loss:  2.0885838130225056 	 ± 0.2472578173703158
	data : 0.11753721237182617
	model : 0.06658101081848145
			 train-loss:  2.088191942341866 	 ± 0.2468356545650929
	data : 0.11823315620422363
	model : 0.06612195968627929
			 train-loss:  2.0897924339914895 	 ± 0.24762556506895542
	data : 0.1189274787902832
	model : 0.06588220596313477
			 train-loss:  2.0892168407440184 	 ± 0.2472966687662081
	data : 0.11930785179138184
	model : 0.06597704887390136
			 train-loss:  2.088327496175272 	 ± 0.24720381842870548
	data : 0.11918668746948242
	model : 0.06648340225219726
			 train-loss:  2.087402368821795 	 ± 0.24714783013687913
	data : 0.11875267028808593
	model : 0.06623139381408691
			 train-loss:  2.08764753087236 	 ± 0.2466896131387635
	data : 0.11883931159973145
	model : 0.06670775413513183
			 train-loss:  2.088610918972436 	 ± 0.2466799325267398
	data : 0.11840314865112304
	model : 0.06775531768798829
			 train-loss:  2.091365559428346 	 ± 0.2500794293180622
	data : 0.11753954887390136
	model : 0.06740069389343262
			 train-loss:  2.0892689656466246 	 ± 0.25182599167248837
	data : 0.11705617904663086
	model : 0.05863852500915527
#epoch  21    val-loss:  2.4153302092301216  train-loss:  2.0892689656466246  lr:  0.00125
			 train-loss:  2.014167070388794 	 ± 0.0
	data : 5.60491156578064
	model : 0.07209992408752441
			 train-loss:  2.010591983795166 	 ± 0.0035750865936279297
	data : 2.88215434551239
	model : 0.06922376155853271
			 train-loss:  2.0410492420196533 	 ± 0.043171865682882234
	data : 1.9610975583394368
	model : 0.06803194681803386
			 train-loss:  2.0122804045677185 	 ± 0.06229603129343923
	data : 1.5007407069206238
	model : 0.06744736433029175
			 train-loss:  1.97928626537323 	 ± 0.0863660193401992
	data : 1.2245543956756593
	model : 0.0678551197052002
			 train-loss:  1.9458731810251872 	 ± 0.10861896139935008
	data : 0.12696990966796876
	model : 0.0673591136932373
			 train-loss:  1.9524146999631609 	 ± 0.10183019085959269
	data : 0.11840877532958985
	model : 0.06852831840515136
			 train-loss:  2.0234103053808212 	 ± 0.21060827671183593
	data : 0.11732273101806641
	model : 0.0695462703704834
			 train-loss:  2.0316190322240195 	 ± 0.1999161935234205
	data : 0.11641473770141601
	model : 0.07062063217163086
			 train-loss:  2.068416106700897 	 ± 0.2194448864752068
	data : 0.11556811332702636
	model : 0.07110495567321777
			 train-loss:  2.061447847973217 	 ± 0.21038964096251866
	data : 0.11529054641723632
	model : 0.07054128646850585
			 train-loss:  2.0883897244930267 	 ± 0.22036257112647709
	data : 0.1157599925994873
	model : 0.07014284133911133
			 train-loss:  2.086597176698538 	 ± 0.2118085522618811
	data : 0.11606993675231933
	model : 0.07006216049194336
			 train-loss:  2.064665104661669 	 ± 0.21888713484411101
	data : 0.11603970527648926
	model : 0.06966261863708496
			 train-loss:  2.0658923387527466 	 ± 0.21151491215226412
	data : 0.11624617576599121
	model : 0.0692251205444336
			 train-loss:  2.0719234719872475 	 ± 0.20612621547304644
	data : 0.11645073890686035
	model : 0.06971540451049804
			 train-loss:  2.0949580599279964 	 ± 0.22017764979791676
	data : 0.11602544784545898
	model : 0.06970577239990235
			 train-loss:  2.087956408659617 	 ± 0.21591284659777565
	data : 0.11621241569519043
	model : 0.06967759132385254
			 train-loss:  2.0857825592944494 	 ± 0.21035641362002938
	data : 0.11644339561462402
	model : 0.06988692283630371
			 train-loss:  2.093939834833145 	 ± 0.20809039250671002
	data : 0.11626853942871093
	model : 0.069940185546875
			 train-loss:  2.119492604618981 	 ± 0.2330204086134187
	data : 0.11624464988708497
	model : 0.07009873390197754
			 train-loss:  2.1420551267537205 	 ± 0.25004162095220184
	data : 0.1160813331604004
	model : 0.07034516334533691
			 train-loss:  2.1516671336215474 	 ± 0.24866666623821518
	data : 0.11587815284729004
	model : 0.07021608352661132
			 train-loss:  2.1459739158550897 	 ± 0.24495742630154843
	data : 0.11598758697509766
	model : 0.07010045051574706
			 train-loss:  2.15018283367157 	 ± 0.24089237184420884
	data : 0.11622238159179688
	model : 0.07012829780578614
			 train-loss:  2.1540896938397336 	 ± 0.23702074576200832
	data : 0.11625447273254394
	model : 0.06993117332458496
			 train-loss:  2.1510428455140858 	 ± 0.2331083477090313
	data : 0.11632037162780762
	model : 0.06985516548156738
			 train-loss:  2.1436594894954135 	 ± 0.23210058557541152
	data : 0.11635398864746094
	model : 0.06994085311889649
			 train-loss:  2.1357481685178032 	 ± 0.23187402634936546
	data : 0.11609883308410644
	model : 0.06984329223632812
			 train-loss:  2.133902140458425 	 ± 0.2281933508976581
	data : 0.11626386642456055
	model : 0.06917905807495117
			 train-loss:  2.1264536380767822 	 ± 0.2281597301556263
	data : 0.11673226356506347
	model : 0.06897616386413574
			 train-loss:  2.120238348841667 	 ± 0.22721709869907886
	data : 0.11697511672973633
	model : 0.06874756813049317
			 train-loss:  2.1081222584753325 	 ± 0.23401009716931212
	data : 0.11707673072814942
	model : 0.06878437995910644
			 train-loss:  2.0999046879656174 	 ± 0.23532648671388812
	data : 0.11709885597229004
	model : 0.06917462348937989
			 train-loss:  2.092545509338379 	 ± 0.23587637917530987
	data : 0.11663155555725098
	model : 0.06894054412841796
			 train-loss:  2.0974904563691883 	 ± 0.23440992526644727
	data : 0.11697893142700196
	model : 0.06844329833984375
			 train-loss:  2.091668908660476 	 ± 0.23384393656318725
	data : 0.11742439270019531
	model : 0.06841216087341309
			 train-loss:  2.105109503394679 	 ± 0.24480195141046798
	data : 0.11767621040344238
	model : 0.06824126243591308
			 train-loss:  2.1057810538854356 	 ± 0.2416785414340298
	data : 0.11796011924743652
	model : 0.06718850135803223
			 train-loss:  2.0970732390880586 	 ± 0.24475604171952392
	data : 0.11882228851318359
	model : 0.06795167922973633
			 train-loss:  2.0970980714007124 	 ± 0.2417528373795771
	data : 0.11816754341125488
	model : 0.06857113838195801
			 train-loss:  2.081566245782943 	 ± 0.2587346944593985
	data : 0.11747922897338867
	model : 0.06853961944580078
			 train-loss:  2.0829703946446263 	 ± 0.2558703223502848
	data : 0.11730937957763672
	model : 0.0686678409576416
			 train-loss:  2.0838711993260817 	 ± 0.2530149565114104
	data : 0.11711974143981933
	model : 0.06949987411499023
			 train-loss:  2.0805392848120796 	 ± 0.25116219979144
	data : 0.11637253761291504
	model : 0.06870861053466797
			 train-loss:  2.0818152272182964 	 ± 0.24856458776005622
	data : 0.11709976196289062
	model : 0.06890525817871093
			 train-loss:  2.0809058585065476 	 ± 0.245983400508383
	data : 0.1170809268951416
	model : 0.06889047622680664
			 train-loss:  2.08732437590758 	 ± 0.24735304657351984
	data : 0.11715817451477051
	model : 0.06887421607971192
			 train-loss:  2.088164592275814 	 ± 0.2448852229865142
	data : 0.11710915565490723
	model : 0.06889996528625489
			 train-loss:  2.0840410470962523 	 ± 0.24413638525618855
	data : 0.11715269088745117
	model : 0.06983656883239746
			 train-loss:  2.0847675660077263 	 ± 0.24178562438437987
	data : 0.11620101928710938
	model : 0.07035288810729981
			 train-loss:  2.0822278673832235 	 ± 0.24013538967104353
	data : 0.1156395435333252
	model : 0.07054657936096191
			 train-loss:  2.0737068023321763 	 ± 0.2456677247474507
	data : 0.11549496650695801
	model : 0.07074103355407715
			 train-loss:  2.082677059703403 	 ± 0.25199139287943406
	data : 0.11537127494812012
	model : 0.07073187828063965
			 train-loss:  2.0883851094679398 	 ± 0.2531887472002939
	data : 0.1153679370880127
	model : 0.07063083648681641
			 train-loss:  2.0917032531329562 	 ± 0.25212174044647667
	data : 0.1154634952545166
	model : 0.0694551944732666
			 train-loss:  2.089553559035586 	 ± 0.2504176039830953
	data : 0.11657676696777344
	model : 0.06849322319030762
			 train-loss:  2.083118964885843 	 ± 0.2529581294272269
	data : 0.1174893856048584
	model : 0.06836042404174805
			 train-loss:  2.079002685466055 	 ± 0.25275682753407896
	data : 0.11772689819335938
	model : 0.06846776008605956
			 train-loss:  2.0788629631201427 	 ± 0.2506439681800532
	data : 0.11773347854614258
	model : 0.06868128776550293
			 train-loss:  2.0733441583445815 	 ± 0.25222996603133846
	data : 0.11752786636352539
	model : 0.06956634521484376
			 train-loss:  2.0726005973354464 	 ± 0.25025497658753143
	data : 0.11698055267333984
	model : 0.07032413482666015
			 train-loss:  2.066859256653559 	 ± 0.2523433587679283
	data : 0.11611423492431641
	model : 0.07011795043945312
			 train-loss:  2.0631379056721926 	 ± 0.25210051064771594
	data : 0.11624937057495117
	model : 0.07003154754638671
			 train-loss:  2.0676177116540764 	 ± 0.2527079296314115
	data : 0.116249418258667
	model : 0.06987929344177246
			 train-loss:  2.070759197076162 	 ± 0.25206186298532984
	data : 0.1163264274597168
	model : 0.06949524879455567
			 train-loss:  2.0721435884931196 	 ± 0.2504264124945762
	data : 0.11644473075866699
	model : 0.06949701309204101
			 train-loss:  2.073984708856134 	 ± 0.2490346238753236
	data : 0.1165135383605957
	model : 0.06956686973571777
			 train-loss:  2.08042123525039 	 ± 0.2528568608077125
	data : 0.11646270751953125
	model : 0.06929564476013184
			 train-loss:  2.082317224570683 	 ± 0.2515377738644409
	data : 0.11667976379394532
	model : 0.06928215026855469
			 train-loss:  2.07910912137636 	 ± 0.2511982128085105
	data : 0.11658301353454589
	model : 0.06955475807189941
			 train-loss:  2.082392798529731 	 ± 0.2509775004474684
	data : 0.1164583683013916
	model : 0.06884994506835937
			 train-loss:  2.086026266829608 	 ± 0.25115211260393916
	data : 0.1170985221862793
	model : 0.06908941268920898
			 train-loss:  2.0841227850398503 	 ± 0.2499789657597408
	data : 0.11689457893371583
	model : 0.06931600570678711
			 train-loss:  2.081907434463501 	 ± 0.24903707715193577
	data : 0.1167799949645996
	model : 0.06865301132202148
			 train-loss:  2.089523889516529 	 ± 0.2560355429660212
	data : 0.11751298904418946
	model : 0.06850905418395996
			 train-loss:  2.0840670412236992 	 ± 0.2587777283980418
	data : 0.11730694770812988
	model : 0.06929244995117187
			 train-loss:  2.084824005762736 	 ± 0.2571993312811704
	data : 0.11670928001403809
	model : 0.06913981437683106
			 train-loss:  2.080555036098142 	 ± 0.258332373616352
	data : 0.11681227684020996
	model : 0.06900944709777831
			 train-loss:  2.081407256424427 	 ± 0.25682444616275985
	data : 0.1169619083404541
	model : 0.06968498229980469
			 train-loss:  2.079643964767456 	 ± 0.25572099213584826
	data : 0.11628923416137696
	model : 0.069035005569458
			 train-loss:  2.075539679062076 	 ± 0.2568271899713102
	data : 0.1170875072479248
	model : 0.06903986930847168
			 train-loss:  2.0736902751118302 	 ± 0.255824096646522
	data : 0.11711344718933106
	model : 0.0689734935760498
			 train-loss:  2.071035162323997 	 ± 0.25544464867818295
	data : 0.11720590591430664
	model : 0.06922492980957032
			 train-loss:  2.0728375533047845 	 ± 0.25447432450646723
	data : 0.11708106994628906
	model : 0.06863150596618653
			 train-loss:  2.0705913651821226 	 ± 0.25383665424559004
	data : 0.11770663261413575
	model : 0.06912436485290527
			 train-loss:  2.0711354126875428 	 ± 0.25242403261912416
	data : 0.11722478866577149
	model : 0.06822500228881836
			 train-loss:  2.068888624960726 	 ± 0.2518590996552666
	data : 0.11791515350341797
	model : 0.06738390922546386
			 train-loss:  2.070159265164579 	 ± 0.2507236606469574
	data : 0.11859393119812012
	model : 0.0672412395477295
			 train-loss:  2.071745550632477 	 ± 0.24977556775882523
	data : 0.1186408519744873
	model : 0.06783638000488282
			 train-loss:  2.0737056666678124 	 ± 0.24909443709480183
	data : 0.11806726455688477
	model : 0.06795434951782227
			 train-loss:  2.072466246459795 	 ± 0.2480189391410052
	data : 0.11801848411560059
	model : 0.06796479225158691
			 train-loss:  2.0719812095806165 	 ± 0.2467257664248111
	data : 0.11798639297485351
	model : 0.06801562309265137
			 train-loss:  2.0710544725681874 	 ± 0.24557256472684766
	data : 0.11797547340393066
	model : 0.06807026863098145
			 train-loss:  2.0718989861638923 	 ± 0.24441384343746703
	data : 0.11795578002929688
	model : 0.0683445930480957
			 train-loss:  2.0673034650584063 	 ± 0.24722892642444683
	data : 0.11752495765686036
	model : 0.06802563667297364
			 train-loss:  2.066285404962363 	 ± 0.24615343941447798
	data : 0.11799836158752441
	model : 0.06903219223022461
			 train-loss:  2.0644494051835975 	 ± 0.24556101440571707
	data : 0.11720504760742187
	model : 0.06931123733520508
			 train-loss:  2.064907329251068 	 ± 0.24435971190802797
	data : 0.11702885627746581
	model : 0.06916542053222656
			 train-loss:  2.0667986488342285 	 ± 0.243862018368043
	data : 0.11704325675964355
	model : 0.0688262939453125
			 train-loss:  2.0698962069974085 	 ± 0.2446208736155889
	data : 0.1173703670501709
	model : 0.06902065277099609
			 train-loss:  2.071979922406814 	 ± 0.24431790798296893
	data : 0.11696882247924804
	model : 0.06888866424560547
			 train-loss:  2.0702396075702407 	 ± 0.24376349300534786
	data : 0.11706266403198243
	model : 0.06926188468933106
			 train-loss:  2.0696070182781954 	 ± 0.24267366078405259
	data : 0.11660475730895996
	model : 0.06913666725158692
			 train-loss:  2.072482893580482 	 ± 0.2432895233879187
	data : 0.11674280166625976
	model : 0.06912260055541992
			 train-loss:  2.073446864227079 	 ± 0.2423406034121434
	data : 0.11678552627563477
	model : 0.07023234367370605
			 train-loss:  2.072709256243483 	 ± 0.24132503009165662
	data : 0.11582608222961426
	model : 0.06946377754211426
			 train-loss:  2.0739159528855926 	 ± 0.24052928310781832
	data : 0.11653404235839844
	model : 0.06974225044250489
			 train-loss:  2.0728447087314152 	 ± 0.23968207947900963
	data : 0.11638555526733399
	model : 0.06924805641174317
			 train-loss:  2.074428757754239 	 ± 0.23916260841292444
	data : 0.11683931350708007
	model : 0.06970748901367188
			 train-loss:  2.0775869021544584 	 ± 0.2403759046685308
	data : 0.11648340225219726
	model : 0.06884369850158692
			 train-loss:  2.0803695214646205 	 ± 0.24108950048536296
	data : 0.11729655265808106
	model : 0.0694389820098877
			 train-loss:  2.087519312326887 	 ± 0.2516647756634294
	data : 0.1166727066040039
	model : 0.06943917274475098
			 train-loss:  2.089041787281371 	 ± 0.25108069186093623
	data : 0.1165884017944336
	model : 0.07011265754699707
			 train-loss:  2.087514690730883 	 ± 0.2505178186682907
	data : 0.11601877212524414
	model : 0.06939134597778321
			 train-loss:  2.088524512175856 	 ± 0.2496706232292193
	data : 0.11677560806274415
	model : 0.06896309852600098
			 train-loss:  2.0935492067255526 	 ± 0.25442357428532136
	data : 0.11708660125732422
	model : 0.06888856887817382
			 train-loss:  2.091930296461461 	 ± 0.2539476839579301
	data : 0.11708173751831055
	model : 0.06792302131652832
			 train-loss:  2.0901148199033335 	 ± 0.25364625158308063
	data : 0.11792511940002441
	model : 0.06806211471557617
			 train-loss:  2.089020778735479 	 ± 0.2528689745779843
	data : 0.1177983283996582
	model : 0.0681847095489502
			 train-loss:  2.090184117151686 	 ± 0.2521441429801745
	data : 0.11753296852111816
	model : 0.06865601539611817
			 train-loss:  2.090179861569014 	 ± 0.25110864340784406
	data : 0.11719679832458496
	model : 0.06814727783203126
			 train-loss:  2.090356474000264 	 ± 0.2500934014677524
	data : 0.11767206192016602
	model : 0.06844735145568848
			 train-loss:  2.0873266804602837 	 ± 0.25133920769458984
	data : 0.11750636100769044
	model : 0.06821684837341309
			 train-loss:  2.087861614227295 	 ± 0.25040269419217276
	data : 0.11774473190307617
	model : 0.06747965812683106
			 train-loss:  2.0877635592506048 	 ± 0.2494094626916606
	data : 0.11844058036804199
	model : 0.06747221946716309
			 train-loss:  2.0872816044514573 	 ± 0.24848449374954018
	data : 0.11845936775207519
	model : 0.06826601028442383
			 train-loss:  2.086672941222787 	 ± 0.24760697525396722
	data : 0.11786713600158691
	model : 0.06795277595520019
			 train-loss:  2.093745172485825 	 ± 0.259299161612519
	data : 0.11800789833068848
	model : 0.06787605285644531
			 train-loss:  2.0941378006568323 	 ± 0.25833842344073876
	data : 0.11793413162231445
	model : 0.0687483310699463
			 train-loss:  2.0926430716769384 	 ± 0.2579141967816324
	data : 0.11713323593139649
	model : 0.06861019134521484
			 train-loss:  2.0906040171782174 	 ± 0.2579931392433438
	data : 0.1171290397644043
	model : 0.0678494930267334
			 train-loss:  2.089302114077977 	 ± 0.2574562854797401
	data : 0.11770758628845215
	model : 0.06855506896972656
			 train-loss:  2.0906756573648595 	 ± 0.2569824989746843
	data : 0.11711864471435547
	model : 0.06861724853515624
			 train-loss:  2.087951280452587 	 ± 0.2579639477131309
	data : 0.11711759567260742
	model : 0.06884727478027344
			 train-loss:  2.0868420057437 	 ± 0.25733676403033223
	data : 0.11709957122802735
	model : 0.06904382705688476
			 train-loss:  2.0864053137981107 	 ± 0.25644643008905665
	data : 0.11688690185546875
	model : 0.06987471580505371
			 train-loss:  2.087766040926394 	 ± 0.2560114868901048
	data : 0.11605467796325683
	model : 0.07009687423706054
			 train-loss:  2.086114545520261 	 ± 0.2558256113243475
	data : 0.11580886840820312
	model : 0.07015538215637207
			 train-loss:  2.088163345200675 	 ± 0.25605220231228526
	data : 0.11559443473815918
	model : 0.06958837509155273
			 train-loss:  2.085002199977848 	 ± 0.2578696257468944
	data : 0.11601381301879883
	model : 0.06956496238708496
			 train-loss:  2.0889647619825014 	 ± 0.26123251046038126
	data : 0.11620006561279297
	model : 0.06943731307983399
			 train-loss:  2.087857280577813 	 ± 0.2606518173892153
	data : 0.11624565124511718
	model : 0.06941366195678711
			 train-loss:  2.0880244283212557 	 ± 0.259752890099467
	data : 0.11626906394958496
	model : 0.06918463706970215
			 train-loss:  2.0888709652012794 	 ± 0.25905489136441323
	data : 0.11645612716674805
	model : 0.07003273963928222
			 train-loss:  2.086702582770831 	 ± 0.2594832480265457
	data : 0.1157198429107666
	model : 0.06910471916198731
			 train-loss:  2.0885022375859372 	 ± 0.25951180473855123
	data : 0.11666054725646972
	model : 0.06923980712890625
			 train-loss:  2.087084639716793 	 ± 0.25920405575302463
	data : 0.11653542518615723
	model : 0.06861481666564942
			 train-loss:  2.0863898496499798 	 ± 0.2584710221724263
	data : 0.11710247993469239
	model : 0.06889052391052246
			 train-loss:  2.0859157427152 	 ± 0.25767300855534303
	data : 0.11699328422546387
	model : 0.06840238571166993
			 train-loss:  2.0858861429012374 	 ± 0.25681862522290505
	data : 0.11735010147094727
	model : 0.06843504905700684
			 train-loss:  2.088364001167448 	 ± 0.2577770228376632
	data : 0.11725239753723145
	model : 0.0683445930480957
			 train-loss:  2.0906481189665453 	 ± 0.25847185506788467
	data : 0.11758880615234375
	model : 0.068764066696167
			 train-loss:  2.088004912648882 	 ± 0.25969756293414686
	data : 0.11723814010620118
	model : 0.06926493644714356
			 train-loss:  2.089074739333122 	 ± 0.2591987011653792
	data : 0.11682167053222656
	model : 0.06939058303833008
			 train-loss:  2.088291049003601 	 ± 0.2585507622600559
	data : 0.11659126281738282
	model : 0.07040157318115234
			 train-loss:  2.088976942050229 	 ± 0.2578683775662168
	data : 0.11569561958312988
	model : 0.0703824520111084
			 train-loss:  2.0891280958924114 	 ± 0.25705802031890806
	data : 0.11576719284057617
	model : 0.07061767578125
			 train-loss:  2.088451198062057 	 ± 0.25638960555594503
	data : 0.11546492576599121
	model : 0.07019624710083008
			 train-loss:  2.0874330945312978 	 ± 0.255909341728173
	data : 0.11568799018859863
	model : 0.07015032768249511
			 train-loss:  2.085169804022179 	 ± 0.2567146683873548
	data : 0.11578059196472168
	model : 0.06947140693664551
			 train-loss:  2.0832464511011852 	 ± 0.2570820900362231
	data : 0.11647238731384277
	model : 0.06968808174133301
			 train-loss:  2.084368870302212 	 ± 0.25669013450903694
	data : 0.11629996299743653
	model : 0.0695612907409668
			 train-loss:  2.083416264231612 	 ± 0.2561951865957152
	data : 0.11635966300964355
	model : 0.06949615478515625
			 train-loss:  2.0849154168909245 	 ± 0.2561381718552312
	data : 0.11653108596801758
	model : 0.06871399879455567
			 train-loss:  2.0854475613099983 	 ± 0.25545697472304113
	data : 0.11729702949523926
	model : 0.06967053413391114
			 train-loss:  2.0882254332125543 	 ± 0.2571934075323925
	data : 0.11635761260986328
	model : 0.06948332786560059
			 train-loss:  2.090679461047763 	 ± 0.25838038682891545
	data : 0.11640567779541015
	model : 0.06962494850158692
			 train-loss:  2.0905638996666 	 ± 0.25761916804876167
	data : 0.11631245613098144
	model : 0.06977686882019044
			 train-loss:  2.092831150223227 	 ± 0.25854587548137137
	data : 0.1161656379699707
	model : 0.07032942771911621
			 train-loss:  2.091707144564355 	 ± 0.25820502262224426
	data : 0.1156221866607666
	model : 0.06998395919799805
			 train-loss:  2.0900514673355013 	 ± 0.2583621006781615
	data : 0.11588912010192871
	model : 0.06989636421203613
			 train-loss:  2.089390462533587 	 ± 0.2577601264435628
	data : 0.1159928321838379
	model : 0.06982169151306153
			 train-loss:  2.0901875879572724 	 ± 0.2572321282914578
	data : 0.11626038551330567
	model : 0.06899890899658204
			 train-loss:  2.090575851712908 	 ± 0.25654725324547245
	data : 0.11687355041503907
	model : 0.0691068172454834
			 train-loss:  2.0893186642365023 	 ± 0.25635742014180407
	data : 0.1167975902557373
	model : 0.06907768249511718
			 train-loss:  2.088621328106034 	 ± 0.2557995645641256
	data : 0.11668529510498046
	model : 0.06913304328918457
			 train-loss:  2.087932531753283 	 ± 0.25524456843811594
	data : 0.11665568351745606
	model : 0.06931471824645996
			 train-loss:  2.0856981523876086 	 ± 0.25627032742035005
	data : 0.11631832122802735
	model : 0.06916317939758301
			 train-loss:  2.083275196949641 	 ± 0.25760527969485797
	data : 0.11653423309326172
	model : 0.06867198944091797
			 train-loss:  2.084376924604342 	 ± 0.2573175709668937
	data : 0.11703934669494628
	model : 0.0685610294342041
			 train-loss:  2.0855488875410058 	 ± 0.25709362357734555
	data : 0.11711974143981933
	model : 0.06824092864990235
			 train-loss:  2.0871268067855 	 ± 0.25727241142075635
	data : 0.11742353439331055
	model : 0.06807184219360352
			 train-loss:  2.0875029933193456 	 ± 0.2566228123675917
	data : 0.11762371063232421
	model : 0.06902065277099609
			 train-loss:  2.087682880582036 	 ± 0.25593992955649275
	data : 0.11678118705749511
	model : 0.06970524787902832
			 train-loss:  2.0874974375130027 	 ± 0.255263453671991
	data : 0.11615095138549805
	model : 0.06973276138305665
			 train-loss:  2.0848327632893855 	 ± 0.25716079702823313
	data : 0.11622357368469238
	model : 0.07009963989257813
			 train-loss:  2.0852981558505523 	 ± 0.25655489406907295
	data : 0.11597819328308105
	model : 0.06993842124938965
			 train-loss:  2.0877303036432417 	 ± 0.25803922565430976
	data : 0.11612763404846191
	model : 0.06895289421081544
			 train-loss:  2.0872538924217223 	 ± 0.2574426061934568
	data : 0.11684932708740234
	model : 0.06863751411437988
			 train-loss:  2.085151558147051 	 ± 0.2583978731810165
	data : 0.11711549758911133
	model : 0.06847553253173828
			 train-loss:  2.085472651446859 	 ± 0.2577622849236778
	data : 0.1172250747680664
	model : 0.06849770545959473
			 train-loss:  2.085579050019615 	 ± 0.2570978668906027
	data : 0.11712923049926757
	model : 0.06866211891174316
			 train-loss:  2.085791815801994 	 ± 0.2564514223421478
	data : 0.11704015731811523
	model : 0.0694117546081543
			 train-loss:  2.0846977600684533 	 ± 0.2562465092326152
	data : 0.11622424125671386
	model : 0.06949815750122071
			 train-loss:  2.0864651239648158 	 ± 0.25678076066396477
	data : 0.11610445976257325
	model : 0.0695566177368164
			 train-loss:  2.0862096856693326 	 ± 0.2561531679808867
	data : 0.11614818572998047
	model : 0.06858763694763184
			 train-loss:  2.086586422390408 	 ± 0.2555602075796608
	data : 0.1169044017791748
	model : 0.06805644035339356
			 train-loss:  2.086460835969628 	 ± 0.2549234129404217
	data : 0.1175358772277832
	model : 0.06806974411010742
			 train-loss:  2.0856067442893984 	 ± 0.2545705831000826
	data : 0.11789336204528808
	model : 0.06824216842651368
			 train-loss:  2.083630978171505 	 ± 0.2554691631403161
	data : 0.11769838333129883
	model : 0.06845173835754395
			 train-loss:  2.0844367305831155 	 ± 0.2550919410550988
	data : 0.11754240989685058
	model : 0.06930456161499024
			 train-loss:  2.0845018595897504 	 ± 0.2544645437162746
	data : 0.11676144599914551
	model : 0.06985697746276856
			 train-loss:  2.0853529852979324 	 ± 0.2541295879725974
	data : 0.11630477905273437
	model : 0.06980228424072266
			 train-loss:  2.086866367154005 	 ± 0.2544288511702095
	data : 0.11622033119201661
	model : 0.06942958831787109
			 train-loss:  2.0847827572267033 	 ± 0.2555578028766742
	data : 0.11655397415161133
	model : 0.06928634643554688
			 train-loss:  2.083755466673109 	 ± 0.25536577982945613
	data : 0.11665511131286621
	model : 0.06941757202148438
			 train-loss:  2.083113881257864 	 ± 0.2549183626188499
	data : 0.11655831336975098
	model : 0.0687530517578125
			 train-loss:  2.0810710419878435 	 ± 0.256008731046815
	data : 0.11717786788940429
	model : 0.06827783584594727
			 train-loss:  2.080530992008391 	 ± 0.25551776567613005
	data : 0.11777582168579101
	model : 0.06863999366760254
			 train-loss:  2.0796573574509103 	 ± 0.25522574384260244
	data : 0.11763811111450195
	model : 0.06837873458862305
			 train-loss:  2.0796935845096156 	 ± 0.25462362858708604
	data : 0.1177170753479004
	model : 0.06826248168945312
			 train-loss:  2.079992390014756 	 ± 0.2540624714777694
	data : 0.11784486770629883
	model : 0.0682229995727539
			 train-loss:  2.0804627693702127 	 ± 0.25356112112874296
	data : 0.11785397529602051
	model : 0.0688971996307373
			 train-loss:  2.0803389421729155 	 ± 0.25297724228060087
	data : 0.11716670989990234
	model : 0.06893134117126465
			 train-loss:  2.0833214902215533 	 ± 0.25615181616382365
	data : 0.11698684692382813
	model : 0.06839408874511718
			 train-loss:  2.084933358403395 	 ± 0.2566565391933868
	data : 0.11754565238952637
	model : 0.06838469505310059
			 train-loss:  2.0830480412605707 	 ± 0.2575688657715437
	data : 0.11754117012023926
	model : 0.06885528564453125
			 train-loss:  2.0831479128092933 	 ± 0.2569843668140978
	data : 0.1170919418334961
	model : 0.06885075569152832
			 train-loss:  2.082539640231566 	 ± 0.2565576106921733
	data : 0.1172555923461914
	model : 0.06813292503356934
			 train-loss:  2.080485472851749 	 ± 0.25778340553247636
	data : 0.11776728630065918
	model : 0.06846385002136231
			 train-loss:  2.081794180848577 	 ± 0.25793693013561414
	data : 0.11750206947326661
	model : 0.06844024658203125
			 train-loss:  2.082160561074056 	 ± 0.25741583600490575
	data : 0.11755285263061524
	model : 0.06807565689086914
			 train-loss:  2.0812846491379395 	 ± 0.2571734560474761
	data : 0.1176994800567627
	model : 0.06748933792114258
			 train-loss:  2.0819732586542767 	 ± 0.25680820860449466
	data : 0.11805214881896972
	model : 0.06802730560302735
			 train-loss:  2.0817619831161163 	 ± 0.25625901590785
	data : 0.11772041320800782
	model : 0.068485689163208
			 train-loss:  2.083467105936899 	 ± 0.2569756334500986
	data : 0.11731762886047363
	model : 0.06833186149597167
			 train-loss:  2.0834676137096 	 ± 0.25641147122958513
	data : 0.11736521720886231
	model : 0.06885509490966797
			 train-loss:  2.082389988753473 	 ± 0.25636791781844
	data : 0.11681942939758301
	model : 0.06895203590393066
			 train-loss:  2.0829467545384945 	 ± 0.25594870156820443
	data : 0.11676373481750488
	model : 0.06849131584167481
			 train-loss:  2.0847735136618346 	 ± 0.25689232596859496
	data : 0.11711831092834472
	model : 0.06800103187561035
			 train-loss:  2.085982618660762 	 ± 0.2569959506320589
	data : 0.11725597381591797
	model : 0.06790008544921874
			 train-loss:  2.0851927949635254 	 ± 0.2567258889415162
	data : 0.11705060005187988
	model : 0.06743450164794922
			 train-loss:  2.0844239679157224 	 ± 0.25644540997453663
	data : 0.11748294830322266
	model : 0.06693148612976074
			 train-loss:  2.0842813116438847 	 ± 0.2559085042000322
	data : 0.11777029037475586
	model : 0.06672773361206055
			 train-loss:  2.0872913023172797 	 ± 0.2595010142103625
	data : 0.11782636642456054
	model : 0.06641678810119629
			 train-loss:  2.088959604375976 	 ± 0.2602181407480971
	data : 0.11812500953674317
	model : 0.066436767578125
			 train-loss:  2.089624793589616 	 ± 0.2598727329415134
	data : 0.11856460571289062
	model : 0.0662600040435791
			 train-loss:  2.0886896723982677 	 ± 0.2597294524315514
	data : 0.1186396598815918
	model : 0.06663918495178223
			 train-loss:  2.089373990893364 	 ± 0.259403603321146
	data : 0.11824283599853516
	model : 0.0669630527496338
			 train-loss:  2.0900721589559343 	 ± 0.25909072179502285
	data : 0.11822586059570313
	model : 0.06768994331359864
			 train-loss:  2.0892890563681106 	 ± 0.25884050465595393
	data : 0.11778702735900878
	model : 0.06794624328613282
			 train-loss:  2.0883538276570324 	 ± 0.25871675478345135
	data : 0.11747016906738281
	model : 0.06853508949279785
			 train-loss:  2.0897028812619505 	 ± 0.25904108718353547
	data : 0.11723694801330567
	model : 0.06873278617858887
			 train-loss:  2.0900973071857374 	 ± 0.25858530022755555
	data : 0.11696300506591797
	model : 0.06906394958496094
			 train-loss:  2.0908996889261697 	 ± 0.2583646223159743
	data : 0.11648383140563964
	model : 0.06855649948120117
			 train-loss:  2.0913621331998695 	 ± 0.2579430832263341
	data : 0.1169203758239746
	model : 0.06807532310485839
			 train-loss:  2.0933028675856127 	 ± 0.259223190529285
	data : 0.11723546981811524
	model : 0.067924165725708
			 train-loss:  2.0927270004548224 	 ± 0.2588610416533703
	data : 0.11702876091003418
	model : 0.06738438606262206
			 train-loss:  2.0926841897964477 	 ± 0.25834368404450997
	data : 0.11774377822875977
	model : 0.06677136421203614
			 train-loss:  2.093180643134858 	 ± 0.2579480051048297
	data : 0.11826109886169434
	model : 0.06696968078613282
			 train-loss:  2.0943163131910656 	 ± 0.25806368070894115
	data : 0.11780714988708496
	model : 0.06719989776611328
			 train-loss:  2.093137879616658 	 ± 0.25823165651416136
	data : 0.11741151809692382
	model : 0.06692848205566407
			 train-loss:  2.0958136789442046 	 ± 0.26121353301426664
	data : 0.11776371002197265
	model : 0.0668067455291748
			 train-loss:  2.0947362614612954 	 ± 0.2612657302091574
	data : 0.11777544021606445
	model : 0.0666689395904541
			 train-loss:  2.09725638711825 	 ± 0.2638421033638946
	data : 0.11685705184936523
	model : 0.05760602951049805
#epoch  22    val-loss:  2.476220174839622  train-loss:  2.09725638711825  lr:  0.00125
			 train-loss:  1.888830542564392 	 ± 0.0
	data : 5.425327777862549
	model : 0.07237815856933594
			 train-loss:  2.0763216614723206 	 ± 0.18749111890792847
	data : 2.7819446325302124
	model : 0.07092952728271484
			 train-loss:  2.219518303871155 	 ± 0.2538618454359849
	data : 1.894025166829427
	model : 0.06937726338704427
			 train-loss:  2.2293265759944916 	 ± 0.22050619953248293
	data : 1.4502773880958557
	model : 0.06872457265853882
			 train-loss:  2.2150988817214965 	 ± 0.1992689046567743
	data : 1.1839167118072509
	model : 0.06897778511047363
			 train-loss:  2.184589405854543 	 ± 0.19427871960400928
	data : 0.12190022468566894
	model : 0.06829919815063476
			 train-loss:  2.1278917619160245 	 ± 0.22724419971518353
	data : 0.11751251220703125
	model : 0.0683260440826416
			 train-loss:  2.110026776790619 	 ± 0.21775913154420562
	data : 0.11717219352722168
	model : 0.06902947425842285
			 train-loss:  2.1080855793423123 	 ± 0.20537868243523327
	data : 0.11670885086059571
	model : 0.06948823928833008
			 train-loss:  2.0640639901161193 	 ± 0.2353794080648223
	data : 0.11635899543762207
	model : 0.06951417922973632
			 train-loss:  2.089206402952021 	 ± 0.23809283120105224
	data : 0.1164121150970459
	model : 0.06981873512268066
			 train-loss:  2.078594595193863 	 ± 0.23065752755202276
	data : 0.11623549461364746
	model : 0.07038092613220215
			 train-loss:  2.129693994155297 	 ± 0.2836267750427874
	data : 0.11562833786010743
	model : 0.07051992416381836
			 train-loss:  2.1258564250809804 	 ± 0.27365961915698717
	data : 0.11542320251464844
	model : 0.07062487602233887
			 train-loss:  2.1129618247350055 	 ± 0.26874660568366493
	data : 0.11538534164428711
	model : 0.07072100639343262
			 train-loss:  2.129849560558796 	 ± 0.26830696274257865
	data : 0.11532020568847656
	model : 0.07039923667907715
			 train-loss:  2.1367803110795864 	 ± 0.2617681531943094
	data : 0.11547470092773438
	model : 0.06990275382995606
			 train-loss:  2.1448583139313593 	 ± 0.2565639823012884
	data : 0.11602363586425782
	model : 0.07033810615539551
			 train-loss:  2.1290707211745414 	 ± 0.25854799323857386
	data : 0.11563620567321778
	model : 0.06959099769592285
			 train-loss:  2.1250391483306883 	 ± 0.2526133994459112
	data : 0.11645674705505371
	model : 0.0691917896270752
			 train-loss:  2.1276472068968273 	 ± 0.24680119421412106
	data : 0.11693363189697266
	model : 0.06934924125671386
			 train-loss:  2.140779874541543 	 ± 0.24852356336512557
	data : 0.11659784317016601
	model : 0.06911587715148926
			 train-loss:  2.1323405763377314 	 ± 0.24626297023099855
	data : 0.11683993339538574
	model : 0.06839995384216309
			 train-loss:  2.122348815202713 	 ± 0.24579415863521253
	data : 0.11751728057861328
	model : 0.07003617286682129
			 train-loss:  2.1349358558654785 	 ± 0.2485972322974072
	data : 0.11577844619750977
	model : 0.06940827369689942
			 train-loss:  2.136577404462374 	 ± 0.2439077805564775
	data : 0.11639566421508789
	model : 0.06891155242919922
			 train-loss:  2.124378712088973 	 ± 0.24729869658202167
	data : 0.11712160110473632
	model : 0.06831350326538085
			 train-loss:  2.1066230663231442 	 ± 0.25977794219996014
	data : 0.11765251159667969
	model : 0.06846156120300292
			 train-loss:  2.111269445254885 	 ± 0.25644105021932173
	data : 0.11733851432800294
	model : 0.06751046180725098
			 train-loss:  2.1155274907747903 	 ± 0.2531713703419887
	data : 0.11837835311889648
	model : 0.06813373565673828
			 train-loss:  2.1087056398391724 	 ± 0.2518417512274044
	data : 0.11774530410766601
	model : 0.06866426467895508
			 train-loss:  2.1219487972557545 	 ± 0.25860989359659503
	data : 0.11743288040161133
	model : 0.06983027458190919
			 train-loss:  2.1285411480701333 	 ± 0.2573774062022268
	data : 0.11634149551391601
	model : 0.0700418472290039
			 train-loss:  2.1463217980721416 	 ± 0.27336386350720576
	data : 0.11633944511413574
	model : 0.07002034187316894
			 train-loss:  2.1419903346470424 	 ± 0.2706115581083214
	data : 0.11616244316101074
	model : 0.0702092170715332
			 train-loss:  2.140950772497389 	 ± 0.26689746299358524
	data : 0.11607871055603028
	model : 0.07013335227966308
			 train-loss:  2.1505070441478007 	 ± 0.26943757978384203
	data : 0.11600208282470703
	model : 0.06972775459289551
			 train-loss:  2.1449970320651404 	 ± 0.26797294814382994
	data : 0.11622805595397949
	model : 0.0698178768157959
			 train-loss:  2.1475667219895582 	 ± 0.2649889758495519
	data : 0.11607947349548339
	model : 0.0693967342376709
			 train-loss:  2.150766372680664 	 ± 0.262417513146528
	data : 0.11649861335754394
	model : 0.06966233253479004
			 train-loss:  2.149317706503519 	 ± 0.2593594270080886
	data : 0.11618890762329101
	model : 0.06961936950683593
			 train-loss:  2.151009167943682 	 ± 0.25648199238200736
	data : 0.11631298065185547
	model : 0.0693099021911621
			 train-loss:  2.161836341369984 	 ± 0.2630147099747977
	data : 0.1165964126586914
	model : 0.06873588562011719
			 train-loss:  2.1604758988727224 	 ± 0.2601617259374894
	data : 0.11704540252685547
	model : 0.06943354606628419
			 train-loss:  2.1546768214967518 	 ± 0.26011482298480615
	data : 0.11649236679077149
	model : 0.06967768669128419
			 train-loss:  2.1509981025820193 	 ± 0.2584527837185428
	data : 0.11633486747741699
	model : 0.06982879638671875
			 train-loss:  2.1486465854847685 	 ± 0.25618542863680754
	data : 0.11624431610107422
	model : 0.07002673149108887
			 train-loss:  2.150433433552583 	 ± 0.2537985908422735
	data : 0.1161808967590332
	model : 0.0695406436920166
			 train-loss:  2.151463749457379 	 ± 0.251296862981813
	data : 0.11655011177062988
	model : 0.0694469928741455
			 train-loss:  2.149090278148651 	 ± 0.24932538180904165
	data : 0.1166506290435791
	model : 0.06898417472839355
			 train-loss:  2.1501387778450463 	 ± 0.24698021859842587
	data : 0.11700229644775391
	model : 0.0689300537109375
			 train-loss:  2.1402467970664683 	 ± 0.2545910088055494
	data : 0.11700625419616699
	model : 0.06917119026184082
			 train-loss:  2.146659066092293 	 ± 0.25638198353785807
	data : 0.11692452430725098
	model : 0.06898012161254882
			 train-loss:  2.1492553794825517 	 ± 0.2546992973526943
	data : 0.11706585884094238
	model : 0.06811566352844238
			 train-loss:  2.14727229421789 	 ± 0.25279360731889144
	data : 0.1176936149597168
	model : 0.06795248985290528
			 train-loss:  2.143002150314195 	 ± 0.25251996214453876
	data : 0.11781849861145019
	model : 0.06758909225463867
			 train-loss:  2.1415875606369554 	 ± 0.2505188282277699
	data : 0.11827459335327148
	model : 0.06716437339782715
			 train-loss:  2.1388189710419754 	 ± 0.24922786727649546
	data : 0.1186720848083496
	model : 0.06799154281616211
			 train-loss:  2.136299729347229 	 ± 0.2478504443240145
	data : 0.11800756454467773
	model : 0.06884818077087403
			 train-loss:  2.133251182238261 	 ± 0.2468893208872041
	data : 0.11749796867370606
	model : 0.06932368278503417
			 train-loss:  2.1319315433502197 	 ± 0.24507054525871944
	data : 0.11695680618286133
	model : 0.06985292434692383
			 train-loss:  2.1288198597969545 	 ± 0.24429798694097637
	data : 0.11632938385009765
	model : 0.07038464546203613
			 train-loss:  2.135379906684633 	 ± 0.2477948794165825
	data : 0.11584935188293458
	model : 0.06961750984191895
			 train-loss:  2.1377535220235586 	 ± 0.24657217285847416
	data : 0.11656465530395507
	model : 0.06956167221069336
			 train-loss:  2.1382862402842595 	 ± 0.24470522589062868
	data : 0.11640610694885253
	model : 0.06857028007507324
			 train-loss:  2.144107087091966 	 ± 0.24733724301132137
	data : 0.11736474037170411
	model : 0.0681389331817627
			 train-loss:  2.145456742884508 	 ± 0.24572925166819432
	data : 0.11770739555358886
	model : 0.06795597076416016
			 train-loss:  2.139535211464938 	 ± 0.2486849525331012
	data : 0.11777296066284179
	model : 0.06824717521667481
			 train-loss:  2.141365405442058 	 ± 0.24733719142287305
	data : 0.11761293411254883
	model : 0.0679901123046875
			 train-loss:  2.137352762903486 	 ± 0.2478159349106131
	data : 0.11767487525939942
	model : 0.06812787055969238
			 train-loss:  2.1322498237583 	 ± 0.2497410030708508
	data : 0.1174971580505371
	model : 0.06848602294921875
			 train-loss:  2.1346859849161572 	 ± 0.24884872368908617
	data : 0.11713528633117676
	model : 0.06865224838256836
			 train-loss:  2.133633458451049 	 ± 0.24729972231431818
	data : 0.11697707176208497
	model : 0.0692831039428711
			 train-loss:  2.1319588213353544 	 ± 0.2460394816673132
	data : 0.11649045944213868
	model : 0.0694993019104004
			 train-loss:  2.137661951382955 	 ± 0.2492693096868265
	data : 0.11649098396301269
	model : 0.06985239982604981
			 train-loss:  2.1358102575728766 	 ± 0.24814265700634552
	data : 0.11627130508422852
	model : 0.06970672607421875
			 train-loss:  2.1365064970858687 	 ± 0.2466007841443158
	data : 0.11634387969970703
	model : 0.06887998580932617
			 train-loss:  2.129548769730788 	 ± 0.2525071763014495
	data : 0.11705045700073242
	model : 0.06858692169189454
			 train-loss:  2.1254806971248192 	 ± 0.2534632632527898
	data : 0.11739082336425781
	model : 0.06875491142272949
			 train-loss:  2.121902050077915 	 ± 0.25387459691004427
	data : 0.11731882095336914
	model : 0.06896862983703614
			 train-loss:  2.123316257088273 	 ± 0.2526194805057983
	data : 0.11720099449157714
	model : 0.06901979446411133
			 train-loss:  2.1284495141448043 	 ± 0.25528949695716285
	data : 0.1171529769897461
	model : 0.06881270408630372
			 train-loss:  2.12302072651415 	 ± 0.2584650768607111
	data : 0.11730241775512695
	model : 0.06893539428710938
			 train-loss:  2.122712396440052 	 ± 0.2569373435423158
	data : 0.11715092658996581
	model : 0.06876535415649414
			 train-loss:  2.1222330317777747 	 ± 0.255459258374267
	data : 0.11718344688415527
	model : 0.06875486373901367
			 train-loss:  2.1194566654604534 	 ± 0.2552563415869278
	data : 0.11713790893554688
	model : 0.0686859130859375
			 train-loss:  2.118091199589872 	 ± 0.25410082531475353
	data : 0.11724910736083985
	model : 0.06962046623229981
			 train-loss:  2.1204395294189453 	 ± 0.25360064219009887
	data : 0.11648874282836914
	model : 0.06961326599121094
			 train-loss:  2.120097425546539 	 ± 0.25219231474533754
	data : 0.1164121150970459
	model : 0.06955914497375489
			 train-loss:  2.124118717511495 	 ± 0.2536404669410893
	data : 0.11647615432739258
	model : 0.06956052780151367
			 train-loss:  2.125340388371394 	 ± 0.2525091052861382
	data : 0.1164743423461914
	model : 0.06890606880187988
			 train-loss:  2.1224579733351003 	 ± 0.2526338305733865
	data : 0.11707196235656739
	model : 0.06900663375854492
			 train-loss:  2.12240324225477 	 ± 0.25127246158922534
	data : 0.11693282127380371
	model : 0.06844749450683593
			 train-loss:  2.121702120659199 	 ± 0.2500237725637775
	data : 0.11755204200744629
	model : 0.06798477172851562
			 train-loss:  2.1230489178707725 	 ± 0.24904692345421872
	data : 0.11804275512695313
	model : 0.06875839233398437
			 train-loss:  2.123681925237179 	 ± 0.24782322174207166
	data : 0.11727743148803711
	model : 0.06968059539794921
			 train-loss:  2.1248613976940667 	 ± 0.24681317205861877
	data : 0.11645522117614746
	model : 0.06956701278686524
			 train-loss:  2.1232763200390097 	 ± 0.24604644317326804
	data : 0.1166038990020752
	model : 0.0703165054321289
			 train-loss:  2.1261139770950934 	 ± 0.24640713017751098
	data : 0.11596112251281739
	model : 0.07097096443176269
			 train-loss:  2.130782459974289 	 ± 0.24953353997255465
	data : 0.1152876377105713
	model : 0.07018318176269531
			 train-loss:  2.1328809391153922 	 ± 0.2491803447592949
	data : 0.11594820022583008
	model : 0.06999845504760742
			 train-loss:  2.135528070085189 	 ± 0.24937892450455915
	data : 0.1161508560180664
	model : 0.06991477012634277
			 train-loss:  2.1323748456621634 	 ± 0.250200381953861
	data : 0.11625871658325196
	model : 0.06976938247680664
			 train-loss:  2.1341819935120068 	 ± 0.2496691441001579
	data : 0.11630024909973144
	model : 0.06873230934143067
			 train-loss:  2.1357846135184877 	 ± 0.24901431793027584
	data : 0.11713209152221679
	model : 0.06863417625427246
			 train-loss:  2.1317679049833766 	 ± 0.2512313926515421
	data : 0.11736054420471191
	model : 0.0683061122894287
			 train-loss:  2.1345531650792773 	 ± 0.2516935549125878
	data : 0.1174689769744873
	model : 0.06832704544067383
			 train-loss:  2.1338095797432794 	 ± 0.25064364572304826
	data : 0.11727948188781738
	model : 0.0684600830078125
			 train-loss:  2.1353037729175814 	 ± 0.2499740157937004
	data : 0.11698288917541504
	model : 0.06956839561462402
			 train-loss:  2.130713451992382 	 ± 0.25340814685628676
	data : 0.11627593040466308
	model : 0.069923734664917
			 train-loss:  2.1321559317477115 	 ± 0.2527173350529765
	data : 0.11581449508666992
	model : 0.07020277976989746
			 train-loss:  2.1285205824034557 	 ± 0.25448530160512145
	data : 0.1156691551208496
	model : 0.07013235092163086
			 train-loss:  2.123974660856534 	 ± 0.2578840281782885
	data : 0.11587338447570801
	model : 0.07094721794128418
			 train-loss:  2.1290763804787085 	 ± 0.26241553770765713
	data : 0.11508803367614746
	model : 0.07024188041687011
			 train-loss:  2.127062053265779 	 ± 0.26215581585051206
	data : 0.11561684608459473
	model : 0.0700216293334961
			 train-loss:  2.125757680884723 	 ± 0.26139791245709176
	data : 0.11594433784484863
	model : 0.07016115188598633
			 train-loss:  2.1231926027526202 	 ± 0.26174051617630384
	data : 0.11581168174743653
	model : 0.07032465934753418
			 train-loss:  2.121529959015927 	 ± 0.2612488347514731
	data : 0.1156097412109375
	model : 0.06959452629089355
			 train-loss:  2.1238346600732885 	 ± 0.26135070417931777
	data : 0.11627111434936524
	model : 0.07026109695434571
			 train-loss:  2.123705542087555 	 ± 0.26026327610701167
	data : 0.11572127342224121
	model : 0.07034811973571778
			 train-loss:  2.1223677189882135 	 ± 0.2595995679389026
	data : 0.11555957794189453
	model : 0.07048959732055664
			 train-loss:  2.120350018876498 	 ± 0.25948438781811656
	data : 0.11550273895263671
	model : 0.07040171623229981
			 train-loss:  2.1219819057278517 	 ± 0.25905525348987163
	data : 0.11578693389892578
	model : 0.0703465461730957
			 train-loss:  2.12311004823254 	 ± 0.25831175058535427
	data : 0.11595492362976074
	model : 0.06936502456665039
			 train-loss:  2.1204365844726563 	 ± 0.25899312627461285
	data : 0.11684656143188477
	model : 0.06917157173156738
			 train-loss:  2.120587797391982 	 ± 0.25796886826757587
	data : 0.11707205772399902
	model : 0.06831774711608887
			 train-loss:  2.117063575842249 	 ± 0.2599786045434054
	data : 0.1177483081817627
	model : 0.06807065010070801
			 train-loss:  2.1167214764282107 	 ± 0.2589897677533928
	data : 0.11789088249206543
	model : 0.06808238029479981
			 train-loss:  2.1158154463583183 	 ± 0.2581875425310812
	data : 0.11787371635437012
	model : 0.06814336776733398
			 train-loss:  2.114609676141005 	 ± 0.25755694988342986
	data : 0.11785392761230469
	model : 0.06805076599121093
			 train-loss:  2.1143412371628156 	 ± 0.25659027977390375
	data : 0.11797509193420411
	model : 0.06789584159851074
			 train-loss:  2.1163685087001687 	 ± 0.25666745475692365
	data : 0.11807937622070312
	model : 0.06793422698974609
			 train-loss:  2.1191987166727397 	 ± 0.2577599422687526
	data : 0.11797118186950684
	model : 0.06786236763000489
			 train-loss:  2.119675949438294 	 ± 0.256855321998697
	data : 0.11791768074035644
	model : 0.06873002052307128
			 train-loss:  2.1182153984352396 	 ± 0.2564601437210172
	data : 0.11699061393737793
	model : 0.06906633377075196
			 train-loss:  2.118638433077756 	 ± 0.25556280729699205
	data : 0.11681241989135742
	model : 0.06997079849243164
			 train-loss:  2.11755857502457 	 ± 0.2549396099223384
	data : 0.11604223251342774
	model : 0.07020697593688965
			 train-loss:  2.1164482149524964 	 ± 0.2543464948442526
	data : 0.11581010818481445
	model : 0.07035083770751953
			 train-loss:  2.113724311478704 	 ± 0.25544204838479323
	data : 0.11576619148254394
	model : 0.07040104866027833
			 train-loss:  2.113000259229115 	 ± 0.2546712293589223
	data : 0.11605329513549804
	model : 0.0701624870300293
			 train-loss:  2.112004573463548 	 ± 0.25403985489358044
	data : 0.11617155075073242
	model : 0.07027235031127929
			 train-loss:  2.1122373871400324 	 ± 0.25315886270648325
	data : 0.11596822738647461
	model : 0.06930608749389648
			 train-loss:  2.110960812001795 	 ± 0.25273037319647407
	data : 0.11685843467712402
	model : 0.06921305656433105
			 train-loss:  2.1093829911616115 	 ± 0.2525570879735786
	data : 0.11689081192016601
	model : 0.06914782524108887
			 train-loss:  2.107317222397903 	 ± 0.2529025330991763
	data : 0.11663613319396973
	model : 0.06909756660461426
			 train-loss:  2.106643503659392 	 ± 0.2521654738996578
	data : 0.11674337387084961
	model : 0.06902728080749512
			 train-loss:  2.1055040570343433 	 ± 0.2516831661810935
	data : 0.11683993339538574
	model : 0.0698962688446045
			 train-loss:  2.104388467363409 	 ± 0.25119586133798644
	data : 0.11599936485290527
	model : 0.06976547241210937
			 train-loss:  2.1056596640772467 	 ± 0.25082869531079033
	data : 0.11624727249145508
	model : 0.06978039741516114
			 train-loss:  2.1069786183039345 	 ± 0.25050909609486666
	data : 0.1163419246673584
	model : 0.06976823806762696
			 train-loss:  2.106953313019102 	 ± 0.2496784102201077
	data : 0.1160773754119873
	model : 0.06961684226989746
			 train-loss:  2.1082635804226526 	 ± 0.2493760581957043
	data : 0.11638960838317872
	model : 0.06945977210998536
			 train-loss:  2.109300678851558 	 ± 0.24888841931521655
	data : 0.11647448539733887
	model : 0.06883430480957031
			 train-loss:  2.110494026890049 	 ± 0.2485177786153269
	data : 0.11710429191589355
	model : 0.06905665397644042
			 train-loss:  2.1096509987308134 	 ± 0.2479356265730745
	data : 0.11690320968627929
	model : 0.07019224166870117
			 train-loss:  2.1105258151506767 	 ± 0.24737955704940584
	data : 0.11605095863342285
	model : 0.07021069526672363
			 train-loss:  2.1085489282182825 	 ± 0.24782356320536628
	data : 0.11586604118347169
	model : 0.07054505348205567
			 train-loss:  2.109451616866679 	 ± 0.2472968606442268
	data : 0.11579837799072265
	model : 0.07130398750305175
			 train-loss:  2.110207817089633 	 ± 0.24670115632637013
	data : 0.11504859924316406
	model : 0.07105779647827148
			 train-loss:  2.1077383056283 	 ± 0.24789258926963786
	data : 0.11520791053771973
	model : 0.06924419403076172
			 train-loss:  2.1070289308239953 	 ± 0.24728438738418287
	data : 0.1171445369720459
	model : 0.06830458641052246
			 train-loss:  2.1088597605257857 	 ± 0.24761212347992786
	data : 0.11817464828491211
	model : 0.0681269645690918
			 train-loss:  2.108210627286712 	 ± 0.24698963639505045
	data : 0.11813735961914062
	model : 0.06809906959533692
			 train-loss:  2.106624747194895 	 ± 0.24706649541983633
	data : 0.1180990219116211
	model : 0.06812558174133301
			 train-loss:  2.110069990158081 	 ± 0.2502369540699307
	data : 0.11815619468688965
	model : 0.06868858337402343
			 train-loss:  2.1102655695145387 	 ± 0.2494947385512503
	data : 0.11723003387451172
	model : 0.069736909866333
			 train-loss:  2.1099341900762685 	 ± 0.2487832654039092
	data : 0.11620035171508789
	model : 0.06969537734985351
			 train-loss:  2.109818553640729 	 ± 0.24804623529776929
	data : 0.11614251136779785
	model : 0.06961221694946289
			 train-loss:  2.1097225465718106 	 ± 0.2473144125565523
	data : 0.11618599891662598
	model : 0.0694505214691162
			 train-loss:  2.1070911919369415 	 ± 0.24894736102124992
	data : 0.11625938415527344
	model : 0.06974625587463379
			 train-loss:  2.1105272302850646 	 ± 0.2522289515805337
	data : 0.1162109375
	model : 0.06961874961853028
			 train-loss:  2.1087880799936696 	 ± 0.2525208458837007
	data : 0.11629457473754883
	model : 0.06985917091369628
			 train-loss:  2.1088124300014077 	 ± 0.2517901616393053
	data : 0.1163184642791748
	model : 0.0701059341430664
			 train-loss:  2.1090476321077896 	 ± 0.2510846429016513
	data : 0.11614198684692383
	model : 0.07033729553222656
			 train-loss:  2.1098607758113315 	 ± 0.2505958870540164
	data : 0.11582207679748535
	model : 0.07027201652526856
			 train-loss:  2.109815466133031 	 ± 0.24988367166100467
	data : 0.11586403846740723
	model : 0.0701791763305664
			 train-loss:  2.1139825441069524 	 ± 0.25523563502927193
	data : 0.11600222587585449
	model : 0.06912994384765625
			 train-loss:  2.1159707508730086 	 ± 0.2558884912644958
	data : 0.11681981086730957
	model : 0.06818809509277343
			 train-loss:  2.1164822645027543 	 ± 0.2552639593869766
	data : 0.11764521598815918
	model : 0.0676607608795166
			 train-loss:  2.117840590741899 	 ± 0.25520179206009574
	data : 0.11819725036621094
	model : 0.06743421554565429
			 train-loss:  2.116452365290394 	 ± 0.25517645326216887
	data : 0.11838226318359375
	model : 0.06754498481750489
			 train-loss:  2.114664761574714 	 ± 0.2556083681077001
	data : 0.11840004920959472
	model : 0.06851563453674317
			 train-loss:  2.1153340326632306 	 ± 0.25506888213387247
	data : 0.1175154209136963
	model : 0.06852006912231445
			 train-loss:  2.113695833346118 	 ± 0.25533833142768436
	data : 0.11754655838012695
	model : 0.06879549026489258
			 train-loss:  2.1150985943304526 	 ± 0.2553572155929002
	data : 0.1173253059387207
	model : 0.06938009262084961
			 train-loss:  2.1148188133393564 	 ± 0.2546982763046687
	data : 0.11681227684020996
	model : 0.06934828758239746
			 train-loss:  2.113774949216588 	 ± 0.25441498070059304
	data : 0.11669044494628907
	model : 0.06901278495788574
			 train-loss:  2.1140713672688665 	 ± 0.2537698178468507
	data : 0.11693024635314941
	model : 0.06983366012573242
			 train-loss:  2.112639892668951 	 ± 0.2538574774054624
	data : 0.11621370315551757
	model : 0.06942453384399414
			 train-loss:  2.111292793248829 	 ± 0.25386495586757096
	data : 0.11656503677368164
	model : 0.06839823722839355
			 train-loss:  2.112367593181071 	 ± 0.25363257247906157
	data : 0.11738615036010742
	model : 0.06823759078979492
			 train-loss:  2.1131524623682103 	 ± 0.25320365753533436
	data : 0.11772642135620118
	model : 0.0684877872467041
			 train-loss:  2.113889381057858 	 ± 0.25275318122804713
	data : 0.11758098602294922
	model : 0.06833710670471191
			 train-loss:  2.115358595381078 	 ± 0.2529258365267473
	data : 0.11780014038085937
	model : 0.06885075569152832
			 train-loss:  2.1152049718759 	 ± 0.25228554923416224
	data : 0.11722874641418457
	model : 0.06955575942993164
			 train-loss:  2.113378428682989 	 ± 0.252930493167288
	data : 0.11660299301147461
	model : 0.06991181373596192
			 train-loss:  2.1113254086015187 	 ± 0.25391969758764765
	data : 0.11637377738952637
	model : 0.0700157642364502
			 train-loss:  2.1119405133555635 	 ± 0.2534247741344989
	data : 0.11640830039978027
	model : 0.06994314193725586
			 train-loss:  2.109180854792571 	 ± 0.25575240671248267
	data : 0.1163900375366211
	model : 0.0696782112121582
			 train-loss:  2.111395199894905 	 ± 0.25701752584939613
	data : 0.11683926582336426
	model : 0.06966161727905273
			 train-loss:  2.10955181940278 	 ± 0.2576993832211678
	data : 0.11687521934509278
	model : 0.06951994895935058
			 train-loss:  2.1088181462618385 	 ± 0.2572710795186857
	data : 0.11683840751647949
	model : 0.06955337524414062
			 train-loss:  2.1071499433423497 	 ± 0.2577295153491272
	data : 0.11673645973205567
	model : 0.06987233161926269
			 train-loss:  2.107577227494296 	 ± 0.2571691171921901
	data : 0.11657924652099609
	model : 0.06959643363952636
			 train-loss:  2.1073582980690935 	 ± 0.25656016480074645
	data : 0.11669893264770508
	model : 0.06928563117980957
			 train-loss:  2.107757818930357 	 ± 0.25600060533342056
	data : 0.11712408065795898
	model : 0.06825871467590332
			 train-loss:  2.1079757760688302 	 ± 0.25540065680169516
	data : 0.11785683631896973
	model : 0.06815781593322753
			 train-loss:  2.1076504009274335 	 ± 0.25482897604161664
	data : 0.11795015335083008
	model : 0.06799101829528809
			 train-loss:  2.1060992627622976 	 ± 0.25520100700834597
	data : 0.11792478561401368
	model : 0.06860780715942383
			 train-loss:  2.1039424135571436 	 ± 0.2564950105827664
	data : 0.11739597320556641
	model : 0.06899061203002929
			 train-loss:  2.103900811118537 	 ± 0.25588719083462946
	data : 0.11719293594360351
	model : 0.07001142501831055
			 train-loss:  2.1019290168330356 	 ± 0.2568847153552162
	data : 0.11636266708374024
	model : 0.07008500099182129
			 train-loss:  2.1017728042154804 	 ± 0.2562910829943976
	data : 0.11638669967651367
	model : 0.07012419700622559
			 train-loss:  2.1030499077288902 	 ± 0.2563700080036175
	data : 0.11655950546264648
	model : 0.0695910930633545
			 train-loss:  2.1031996926596 	 ± 0.2557824892940547
	data : 0.11695513725280762
	model : 0.06963262557983399
			 train-loss:  2.1023691219312175 	 ± 0.2554801500057261
	data : 0.11667122840881347
	model : 0.06969842910766602
			 train-loss:  2.102946802218389 	 ± 0.255032165576365
	data : 0.11653332710266114
	model : 0.06953983306884766
			 train-loss:  2.1018824216422685 	 ± 0.25492918823060706
	data : 0.11654949188232422
	model : 0.06945767402648925
			 train-loss:  2.1027962170779433 	 ± 0.254704088899927
	data : 0.11640620231628418
	model : 0.06993060111999512
			 train-loss:  2.101562795855782 	 ± 0.2547792399525717
	data : 0.11608548164367676
	model : 0.07009706497192383
			 train-loss:  2.1004889146235195 	 ± 0.2547007031116005
	data : 0.11598100662231445
	model : 0.0690371036529541
			 train-loss:  2.1000298659006753 	 ± 0.2542180172939784
	data : 0.11701979637145996
	model : 0.06898360252380371
			 train-loss:  2.103361421636402 	 ± 0.2584589533390735
	data : 0.11688222885131835
	model : 0.06862902641296387
			 train-loss:  2.1032304668000767 	 ± 0.2578888053782439
	data : 0.11712899208068847
	model : 0.0685420036315918
			 train-loss:  2.102066830529107 	 ± 0.2579037765328924
	data : 0.11707549095153809
	model : 0.0675417423248291
			 train-loss:  2.1003712468442663 	 ± 0.2585863927951983
	data : 0.11779756546020508
	model : 0.0678821086883545
			 train-loss:  2.0991874649661226 	 ± 0.2586291888651543
	data : 0.11749143600463867
	model : 0.06760697364807129
			 train-loss:  2.098407007100289 	 ± 0.2583291568166607
	data : 0.11794328689575195
	model : 0.06786108016967773
			 train-loss:  2.098427186366252 	 ± 0.2577646823379334
	data : 0.11769943237304688
	model : 0.06783008575439453
			 train-loss:  2.09818375006966 	 ± 0.25723009403296637
	data : 0.11793370246887207
	model : 0.06776447296142578
			 train-loss:  2.097759690635648 	 ± 0.2567532720897388
	data : 0.11799564361572265
	model : 0.06763954162597656
			 train-loss:  2.0981936228686364 	 ± 0.25628420119088774
	data : 0.11824111938476563
	model : 0.06743741035461426
			 train-loss:  2.0969952263033953 	 ± 0.2563842513501156
	data : 0.11846551895141602
	model : 0.06705341339111329
			 train-loss:  2.0980702562209887 	 ± 0.25636156145305394
	data : 0.1189107894897461
	model : 0.06653938293457032
			 train-loss:  2.0984080603782167 	 ± 0.25586771469257613
	data : 0.11906285285949707
	model : 0.06698012351989746
			 train-loss:  2.099583961700989 	 ± 0.2559605911968904
	data : 0.11854052543640137
	model : 0.06741929054260254
			 train-loss:  2.1005417017997066 	 ± 0.25584343057365005
	data : 0.11786298751831055
	model : 0.06771483421325683
			 train-loss:  2.1006130135359884 	 ± 0.25530773896022774
	data : 0.11750712394714355
	model : 0.0679847240447998
			 train-loss:  2.101669199297119 	 ± 0.2552935743721822
	data : 0.11725521087646484
	model : 0.06818280220031739
			 train-loss:  2.100655828913053 	 ± 0.25524239690931694
	data : 0.11706604957580566
	model : 0.067500638961792
			 train-loss:  2.1004922172340614 	 ± 0.25472490889123606
	data : 0.117741060256958
	model : 0.06740303039550781
			 train-loss:  2.100302238109683 	 ± 0.2542151814420417
	data : 0.11771931648254394
	model : 0.06735539436340332
			 train-loss:  2.098554230030672 	 ± 0.2551447614619239
	data : 0.11776719093322754
	model : 0.06729931831359863
			 train-loss:  2.0988093052731185 	 ± 0.2546524320322177
	data : 0.11773238182067872
	model : 0.06751704216003418
			 train-loss:  2.096781566678261 	 ± 0.25609849004206836
	data : 0.11767854690551757
	model : 0.06865143775939941
			 train-loss:  2.0951044985918497 	 ± 0.25692197575471926
	data : 0.11699252128601074
	model : 0.06824245452880859
			 train-loss:  2.0937315939891676 	 ± 0.257303976655685
	data : 0.11764359474182129
	model : 0.06855006217956543
			 train-loss:  2.0927256922568045 	 ± 0.2572708756546537
	data : 0.11733756065368653
	model : 0.06803812980651855
			 train-loss:  2.092936226641797 	 ± 0.25677515361248304
	data : 0.11789908409118652
	model : 0.06766581535339355
			 train-loss:  2.094178858757019 	 ± 0.2570101848245223
	data : 0.11834611892700195
	model : 0.06744341850280762
			 train-loss:  2.095649990902479 	 ± 0.257550244161417
	data : 0.1183657169342041
	model : 0.06796245574951172
			 train-loss:  2.0968600795382546 	 ± 0.25775268859412415
	data : 0.11801543235778808
	model : 0.06729693412780761
			 train-loss:  2.096567685425046 	 ± 0.2572846641124969
	data : 0.11841368675231934
	model : 0.06786541938781739
			 train-loss:  2.097302590768168 	 ± 0.25704363113337925
	data : 0.11774230003356934
	model : 0.06795849800109863
			 train-loss:  2.0958165622224993 	 ± 0.2576300215402039
	data : 0.11740174293518066
	model : 0.06714835166931152
			 train-loss:  2.0967115885578096 	 ± 0.25752326297417244
	data : 0.11704525947570801
	model : 0.05816097259521484
#epoch  23    val-loss:  2.5020117445995935  train-loss:  2.0967115885578096  lr:  0.00125
			 train-loss:  2.4948647022247314 	 ± 0.0
	data : 5.755661249160767
	model : 0.07469058036804199
			 train-loss:  2.2501837015151978 	 ± 0.2446810007095337
	data : 2.9439761638641357
	model : 0.07153105735778809
			 train-loss:  2.055975635846456 	 ± 0.33962637355443204
	data : 2.001791556676229
	model : 0.06966646512349446
			 train-loss:  2.0089646577835083 	 ± 0.3051878952147443
	data : 1.5313300490379333
	model : 0.06876367330551147
			 train-loss:  2.000793290138245 	 ± 0.27345713760160084
	data : 1.2489388465881348
	model : 0.06890826225280762
			 train-loss:  2.0046074986457825 	 ± 0.24977672567130613
	data : 0.12092223167419433
	model : 0.06717782020568848
			 train-loss:  2.0128431149891446 	 ± 0.23212654859481455
	data : 0.11827678680419922
	model : 0.0668367862701416
			 train-loss:  1.980424240231514 	 ± 0.23346151037393664
	data : 0.11853408813476562
	model : 0.06765499114990234
			 train-loss:  2.004694448577033 	 ± 0.23056580443054794
	data : 0.11773271560668945
	model : 0.06857461929321289
			 train-loss:  1.9927417516708374 	 ± 0.22165363481210398
	data : 0.117083740234375
	model : 0.06869049072265625
			 train-loss:  2.0257292010567407 	 ± 0.2356812635980295
	data : 0.11708536148071289
	model : 0.06939496994018554
			 train-loss:  2.0154435634613037 	 ± 0.22821172389930255
	data : 0.11648402214050294
	model : 0.06997141838073731
			 train-loss:  2.0387945908766527 	 ± 0.23370416641427239
	data : 0.11613407135009765
	model : 0.06923246383666992
			 train-loss:  2.0335669772965566 	 ± 0.22599035566546688
	data : 0.11671938896179199
	model : 0.0690469741821289
			 train-loss:  2.0135747671127318 	 ± 0.23078670531328951
	data : 0.11665821075439453
	model : 0.06905651092529297
			 train-loss:  1.9979742914438248 	 ± 0.23148265487174285
	data : 0.11681585311889649
	model : 0.06920166015625
			 train-loss:  2.014143705368042 	 ± 0.23369939182798016
	data : 0.116778564453125
	model : 0.06931447982788086
			 train-loss:  2.0169327523973255 	 ± 0.22740592747240296
	data : 0.11657958030700684
	model : 0.06997079849243164
			 train-loss:  2.0093851277702734 	 ± 0.22364501892640276
	data : 0.11607112884521484
	model : 0.06982569694519043
			 train-loss:  1.9989885449409486 	 ± 0.22264305406204504
	data : 0.11638932228088379
	model : 0.06966381072998047
			 train-loss:  1.9971821989331926 	 ± 0.21742749229498304
	data : 0.11659865379333496
	model : 0.06963181495666504
			 train-loss:  1.995469402183186 	 ± 0.21257344819549914
	data : 0.11657967567443847
	model : 0.06863627433776856
			 train-loss:  2.0118233069129614 	 ± 0.22160032181587513
	data : 0.11739187240600586
	model : 0.06781439781188965
			 train-loss:  2.008756622672081 	 ± 0.21743250612713594
	data : 0.11816062927246093
	model : 0.06783947944641114
			 train-loss:  2.0047661447525025 	 ± 0.2139345527691527
	data : 0.1181638240814209
	model : 0.0674222469329834
			 train-loss:  2.0033792165609507 	 ± 0.20989467524387023
	data : 0.1185293197631836
	model : 0.06740961074829102
			 train-loss:  2.0023997492260404 	 ± 0.20603160582637003
	data : 0.11862030029296874
	model : 0.06764965057373047
			 train-loss:  2.019463151693344 	 ± 0.2208943142761668
	data : 0.1185692310333252
	model : 0.06852598190307617
			 train-loss:  2.0255522275793143 	 ± 0.2194308267788692
	data : 0.11772518157958985
	model : 0.06874699592590332
			 train-loss:  2.0282204508781434 	 ± 0.21622061542422247
	data : 0.11733722686767578
	model : 0.06931767463684083
			 train-loss:  2.029603638956624 	 ± 0.21283947613507928
	data : 0.11680536270141602
	model : 0.06837286949157714
			 train-loss:  2.02880684658885 	 ± 0.2095344333052616
	data : 0.1176518440246582
	model : 0.06905803680419922
			 train-loss:  2.039476651133913 	 ± 0.21498201147815205
	data : 0.11692647933959961
	model : 0.06900315284729004
			 train-loss:  2.0318446860593906 	 ± 0.21628703458066448
	data : 0.11711034774780274
	model : 0.069921875
			 train-loss:  2.0328295980181013 	 ± 0.2132521730926454
	data : 0.11640992164611816
	model : 0.06999869346618652
			 train-loss:  2.031735029485491 	 ± 0.2103691667606944
	data : 0.11637358665466309
	model : 0.07101726531982422
			 train-loss:  2.0235346587928564 	 ± 0.21326030823529513
	data : 0.11548061370849609
	model : 0.07032804489135742
			 train-loss:  2.0279126982939872 	 ± 0.21211389757818716
	data : 0.11615486145019531
	model : 0.07063822746276856
			 train-loss:  2.04227091715886 	 ± 0.22731625051144427
	data : 0.11579909324645996
	model : 0.06960291862487793
			 train-loss:  2.046071594953537 	 ± 0.2257082668383595
	data : 0.11665339469909668
	model : 0.0695610523223877
			 train-loss:  2.051806688308716 	 ± 0.22587016526697395
	data : 0.11662225723266602
	model : 0.06953616142272949
			 train-loss:  2.0500647794632685 	 ± 0.22344358860967065
	data : 0.11645917892456055
	model : 0.07036762237548828
			 train-loss:  2.049448617669039 	 ± 0.22086622404809847
	data : 0.1156705379486084
	model : 0.06998987197875976
			 train-loss:  2.0527801676229998 	 ± 0.2194321679030343
	data : 0.1157832145690918
	model : 0.06997780799865723
			 train-loss:  2.053317101796468 	 ± 0.2170095639630589
	data : 0.1157369613647461
	model : 0.06973509788513184
			 train-loss:  2.04189742129782 	 ± 0.22789865362392364
	data : 0.11584963798522949
	model : 0.06953229904174804
			 train-loss:  2.0439695601767682 	 ± 0.22589875999194478
	data : 0.11627721786499023
	model : 0.06943602561950683
			 train-loss:  2.0426348447799683 	 ± 0.22372046939516607
	data : 0.116357421875
	model : 0.06867280006408691
			 train-loss:  2.0448227512593173 	 ± 0.2219440817159462
	data : 0.11716995239257813
	model : 0.06907467842102051
			 train-loss:  2.0420127296447754 	 ± 0.22059217291825053
	data : 0.11691722869873047
	model : 0.06837873458862305
			 train-loss:  2.042204029419843 	 ± 0.21842298667142837
	data : 0.1175194263458252
	model : 0.06852283477783203
			 train-loss:  2.0400494635105133 	 ± 0.21685911988063955
	data : 0.11730799674987794
	model : 0.06861920356750488
			 train-loss:  2.0498190758363255 	 ± 0.22606131098609678
	data : 0.11715893745422364
	model : 0.06950526237487793
			 train-loss:  2.0493223512614214 	 ± 0.22398756255360097
	data : 0.11633887290954589
	model : 0.06905574798583984
			 train-loss:  2.059252225268971 	 ± 0.23362955304705693
	data : 0.11659488677978516
	model : 0.07061610221862794
			 train-loss:  2.054930288876806 	 ± 0.2337422258600734
	data : 0.11502814292907715
	model : 0.06968474388122559
			 train-loss:  2.0555099499853036 	 ± 0.23172338737971035
	data : 0.11578302383422852
	model : 0.0696065902709961
			 train-loss:  2.0629927310450324 	 ± 0.23656179857825776
	data : 0.11611275672912598
	model : 0.0696187973022461
			 train-loss:  2.057546617621082 	 ± 0.2381874757228877
	data : 0.11615223884582519
	model : 0.06881504058837891
			 train-loss:  2.0632430414358773 	 ± 0.24021287318496345
	data : 0.11705751419067383
	model : 0.06847810745239258
			 train-loss:  2.0604089830742507 	 ± 0.23924506169609025
	data : 0.11749749183654785
	model : 0.06933503150939942
			 train-loss:  2.0621550659979544 	 ± 0.23769934774681023
	data : 0.1168288230895996
	model : 0.06927356719970704
			 train-loss:  2.073328082523649 	 ± 0.25168231853217843
	data : 0.11659202575683594
	model : 0.06906647682189941
			 train-loss:  2.0712755639106035 	 ± 0.250239181751735
	data : 0.11694364547729492
	model : 0.07000641822814942
			 train-loss:  2.072305387717027 	 ± 0.24844344075687455
	data : 0.1160618782043457
	model : 0.0696803092956543
			 train-loss:  2.068299802866849 	 ± 0.2486600794515453
	data : 0.11629037857055664
	model : 0.06973371505737305
			 train-loss:  2.0754958515736592 	 ± 0.2536270049178542
	data : 0.11618142127990723
	model : 0.06909480094909667
			 train-loss:  2.076332383296069 	 ± 0.25184829358374594
	data : 0.11685061454772949
	model : 0.0692744255065918
			 train-loss:  2.072097284206446 	 ± 0.2524440043062387
	data : 0.11641454696655273
	model : 0.0692406177520752
			 train-loss:  2.0714172465460643 	 ± 0.25069799519945946
	data : 0.11655468940734863
	model : 0.06926188468933106
			 train-loss:  2.0751709568668417 	 ± 0.25089959080848345
	data : 0.11675777435302734
	model : 0.06917314529418946
			 train-loss:  2.073994141485956 	 ± 0.24934838724048944
	data : 0.11671290397644044
	model : 0.06977295875549316
			 train-loss:  2.07505201476894 	 ± 0.24779726799160245
	data : 0.11616325378417969
	model : 0.06971759796142578
			 train-loss:  2.0731371077331335 	 ± 0.24666047640372082
	data : 0.11633963584899902
	model : 0.06943192481994628
			 train-loss:  2.0734347518285117 	 ± 0.2450239332936078
	data : 0.11646580696105957
	model : 0.0694284439086914
			 train-loss:  2.0775604326474038 	 ± 0.24601496724233332
	data : 0.11645607948303223
	model : 0.06950607299804687
			 train-loss:  2.075877023981763 	 ± 0.24485244559013486
	data : 0.11640348434448242
	model : 0.0697166919708252
			 train-loss:  2.0744306445121765 	 ± 0.24360866269398027
	data : 0.11636185646057129
	model : 0.06997976303100586
			 train-loss:  2.0692973695223844 	 ± 0.24627081773530834
	data : 0.11627359390258789
	model : 0.07006406784057617
			 train-loss:  2.0717265412211416 	 ± 0.24567736639419735
	data : 0.11622319221496583
	model : 0.07004318237304688
			 train-loss:  2.0758206211490395 	 ± 0.24688688685122048
	data : 0.11622400283813476
	model : 0.07006006240844727
			 train-loss:  2.075679501382316 	 ± 0.24538014810403308
	data : 0.1163393497467041
	model : 0.06900315284729004
			 train-loss:  2.080508848270738 	 ± 0.24778706749091775
	data : 0.11710944175720214
	model : 0.06792054176330567
			 train-loss:  2.08808105190595 	 ± 0.25578618678632714
	data : 0.11790337562561035
	model : 0.06826968193054199
			 train-loss:  2.0925068841261023 	 ± 0.2574922138414434
	data : 0.11768760681152343
	model : 0.06827764511108399
			 train-loss:  2.094430991383486 	 ± 0.2566046958853705
	data : 0.1178359031677246
	model : 0.06806869506835937
			 train-loss:  2.09471106666258 	 ± 0.2551389146073895
	data : 0.11798462867736817
	model : 0.0681009292602539
			 train-loss:  2.0925822339274665 	 ± 0.25446103396539377
	data : 0.1180417537689209
	model : 0.06881828308105468
			 train-loss:  2.092228811778379 	 ± 0.2530491589738106
	data : 0.11737546920776368
	model : 0.06780424118041992
			 train-loss:  2.095353118578593 	 ± 0.25335971347493524
	data : 0.11823654174804688
	model : 0.06807308197021485
			 train-loss:  2.0950001255496518 	 ± 0.25198603450949975
	data : 0.11781196594238282
	model : 0.0683225154876709
			 train-loss:  2.0963781968407007 	 ± 0.250957354039905
	data : 0.11768784523010253
	model : 0.06826872825622558
			 train-loss:  2.0941308570164505 	 ± 0.25053351662877615
	data : 0.1177565574645996
	model : 0.0684391975402832
			 train-loss:  2.0923577826073827 	 ± 0.24978326843656884
	data : 0.11759705543518066
	model : 0.06890478134155273
			 train-loss:  2.0951632825951827 	 ± 0.24994956429061665
	data : 0.11725249290466308
	model : 0.06848945617675781
			 train-loss:  2.091242997596661 	 ± 0.25156316494373016
	data : 0.11749258041381835
	model : 0.06829423904418945
			 train-loss:  2.0906649827957153 	 ± 0.2503271599813938
	data : 0.11762151718139649
	model : 0.06917738914489746
			 train-loss:  2.0876870982500972 	 ± 0.25076769707072105
	data : 0.11691832542419434
	model : 0.06894779205322266
			 train-loss:  2.088840308815542 	 ± 0.24975902631015467
	data : 0.11719684600830078
	model : 0.06936554908752442
			 train-loss:  2.0899830079078674 	 ± 0.2487670514186056
	data : 0.11665983200073242
	model : 0.06949739456176758
			 train-loss:  2.086829426264999 	 ± 0.24953322496789937
	data : 0.11663241386413574
	model : 0.0697366714477539
			 train-loss:  2.0844813702153226 	 ± 0.24942578321041758
	data : 0.11627793312072754
	model : 0.06902489662170411
			 train-loss:  2.087425690252804 	 ± 0.24998689878038402
	data : 0.11694831848144531
	model : 0.069205904006958
			 train-loss:  2.0887463046954227 	 ± 0.24914290141699458
	data : 0.11674966812133789
	model : 0.0690755844116211
			 train-loss:  2.0882756278628394 	 ± 0.24800012393444137
	data : 0.11682496070861817
	model : 0.06882305145263672
			 train-loss:  2.091370978445377 	 ± 0.2488571075446907
	data : 0.11717958450317383
	model : 0.06882538795471191
			 train-loss:  2.091640862349038 	 ± 0.24770707901369132
	data : 0.11718754768371582
	model : 0.06967840194702149
			 train-loss:  2.095179142775359 	 ± 0.24925938060673236
	data : 0.11638622283935547
	model : 0.06974072456359863
			 train-loss:  2.098174974459027 	 ± 0.2500590678958481
	data : 0.11654176712036132
	model : 0.06894063949584961
			 train-loss:  2.097686620192094 	 ± 0.24897205163354774
	data : 0.11722474098205567
	model : 0.06857948303222657
			 train-loss:  2.097876267390208 	 ± 0.24785599967835062
	data : 0.1173180103302002
	model : 0.06768999099731446
			 train-loss:  2.0957352189081058 	 ± 0.24777595743913372
	data : 0.11807971000671387
	model : 0.06745796203613282
			 train-loss:  2.0935926363531467 	 ± 0.24771713461815373
	data : 0.11811337471008301
	model : 0.0667658805847168
			 train-loss:  2.0966442499244424 	 ± 0.24875247747725557
	data : 0.1187870979309082
	model : 0.06770114898681641
			 train-loss:  2.093922611941462 	 ± 0.2493675219137517
	data : 0.11799225807189942
	model : 0.0682896614074707
			 train-loss:  2.0927949496384324 	 ± 0.24858464794330212
	data : 0.11757760047912598
	model : 0.06918926239013672
			 train-loss:  2.0877739639363737 	 ± 0.2533585701158967
	data : 0.11690411567687989
	model : 0.06890406608581542
			 train-loss:  2.0900971131809687 	 ± 0.2535311200504516
	data : 0.11731853485107421
	model : 0.06883139610290527
			 train-loss:  2.089729734829494 	 ± 0.2524951552404503
	data : 0.11732535362243653
	model : 0.06789908409118653
			 train-loss:  2.0882824788490932 	 ± 0.2519360497419874
	data : 0.11816000938415527
	model : 0.06795501708984375
			 train-loss:  2.089287652457056 	 ± 0.2511343423920703
	data : 0.118194580078125
	model : 0.06777262687683105
			 train-loss:  2.087276524207631 	 ± 0.251079478278539
	data : 0.11825556755065918
	model : 0.06910548210144044
			 train-loss:  2.0838798400832386 	 ± 0.2528555857114131
	data : 0.11684594154357911
	model : 0.0690925121307373
			 train-loss:  2.0871568331795354 	 ± 0.2544429068562884
	data : 0.11696906089782715
	model : 0.0701932430267334
			 train-loss:  2.085492346763611 	 ± 0.2540999944318239
	data : 0.11590900421142578
	model : 0.06935334205627441
			 train-loss:  2.084648747292776 	 ± 0.2532653345823513
	data : 0.11648612022399903
	model : 0.06957869529724121
			 train-loss:  2.0851663210260587 	 ± 0.2523331474789926
	data : 0.11639623641967774
	model : 0.06812500953674316
			 train-loss:  2.087583689019084 	 ± 0.2528175735962771
	data : 0.1177492618560791
	model : 0.06882967948913574
			 train-loss:  2.0848046310188235 	 ± 0.2537908822986372
	data : 0.11682524681091308
	model : 0.06848254203796386
			 train-loss:  2.0839038041921762 	 ± 0.2530198291111349
	data : 0.11732392311096192
	model : 0.06835389137268066
			 train-loss:  2.084274011713858 	 ± 0.2520875958415371
	data : 0.11747655868530274
	model : 0.0674593448638916
			 train-loss:  2.081901279362765 	 ± 0.2525950158993571
	data : 0.11824779510498047
	model : 0.06745362281799316
			 train-loss:  2.085776952872599 	 ± 0.25555285651768095
	data : 0.118324613571167
	model : 0.0673448085784912
			 train-loss:  2.0870496230338937 	 ± 0.2550202217983658
	data : 0.11831669807434082
	model : 0.06759743690490723
			 train-loss:  2.0846093981354326 	 ± 0.2556393943337193
	data : 0.11809401512145996
	model : 0.06806597709655762
			 train-loss:  2.0860313729328266 	 ± 0.2552331206256149
	data : 0.11792621612548829
	model : 0.06785368919372559
			 train-loss:  2.0870518136198504 	 ± 0.254578198792484
	data : 0.11789822578430176
	model : 0.06749281883239747
			 train-loss:  2.084330732407777 	 ± 0.25564586434253617
	data : 0.11820707321166993
	model : 0.06745514869689942
			 train-loss:  2.0816122885230635 	 ± 0.256718602380586
	data : 0.11835556030273438
	model : 0.06735424995422364
			 train-loss:  2.0824494872774397 	 ± 0.2559904688583543
	data : 0.11834535598754883
	model : 0.06697821617126465
			 train-loss:  2.0845936037969928 	 ± 0.25633956662967405
	data : 0.11870312690734863
	model : 0.06826677322387695
			 train-loss:  2.083410749972706 	 ± 0.2558212389836261
	data : 0.11775741577148438
	model : 0.06951866149902344
			 train-loss:  2.0820809110894904 	 ± 0.25541725738770354
	data : 0.11665339469909668
	model : 0.0698310375213623
			 train-loss:  2.082805800769064 	 ± 0.2546764130767145
	data : 0.11646943092346192
	model : 0.07006473541259765
			 train-loss:  2.079090816399147 	 ± 0.25768221550981213
	data : 0.1164323329925537
	model : 0.071528959274292
			 train-loss:  2.0825723221857255 	 ± 0.26019772633031923
	data : 0.11474428176879883
	model : 0.07109332084655762
			 train-loss:  2.081723067225242 	 ± 0.2595141485508766
	data : 0.11515254974365234
	model : 0.07017750740051269
			 train-loss:  2.0828060169477722 	 ± 0.2589689955707881
	data : 0.11599144935607911
	model : 0.0699610710144043
			 train-loss:  2.0882123380699413 	 ± 0.2663468227504516
	data : 0.11598901748657227
	model : 0.06969528198242188
			 train-loss:  2.0870060181617736 	 ± 0.26586560214966537
	data : 0.1159933090209961
	model : 0.06948823928833008
			 train-loss:  2.084997579751425 	 ± 0.26612305939209535
	data : 0.11643562316894532
	model : 0.06909542083740235
			 train-loss:  2.085719414447483 	 ± 0.26539447955317524
	data : 0.11665115356445313
	model : 0.06890592575073243
			 train-loss:  2.0862416628918616 	 ± 0.26460410491304837
	data : 0.11663918495178223
	model : 0.06810693740844727
			 train-loss:  2.0850387811660767 	 ± 0.2641629549386781
	data : 0.1174652099609375
	model : 0.06836905479431152
			 train-loss:  2.084121606426854 	 ± 0.2635553189426239
	data : 0.11736912727355957
	model : 0.06832351684570312
			 train-loss:  2.0853493251861672 	 ± 0.2631535125901041
	data : 0.11746025085449219
	model : 0.06866917610168458
			 train-loss:  2.0848445095074406 	 ± 0.2623898715785357
	data : 0.11727519035339355
	model : 0.06849966049194336
			 train-loss:  2.0854774570163293 	 ± 0.26167841484759585
	data : 0.11737523078918458
	model : 0.06850533485412598
			 train-loss:  2.0869672920718885 	 ± 0.2615255758685297
	data : 0.11734304428100586
	model : 0.06842899322509766
			 train-loss:  2.0870364122092724 	 ± 0.26070848433261024
	data : 0.11741294860839843
	model : 0.06823043823242188
			 train-loss:  2.0870221098017248 	 ± 0.259897632474685
	data : 0.1176023006439209
	model : 0.06868920326232911
			 train-loss:  2.0873756327746826 	 ± 0.25913306519053
	data : 0.11743769645690919
	model : 0.06938118934631347
			 train-loss:  2.0896705681560963 	 ± 0.25998306218088935
	data : 0.11700057983398438
	model : 0.07009739875793457
			 train-loss:  2.0889705332314095 	 ± 0.25934326513820255
	data : 0.11641669273376465
	model : 0.06985397338867187
			 train-loss:  2.0894567836414684 	 ± 0.2586311569985746
	data : 0.11654500961303711
	model : 0.06978135108947754
			 train-loss:  2.0927975450653626 	 ± 0.2613974683240669
	data : 0.11661720275878906
	model : 0.06910123825073242
			 train-loss:  2.091668472318592 	 ± 0.2610193491084186
	data : 0.11691579818725586
	model : 0.06914429664611817
			 train-loss:  2.0942376887514476 	 ± 0.2623507273624788
	data : 0.11692509651184083
	model : 0.06918301582336425
			 train-loss:  2.0943940116103583 	 ± 0.26158123753643103
	data : 0.11686739921569825
	model : 0.06837444305419922
			 train-loss:  2.09550000148661 	 ± 0.2612067541956819
	data : 0.11756162643432617
	model : 0.06868424415588378
			 train-loss:  2.093128140209711 	 ± 0.2622715044145754
	data : 0.11718530654907226
	model : 0.0683854103088379
			 train-loss:  2.0935007583263308 	 ± 0.2615533670272699
	data : 0.11773977279663086
	model : 0.06778922080993652
			 train-loss:  2.0912808517500157 	 ± 0.2624163538847227
	data : 0.11813325881958008
	model : 0.06788530349731445
			 train-loss:  2.0909525758918677 	 ± 0.26169681990269084
	data : 0.11817107200622559
	model : 0.06881260871887207
			 train-loss:  2.0904295682907104 	 ± 0.2610392246974553
	data : 0.11729507446289063
	model : 0.068458890914917
			 train-loss:  2.0899672257629307 	 ± 0.2603684263806388
	data : 0.11760730743408203
	model : 0.06927437782287597
			 train-loss:  2.0912643792265553 	 ± 0.26020156082073903
	data : 0.11689510345458984
	model : 0.06999750137329101
			 train-loss:  2.0927451065417086 	 ± 0.26021639045923217
	data : 0.11631245613098144
	model : 0.06955528259277344
			 train-loss:  2.0921471685004636 	 ± 0.25961110853126995
	data : 0.11661906242370605
	model : 0.06934957504272461
			 train-loss:  2.091238886117935 	 ± 0.25917400642037136
	data : 0.11702723503112793
	model : 0.06892557144165039
			 train-loss:  2.0923814898696396 	 ± 0.25891128208678643
	data : 0.11741933822631836
	model : 0.06853570938110351
			 train-loss:  2.0934986852027557 	 ± 0.25863611161236355
	data : 0.11760835647583008
	model : 0.06860976219177246
			 train-loss:  2.0949679717340106 	 ± 0.25868901564912133
	data : 0.11737141609191895
	model : 0.06913018226623535
			 train-loss:  2.095349937029507 	 ± 0.25803683916913894
	data : 0.11690363883972169
	model : 0.06943063735961914
			 train-loss:  2.0928480425396483 	 ± 0.2595666500059465
	data : 0.11646337509155273
	model : 0.06990022659301758
			 train-loss:  2.0918514106863286 	 ± 0.2592226288849329
	data : 0.11604533195495606
	model : 0.07091851234436035
			 train-loss:  2.0913139256564053 	 ± 0.2586324921799158
	data : 0.11514039039611816
	model : 0.0708956241607666
			 train-loss:  2.0909748584666152 	 ± 0.25798539263085246
	data : 0.11537055969238282
	model : 0.07087469100952148
			 train-loss:  2.093179869273352 	 ± 0.25907215572825204
	data : 0.11541290283203125
	model : 0.07018308639526367
			 train-loss:  2.0931331220426057 	 ± 0.2583902866878326
	data : 0.11606779098510742
	model : 0.07005634307861328
			 train-loss:  2.092948048526704 	 ± 0.25772561062384713
	data : 0.11610307693481445
	model : 0.06939029693603516
			 train-loss:  2.0941865717371306 	 ± 0.2576228297174349
	data : 0.11667938232421875
	model : 0.06925373077392578
			 train-loss:  2.094810173301499 	 ± 0.25709979308983993
	data : 0.11665401458740235
	model : 0.06873435974121093
			 train-loss:  2.097003810184518 	 ± 0.25824078811809864
	data : 0.11704010963439941
	model : 0.06925363540649414
			 train-loss:  2.0962446805758352 	 ± 0.25779470752509515
	data : 0.11674013137817382
	model : 0.06911673545837402
			 train-loss:  2.095944354120566 	 ± 0.2571704249091187
	data : 0.11693110466003417
	model : 0.06911087036132812
			 train-loss:  2.0942893306615993 	 ± 0.2575612017486875
	data : 0.11701674461364746
	model : 0.0685429573059082
			 train-loss:  2.0937096801671116 	 ± 0.2570387603952054
	data : 0.11766343116760254
	model : 0.06835646629333496
			 train-loss:  2.0939452570287425 	 ± 0.2564135487823982
	data : 0.11783480644226074
	model : 0.06872501373291015
			 train-loss:  2.094211749434471 	 ± 0.25579933753129075
	data : 0.11744327545166015
	model : 0.06919307708740234
			 train-loss:  2.0935707922598614 	 ± 0.2553231824375402
	data : 0.11687111854553223
	model : 0.06893048286437989
			 train-loss:  2.094546634371918 	 ± 0.255065895231167
	data : 0.1170886516571045
	model : 0.06964540481567383
			 train-loss:  2.093698205619023 	 ± 0.2547224589126214
	data : 0.11657028198242188
	model : 0.07041311264038086
			 train-loss:  2.096332794311 	 ± 0.2568550354529253
	data : 0.11602625846862794
	model : 0.06915216445922852
			 train-loss:  2.09431524451186 	 ± 0.2578431031273939
	data : 0.11695299148559571
	model : 0.06895861625671387
			 train-loss:  2.094993643969008 	 ± 0.25739984225888574
	data : 0.11723294258117675
	model : 0.06920218467712402
			 train-loss:  2.094435995903568 	 ± 0.25690205912762504
	data : 0.11702885627746581
	model : 0.06901106834411622
			 train-loss:  2.0927761518038235 	 ± 0.2573939951898034
	data : 0.11716976165771484
	model : 0.06976170539855957
			 train-loss:  2.091978347472597 	 ± 0.25703514431596935
	data : 0.11611146926879883
	model : 0.07069582939147949
			 train-loss:  2.091670530750638 	 ± 0.25646103666484277
	data : 0.11536474227905273
	model : 0.07052650451660156
			 train-loss:  2.091205315567306 	 ± 0.255941391190303
	data : 0.11551342010498047
	model : 0.07055792808532715
			 train-loss:  2.0902105170600818 	 ± 0.25574560769898375
	data : 0.1153482437133789
	model : 0.07081460952758789
			 train-loss:  2.0900359931686117 	 ± 0.2551572133413242
	data : 0.11515636444091797
	model : 0.06997647285461425
			 train-loss:  2.091277746953697 	 ± 0.2552046423396128
	data : 0.11618280410766602
	model : 0.07002224922180175
			 train-loss:  2.089665108503297 	 ± 0.25570101872874923
	data : 0.11612963676452637
	model : 0.07019104957580566
			 train-loss:  2.088800637258424 	 ± 0.25542314620773626
	data : 0.1160168170928955
	model : 0.07015376091003418
			 train-loss:  2.088701656337158 	 ± 0.2548380860952356
	data : 0.11604156494140624
	model : 0.0700376033782959
			 train-loss:  2.091410749002334 	 ± 0.25736579357635864
	data : 0.11620979309082032
	model : 0.07018046379089356
			 train-loss:  2.0922573652441643 	 ± 0.2570816065590235
	data : 0.1159505844116211
	model : 0.07012338638305664
			 train-loss:  2.0927396216175773 	 ± 0.2565959315845665
	data : 0.1160550594329834
	model : 0.06899256706237793
			 train-loss:  2.094355653853438 	 ± 0.2571343814697256
	data : 0.11703200340270996
	model : 0.06883015632629394
			 train-loss:  2.0947083981187493 	 ± 0.256608183036267
	data : 0.11713848114013672
	model : 0.06810321807861328
			 train-loss:  2.0930285523290593 	 ± 0.2572526694608051
	data : 0.11754412651062011
	model : 0.06717457771301269
			 train-loss:  2.0902480813009396 	 ± 0.2600144448568323
	data : 0.11853842735290528
	model : 0.06714563369750977
			 train-loss:  2.092015912797716 	 ± 0.2607816803807129
	data : 0.11860566139221192
	model : 0.06763801574707032
			 train-loss:  2.0916269500698665 	 ± 0.26026949344571293
	data : 0.1180955410003662
	model : 0.06677398681640626
			 train-loss:  2.092321298195927 	 ± 0.25990527729590535
	data : 0.11901764869689942
	model : 0.06673140525817871
			 train-loss:  2.0938105436793544 	 ± 0.26030353505479975
	data : 0.11890139579772949
	model : 0.06684799194335937
			 train-loss:  2.0938266448058416 	 ± 0.2597346786845949
	data : 0.11859431266784667
	model : 0.0659017562866211
			 train-loss:  2.094891127296116 	 ± 0.2596695495805056
	data : 0.11937332153320312
	model : 0.06565709114074707
			 train-loss:  2.0953595431852134 	 ± 0.2592042491500599
	data : 0.11952261924743653
	model : 0.06591925621032715
			 train-loss:  2.0957634829241654 	 ± 0.25871786943779795
	data : 0.1191624641418457
	model : 0.0666433334350586
			 train-loss:  2.0964203781324393 	 ± 0.2583559021740042
	data : 0.11865358352661133
	model : 0.06722922325134277
			 train-loss:  2.0974077485565448 	 ± 0.2582434453786298
	data : 0.11838111877441407
	model : 0.068198823928833
			 train-loss:  2.097792071484505 	 ± 0.25776045853376695
	data : 0.11747126579284668
	model : 0.068412446975708
			 train-loss:  2.0989314782417425 	 ± 0.25780615752806046
	data : 0.11748695373535156
	model : 0.06901316642761231
			 train-loss:  2.096598914403956 	 ± 0.259745293577487
	data : 0.11695365905761719
	model : 0.06897850036621093
			 train-loss:  2.094734548019762 	 ± 0.26078328061532374
	data : 0.11705098152160645
	model : 0.06886744499206543
			 train-loss:  2.094536414705061 	 ± 0.26025508756822585
	data : 0.11697268486022949
	model : 0.06865496635437011
			 train-loss:  2.0947293316324553 	 ± 0.25972944736108117
	data : 0.11699624061584472
	model : 0.06889605522155762
			 train-loss:  2.0927706012092684 	 ± 0.26096026772567193
	data : 0.11656651496887208
	model : 0.06862945556640625
			 train-loss:  2.0915246172384783 	 ± 0.26113789847944713
	data : 0.1168581485748291
	model : 0.06840653419494629
			 train-loss:  2.091081254766802 	 ± 0.26069127793691677
	data : 0.11702489852905273
	model : 0.06783885955810547
			 train-loss:  2.089930763987244 	 ± 0.26077396322014296
	data : 0.11745452880859375
	model : 0.06782922744750977
			 train-loss:  2.09092279745608 	 ± 0.26070217529237
	data : 0.1176335334777832
	model : 0.06786260604858399
			 train-loss:  2.0900157408985667 	 ± 0.260558851929081
	data : 0.11769194602966308
	model : 0.06771721839904785
			 train-loss:  2.090159953847105 	 ± 0.2600407073275177
	data : 0.11766939163208008
	model : 0.06776976585388184
			 train-loss:  2.089033413798578 	 ± 0.2601191444401387
	data : 0.11766228675842286
	model : 0.06839818954467773
			 train-loss:  2.08878390137929 	 ± 0.2596260273677058
	data : 0.11736726760864258
	model : 0.0685056209564209
			 train-loss:  2.091918143749237 	 ± 0.2637841858860165
	data : 0.11719746589660644
	model : 0.06863274574279785
			 train-loss:  2.0917931177701607 	 ± 0.2632656169889086
	data : 0.11700859069824218
	model : 0.06912012100219726
			 train-loss:  2.091256001165935 	 ± 0.26288050951519504
	data : 0.11667156219482422
	model : 0.06934914588928223
			 train-loss:  2.0940505793914492 	 ± 0.2660846699755951
	data : 0.11650691032409669
	model : 0.06936841011047364
			 train-loss:  2.0935478707936803 	 ± 0.2656807189650196
	data : 0.11634516716003418
	model : 0.0689056396484375
			 train-loss:  2.0938478161306944 	 ± 0.26520235160760586
	data : 0.11689658164978027
	model : 0.068902587890625
			 train-loss:  2.0992690976709127 	 ± 0.2784817348646329
	data : 0.11592297554016114
	model : 0.05989842414855957
#epoch  24    val-loss:  2.4525104949348853  train-loss:  2.0992690976709127  lr:  0.00125
			 train-loss:  1.9405770301818848 	 ± 0.0
	data : 5.72265625
	model : 0.07274007797241211
			 train-loss:  1.9319871664047241 	 ± 0.008589863777160645
	data : 2.926214337348938
	model : 0.07223618030548096
			 train-loss:  1.8903566201527913 	 ± 0.059290768834128635
	data : 1.9903361797332764
	model : 0.0715630054473877
			 train-loss:  1.8361648917198181 	 ± 0.10698961044590226
	data : 1.521597445011139
	model : 0.07042258977890015
			 train-loss:  1.9708647727966309 	 ± 0.28589098139522123
	data : 1.2409873962402345
	model : 0.07017979621887208
			 train-loss:  1.944816768169403 	 ± 0.26740207633431473
	data : 0.11985301971435547
	model : 0.06905674934387207
			 train-loss:  1.944682002067566 	 ± 0.24756643707651158
	data : 0.11743731498718261
	model : 0.06807351112365723
			 train-loss:  1.9987768977880478 	 ± 0.2722348294615923
	data : 0.11746697425842285
	model : 0.06738672256469727
			 train-loss:  2.0365501642227173 	 ± 0.27801387541117517
	data : 0.11800704002380372
	model : 0.06793103218078614
			 train-loss:  2.0076654672622682 	 ± 0.2776174974591826
	data : 0.11755485534667968
	model : 0.0689126968383789
			 train-loss:  2.0256881497123023 	 ± 0.2707640079565125
	data : 0.11660723686218262
	model : 0.0694580078125
			 train-loss:  1.9915118714173634 	 ± 0.28293447471686645
	data : 0.11626434326171875
	model : 0.07017107009887695
			 train-loss:  1.9962108043523936 	 ± 0.27232156885582387
	data : 0.11567792892456055
	model : 0.07065911293029785
			 train-loss:  1.9887815969330924 	 ± 0.2637792133580714
	data : 0.11531667709350586
	model : 0.07073969841003418
			 train-loss:  1.9776692946751913 	 ± 0.2582045845844133
	data : 0.11535987854003907
	model : 0.06993980407714843
			 train-loss:  1.9699930995702744 	 ± 0.25176698870377173
	data : 0.11614794731140136
	model : 0.06981325149536133
			 train-loss:  1.9679375606424667 	 ± 0.24438820744783432
	data : 0.11639695167541504
	model : 0.06902976036071777
			 train-loss:  1.977167718940311 	 ± 0.24053240167119264
	data : 0.11701793670654297
	model : 0.0685685157775879
			 train-loss:  1.9794622785166691 	 ± 0.234319359648427
	data : 0.11756424903869629
	model : 0.06855621337890624
			 train-loss:  2.0002831161022185 	 ± 0.24575785331960265
	data : 0.1175532341003418
	model : 0.06856579780578613
			 train-loss:  2.0215231407256353 	 ± 0.2579605618331343
	data : 0.11744060516357421
	model : 0.06881146430969239
			 train-loss:  2.012399071996862 	 ± 0.25547438475260814
	data : 0.11716580390930176
	model : 0.0694951057434082
			 train-loss:  2.022934535275335 	 ± 0.2546985871650352
	data : 0.11650900840759278
	model : 0.06928634643554688
			 train-loss:  2.010749285419782 	 ± 0.2560926468373814
	data : 0.11666598320007324
	model : 0.06913342475891113
			 train-loss:  2.0028296661376954 	 ± 0.2539003640867132
	data : 0.11664052009582519
	model : 0.06817150115966797
			 train-loss:  1.9917402405005236 	 ± 0.2550692859649703
	data : 0.11756200790405273
	model : 0.06806597709655762
			 train-loss:  1.9994639688067966 	 ± 0.2533806501083535
	data : 0.11773591041564942
	model : 0.06792349815368652
			 train-loss:  2.0043535189969197 	 ± 0.25010866038117396
	data : 0.11789731979370117
	model : 0.06778502464294434
			 train-loss:  2.010634516847545 	 ± 0.24799581019418723
	data : 0.11801042556762695
	model : 0.06790928840637207
			 train-loss:  2.01861545642217 	 ± 0.2475863976670159
	data : 0.11805229187011719
	model : 0.06881637573242187
			 train-loss:  2.021894174237405 	 ± 0.24422148993102333
	data : 0.1172243595123291
	model : 0.06879901885986328
			 train-loss:  2.016445528715849 	 ± 0.24228201907742075
	data : 0.11726932525634766
	model : 0.0687985897064209
			 train-loss:  2.021006869547295 	 ± 0.23997407771965354
	data : 0.11731576919555664
	model : 0.06974577903747559
			 train-loss:  2.0221149115001453 	 ± 0.2365043810422269
	data : 0.11667561531066895
	model : 0.07019405364990235
			 train-loss:  2.018404367991856 	 ± 0.2341032150261133
	data : 0.11637516021728515
	model : 0.07073683738708496
			 train-loss:  2.016874313354492 	 ± 0.2310062997973501
	data : 0.11602973937988281
	model : 0.07086200714111328
			 train-loss:  2.010703663568239 	 ± 0.23085149289036885
	data : 0.11558008193969727
	model : 0.07105574607849122
			 train-loss:  2.0097062587738037 	 ± 0.22787450073455345
	data : 0.11544122695922851
	model : 0.0709263801574707
			 train-loss:  2.0149851762331448 	 ± 0.22727577262632784
	data : 0.11509747505187988
	model : 0.07036414146423339
			 train-loss:  2.0066464275121687 	 ± 0.23037963100359699
	data : 0.11552948951721191
	model : 0.06962037086486816
			 train-loss:  2.0098150735948144 	 ± 0.22843353608655617
	data : 0.11624789237976074
	model : 0.06983208656311035
			 train-loss:  2.0045757180168513 	 ± 0.22817742966110716
	data : 0.11617379188537598
	model : 0.06975750923156739
			 train-loss:  2.001580920330314 	 ± 0.22634225636434918
	data : 0.11613879203796387
	model : 0.06995458602905273
			 train-loss:  2.0053534616123545 	 ± 0.22511876682742588
	data : 0.11614723205566406
	model : 0.07007966041564942
			 train-loss:  2.0001239856084188 	 ± 0.22528994128523008
	data : 0.11587467193603515
	model : 0.07025017738342285
			 train-loss:  2.006197081959766 	 ± 0.22652127587614448
	data : 0.11559367179870605
	model : 0.06986894607543945
			 train-loss:  2.003298980124453 	 ± 0.22495888480576098
	data : 0.11589584350585938
	model : 0.06982631683349609
			 train-loss:  2.00081088890632 	 ± 0.22325580815409504
	data : 0.11605396270751953
	model : 0.06965851783752441
			 train-loss:  2.0015636633853524 	 ± 0.221027484136643
	data : 0.1162635326385498
	model : 0.06959872245788574
			 train-loss:  2.0061477541923525 	 ± 0.2211464815607808
	data : 0.11627254486083985
	model : 0.06959671974182129
			 train-loss:  2.0096917503020344 	 ± 0.2203969716981963
	data : 0.11637701988220214
	model : 0.06972031593322754
			 train-loss:  2.0145284877373624 	 ± 0.22098368573516267
	data : 0.116298246383667
	model : 0.0697866439819336
			 train-loss:  2.015743374824524 	 ± 0.21906425208599803
	data : 0.11617069244384766
	model : 0.06931266784667969
			 train-loss:  2.0156827215795166 	 ± 0.2170268499986273
	data : 0.11654720306396485
	model : 0.06938090324401855
			 train-loss:  2.0121955654837866 	 ± 0.2165662300828553
	data : 0.11673216819763184
	model : 0.06922450065612792
			 train-loss:  2.0185957508427754 	 ± 0.21980979474045562
	data : 0.11685137748718262
	model : 0.06919665336608886
			 train-loss:  2.0250520120587265 	 ± 0.22316575895734694
	data : 0.11683201789855957
	model : 0.06911544799804688
			 train-loss:  2.0275842937929878 	 ± 0.22205808701979635
	data : 0.11680612564086915
	model : 0.06955785751342773
			 train-loss:  2.027918294324713 	 ± 0.22018289002320107
	data : 0.11645302772521973
	model : 0.0691986083984375
			 train-loss:  2.0279975215593975 	 ± 0.21834117106411197
	data : 0.11671242713928223
	model : 0.06944499015808106
			 train-loss:  2.028669322123293 	 ± 0.2166066100837452
	data : 0.11642403602600097
	model : 0.06949319839477539
			 train-loss:  2.029787236644376 	 ± 0.21503001760403176
	data : 0.1164128303527832
	model : 0.06953368186950684
			 train-loss:  2.0274557386125838 	 ± 0.2141051100888949
	data : 0.1163865089416504
	model : 0.06956219673156738
			 train-loss:  2.030788902193308 	 ± 0.21406695512280924
	data : 0.11632647514343261
	model : 0.0691293716430664
			 train-loss:  2.024429361636822 	 ± 0.21842176374260386
	data : 0.1165844440460205
	model : 0.06909537315368652
			 train-loss:  2.0251548290252686 	 ± 0.21683963445104446
	data : 0.11663570404052734
	model : 0.06905641555786132
			 train-loss:  2.0237377490570294 	 ± 0.21552303869261594
	data : 0.11681113243103028
	model : 0.06826195716857911
			 train-loss:  2.0248235832242405 	 ± 0.21411698840374413
	data : 0.1175929069519043
	model : 0.06764178276062012
			 train-loss:  2.0212851002596426 	 ± 0.21455318060467074
	data : 0.11803855895996093
	model : 0.06882452964782715
			 train-loss:  2.021908628940582 	 ± 0.21307810406850855
	data : 0.11711301803588867
	model : 0.06791181564331054
			 train-loss:  2.0257727814392306 	 ± 0.2140280984833459
	data : 0.11782126426696778
	model : 0.06808223724365234
			 train-loss:  2.0300274011161594 	 ± 0.21553893229360993
	data : 0.11773056983947754
	model : 0.0689284324645996
			 train-loss:  2.024592352240053 	 ± 0.2189691670014905
	data : 0.11720390319824218
	model : 0.06890482902526855
			 train-loss:  2.024360010752807 	 ± 0.21749367275450812
	data : 0.11731495857238769
	model : 0.06770596504211426
			 train-loss:  2.0255917437871296 	 ± 0.21629853140868865
	data : 0.11821570396423339
	model : 0.06854729652404785
			 train-loss:  2.0257136586465334 	 ± 0.21487339669480646
	data : 0.11731195449829102
	model : 0.06762504577636719
			 train-loss:  2.028814921131382 	 ± 0.21517879480363084
	data : 0.1178400993347168
	model : 0.06763277053833008
			 train-loss:  2.0283777988873997 	 ± 0.21382939997914036
	data : 0.11772770881652832
	model : 0.06820516586303711
			 train-loss:  2.0294072869457778 	 ± 0.2126661890135918
	data : 0.1172952651977539
	model : 0.06904950141906738
			 train-loss:  2.028613641858101 	 ± 0.21145054156867385
	data : 0.11646924018859864
	model : 0.06911048889160157
			 train-loss:  2.0264586872524686 	 ± 0.2110233307731453
	data : 0.11650681495666504
	model : 0.06900744438171387
			 train-loss:  2.022166810384611 	 ± 0.21325999109630425
	data : 0.1168853759765625
	model : 0.0682450771331787
			 train-loss:  2.0197791292006713 	 ± 0.21307125070218416
	data : 0.11744928359985352
	model : 0.06847958564758301
			 train-loss:  2.0217518281368982 	 ± 0.21256031435893888
	data : 0.11740326881408691
	model : 0.06845560073852539
			 train-loss:  2.0216507028130923 	 ± 0.21130829287463368
	data : 0.11748552322387695
	model : 0.06827573776245117
			 train-loss:  2.02358531258827 	 ± 0.2108319839906501
	data : 0.11770792007446289
	model : 0.06916069984436035
			 train-loss:  2.027583984122879 	 ± 0.2128715355761919
	data : 0.11675419807434081
	model : 0.06917877197265625
			 train-loss:  2.026512935757637 	 ± 0.21189421209109444
	data : 0.11677193641662598
	model : 0.06906714439392089
			 train-loss:  2.027372816975197 	 ± 0.2108547818490307
	data : 0.11666479110717773
	model : 0.06897687911987305
			 train-loss:  2.0328001274002925 	 ± 0.21584091153061183
	data : 0.11689658164978027
	model : 0.06902456283569336
			 train-loss:  2.0336043507188233 	 ± 0.21478724476062172
	data : 0.11684136390686035
	model : 0.06895499229431153
			 train-loss:  2.0337302075779955 	 ± 0.21362010720558158
	data : 0.11689162254333496
	model : 0.06894750595092773
			 train-loss:  2.0346261032166018 	 ± 0.21264220826209068
	data : 0.11689524650573731
	model : 0.0689004898071289
			 train-loss:  2.033851831517321 	 ± 0.21163986653755393
	data : 0.11703839302062988
	model : 0.0690546989440918
			 train-loss:  2.0329463067807647 	 ± 0.21070600844075565
	data : 0.11700253486633301
	model : 0.06924829483032227
			 train-loss:  2.033949284503857 	 ± 0.20983355218762925
	data : 0.11700439453125
	model : 0.06961016654968262
			 train-loss:  2.031879792508391 	 ± 0.20973161275335758
	data : 0.11666512489318848
	model : 0.0702672004699707
			 train-loss:  2.0348854247404606 	 ± 0.21074814333764874
	data : 0.11609444618225098
	model : 0.07005100250244141
			 train-loss:  2.036201719081763 	 ± 0.21008556256289015
	data : 0.11605854034423828
	model : 0.06936602592468262
			 train-loss:  2.033243246078491 	 ± 0.2110949735308497
	data : 0.11634116172790528
	model : 0.06899094581604004
			 train-loss:  2.0290602646251714 	 ± 0.21417194574516016
	data : 0.11647214889526367
	model : 0.06859302520751953
			 train-loss:  2.0365302258846807 	 ± 0.22595520044357734
	data : 0.11702995300292969
	model : 0.06891241073608398
			 train-loss:  2.0390086243453536 	 ± 0.22624455021021858
	data : 0.11662697792053223
	model : 0.06859388351440429
			 train-loss:  2.0404937106829424 	 ± 0.22565810925961727
	data : 0.11689438819885253
	model : 0.06903119087219238
			 train-loss:  2.043058629263015 	 ± 0.22609911717191788
	data : 0.11661810874938965
	model : 0.06933679580688476
			 train-loss:  2.0394677958398497 	 ± 0.22801845950867738
	data : 0.11630206108093262
	model : 0.06868762969970703
			 train-loss:  2.0356898029273918 	 ± 0.23025957060804705
	data : 0.11672649383544922
	model : 0.068670654296875
			 train-loss:  2.0366987265922405 	 ± 0.22942856627780925
	data : 0.11691656112670898
	model : 0.06925015449523926
			 train-loss:  2.037663215890937 	 ± 0.22859357025159793
	data : 0.11648597717285156
	model : 0.06935172080993653
			 train-loss:  2.0361547058278866 	 ± 0.2280965045879877
	data : 0.11637072563171387
	model : 0.06938290596008301
			 train-loss:  2.036384782275638 	 ± 0.22707953984155013
	data : 0.11648778915405274
	model : 0.07091455459594727
			 train-loss:  2.0360040898833955 	 ± 0.22609909651304344
	data : 0.11505370140075684
	model : 0.07075920104980468
			 train-loss:  2.0347171515490103 	 ± 0.22550809413860926
	data : 0.11532526016235352
	model : 0.07045674324035645
			 train-loss:  2.036307496982708 	 ± 0.22515242196559238
	data : 0.11573190689086914
	model : 0.07051057815551758
			 train-loss:  2.038860900505729 	 ± 0.22582308167839032
	data : 0.1157351016998291
	model : 0.07048921585083008
			 train-loss:  2.0386136986058334 	 ± 0.22486322617333623
	data : 0.11558561325073242
	model : 0.0698080062866211
			 train-loss:  2.0386250477570753 	 ± 0.22390024343857623
	data : 0.11635832786560059
	model : 0.06973176002502442
			 train-loss:  2.0385161345287903 	 ± 0.2229526074976342
	data : 0.11622767448425293
	model : 0.06939358711242676
			 train-loss:  2.0379739548979687 	 ± 0.222091961065416
	data : 0.11658124923706055
	model : 0.06906142234802246
			 train-loss:  2.0342259615659715 	 ± 0.22491208476080926
	data : 0.11699457168579101
	model : 0.06887154579162598
			 train-loss:  2.0334962704950126 	 ± 0.22412335492404656
	data : 0.11720328330993653
	model : 0.06806268692016601
			 train-loss:  2.031967675099607 	 ± 0.22383537589828661
	data : 0.11792011260986328
	model : 0.06806483268737792
			 train-loss:  2.0358765396645397 	 ± 0.22706608077196386
	data : 0.11786794662475586
	model : 0.06869654655456543
			 train-loss:  2.036630690097809 	 ± 0.22630325225937234
	data : 0.11723017692565918
	model : 0.06886425018310546
			 train-loss:  2.0383016128540037 	 ± 0.22616291155181237
	data : 0.11715078353881836
	model : 0.06883654594421387
			 train-loss:  2.036588610164703 	 ± 0.22607633696478982
	data : 0.11724944114685058
	model : 0.06944570541381836
			 train-loss:  2.03653870229646 	 ± 0.22518521046891685
	data : 0.11668009757995605
	model : 0.06946983337402343
			 train-loss:  2.0410744566470385 	 ± 0.2300543400569544
	data : 0.116737699508667
	model : 0.06861648559570313
			 train-loss:  2.0412490367889404 	 ± 0.2291694335616034
	data : 0.11736326217651367
	model : 0.06879143714904785
			 train-loss:  2.0445195711576023 	 ± 0.2312887260228119
	data : 0.11713953018188476
	model : 0.06904335021972656
			 train-loss:  2.0433546814299723 	 ± 0.23078675409559832
	data : 0.1168668270111084
	model : 0.06905713081359863
			 train-loss:  2.040648411620747 	 ± 0.23198804343084453
	data : 0.11684036254882812
	model : 0.06898846626281738
			 train-loss:  2.043013882816286 	 ± 0.23270668706027947
	data : 0.11699471473693848
	model : 0.06988735198974609
			 train-loss:  2.0441341827164834 	 ± 0.23219647791537
	data : 0.11624164581298828
	model : 0.06909184455871582
			 train-loss:  2.046720333452578 	 ± 0.2332639020649693
	data : 0.11693177223205567
	model : 0.06905579566955566
			 train-loss:  2.048815029508927 	 ± 0.23367564246201933
	data : 0.11705198287963867
	model : 0.06919879913330078
			 train-loss:  2.0493221839849096 	 ± 0.23289635951292156
	data : 0.1168440818786621
	model : 0.06866836547851562
			 train-loss:  2.05189874033997 	 ± 0.234002477900064
	data : 0.11717681884765625
	model : 0.06839132308959961
			 train-loss:  2.0511645172997346 	 ± 0.23331870216971945
	data : 0.11741290092468262
	model : 0.06901631355285645
			 train-loss:  2.051314502102988 	 ± 0.2324906525618126
	data : 0.11671366691589355
	model : 0.06928839683532714
			 train-loss:  2.0523494101585227 	 ± 0.23198814959122066
	data : 0.11662225723266602
	model : 0.06915740966796875
			 train-loss:  2.052818817152104 	 ± 0.23123703503380214
	data : 0.1169506549835205
	model : 0.0698404312133789
			 train-loss:  2.0540859182397804 	 ± 0.2309212717362722
	data : 0.1162867546081543
	model : 0.07009773254394532
			 train-loss:  2.0532817997866206 	 ± 0.23031888440023152
	data : 0.1162271499633789
	model : 0.07040591239929199
			 train-loss:  2.050597911045469 	 ± 0.23177190911364112
	data : 0.11603155136108398
	model : 0.07050414085388183
			 train-loss:  2.0537336003290463 	 ± 0.2340427401378564
	data : 0.11584219932556153
	model : 0.07019386291503907
			 train-loss:  2.056895316052599 	 ± 0.23635325149966932
	data : 0.11601686477661133
	model : 0.06985411643981934
			 train-loss:  2.0580425455763534 	 ± 0.23596372532957363
	data : 0.1163172721862793
	model : 0.0703470230102539
			 train-loss:  2.057919718275134 	 ± 0.2351753148696167
	data : 0.11580982208251953
	model : 0.07027363777160645
			 train-loss:  2.0578268877665202 	 ± 0.23439282527527583
	data : 0.11577019691467286
	model : 0.06978335380554199
			 train-loss:  2.059457155252924 	 ± 0.23446710071013716
	data : 0.1160916805267334
	model : 0.06985893249511718
			 train-loss:  2.057608822458669 	 ± 0.23479567992461306
	data : 0.11599903106689453
	model : 0.0692605972290039
			 train-loss:  2.056356398108738 	 ± 0.23453595272845307
	data : 0.11664447784423829
	model : 0.06794342994689942
			 train-loss:  2.0549044980631246 	 ± 0.2344620442173743
	data : 0.11783604621887207
	model : 0.0680084228515625
			 train-loss:  2.055570428602157 	 ± 0.23385055609523261
	data : 0.11798486709594727
	model : 0.06786551475524902
			 train-loss:  2.0562483286246276 	 ± 0.23325256865225008
	data : 0.11801266670227051
	model : 0.06723170280456543
			 train-loss:  2.0565227824411574 	 ± 0.2325338074114608
	data : 0.1185112476348877
	model : 0.06718926429748535
			 train-loss:  2.0552777889408644 	 ± 0.23232110387362587
	data : 0.1186720848083496
	model : 0.06801333427429199
			 train-loss:  2.0570983999180346 	 ± 0.23271732320486777
	data : 0.11797304153442383
	model : 0.06788458824157714
			 train-loss:  2.0550827838480474 	 ± 0.23337703327642162
	data : 0.11802411079406738
	model : 0.06824407577514649
			 train-loss:  2.0542337664906283 	 ± 0.23289886561991874
	data : 0.11786680221557617
	model : 0.06913547515869141
			 train-loss:  2.054978322835616 	 ± 0.23237105629931748
	data : 0.11712155342102051
	model : 0.06989078521728516
			 train-loss:  2.057437993997445 	 ± 0.23376299799998995
	data : 0.11644706726074219
	model : 0.07023429870605469
			 train-loss:  2.0566074223053166 	 ± 0.23329033972107255
	data : 0.11581864356994628
	model : 0.07024259567260742
			 train-loss:  2.058955649173621 	 ± 0.23451836100784115
	data : 0.11560068130493165
	model : 0.0701296329498291
			 train-loss:  2.058346808674824 	 ± 0.23394167332985283
	data : 0.1157266616821289
	model : 0.07009415626525879
			 train-loss:  2.0587236195981147 	 ± 0.23329071878066135
	data : 0.11579508781433105
	model : 0.06936049461364746
			 train-loss:  2.0592900344303677 	 ± 0.2327105104209052
	data : 0.11640782356262207
	model : 0.06916117668151855
			 train-loss:  2.058265190152727 	 ± 0.23240093392436215
	data : 0.11682291030883789
	model : 0.0693206787109375
			 train-loss:  2.0598085971439586 	 ± 0.23258345348201745
	data : 0.11684765815734863
	model : 0.06933736801147461
			 train-loss:  2.0606125230677645 	 ± 0.23213915634463375
	data : 0.11676211357116699
	model : 0.06839542388916016
			 train-loss:  2.0619602792484817 	 ± 0.23213335510934213
	data : 0.11757383346557618
	model : 0.06904659271240235
			 train-loss:  2.064684001696592 	 ± 0.23420167719005155
	data : 0.11677165031433105
	model : 0.06932559013366699
			 train-loss:  2.065882190205585 	 ± 0.23405888512132964
	data : 0.1166426658630371
	model : 0.06901860237121582
			 train-loss:  2.0657040044239587 	 ± 0.23340102251335185
	data : 0.11691193580627442
	model : 0.06906375885009766
			 train-loss:  2.068126791580157 	 ± 0.23493349367801267
	data : 0.11673641204833984
	model : 0.070090913772583
			 train-loss:  2.066199198281024 	 ± 0.2356604883702098
	data : 0.11582212448120117
	model : 0.06956491470336915
			 train-loss:  2.0656289685977978 	 ± 0.235120012308089
	data : 0.11641778945922851
	model : 0.06899218559265137
			 train-loss:  2.0658147994366436 	 ± 0.2344754407822325
	data : 0.11686363220214843
	model : 0.0682408332824707
			 train-loss:  2.066990585459603 	 ± 0.2343517816325248
	data : 0.11755013465881348
	model : 0.06818323135375977
			 train-loss:  2.0696497022776312 	 ± 0.23641085076606894
	data : 0.11775441169738769
	model : 0.06804442405700684
			 train-loss:  2.070546860878284 	 ± 0.2360692434112727
	data : 0.11776657104492187
	model : 0.06786437034606933
			 train-loss:  2.07254147334177 	 ± 0.23695620374413945
	data : 0.11790809631347657
	model : 0.06814050674438477
			 train-loss:  2.0707865221344908 	 ± 0.23750095207268662
	data : 0.11752195358276367
	model : 0.06912474632263184
			 train-loss:  2.069053186597051 	 ± 0.23802231056346962
	data : 0.11674599647521973
	model : 0.0692711353302002
			 train-loss:  2.0711588378875487 	 ± 0.23910305843739188
	data : 0.1165694236755371
	model : 0.06939992904663086
			 train-loss:  2.072360490732652 	 ± 0.23902537001259816
	data : 0.11657204627990722
	model : 0.07015514373779297
			 train-loss:  2.071828622133174 	 ± 0.23849974264551338
	data : 0.11580090522766114
	model : 0.06999263763427735
			 train-loss:  2.072407880788127 	 ± 0.23800051520232204
	data : 0.11595416069030762
	model : 0.06972789764404297
			 train-loss:  2.0740930274913185 	 ± 0.23850120460658877
	data : 0.11627626419067383
	model : 0.06958951950073242
			 train-loss:  2.074024580536088 	 ± 0.23787790757125263
	data : 0.11639370918273925
	model : 0.06952462196350098
			 train-loss:  2.0737956607093415 	 ± 0.2372787177684025
	data : 0.11650786399841309
	model : 0.06980586051940918
			 train-loss:  2.0755389397626094 	 ± 0.23789276174905946
	data : 0.11627240180969238
	model : 0.06982216835021973
			 train-loss:  2.076152322833071 	 ± 0.23743180886144485
	data : 0.11630210876464844
	model : 0.06993227005004883
			 train-loss:  2.075885254908831 	 ± 0.2368514391449897
	data : 0.1161931037902832
	model : 0.06976447105407715
			 train-loss:  2.077328936786068 	 ± 0.23710505950374997
	data : 0.11631317138671875
	model : 0.06953997611999511
			 train-loss:  2.078697804267031 	 ± 0.23727768335736615
	data : 0.11659502983093262
	model : 0.06940264701843261
			 train-loss:  2.0773084091417715 	 ± 0.23747977716233143
	data : 0.11677746772766114
	model : 0.06944313049316406
			 train-loss:  2.0781337783564275 	 ± 0.23716687951011398
	data : 0.11686296463012695
	model : 0.06955723762512207
			 train-loss:  2.0775878828763963 	 ± 0.236698522314844
	data : 0.11660208702087402
	model : 0.0696263313293457
			 train-loss:  2.0782997068481066 	 ± 0.23632348988911547
	data : 0.11665220260620117
	model : 0.06934828758239746
			 train-loss:  2.0787925348423495 	 ± 0.23584132689478898
	data : 0.11696105003356934
	model : 0.06913084983825683
			 train-loss:  2.0815679938922376 	 ± 0.23854386741929978
	data : 0.11711659431457519
	model : 0.06917724609375
			 train-loss:  2.0830304394750034 	 ± 0.23886901264188654
	data : 0.11713051795959473
	model : 0.06821627616882324
			 train-loss:  2.0828131530343033 	 ± 0.23830590225518233
	data : 0.11806116104125977
	model : 0.06840152740478515
			 train-loss:  2.087006346114631 	 ± 0.24519076830569808
	data : 0.11787824630737305
	model : 0.06901907920837402
			 train-loss:  2.0872012653212617 	 ± 0.24461380158195314
	data : 0.11714410781860352
	model : 0.06923937797546387
			 train-loss:  2.0866320786567836 	 ± 0.24416244956362487
	data : 0.11703634262084961
	model : 0.06833534240722657
			 train-loss:  2.0860462816138017 	 ± 0.2437241022120484
	data : 0.117732572555542
	model : 0.06921610832214356
			 train-loss:  2.0874559322992963 	 ± 0.24399565786119282
	data : 0.11702980995178222
	model : 0.06946048736572266
			 train-loss:  2.0858627326115613 	 ± 0.24450924289588305
	data : 0.1169358253479004
	model : 0.06844534873962402
			 train-loss:  2.08732514898732 	 ± 0.2448551078485606
	data : 0.11790170669555664
	model : 0.0686112880706787
			 train-loss:  2.0884429322721814 	 ± 0.24482122195032385
	data : 0.1177030086517334
	model : 0.06958656311035157
			 train-loss:  2.08966543072852 	 ± 0.24489932276883689
	data : 0.11697211265563964
	model : 0.06951169967651367
			 train-loss:  2.089180553791135 	 ± 0.24443206470779716
	data : 0.11690597534179688
	model : 0.0683563232421875
			 train-loss:  2.089929023274669 	 ± 0.24411241678642578
	data : 0.11767396926879883
	model : 0.06913957595825196
			 train-loss:  2.091718363871772 	 ± 0.24496496791716554
	data : 0.11690011024475097
	model : 0.06802968978881836
			 train-loss:  2.091519658718634 	 ± 0.2444200036357283
	data : 0.11787915229797363
	model : 0.0679558277130127
			 train-loss:  2.089240311487625 	 ± 0.24617260200984653
	data : 0.11800088882446289
	model : 0.06746530532836914
			 train-loss:  2.088703614473343 	 ± 0.24574086490406294
	data : 0.1184814453125
	model : 0.06810965538024902
			 train-loss:  2.0879748293716984 	 ± 0.24542243035845895
	data : 0.11786718368530273
	model : 0.06830286979675293
			 train-loss:  2.0874206976847605 	 ± 0.24500757933331452
	data : 0.1178309440612793
	model : 0.07012720108032226
			 train-loss:  2.089209638903494 	 ± 0.24590647784198627
	data : 0.11605277061462402
	model : 0.0700960636138916
			 train-loss:  2.089350126683712 	 ± 0.2453659345062272
	data : 0.11588940620422364
	model : 0.06984438896179199
			 train-loss:  2.090321576860216 	 ± 0.24525141952480092
	data : 0.11603779792785644
	model : 0.06972002983093262
			 train-loss:  2.089929479413328 	 ± 0.2447788953989654
	data : 0.1161102294921875
	model : 0.0696375846862793
			 train-loss:  2.0903641179794783 	 ± 0.24432652576674682
	data : 0.11597089767456055
	model : 0.06807804107666016
			 train-loss:  2.088368640134209 	 ± 0.24563698329958447
	data : 0.11760144233703614
	model : 0.06734218597412109
			 train-loss:  2.0877787738908324 	 ± 0.24526185135401157
	data : 0.11804356575012206
	model : 0.06805720329284667
			 train-loss:  2.0885236994079923 	 ± 0.24498758055330488
	data : 0.11737227439880371
	model : 0.06741280555725097
			 train-loss:  2.0872586511430287 	 ± 0.24520842628271317
	data : 0.11815910339355469
	model : 0.06679887771606445
			 train-loss:  2.0899115374376036 	 ± 0.24797930677513486
	data : 0.11868886947631836
	model : 0.06678180694580078
			 train-loss:  2.091192998087969 	 ± 0.24821521262173482
	data : 0.11835823059082032
	model : 0.06644749641418457
			 train-loss:  2.0898955836255326 	 ± 0.2484747533127876
	data : 0.11876797676086426
	model : 0.06599736213684082
			 train-loss:  2.090421534599142 	 ± 0.24807601860944753
	data : 0.11897892951965332
	model : 0.06635127067565919
			 train-loss:  2.0890340128187406 	 ± 0.24846200307012256
	data : 0.11839094161987304
	model : 0.06644263267517089
			 train-loss:  2.087975367212094 	 ± 0.24847008075582233
	data : 0.11843228340148926
	model : 0.0665830135345459
			 train-loss:  2.0878730120779085 	 ± 0.24795254226986538
	data : 0.11864371299743652
	model : 0.06745457649230957
			 train-loss:  2.086030338099811 	 ± 0.2490609147677118
	data : 0.1180201530456543
	model : 0.06775288581848145
			 train-loss:  2.086763821542263 	 ± 0.24880003389037053
	data : 0.11797795295715333
	model : 0.06827640533447266
			 train-loss:  2.085737689896738 	 ± 0.24879170288152663
	data : 0.11783528327941895
	model : 0.06881237030029297
			 train-loss:  2.086546031404133 	 ± 0.2485940680361412
	data : 0.11733059883117676
	model : 0.06928629875183105
			 train-loss:  2.086707723974691 	 ± 0.2480947817113339
	data : 0.1168933391571045
	model : 0.069423246383667
			 train-loss:  2.0863472013199917 	 ± 0.24764964514578305
	data : 0.11681103706359863
	model : 0.06887688636779785
			 train-loss:  2.0865985534629043 	 ± 0.24717490609382164
	data : 0.11718554496765136
	model : 0.06883883476257324
			 train-loss:  2.0865485479192034 	 ± 0.24667324827538634
	data : 0.11710166931152344
	model : 0.0687286376953125
			 train-loss:  2.0848940490228443 	 ± 0.2475373439888354
	data : 0.11706156730651855
	model : 0.0684272289276123
			 train-loss:  2.083946921171681 	 ± 0.2474858233436319
	data : 0.11727595329284668
	model : 0.06830158233642578
			 train-loss:  2.086280253996332 	 ± 0.24970677510860828
	data : 0.11734919548034668
	model : 0.06876416206359863
			 train-loss:  2.0864975690841674 	 ± 0.24923045334991334
	data : 0.11693105697631836
	model : 0.06821465492248535
			 train-loss:  2.0871960204910947 	 ± 0.2489785213366464
	data : 0.11729145050048828
	model : 0.06776361465454102
			 train-loss:  2.087620148583064 	 ± 0.24857486168362689
	data : 0.11757016181945801
	model : 0.06781034469604492
			 train-loss:  2.0876462883628877 	 ± 0.24808346766290645
	data : 0.11735525131225585
	model : 0.06718330383300782
			 train-loss:  2.087696940880122 	 ± 0.24759594361050574
	data : 0.11776056289672851
	model : 0.06724023818969727
			 train-loss:  2.0873224178949994 	 ± 0.24718206318627853
	data : 0.11760973930358887
	model : 0.06744656562805176
			 train-loss:  2.0927249980159104 	 ± 0.26134883173791057
	data : 0.1166346549987793
	model : 0.05894169807434082
#epoch  25    val-loss:  2.4137775019595495  train-loss:  2.0927249980159104  lr:  0.00125
			 train-loss:  1.9769020080566406 	 ± 0.0
	data : 5.721634864807129
	model : 0.07264113426208496
			 train-loss:  2.0318115949630737 	 ± 0.054909586906433105
	data : 2.9291012287139893
	model : 0.06968867778778076
			 train-loss:  2.0665369033813477 	 ± 0.06649613456669427
	data : 1.9924196402231853
	model : 0.06857442855834961
			 train-loss:  2.215571403503418 	 ± 0.26448090406329916
	data : 1.523968517780304
	model : 0.06879985332489014
			 train-loss:  2.154047894477844 	 ± 0.26664712175943595
	data : 1.2422361850738526
	model : 0.06881747245788575
			 train-loss:  2.1826149423917136 	 ± 0.25165642071031297
	data : 0.12121925354003907
	model : 0.0673938274383545
			 train-loss:  2.180527397564479 	 ± 0.2330446780988561
	data : 0.11780214309692383
	model : 0.06796021461486816
			 train-loss:  2.115187644958496 	 ± 0.2782194765570918
	data : 0.11735568046569825
	model : 0.0684354305267334
			 train-loss:  2.116635322570801 	 ± 0.2623397950410026
	data : 0.11714692115783691
	model : 0.06848740577697754
			 train-loss:  2.0771723985671997 	 ± 0.27560089367446844
	data : 0.11723465919494629
	model : 0.06841907501220704
			 train-loss:  2.0378179875287143 	 ± 0.29075500299645024
	data : 0.11739983558654785
	model : 0.06940398216247559
			 train-loss:  2.0444143315156302 	 ± 0.2792350773346857
	data : 0.11661114692687988
	model : 0.06856589317321778
			 train-loss:  2.0502250286249013 	 ± 0.2690344496541179
	data : 0.1172872543334961
	model : 0.06876559257507324
			 train-loss:  2.0516290068626404 	 ± 0.25929749964352516
	data : 0.11706938743591308
	model : 0.06788268089294433
			 train-loss:  2.042827335993449 	 ± 0.2526606792542369
	data : 0.11797013282775878
	model : 0.06797027587890625
			 train-loss:  2.0230672284960747 	 ± 0.25632890568223743
	data : 0.11793951988220215
	model : 0.06786880493164063
			 train-loss:  2.0582961264778588 	 ± 0.2858264174008033
	data : 0.11805405616760253
	model : 0.06876225471496582
			 train-loss:  2.0662146475580006 	 ± 0.2796855084750057
	data : 0.11730093955993652
	model : 0.06883306503295898
			 train-loss:  2.083765061278092 	 ± 0.28222550960403325
	data : 0.11732983589172363
	model : 0.06988778114318847
			 train-loss:  2.073575633764267 	 ± 0.27864195769091193
	data : 0.11628413200378418
	model : 0.07012639045715333
			 train-loss:  2.0778853495915732 	 ± 0.27260888998985994
	data : 0.11615900993347168
	model : 0.06924934387207031
			 train-loss:  2.0841510891914368 	 ± 0.26788444172257625
	data : 0.11701827049255371
	model : 0.06840863227844238
			 train-loss:  2.0852470449779346 	 ± 0.2620465770772568
	data : 0.11765408515930176
	model : 0.06752853393554688
			 train-loss:  2.0788766195376716 	 ± 0.25834205665098897
	data : 0.11844468116760254
	model : 0.06696610450744629
			 train-loss:  2.076999611854553 	 ± 0.2532894575253694
	data : 0.11900010108947753
	model : 0.06628904342651368
			 train-loss:  2.0892102397405186 	 ± 0.2557645675246295
	data : 0.11939897537231445
	model : 0.06701998710632324
			 train-loss:  2.088246632505346 	 ± 0.2510315900393858
	data : 0.11861944198608398
	model : 0.0679361343383789
			 train-loss:  2.093764445611409 	 ± 0.24816991385597362
	data : 0.11792144775390626
	model : 0.06883177757263184
			 train-loss:  2.0939572720692077 	 ± 0.24385572075693573
	data : 0.11706361770629883
	model : 0.06846852302551269
			 train-loss:  2.0831372419993084 	 ± 0.24673577756724704
	data : 0.11735119819641113
	model : 0.06905660629272461
			 train-loss:  2.077487918638414 	 ± 0.2446878941978846
	data : 0.11680364608764648
	model : 0.06922383308410644
			 train-loss:  2.082837011665106 	 ± 0.242668821803463
	data : 0.11666727066040039
	model : 0.06899967193603515
			 train-loss:  2.0859836267702505 	 ± 0.23962576121730028
	data : 0.11687026023864747
	model : 0.06892337799072265
			 train-loss:  2.0797093475566193 	 ± 0.2388111475039993
	data : 0.11701488494873047
	model : 0.06961159706115723
			 train-loss:  2.069965069634574 	 ± 0.24213559453910855
	data : 0.11641855239868164
	model : 0.07022805213928222
			 train-loss:  2.0746367838647632 	 ± 0.24034333118637236
	data : 0.11570749282836915
	model : 0.06927804946899414
			 train-loss:  2.070127925357303 	 ± 0.2386117680635397
	data : 0.11656184196472168
	model : 0.06970796585083008
			 train-loss:  2.0665120890266016 	 ± 0.2364762552736768
	data : 0.11635456085205079
	model : 0.06900200843811036
			 train-loss:  2.0812337398529053 	 ± 0.2504451499515747
	data : 0.11692414283752442
	model : 0.06915926933288574
			 train-loss:  2.0741312205791473 	 ± 0.25124109780694737
	data : 0.1167271614074707
	model : 0.06870856285095214
			 train-loss:  2.081784364653797 	 ± 0.25283463102282844
	data : 0.11736345291137695
	model : 0.06985230445861816
			 train-loss:  2.0732899052756175 	 ± 0.2556593591705137
	data : 0.11651644706726075
	model : 0.06958627700805664
			 train-loss:  2.076842518739922 	 ± 0.2537158899912753
	data : 0.11657295227050782
	model : 0.069842529296875
			 train-loss:  2.0786413767121057 	 ± 0.2510934123421965
	data : 0.11617660522460938
	model : 0.07032976150512696
			 train-loss:  2.0868012481265596 	 ± 0.2541190888244405
	data : 0.11582088470458984
	model : 0.07022476196289062
			 train-loss:  2.0902798745943154 	 ± 0.25242268518752353
	data : 0.11592068672180175
	model : 0.07082114219665528
			 train-loss:  2.0912386001424585 	 ± 0.24980754142091444
	data : 0.11501822471618653
	model : 0.07089958190917969
			 train-loss:  2.093680202960968 	 ± 0.24775777559103687
	data : 0.11489124298095703
	model : 0.07150764465332031
			 train-loss:  2.085639408656529 	 ± 0.2514648927890165
	data : 0.11457066535949707
	model : 0.07119164466857911
			 train-loss:  2.085865650177002 	 ± 0.2489425807999456
	data : 0.11495471000671387
	model : 0.07109212875366211
			 train-loss:  2.0834468088898004 	 ± 0.24708258321575846
	data : 0.1150350570678711
	model : 0.07025718688964844
			 train-loss:  2.0935996977182536 	 ± 0.25521149462079135
	data : 0.11596136093139649
	model : 0.070147705078125
			 train-loss:  2.0916752702784986 	 ± 0.25317298849547976
	data : 0.11595449447631836
	model : 0.06945285797119141
			 train-loss:  2.090790649255117 	 ± 0.2509005064009566
	data : 0.11636877059936523
	model : 0.06932506561279297
			 train-loss:  2.089143018289046 	 ± 0.24890378151292655
	data : 0.11647834777832031
	model : 0.06884918212890626
			 train-loss:  2.090788553868021 	 ± 0.24697310690210725
	data : 0.11691203117370605
	model : 0.06880331039428711
			 train-loss:  2.0976279681189016 	 ± 0.2500903039881821
	data : 0.11696138381958007
	model : 0.06876997947692871
			 train-loss:  2.0950095304127396 	 ± 0.24871188076155065
	data : 0.11707696914672852
	model : 0.06915774345397949
			 train-loss:  2.087802917270337 	 ± 0.2526289998653507
	data : 0.11676197052001953
	model : 0.06918778419494628
			 train-loss:  2.0851738770802815 	 ± 0.25132751673268744
	data : 0.11695704460144044
	model : 0.07011570930480956
			 train-loss:  2.0856251833868806 	 ± 0.24928345506904037
	data : 0.1160965919494629
	model : 0.07018857002258301
			 train-loss:  2.0828838329161368 	 ± 0.24819017286862194
	data : 0.11610927581787109
	model : 0.07031025886535644
			 train-loss:  2.0828455308127025 	 ± 0.2462127151253567
	data : 0.1160426139831543
	model : 0.06957859992980957
			 train-loss:  2.086684411391616 	 ± 0.2461746004223936
	data : 0.11660275459289551
	model : 0.0694692611694336
			 train-loss:  2.090654628093426 	 ± 0.24632986861282744
	data : 0.11640839576721192
	model : 0.0681044578552246
			 train-loss:  2.0921073266954133 	 ± 0.24473701409979448
	data : 0.1175844669342041
	model : 0.0679891586303711
			 train-loss:  2.0882966589571823 	 ± 0.2448686000570289
	data : 0.11761035919189453
	model : 0.06769695281982421
			 train-loss:  2.088328379042008 	 ± 0.24306156569323442
	data : 0.11781835556030273
	model : 0.06837716102600097
			 train-loss:  2.087743662405705 	 ± 0.24134199215644364
	data : 0.11727523803710938
	model : 0.06836013793945313
			 train-loss:  2.081814285687038 	 ± 0.2446216264892716
	data : 0.11734142303466796
	model : 0.06950020790100098
			 train-loss:  2.0913180599749928 	 ± 0.2555766963259906
	data : 0.11661462783813477
	model : 0.06955022811889648
			 train-loss:  2.097097893555959 	 ± 0.25842618475172136
	data : 0.11639957427978516
	model : 0.0697598934173584
			 train-loss:  2.098568031232651 	 ± 0.25695302356954575
	data : 0.1161919116973877
	model : 0.06993522644042968
			 train-loss:  2.098558106937924 	 ± 0.2552109631722184
	data : 0.11605172157287598
	model : 0.06987438201904297
			 train-loss:  2.1006570498148602 	 ± 0.2541460448426487
	data : 0.11616935729980468
	model : 0.07012519836425782
			 train-loss:  2.098774343729019 	 ± 0.2529944358333274
	data : 0.11585531234741211
	model : 0.07065958976745605
			 train-loss:  2.0971725792079776 	 ± 0.2517338371784882
	data : 0.115531587600708
	model : 0.06993436813354492
			 train-loss:  2.0943541725476584 	 ± 0.25133470291117127
	data : 0.1162527084350586
	model : 0.06965231895446777
			 train-loss:  2.0963634644882587 	 ± 0.2503685871478096
	data : 0.1165069580078125
	model : 0.06973543167114257
			 train-loss:  2.0918318748474123 	 ± 0.252038019218688
	data : 0.11627554893493652
	model : 0.0693441390991211
			 train-loss:  2.0892153860610208 	 ± 0.25156829616498194
	data : 0.11663918495178223
	model : 0.06877508163452148
			 train-loss:  2.088159286394352 	 ± 0.25021023720931357
	data : 0.11715412139892578
	model : 0.06935877799987793
			 train-loss:  2.086962083736098 	 ± 0.2489345573824844
	data : 0.11659164428710937
	model : 0.0687777042388916
			 train-loss:  2.087318819193613 	 ± 0.24746970974665877
	data : 0.11720376014709473
	model : 0.06873841285705566
			 train-loss:  2.087995637164396 	 ± 0.24608789242521692
	data : 0.11722235679626465
	model : 0.06857366561889648
			 train-loss:  2.084048016126766 	 ± 0.247345282896692
	data : 0.11736774444580078
	model : 0.06874146461486816
			 train-loss:  2.083658204681572 	 ± 0.2459462179223017
	data : 0.11736984252929687
	model : 0.06882128715515137
			 train-loss:  2.085908356038007 	 ± 0.24544379745265013
	data : 0.11726536750793456
	model : 0.0697329044342041
			 train-loss:  2.0850960645782814 	 ± 0.24417992924496
	data : 0.1163787841796875
	model : 0.06962947845458985
			 train-loss:  2.085712340142992 	 ± 0.24288917753735495
	data : 0.11656045913696289
	model : 0.06957521438598632
			 train-loss:  2.086154791025015 	 ± 0.24158740206271173
	data : 0.11654953956604004
	model : 0.0690584659576416
			 train-loss:  2.081484846446825 	 ± 0.24436578972232065
	data : 0.11686367988586426
	model : 0.06826915740966796
			 train-loss:  2.08034392838837 	 ± 0.24329468169611873
	data : 0.11772370338439941
	model : 0.06752748489379883
			 train-loss:  2.077837957980785 	 ± 0.2432007943125742
	data : 0.1183666706085205
	model : 0.06761226654052735
			 train-loss:  2.0823292870270578 	 ± 0.24580521094549249
	data : 0.11838774681091309
	model : 0.06789054870605468
			 train-loss:  2.0792683785160384 	 ± 0.24633492426642276
	data : 0.11811065673828125
	model : 0.06837921142578125
			 train-loss:  2.0806409354062425 	 ± 0.2454305895139021
	data : 0.11766953468322754
	model : 0.06857786178588868
			 train-loss:  2.0781343615784937 	 ± 0.24541997051583658
	data : 0.1173408031463623
	model : 0.06845064163208008
			 train-loss:  2.075853448925596 	 ± 0.2452191254798865
	data : 0.11757779121398926
	model : 0.06761927604675293
			 train-loss:  2.07615788936615 	 ± 0.24400875193384297
	data : 0.11828022003173828
	model : 0.0668525218963623
			 train-loss:  2.0749412668813574 	 ± 0.2431024072556616
	data : 0.1191331386566162
	model : 0.06671676635742188
			 train-loss:  2.0736829521609286 	 ± 0.24223810508469235
	data : 0.11951870918273926
	model : 0.06663165092468262
			 train-loss:  2.0789737134303863 	 ± 0.24691050485602037
	data : 0.11946988105773926
	model : 0.0667048454284668
			 train-loss:  2.0771177193293204 	 ± 0.24644147984015213
	data : 0.11915245056152343
	model : 0.06663727760314941
			 train-loss:  2.0759227571033296 	 ± 0.24556769908739065
	data : 0.11928696632385254
	model : 0.06737813949584961
			 train-loss:  2.0737728319078124 	 ± 0.24539747893319602
	data : 0.11840963363647461
	model : 0.06656956672668457
			 train-loss:  2.07324687454188 	 ± 0.24430808941411178
	data : 0.11906929016113281
	model : 0.06742849349975585
			 train-loss:  2.070907241768307 	 ± 0.24437572635070504
	data : 0.11844458580017089
	model : 0.06816835403442383
			 train-loss:  2.06886042804893 	 ± 0.24418040576554947
	data : 0.1180842399597168
	model : 0.06888980865478515
			 train-loss:  2.068646188215776 	 ± 0.24307825183200776
	data : 0.11732187271118164
	model : 0.06796030998229981
			 train-loss:  2.0748770258448146 	 ± 0.25064973169939
	data : 0.11817188262939453
	model : 0.06875596046447754
			 train-loss:  2.076647379568645 	 ± 0.250224376981279
	data : 0.11732087135314942
	model : 0.06862373352050781
			 train-loss:  2.0786893810846108 	 ± 0.2500503199116514
	data : 0.11751632690429688
	model : 0.068701171875
			 train-loss:  2.078411608411555 	 ± 0.24896870278339847
	data : 0.11730680465698243
	model : 0.06904392242431641
			 train-loss:  2.0777276640352995 	 ± 0.24799140764746214
	data : 0.11698193550109863
	model : 0.07008605003356934
			 train-loss:  2.0794802678042443 	 ± 0.2476344168449766
	data : 0.11599497795104981
	model : 0.06930031776428222
			 train-loss:  2.0796264787005563 	 ± 0.24657890760850767
	data : 0.11682963371276855
	model : 0.06888060569763184
			 train-loss:  2.0780330290228632 	 ± 0.24613607254668327
	data : 0.11737709045410157
	model : 0.06885499954223633
			 train-loss:  2.0790495812392034 	 ± 0.24534833258968533
	data : 0.11746025085449219
	model : 0.06859493255615234
			 train-loss:  2.0806457420190174 	 ± 0.24494356978897822
	data : 0.11770548820495605
	model : 0.06914782524108887
			 train-loss:  2.0810100815512915 	 ± 0.24396195541291957
	data : 0.11708011627197265
	model : 0.07006382942199707
			 train-loss:  2.0796170351935213 	 ± 0.24344280193121823
	data : 0.11616330146789551
	model : 0.07048478126525878
			 train-loss:  2.0823985871260726 	 ± 0.24439003915951782
	data : 0.1156090259552002
	model : 0.06957755088806153
			 train-loss:  2.0809258212966304 	 ± 0.24395003137655455
	data : 0.11641530990600586
	model : 0.06979999542236329
			 train-loss:  2.083044686317444 	 ± 0.24411520620237878
	data : 0.11608328819274902
	model : 0.06907973289489747
			 train-loss:  2.083425498197949 	 ± 0.24318183911011154
	data : 0.11696038246154786
	model : 0.06897239685058594
			 train-loss:  2.0834324388053473 	 ± 0.24222255070417478
	data : 0.11698107719421387
	model : 0.06901941299438477
			 train-loss:  2.085404592566192 	 ± 0.24229598325355522
	data : 0.11692953109741211
	model : 0.0695584774017334
			 train-loss:  2.089802920356277 	 ± 0.2464314196595696
	data : 0.11657519340515136
	model : 0.06946983337402343
			 train-loss:  2.0858984241118796 	 ± 0.24945524537065158
	data : 0.1166562557220459
	model : 0.06950702667236328
			 train-loss:  2.085063803286953 	 ± 0.24868344156153685
	data : 0.11667723655700683
	model : 0.06972241401672363
			 train-loss:  2.0854196891640173 	 ± 0.24777315202450956
	data : 0.11646947860717774
	model : 0.0696493148803711
			 train-loss:  2.087553354134237 	 ± 0.24805418326555012
	data : 0.11653800010681152
	model : 0.07010412216186523
			 train-loss:  2.088745480153098 	 ± 0.24750900398008652
	data : 0.11611909866333008
	model : 0.07013511657714844
			 train-loss:  2.0885394908763746 	 ± 0.24660212867699471
	data : 0.11619954109191895
	model : 0.0692814826965332
			 train-loss:  2.0883856012540707 	 ± 0.2457003365830376
	data : 0.11699700355529785
	model : 0.06919264793395996
			 train-loss:  2.0899998741428347 	 ± 0.24552475953314767
	data : 0.11712331771850586
	model : 0.06924724578857422
			 train-loss:  2.0905545068823774 	 ± 0.244719680650777
	data : 0.11702065467834473
	model : 0.06914033889770507
			 train-loss:  2.089758539371353 	 ± 0.2440170220429281
	data : 0.1170508861541748
	model : 0.06821236610412598
			 train-loss:  2.0860062173434666 	 ± 0.2471357916841018
	data : 0.11799607276916504
	model : 0.06905303001403809
			 train-loss:  2.09070501090787 	 ± 0.2524558342351929
	data : 0.11702151298522949
	model : 0.06854853630065919
			 train-loss:  2.0907803908200333 	 ± 0.2515669270760294
	data : 0.11738953590393067
	model : 0.06855840682983398
			 train-loss:  2.0894124757993473 	 ± 0.25121518488997013
	data : 0.11747384071350098
	model : 0.06862006187438965
			 train-loss:  2.0861058880885444 	 ± 0.2534448764173831
	data : 0.11736435890197754
	model : 0.06900920867919921
			 train-loss:  2.084717867292207 	 ± 0.25311803733034
	data : 0.11705431938171387
	model : 0.06890339851379394
			 train-loss:  2.086024677916749 	 ± 0.25274006045985753
	data : 0.11716732978820801
	model : 0.06927652359008789
			 train-loss:  2.0865729698518507 	 ± 0.2519660457205609
	data : 0.11694517135620117
	model : 0.06922821998596192
			 train-loss:  2.088152062248539 	 ± 0.25184215702791257
	data : 0.11699428558349609
	model : 0.06921153068542481
			 train-loss:  2.0864831785227627 	 ± 0.2518154285315362
	data : 0.11691174507141114
	model : 0.06987223625183106
			 train-loss:  2.085219852924347 	 ± 0.2514479523144278
	data : 0.11634106636047363
	model : 0.06911740303039551
			 train-loss:  2.0869704627043366 	 ± 0.25152942611151846
	data : 0.11710810661315918
	model : 0.06834068298339843
			 train-loss:  2.0866987681702565 	 ± 0.2507228910556596
	data : 0.11782846450805665
	model : 0.06750278472900391
			 train-loss:  2.0858020096822503 	 ± 0.2501466375591618
	data : 0.1186676025390625
	model : 0.06668453216552735
			 train-loss:  2.0865964363147684 	 ± 0.24952671256381279
	data : 0.11956467628479003
	model : 0.06566910743713379
			 train-loss:  2.0878225664938648 	 ± 0.2491854783654686
	data : 0.12054853439331055
	model : 0.06562490463256836
			 train-loss:  2.089699835349352 	 ± 0.2494826836498704
	data : 0.12058467864990234
	model : 0.06654257774353027
			 train-loss:  2.088855566492506 	 ± 0.24891034790405278
	data : 0.11969571113586426
	model : 0.06756796836853027
			 train-loss:  2.089450212219093 	 ± 0.2482332535752863
	data : 0.11871490478515626
	model : 0.0683178424835205
			 train-loss:  2.088130465843393 	 ± 0.24800684670574472
	data : 0.11793999671936035
	model : 0.06896162033081055
			 train-loss:  2.0865735739469526 	 ± 0.2480088241491031
	data : 0.11720614433288574
	model : 0.06991014480590821
			 train-loss:  2.0851476592306764 	 ± 0.24789444223805335
	data : 0.11658954620361328
	model : 0.06955142021179199
			 train-loss:  2.084479017022215 	 ± 0.24727374215394798
	data : 0.116825532913208
	model : 0.06940855979919433
			 train-loss:  2.0852066856220457 	 ± 0.2466879899586661
	data : 0.11689720153808594
	model : 0.0695378303527832
			 train-loss:  2.08357993059042 	 ± 0.24681014897867753
	data : 0.11676082611083985
	model : 0.06976871490478516
			 train-loss:  2.0882369554404057 	 ± 0.25318546305443146
	data : 0.1165071964263916
	model : 0.06942787170410156
			 train-loss:  2.0874479312494576 	 ± 0.252625095835486
	data : 0.11655731201171875
	model : 0.06954784393310547
			 train-loss:  2.0868836041696057 	 ± 0.2519725221429499
	data : 0.11662712097167968
	model : 0.06881914138793946
			 train-loss:  2.0890672767446157 	 ± 0.2528014269578717
	data : 0.11731991767883301
	model : 0.06875615119934082
			 train-loss:  2.0898776639848067 	 ± 0.25227115277008194
	data : 0.11744809150695801
	model : 0.06914362907409669
			 train-loss:  2.089182273079367 	 ± 0.25169048519259746
	data : 0.11709003448486328
	model : 0.06947154998779297
			 train-loss:  2.0904169152354637 	 ± 0.2514692458780694
	data : 0.11699652671813965
	model : 0.06975979804992676
			 train-loss:  2.0896815616031024 	 ± 0.250921488327183
	data : 0.11661300659179688
	model : 0.07066459655761718
			 train-loss:  2.0933266402669037 	 ± 0.2547213151557403
	data : 0.11575970649719239
	model : 0.07043285369873047
			 train-loss:  2.0914198030000444 	 ± 0.2552236093964065
	data : 0.11593418121337891
	model : 0.06996655464172363
			 train-loss:  2.0899331760406494 	 ± 0.25524775769440416
	data : 0.11627821922302246
	model : 0.069877290725708
			 train-loss:  2.090747069228779 	 ± 0.2547492162285159
	data : 0.11633601188659667
	model : 0.0701070785522461
			 train-loss:  2.090382458799976 	 ± 0.2540746152300016
	data : 0.11642370223999024
	model : 0.0700103759765625
			 train-loss:  2.091560817836376 	 ± 0.2538444750260693
	data : 0.11654906272888184
	model : 0.07107529640197754
			 train-loss:  2.0919789322261706 	 ± 0.2531958768615922
	data : 0.11548709869384766
	model : 0.07054834365844727
			 train-loss:  2.0901920669608645 	 ± 0.2536208242980978
	data : 0.11602897644042968
	model : 0.07057275772094726
			 train-loss:  2.0915699367365126 	 ± 0.25359392433880784
	data : 0.11602897644042968
	model : 0.07020092010498047
			 train-loss:  2.091449045217954 	 ± 0.2529015080074157
	data : 0.11606016159057617
	model : 0.07008557319641114
			 train-loss:  2.090464756788452 	 ± 0.25255889410560955
	data : 0.11592221260070801
	model : 0.06920170783996582
			 train-loss:  2.093203765542611 	 ± 0.2545824599111637
	data : 0.11674046516418457
	model : 0.06973872184753419
			 train-loss:  2.0944384143159196 	 ± 0.25444522852513596
	data : 0.11623592376708984
	model : 0.06992392539978028
			 train-loss:  2.093318604012971 	 ± 0.25421699871196973
	data : 0.1158409595489502
	model : 0.06907024383544921
			 train-loss:  2.091956609710653 	 ± 0.25421589768951713
	data : 0.1168257713317871
	model : 0.06901741027832031
			 train-loss:  2.0911130372514117 	 ± 0.25380118341852453
	data : 0.11715116500854492
	model : 0.06892423629760742
			 train-loss:  2.0903664398445656 	 ± 0.25333577131700513
	data : 0.11739287376403809
	model : 0.06809921264648437
			 train-loss:  2.0901742037973907 	 ± 0.2526820397012292
	data : 0.11819090843200683
	model : 0.06779088973999023
			 train-loss:  2.090810479293943 	 ± 0.25217226291827316
	data : 0.11855463981628418
	model : 0.06874513626098633
			 train-loss:  2.089174523949623 	 ± 0.2525288709619789
	data : 0.11778054237365723
	model : 0.06868252754211426
			 train-loss:  2.0884633292805965 	 ± 0.25206650917201595
	data : 0.11768441200256348
	model : 0.06832880973815918
			 train-loss:  2.089594826870358 	 ± 0.251906941909154
	data : 0.11799917221069336
	model : 0.06936345100402833
			 train-loss:  2.0889180146730864 	 ± 0.25143697584105884
	data : 0.11696929931640625
	model : 0.06942076683044433
			 train-loss:  2.089300122796273 	 ± 0.25085149046726896
	data : 0.1169349193572998
	model : 0.06859655380249023
			 train-loss:  2.091873967708065 	 ± 0.2527953389170171
	data : 0.11747603416442871
	model : 0.06779870986938477
			 train-loss:  2.090104948992681 	 ± 0.2533756609720883
	data : 0.11832962036132813
	model : 0.06820259094238282
			 train-loss:  2.0900511424146107 	 ± 0.25273937097053517
	data : 0.11773171424865722
	model : 0.06804938316345215
			 train-loss:  2.0914946454763412 	 ± 0.2529277766619254
	data : 0.11800894737243653
	model : 0.06822223663330078
			 train-loss:  2.090752344819444 	 ± 0.2525161208990165
	data : 0.11767091751098632
	model : 0.06911768913269042
			 train-loss:  2.089310952342383 	 ± 0.2527178784281222
	data : 0.11691403388977051
	model : 0.06914143562316895
			 train-loss:  2.088668844382751 	 ± 0.2522597842802319
	data : 0.11692118644714355
	model : 0.06914982795715333
			 train-loss:  2.0883583964086045 	 ± 0.25167961224425195
	data : 0.11701769828796386
	model : 0.06913890838623046
			 train-loss:  2.0899761641897805 	 ± 0.2521260438810198
	data : 0.11691484451293946
	model : 0.06934576034545899
			 train-loss:  2.091274813540931 	 ± 0.2521997069392485
	data : 0.11701250076293945
	model : 0.06917753219604492
			 train-loss:  2.090270049906007 	 ± 0.25200275877077055
	data : 0.11714916229248047
	model : 0.07027368545532227
			 train-loss:  2.0893677920103073 	 ± 0.2517311828086941
	data : 0.11619157791137695
	model : 0.07032122611999511
			 train-loss:  2.0891262001968456 	 ± 0.2511524032622695
	data : 0.1161491870880127
	model : 0.07031807899475098
			 train-loss:  2.087661187989371 	 ± 0.25144726877828244
	data : 0.1162135124206543
	model : 0.06909494400024414
			 train-loss:  2.087149107625699 	 ± 0.25096045205209566
	data : 0.11721134185791016
	model : 0.06916766166687012
			 train-loss:  2.08568648291084 	 ± 0.2512676935023556
	data : 0.11708159446716308
	model : 0.06907453536987304
			 train-loss:  2.08649051077489 	 ± 0.2509503793734735
	data : 0.11720008850097656
	model : 0.06912250518798828
			 train-loss:  2.0858332676308176 	 ± 0.2505470442870001
	data : 0.11726183891296386
	model : 0.06900558471679688
			 train-loss:  2.085273374513138 	 ± 0.25009785069750623
	data : 0.11731104850769043
	model : 0.06985301971435547
			 train-loss:  2.0839861234029136 	 ± 0.25023112283757803
	data : 0.11639299392700195
	model : 0.06976284980773925
			 train-loss:  2.082507093930574 	 ± 0.2505984231853835
	data : 0.116455078125
	model : 0.06969332695007324
			 train-loss:  2.082961294629158 	 ± 0.2501125047012475
	data : 0.11650686264038086
	model : 0.06871204376220703
			 train-loss:  2.08343151932982 	 ± 0.24963738145337883
	data : 0.11722731590270996
	model : 0.06800370216369629
			 train-loss:  2.083834778178822 	 ± 0.24914085999956698
	data : 0.11797928810119629
	model : 0.06801133155822754
			 train-loss:  2.0856281006498034 	 ± 0.24999564858989892
	data : 0.11803722381591797
	model : 0.06793127059936524
			 train-loss:  2.0848936830555 	 ± 0.24967078959414635
	data : 0.11813769340515137
	model : 0.06708683967590331
			 train-loss:  2.0863280264251434 	 ± 0.25002540161439285
	data : 0.11883926391601562
	model : 0.0671802043914795
			 train-loss:  2.086166233888694 	 ± 0.24947838448315446
	data : 0.11889100074768066
	model : 0.06736745834350585
			 train-loss:  2.0868439133961996 	 ± 0.2491299186653806
	data : 0.11869001388549805
	model : 0.06661086082458496
			 train-loss:  2.08734687028733 	 ± 0.24869259453860273
	data : 0.11944355964660644
	model : 0.06631321907043457
			 train-loss:  2.0878427679843314 	 ± 0.24825616826663974
	data : 0.11955909729003907
	model : 0.06682252883911133
			 train-loss:  2.088122967042421 	 ± 0.24774711954647874
	data : 0.11905379295349121
	model : 0.06723737716674805
			 train-loss:  2.086756488641797 	 ± 0.2480651976451577
	data : 0.1186246395111084
	model : 0.06714811325073242
			 train-loss:  2.0874943297842274 	 ± 0.24777704243077764
	data : 0.11862883567810059
	model : 0.06785664558410645
			 train-loss:  2.0877694051503104 	 ± 0.24727533939885127
	data : 0.11793618202209473
	model : 0.06811237335205078
			 train-loss:  2.0883076396481743 	 ± 0.2468774126970292
	data : 0.1177443027496338
	model : 0.06771616935729981
			 train-loss:  2.0893587728426692 	 ± 0.24686678174709833
	data : 0.1178980827331543
	model : 0.0678316593170166
			 train-loss:  2.088199581345941 	 ± 0.246973388806197
	data : 0.11778602600097657
	model : 0.06830325126647949
			 train-loss:  2.0891383916773694 	 ± 0.24686542350389484
	data : 0.11725249290466308
	model : 0.06812458038330078
			 train-loss:  2.0889598904019695 	 ± 0.2463570457022916
	data : 0.11706042289733887
	model : 0.06797618865966797
			 train-loss:  2.0888207899367255 	 ± 0.2458460428764676
	data : 0.11707806587219238
	model : 0.0681386947631836
			 train-loss:  2.0893359750258824 	 ± 0.24545718480093895
	data : 0.11690201759338378
	model : 0.0674583911895752
			 train-loss:  2.088756644077381 	 ± 0.2451061389246424
	data : 0.11737713813781739
	model : 0.06733841896057129
			 train-loss:  2.0882831772168475 	 ± 0.24470446514539326
	data : 0.11748814582824707
	model : 0.06744003295898438
			 train-loss:  2.0879290549092273 	 ± 0.24425786786864664
	data : 0.11772165298461915
	model : 0.06755175590515136
			 train-loss:  2.088142673831341 	 ± 0.2437752382371835
	data : 0.11772541999816895
	model : 0.06799020767211914
			 train-loss:  2.088246490729689 	 ± 0.24327848670074
	data : 0.11744523048400879
	model : 0.06843795776367187
			 train-loss:  2.0866070507002656 	 ± 0.2441208502805905
	data : 0.11721301078796387
	model : 0.06816563606262208
			 train-loss:  2.0867603827495964 	 ± 0.24363390835735493
	data : 0.11763119697570801
	model : 0.06819968223571778
			 train-loss:  2.087343922475489 	 ± 0.2433097157676796
	data : 0.11774721145629882
	model : 0.0679244041442871
			 train-loss:  2.086587198350111 	 ± 0.24310658268727442
	data : 0.1180884838104248
	model : 0.0675267219543457
			 train-loss:  2.0865394232734555 	 ± 0.24261711521047935
	data : 0.11855459213256836
	model : 0.06806221008300781
			 train-loss:  2.087985611344916 	 ± 0.24319816777253073
	data : 0.11811318397521972
	model : 0.06812911033630371
			 train-loss:  2.0871469645500182 	 ± 0.24307179226698786
	data : 0.11789741516113281
	model : 0.06760811805725098
			 train-loss:  2.08697279943413 	 ± 0.24260273196943857
	data : 0.11813941001892089
	model : 0.0682992935180664
			 train-loss:  2.087518195784281 	 ± 0.24227503258864302
	data : 0.11761355400085449
	model : 0.06885504722595215
			 train-loss:  2.0869971301715835 	 ± 0.24193719653787019
	data : 0.11679072380065918
	model : 0.06897573471069336
			 train-loss:  2.08651146503884 	 ± 0.24158401243425554
	data : 0.1165628433227539
	model : 0.06949238777160645
			 train-loss:  2.08705045522428 	 ± 0.24126282524980777
	data : 0.11614427566528321
	model : 0.07034153938293457
			 train-loss:  2.087213570717722 	 ± 0.24080523565766246
	data : 0.11435198783874512
	model : 0.061412382125854495
#epoch  26    val-loss:  2.4260234895505404  train-loss:  2.087213570717722  lr:  0.00125
			 train-loss:  2.1072018146514893 	 ± 0.0
	data : 5.7336907386779785
	model : 0.0772097110748291
			 train-loss:  1.984416902065277 	 ± 0.12278491258621216
	data : 2.9302611351013184
	model : 0.07233595848083496
			 train-loss:  2.014487544695536 	 ± 0.10890015428025204
	data : 1.9934440453847249
	model : 0.0704513390858968
			 train-loss:  2.033016473054886 	 ± 0.0996212640962498
	data : 1.524717092514038
	model : 0.0695338249206543
			 train-loss:  2.139300227165222 	 ± 0.23048744555670167
	data : 1.2433241367340089
	model : 0.0698915958404541
			 train-loss:  2.0897754232088723 	 ± 0.2377686196717703
	data : 0.11949610710144043
	model : 0.06891851425170899
			 train-loss:  2.1054188013076782 	 ± 0.2234411206221214
	data : 0.11698155403137207
	model : 0.06956267356872559
			 train-loss:  2.147660657763481 	 ± 0.2370143743417039
	data : 0.1160421371459961
	model : 0.0698552131652832
			 train-loss:  2.1112897793451944 	 ± 0.24600159135320773
	data : 0.11588449478149414
	model : 0.07042937278747559
			 train-loss:  2.1416568398475646 	 ± 0.2505285012238549
	data : 0.11548175811767578
	model : 0.07012600898742676
			 train-loss:  2.135643102905967 	 ± 0.23962536153053043
	data : 0.1157686710357666
	model : 0.06959052085876465
			 train-loss:  2.1538327435652413 	 ± 0.2372230617619202
	data : 0.11607818603515625
	model : 0.06939311027526855
			 train-loss:  2.165844981486981 	 ± 0.23168400471381426
	data : 0.11615047454833985
	model : 0.06978979110717773
			 train-loss:  2.134837099484035 	 ± 0.24968525420605125
	data : 0.11578550338745117
	model : 0.07004661560058593
			 train-loss:  2.1242981354395547 	 ± 0.24442078697860967
	data : 0.1158179759979248
	model : 0.06981086730957031
			 train-loss:  2.148279659450054 	 ± 0.2542329216408303
	data : 0.11596865653991699
	model : 0.06963071823120118
			 train-loss:  2.1421960031285003 	 ± 0.24783972678390703
	data : 0.11623554229736328
	model : 0.0694808006286621
			 train-loss:  2.157061596711477 	 ± 0.24853332997330782
	data : 0.116477632522583
	model : 0.06900963783264161
			 train-loss:  2.151684415967841 	 ± 0.24297793952468705
	data : 0.1168696403503418
	model : 0.0687863826751709
			 train-loss:  2.1242367029190063 	 ± 0.26533097674695166
	data : 0.11715464591979981
	model : 0.0690467357635498
			 train-loss:  2.1058055730093095 	 ± 0.2717393091885819
	data : 0.11691398620605468
	model : 0.0689394474029541
			 train-loss:  2.139802038669586 	 ± 0.30782582636336886
	data : 0.11702127456665039
	model : 0.06918883323669434
			 train-loss:  2.128661756930144 	 ± 0.30556048573349665
	data : 0.11686015129089355
	model : 0.06893625259399414
			 train-loss:  2.1400194466114044 	 ± 0.30404579202683213
	data : 0.11726174354553223
	model : 0.06835904121398925
			 train-loss:  2.1464663314819337 	 ± 0.2995723379954619
	data : 0.11767868995666504
	model : 0.06754698753356933
			 train-loss:  2.128657423532926 	 ± 0.30695413236785235
	data : 0.11852269172668457
	model : 0.0671966552734375
			 train-loss:  2.1233723472665855 	 ± 0.30241926438253774
	data : 0.11886754035949706
	model : 0.0666593074798584
			 train-loss:  2.13524152977126 	 ± 0.30330639239166785
	data : 0.1193094253540039
	model : 0.06778602600097657
			 train-loss:  2.1318674621910882 	 ± 0.2985653958429597
	data : 0.11797738075256348
	model : 0.06773815155029297
			 train-loss:  2.1134830673535663 	 ± 0.3097926924698993
	data : 0.11800894737243653
	model : 0.06878838539123536
			 train-loss:  2.113136056930788 	 ± 0.3047610039998905
	data : 0.11705584526062011
	model : 0.06933503150939942
			 train-loss:  2.1122724525630474 	 ± 0.2999998542881734
	data : 0.11652956008911133
	model : 0.07057085037231445
			 train-loss:  2.1150797677762583 	 ± 0.2958459651466624
	data : 0.11537065505981445
	model : 0.07013969421386719
			 train-loss:  2.114262612426982 	 ± 0.29150061878563366
	data : 0.11603360176086426
	model : 0.07168040275573731
			 train-loss:  2.1126356499535697 	 ± 0.2874627282066728
	data : 0.1144944190979004
	model : 0.07120847702026367
			 train-loss:  2.111829568942388 	 ± 0.2834821868640721
	data : 0.11497735977172852
	model : 0.07119436264038086
			 train-loss:  2.110153484988857 	 ± 0.27980588544391677
	data : 0.11490716934204101
	model : 0.07041501998901367
			 train-loss:  2.106054425239563 	 ± 0.27722323141998056
	data : 0.11563162803649903
	model : 0.07027978897094726
			 train-loss:  2.0991731454164553 	 ± 0.27691427180624634
	data : 0.11572957038879395
	model : 0.06947226524353027
			 train-loss:  2.096807673573494 	 ± 0.27382968959973636
	data : 0.11669449806213379
	model : 0.06997504234313964
			 train-loss:  2.10186246255549 	 ± 0.2723525073459041
	data : 0.1162506103515625
	model : 0.07014374732971192
			 train-loss:  2.0939942655109225 	 ± 0.2737664062555094
	data : 0.11629605293273926
	model : 0.06926789283752441
			 train-loss:  2.0943881467331287 	 ± 0.27057639102547115
	data : 0.11715359687805176
	model : 0.06849689483642578
			 train-loss:  2.1008225841955706 	 ± 0.2707913756617532
	data : 0.11780591011047363
	model : 0.06846446990966797
			 train-loss:  2.0962473418977527 	 ± 0.26948006189567425
	data : 0.11767640113830566
	model : 0.06820116043090821
			 train-loss:  2.099551980910094 	 ± 0.26745513203161553
	data : 0.11787481307983398
	model : 0.06759696006774903
			 train-loss:  2.09953119146063 	 ± 0.2645946046603105
	data : 0.11835508346557617
	model : 0.06836786270141601
			 train-loss:  2.0950961535175643 	 ± 0.2635834330748272
	data : 0.11774334907531739
	model : 0.06924200057983398
			 train-loss:  2.0986447650559095 	 ± 0.2620358580979747
	data : 0.11707673072814942
	model : 0.06965088844299316
			 train-loss:  2.103666350841522 	 ± 0.2617730600670555
	data : 0.11666555404663086
	model : 0.06979703903198242
			 train-loss:  2.1179953299316705 	 ± 0.27829388596873617
	data : 0.11653099060058594
	model : 0.0704127311706543
			 train-loss:  2.1116516360869775 	 ± 0.2793035575311536
	data : 0.11599092483520508
	model : 0.07027220726013184
			 train-loss:  2.1141353643165446 	 ± 0.2772352152237357
	data : 0.11593284606933593
	model : 0.06965427398681641
			 train-loss:  2.1224655795980385 	 ± 0.28127183432745645
	data : 0.11628403663635253
	model : 0.06889300346374512
			 train-loss:  2.1230318026109174 	 ± 0.27873414592293255
	data : 0.11706271171569824
	model : 0.06881837844848633
			 train-loss:  2.1222980277878896 	 ± 0.2762878343903544
	data : 0.11693811416625977
	model : 0.06907567977905274
			 train-loss:  2.1165057504386233 	 ± 0.2772626628001845
	data : 0.11666932106018066
	model : 0.06909122467041015
			 train-loss:  2.122453412105297 	 ± 0.27850586535653704
	data : 0.11673955917358399
	model : 0.06971845626831055
			 train-loss:  2.121620778310097 	 ± 0.2762083587472134
	data : 0.11619524955749512
	model : 0.07002806663513184
			 train-loss:  2.1301021039485932 	 ± 0.2815378837889947
	data : 0.11598563194274902
	model : 0.06989359855651855
			 train-loss:  2.131310570435446 	 ± 0.2793775233128447
	data : 0.11615843772888183
	model : 0.06937589645385742
			 train-loss:  2.1280275987040613 	 ± 0.2782990345483882
	data : 0.1165226936340332
	model : 0.0695995807647705
			 train-loss:  2.1280565091541837 	 ± 0.27608157087057655
	data : 0.1162872314453125
	model : 0.06961393356323242
			 train-loss:  2.1270737815648317 	 ± 0.2740272296562474
	data : 0.11626439094543457
	model : 0.06960878372192383
			 train-loss:  2.1224006212674653 	 ± 0.27446919129231917
	data : 0.11613831520080567
	model : 0.06972684860229492
			 train-loss:  2.127553777261214 	 ± 0.2755322101994586
	data : 0.11620430946350098
	model : 0.06916041374206543
			 train-loss:  2.124387349655379 	 ± 0.2746754954605852
	data : 0.11661357879638672
	model : 0.06921887397766113
			 train-loss:  2.1246412676923416 	 ± 0.27265626410707183
	data : 0.11654114723205566
	model : 0.06916971206665039
			 train-loss:  2.1264406321705254 	 ± 0.2710796758490945
	data : 0.11652250289916992
	model : 0.06976995468139649
			 train-loss:  2.122256050791059 	 ± 0.2713718044995856
	data : 0.11589527130126953
	model : 0.06928582191467285
			 train-loss:  2.1263487708400675 	 ± 0.27162098737833423
	data : 0.1165616512298584
	model : 0.06981182098388672
			 train-loss:  2.1261325942145453 	 ± 0.26973428569662977
	data : 0.11615667343139649
	model : 0.06883845329284669
			 train-loss:  2.127792188566025 	 ± 0.26825030396924315
	data : 0.11717243194580078
	model : 0.06791081428527831
			 train-loss:  2.1319607818448865 	 ± 0.268801693689431
	data : 0.11796412467956544
	model : 0.0666959285736084
			 train-loss:  2.1234439547856647 	 ± 0.2768729827619476
	data : 0.11901459693908692
	model : 0.06708750724792481
			 train-loss:  2.1230881888615456 	 ± 0.27506267442025795
	data : 0.11844635009765625
	model : 0.06733250617980957
			 train-loss:  2.1231443309164666 	 ± 0.2732711543931094
	data : 0.11834897994995117
	model : 0.06750712394714356
			 train-loss:  2.1233230294325414 	 ± 0.2715182934044395
	data : 0.1180349349975586
	model : 0.06752934455871581
			 train-loss:  2.1302351906329773 	 ± 0.27661466574832927
	data : 0.11804170608520508
	model : 0.06777377128601074
			 train-loss:  2.138470374047756 	 ± 0.28445890531823903
	data : 0.11793828010559082
	model : 0.06762290000915527
			 train-loss:  2.14461765318741 	 ± 0.28799482354973815
	data : 0.11802740097045898
	model : 0.06770873069763184
			 train-loss:  2.142005551152113 	 ± 0.28719716537227513
	data : 0.11807136535644532
	model : 0.0676081657409668
			 train-loss:  2.142431060951876 	 ± 0.28548782288269103
	data : 0.1181955337524414
	model : 0.06863489151000976
			 train-loss:  2.1412857515471324 	 ± 0.28397516363301184
	data : 0.11750650405883789
	model : 0.06902551651000977
			 train-loss:  2.1407307456521427 	 ± 0.2823456039156862
	data : 0.11711468696594238
	model : 0.06902079582214356
			 train-loss:  2.1373042852379553 	 ± 0.2824712872151436
	data : 0.11717753410339356
	model : 0.06915922164916992
			 train-loss:  2.1388713806525046 	 ± 0.2812189522640545
	data : 0.11707687377929688
	model : 0.06928606033325195
			 train-loss:  2.138097336346453 	 ± 0.2797097455348401
	data : 0.11687803268432617
	model : 0.06868081092834473
			 train-loss:  2.141012131498101 	 ± 0.2794747197023054
	data : 0.11725053787231446
	model : 0.06783480644226074
			 train-loss:  2.1403206838501823 	 ± 0.2779942878427083
	data : 0.11794824600219726
	model : 0.06799192428588867
			 train-loss:  2.137506061857873 	 ± 0.27774911876468256
	data : 0.11757040023803711
	model : 0.06791338920593262
			 train-loss:  2.1384606685327445 	 ± 0.27638554761904705
	data : 0.11746907234191895
	model : 0.06832323074340821
			 train-loss:  2.138067441601907 	 ± 0.27492146127761413
	data : 0.11725816726684571
	model : 0.06876249313354492
			 train-loss:  2.135182574708411 	 ± 0.2748667614758954
	data : 0.11681141853332519
	model : 0.0694284439086914
			 train-loss:  2.1365111238078067 	 ± 0.27371950814916696
	data : 0.11637444496154785
	model : 0.06956167221069336
			 train-loss:  2.1396072320640087 	 ± 0.27395727221218336
	data : 0.11643476486206054
	model : 0.06887989044189453
			 train-loss:  2.138719208461722 	 ± 0.27268031325308617
	data : 0.11703567504882813
	model : 0.06925287246704101
			 train-loss:  2.1376197082655772 	 ± 0.27150155924213765
	data : 0.11690492630004883
	model : 0.06929607391357422
			 train-loss:  2.1343394216865 	 ± 0.27207173205033247
	data : 0.11696419715881348
	model : 0.06945452690124512
			 train-loss:  2.1349071502685546 	 ± 0.27076688565185464
	data : 0.11703681945800781
	model : 0.06930766105651856
			 train-loss:  2.1325202295095615 	 ± 0.27047838592896517
	data : 0.11725864410400391
	model : 0.06980366706848144
			 train-loss:  2.1312747118519804 	 ± 0.26944015891308554
	data : 0.11692981719970703
	model : 0.06983294486999511
			 train-loss:  2.133934074235194 	 ± 0.2694708324404969
	data : 0.1167642593383789
	model : 0.06894440650939941
			 train-loss:  2.132116334942671 	 ± 0.2688059579845353
	data : 0.117500638961792
	model : 0.06836681365966797
			 train-loss:  2.1367083220254806 	 ± 0.2715906192435755
	data : 0.11784987449645996
	model : 0.0683983325958252
			 train-loss:  2.1334025848586604 	 ± 0.2724206873596871
	data : 0.1177980899810791
	model : 0.06937608718872071
			 train-loss:  2.134451343634418 	 ± 0.2713596138710844
	data : 0.11679973602294921
	model : 0.06929850578308105
			 train-loss:  2.1314987816192486 	 ± 0.27182165192865393
	data : 0.11675782203674316
	model : 0.06992015838623047
			 train-loss:  2.136630709018182 	 ± 0.2757780072619633
	data : 0.11625280380249023
	model : 0.069488525390625
			 train-loss:  2.1336569103327663 	 ± 0.2762717012654674
	data : 0.11660652160644532
	model : 0.06966071128845215
			 train-loss:  2.133315383850991 	 ± 0.27504774359897805
	data : 0.11650500297546387
	model : 0.06877508163452148
			 train-loss:  2.1307197298322404 	 ± 0.2751793182003672
	data : 0.1174731731414795
	model : 0.06880154609680175
			 train-loss:  2.127848667381084 	 ± 0.2756388076094784
	data : 0.11736440658569336
	model : 0.06885218620300293
			 train-loss:  2.1268732673243473 	 ± 0.2746230112660555
	data : 0.11731090545654296
	model : 0.06986813545227051
			 train-loss:  2.1253759114638617 	 ± 0.27389338779215633
	data : 0.11614818572998047
	model : 0.06983470916748047
			 train-loss:  2.1271695844058334 	 ± 0.27338776441918733
	data : 0.11612672805786133
	model : 0.06976447105407715
			 train-loss:  2.127940587508373 	 ± 0.2723435598516169
	data : 0.11608633995056153
	model : 0.0688333511352539
			 train-loss:  2.1249225735664368 	 ± 0.2731448890723745
	data : 0.1170419692993164
	model : 0.06891999244689942
			 train-loss:  2.1246602525230216 	 ± 0.2720097263004859
	data : 0.11704487800598144
	model : 0.06923131942749024
			 train-loss:  2.122703327735265 	 ± 0.27171387667130414
	data : 0.11707401275634766
	model : 0.06888809204101562
			 train-loss:  2.1221388627674953 	 ± 0.2706594039365133
	data : 0.1172715187072754
	model : 0.0681833267211914
			 train-loss:  2.1199516468360775 	 ± 0.2706194805546404
	data : 0.11806716918945312
	model : 0.06883149147033692
			 train-loss:  2.1205198629115656 	 ± 0.26959022166645386
	data : 0.11729679107666016
	model : 0.06901097297668457
			 train-loss:  2.1223650459320313 	 ± 0.26927967933609437
	data : 0.11707544326782227
	model : 0.0687495231628418
			 train-loss:  2.1212595233917235 	 ± 0.2684827809535036
	data : 0.11724109649658203
	model : 0.06897320747375488
			 train-loss:  2.1230173773235745 	 ± 0.26813648137383284
	data : 0.11724481582641602
	model : 0.06968894004821777
			 train-loss:  2.125679044272956 	 ± 0.26874466787926005
	data : 0.1163520336151123
	model : 0.06993260383605956
			 train-loss:  2.127989586442709 	 ± 0.26895622642826855
	data : 0.11616835594177247
	model : 0.06984190940856934
			 train-loss:  2.128186129784399 	 ± 0.26792096006701743
	data : 0.1161376953125
	model : 0.06986618041992188
			 train-loss:  2.1264523588694058 	 ± 0.2676139821081099
	data : 0.11613698005676269
	model : 0.06997137069702149
			 train-loss:  2.1237631444712632 	 ± 0.26834807719713266
	data : 0.11590323448181153
	model : 0.07009873390197754
			 train-loss:  2.123062682874275 	 ± 0.26744986387328606
	data : 0.1160090446472168
	model : 0.07006692886352539
			 train-loss:  2.122205486871246 	 ± 0.266624466325594
	data : 0.11614022254943848
	model : 0.06924991607666016
			 train-loss:  2.1215004920959473 	 ± 0.2657521352750052
	data : 0.11691703796386718
	model : 0.06903443336486817
			 train-loss:  2.121278225934064 	 ± 0.264778539729522
	data : 0.11711935997009278
	model : 0.06809754371643066
			 train-loss:  2.119522351552458 	 ± 0.26459099624551896
	data : 0.11797637939453125
	model : 0.06720752716064453
			 train-loss:  2.119848829986405 	 ± 0.26365105935761435
	data : 0.11877059936523438
	model : 0.06800556182861328
			 train-loss:  2.117967592633289 	 ± 0.2636152920680936
	data : 0.11789045333862305
	model : 0.06890010833740234
			 train-loss:  2.1195029672101247 	 ± 0.2632838585531662
	data : 0.11706042289733887
	model : 0.06909551620483398
			 train-loss:  2.1168651453086307 	 ± 0.26417879423910895
	data : 0.11679615974426269
	model : 0.06996264457702636
			 train-loss:  2.114098681625745 	 ± 0.26526766233830357
	data : 0.11604375839233398
	model : 0.07058005332946778
			 train-loss:  2.1118856131190986 	 ± 0.2656350178069621
	data : 0.1156233310699463
	model : 0.07023968696594238
			 train-loss:  2.109554815125632 	 ± 0.2661577620988494
	data : 0.11618647575378419
	model : 0.07023305892944336
			 train-loss:  2.10897074557013 	 ± 0.2653239394105582
	data : 0.11637496948242188
	model : 0.07005820274353028
			 train-loss:  2.1082366951580704 	 ± 0.2645541328773434
	data : 0.11642937660217285
	model : 0.0700767993927002
			 train-loss:  2.106303489371522 	 ± 0.26467228706045237
	data : 0.11653480529785157
	model : 0.07022452354431152
			 train-loss:  2.1089265119461786 	 ± 0.26566782905201575
	data : 0.11615824699401855
	model : 0.06959314346313476
			 train-loss:  2.1067470102696806 	 ± 0.2660841803593466
	data : 0.11668276786804199
	model : 0.06944584846496582
			 train-loss:  2.110153262247175 	 ± 0.26840789056649095
	data : 0.11674957275390625
	model : 0.0700110912322998
			 train-loss:  2.109811004002889 	 ± 0.26754432226964914
	data : 0.11637554168701172
	model : 0.06976437568664551
			 train-loss:  2.1091012923133294 	 ± 0.2667985728632626
	data : 0.11652107238769531
	model : 0.06963582038879394
			 train-loss:  2.1068589373638758 	 ± 0.2673432826815654
	data : 0.11676335334777832
	model : 0.06970481872558594
			 train-loss:  2.108351702783622 	 ± 0.26710297585642045
	data : 0.11666102409362793
	model : 0.06965155601501465
			 train-loss:  2.108915273245279 	 ± 0.26632559315758725
	data : 0.11676855087280273
	model : 0.06834831237792968
			 train-loss:  2.1107809112917995 	 ± 0.2664727495708594
	data : 0.11799545288085937
	model : 0.06860036849975586
			 train-loss:  2.1112762766006665 	 ± 0.26568888475757485
	data : 0.11767268180847168
	model : 0.0687363624572754
			 train-loss:  2.10874633424601 	 ± 0.26671980959513003
	data : 0.11753153800964355
	model : 0.06829276084899902
			 train-loss:  2.1073035401634024 	 ± 0.2664883241377792
	data : 0.11783146858215332
	model : 0.06760115623474121
			 train-loss:  2.1080000018173792 	 ± 0.26579319878269525
	data : 0.11838221549987793
	model : 0.06769347190856934
			 train-loss:  2.109652692824602 	 ± 0.26577956559672566
	data : 0.11826853752136231
	model : 0.06740598678588867
			 train-loss:  2.1090668410247897 	 ± 0.2650564899998892
	data : 0.11850709915161133
	model : 0.06726460456848145
			 train-loss:  2.1070054328000105 	 ± 0.2655285755375846
	data : 0.1187978744506836
	model : 0.06769232749938965
			 train-loss:  2.1065406009463445 	 ± 0.26477892463818875
	data : 0.1185072422027588
	model : 0.06856217384338378
			 train-loss:  2.1068844519010406 	 ± 0.2640069391568678
	data : 0.1177138328552246
	model : 0.06939101219177246
			 train-loss:  2.1069370009682395 	 ± 0.2632065625848191
	data : 0.1169210433959961
	model : 0.0698404312133789
			 train-loss:  2.1048432265419557 	 ± 0.26378722704217145
	data : 0.11654138565063477
	model : 0.06995849609375
			 train-loss:  2.1028823145849262 	 ± 0.26420698599425346
	data : 0.11615643501281739
	model : 0.06978392601013184
			 train-loss:  2.105841278320267 	 ± 0.2661803644326913
	data : 0.11625709533691406
	model : 0.06961970329284668
			 train-loss:  2.106857387977239 	 ± 0.26571827248171254
	data : 0.11622772216796876
	model : 0.06953859329223633
			 train-loss:  2.108081164780785 	 ± 0.265412827786927
	data : 0.11631712913513184
	model : 0.06915826797485351
			 train-loss:  2.10788949679213 	 ± 0.2646474283700945
	data : 0.11656513214111328
	model : 0.069744873046875
			 train-loss:  2.1055011950259983 	 ± 0.26571873031424437
	data : 0.11632218360900878
	model : 0.07015681266784668
			 train-loss:  2.1045137609360536 	 ± 0.26526593900064444
	data : 0.116192626953125
	model : 0.07066516876220703
			 train-loss:  2.103045774602342 	 ± 0.26520638793187606
	data : 0.11603870391845703
	model : 0.07035207748413086
			 train-loss:  2.1021150473185948 	 ± 0.26473240310119867
	data : 0.11628422737121583
	model : 0.06965498924255371
			 train-loss:  2.1019032631408083 	 ± 0.26399411771622205
	data : 0.11708040237426758
	model : 0.0691770076751709
			 train-loss:  2.1015356078659746 	 ± 0.26329249707208546
	data : 0.1173792839050293
	model : 0.0683095932006836
			 train-loss:  2.102127566096488 	 ± 0.26266995959882966
	data : 0.11790909767150878
	model : 0.06819233894348145
			 train-loss:  2.099800169800913 	 ± 0.2637692999536993
	data : 0.11778082847595214
	model : 0.06834802627563477
			 train-loss:  2.0976182142893474 	 ± 0.2646505742794951
	data : 0.11779122352600098
	model : 0.06894879341125489
			 train-loss:  2.0981550058607237 	 ± 0.26401672613900906
	data : 0.11714820861816407
	model : 0.06844868659973144
			 train-loss:  2.098208234860347 	 ± 0.26329138028536625
	data : 0.11759166717529297
	model : 0.06925954818725585
			 train-loss:  2.1011892772111738 	 ± 0.26563302492022817
	data : 0.11710643768310547
	model : 0.06906290054321289
			 train-loss:  2.1017679973788885 	 ± 0.2650258677406763
	data : 0.11731967926025391
	model : 0.0691148281097412
			 train-loss:  2.1021057889268207 	 ± 0.26434832483259413
	data : 0.11719713211059571
	model : 0.06924591064453126
			 train-loss:  2.1043407917022705 	 ± 0.26538360280762097
	data : 0.1171480655670166
	model : 0.06963815689086914
			 train-loss:  2.1020744803117557 	 ± 0.26647168892336265
	data : 0.11667685508728028
	model : 0.06963157653808594
			 train-loss:  2.100528287760755 	 ± 0.26660181238221226
	data : 0.11679258346557617
	model : 0.0696685791015625
			 train-loss:  2.1011696246565963 	 ± 0.2660409497333826
	data : 0.11670255661010742
	model : 0.06967010498046874
			 train-loss:  2.098830297118739 	 ± 0.2672818099800537
	data : 0.11688933372497559
	model : 0.06989212036132812
			 train-loss:  2.0989577807681097 	 ± 0.2665869927443886
	data : 0.11681046485900878
	model : 0.06977763175964355
			 train-loss:  2.0985023640096188 	 ± 0.2659663321897766
	data : 0.1171572208404541
	model : 0.0688873291015625
			 train-loss:  2.0977128398233127 	 ± 0.2655018910644629
	data : 0.11776261329650879
	model : 0.06808772087097167
			 train-loss:  2.095678901549467 	 ± 0.26631995761612764
	data : 0.11859731674194336
	model : 0.06827278137207031
			 train-loss:  2.0980195210530206 	 ± 0.2676292660411547
	data : 0.11832795143127442
	model : 0.06820006370544433
			 train-loss:  2.096673405292083 	 ± 0.2676066770372882
	data : 0.1181706428527832
	model : 0.06888489723205567
			 train-loss:  2.0969518120518797 	 ± 0.2669550639312751
	data : 0.11747713088989258
	model : 0.06943459510803222
			 train-loss:  2.097371944875428 	 ± 0.26634536727365743
	data : 0.11699895858764649
	model : 0.07012052536010742
			 train-loss:  2.0972204082575274 	 ± 0.2656838717894066
	data : 0.1163900375366211
	model : 0.0698775291442871
			 train-loss:  2.0964500111341478 	 ± 0.26524156718768555
	data : 0.11650853157043457
	model : 0.07019424438476562
			 train-loss:  2.0955803945883 	 ± 0.2648666082834169
	data : 0.11624107360839844
	model : 0.06933794021606446
			 train-loss:  2.094339902448182 	 ± 0.2647948725702205
	data : 0.11690406799316407
	model : 0.0687744140625
			 train-loss:  2.0945975457506227 	 ± 0.26416724380661855
	data : 0.11739163398742676
	model : 0.0688082218170166
			 train-loss:  2.096039009444854 	 ± 0.2643180841471716
	data : 0.11729578971862793
	model : 0.06813383102416992
			 train-loss:  2.0966617880797966 	 ± 0.26382261355650627
	data : 0.11762866973876954
	model : 0.06705265045166016
			 train-loss:  2.096318128618222 	 ± 0.263227480914481
	data : 0.11882171630859376
	model : 0.06647286415100098
			 train-loss:  2.0955867548495677 	 ± 0.26280062713068847
	data : 0.1191619873046875
	model : 0.06734266281127929
			 train-loss:  2.094661237528691 	 ± 0.2625060816089596
	data : 0.11846709251403809
	model : 0.06740045547485352
			 train-loss:  2.0949367581372056 	 ± 0.2619074688561903
	data : 0.11869583129882813
	model : 0.06793732643127441
			 train-loss:  2.0946507629894073 	 ± 0.2613158465967526
	data : 0.11826720237731933
	model : 0.06841144561767579
			 train-loss:  2.0958849033473226 	 ± 0.26130861637892305
	data : 0.11771531105041504
	model : 0.06935853958129883
			 train-loss:  2.0985689270046524 	 ± 0.26359087159726025
	data : 0.11689281463623047
	model : 0.0699239730834961
			 train-loss:  2.097439085373856 	 ± 0.2634854391439135
	data : 0.11620101928710938
	model : 0.06994571685791015
			 train-loss:  2.0971235471351126 	 ± 0.26290943296939256
	data : 0.11617579460144042
	model : 0.06938252449035645
			 train-loss:  2.0973261190015218 	 ± 0.2623140422616135
	data : 0.11689972877502441
	model : 0.06980738639831544
			 train-loss:  2.095681466438152 	 ± 0.262814854627994
	data : 0.11646728515625
	model : 0.0696950912475586
			 train-loss:  2.0984276439737064 	 ± 0.2652966403046113
	data : 0.11678099632263184
	model : 0.06913313865661622
			 train-loss:  2.098610102583509 	 ± 0.26470110863825674
	data : 0.11747980117797852
	model : 0.06830644607543945
			 train-loss:  2.099604585943701 	 ± 0.2645039491762575
	data : 0.11802105903625489
	model : 0.0683863639831543
			 train-loss:  2.1006270343607123 	 ± 0.26433552781467184
	data : 0.11791658401489258
	model : 0.06806883811950684
			 train-loss:  2.1011895194851973 	 ± 0.263868733104477
	data : 0.11828527450561524
	model : 0.06804814338684081
			 train-loss:  2.1014128268302024 	 ± 0.2632946921466295
	data : 0.11813201904296874
	model : 0.06797275543212891
			 train-loss:  2.1018005732463614 	 ± 0.26276720044308166
	data : 0.11813693046569824
	model : 0.06781182289123536
			 train-loss:  2.10149358106511 	 ± 0.2622200875750725
	data : 0.11834859848022461
	model : 0.06770925521850586
			 train-loss:  2.1005702760484484 	 ± 0.2620014033380224
	data : 0.1183344841003418
	model : 0.06696443557739258
			 train-loss:  2.099288229921223 	 ± 0.26212748252582757
	data : 0.11877350807189942
	model : 0.06660480499267578
			 train-loss:  2.098764799765028 	 ± 0.2616678152814005
	data : 0.11887941360473633
	model : 0.06642723083496094
			 train-loss:  2.0973628759384155 	 ± 0.26194633412427254
	data : 0.11902494430541992
	model : 0.06689996719360351
			 train-loss:  2.0979744575950257 	 ± 0.2615368589946772
	data : 0.11853251457214356
	model : 0.06728715896606445
			 train-loss:  2.0988250494003298 	 ± 0.26128492806191395
	data : 0.1181908130645752
	model : 0.06781792640686035
			 train-loss:  2.0977239433305086 	 ± 0.26125300623394565
	data : 0.11788411140441894
	model : 0.06746902465820312
			 train-loss:  2.097621277488511 	 ± 0.2606940228119631
	data : 0.11853318214416504
	model : 0.06704764366149903
			 train-loss:  2.098614969990284 	 ± 0.260573936467173
	data : 0.11872701644897461
	model : 0.06704721450805665
			 train-loss:  2.0975588453121676 	 ± 0.2605158319570174
	data : 0.11877102851867676
	model : 0.0667717456817627
			 train-loss:  2.095464838819301 	 ± 0.26192700235961797
	data : 0.11901817321777344
	model : 0.06647691726684571
			 train-loss:  2.0953818372750685 	 ± 0.26137458023014454
	data : 0.11922335624694824
	model : 0.06676735877990722
			 train-loss:  2.094191465699723 	 ± 0.26146285306506584
	data : 0.11880569458007813
	model : 0.06662478446960449
			 train-loss:  2.0961218521374616 	 ± 0.26259996122419405
	data : 0.11909260749816894
	model : 0.06626577377319336
			 train-loss:  2.0955409036021853 	 ± 0.26220323153870717
	data : 0.1191490650177002
	model : 0.0667241096496582
			 train-loss:  2.0962815076112746 	 ± 0.2619067853372152
	data : 0.1186783790588379
	model : 0.06711935997009277
			 train-loss:  2.0950254167263935 	 ± 0.2620862461155981
	data : 0.1182516098022461
	model : 0.06695079803466797
			 train-loss:  2.0949564502258933 	 ± 0.2615463764211298
	data : 0.11847491264343261
	model : 0.0674807071685791
			 train-loss:  2.093466438874296 	 ± 0.26203486627934647
	data : 0.11789803504943848
	model : 0.06807861328125
			 train-loss:  2.093824130589845 	 ± 0.2615567981007872
	data : 0.11765470504760742
	model : 0.0681647777557373
			 train-loss:  2.0934196773840457 	 ± 0.2610989089465272
	data : 0.1176154613494873
	model : 0.06786398887634278
			 train-loss:  2.091830356818874 	 ± 0.26175249944292395
	data : 0.11781797409057618
	model : 0.06842598915100098
			 train-loss:  2.0908508797888814 	 ± 0.26167344374126644
	data : 0.11726512908935546
	model : 0.06869077682495117
			 train-loss:  2.08923695885366 	 ± 0.2623742772501539
	data : 0.11689081192016601
	model : 0.06869540214538575
			 train-loss:  2.0899020299375297 	 ± 0.2620562720046874
	data : 0.11681880950927734
	model : 0.06886758804321289
			 train-loss:  2.0893281564712525 	 ± 0.26168836255415656
	data : 0.11683831214904786
	model : 0.06938385963439941
			 train-loss:  2.0896298372413056 	 ± 0.2612101070960333
	data : 0.11647472381591797
	model : 0.06934833526611328
			 train-loss:  2.0902002255121865 	 ± 0.2608478948759193
	data : 0.11657090187072754
	model : 0.06929492950439453
			 train-loss:  2.090623946057949 	 ± 0.26041875679047566
	data : 0.11683511734008789
	model : 0.0694911003112793
			 train-loss:  2.0906423762088684 	 ± 0.2599057812071355
	data : 0.1165700912475586
	model : 0.0692873477935791
			 train-loss:  2.0902500685523537 	 ± 0.25947100244311117
	data : 0.11671915054321289
	model : 0.06917877197265625
			 train-loss:  2.0912508657202125 	 ± 0.2594563908630043
	data : 0.11586828231811523
	model : 0.0603147029876709
#epoch  27    val-loss:  2.4437820723182275  train-loss:  2.0912508657202125  lr:  0.00125
			 train-loss:  2.1218037605285645 	 ± 0.0
	data : 5.936507225036621
	model : 0.07323360443115234
			 train-loss:  1.9287285804748535 	 ± 0.19307518005371094
	data : 3.036118984222412
	model : 0.07177972793579102
			 train-loss:  2.159553607304891 	 ± 0.36250848650850953
	data : 2.062873204549154
	model : 0.06962887446085612
			 train-loss:  2.1877231001853943 	 ± 0.3177103446229787
	data : 1.5769771337509155
	model : 0.06935065984725952
			 train-loss:  2.1340847969055177 	 ± 0.30374357734664553
	data : 1.2848943233489991
	model : 0.06944580078125
			 train-loss:  2.0632900993029275 	 ± 0.31928500334235155
	data : 0.1208521842956543
	model : 0.06878867149353027
			 train-loss:  2.048585534095764 	 ± 0.2977868120818471
	data : 0.11693191528320312
	model : 0.06882076263427735
			 train-loss:  2.018512800335884 	 ± 0.28969457619911365
	data : 0.11662592887878417
	model : 0.06974725723266602
			 train-loss:  1.9958494106928508 	 ± 0.2805480518163477
	data : 0.11596007347106933
	model : 0.06998043060302735
			 train-loss:  2.083308184146881 	 ± 0.373734962141494
	data : 0.11593680381774903
	model : 0.06996316909790039
			 train-loss:  2.082024325023998 	 ± 0.35636543190640885
	data : 0.11600522994995117
	model : 0.07037854194641113
			 train-loss:  2.0527483522892 	 ± 0.35474103607882224
	data : 0.11562809944152833
	model : 0.07033796310424804
			 train-loss:  2.0527828748409567 	 ± 0.34082418409825566
	data : 0.11578893661499023
	model : 0.07033472061157227
			 train-loss:  2.0511052863938466 	 ± 0.3284820957511291
	data : 0.1157193660736084
	model : 0.07040657997131347
			 train-loss:  2.035891596476237 	 ± 0.3224089180939513
	data : 0.11559481620788574
	model : 0.07056670188903809
			 train-loss:  2.0685008466243744 	 ± 0.33675100443905465
	data : 0.11553044319152832
	model : 0.07000365257263183
			 train-loss:  2.060757875442505 	 ± 0.3281612949712133
	data : 0.11583328247070312
	model : 0.06900124549865723
			 train-loss:  2.0762938261032104 	 ± 0.3252849287120171
	data : 0.11674213409423828
	model : 0.06803483963012695
			 train-loss:  2.091469350614046 	 ± 0.3230892450732853
	data : 0.11756811141967774
	model : 0.06793498992919922
			 train-loss:  2.0836627960205076 	 ± 0.3167415837330805
	data : 0.11760015487670898
	model : 0.06758956909179688
			 train-loss:  2.08248504002889 	 ± 0.30915300597482187
	data : 0.11775498390197754
	model : 0.06808881759643555
			 train-loss:  2.0758638273585928 	 ± 0.30356529134503596
	data : 0.1175088882446289
	model : 0.06911869049072265
			 train-loss:  2.0740942644036333 	 ± 0.2970087071663795
	data : 0.11677088737487792
	model : 0.06992378234863281
			 train-loss:  2.073499023914337 	 ± 0.2907692053599204
	data : 0.11610932350158691
	model : 0.06991605758666992
			 train-loss:  2.0695324325561524 	 ± 0.28555642854308466
	data : 0.1162346363067627
	model : 0.07008562088012696
			 train-loss:  2.079407279308026 	 ± 0.2843308633372994
	data : 0.1161203384399414
	model : 0.06885313987731934
			 train-loss:  2.084377438933761 	 ± 0.2801643817519754
	data : 0.11721310615539551
	model : 0.06875944137573242
			 train-loss:  2.0776177602154866 	 ± 0.2773490763000381
	data : 0.11728448867797851
	model : 0.06817126274108887
			 train-loss:  2.074782971678109 	 ± 0.2729377557334462
	data : 0.11779108047485351
	model : 0.06738920211791992
			 train-loss:  2.0716311653455097 	 ± 0.2688864701559095
	data : 0.11842565536499024
	model : 0.06727352142333984
			 train-loss:  2.062314102726598 	 ± 0.2693917362840829
	data : 0.11855831146240234
	model : 0.06731867790222168
			 train-loss:  2.0814837738871574 	 ± 0.28582477242334886
	data : 0.11871323585510254
	model : 0.06728520393371581
			 train-loss:  2.080341201840025 	 ± 0.28153498056190934
	data : 0.11855831146240234
	model : 0.06927614212036133
			 train-loss:  2.088282213491552 	 ± 0.2810901644934035
	data : 0.1167520523071289
	model : 0.0699005126953125
			 train-loss:  2.08437100819179 	 ± 0.27798258920581287
	data : 0.1162229061126709
	model : 0.07006473541259765
			 train-loss:  2.092128468884362 	 ± 0.27791014355237403
	data : 0.11605515480041503
	model : 0.07087068557739258
			 train-loss:  2.0891661418450846 	 ± 0.2747044865362507
	data : 0.11529579162597656
	model : 0.06997261047363282
			 train-loss:  2.098986886049572 	 ± 0.277570239459583
	data : 0.11606798171997071
	model : 0.06783881187438964
			 train-loss:  2.1008376005368357 	 ± 0.2742259549917671
	data : 0.11803951263427734
	model : 0.06711788177490234
			 train-loss:  2.0958698451519013 	 ± 0.2725478722727169
	data : 0.11861281394958496
	model : 0.06721487045288085
			 train-loss:  2.093200215479223 	 ± 0.2697325622321782
	data : 0.11856489181518555
	model : 0.06713404655456542
			 train-loss:  2.0956093612171354 	 ± 0.26694819897487015
	data : 0.11868829727172851
	model : 0.06844902038574219
			 train-loss:  2.093283087708229 	 ± 0.2642562875003183
	data : 0.11758413314819335
	model : 0.06935029029846192
			 train-loss:  2.084582442587072 	 ± 0.2673938317130571
	data : 0.11693081855773926
	model : 0.0704310417175293
			 train-loss:  2.0805197053485447 	 ± 0.26577592255890387
	data : 0.11603302955627441
	model : 0.07016544342041016
			 train-loss:  2.0768201999042346 	 ± 0.2640400402131018
	data : 0.11624298095703126
	model : 0.07043938636779785
			 train-loss:  2.0769080380175975 	 ± 0.26121668071874277
	data : 0.11582069396972657
	model : 0.06932992935180664
			 train-loss:  2.0751666525999704 	 ± 0.2587568999927986
	data : 0.11673192977905274
	model : 0.06927037239074707
			 train-loss:  2.0868833502944635 	 ± 0.26865999001647023
	data : 0.11676192283630371
	model : 0.06912288665771485
			 train-loss:  2.0913493537902834 	 ± 0.2677908523548665
	data : 0.11703801155090332
	model : 0.06931161880493164
			 train-loss:  2.098248617321837 	 ± 0.2696030711738787
	data : 0.1168848991394043
	model : 0.06918439865112305
			 train-loss:  2.1015195479759803 	 ± 0.2680180221316433
	data : 0.11709985733032227
	model : 0.06998834609985352
			 train-loss:  2.102355903049685 	 ± 0.2655460065194644
	data : 0.1164355754852295
	model : 0.07015438079833984
			 train-loss:  2.094841369876155 	 ± 0.2687036829093527
	data : 0.11620526313781739
	model : 0.07012767791748047
			 train-loss:  2.094407003576105 	 ± 0.2662688490746311
	data : 0.11611080169677734
	model : 0.06961135864257813
			 train-loss:  2.0910242668219974 	 ± 0.26507056526088685
	data : 0.11663718223571777
	model : 0.06888718605041504
			 train-loss:  2.090799662104824 	 ± 0.2627404724266946
	data : 0.11700477600097656
	model : 0.06909031867980957
			 train-loss:  2.0891523381759383 	 ± 0.2607623801663763
	data : 0.1168748378753662
	model : 0.06875147819519042
			 train-loss:  2.0871800729783914 	 ± 0.25897902850356297
	data : 0.11709723472595215
	model : 0.06880683898925781
			 train-loss:  2.089788023630778 	 ± 0.25759189539712524
	data : 0.11715397834777833
	model : 0.06929512023925781
			 train-loss:  2.0957711055630543 	 ± 0.25964140089214927
	data : 0.116644287109375
	model : 0.06912498474121094
			 train-loss:  2.1004371873794065 	 ± 0.26010469334185
	data : 0.11693902015686035
	model : 0.06912717819213868
			 train-loss:  2.106173632636903 	 ± 0.26195570842389054
	data : 0.1167604923248291
	model : 0.06935014724731445
			 train-loss:  2.1033354736864567 	 ± 0.2608755785711904
	data : 0.1167219638824463
	model : 0.06982684135437012
			 train-loss:  2.1051913554851827 	 ± 0.25928649424123407
	data : 0.11600713729858399
	model : 0.06995606422424316
			 train-loss:  2.113083055525115 	 ± 0.26506412042657973
	data : 0.11587085723876953
	model : 0.07028326988220215
			 train-loss:  2.10492520189997 	 ± 0.2712981465607205
	data : 0.11563673019409179
	model : 0.07000765800476075
			 train-loss:  2.102204661158954 	 ± 0.27021506586343524
	data : 0.1159599781036377
	model : 0.07012372016906739
			 train-loss:  2.1001644410948823 	 ± 0.26877690775777746
	data : 0.11587038040161132
	model : 0.06963238716125489
			 train-loss:  2.0992819547653196 	 ± 0.2669508333964213
	data : 0.11645584106445313
	model : 0.0695267677307129
			 train-loss:  2.0954216701883666 	 ± 0.26702466660174623
	data : 0.11666350364685059
	model : 0.07118358612060546
			 train-loss:  2.0910828196340137 	 ± 0.26767234435930015
	data : 0.11496977806091309
	model : 0.07121767997741699
			 train-loss:  2.0965618385027533 	 ± 0.26986739798003273
	data : 0.1149622917175293
	model : 0.0715261459350586
			 train-loss:  2.093810534155047 	 ± 0.26906659227025725
	data : 0.11470241546630859
	model : 0.0717916488647461
			 train-loss:  2.0896747223536174 	 ± 0.26962437853414284
	data : 0.11455998420715333
	model : 0.0710442066192627
			 train-loss:  2.0895337321256338 	 ± 0.26784744328597154
	data : 0.11524376869201661
	model : 0.07017631530761718
			 train-loss:  2.0904545768514855 	 ± 0.26622355246244195
	data : 0.11618871688842773
	model : 0.07053022384643555
			 train-loss:  2.0887729877080674 	 ± 0.26492274805135796
	data : 0.11583466529846191
	model : 0.07020859718322754
			 train-loss:  2.086753928208653 	 ± 0.26384395289626084
	data : 0.1160616397857666
	model : 0.07010712623596191
			 train-loss:  2.085327424108982 	 ± 0.26249613194900434
	data : 0.11604256629943847
	model : 0.07080559730529785
			 train-loss:  2.0799007607095037 	 ± 0.2653477792753503
	data : 0.11526775360107422
	model : 0.07031393051147461
			 train-loss:  2.0765279662318346 	 ± 0.26546605688929203
	data : 0.11570992469787597
	model : 0.06995735168457032
			 train-loss:  2.07714731291116 	 ± 0.26392161485080634
	data : 0.11615109443664551
	model : 0.0696639060974121
			 train-loss:  2.085332207736515 	 ± 0.27273755683316797
	data : 0.11645550727844238
	model : 0.06950292587280274
			 train-loss:  2.088032541555517 	 ± 0.27225568786229204
	data : 0.1166391372680664
	model : 0.06955218315124512
			 train-loss:  2.085003381551698 	 ± 0.27210513822228855
	data : 0.11663436889648438
	model : 0.06990022659301758
			 train-loss:  2.086182273667434 	 ± 0.2707576029378884
	data : 0.11613168716430664
	model : 0.06931629180908203
			 train-loss:  2.088064109737223 	 ± 0.26978641314340673
	data : 0.11654624938964844
	model : 0.06951828002929687
			 train-loss:  2.088414789585585 	 ± 0.26828664692861287
	data : 0.1162834644317627
	model : 0.06935911178588867
			 train-loss:  2.088777184486389 	 ± 0.2668139066780297
	data : 0.11639189720153809
	model : 0.06840982437133789
			 train-loss:  2.0917883810106215 	 ± 0.26687715195885875
	data : 0.11713585853576661
	model : 0.06819343566894531
			 train-loss:  2.0916628708010134 	 ± 0.2654254701372257
	data : 0.1175912857055664
	model : 0.06859855651855469
			 train-loss:  2.096309277319139 	 ± 0.2677299807769184
	data : 0.11723756790161133
	model : 0.06867547035217285
			 train-loss:  2.096841885688457 	 ± 0.26635160572942973
	data : 0.11719927787780762
	model : 0.06924681663513184
			 train-loss:  2.101880911776894 	 ± 0.26941276003504994
	data : 0.11674275398254394
	model : 0.07025713920593261
			 train-loss:  2.1023394092917442 	 ± 0.2680431510029074
	data : 0.1158895492553711
	model : 0.07035222053527831
			 train-loss:  2.1041727852575556 	 ± 0.2672622689474559
	data : 0.11582484245300292
	model : 0.07037014961242676
			 train-loss:  2.099628905860745 	 ± 0.26963492968226455
	data : 0.11579356193542481
	model : 0.06930923461914062
			 train-loss:  2.1024500745715518 	 ± 0.26971948931900785
	data : 0.11679534912109375
	model : 0.06883230209350585
			 train-loss:  2.1020151114463808 	 ± 0.2684023975087162
	data : 0.11706438064575195
	model : 0.06876392364501953
			 train-loss:  2.101762599284106 	 ± 0.26708230457271026
	data : 0.11716294288635254
	model : 0.0689012050628662
			 train-loss:  2.0968274076779685 	 ± 0.27035824831364946
	data : 0.11707262992858887
	model : 0.06870484352111816
			 train-loss:  2.09686070159801 	 ± 0.2690428387178057
	data : 0.11743636131286621
	model : 0.0695713996887207
			 train-loss:  2.0967685660490623 	 ± 0.2677478719922332
	data : 0.11664004325866699
	model : 0.06963801383972168
			 train-loss:  2.09732057821183 	 ± 0.2665292890503058
	data : 0.1167074203491211
	model : 0.06943421363830567
			 train-loss:  2.0975101095325543 	 ± 0.26527620555889997
	data : 0.11700787544250488
	model : 0.06860637664794922
			 train-loss:  2.0991542239055456 	 ± 0.26457573182382127
	data : 0.11742076873779297
	model : 0.06876492500305176
			 train-loss:  2.0980928672684565 	 ± 0.26357674388156416
	data : 0.11715044975280761
	model : 0.06883387565612793
			 train-loss:  2.0948863007606717 	 ± 0.26447268124106416
	data : 0.11695408821105957
	model : 0.06878643035888672
			 train-loss:  2.0964202989231455 	 ± 0.2637544722108622
	data : 0.11701679229736328
	model : 0.06949119567871094
			 train-loss:  2.1030229405239895 	 ± 0.27154213938949107
	data : 0.11641044616699218
	model : 0.07036714553833008
			 train-loss:  2.1045834805284227 	 ± 0.2708266986561603
	data : 0.11576304435729981
	model : 0.06954035758972169
			 train-loss:  2.1037547736041313 	 ± 0.26976828556533117
	data : 0.11640992164611816
	model : 0.07041587829589843
			 train-loss:  2.1016863448578014 	 ± 0.26948100165215
	data : 0.11561660766601563
	model : 0.07045440673828125
			 train-loss:  2.10147545752318 	 ± 0.2683162348363858
	data : 0.11557221412658691
	model : 0.06995272636413574
			 train-loss:  2.101691466980967 	 ± 0.2671672384251294
	data : 0.11592960357666016
	model : 0.0697854995727539
			 train-loss:  2.1024523097225742 	 ± 0.2661492294514745
	data : 0.11606168746948242
	model : 0.07059426307678222
			 train-loss:  2.099908962088116 	 ± 0.2664431279734063
	data : 0.11542630195617676
	model : 0.06930088996887207
			 train-loss:  2.0967956150279328 	 ± 0.2674680078932323
	data : 0.11679973602294921
	model : 0.06939229965209961
			 train-loss:  2.097977779308955 	 ± 0.266663232861374
	data : 0.11671695709228516
	model : 0.06944079399108886
			 train-loss:  2.09596650166945 	 ± 0.26647143991663114
	data : 0.11663966178894043
	model : 0.06944165229797364
			 train-loss:  2.093227748010979 	 ± 0.26708162953112247
	data : 0.11670780181884766
	model : 0.06878895759582519
			 train-loss:  2.0948332441531545 	 ± 0.26658418257762245
	data : 0.1172861099243164
	model : 0.06928973197937012
			 train-loss:  2.0937916338443756 	 ± 0.2657582614215902
	data : 0.11677031517028809
	model : 0.0692476749420166
			 train-loss:  2.0944838933944703 	 ± 0.2648053201373004
	data : 0.1167867660522461
	model : 0.06930556297302246
			 train-loss:  2.0923069270830306 	 ± 0.2648730501278114
	data : 0.11677708625793456
	model : 0.06946110725402832
			 train-loss:  2.091914440703204 	 ± 0.2638649642001795
	data : 0.11670589447021484
	model : 0.07025713920593261
			 train-loss:  2.0915495147928596 	 ± 0.26286439264271444
	data : 0.11603870391845703
	model : 0.0700521469116211
			 train-loss:  2.088296182395876 	 ± 0.2644178925408821
	data : 0.11618890762329101
	model : 0.07005801200866699
			 train-loss:  2.089059310693007 	 ± 0.2635415057401734
	data : 0.11620521545410156
	model : 0.07001819610595703
			 train-loss:  2.0875912722740466 	 ± 0.26306673843255396
	data : 0.11625776290893555
	model : 0.06992335319519043
			 train-loss:  2.0837585898962887 	 ± 0.26571442154849106
	data : 0.11631040573120117
	model : 0.06975178718566895
			 train-loss:  2.084443842558036 	 ± 0.26483066083676254
	data : 0.11646223068237305
	model : 0.0695638656616211
			 train-loss:  2.0874543447992693 	 ± 0.266115156977907
	data : 0.11666836738586425
	model : 0.06856021881103516
			 train-loss:  2.0891677335456564 	 ± 0.2658685545270999
	data : 0.11749753952026368
	model : 0.06756978034973145
			 train-loss:  2.089251321028261 	 ± 0.2648910735835464
	data : 0.11843962669372558
	model : 0.06687726974487304
			 train-loss:  2.0897116060674628 	 ± 0.26397712818742974
	data : 0.11910490989685059
	model : 0.06702003479003907
			 train-loss:  2.0880674244700996 	 ± 0.26372205988827413
	data : 0.11902532577514649
	model : 0.06741266250610352
			 train-loss:  2.085773558067761 	 ± 0.2641497711534648
	data : 0.1187281608581543
	model : 0.06758584976196289
			 train-loss:  2.08846293091774 	 ± 0.2651076328152587
	data : 0.11873040199279786
	model : 0.0686720848083496
			 train-loss:  2.0877085273147475 	 ± 0.2643166285793959
	data : 0.11764464378356934
	model : 0.06917428970336914
			 train-loss:  2.0908600729955755 	 ± 0.266029568052381
	data : 0.11700787544250488
	model : 0.06979279518127442
			 train-loss:  2.0929985663273953 	 ± 0.26631975461368523
	data : 0.11602530479431153
	model : 0.06955814361572266
			 train-loss:  2.0914152190089226 	 ± 0.26606797618132494
	data : 0.11591229438781739
	model : 0.07055444717407226
			 train-loss:  2.0971706283503564 	 ± 0.2739961827914677
	data : 0.11494097709655762
	model : 0.07051482200622558
			 train-loss:  2.0964512849507266 	 ± 0.27319358392552656
	data : 0.11507196426391601
	model : 0.07063403129577636
			 train-loss:  2.0995134017905412 	 ± 0.2747653411732873
	data : 0.115059232711792
	model : 0.06912879943847657
			 train-loss:  2.0994940488725096 	 ± 0.27383560708558385
	data : 0.11655526161193848
	model : 0.06928977966308594
			 train-loss:  2.0989904123664704 	 ± 0.27298391624357743
	data : 0.11659059524536133
	model : 0.06907172203063965
			 train-loss:  2.1002222339312238 	 ± 0.2724876278688091
	data : 0.11680974960327148
	model : 0.06889829635620118
			 train-loss:  2.0995093741953768 	 ± 0.2717241507491619
	data : 0.11689629554748535
	model : 0.06865825653076171
			 train-loss:  2.0995065562034907 	 ± 0.27082884854957867
	data : 0.11723289489746094
	model : 0.06866788864135742
			 train-loss:  2.097808998394636 	 ± 0.27075243962463685
	data : 0.11736230850219727
	model : 0.06774868965148925
			 train-loss:  2.0991270015766093 	 ± 0.2703639141852635
	data : 0.11821703910827637
	model : 0.0676795482635498
			 train-loss:  2.0974362727134457 	 ± 0.2703058891295596
	data : 0.11816768646240235
	model : 0.06794757843017578
			 train-loss:  2.097655536272587 	 ± 0.269451959387621
	data : 0.11782946586608886
	model : 0.06813759803771972
			 train-loss:  2.098624508851653 	 ± 0.2688649845201873
	data : 0.11768856048583984
	model : 0.06885104179382324
			 train-loss:  2.0981213880490652 	 ± 0.2680869262093085
	data : 0.11704630851745605
	model : 0.06982707977294922
			 train-loss:  2.1004416747663006 	 ± 0.26882933817434207
	data : 0.11604928970336914
	model : 0.06930031776428222
			 train-loss:  2.098032733798027 	 ± 0.2697039245899123
	data : 0.11674127578735352
	model : 0.06913151741027831
			 train-loss:  2.0959372298317667 	 ± 0.2701684406277804
	data : 0.11690254211425781
	model : 0.06871681213378907
			 train-loss:  2.0952638961650707 	 ± 0.2694687710326554
	data : 0.1173933982849121
	model : 0.06889724731445312
			 train-loss:  2.0960335790013973 	 ± 0.2688194713124051
	data : 0.11730566024780273
	model : 0.06797256469726562
			 train-loss:  2.096581608783908 	 ± 0.26808996520401557
	data : 0.1182934284210205
	model : 0.06887445449829102
			 train-loss:  2.0950969898339475 	 ± 0.26795169619858294
	data : 0.11740312576293946
	model : 0.06891727447509766
			 train-loss:  2.09548948759056 	 ± 0.2671909649341156
	data : 0.11736187934875489
	model : 0.06937956809997559
			 train-loss:  2.0941987166147746 	 ± 0.2669083954212122
	data : 0.11685266494750976
	model : 0.06859197616577148
			 train-loss:  2.094039711214247 	 ± 0.2661207725754704
	data : 0.11756310462951661
	model : 0.06932473182678223
			 train-loss:  2.093535189092512 	 ± 0.2654128367696902
	data : 0.11689295768737792
	model : 0.06903800964355469
			 train-loss:  2.093694274565753 	 ± 0.264639140507754
	data : 0.11705560684204101
	model : 0.06894173622131347
			 train-loss:  2.0925188524681224 	 ± 0.26430890029326204
	data : 0.11715030670166016
	model : 0.06808233261108398
			 train-loss:  2.0910651295684106 	 ± 0.26422417134150533
	data : 0.11798238754272461
	model : 0.06870861053466797
			 train-loss:  2.0902764790319983 	 ± 0.26366235981930064
	data : 0.11729655265808106
	model : 0.06903877258300781
			 train-loss:  2.0895092542144074 	 ± 0.26309721746487497
	data : 0.11707401275634766
	model : 0.06899151802062989
			 train-loss:  2.0905200127192907 	 ± 0.2626830147662043
	data : 0.11709814071655274
	model : 0.06905279159545899
			 train-loss:  2.09089512581175 	 ± 0.26198269326075097
	data : 0.11703543663024903
	model : 0.06964297294616699
			 train-loss:  2.092222096556324 	 ± 0.26183405705144513
	data : 0.11639766693115235
	model : 0.0698843002319336
			 train-loss:  2.0939933530400308 	 ± 0.2621587919583982
	data : 0.11626310348510742
	model : 0.06880249977111816
			 train-loss:  2.0965962090305776 	 ± 0.2637218347804046
	data : 0.11720423698425293
	model : 0.06861281394958496
			 train-loss:  2.094224124484592 	 ± 0.26489623665331513
	data : 0.11747260093688965
	model : 0.06769881248474122
			 train-loss:  2.0939682214958233 	 ± 0.2641857757085957
	data : 0.11827230453491211
	model : 0.06794333457946777
			 train-loss:  2.0942860134355317 	 ± 0.263493679979692
	data : 0.11821179389953614
	model : 0.06755228042602539
			 train-loss:  2.0964561681278417 	 ± 0.26439869209175476
	data : 0.11860189437866211
	model : 0.06846389770507813
			 train-loss:  2.0970733165740967 	 ± 0.26381137285521034
	data : 0.11778774261474609
	model : 0.06869297027587891
			 train-loss:  2.098156788542464 	 ± 0.26350757710074546
	data : 0.11741523742675782
	model : 0.06956639289855956
			 train-loss:  2.098466543741124 	 ± 0.2628320385463895
	data : 0.11676230430603027
	model : 0.06958994865417481
			 train-loss:  2.0965917913671483 	 ± 0.2633723573669662
	data : 0.11664342880249023
	model : 0.0701904296875
			 train-loss:  2.0951353330561457 	 ± 0.26342496751100686
	data : 0.11613121032714843
	model : 0.06987566947937011
			 train-loss:  2.0965590319305503 	 ± 0.2634513559825085
	data : 0.11640696525573731
	model : 0.06933355331420898
			 train-loss:  2.0958204796439723 	 ± 0.26295324836688494
	data : 0.11692953109741211
	model : 0.06920843124389649
			 train-loss:  2.0957143880933993 	 ± 0.26226806274080905
	data : 0.11671581268310546
	model : 0.06923623085021972
			 train-loss:  2.096873339265585 	 ± 0.2620740906534923
	data : 0.11674494743347168
	model : 0.06820955276489257
			 train-loss:  2.0976570331988555 	 ± 0.2616197263937621
	data : 0.1176717758178711
	model : 0.06844215393066407
			 train-loss:  2.096004132143001 	 ± 0.26195298123700733
	data : 0.11740241050720215
	model : 0.06864805221557617
			 train-loss:  2.0976564578520946 	 ± 0.26229206082332407
	data : 0.11730279922485351
	model : 0.06872611045837403
			 train-loss:  2.0980527826717923 	 ± 0.261680623539471
	data : 0.11748099327087402
	model : 0.06874256134033203
			 train-loss:  2.099120808131804 	 ± 0.2614435388182317
	data : 0.11741256713867188
	model : 0.06865663528442383
			 train-loss:  2.098719738950633 	 ± 0.2608432420827242
	data : 0.11758899688720703
	model : 0.06873421669006348
			 train-loss:  2.101145558620817 	 ± 0.26241654376252666
	data : 0.11744632720947265
	model : 0.0690680980682373
			 train-loss:  2.100596306324005 	 ± 0.26187432895779195
	data : 0.11708965301513671
	model : 0.06913084983825683
			 train-loss:  2.0984683125766357 	 ± 0.2629499015944081
	data : 0.11701393127441406
	model : 0.06833257675170898
			 train-loss:  2.0986615968222666 	 ± 0.26231254166926565
	data : 0.11774897575378418
	model : 0.06916155815124511
			 train-loss:  2.0970085283805586 	 ± 0.2627183031687728
	data : 0.11697368621826172
	model : 0.06893429756164551
			 train-loss:  2.096274304623697 	 ± 0.26228229677192033
	data : 0.1172698974609375
	model : 0.06894431114196778
			 train-loss:  2.095524773365114 	 ± 0.2618607245514034
	data : 0.11737957000732421
	model : 0.06898994445800781
			 train-loss:  2.097726254208574 	 ± 0.2631191861329538
	data : 0.11730661392211914
	model : 0.06988139152526855
			 train-loss:  2.097942719137035 	 ± 0.262501249483088
	data : 0.11626310348510742
	model : 0.07013020515441895
			 train-loss:  2.098121054470539 	 ± 0.26188204616015204
	data : 0.11618509292602539
	model : 0.06959609985351563
			 train-loss:  2.0971321295322984 	 ± 0.26164380340682925
	data : 0.11657247543334961
	model : 0.06989932060241699
			 train-loss:  2.0961583228338334 	 ± 0.26139947676768865
	data : 0.1162785530090332
	model : 0.06922321319580078
			 train-loss:  2.096499828365742 	 ± 0.26082626505742906
	data : 0.1169816017150879
	model : 0.06838784217834473
			 train-loss:  2.0960085864336984 	 ± 0.2603082037570228
	data : 0.11787142753601074
	model : 0.06814579963684082
			 train-loss:  2.096331627715921 	 ± 0.259739023993229
	data : 0.11773381233215333
	model : 0.06888437271118164
			 train-loss:  2.0948641060668733 	 ± 0.2600150526005959
	data : 0.11726484298706055
	model : 0.0678776741027832
			 train-loss:  2.095588794419932 	 ± 0.2596261921178878
	data : 0.11814727783203124
	model : 0.0682450294494629
			 train-loss:  2.0956780640063464 	 ± 0.2590278156479038
	data : 0.11769580841064453
	model : 0.06860923767089844
			 train-loss:  2.0959370163728566 	 ± 0.25845831000711056
	data : 0.11733036041259766
	model : 0.06866397857666015
			 train-loss:  2.0949219224649833 	 ± 0.2582980313238281
	data : 0.11743626594543458
	model : 0.06875762939453126
			 train-loss:  2.0951788392785478 	 ± 0.25773555153725797
	data : 0.11727781295776367
	model : 0.06938986778259278
			 train-loss:  2.094326165047559 	 ± 0.2574585316244168
	data : 0.11679558753967285
	model : 0.06915593147277832
			 train-loss:  2.0938189337156476 	 ± 0.25698553704366817
	data : 0.11692380905151367
	model : 0.06938862800598145
			 train-loss:  2.0964015684686266 	 ± 0.25926463742292083
	data : 0.1167475700378418
	model : 0.06917638778686523
			 train-loss:  2.0959027978871436 	 ± 0.2587893987997063
	data : 0.11679134368896485
	model : 0.06906447410583497
			 train-loss:  2.0963360954608237 	 ± 0.2582921573179601
	data : 0.11676549911499023
	model : 0.06911697387695312
			 train-loss:  2.097615286509196 	 ± 0.2584276804056704
	data : 0.11665549278259277
	model : 0.0692986011505127
			 train-loss:  2.096492779993378 	 ± 0.25840445514886334
	data : 0.11651730537414551
	model : 0.06908855438232422
			 train-loss:  2.095662379579922 	 ± 0.2581366895989284
	data : 0.11639480590820313
	model : 0.06896352767944336
			 train-loss:  2.093934048686111 	 ± 0.25888293049098426
	data : 0.11643972396850585
	model : 0.06880702972412109
			 train-loss:  2.0945947034910777 	 ± 0.258509613532768
	data : 0.11646847724914551
	model : 0.06853208541870118
			 train-loss:  2.0951566654702893 	 ± 0.25808716676174065
	data : 0.11656656265258789
	model : 0.0686835765838623
			 train-loss:  2.094669702249172 	 ± 0.25763380153037263
	data : 0.11653356552124024
	model : 0.0689002513885498
			 train-loss:  2.0933619665688483 	 ± 0.2578451583190721
	data : 0.1168250560760498
	model : 0.06910204887390137
			 train-loss:  2.0922864383893973 	 ± 0.2578122467883849
	data : 0.11674599647521973
	model : 0.0693288803100586
			 train-loss:  2.0913627677493625 	 ± 0.257646841511814
	data : 0.11661248207092285
	model : 0.06950078010559083
			 train-loss:  2.092061673833969 	 ± 0.25732026874240194
	data : 0.11652460098266601
	model : 0.06953811645507812
			 train-loss:  2.0928534026873313 	 ± 0.2570611993634111
	data : 0.11649951934814454
	model : 0.06960825920104981
			 train-loss:  2.0909640225680066 	 ± 0.2581551905703942
	data : 0.11606202125549317
	model : 0.0696650505065918
			 train-loss:  2.0894562386665023 	 ± 0.25865591795854165
	data : 0.11623492240905761
	model : 0.06945099830627441
			 train-loss:  2.0905781530436114 	 ± 0.2586938812098852
	data : 0.11638755798339843
	model : 0.06931948661804199
			 train-loss:  2.091074675321579 	 ± 0.258268468922815
	data : 0.11635513305664062
	model : 0.06907439231872559
			 train-loss:  2.090243846549038 	 ± 0.258053277648706
	data : 0.116510009765625
	model : 0.06882772445678711
			 train-loss:  2.089490817598075 	 ± 0.2577847596713848
	data : 0.11674032211303711
	model : 0.06863393783569335
			 train-loss:  2.0896654658847384 	 ± 0.2572681378360125
	data : 0.11663260459899902
	model : 0.06857986450195312
			 train-loss:  2.0871572606876247 	 ± 0.25970054419425054
	data : 0.11667966842651367
	model : 0.06865396499633789
			 train-loss:  2.0873015924375884 	 ± 0.25917980713142846
	data : 0.11685223579406738
	model : 0.06877670288085938
			 train-loss:  2.0876101047042908 	 ± 0.25869755649929055
	data : 0.11665425300598145
	model : 0.06873183250427246
			 train-loss:  2.088514165357057 	 ± 0.25856244653846877
	data : 0.11661291122436523
	model : 0.06870713233947753
			 train-loss:  2.0887631384595746 	 ± 0.25807029070685605
	data : 0.11655759811401367
	model : 0.06897282600402832
			 train-loss:  2.0890851724578674 	 ± 0.2576014810206382
	data : 0.11638679504394531
	model : 0.06898465156555175
			 train-loss:  2.0881117882728577 	 ± 0.25754419164109527
	data : 0.1163252830505371
	model : 0.06915059089660644
			 train-loss:  2.0880429483504885 	 ± 0.25703294803434673
	data : 0.11633224487304687
	model : 0.0695608139038086
			 train-loss:  2.090477030901682 	 ± 0.2594048653850035
	data : 0.11629958152770996
	model : 0.06970376968383789
			 train-loss:  2.0912644924382446 	 ± 0.2591933186260707
	data : 0.11638088226318359
	model : 0.06955437660217285
			 train-loss:  2.0912296692217427 	 ± 0.2586831853869017
	data : 0.1164463996887207
	model : 0.06961526870727539
			 train-loss:  2.0925764425128115 	 ± 0.25906616066033133
	data : 0.11622276306152343
	model : 0.06943869590759277
			 train-loss:  2.0921619688160717 	 ± 0.25864437478422686
	data : 0.11516528129577637
	model : 0.06041455268859863
#epoch  28    val-loss:  2.403111740162498  train-loss:  2.0921619688160717  lr:  0.00125
			 train-loss:  2.1086862087249756 	 ± 0.0
	data : 5.945040702819824
	model : 0.07396793365478516
			 train-loss:  2.1593985557556152 	 ± 0.05071234703063965
	data : 3.0399415493011475
	model : 0.07161307334899902
			 train-loss:  2.112471262613932 	 ± 0.07822299178703729
	data : 2.0655715465545654
	model : 0.07004642486572266
			 train-loss:  2.189440667629242 	 ± 0.14953927664101663
	data : 1.5787214040756226
	model : 0.0692405104637146
			 train-loss:  2.0841944217681885 	 ± 0.2493926327661586
	data : 1.2865484714508058
	model : 0.06949405670166016
			 train-loss:  2.014333268006643 	 ± 0.27610410190633133
	data : 0.12062363624572754
	model : 0.06863741874694824
			 train-loss:  2.0212545565196445 	 ± 0.2561843165695856
	data : 0.11701850891113282
	model : 0.06804709434509278
			 train-loss:  2.021974191069603 	 ± 0.239646048718513
	data : 0.11763348579406738
	model : 0.06872344017028809
			 train-loss:  1.9880671898523967 	 ± 0.24545176715651595
	data : 0.11709752082824706
	model : 0.06947035789489746
			 train-loss:  1.9686879754066466 	 ± 0.24000395532790733
	data : 0.11649961471557617
	model : 0.06941089630126954
			 train-loss:  1.958403143015775 	 ± 0.23113446327237888
	data : 0.1164919376373291
	model : 0.06949586868286133
			 train-loss:  1.951421211163203 	 ± 0.22250265810965042
	data : 0.11620664596557617
	model : 0.07021083831787109
			 train-loss:  1.9768800460375273 	 ± 0.2312509374622138
	data : 0.11541709899902344
	model : 0.07013564109802246
			 train-loss:  1.9786739264215742 	 ± 0.22293282560423863
	data : 0.11568746566772461
	model : 0.06911988258361816
			 train-loss:  1.9837706009546916 	 ± 0.21621618894349373
	data : 0.11672654151916503
	model : 0.06909165382385254
			 train-loss:  1.990877203643322 	 ± 0.21115197693907886
	data : 0.11685070991516114
	model : 0.06850142478942871
			 train-loss:  2.0117705639670875 	 ± 0.22123973680054815
	data : 0.11746468544006347
	model : 0.06851305961608886
			 train-loss:  2.014555699295468 	 ± 0.21531282086056283
	data : 0.11747889518737793
	model : 0.0685889720916748
			 train-loss:  1.9978292929498773 	 ± 0.22125898706268726
	data : 0.11734442710876465
	model : 0.06940817832946777
			 train-loss:  2.027202087640762 	 ± 0.25079916801957475
	data : 0.1165771484375
	model : 0.06908106803894043
			 train-loss:  2.040538884344555 	 ± 0.2519174009604379
	data : 0.11671051979064942
	model : 0.06943717002868652
			 train-loss:  2.060530624606393 	 ± 0.26262289820869955
	data : 0.11629900932312012
	model : 0.06846752166748046
			 train-loss:  2.068563031113666 	 ± 0.25959870377788585
	data : 0.1170867919921875
	model : 0.06745634078979493
			 train-loss:  2.060306931535403 	 ± 0.2571988754323965
	data : 0.1179288387298584
	model : 0.06752457618713378
			 train-loss:  2.0580465745925904 	 ± 0.2522455791227191
	data : 0.11793751716613769
	model : 0.06783046722412109
			 train-loss:  2.0460025713993955 	 ± 0.2545723044539907
	data : 0.11775140762329102
	model : 0.06756987571716308
			 train-loss:  2.039376872557181 	 ± 0.2520876708493017
	data : 0.11816792488098145
	model : 0.06780214309692383
			 train-loss:  2.0465173423290253 	 ± 0.25031029898033147
	data : 0.11806130409240723
	model : 0.06878390312194824
			 train-loss:  2.0536972530956925 	 ± 0.2488737659547641
	data : 0.11726760864257812
	model : 0.06793036460876464
			 train-loss:  2.0551657557487486 	 ± 0.24481847323212513
	data : 0.11801228523254395
	model : 0.06797428131103515
			 train-loss:  2.0718743685753114 	 ± 0.25763927305803663
	data : 0.11810412406921386
	model : 0.0680532455444336
			 train-loss:  2.062193762511015 	 ± 0.2592466409124233
	data : 0.11800293922424317
	model : 0.0679234504699707
			 train-loss:  2.0729865991708003 	 ± 0.2624875620511891
	data : 0.11809167861938477
	model : 0.06699204444885254
			 train-loss:  2.068435188601999 	 ± 0.2599170329764789
	data : 0.11885747909545899
	model : 0.067924165725708
			 train-loss:  2.0691277742385865 	 ± 0.25620885398731313
	data : 0.11791791915893554
	model : 0.06699619293212891
			 train-loss:  2.07573824789789 	 ± 0.25563451157379763
	data : 0.11859097480773925
	model : 0.0673293113708496
			 train-loss:  2.078740393793261 	 ± 0.2527988882545984
	data : 0.11821570396423339
	model : 0.06831755638122558
			 train-loss:  2.068014122937855 	 ± 0.2578419365246587
	data : 0.11732115745544433
	model : 0.06843199729919433
			 train-loss:  2.058121335812104 	 ± 0.26171881986428114
	data : 0.11717395782470703
	model : 0.06840643882751465
			 train-loss:  2.058314922451973 	 ± 0.25842945596443057
	data : 0.11745200157165528
	model : 0.06911182403564453
			 train-loss:  2.0586696397967454 	 ± 0.25526828070082397
	data : 0.11699366569519043
	model : 0.06910200119018554
			 train-loss:  2.0659666770980474 	 ± 0.2565025113459453
	data : 0.11710658073425292
	model : 0.06853628158569336
			 train-loss:  2.0693528402683348 	 ± 0.25445045152793827
	data : 0.11764659881591796
	model : 0.06904692649841308
			 train-loss:  2.0742433965206146 	 ± 0.2535784070381068
	data : 0.11733832359313964
	model : 0.06910209655761719
			 train-loss:  2.065296750598484 	 ± 0.25767216344935207
	data : 0.11694316864013672
	model : 0.06930837631225586
			 train-loss:  2.066755543584409 	 ± 0.2550437980754272
	data : 0.11671252250671386
	model : 0.069268798828125
			 train-loss:  2.057025909423828 	 ± 0.2608025669414647
	data : 0.1166388988494873
	model : 0.06976485252380371
			 train-loss:  2.0569758315881095 	 ± 0.2580718031126259
	data : 0.11622295379638672
	model : 0.0700411319732666
			 train-loss:  2.0582296945610823 	 ± 0.25557252320876483
	data : 0.11586332321166992
	model : 0.06996159553527832
			 train-loss:  2.060314269065857 	 ± 0.2534243386483243
	data : 0.1162684440612793
	model : 0.06904263496398926
			 train-loss:  2.0615774603451 	 ± 0.2510864113461588
	data : 0.11690893173217773
	model : 0.06910905838012696
			 train-loss:  2.0620416081868687 	 ± 0.24868249037501272
	data : 0.1167940616607666
	model : 0.06906614303588868
			 train-loss:  2.0609427443090476 	 ± 0.24645267784178632
	data : 0.1167837142944336
	model : 0.06913080215454101
			 train-loss:  2.060192011020802 	 ± 0.24422120788489557
	data : 0.11675848960876464
	model : 0.06823992729187012
			 train-loss:  2.061639404296875 	 ± 0.24222446036791037
	data : 0.11750826835632325
	model : 0.06886434555053711
			 train-loss:  2.0677006372383664 	 ± 0.24422445121253708
	data : 0.11702914237976074
	model : 0.06868023872375488
			 train-loss:  2.0629072795834458 	 ± 0.24471583599516533
	data : 0.11722311973571778
	model : 0.06775116920471191
			 train-loss:  2.0618575297552963 	 ± 0.24272646838649825
	data : 0.11803302764892579
	model : 0.06764535903930664
			 train-loss:  2.065090957334486 	 ± 0.2419172440407382
	data : 0.11806669235229492
	model : 0.06851563453674317
			 train-loss:  2.062868920962016 	 ± 0.24049919551006663
	data : 0.11720166206359864
	model : 0.0688521385192871
			 train-loss:  2.061748766508259 	 ± 0.23867750904709006
	data : 0.11696314811706543
	model : 0.06849193572998047
			 train-loss:  2.0649826295914187 	 ± 0.23808834829529263
	data : 0.11733746528625488
	model : 0.06952357292175293
			 train-loss:  2.0661391833471874 	 ± 0.2363666961885694
	data : 0.11642594337463379
	model : 0.06943578720092773
			 train-loss:  2.060919065028429 	 ± 0.2381448836384517
	data : 0.11644964218139649
	model : 0.06937265396118164
			 train-loss:  2.0630056051107553 	 ± 0.2368947270441526
	data : 0.11662154197692871
	model : 0.06932883262634278
			 train-loss:  2.069006641705831 	 ± 0.24002006321158806
	data : 0.11663851737976075
	model : 0.06936144828796387
			 train-loss:  2.0694377457917628 	 ± 0.2382478786001674
	data : 0.1165163516998291
	model : 0.06913304328918457
			 train-loss:  2.0677945806699642 	 ± 0.23687172697910955
	data : 0.11679506301879883
	model : 0.06911215782165528
			 train-loss:  2.065737997276196 	 ± 0.23575975250792897
	data : 0.1168257713317871
	model : 0.0691537857055664
			 train-loss:  2.0655344213758196 	 ± 0.23407580495827288
	data : 0.11673808097839355
	model : 0.06915750503540039
			 train-loss:  2.0695018936210956 	 ± 0.2347799607449404
	data : 0.11688580513000488
	model : 0.07018938064575195
			 train-loss:  2.0686460865868463 	 ± 0.23325533772386634
	data : 0.11602263450622559
	model : 0.07069082260131836
			 train-loss:  2.069350053186286 	 ± 0.2317291905522878
	data : 0.11555109024047852
	model : 0.07065658569335938
			 train-loss:  2.070121301187051 	 ± 0.23025243880139304
	data : 0.11562108993530273
	model : 0.07045211791992187
			 train-loss:  2.0656481329600016 	 ± 0.23192668327457153
	data : 0.11581315994262695
	model : 0.07031965255737305
			 train-loss:  2.069548588050039 	 ± 0.232858840671354
	data : 0.11574873924255372
	model : 0.06881804466247558
			 train-loss:  2.0647614838240984 	 ± 0.23507591463827615
	data : 0.11692109107971191
	model : 0.06841378211975098
			 train-loss:  2.0726418923109007 	 ± 0.2435856724727833
	data : 0.11717133522033692
	model : 0.06758441925048828
			 train-loss:  2.0736688631999343 	 ± 0.24220896142852433
	data : 0.11783547401428222
	model : 0.0668856143951416
			 train-loss:  2.077424040436745 	 ± 0.2429935665077159
	data : 0.11837644577026367
	model : 0.06778335571289062
			 train-loss:  2.076472912305667 	 ± 0.2416387459459278
	data : 0.11740479469299317
	model : 0.06888446807861329
			 train-loss:  2.0743566736942385 	 ± 0.24091487053226562
	data : 0.11671204566955566
	model : 0.06811385154724121
			 train-loss:  2.077829787530095 	 ± 0.24151568200639265
	data : 0.11741108894348144
	model : 0.06918392181396485
			 train-loss:  2.079062043201356 	 ± 0.24033612642675042
	data : 0.11657743453979492
	model : 0.06931352615356445
			 train-loss:  2.084873007325565 	 ± 0.24478227201441444
	data : 0.11635193824768067
	model : 0.06786665916442872
			 train-loss:  2.085575762183167 	 ± 0.24344119216218738
	data : 0.11792464256286621
	model : 0.06797361373901367
			 train-loss:  2.0817146520505005 	 ± 0.24467228249358464
	data : 0.11787495613098145
	model : 0.069588041305542
			 train-loss:  2.079613696445118 	 ± 0.2440661106662843
	data : 0.11646881103515624
	model : 0.06944851875305176
			 train-loss:  2.0758239097809525 	 ± 0.24528118254865083
	data : 0.11650123596191406
	model : 0.07023425102233886
			 train-loss:  2.0731578045421175 	 ± 0.2452080845580642
	data : 0.1160388469696045
	model : 0.07072086334228515
			 train-loss:  2.0715142095481958 	 ± 0.24435505892503148
	data : 0.11556291580200195
	model : 0.07047815322875976
			 train-loss:  2.0692100615605065 	 ± 0.2440153838890467
	data : 0.11591415405273438
	model : 0.07018404006958008
			 train-loss:  2.0727530589667698 	 ± 0.24506757276721264
	data : 0.11632556915283203
	model : 0.07030119895935058
			 train-loss:  2.072079010466312 	 ± 0.24384719159969864
	data : 0.11620912551879883
	model : 0.07025799751281739
			 train-loss:  2.0685938998272544 	 ± 0.24490256523911147
	data : 0.11623620986938477
	model : 0.07043375968933105
			 train-loss:  2.0720330911378064 	 ± 0.24591902482545583
	data : 0.11603055000305176
	model : 0.07007856369018554
			 train-loss:  2.0711224435530986 	 ± 0.24481076757476772
	data : 0.1161353588104248
	model : 0.06952385902404785
			 train-loss:  2.0775188511731675 	 ± 0.2515738831241372
	data : 0.1164398193359375
	model : 0.06940169334411621
			 train-loss:  2.077935465658554 	 ± 0.2503340592899162
	data : 0.11639652252197266
	model : 0.06952424049377441
			 train-loss:  2.077288728952408 	 ± 0.24916235348965177
	data : 0.11611990928649903
	model : 0.07029209136962891
			 train-loss:  2.0795382332093646 	 ± 0.24894423727689918
	data : 0.11534333229064941
	model : 0.070521879196167
			 train-loss:  2.085127766225852 	 ± 0.2540102032765489
	data : 0.11519536972045899
	model : 0.06982817649841308
			 train-loss:  2.0856387094386575 	 ± 0.25282680343506403
	data : 0.11594791412353515
	model : 0.06990065574645996
			 train-loss:  2.0835296385563336 	 ± 0.2525171802065004
	data : 0.1160588264465332
	model : 0.07058591842651367
			 train-loss:  2.0828930934270224 	 ± 0.25139566626343846
	data : 0.11528244018554687
	model : 0.069873046875
			 train-loss:  2.08730563015308 	 ± 0.2542596253920494
	data : 0.11604976654052734
	model : 0.07023825645446777
			 train-loss:  2.0844686510406922 	 ± 0.2547487121475342
	data : 0.11561470031738282
	model : 0.0711815357208252
			 train-loss:  2.083939337068134 	 ± 0.2536256840326291
	data : 0.11509013175964355
	model : 0.07079319953918457
			 train-loss:  2.0840937052298028 	 ± 0.25246467973748077
	data : 0.11537694931030273
	model : 0.0691333293914795
			 train-loss:  2.0840347929434344 	 ± 0.2513152456272013
	data : 0.11707420349121093
	model : 0.06933956146240235
			 train-loss:  2.0816968808303007 	 ± 0.25137937692396767
	data : 0.11676959991455078
	model : 0.0691598892211914
			 train-loss:  2.082099019416741 	 ± 0.2502904930707913
	data : 0.11687884330749512
	model : 0.0687222957611084
			 train-loss:  2.081542079427601 	 ± 0.2492502514805248
	data : 0.11692962646484376
	model : 0.06818146705627441
			 train-loss:  2.0828168465380084 	 ± 0.2485243533597056
	data : 0.11749014854431153
	model : 0.06815199851989746
			 train-loss:  2.0877308254656586 	 ± 0.252942784847951
	data : 0.11747198104858399
	model : 0.06785669326782226
			 train-loss:  2.0871390402317047 	 ± 0.25193009814922274
	data : 0.11781783103942871
	model : 0.06782441139221192
			 train-loss:  2.08943966209379 	 ± 0.25207197001467085
	data : 0.11774349212646484
	model : 0.06805253028869629
			 train-loss:  2.088608001248311 	 ± 0.2511627466931509
	data : 0.11747202873229981
	model : 0.06909193992614746
			 train-loss:  2.086413322376604 	 ± 0.2512388883159252
	data : 0.1168121337890625
	model : 0.0700235366821289
			 train-loss:  2.089365753531456 	 ± 0.2522543873970143
	data : 0.11602091789245605
	model : 0.07013583183288574
			 train-loss:  2.0877914645455102 	 ± 0.25180110341339845
	data : 0.1160083293914795
	model : 0.07018933296203614
			 train-loss:  2.089927937163681 	 ± 0.25186583488428177
	data : 0.11605644226074219
	model : 0.06992311477661133
			 train-loss:  2.0928200783768323 	 ± 0.25286581888353044
	data : 0.1163325309753418
	model : 0.0698580265045166
			 train-loss:  2.090426681503173 	 ± 0.25323912617173955
	data : 0.11623749732971192
	model : 0.07009525299072265
			 train-loss:  2.092152599334717 	 ± 0.2529553020532014
	data : 0.11588420867919921
	model : 0.06994953155517578
			 train-loss:  2.0933502704378157 	 ± 0.2523050899102914
	data : 0.11601781845092773
	model : 0.06972661018371581
			 train-loss:  2.0925931864836085 	 ± 0.25145344619925847
	data : 0.11630015373229981
	model : 0.06943869590759277
			 train-loss:  2.090168969705701 	 ± 0.25195479228340584
	data : 0.11663589477539063
	model : 0.06869325637817383
			 train-loss:  2.0929193145544955 	 ± 0.252897919491708
	data : 0.1172867774963379
	model : 0.06823596954345704
			 train-loss:  2.091549297479483 	 ± 0.2524034550954474
	data : 0.11787152290344238
	model : 0.06830706596374511
			 train-loss:  2.0930413253434743 	 ± 0.2520130680097609
	data : 0.11779885292053223
	model : 0.06849827766418456
			 train-loss:  2.093306162140586 	 ± 0.2510749565141517
	data : 0.11750450134277343
	model : 0.06914129257202148
			 train-loss:  2.0940475463867188 	 ± 0.25027427550129483
	data : 0.11680512428283692
	model : 0.06983838081359864
			 train-loss:  2.0925583447983014 	 ± 0.2499294461085775
	data : 0.11623497009277343
	model : 0.06990838050842285
			 train-loss:  2.0936965465545656 	 ± 0.2493504036028422
	data : 0.11601819992065429
	model : 0.06969447135925293
			 train-loss:  2.0903497899279877 	 ± 0.25145686580709375
	data : 0.11620874404907226
	model : 0.06952643394470215
			 train-loss:  2.089900935653352 	 ± 0.250592135556602
	data : 0.11648316383361816
	model : 0.06989583969116211
			 train-loss:  2.088907918204432 	 ± 0.24995292627459254
	data : 0.11631922721862793
	model : 0.07017502784729004
			 train-loss:  2.087313916185777 	 ± 0.24975514135961915
	data : 0.11612639427185059
	model : 0.07030057907104492
			 train-loss:  2.0856272876262665 	 ± 0.24965474487942693
	data : 0.11618928909301758
	model : 0.07049617767333985
			 train-loss:  2.0852805240779904 	 ± 0.24880170213883665
	data : 0.11598320007324218
	model : 0.07073740959167481
			 train-loss:  2.0852903102485225 	 ± 0.24792411922225108
	data : 0.11583576202392579
	model : 0.07024192810058594
			 train-loss:  2.0854754139493394 	 ± 0.2470655774174126
	data : 0.1164334774017334
	model : 0.07006826400756835
			 train-loss:  2.0860662634174028 	 ± 0.2463075773886791
	data : 0.11630740165710449
	model : 0.0701019287109375
			 train-loss:  2.0851805275884168 	 ± 0.2456867898090243
	data : 0.1161870002746582
	model : 0.0701286792755127
			 train-loss:  2.0850375204870146 	 ± 0.24485000661424786
	data : 0.11625499725341797
	model : 0.07020068168640137
			 train-loss:  2.0831237690789357 	 ± 0.24510897396984355
	data : 0.11610107421875
	model : 0.07024407386779785
			 train-loss:  2.0808967021671503 	 ± 0.2457673055903407
	data : 0.11594209671020508
	model : 0.07044081687927246
			 train-loss:  2.078569201815048 	 ± 0.2465723889862159
	data : 0.11602311134338379
	model : 0.07052927017211914
			 train-loss:  2.079504357179006 	 ± 0.24601407725566576
	data : 0.11621618270874023
	model : 0.07045164108276367
			 train-loss:  2.081653783652956 	 ± 0.24660721249011958
	data : 0.11603455543518067
	model : 0.07031922340393067
			 train-loss:  2.080426027900294 	 ± 0.24625724925158668
	data : 0.11623501777648926
	model : 0.06990542411804199
			 train-loss:  2.0798191541160636 	 ± 0.24556517809301331
	data : 0.1166081428527832
	model : 0.07017087936401367
			 train-loss:  2.080487062404682 	 ± 0.2449059760871477
	data : 0.11606149673461914
	model : 0.06992635726928711
			 train-loss:  2.0827034565710254 	 ± 0.24565928908963558
	data : 0.11606040000915527
	model : 0.06979250907897949
			 train-loss:  2.083112569955679 	 ± 0.24492362070204973
	data : 0.11631541252136231
	model : 0.06980609893798828
			 train-loss:  2.0839569599005827 	 ± 0.24437004850075245
	data : 0.11639652252197266
	model : 0.06987028121948242
			 train-loss:  2.0826490212090407 	 ± 0.24414615938993534
	data : 0.11629509925842285
	model : 0.06961822509765625
			 train-loss:  2.08367103600652 	 ± 0.24371600567055887
	data : 0.11665425300598145
	model : 0.07006678581237794
			 train-loss:  2.081512928009033 	 ± 0.24447246979796802
	data : 0.11621661186218261
	model : 0.06936368942260743
			 train-loss:  2.0843126507279295 	 ± 0.24627163411322622
	data : 0.11688089370727539
	model : 0.07011919021606446
			 train-loss:  2.0852754837200966 	 ± 0.2458141399952756
	data : 0.11599822044372558
	model : 0.06955461502075196
			 train-loss:  2.085832319376659 	 ± 0.24516141435879404
	data : 0.11644163131713867
	model : 0.06884498596191406
			 train-loss:  2.088677231858416 	 ± 0.24709689489159786
	data : 0.11712007522583008
	model : 0.0685297966003418
			 train-loss:  2.088422993457679 	 ± 0.24636849289336346
	data : 0.11739988327026367
	model : 0.06905918121337891
			 train-loss:  2.0906716484621346 	 ± 0.24731781422962984
	data : 0.11695284843444824
	model : 0.06738924980163574
			 train-loss:  2.0931434360092984 	 ± 0.24862432090740533
	data : 0.11861610412597656
	model : 0.06841363906860351
			 train-loss:  2.0920612939766476 	 ± 0.24827741418740423
	data : 0.11765046119689941
	model : 0.06852869987487793
			 train-loss:  2.094486982864741 	 ± 0.2495304333059454
	data : 0.11735353469848633
	model : 0.06933388710021973
			 train-loss:  2.095654180470635 	 ± 0.2492577121394198
	data : 0.11658029556274414
	model : 0.06969146728515625
			 train-loss:  2.0962054297240855 	 ± 0.2486317278040501
	data : 0.11616554260253906
	model : 0.06992568969726562
			 train-loss:  2.0939219192016956 	 ± 0.249699812776978
	data : 0.115928316116333
	model : 0.06978936195373535
			 train-loss:  2.0921826610675436 	 ± 0.25001978949724646
	data : 0.1160468578338623
	model : 0.0695263385772705
			 train-loss:  2.0927401726273285 	 ± 0.24940812853719155
	data : 0.11645126342773438
	model : 0.06844196319580079
			 train-loss:  2.0951753153119768 	 ± 0.25076037734156015
	data : 0.11752786636352539
	model : 0.06846604347229004
			 train-loss:  2.095024186101827 	 ± 0.2500549674907821
	data : 0.11745195388793946
	model : 0.06838164329528809
			 train-loss:  2.095015216008418 	 ± 0.2493476253970123
	data : 0.11761245727539063
	model : 0.06836514472961426
			 train-loss:  2.0955657342846474 	 ± 0.2487540718732238
	data : 0.11751179695129395
	model : 0.0697199821472168
			 train-loss:  2.0957661394300406 	 ± 0.248072664176842
	data : 0.11601157188415527
	model : 0.06998863220214843
			 train-loss:  2.0946702546543543 	 ± 0.24781672667404903
	data : 0.11561145782470703
	model : 0.07001700401306152
			 train-loss:  2.0967807664397013 	 ± 0.24874806081225082
	data : 0.11575055122375488
	model : 0.07003145217895508
			 train-loss:  2.0968552893334693 	 ± 0.2480657718403847
	data : 0.1155787467956543
	model : 0.07019104957580566
			 train-loss:  2.0965308595876224 	 ± 0.24742578237666535
	data : 0.11559157371520996
	model : 0.06940937042236328
			 train-loss:  2.0971582104330477 	 ± 0.24689841262424456
	data : 0.1166292667388916
	model : 0.0685262680053711
			 train-loss:  2.0988713341790275 	 ± 0.24732432329688542
	data : 0.11749577522277832
	model : 0.0683450698852539
			 train-loss:  2.1008989490488523 	 ± 0.24819554721789097
	data : 0.11756582260131836
	model : 0.06889185905456544
			 train-loss:  2.102286742970268 	 ± 0.24825358642725137
	data : 0.11700654029846191
	model : 0.06876077651977539
			 train-loss:  2.1034032144445054 	 ± 0.24806273794870634
	data : 0.1171454906463623
	model : 0.06872038841247559
			 train-loss:  2.1027341317878196 	 ± 0.2475756481639918
	data : 0.1171867847442627
	model : 0.07012853622436524
			 train-loss:  2.102940139017607 	 ± 0.2469395150669822
	data : 0.11601037979125976
	model : 0.07050914764404297
			 train-loss:  2.1017118892120443 	 ± 0.24687343966654227
	data : 0.11577816009521484
	model : 0.0700758934020996
			 train-loss:  2.1040185894817114 	 ± 0.24828481826691123
	data : 0.1163548469543457
	model : 0.07007355690002441
			 train-loss:  2.1020515354186142 	 ± 0.249136209616605
	data : 0.11652617454528809
	model : 0.07014355659484864
			 train-loss:  2.10203713424427 	 ± 0.24849335693033556
	data : 0.11634955406188965
	model : 0.06929221153259277
			 train-loss:  2.10089894258059 	 ± 0.24836185362397756
	data : 0.11716103553771973
	model : 0.06897501945495606
			 train-loss:  2.0990716790666384 	 ± 0.249038113484752
	data : 0.11740293502807617
	model : 0.06932973861694336
			 train-loss:  2.0996045364341156 	 ± 0.24851722550251962
	data : 0.11716523170471191
	model : 0.06923146247863769
			 train-loss:  2.100810819201999 	 ± 0.24846638877438867
	data : 0.11706328392028809
	model : 0.06915640830993652
			 train-loss:  2.1007944938525482 	 ± 0.2478414215679553
	data : 0.11708607673645019
	model : 0.06952247619628907
			 train-loss:  2.1012592160701753 	 ± 0.24730794725356578
	data : 0.11681141853332519
	model : 0.06965851783752441
			 train-loss:  2.0991067150932046 	 ± 0.24856304670138077
	data : 0.11659760475158691
	model : 0.06987700462341309
			 train-loss:  2.0981552990356294 	 ± 0.2483136575652998
	data : 0.11629056930541992
	model : 0.06991934776306152
			 train-loss:  2.097424985152747 	 ± 0.24791867354935018
	data : 0.11640744209289551
	model : 0.06958537101745606
			 train-loss:  2.095955922907474 	 ± 0.2481944387141014
	data : 0.1168677806854248
	model : 0.0696683406829834
			 train-loss:  2.097147977061388 	 ± 0.24817306835905337
	data : 0.11681280136108399
	model : 0.070377779006958
			 train-loss:  2.098160454949129 	 ± 0.2479940316827539
	data : 0.11611018180847169
	model : 0.06951227188110351
			 train-loss:  2.097748872162639 	 ± 0.24746480509036226
	data : 0.11683964729309082
	model : 0.06860871315002441
			 train-loss:  2.0978941006156115 	 ± 0.24687806343919014
	data : 0.11746621131896973
	model : 0.06981287002563477
			 train-loss:  2.096560023618087 	 ± 0.24703713817495368
	data : 0.11614451408386231
	model : 0.06938276290893555
			 train-loss:  2.0958307896341597 	 ± 0.24667363803382308
	data : 0.11623716354370117
	model : 0.06843857765197754
			 train-loss:  2.0950835649436117 	 ± 0.24632652618042875
	data : 0.11738071441650391
	model : 0.06936421394348144
			 train-loss:  2.0948850765543163 	 ± 0.2457617937801123
	data : 0.11659073829650879
	model : 0.07003750801086425
			 train-loss:  2.093772867475877 	 ± 0.24571842111984035
	data : 0.11602106094360351
	model : 0.06917314529418946
			 train-loss:  2.093891998874807 	 ± 0.24514980600947103
	data : 0.11701822280883789
	model : 0.06949515342712402
			 train-loss:  2.0930355094199955 	 ± 0.2448997441670555
	data : 0.11691350936889648
	model : 0.0696950912475586
			 train-loss:  2.0923167864481607 	 ± 0.2445593579132746
	data : 0.11647067070007325
	model : 0.06963524818420411
			 train-loss:  2.0908696365795927 	 ± 0.24492043029422628
	data : 0.11660575866699219
	model : 0.06997489929199219
			 train-loss:  2.0908930782878072 	 ± 0.24435828449957853
	data : 0.11641964912414551
	model : 0.06940946578979493
			 train-loss:  2.089225010784794 	 ± 0.2450405931503101
	data : 0.11673412322998047
	model : 0.06915464401245117
			 train-loss:  2.087290990894491 	 ± 0.24615262699362192
	data : 0.11698989868164063
	model : 0.0690645694732666
			 train-loss:  2.0888689993733194 	 ± 0.24670786813194667
	data : 0.11726317405700684
	model : 0.06829781532287597
			 train-loss:  2.089251940851813 	 ± 0.2462174139263156
	data : 0.11783699989318848
	model : 0.06749420166015625
			 train-loss:  2.0870800921735206 	 ± 0.24778684500793777
	data : 0.11835012435913086
	model : 0.06750741004943847
			 train-loss:  2.087230053863355 	 ± 0.2472432726102216
	data : 0.11837258338928222
	model : 0.06689610481262206
			 train-loss:  2.087080018255446 	 ± 0.2467034510327667
	data : 0.11887307167053222
	model : 0.06598925590515137
			 train-loss:  2.087711325261445 	 ± 0.2463391212623103
	data : 0.11945734024047852
	model : 0.0661656379699707
			 train-loss:  2.0907027022954128 	 ± 0.2498758909938789
	data : 0.11894211769104004
	model : 0.06796274185180665
			 train-loss:  2.0917966182817493 	 ± 0.24987146743939206
	data : 0.11701359748840331
	model : 0.06816167831420898
			 train-loss:  2.0911743245270573 	 ± 0.24950230045565608
	data : 0.11698055267333984
	model : 0.06864304542541504
			 train-loss:  2.090962619366853 	 ± 0.24897992539344532
	data : 0.11646356582641601
	model : 0.06970019340515136
			 train-loss:  2.0946172026844767 	 ± 0.2545476755921293
	data : 0.11538047790527343
	model : 0.06954946517944335
			 train-loss:  2.09285227029488 	 ± 0.255411030054282
	data : 0.11559267044067383
	model : 0.06840729713439941
			 train-loss:  2.0945916764214316 	 ± 0.2562357126313959
	data : 0.11684651374816894
	model : 0.0687840461730957
			 train-loss:  2.0954136894299435 	 ± 0.2559953035153047
	data : 0.11626362800598145
	model : 0.06856331825256348
			 train-loss:  2.09516378209946 	 ± 0.2554786551124607
	data : 0.1164327621459961
	model : 0.06829876899719238
			 train-loss:  2.09530765979977 	 ± 0.2549463529660851
	data : 0.11710071563720703
	model : 0.0694847583770752
			 train-loss:  2.0950816487461204 	 ± 0.2544316142858129
	data : 0.1161280632019043
	model : 0.0693967342376709
			 train-loss:  2.0953127882059883 	 ± 0.2539214652328185
	data : 0.1164919376373291
	model : 0.06959328651428223
			 train-loss:  2.0966034459269696 	 ± 0.25417080035870626
	data : 0.11650123596191406
	model : 0.0696929931640625
			 train-loss:  2.097274646659692 	 ± 0.2538528893484998
	data : 0.11651086807250977
	model : 0.06967144012451172
			 train-loss:  2.097047649973161 	 ± 0.2533500834558738
	data : 0.11651344299316406
	model : 0.06947841644287109
			 train-loss:  2.096646776376677 	 ± 0.2529026708069811
	data : 0.11666579246520996
	model : 0.06908478736877441
			 train-loss:  2.096954956466769 	 ± 0.2524272884435143
	data : 0.11687850952148438
	model : 0.06861376762390137
			 train-loss:  2.0963223386006278 	 ± 0.2521024399235166
	data : 0.11720447540283203
	model : 0.06930155754089355
			 train-loss:  2.0957055427590197 	 ± 0.2517718334005929
	data : 0.11641254425048828
	model : 0.06890373229980469
			 train-loss:  2.095712440285256 	 ± 0.2512596041152875
	data : 0.11673207283020019
	model : 0.06808128356933593
			 train-loss:  2.0944951598943486 	 ± 0.25147626487374974
	data : 0.11737704277038574
	model : 0.06868882179260254
			 train-loss:  2.0930390857881114 	 ± 0.2520098966963798
	data : 0.11660304069519042
	model : 0.06822924613952637
			 train-loss:  2.092683973082577 	 ± 0.251565510164692
	data : 0.11693463325500489
	model : 0.0680117130279541
			 train-loss:  2.093031448364258 	 ± 0.25112174161067274
	data : 0.11722078323364257
	model : 0.06873846054077148
			 train-loss:  2.092745386746775 	 ± 0.25066181077211874
	data : 0.11640286445617676
	model : 0.06872763633728027
			 train-loss:  2.093456870979733 	 ± 0.2504177935273598
	data : 0.11670961380004882
	model : 0.06838960647583008
			 train-loss:  2.0928788119154014 	 ± 0.25009081500163527
	data : 0.11733145713806152
	model : 0.0690310001373291
			 train-loss:  2.0913389897721957 	 ± 0.2507968304599813
	data : 0.11698365211486816
	model : 0.06915082931518554
			 train-loss:  2.0903087139129637 	 ± 0.2508425804899316
	data : 0.1169593334197998
	model : 0.06846966743469238
			 train-loss:  2.0902540124952793 	 ± 0.25035369808756086
	data : 0.11667923927307129
	model : 0.06012992858886719
#epoch  29    val-loss:  2.446899564642655  train-loss:  2.0902540124952793  lr:  0.00125
			 train-loss:  2.10685396194458 	 ± 0.0
	data : 5.602381706237793
	model : 0.07201385498046875
			 train-loss:  2.1430246829986572 	 ± 0.03617072105407715
	data : 2.929181218147278
	model : 0.07079541683197021
			 train-loss:  2.196027914683024 	 ± 0.080566116817315
	data : 1.9914881388346355
	model : 0.06990925470987956
			 train-loss:  2.219162106513977 	 ± 0.08045959780709774
	data : 1.5228326916694641
	model : 0.06882727146148682
			 train-loss:  2.2270081996917725 	 ± 0.07365624356456507
	data : 1.2421655654907227
	model : 0.06823415756225586
			 train-loss:  2.1934595505396524 	 ± 0.10074023267640213
	data : 0.14553227424621581
	model : 0.06768603324890136
			 train-loss:  2.1868981633867537 	 ± 0.09464198897176616
	data : 0.11761727333068847
	model : 0.06765589714050294
			 train-loss:  2.146930977702141 	 ± 0.13790975069879452
	data : 0.11771721839904785
	model : 0.0684929370880127
			 train-loss:  2.145148131582472 	 ± 0.13012030740948563
	data : 0.11703004837036132
	model : 0.06980133056640625
			 train-loss:  2.126209580898285 	 ± 0.13589033548587207
	data : 0.11585893630981445
	model : 0.07047672271728515
			 train-loss:  2.099258715456182 	 ± 0.1550836252634054
	data : 0.11530232429504395
	model : 0.07063655853271485
			 train-loss:  2.1174003183841705 	 ± 0.16020918174125257
	data : 0.11526823043823242
	model : 0.07035002708435059
			 train-loss:  2.1085758484326877 	 ± 0.15693009249066234
	data : 0.11557559967041016
	model : 0.06901240348815918
			 train-loss:  2.1065373676163808 	 ± 0.15140012847080314
	data : 0.1168283462524414
	model : 0.06857690811157227
			 train-loss:  2.096295054753621 	 ± 0.15120362184753297
	data : 0.1172370433807373
	model : 0.07028560638427735
			 train-loss:  2.089178279042244 	 ± 0.14897434092475217
	data : 0.11554112434387206
	model : 0.07035117149353028
			 train-loss:  2.0846089194802677 	 ± 0.1456774779304148
	data : 0.11528515815734863
	model : 0.06998400688171387
			 train-loss:  2.0882085694207086 	 ± 0.14234889797432915
	data : 0.11544771194458008
	model : 0.07083706855773926
			 train-loss:  2.104082458897641 	 ± 0.15405314242650714
	data : 0.11480741500854492
	model : 0.07085347175598145
			 train-loss:  2.0993602871894836 	 ± 0.15155669356413357
	data : 0.11490001678466796
	model : 0.06921429634094238
			 train-loss:  2.0860267196382796 	 ± 0.15947203325672496
	data : 0.1167797565460205
	model : 0.06905188560485839
			 train-loss:  2.0962710326368157 	 ± 0.16272438292496705
	data : 0.11704864501953124
	model : 0.06902551651000977
			 train-loss:  2.0977168860642808 	 ± 0.15929201166991436
	data : 0.11704998016357422
	model : 0.06810102462768555
			 train-loss:  2.1076161911090217 	 ± 0.16300494103785398
	data : 0.11777472496032715
	model : 0.06768302917480469
			 train-loss:  2.0921462249755858 	 ± 0.17678083273566603
	data : 0.11816062927246093
	model : 0.06786327362060547
			 train-loss:  2.0854720931786757 	 ± 0.17653068958241658
	data : 0.1180300235748291
	model : 0.06804814338684081
			 train-loss:  2.095122456550598 	 ± 0.18008404537357828
	data : 0.1176936149597168
	model : 0.06888298988342285
			 train-loss:  2.096584767103195 	 ± 0.17700218985491012
	data : 0.1169504165649414
	model : 0.06968994140625
			 train-loss:  2.0888441390004653 	 ± 0.17868162522487316
	data : 0.11624870300292969
	model : 0.0691133975982666
			 train-loss:  2.0831313053766887 	 ± 0.17835174046140082
	data : 0.11659717559814453
	model : 0.06907291412353515
			 train-loss:  2.0781038845739057 	 ± 0.17759922374336343
	data : 0.11649737358093262
	model : 0.06836013793945313
			 train-loss:  2.0665347948670387 	 ± 0.18629270470602624
	data : 0.11731858253479004
	model : 0.06783332824707031
			 train-loss:  2.0625849600994224 	 ± 0.18480406987911227
	data : 0.11781520843505859
	model : 0.06708807945251465
			 train-loss:  2.04992971350165 	 ± 0.19604382530786327
	data : 0.11850018501281738
	model : 0.0687528133392334
			 train-loss:  2.052618888446263 	 ± 0.1938581106041968
	data : 0.11695899963378906
	model : 0.06779651641845703
			 train-loss:  2.045392761627833 	 ± 0.1958689436934394
	data : 0.11768789291381836
	model : 0.06766958236694336
			 train-loss:  2.046566296268154 	 ± 0.19333219976355234
	data : 0.117802095413208
	model : 0.06824727058410644
			 train-loss:  2.0485724430335197 	 ± 0.19116128370616992
	data : 0.11752724647521973
	model : 0.06909947395324707
			 train-loss:  2.0512005457511315 	 ± 0.18938877691023467
	data : 0.11659770011901856
	model : 0.06830496788024902
			 train-loss:  2.053190162777901 	 ± 0.18741875722477983
	data : 0.11747584342956544
	model : 0.06830797195434571
			 train-loss:  2.057408463664171 	 ± 0.18703161886523026
	data : 0.11762619018554688
	model : 0.06896343231201171
			 train-loss:  2.051300335498083 	 ± 0.18888522454763623
	data : 0.11710109710693359
	model : 0.06872978210449218
			 train-loss:  2.0439803877542184 	 ± 0.19260930517756542
	data : 0.1172102928161621
	model : 0.06892285346984864
			 train-loss:  2.0402007319710473 	 ± 0.19201429518718705
	data : 0.11715784072875976
	model : 0.06814107894897461
			 train-loss:  2.036681890487671 	 ± 0.1912981608982151
	data : 0.11773810386657715
	model : 0.06809768676757813
			 train-loss:  2.04017740747203 	 ± 0.19065487650645502
	data : 0.1176600456237793
	model : 0.06810269355773926
			 train-loss:  2.0376521171407496 	 ± 0.1893917610567322
	data : 0.11756420135498047
	model : 0.06799745559692383
			 train-loss:  2.033202158908049 	 ± 0.18987538905618434
	data : 0.11766605377197266
	model : 0.06777653694152833
			 train-loss:  2.0286468875651456 	 ± 0.1905594883111513
	data : 0.1179743766784668
	model : 0.0687265396118164
			 train-loss:  2.0361793088912963 	 ± 0.1958744276172811
	data : 0.11719675064086914
	model : 0.06965727806091308
			 train-loss:  2.036928373224595 	 ± 0.19401689677919132
	data : 0.11639909744262696
	model : 0.06981821060180664
			 train-loss:  2.0433928691423855 	 ± 0.1976105665308099
	data : 0.11635966300964355
	model : 0.07009553909301758
			 train-loss:  2.041928437520873 	 ± 0.1960220953818977
	data : 0.11616101264953613
	model : 0.07029681205749512
			 train-loss:  2.0472423787470215 	 ± 0.1980144099482889
	data : 0.11575531959533691
	model : 0.06961793899536133
			 train-loss:  2.0437127980318937 	 ± 0.19791293932710555
	data : 0.11617555618286132
	model : 0.07054753303527832
			 train-loss:  2.0441349829946245 	 ± 0.19616288853999672
	data : 0.11520972251892089
	model : 0.07046031951904297
			 train-loss:  2.044954036411486 	 ± 0.19453113057463683
	data : 0.11526765823364257
	model : 0.07029728889465332
			 train-loss:  2.0440881601695358 	 ± 0.192957615920497
	data : 0.11522789001464843
	model : 0.07009015083312989
			 train-loss:  2.0437748856463673 	 ± 0.19133026941504616
	data : 0.11566925048828125
	model : 0.07072768211364747
			 train-loss:  2.047116281588872 	 ± 0.1914572575755685
	data : 0.11534233093261718
	model : 0.06930351257324219
			 train-loss:  2.0460208458978624 	 ± 0.19007094504351993
	data : 0.11661553382873535
	model : 0.06928763389587403
			 train-loss:  2.040584300794909 	 ± 0.19325420709941288
	data : 0.11655201911926269
	model : 0.0687779426574707
			 train-loss:  2.0438804494010077 	 ± 0.1934631256184179
	data : 0.1170802116394043
	model : 0.06881356239318848
			 train-loss:  2.043478900566697 	 ± 0.191972203674666
	data : 0.11693482398986817
	model : 0.06898951530456543
			 train-loss:  2.0365092442585873 	 ± 0.19848230072684803
	data : 0.11690611839294433
	model : 0.06964569091796875
			 train-loss:  2.0387877532930085 	 ± 0.19782765376787828
	data : 0.11642618179321289
	model : 0.06941213607788085
			 train-loss:  2.037082035150101 	 ± 0.19683416821348962
	data : 0.11658334732055664
	model : 0.07003211975097656
			 train-loss:  2.042209702379563 	 ± 0.19983883182206127
	data : 0.11599278450012207
	model : 0.06973443031311036
			 train-loss:  2.0442377657130146 	 ± 0.1990890988252491
	data : 0.11624612808227539
	model : 0.06952548027038574
			 train-loss:  2.0477697202137537 	 ± 0.19982739670731922
	data : 0.11623358726501465
	model : 0.06902780532836914
			 train-loss:  2.052306265898154 	 ± 0.20201286458716405
	data : 0.11664085388183594
	model : 0.06913280487060547
			 train-loss:  2.06011591023869 	 ± 0.21112255143656594
	data : 0.11632709503173828
	model : 0.06838030815124511
			 train-loss:  2.05738512620534 	 ± 0.21094801118388845
	data : 0.11701359748840331
	model : 0.06824750900268554
			 train-loss:  2.0587567848128243 	 ± 0.20984534872860675
	data : 0.11726155281066894
	model : 0.0683720588684082
			 train-loss:  2.0585926516850788 	 ± 0.2084464671032948
	data : 0.1171605110168457
	model : 0.06848559379577637
			 train-loss:  2.057669445088035 	 ± 0.20722486175354857
	data : 0.11691012382507324
	model : 0.06808481216430665
			 train-loss:  2.0539046015058244 	 ± 0.20847465251862946
	data : 0.11753959655761718
	model : 0.0689424991607666
			 train-loss:  2.050160056505448 	 ± 0.20972397153546332
	data : 0.11675024032592773
	model : 0.0695765495300293
			 train-loss:  2.0452237370647963 	 ± 0.2129038007822528
	data : 0.11604433059692383
	model : 0.06974854469299316
			 train-loss:  2.0483800798654554 	 ± 0.21342086477856798
	data : 0.11589469909667968
	model : 0.0690910816192627
			 train-loss:  2.0484571515777965 	 ± 0.21210048088556863
	data : 0.11677703857421876
	model : 0.06955552101135254
			 train-loss:  2.049930970843245 	 ± 0.21122012297480042
	data : 0.11628694534301758
	model : 0.0694361686706543
			 train-loss:  2.054733641176339 	 ± 0.2144010393323881
	data : 0.11633706092834473
	model : 0.06830244064331055
			 train-loss:  2.0539530714352927 	 ± 0.21323963253127537
	data : 0.1172290325164795
	model : 0.06707234382629394
			 train-loss:  2.056614112854004 	 ± 0.2133799478951968
	data : 0.11827869415283203
	model : 0.0670464038848877
			 train-loss:  2.058212172153384 	 ± 0.21264675956996698
	data : 0.11827468872070312
	model : 0.06721110343933105
			 train-loss:  2.061382981552475 	 ± 0.21345617019867416
	data : 0.11822571754455566
	model : 0.06697964668273926
			 train-loss:  2.0592488091100347 	 ± 0.2131713567527135
	data : 0.11840434074401855
	model : 0.06769251823425293
			 train-loss:  2.0598994172021245 	 ± 0.21205822870432645
	data : 0.11799612045288085
	model : 0.0689474105834961
			 train-loss:  2.057909177409278 	 ± 0.21171106269727424
	data : 0.11692223548889161
	model : 0.06992192268371582
			 train-loss:  2.055334678063026 	 ± 0.2119564896811924
	data : 0.11590828895568847
	model : 0.06995658874511719
			 train-loss:  2.0560254143631975 	 ± 0.21090436197004683
	data : 0.11607818603515625
	model : 0.0703202247619629
			 train-loss:  2.0523911201825706 	 ± 0.21264408763366285
	data : 0.11574463844299317
	model : 0.06972999572753906
			 train-loss:  2.0472913511255952 	 ± 0.2171524386663701
	data : 0.1162592887878418
	model : 0.06990914344787598
			 train-loss:  2.0457629241441424 	 ± 0.2165142112090109
	data : 0.11618943214416504
	model : 0.07006769180297852
			 train-loss:  2.0461142199734845 	 ± 0.21541079541863392
	data : 0.11624031066894532
	model : 0.07016334533691407
			 train-loss:  2.050513686593046 	 ± 0.21858991519984772
	data : 0.11604337692260742
	model : 0.06983332633972168
			 train-loss:  2.054282464543167 	 ± 0.2206167306481578
	data : 0.11649789810180664
	model : 0.07060399055480956
			 train-loss:  2.0563338489243477 	 ± 0.22043708743784382
	data : 0.11568365097045899
	model : 0.06916084289550781
			 train-loss:  2.0589541685581207 	 ± 0.22087626818250353
	data : 0.1169614315032959
	model : 0.06902041435241699
			 train-loss:  2.059462850636775 	 ± 0.2198389607480281
	data : 0.11684169769287109
	model : 0.06897859573364258
			 train-loss:  2.0630887571503136 	 ± 0.22177290250673828
	data : 0.11692399978637695
	model : 0.06914010047912597
			 train-loss:  2.064796259102312 	 ± 0.22136643989239382
	data : 0.11665778160095215
	model : 0.06907567977905274
			 train-loss:  2.0659583245332422 	 ± 0.22061506712989318
	data : 0.11679821014404297
	model : 0.07001309394836426
			 train-loss:  2.064165601276216 	 ± 0.22032184533317556
	data : 0.11595582962036133
	model : 0.06995058059692383
			 train-loss:  2.061501238705977 	 ± 0.22097319237535698
	data : 0.11610279083251954
	model : 0.06997499465942383
			 train-loss:  2.064379102715822 	 ± 0.22192500370491003
	data : 0.11600866317749023
	model : 0.0700869083404541
			 train-loss:  2.0675242046515145 	 ± 0.2232780564318982
	data : 0.11594443321228028
	model : 0.07019386291503907
			 train-loss:  2.0653944660764223 	 ± 0.22335081735335097
	data : 0.11578493118286133
	model : 0.07026758193969726
			 train-loss:  2.066187883507122 	 ± 0.22248752547814177
	data : 0.11580543518066407
	model : 0.07006807327270508
			 train-loss:  2.065010447759886 	 ± 0.22182706280676986
	data : 0.11602096557617188
	model : 0.06987161636352539
			 train-loss:  2.0646868912237033 	 ± 0.22086085177091055
	data : 0.11644320487976074
	model : 0.06968703269958496
			 train-loss:  2.063111538380648 	 ± 0.22051256928024368
	data : 0.11649880409240723
	model : 0.0687479019165039
			 train-loss:  2.067380071732036 	 ± 0.22418330393384153
	data : 0.11729707717895507
	model : 0.06840085983276367
			 train-loss:  2.070049743030382 	 ± 0.22501915663020383
	data : 0.11744823455810546
	model : 0.06854615211486817
			 train-loss:  2.0771715774618347 	 ± 0.23670653681517786
	data : 0.11722097396850586
	model : 0.0682945728302002
			 train-loss:  2.0725302604528575 	 ± 0.2409355667996011
	data : 0.11722679138183593
	model : 0.0685502052307129
			 train-loss:  2.0709168082576688 	 ± 0.24054641198130106
	data : 0.1172487735748291
	model : 0.06918959617614746
			 train-loss:  2.069826265343097 	 ± 0.2398263364897299
	data : 0.11661643981933593
	model : 0.06941304206848145
			 train-loss:  2.0713183532158532 	 ± 0.23937898591351803
	data : 0.11644330024719238
	model : 0.0686422348022461
			 train-loss:  2.0765931123544363 	 ± 0.2452906362975051
	data : 0.11715264320373535
	model : 0.06924958229064941
			 train-loss:  2.075671561428758 	 ± 0.24449351738965258
	data : 0.11666727066040039
	model : 0.06912212371826172
			 train-loss:  2.072903411175177 	 ± 0.24540972176278858
	data : 0.11670975685119629
	model : 0.06941723823547363
			 train-loss:  2.0711660106335916 	 ± 0.2451765109026306
	data : 0.11643595695495605
	model : 0.06888384819030761
			 train-loss:  2.07336923122406 	 ± 0.24542320136187884
	data : 0.11681194305419922
	model : 0.06983399391174316
			 train-loss:  2.07361249223588 	 ± 0.24446248927659778
	data : 0.1161839485168457
	model : 0.06971659660339355
			 train-loss:  2.0717194117899016 	 ± 0.2444235982956771
	data : 0.11612935066223144
	model : 0.06984214782714844
			 train-loss:  2.072507480159402 	 ± 0.24362887268620315
	data : 0.11619868278503417
	model : 0.06988987922668458
			 train-loss:  2.074984602225843 	 ± 0.24429559191884784
	data : 0.11633033752441406
	model : 0.07046718597412109
			 train-loss:  2.0755823868971603 	 ± 0.2434488743488495
	data : 0.11591887474060059
	model : 0.07031517028808594
			 train-loss:  2.0744466727016535 	 ± 0.24286336064603936
	data : 0.11584806442260742
	model : 0.0701589584350586
			 train-loss:  2.073607048301986 	 ± 0.2421324532799622
	data : 0.11605234146118164
	model : 0.07004098892211914
			 train-loss:  2.0748271789765895 	 ± 0.24162744639608857
	data : 0.11600632667541504
	model : 0.06994023323059081
			 train-loss:  2.0750012549001777 	 ± 0.24073253395453667
	data : 0.1159942626953125
	model : 0.06994547843933105
			 train-loss:  2.0736812467928285 	 ± 0.24032553373609575
	data : 0.11589264869689941
	model : 0.06996569633483887
			 train-loss:  2.073142980828005 	 ± 0.23952201697457584
	data : 0.11595087051391602
	model : 0.07002434730529786
			 train-loss:  2.07077940972182 	 ± 0.24023278580500212
	data : 0.11572494506835937
	model : 0.06995229721069336
			 train-loss:  2.0723021263661594 	 ± 0.24002343058119952
	data : 0.11570539474487304
	model : 0.06997013092041016
			 train-loss:  2.0701155928399064 	 ± 0.24053387704505794
	data : 0.11583027839660645
	model : 0.07002239227294922
			 train-loss:  2.069586285523006 	 ± 0.23975451626651464
	data : 0.11581192016601563
	model : 0.06974763870239258
			 train-loss:  2.0705919891384474 	 ± 0.23919898465608475
	data : 0.1159705638885498
	model : 0.06866788864135742
			 train-loss:  2.068717187559101 	 ± 0.23939261018737892
	data : 0.11708078384399415
	model : 0.06868276596069336
			 train-loss:  2.0707726862047102 	 ± 0.23980830141071052
	data : 0.11713147163391113
	model : 0.06826591491699219
			 train-loss:  2.0702976236740747 	 ± 0.23904169727463218
	data : 0.11752781867980958
	model : 0.06832795143127442
			 train-loss:  2.0710860696332207 	 ± 0.23840380619461624
	data : 0.1174853801727295
	model : 0.06874794960021972
			 train-loss:  2.073124642241491 	 ± 0.23885073250913663
	data : 0.11707816123962403
	model : 0.06985077857971192
			 train-loss:  2.073186462428294 	 ± 0.2380381006979192
	data : 0.116080904006958
	model : 0.07003650665283204
			 train-loss:  2.07226545746262 	 ± 0.2374952165325251
	data : 0.11584239006042481
	model : 0.07052979469299317
			 train-loss:  2.0711809836778063 	 ± 0.23706431201354872
	data : 0.1154360294342041
	model : 0.0704345703125
			 train-loss:  2.0736372534434 	 ± 0.23816754751637512
	data : 0.11574273109436035
	model : 0.069317626953125
			 train-loss:  2.0721138534166954 	 ± 0.23810971979197865
	data : 0.11674871444702148
	model : 0.06892209053039551
			 train-loss:  2.071870937159187 	 ± 0.23734394313699964
	data : 0.11708307266235352
	model : 0.06869072914123535
			 train-loss:  2.0715312381195865 	 ± 0.23660410706653573
	data : 0.11726288795471192
	model : 0.0684621810913086
			 train-loss:  2.0720588680985688 	 ± 0.23592494868872263
	data : 0.1176069736480713
	model : 0.0678853988647461
			 train-loss:  2.072796227855067 	 ± 0.2353406267471551
	data : 0.11804265975952148
	model : 0.06884970664978027
			 train-loss:  2.072388094205123 	 ± 0.23464014154293766
	data : 0.11715798377990723
	model : 0.06812214851379395
			 train-loss:  2.071723723107842 	 ± 0.23403883773072406
	data : 0.11777029037475586
	model : 0.06836037635803223
			 train-loss:  2.0731472493726995 	 ± 0.23397789339942487
	data : 0.11762433052062989
	model : 0.06844849586486816
			 train-loss:  2.073715365157937 	 ± 0.2333502468023809
	data : 0.11725921630859375
	model : 0.06869492530822754
			 train-loss:  2.0701555334031583 	 ± 0.2369112122920016
	data : 0.11703429222106934
	model : 0.06861577033996583
			 train-loss:  2.068824699206382 	 ± 0.23677349351147703
	data : 0.11712307929992676
	model : 0.0694185733795166
			 train-loss:  2.0716025586481446 	 ± 0.2386587195714929
	data : 0.11640377044677734
	model : 0.06906957626342773
			 train-loss:  2.0736198388725704 	 ± 0.2393069052489798
	data : 0.11673216819763184
	model : 0.06877899169921875
			 train-loss:  2.0754043045567303 	 ± 0.23966152137193136
	data : 0.11692657470703124
	model : 0.06828150749206544
			 train-loss:  2.077586259986415 	 ± 0.2405625289306231
	data : 0.11743330955505371
	model : 0.06835737228393554
			 train-loss:  2.07816952754216 	 ± 0.2399538435816656
	data : 0.11742820739746093
	model : 0.06855645179748535
			 train-loss:  2.080426114048073 	 ± 0.24099454733190506
	data : 0.11723332405090332
	model : 0.06879777908325195
			 train-loss:  2.078302814137368 	 ± 0.24183790122457108
	data : 0.11710138320922851
	model : 0.06932435035705567
			 train-loss:  2.081548697144322 	 ± 0.24476419079498868
	data : 0.11664814949035644
	model : 0.07045230865478516
			 train-loss:  2.0815007300937878 	 ± 0.24404403098642766
	data : 0.11570367813110352
	model : 0.07049055099487304
			 train-loss:  2.0842241369492824 	 ± 0.2459066495308553
	data : 0.11560282707214356
	model : 0.06948833465576172
			 train-loss:  2.085372606682223 	 ± 0.24565027192267394
	data : 0.11654319763183593
	model : 0.06846966743469238
			 train-loss:  2.0890911404108037 	 ± 0.24974702267386112
	data : 0.11724395751953125
	model : 0.06754655838012695
			 train-loss:  2.0888516731645868 	 ± 0.24904824255538258
	data : 0.11802501678466797
	model : 0.06725606918334961
			 train-loss:  2.090361602646964 	 ± 0.24913309442703482
	data : 0.11810903549194336
	model : 0.06719861030578614
			 train-loss:  2.0917360965501177 	 ± 0.2490888586288751
	data : 0.11841874122619629
	model : 0.0674433708190918
			 train-loss:  2.090976928587014 	 ± 0.24858832723831056
	data : 0.11824092864990235
	model : 0.06891980171203613
			 train-loss:  2.090121781558133 	 ± 0.24815000090872757
	data : 0.11699581146240234
	model : 0.06960301399230957
			 train-loss:  2.0902538492692915 	 ± 0.24746214674099395
	data : 0.11652154922485351
	model : 0.06879487037658691
			 train-loss:  2.0897876414987775 	 ± 0.24685261038082945
	data : 0.11716885566711426
	model : 0.06879448890686035
			 train-loss:  2.0903384007143053 	 ± 0.24628062759724922
	data : 0.11716766357421875
	model : 0.06955857276916504
			 train-loss:  2.09171365578096 	 ± 0.2462990318272314
	data : 0.11647286415100097
	model : 0.06907424926757813
			 train-loss:  2.0890769007427443 	 ± 0.24818757210893833
	data : 0.11700453758239746
	model : 0.0696500301361084
			 train-loss:  2.090016520541647 	 ± 0.24783839952972406
	data : 0.11662297248840332
	model : 0.06956667900085449
			 train-loss:  2.0904078393369105 	 ± 0.24722464954716164
	data : 0.11682133674621582
	model : 0.06971368789672852
			 train-loss:  2.089517805525052 	 ± 0.24685618218028638
	data : 0.11669840812683105
	model : 0.06961426734924317
			 train-loss:  2.0902442339269873 	 ± 0.24639451095301262
	data : 0.1166773796081543
	model : 0.06889834403991699
			 train-loss:  2.0898946390507067 	 ± 0.24578482996174758
	data : 0.11726236343383789
	model : 0.0686535358428955
			 train-loss:  2.088387582667921 	 ± 0.2460031325089953
	data : 0.11738500595092774
	model : 0.06933197975158692
			 train-loss:  2.088336959638094 	 ± 0.24535588881888756
	data : 0.11692790985107422
	model : 0.0688084602355957
			 train-loss:  2.087730654247144 	 ± 0.24485541993025658
	data : 0.11720790863037109
	model : 0.06793355941772461
			 train-loss:  2.0890731271356344 	 ± 0.2449206849626277
	data : 0.11805515289306641
	model : 0.06870465278625489
			 train-loss:  2.0906109816052134 	 ± 0.24521299353831413
	data : 0.11739501953125
	model : 0.06842360496520997
			 train-loss:  2.0923104046546306 	 ± 0.24571702880667765
	data : 0.11757211685180664
	model : 0.06839804649353028
			 train-loss:  2.092093814336337 	 ± 0.2451047411546778
	data : 0.11742210388183594
	model : 0.06880350112915039
			 train-loss:  2.0925201208007578 	 ± 0.24455114185371404
	data : 0.11718630790710449
	model : 0.0687744140625
			 train-loss:  2.0924151857492284 	 ± 0.2439340878280823
	data : 0.11711831092834472
	model : 0.06866388320922852
			 train-loss:  2.091069035457842 	 ± 0.24404979517235903
	data : 0.11710782051086426
	model : 0.06804347038269043
			 train-loss:  2.0909275451497216 	 ± 0.2434439738081079
	data : 0.11766700744628907
	model : 0.06754279136657715
			 train-loss:  2.0895672953128814 	 ± 0.24359156222182954
	data : 0.11807661056518555
	model : 0.06751184463500977
			 train-loss:  2.089737964506766 	 ± 0.2429968447851039
	data : 0.1178741455078125
	model : 0.06854634284973145
			 train-loss:  2.091205867210237 	 ± 0.24328636376144536
	data : 0.11712656021118165
	model : 0.06852264404296875
			 train-loss:  2.0916074825625115 	 ± 0.24275351408926665
	data : 0.11723675727844238
	model : 0.06911168098449708
			 train-loss:  2.0915219304608366 	 ± 0.2421608668403209
	data : 0.11677327156066894
	model : 0.0697657585144043
			 train-loss:  2.0913467383966213 	 ± 0.24158246764213698
	data : 0.11633434295654296
	model : 0.0702099323272705
			 train-loss:  2.0905668538750954 	 ± 0.24125393807091172
	data : 0.11614303588867188
	model : 0.069266939163208
			 train-loss:  2.092388938590524 	 ± 0.24208718467740206
	data : 0.11712203025817872
	model : 0.0693359375
			 train-loss:  2.0953620935861883 	 ± 0.245263637876035
	data : 0.11706972122192383
	model : 0.06936841011047364
			 train-loss:  2.094382299190503 	 ± 0.2450838878105732
	data : 0.11689515113830566
	model : 0.06865987777709961
			 train-loss:  2.0931026038669405 	 ± 0.24519858393676994
	data : 0.11735301017761231
	model : 0.06839280128479004
			 train-loss:  2.095076844590535 	 ± 0.24628419983732533
	data : 0.1174238681793213
	model : 0.0693629264831543
			 train-loss:  2.094704472793723 	 ± 0.2457621853327711
	data : 0.11656513214111328
	model : 0.0693807601928711
			 train-loss:  2.093175489018221 	 ± 0.24619321658024576
	data : 0.11655669212341309
	model : 0.06924657821655274
			 train-loss:  2.0918385787544964 	 ± 0.2463910946843078
	data : 0.1167595386505127
	model : 0.07000112533569336
			 train-loss:  2.0905145456624585 	 ± 0.24657932132941543
	data : 0.11617522239685059
	model : 0.07028703689575196
			 train-loss:  2.0930063713479927 	 ± 0.24870635476343772
	data : 0.11602697372436524
	model : 0.07105498313903809
			 train-loss:  2.0913621181716566 	 ± 0.24930659097097696
	data : 0.11517963409423829
	model : 0.07027153968811035
			 train-loss:  2.09094385269585 	 ± 0.24881043089018015
	data : 0.11588282585144043
	model : 0.0704127311706543
			 train-loss:  2.0911742367156565 	 ± 0.24826502488920912
	data : 0.11575284004211425
	model : 0.07047834396362304
			 train-loss:  2.0914025274190036 	 ± 0.24772318151973297
	data : 0.11563897132873535
	model : 0.06992540359497071
			 train-loss:  2.0915375716006595 	 ± 0.24717020264116482
	data : 0.11603879928588867
	model : 0.06905841827392578
			 train-loss:  2.091810139449867 	 ± 0.24664617104877815
	data : 0.11691036224365234
	model : 0.06922006607055664
			 train-loss:  2.092591099674926 	 ± 0.2463674725693257
	data : 0.11675267219543457
	model : 0.068748140335083
			 train-loss:  2.091925809425967 	 ± 0.2460176119194455
	data : 0.11724333763122559
	model : 0.06818547248840331
			 train-loss:  2.091213844617208 	 ± 0.2457014674211111
	data : 0.11791977882385254
	model : 0.06823210716247559
			 train-loss:  2.090738927896044 	 ± 0.24526075616684112
	data : 0.11792449951171875
	model : 0.06754622459411622
			 train-loss:  2.0932346550903658 	 ± 0.24757932848508277
	data : 0.11844568252563477
	model : 0.06725478172302246
			 train-loss:  2.092896435867276 	 ± 0.24708834637671662
	data : 0.11855916976928711
	model : 0.06697807312011719
			 train-loss:  2.093306223377911 	 ± 0.2466258959151521
	data : 0.11848111152648926
	model : 0.06677508354187012
			 train-loss:  2.0926299914069797 	 ± 0.2463018437864575
	data : 0.11818718910217285
	model : 0.06578412055969238
			 train-loss:  2.093429416805119 	 ± 0.24606700218966635
	data : 0.11892752647399903
	model : 0.06561245918273925
			 train-loss:  2.0937155269343277 	 ± 0.24557461607262362
	data : 0.11888203620910645
	model : 0.06568498611450195
			 train-loss:  2.093707077492972 	 ± 0.24504709906361088
	data : 0.11870226860046387
	model : 0.06589174270629883
			 train-loss:  2.093993984735929 	 ± 0.2445621487541398
	data : 0.1186518669128418
	model : 0.06562423706054688
			 train-loss:  2.0949702212151062 	 ± 0.24449773421272883
	data : 0.11919674873352051
	model : 0.06601800918579101
			 train-loss:  2.0947424880528853 	 ± 0.24400415621102198
	data : 0.11880970001220703
	model : 0.06629271507263183
			 train-loss:  2.0928634616393076 	 ± 0.24519393883462576
	data : 0.11861414909362793
	model : 0.06631975173950196
			 train-loss:  2.092002835093426 	 ± 0.24503673808883097
	data : 0.11876707077026367
	model : 0.0661858081817627
			 train-loss:  2.0928322096748833 	 ± 0.24485809821592483
	data : 0.11906595230102539
	model : 0.06656904220581054
			 train-loss:  2.0919363891084988 	 ± 0.2447395961219298
	data : 0.1186408519744873
	model : 0.0669905662536621
			 train-loss:  2.0918665936379015 	 ± 0.24423370330123131
	data : 0.1185938835144043
	model : 0.0669926643371582
			 train-loss:  2.0923698258794046 	 ± 0.24385373749887906
	data : 0.11866922378540039
	model : 0.06766853332519532
			 train-loss:  2.091301880255648 	 ± 0.2439178913460459
	data : 0.11806573867797851
	model : 0.06805500984191895
			 train-loss:  2.090906974233565 	 ± 0.24349537561231915
	data : 0.11773767471313476
	model : 0.06784772872924805
			 train-loss:  2.0911747090670527 	 ± 0.2430339242710638
	data : 0.11792025566101075
	model : 0.06738519668579102
			 train-loss:  2.0895437632149796 	 ± 0.24387923426701325
	data : 0.11803808212280273
	model : 0.06724486351013184
			 train-loss:  2.0891867485123607 	 ± 0.24344945685857908
	data : 0.11824245452880859
	model : 0.06711573600769043
			 train-loss:  2.0901457052077017 	 ± 0.24342513494978416
	data : 0.1183821678161621
	model : 0.0671614170074463
			 train-loss:  2.0906912744284636 	 ± 0.24308771542755533
	data : 0.11795902252197266
	model : 0.06745758056640624
			 train-loss:  2.0894110674858095 	 ± 0.2434406793582573
	data : 0.11760964393615722
	model : 0.06756753921508789
			 train-loss:  2.089991189094179 	 ± 0.24312834185909712
	data : 0.11756224632263183
	model : 0.06776065826416015
			 train-loss:  2.089390317125926 	 ± 0.24283213220753266
	data : 0.11699113845825196
	model : 0.06776185035705566
			 train-loss:  2.0891558506743237 	 ± 0.24238033152016916
	data : 0.11684436798095703
	model : 0.06782279014587403
			 train-loss:  2.089327628687611 	 ± 0.2419181645257604
	data : 0.11685724258422851
	model : 0.06742696762084961
			 train-loss:  2.08994105610193 	 ± 0.2416411996204732
	data : 0.11736388206481933
	model : 0.06724076271057129
			 train-loss:  2.087276881095022 	 ± 0.2448924822086269
	data : 0.11652598381042481
	model : 0.058365917205810545
#epoch  30    val-loss:  2.4972072149577893  train-loss:  2.087276881095022  lr:  0.00125
			 train-loss:  2.3964314460754395 	 ± 0.0
	data : 5.918165445327759
	model : 0.07513809204101562
			 train-loss:  2.2071964740753174 	 ± 0.18923497200012207
	data : 3.025978684425354
	model : 0.074759840965271
			 train-loss:  2.2729984124501548 	 ± 0.1803691768005177
	data : 2.05635674794515
	model : 0.07297174135843913
			 train-loss:  2.1727576553821564 	 ± 0.23354744290500942
	data : 1.5711582899093628
	model : 0.07129627466201782
			 train-loss:  2.0850183248519896 	 ± 0.27281548152093976
	data : 1.2805980682373046
	model : 0.07106728553771972
			 train-loss:  2.0970330437024436 	 ± 0.2504901990295477
	data : 0.11992640495300293
	model : 0.0699690341949463
			 train-loss:  2.0577432087489536 	 ± 0.2510853771549823
	data : 0.11640143394470215
	model : 0.0688586711883545
			 train-loss:  2.0801726281642914 	 ± 0.24224973742109845
	data : 0.11637420654296875
	model : 0.06885318756103516
			 train-loss:  2.088905175526937 	 ± 0.22972689434110188
	data : 0.11652393341064453
	model : 0.06955895423889161
			 train-loss:  2.033099615573883 	 ± 0.27481875132587286
	data : 0.11595015525817871
	model : 0.06939859390258789
			 train-loss:  2.0429993217641655 	 ± 0.2638928709800542
	data : 0.11624112129211425
	model : 0.06945819854736328
			 train-loss:  2.0288143952687583 	 ± 0.2570009603310245
	data : 0.11622819900512696
	model : 0.0697868824005127
			 train-loss:  2.0380555666410007 	 ± 0.2489850483729257
	data : 0.11616339683532714
	model : 0.06976943016052246
			 train-loss:  2.0404714345932007 	 ± 0.24008606107894712
	data : 0.11627473831176757
	model : 0.06949119567871094
			 train-loss:  2.0437206268310546 	 ± 0.23226356532803366
	data : 0.11661381721496582
	model : 0.06966800689697265
			 train-loss:  2.0637351125478745 	 ± 0.23787267733041745
	data : 0.11640801429748535
	model : 0.0689361572265625
			 train-loss:  2.0676172340617462 	 ± 0.23129225793500469
	data : 0.11693863868713379
	model : 0.06887502670288086
			 train-loss:  2.0689145856433444 	 ± 0.22483931063664075
	data : 0.11681079864501953
	model : 0.06813831329345703
			 train-loss:  2.054178237915039 	 ± 0.22759816643309816
	data : 0.11753158569335938
	model : 0.06759839057922364
			 train-loss:  2.0376748204231263 	 ± 0.2332075729577793
	data : 0.11812233924865723
	model : 0.0675689697265625
			 train-loss:  2.0537013666970387 	 ± 0.23860632247186522
	data : 0.11823735237121583
	model : 0.06821861267089843
			 train-loss:  2.0520506447011773 	 ± 0.23324308579737768
	data : 0.11771454811096191
	model : 0.06816802024841309
			 train-loss:  2.038571114125459 	 ± 0.2367157967881698
	data : 0.11774835586547852
	model : 0.06813979148864746
			 train-loss:  2.0342432955900827 	 ± 0.23265939353987494
	data : 0.11781606674194336
	model : 0.06890263557434081
			 train-loss:  2.0307260227203368 	 ± 0.22860902578160536
	data : 0.11703853607177735
	model : 0.06885604858398438
			 train-loss:  2.019467239196484 	 ± 0.23112985867958827
	data : 0.1169663429260254
	model : 0.06800484657287598
			 train-loss:  2.001119759347704 	 ± 0.24534635930840382
	data : 0.11777162551879883
	model : 0.06801528930664062
			 train-loss:  2.0039838424750736 	 ± 0.24138454942657026
	data : 0.117832612991333
	model : 0.06868748664855957
			 train-loss:  1.9962001011289399 	 ± 0.24073582335022745
	data : 0.11702699661254883
	model : 0.06896100044250489
			 train-loss:  2.0101760625839233 	 ± 0.24836755601685537
	data : 0.11690583229064941
	model : 0.06891570091247559
			 train-loss:  2.0139191381392942 	 ± 0.24518743048111385
	data : 0.11689677238464355
	model : 0.0690760612487793
			 train-loss:  2.016468994319439 	 ± 0.24174320766382004
	data : 0.11667795181274414
	model : 0.06895813941955567
			 train-loss:  2.016562353480946 	 ± 0.23805284073946487
	data : 0.11665267944335937
	model : 0.0693023681640625
			 train-loss:  2.009309137568754 	 ± 0.2381984819156649
	data : 0.11649770736694336
	model : 0.0691028118133545
			 train-loss:  2.021529531478882 	 ± 0.24534650850610454
	data : 0.11664414405822754
	model : 0.0682344913482666
			 train-loss:  2.015263193183475 	 ± 0.24473899009180777
	data : 0.11758279800415039
	model : 0.06813588142395019
			 train-loss:  2.0083704735781693 	 ± 0.24492585767611583
	data : 0.1175529956817627
	model : 0.06827812194824219
			 train-loss:  2.0064126817803634 	 ± 0.24197488609169734
	data : 0.11752562522888184
	model : 0.06819028854370117
			 train-loss:  2.001013098618923 	 ± 0.24116057941416572
	data : 0.11747546195983886
	model : 0.06826992034912109
			 train-loss:  1.9992520093917847 	 ± 0.2383808306306602
	data : 0.11740093231201172
	model : 0.06911120414733887
			 train-loss:  2.0059703268655915 	 ± 0.23925899384475938
	data : 0.11687912940979003
	model : 0.06994743347167968
			 train-loss:  2.006434451966059 	 ± 0.23641219306296812
	data : 0.11630668640136718
	model : 0.06900653839111329
			 train-loss:  2.0126335454541584 	 ± 0.2370758289007173
	data : 0.11719861030578613
	model : 0.06899166107177734
			 train-loss:  2.0217967412688513 	 ± 0.2419463209609655
	data : 0.11726422309875488
	model : 0.06891546249389649
			 train-loss:  2.026144568125407 	 ± 0.24097496885913391
	data : 0.11720833778381348
	model : 0.06873626708984375
			 train-loss:  2.0274732786676157 	 ± 0.2385078901793501
	data : 0.11726999282836914
	model : 0.06870956420898437
			 train-loss:  2.0316659998386464 	 ± 0.23766426500188093
	data : 0.11726741790771485
	model : 0.06904258728027343
			 train-loss:  2.0404076923926673 	 ± 0.2426914799053485
	data : 0.11668930053710938
	model : 0.0687495231628418
			 train-loss:  2.0450847684120643 	 ± 0.24237807818254234
	data : 0.11693253517150878
	model : 0.06874308586120606
			 train-loss:  2.039434413909912 	 ± 0.24318015733565324
	data : 0.11702313423156738
	model : 0.06903371810913086
			 train-loss:  2.036097054388009 	 ± 0.24193789779913383
	data : 0.11671972274780273
	model : 0.06824650764465331
			 train-loss:  2.035302540430656 	 ± 0.23966745179538568
	data : 0.1175835132598877
	model : 0.06854948997497559
			 train-loss:  2.0377903861819573 	 ± 0.23807257768397908
	data : 0.1173163890838623
	model : 0.06891412734985351
			 train-loss:  2.042706730189147 	 ± 0.23855813266535927
	data : 0.11707234382629395
	model : 0.06800918579101563
			 train-loss:  2.0463814973831176 	 ± 0.23791693309961806
	data : 0.11768298149108887
	model : 0.06875743865966796
			 train-loss:  2.0467476610626494 	 ± 0.2357987429401959
	data : 0.11692981719970703
	model : 0.06959447860717774
			 train-loss:  2.050463887683132 	 ± 0.2353698563128925
	data : 0.11610989570617676
	model : 0.06966667175292969
			 train-loss:  2.0523449860770127 	 ± 0.2337637929510687
	data : 0.11629738807678222
	model : 0.06957221031188965
			 train-loss:  2.063166058669656 	 ± 0.24598955525420574
	data : 0.11608281135559081
	model : 0.07044386863708496
			 train-loss:  2.0619786202907564 	 ± 0.24410149021170802
	data : 0.11543469429016114
	model : 0.06945643424987794
			 train-loss:  2.068594946235907 	 ± 0.2474576157118115
	data : 0.11640148162841797
	model : 0.06899805068969726
			 train-loss:  2.0631775606063103 	 ± 0.24907396089088696
	data : 0.11665029525756836
	model : 0.06922664642333984
			 train-loss:  2.0641666234485685 	 ± 0.2472119770877591
	data : 0.11639404296875
	model : 0.06927633285522461
			 train-loss:  2.06209615431726 	 ± 0.2458229650848099
	data : 0.11654219627380372
	model : 0.06925501823425292
			 train-loss:  2.0599897366303663 	 ± 0.24450607768440205
	data : 0.11659278869628906
	model : 0.06958904266357421
			 train-loss:  2.0588621804208467 	 ± 0.24281691770892794
	data : 0.11614651679992676
	model : 0.07004294395446778
			 train-loss:  2.056515768392762 	 ± 0.2417507550302349
	data : 0.11585054397583008
	model : 0.06946086883544922
			 train-loss:  2.055895573952619 	 ± 0.24002028319361432
	data : 0.1162353515625
	model : 0.06922955513000488
			 train-loss:  2.0578771708668144 	 ± 0.23883431499613103
	data : 0.11664066314697266
	model : 0.06876940727233886
			 train-loss:  2.065044219153268 	 ± 0.24448157565011047
	data : 0.11705589294433594
	model : 0.06887869834899903
			 train-loss:  2.0637990169122187 	 ± 0.2429772198651781
	data : 0.11675310134887695
	model : 0.06887197494506836
			 train-loss:  2.067470356822014 	 ± 0.24325901512621248
	data : 0.11675972938537597
	model : 0.06934905052185059
			 train-loss:  2.069328188896179 	 ± 0.2421008954738811
	data : 0.11632533073425293
	model : 0.06871647834777832
			 train-loss:  2.0700226716093115 	 ± 0.24053271397209475
	data : 0.1167271614074707
	model : 0.06942739486694335
			 train-loss:  2.0698882977167767 	 ± 0.23892657764876796
	data : 0.11612787246704101
	model : 0.068328857421875
			 train-loss:  2.066004182163038 	 ± 0.23972120398770116
	data : 0.11729578971862793
	model : 0.06834778785705567
			 train-loss:  2.0676804208136224 	 ± 0.23860738381287902
	data : 0.1171783447265625
	model : 0.06831350326538085
			 train-loss:  2.0667455868843274 	 ± 0.23721479415940855
	data : 0.11739168167114258
	model : 0.06901450157165527
			 train-loss:  2.0723782913594304 	 ± 0.24090102381689352
	data : 0.11676406860351562
	model : 0.06888971328735352
			 train-loss:  2.0730727910995483 	 ± 0.23947022997923983
	data : 0.11694674491882324
	model : 0.06977410316467285
			 train-loss:  2.0761796044714655 	 ± 0.2396042549589664
	data : 0.11625108718872071
	model : 0.06888623237609863
			 train-loss:  2.081413769140476 	 ± 0.24275335111760682
	data : 0.11714892387390137
	model : 0.0693295955657959
			 train-loss:  2.0793702430035697 	 ± 0.2419951056696509
	data : 0.11675729751586914
	model : 0.06866788864135742
			 train-loss:  2.083340068658193 	 ± 0.2432540003734534
	data : 0.11745829582214355
	model : 0.06862049102783203
			 train-loss:  2.0815726925345026 	 ± 0.24236077545468526
	data : 0.11757869720458984
	model : 0.06849541664123535
			 train-loss:  2.0786949548610423 	 ± 0.24240390593099487
	data : 0.11783480644226074
	model : 0.06933026313781739
			 train-loss:  2.0782003197176704 	 ± 0.24105040217928364
	data : 0.1171104907989502
	model : 0.06912384033203126
			 train-loss:  2.080389070239934 	 ± 0.24054478510590466
	data : 0.11724319458007812
	model : 0.07003698348999024
			 train-loss:  2.0835788933078896 	 ± 0.2410540574121209
	data : 0.11621999740600586
	model : 0.06920132637023926
			 train-loss:  2.080408063199785 	 ± 0.24157037311844753
	data : 0.11681814193725586
	model : 0.06928644180297852
			 train-loss:  2.080334337203057 	 ± 0.24024041488930287
	data : 0.11677393913269044
	model : 0.06932988166809081
			 train-loss:  2.0834374829478888 	 ± 0.24075797131683885
	data : 0.1165726661682129
	model : 0.06899418830871581
			 train-loss:  2.0858280261357627 	 ± 0.24055535688417348
	data : 0.1167872428894043
	model : 0.06874203681945801
			 train-loss:  2.0855004495762763 	 ± 0.2392932387628695
	data : 0.11708941459655761
	model : 0.0692354679107666
			 train-loss:  2.085729005462245 	 ± 0.23804078311672416
	data : 0.11660847663879395
	model : 0.06924195289611816
			 train-loss:  2.084536482890447 	 ± 0.23708283570209673
	data : 0.11648225784301758
	model : 0.06845741271972657
			 train-loss:  2.084547885914439 	 ± 0.2358576196694656
	data : 0.11730384826660156
	model : 0.06870012283325196
			 train-loss:  2.0889318256962057 	 ± 0.23859046876567386
	data : 0.11716303825378419
	model : 0.06881299018859863
			 train-loss:  2.086315144192089 	 ± 0.23879157395432107
	data : 0.11707100868225098
	model : 0.06919159889221191
			 train-loss:  2.0852292478084564 	 ± 0.23784015547907117
	data : 0.11679916381835938
	model : 0.06918697357177735
			 train-loss:  2.0847822734625034 	 ± 0.2367020058119799
	data : 0.11670608520507812
	model : 0.06993603706359863
			 train-loss:  2.0846321571107005 	 ± 0.23554367543532256
	data : 0.11603717803955078
	model : 0.06990218162536621
			 train-loss:  2.0817336839379617 	 ± 0.2362183126622495
	data : 0.11599297523498535
	model : 0.06965980529785157
			 train-loss:  2.082863288430067 	 ± 0.23535927889792624
	data : 0.11615843772888183
	model : 0.06961846351623535
			 train-loss:  2.0831909304573424 	 ± 0.2342596693399616
	data : 0.11611943244934082
	model : 0.06971650123596192
			 train-loss:  2.0876460468994 	 ± 0.23757930487434076
	data : 0.11617746353149414
	model : 0.06991124153137207
			 train-loss:  2.089716622762591 	 ± 0.2374254934151959
	data : 0.11597557067871093
	model : 0.06905760765075683
			 train-loss:  2.0915709722925118 	 ± 0.2371009151682461
	data : 0.11684541702270508
	model : 0.06958141326904296
			 train-loss:  2.0918223103252025 	 ± 0.23602524347746712
	data : 0.11652951240539551
	model : 0.06938529014587402
			 train-loss:  2.091327807036313 	 ± 0.23500666823391872
	data : 0.11686816215515136
	model : 0.06940298080444336
			 train-loss:  2.0892071530625627 	 ± 0.23500057960997006
	data : 0.11694126129150391
	model : 0.06938276290893555
			 train-loss:  2.0847173428961208 	 ± 0.23868341272340254
	data : 0.11693110466003417
	model : 0.07042279243469238
			 train-loss:  2.083591892655972 	 ± 0.23792326015444176
	data : 0.11598310470581055
	model : 0.06989340782165528
			 train-loss:  2.0865448911984763 	 ± 0.23894832818977937
	data : 0.11629023551940917
	model : 0.07020626068115235
			 train-loss:  2.084149126384569 	 ± 0.23927837031293683
	data : 0.1160360336303711
	model : 0.0699589729309082
			 train-loss:  2.0872384988028427 	 ± 0.24053721928860872
	data : 0.1162257194519043
	model : 0.06886906623840332
			 train-loss:  2.089808521107731 	 ± 0.24110126799052367
	data : 0.11712651252746582
	model : 0.06876020431518555
			 train-loss:  2.087446918932058 	 ± 0.24143264921537938
	data : 0.11731791496276855
	model : 0.06903643608093261
			 train-loss:  2.087345773432435 	 ± 0.2404185967878476
	data : 0.11704893112182617
	model : 0.06906967163085938
			 train-loss:  2.087776556611061 	 ± 0.23946087187176074
	data : 0.1170133113861084
	model : 0.06929802894592285
			 train-loss:  2.085339387586294 	 ± 0.23995913720948503
	data : 0.11675086021423339
	model : 0.06939797401428223
			 train-loss:  2.0827624827134805 	 ± 0.2406489356433452
	data : 0.11675782203674316
	model : 0.06929221153259277
			 train-loss:  2.0895535102704677 	 ± 0.2511324041854824
	data : 0.11674289703369141
	model : 0.06929879188537598
			 train-loss:  2.092155534413553 	 ± 0.251776986304207
	data : 0.11688332557678223
	model : 0.06924629211425781
			 train-loss:  2.091244843482971 	 ± 0.2509728232581744
	data : 0.11683115959167481
	model : 0.06932306289672852
			 train-loss:  2.091888693590013 	 ± 0.25007854009778974
	data : 0.11687445640563965
	model : 0.0701179027557373
			 train-loss:  2.090948336706387 	 ± 0.2493155813948993
	data : 0.11613931655883789
	model : 0.0700070858001709
			 train-loss:  2.0896552177146077 	 ± 0.24876698259851301
	data : 0.1163257122039795
	model : 0.0691004753112793
			 train-loss:  2.087446376334789 	 ± 0.24905780921245504
	data : 0.11703500747680665
	model : 0.06913728713989258
			 train-loss:  2.088642603617448 	 ± 0.24846978464963682
	data : 0.11696243286132812
	model : 0.06942610740661621
			 train-loss:  2.0885436671380777 	 ± 0.24752218038957094
	data : 0.11656980514526367
	model : 0.0697479248046875
			 train-loss:  2.0885322265552753 	 ± 0.24658284865498623
	data : 0.11645832061767578
	model : 0.06996369361877441
			 train-loss:  2.0873159609342875 	 ± 0.24605122017176595
	data : 0.11624083518981934
	model : 0.07089176177978515
			 train-loss:  2.0884304384687056 	 ± 0.2454681179486508
	data : 0.11542844772338867
	model : 0.07053394317626953
			 train-loss:  2.088703891966078 	 ± 0.24457777224201882
	data : 0.11580710411071778
	model : 0.07026562690734864
			 train-loss:  2.089707804076812 	 ± 0.2439559482113355
	data : 0.11608500480651855
	model : 0.06913895606994629
			 train-loss:  2.092485659313898 	 ± 0.24521324080591186
	data : 0.1168734073638916
	model : 0.06964759826660157
			 train-loss:  2.0918685111446655 	 ± 0.24442993199429566
	data : 0.11620607376098632
	model : 0.06988639831542968
			 train-loss:  2.089451644918044 	 ± 0.24519840093679968
	data : 0.11591506004333496
	model : 0.07021079063415528
			 train-loss:  2.0917880833148956 	 ± 0.24586907981008582
	data : 0.11562652587890625
	model : 0.06912870407104492
			 train-loss:  2.094389649147683 	 ± 0.24692187001794338
	data : 0.11649985313415527
	model : 0.06913776397705078
			 train-loss:  2.092240666839438 	 ± 0.2473705650010231
	data : 0.11650609970092773
	model : 0.06861863136291504
			 train-loss:  2.093876411031176 	 ± 0.24727357836617847
	data : 0.11719598770141601
	model : 0.06842617988586426
			 train-loss:  2.0926066603925495 	 ± 0.2468808701321665
	data : 0.1174440860748291
	model : 0.0683013916015625
			 train-loss:  2.095173429620677 	 ± 0.2479486517270966
	data : 0.11760511398315429
	model : 0.06892528533935546
			 train-loss:  2.0951927688023817 	 ± 0.24709816323735287
	data : 0.11697502136230468
	model : 0.06939411163330078
			 train-loss:  2.094350425564513 	 ± 0.24646650536730996
	data : 0.11674270629882813
	model : 0.06872835159301757
			 train-loss:  2.096462589663428 	 ± 0.24696375433665907
	data : 0.11730189323425293
	model : 0.06861314773559571
			 train-loss:  2.098213919057142 	 ± 0.2470540398963146
	data : 0.11738357543945313
	model : 0.0679664134979248
			 train-loss:  2.1012032270431518 	 ± 0.2489181639507464
	data : 0.11784968376159669
	model : 0.0675175666809082
			 train-loss:  2.099918062323766 	 ± 0.24859136509746188
	data : 0.11831769943237305
	model : 0.0675137996673584
			 train-loss:  2.1046610192248694 	 ± 0.25453476161159017
	data : 0.11829161643981934
	model : 0.06802091598510743
			 train-loss:  2.103970332862505 	 ± 0.2538444513704961
	data : 0.11767988204956055
	model : 0.06833844184875489
			 train-loss:  2.103365379494506 	 ± 0.25312956478917426
	data : 0.11755776405334473
	model : 0.0691946029663086
			 train-loss:  2.105163318880143 	 ± 0.2532962898557497
	data : 0.11689152717590331
	model : 0.06959419250488282
			 train-loss:  2.103715030810772 	 ± 0.25312616061578225
	data : 0.11654987335205078
	model : 0.0700268268585205
			 train-loss:  2.103997938951869 	 ± 0.25234347944316365
	data : 0.1162142276763916
	model : 0.07024612426757812
			 train-loss:  2.103658252879034 	 ± 0.25157966305887164
	data : 0.11602258682250977
	model : 0.06949439048767089
			 train-loss:  2.1023005644480386 	 ± 0.25136727340752646
	data : 0.11672239303588867
	model : 0.06924805641174317
			 train-loss:  2.1026424750685693 	 ± 0.2506176056937614
	data : 0.11677064895629882
	model : 0.06957192420959472
			 train-loss:  2.1016234048405047 	 ± 0.25017039350688275
	data : 0.11670575141906739
	model : 0.06901907920837402
			 train-loss:  2.099793321556515 	 ± 0.2504757856763522
	data : 0.11728544235229492
	model : 0.06811361312866211
			 train-loss:  2.099269689226443 	 ± 0.2497951994848646
	data : 0.11832003593444824
	model : 0.06864209175109863
			 train-loss:  2.101381834687256 	 ± 0.2504881975018681
	data : 0.1178396224975586
	model : 0.06888809204101562
			 train-loss:  2.1047378807356862 	 ± 0.2533993049933214
	data : 0.11773891448974609
	model : 0.06800246238708496
			 train-loss:  2.102872628763498 	 ± 0.25376850765651077
	data : 0.11823091506958008
	model : 0.0684328556060791
			 train-loss:  2.102081723555833 	 ± 0.2532127053254196
	data : 0.11772570610046387
	model : 0.06940197944641113
			 train-loss:  2.0996243492478417 	 ± 0.254447417448295
	data : 0.11677360534667969
	model : 0.06888270378112793
			 train-loss:  2.0988816107518575 	 ± 0.25387609111937753
	data : 0.11722321510314941
	model : 0.06853160858154297
			 train-loss:  2.0979065874043634 	 ± 0.2534454522328947
	data : 0.11750969886779786
	model : 0.06992597579956054
			 train-loss:  2.0968536257046706 	 ± 0.25307595728530824
	data : 0.11646671295166015
	model : 0.06988811492919922
			 train-loss:  2.097741738308308 	 ± 0.25260630740581685
	data : 0.11646251678466797
	model : 0.06943998336791993
			 train-loss:  2.0978433741310427 	 ± 0.2518787002748272
	data : 0.1168290138244629
	model : 0.06915936470031739
			 train-loss:  2.097620640677967 	 ± 0.25117095366368597
	data : 0.11689257621765137
	model : 0.06971030235290528
			 train-loss:  2.0963578149250575 	 ± 0.2510056463822049
	data : 0.1164172649383545
	model : 0.06913704872131347
			 train-loss:  2.094668710096316 	 ± 0.2512869800242471
	data : 0.11688461303710937
	model : 0.06840033531188965
			 train-loss:  2.094552879953115 	 ± 0.25058083609095194
	data : 0.11753902435302735
	model : 0.06874899864196778
			 train-loss:  2.095956520418103 	 ± 0.2505727936615781
	data : 0.11716065406799317
	model : 0.06865124702453614
			 train-loss:  2.0949718672470006 	 ± 0.2502169849338181
	data : 0.1173396110534668
	model : 0.06831502914428711
			 train-loss:  2.0956649449136524 	 ± 0.24969320809818157
	data : 0.11765151023864746
	model : 0.06826925277709961
			 train-loss:  2.094948269385659 	 ± 0.24918806865624463
	data : 0.11767969131469727
	model : 0.06895885467529297
			 train-loss:  2.094522869193947 	 ± 0.24856843856202557
	data : 0.11717309951782226
	model : 0.06892495155334473
			 train-loss:  2.093262135005388 	 ± 0.24847116289859333
	data : 0.11718277931213379
	model : 0.06888089179992676
			 train-loss:  2.0939563493365827 	 ± 0.24797294300430822
	data : 0.1172342300415039
	model : 0.06903576850891113
			 train-loss:  2.093116661664602 	 ± 0.24756399741747095
	data : 0.1169820785522461
	model : 0.0691828727722168
			 train-loss:  2.0911266528150088 	 ± 0.2483768363155886
	data : 0.11674208641052246
	model : 0.06933698654174805
			 train-loss:  2.0930025010185447 	 ± 0.24902942024136962
	data : 0.1166916847229004
	model : 0.06929335594177247
			 train-loss:  2.0951891408321703 	 ± 0.2501597535031436
	data : 0.11660208702087402
	model : 0.06951165199279785
			 train-loss:  2.0963413557678305 	 ± 0.24999676065744103
	data : 0.11627097129821777
	model : 0.06931767463684083
			 train-loss:  2.098033223026677 	 ± 0.25042052211549615
	data : 0.11659436225891114
	model : 0.06931929588317871
			 train-loss:  2.0977262857696775 	 ± 0.2497999419290667
	data : 0.11664090156555176
	model : 0.06971993446350097
			 train-loss:  2.097780772174398 	 ± 0.24914970994420668
	data : 0.11619763374328614
	model : 0.06973099708557129
			 train-loss:  2.0960046145582445 	 ± 0.2497191465121485
	data : 0.11634931564331055
	model : 0.07023301124572753
			 train-loss:  2.0960979019243693 	 ± 0.24907808056453618
	data : 0.11602778434753418
	model : 0.0700533390045166
			 train-loss:  2.096909030278524 	 ± 0.24869534615807953
	data : 0.11601877212524414
	model : 0.06997981071472167
			 train-loss:  2.0970899049116642 	 ± 0.24807296650857985
	data : 0.11605334281921387
	model : 0.06973824501037598
			 train-loss:  2.095071839802156 	 ± 0.24905026989971868
	data : 0.11642622947692871
	model : 0.06901426315307617
			 train-loss:  2.09670353778685 	 ± 0.2494739955827457
	data : 0.11724629402160644
	model : 0.0691096305847168
			 train-loss:  2.098032907025898 	 ± 0.24954846106246148
	data : 0.11703486442565918
	model : 0.06947855949401856
			 train-loss:  2.100593386888504 	 ± 0.25153074884551413
	data : 0.1166752815246582
	model : 0.06947770118713378
			 train-loss:  2.10088244362257 	 ± 0.25093756914745596
	data : 0.11664304733276368
	model : 0.06999373435974121
			 train-loss:  2.101048463642007 	 ± 0.25032673187992405
	data : 0.11589422225952148
	model : 0.06985235214233398
			 train-loss:  2.099184591781917 	 ± 0.25111060777049604
	data : 0.11585159301757812
	model : 0.06991643905639648
			 train-loss:  2.0975655311462926 	 ± 0.2515543126286029
	data : 0.11585497856140137
	model : 0.07023639678955078
			 train-loss:  2.09666399723146 	 ± 0.2512701635601725
	data : 0.11571578979492188
	model : 0.07087697982788085
			 train-loss:  2.096152063712333 	 ± 0.25076668807808905
	data : 0.11532392501831054
	model : 0.07011260986328124
			 train-loss:  2.0951168986334316 	 ± 0.25060105244756575
	data : 0.1159846305847168
	model : 0.07013101577758789
			 train-loss:  2.093328925852592 	 ± 0.2513179408713112
	data : 0.1160998821258545
	model : 0.0699500560760498
			 train-loss:  2.093741639949488 	 ± 0.25078662705717086
	data : 0.11632471084594727
	model : 0.06915163993835449
			 train-loss:  2.0938643279529754 	 ± 0.25019509054996053
	data : 0.11715455055236816
	model : 0.0683784008026123
			 train-loss:  2.0941782658698997 	 ± 0.24964296364513977
	data : 0.11774587631225586
	model : 0.06860499382019043
			 train-loss:  2.094628373969276 	 ± 0.249139293313778
	data : 0.11777358055114746
	model : 0.06949086189270019
			 train-loss:  2.095945319659273 	 ± 0.24929231545426409
	data : 0.11694726943969727
	model : 0.0704352855682373
			 train-loss:  2.095771305471937 	 ± 0.24872214102838416
	data : 0.11597046852111817
	model : 0.07068166732788086
			 train-loss:  2.0952457821646404 	 ± 0.24826210207990965
	data : 0.11570243835449219
	model : 0.07077393531799317
			 train-loss:  2.0949383405623614 	 ± 0.24772777454124914
	data : 0.11569271087646485
	model : 0.06975011825561524
			 train-loss:  2.095649001235786 	 ± 0.24737690258364955
	data : 0.11639986038208008
	model : 0.0695406436920166
			 train-loss:  2.0949659243636174 	 ± 0.24701390710032325
	data : 0.11633491516113281
	model : 0.06865077018737793
			 train-loss:  2.0940521612559277 	 ± 0.24681831568737855
	data : 0.11721034049987793
	model : 0.06793928146362305
			 train-loss:  2.0931312842802567 	 ± 0.2466335141272593
	data : 0.11785550117492676
	model : 0.06809124946594239
			 train-loss:  2.0925257659066316 	 ± 0.24623873296737153
	data : 0.11773667335510254
	model : 0.06846709251403808
			 train-loss:  2.0920855225743473 	 ± 0.24577067077771764
	data : 0.11758646965026856
	model : 0.06850109100341797
			 train-loss:  2.093696288463781 	 ± 0.24639064504163008
	data : 0.11767029762268066
	model : 0.06816964149475098
			 train-loss:  2.0929044022091796 	 ± 0.2461242983553608
	data : 0.11783990859985352
	model : 0.06860904693603516
			 train-loss:  2.091823648876614 	 ± 0.2461088714222469
	data : 0.1171501636505127
	model : 0.06774773597717285
			 train-loss:  2.0922634896978867 	 ± 0.24565239283379886
	data : 0.11779203414916992
	model : 0.06778054237365723
			 train-loss:  2.09345612546946 	 ± 0.24576557644895453
	data : 0.11761965751647949
	model : 0.06755900382995605
			 train-loss:  2.0942288114313494 	 ± 0.24550220371963757
	data : 0.11785221099853516
	model : 0.06716775894165039
			 train-loss:  2.094329124975413 	 ± 0.24497026913804554
	data : 0.11831016540527343
	model : 0.06695437431335449
			 train-loss:  2.095455339680547 	 ± 0.24503055355654785
	data : 0.11865992546081543
	model : 0.06754240989685059
			 train-loss:  2.0943282723942875 	 ± 0.2450963553999694
	data : 0.11812453269958496
	model : 0.06747775077819824
			 train-loss:  2.094681281467964 	 ± 0.24462640411781408
	data : 0.11813664436340332
	model : 0.0673067569732666
			 train-loss:  2.0961102753749734 	 ± 0.2450693659203242
	data : 0.11830430030822754
	model : 0.06741323471069335
			 train-loss:  2.09793851722 	 ± 0.24613233604184848
	data : 0.11819148063659668
	model : 0.06790838241577149
			 train-loss:  2.0989247778628735 	 ± 0.24607102423755103
	data : 0.11771783828735352
	model : 0.06804695129394531
			 train-loss:  2.099032313136731 	 ± 0.24555466734090983
	data : 0.11763491630554199
	model : 0.06807703971862793
			 train-loss:  2.0984238205076773 	 ± 0.2452143117071171
	data : 0.11752357482910156
	model : 0.06790328025817871
			 train-loss:  2.0993519615726313 	 ± 0.2451154299996542
	data : 0.11767292022705078
	model : 0.06803126335144043
			 train-loss:  2.0975068603100637 	 ± 0.24625278387899613
	data : 0.11767563819885254
	model : 0.06792259216308594
			 train-loss:  2.0963882595300674 	 ± 0.2463469465539443
	data : 0.11782903671264648
	model : 0.06787810325622559
			 train-loss:  2.0967564187109224 	 ± 0.24590147506084734
	data : 0.11769561767578125
	model : 0.0684399127960205
			 train-loss:  2.0970657699364277 	 ± 0.24543987624210703
	data : 0.11727786064147949
	model : 0.06832566261291503
			 train-loss:  2.0950267025472695 	 ± 0.24697978700122816
	data : 0.11743450164794922
	model : 0.06841206550598145
			 train-loss:  2.0964288794603503 	 ± 0.24744046060924504
	data : 0.11746854782104492
	model : 0.06816425323486328
			 train-loss:  2.096848010530277 	 ± 0.2470217399421255
	data : 0.11757516860961914
	model : 0.06748600006103515
			 train-loss:  2.094484118911309 	 ± 0.24928046104552062
	data : 0.1183239459991455
	model : 0.0673799991607666
			 train-loss:  2.094737015272442 	 ± 0.24880695247199305
	data : 0.11840333938598632
	model : 0.06719732284545898
			 train-loss:  2.0957685103339534 	 ± 0.2488334516037924
	data : 0.11842818260192871
	model : 0.06762971878051757
			 train-loss:  2.095064089480174 	 ± 0.24858093126742753
	data : 0.11781344413757325
	model : 0.0676501750946045
			 train-loss:  2.0947312717437745 	 ± 0.24813885349939113
	data : 0.11803474426269531
	model : 0.06829514503479003
			 train-loss:  2.0948813806966955 	 ± 0.24765543294918813
	data : 0.11737499237060547
	model : 0.06796321868896485
			 train-loss:  2.0938506637300764 	 ± 0.24770241106073068
	data : 0.1175492286682129
	model : 0.06893515586853027
			 train-loss:  2.0911293910897295 	 ± 0.25095838408358595
	data : 0.11676898002624511
	model : 0.0683053970336914
			 train-loss:  2.091583969555502 	 ± 0.25056822957203817
	data : 0.11739354133605957
	model : 0.06848092079162597
			 train-loss:  2.092356296146617 	 ± 0.25037917691791534
	data : 0.11713109016418458
	model : 0.06869826316833497
			 train-loss:  2.089917169418186 	 ± 0.2529069630573208
	data : 0.11598029136657714
	model : 0.06018838882446289
#epoch  31    val-loss:  2.475822009538349  train-loss:  2.089917169418186  lr:  0.00125
			 train-loss:  2.005872964859009 	 ± 0.0
	data : 6.052431583404541
	model : 0.07152605056762695
			 train-loss:  2.2451571226119995 	 ± 0.23928415775299072
	data : 3.093720316886902
	model : 0.07176434993743896
			 train-loss:  2.242175499598185 	 ± 0.1954201940758216
	data : 2.1021111011505127
	model : 0.07108298937479655
			 train-loss:  2.265971839427948 	 ± 0.17418549468125782
	data : 1.6052733063697815
	model : 0.06972712278366089
			 train-loss:  2.235363006591797 	 ± 0.16739197064450068
	data : 1.3079556941986084
	model : 0.06982002258300782
			 train-loss:  2.1712822119394937 	 ± 0.2094798345423595
	data : 0.12043147087097168
	model : 0.06948518753051758
			 train-loss:  2.1367981944765364 	 ± 0.2115368930387994
	data : 0.11674680709838867
	model : 0.06910710334777832
			 train-loss:  2.1670845597982407 	 ± 0.21348354139779263
	data : 0.11621484756469727
	model : 0.06914410591125489
			 train-loss:  2.1629527807235718 	 ± 0.20161319805264047
	data : 0.11637053489685059
	model : 0.06959075927734375
			 train-loss:  2.192868149280548 	 ± 0.2112757841050636
	data : 0.11601152420043945
	model : 0.06957111358642579
			 train-loss:  2.1822983893481167 	 ± 0.2041977095066334
	data : 0.11616253852844238
	model : 0.06960549354553222
			 train-loss:  2.179366280635198 	 ± 0.19574613422404355
	data : 0.11603693962097168
	model : 0.0700448989868164
			 train-loss:  2.1543146555240336 	 ± 0.20712346381115407
	data : 0.1155858039855957
	model : 0.07017908096313477
			 train-loss:  2.1509657502174377 	 ± 0.1999540734311006
	data : 0.11564040184020996
	model : 0.07055163383483887
			 train-loss:  2.1804768959681193 	 ± 0.2225059493859933
	data : 0.11522274017333985
	model : 0.07051677703857422
			 train-loss:  2.15396212041378 	 ± 0.2386631334289122
	data : 0.11534886360168457
	model : 0.07035279273986816
			 train-loss:  2.1436208907295677 	 ± 0.23520321884871165
	data : 0.11560549736022949
	model : 0.06983399391174316
			 train-loss:  2.1089993119239807 	 ± 0.26948896686103474
	data : 0.11607890129089356
	model : 0.06901893615722657
			 train-loss:  2.137560775405482 	 ± 0.2889387496356334
	data : 0.11678686141967773
	model : 0.06912465095520019
			 train-loss:  2.1235362589359283 	 ± 0.288181149922829
	data : 0.1167363166809082
	model : 0.06931929588317871
			 train-loss:  2.114463800475711 	 ± 0.28414763889419614
	data : 0.11654887199401856
	model : 0.06945362091064453
			 train-loss:  2.1042473424564707 	 ± 0.28153469131885667
	data : 0.11629605293273926
	model : 0.06980962753295898
			 train-loss:  2.1000969720923384 	 ± 0.27603365844520217
	data : 0.11598706245422363
	model : 0.07055792808532715
			 train-loss:  2.094330678383509 	 ± 0.271633134384114
	data : 0.11533503532409668
	model : 0.06998004913330078
			 train-loss:  2.0923673343658447 	 ± 0.26631877624584677
	data : 0.11601676940917968
	model : 0.06979098320007324
			 train-loss:  2.1025021076202393 	 ± 0.26601808233246016
	data : 0.11628541946411133
	model : 0.06961450576782227
			 train-loss:  2.0927658875783286 	 ± 0.2657241327627828
	data : 0.11633367538452148
	model : 0.06942009925842285
			 train-loss:  2.103130502360208 	 ± 0.26643579894724667
	data : 0.11652145385742188
	model : 0.06945948600769043
			 train-loss:  2.0978117770162124 	 ± 0.2633101957504272
	data : 0.11638569831848145
	model : 0.06914467811584472
			 train-loss:  2.098002604643504 	 ± 0.2588865387003365
	data : 0.11672921180725097
	model : 0.0682450771331787
			 train-loss:  2.094804867621391 	 ± 0.2552782753051186
	data : 0.11729311943054199
	model : 0.06838445663452149
			 train-loss:  2.0856236666440964 	 ± 0.2564052596050237
	data : 0.11733002662658691
	model : 0.06792178153991699
			 train-loss:  2.0791965289549394 	 ± 0.25509465749440696
	data : 0.11771683692932129
	model : 0.06790051460266114
			 train-loss:  2.088183722075294 	 ± 0.2565633689408256
	data : 0.11789684295654297
	model : 0.0687859058380127
			 train-loss:  2.091186921937125 	 ± 0.25347723351108015
	data : 0.11703076362609863
	model : 0.0688100814819336
			 train-loss:  2.0895624591244593 	 ± 0.2501166264008174
	data : 0.11698107719421387
	model : 0.06797900199890136
			 train-loss:  2.0932530615780807 	 ± 0.24770527020195537
	data : 0.11779747009277344
	model : 0.0674250602722168
			 train-loss:  2.0929247109513534 	 ± 0.24443242093987014
	data : 0.1181027889251709
	model : 0.06750288009643554
			 train-loss:  2.0904879692273264 	 ± 0.24174544813195506
	data : 0.11790270805358886
	model : 0.06766624450683593
			 train-loss:  2.0842815309762956 	 ± 0.24183075756610703
	data : 0.11799826622009277
	model : 0.06769204139709473
			 train-loss:  2.080453736026113 	 ± 0.2400870734046371
	data : 0.11815485954284669
	model : 0.06808409690856934
			 train-loss:  2.0902153565770103 	 ± 0.2453084540051569
	data : 0.11767802238464356
	model : 0.06896705627441406
			 train-loss:  2.0884496749833574 	 ± 0.24270914845302347
	data : 0.116998291015625
	model : 0.06876530647277831
			 train-loss:  2.090188657695597 	 ± 0.24020606424071544
	data : 0.11728167533874512
	model : 0.06839160919189453
			 train-loss:  2.0863938517040674 	 ± 0.238852210888708
	data : 0.11765341758728028
	model : 0.06929292678833007
			 train-loss:  2.084828503753828 	 ± 0.23647498210786458
	data : 0.11675748825073243
	model : 0.06892075538635253
			 train-loss:  2.0883675255673997 	 ± 0.23517388486250917
	data : 0.11724295616149902
	model : 0.0691096305847168
			 train-loss:  2.091873216132323 	 ± 0.23394904520707582
	data : 0.11714215278625488
	model : 0.06926965713500977
			 train-loss:  2.0851273171755733 	 ± 0.23621921241527188
	data : 0.11691579818725586
	model : 0.0691558837890625
			 train-loss:  2.0869108986854554 	 ± 0.23417814367469647
	data : 0.11704926490783692
	model : 0.06861863136291504
			 train-loss:  2.091804717101303 	 ± 0.23443889075411126
	data : 0.11758632659912109
	model : 0.06962528228759765
			 train-loss:  2.0859558834479404 	 ± 0.2359010203183577
	data : 0.11665492057800293
	model : 0.0695411205291748
			 train-loss:  2.0899641086470404 	 ± 0.23544581108637863
	data : 0.11666698455810547
	model : 0.06979546546936036
			 train-loss:  2.09237312608295 	 ± 0.2339139568947725
	data : 0.11643290519714355
	model : 0.06992058753967285
			 train-loss:  2.0937103033065796 	 ± 0.23198590889509219
	data : 0.11614651679992676
	model : 0.06955385208129883
			 train-loss:  2.0940253755875995 	 ± 0.2299171496428385
	data : 0.11654243469238282
	model : 0.06939258575439453
			 train-loss:  2.0906135471243608 	 ± 0.2293171736587684
	data : 0.11666865348815918
	model : 0.06933102607727051
			 train-loss:  2.089160925355451 	 ± 0.22759609161838062
	data : 0.11674156188964843
	model : 0.06903810501098633
			 train-loss:  2.0938210547980614 	 ± 0.22843290024452984
	data : 0.11700105667114258
	model : 0.0692251205444336
			 train-loss:  2.0916705171267194 	 ± 0.22712278765717844
	data : 0.11689605712890624
	model : 0.06923146247863769
			 train-loss:  2.096313214692913 	 ± 0.22810608781951028
	data : 0.1169811725616455
	model : 0.06967411041259766
			 train-loss:  2.09206320201197 	 ± 0.22868094437921602
	data : 0.11652874946594238
	model : 0.06979613304138184
			 train-loss:  2.094897949506366 	 ± 0.22795419397942307
	data : 0.11655726432800292
	model : 0.07056059837341308
			 train-loss:  2.08812271989882 	 ± 0.2324717752130867
	data : 0.11588878631591797
	model : 0.07067685127258301
			 train-loss:  2.0842874838755683 	 ± 0.23270812581756944
	data : 0.1159898281097412
	model : 0.07161774635314941
			 train-loss:  2.087512856180018 	 ± 0.23239786456843992
	data : 0.11488761901855468
	model : 0.07131733894348144
			 train-loss:  2.083395580747234 	 ± 0.23306972263781045
	data : 0.11512999534606934
	model : 0.07121915817260742
			 train-loss:  2.081775312914568 	 ± 0.23172946086482835
	data : 0.1151151180267334
	model : 0.0705066204071045
			 train-loss:  2.0801022501959316 	 ± 0.23045746835613765
	data : 0.11589832305908203
	model : 0.07024812698364258
			 train-loss:  2.0742449283599855 	 ± 0.23392132938726237
	data : 0.11601772308349609
	model : 0.07024192810058594
			 train-loss:  2.0686585466626664 	 ± 0.23692410553069584
	data : 0.11591238975524902
	model : 0.07019257545471191
			 train-loss:  2.067468696170383 	 ± 0.23548656899679732
	data : 0.11618070602416992
	model : 0.07022590637207031
			 train-loss:  2.0669810445341343 	 ± 0.23390468844691048
	data : 0.11609363555908203
	model : 0.06963891983032226
			 train-loss:  2.0678614249100558 	 ± 0.23244061622524925
	data : 0.11657948493957519
	model : 0.06924910545349121
			 train-loss:  2.0675665156046548 	 ± 0.2308997490133769
	data : 0.1168182373046875
	model : 0.06911821365356445
			 train-loss:  2.072705814712926 	 ± 0.233653835001567
	data : 0.11707105636596679
	model : 0.06901350021362304
			 train-loss:  2.071191982789473 	 ± 0.23250649099433782
	data : 0.11703386306762695
	model : 0.06893229484558105
			 train-loss:  2.0718401028559756 	 ± 0.23108125281441624
	data : 0.11712493896484374
	model : 0.0695106029510498
			 train-loss:  2.077292333675336 	 ± 0.23460883717075198
	data : 0.11658110618591308
	model : 0.07004227638244628
			 train-loss:  2.0768632858991625 	 ± 0.233169107355959
	data : 0.11605634689331054
	model : 0.07019810676574707
			 train-loss:  2.070067395398646 	 ± 0.2395649286322765
	data : 0.11593098640441894
	model : 0.0703132152557373
			 train-loss:  2.067852167094626 	 ± 0.23893293328588563
	data : 0.11580300331115723
	model : 0.07028408050537109
			 train-loss:  2.0680825178881728 	 ± 0.23749837687188965
	data : 0.11583867073059081
	model : 0.0700075626373291
			 train-loss:  2.066442763521558 	 ± 0.23655264773986018
	data : 0.11604399681091308
	model : 0.06908292770385742
			 train-loss:  2.0610791164285995 	 ± 0.24024031162968523
	data : 0.11692256927490234
	model : 0.06820087432861328
			 train-loss:  2.061896719211756 	 ± 0.23895840278338148
	data : 0.11799302101135253
	model : 0.06728830337524414
			 train-loss:  2.0651015098067536 	 ± 0.2394327928361145
	data : 0.11891422271728516
	model : 0.06735143661499024
			 train-loss:  2.067124676975337 	 ± 0.23881523381301825
	data : 0.11888318061828614
	model : 0.0683372974395752
			 train-loss:  2.0679665402080234 	 ± 0.23760106783092022
	data : 0.11781110763549804
	model : 0.06835627555847168
			 train-loss:  2.064929877387153 	 ± 0.23800776326211567
	data : 0.11769204139709473
	model : 0.06926746368408203
			 train-loss:  2.0714867429418877 	 ± 0.2447335632015708
	data : 0.11665682792663574
	model : 0.07015132904052734
			 train-loss:  2.075763165950775 	 ± 0.24679481119026983
	data : 0.11557717323303222
	model : 0.07007389068603516
			 train-loss:  2.073140355848497 	 ± 0.24675015252942525
	data : 0.11564097404479981
	model : 0.06925616264343262
			 train-loss:  2.071971500173528 	 ± 0.24569285096698115
	data : 0.11660261154174804
	model : 0.07016034126281738
			 train-loss:  2.074292085045262 	 ± 0.24542973930515458
	data : 0.11587004661560059
	model : 0.06945290565490722
			 train-loss:  2.0732909155388675 	 ± 0.24434304465266987
	data : 0.11659846305847169
	model : 0.06916890144348145
			 train-loss:  2.078454247455007 	 ± 0.24828891012265175
	data : 0.11687765121459961
	model : 0.06921472549438476
			 train-loss:  2.0813150101778457 	 ± 0.24862053899067876
	data : 0.11682691574096679
	model : 0.06921505928039551
			 train-loss:  2.0837631406206074 	 ± 0.24854608146439336
	data : 0.11677241325378418
	model : 0.06931309700012207
			 train-loss:  2.0811423909664155 	 ± 0.24867120382432015
	data : 0.11667428016662598
	model : 0.0698728084564209
			 train-loss:  2.077677916772295 	 ± 0.2498507046040489
	data : 0.11626591682434081
	model : 0.07005081176757813
			 train-loss:  2.0812015241267634 	 ± 0.25113215206817113
	data : 0.11637473106384277
	model : 0.07005386352539063
			 train-loss:  2.080823441153591 	 ± 0.24993926047069337
	data : 0.11637420654296875
	model : 0.07060837745666504
			 train-loss:  2.0778188579357586 	 ± 0.2505968853754892
	data : 0.11594305038452149
	model : 0.07032856941223145
			 train-loss:  2.077898299126398 	 ± 0.2494020278469406
	data : 0.11633157730102539
	model : 0.06946678161621093
			 train-loss:  2.079179881878619 	 ± 0.24856995757642292
	data : 0.1169619083404541
	model : 0.06931657791137695
			 train-loss:  2.0809924123443175 	 ± 0.24810847075006898
	data : 0.11693949699401855
	model : 0.06929898262023926
			 train-loss:  2.0848261835398496 	 ± 0.250120967282986
	data : 0.11701440811157227
	model : 0.06881566047668457
			 train-loss:  2.087719275317061 	 ± 0.2507797970182393
	data : 0.11731796264648438
	model : 0.06903810501098633
			 train-loss:  2.089321733604778 	 ± 0.25019726781853113
	data : 0.11699943542480469
	model : 0.06978807449340821
			 train-loss:  2.0897969630387454 	 ± 0.24911756972978769
	data : 0.11634588241577148
	model : 0.06931967735290527
			 train-loss:  2.092923278255122 	 ± 0.2501806493325957
	data : 0.11673669815063477
	model : 0.06877384185791016
			 train-loss:  2.0917073245597098 	 ± 0.24940340252857357
	data : 0.11712002754211426
	model : 0.0684093952178955
			 train-loss:  2.094381984911467 	 ± 0.2499296009197019
	data : 0.11741089820861816
	model : 0.06830978393554688
			 train-loss:  2.0965953080550483 	 ± 0.24996018946903512
	data : 0.1175422191619873
	model : 0.06833691596984863
			 train-loss:  2.092660523694137 	 ± 0.2524321008025554
	data : 0.11764059066772461
	model : 0.06901297569274903
			 train-loss:  2.0883943331547274 	 ± 0.25551629617511545
	data : 0.11696343421936035
	model : 0.06961164474487305
			 train-loss:  2.0923623604289556 	 ± 0.25802611827677935
	data : 0.11664776802062989
	model : 0.07007308006286621
			 train-loss:  2.093326870132895 	 ± 0.2571532150852749
	data : 0.11620287895202637
	model : 0.07024211883544922
			 train-loss:  2.096019557118416 	 ± 0.2577586628020321
	data : 0.11595344543457031
	model : 0.07033586502075195
			 train-loss:  2.094219874744573 	 ± 0.2574472839957179
	data : 0.1157947063446045
	model : 0.07032394409179688
			 train-loss:  2.0927419691789346 	 ± 0.25690488800406747
	data : 0.11602330207824707
	model : 0.07032566070556641
			 train-loss:  2.0907519290117715 	 ± 0.2568008699958939
	data : 0.11594910621643066
	model : 0.07006125450134278
			 train-loss:  2.0934044238059752 	 ± 0.25744951608604594
	data : 0.11632795333862304
	model : 0.0699467658996582
			 train-loss:  2.0899983987808226 	 ± 0.2592075155330171
	data : 0.1165015697479248
	model : 0.07057271003723145
			 train-loss:  2.0878072806767056 	 ± 0.25933649680868914
	data : 0.11585435867309571
	model : 0.07072219848632813
			 train-loss:  2.0901033784460834 	 ± 0.2595960872928785
	data : 0.1155881404876709
	model : 0.07098865509033203
			 train-loss:  2.09022437594831 	 ± 0.2585836469864339
	data : 0.11536684036254882
	model : 0.07032198905944824
			 train-loss:  2.089305460915085 	 ± 0.257789156248881
	data : 0.11599926948547364
	model : 0.07029561996459961
			 train-loss:  2.0891342667432933 	 ± 0.25680310651578225
	data : 0.11598930358886719
	model : 0.06972389221191407
			 train-loss:  2.091886584995357 	 ± 0.25773862763139244
	data : 0.1165931224822998
	model : 0.06977062225341797
			 train-loss:  2.0917814509435133 	 ± 0.2567633085586205
	data : 0.11652460098266601
	model : 0.06940283775329589
			 train-loss:  2.0921583310105745 	 ± 0.2558328575530109
	data : 0.11661314964294434
	model : 0.06929621696472169
			 train-loss:  2.0942208597909158 	 ± 0.2559839841146191
	data : 0.11666970252990723
	model : 0.06910891532897949
			 train-loss:  2.0966657523755674 	 ± 0.2565996764957718
	data : 0.11677317619323731
	model : 0.06899485588073731
			 train-loss:  2.099768027663231 	 ± 0.25818308714867
	data : 0.11680369377136231
	model : 0.06880741119384766
			 train-loss:  2.1000519364419645 	 ± 0.25726039380083165
	data : 0.11693477630615234
	model : 0.06877341270446777
			 train-loss:  2.1001546685246453 	 ± 0.25632941656060204
	data : 0.11714506149291992
	model : 0.06960983276367187
			 train-loss:  2.1003022837124283 	 ± 0.2554115906969646
	data : 0.11628036499023438
	model : 0.06982545852661133
			 train-loss:  2.1006169634205953 	 ± 0.2545248122320958
	data : 0.11603994369506836
	model : 0.07004823684692382
			 train-loss:  2.100306322388615 	 ± 0.2536472682505113
	data : 0.1158266544342041
	model : 0.07003779411315918
			 train-loss:  2.0996445442589238 	 ± 0.25287469360001363
	data : 0.11583223342895507
	model : 0.06949691772460938
			 train-loss:  2.1000748289214983 	 ± 0.25204112588079847
	data : 0.11619138717651367
	model : 0.06952891349792481
			 train-loss:  2.1003287906448045 	 ± 0.2511828182630525
	data : 0.11619901657104492
	model : 0.06948728561401367
			 train-loss:  2.101075993735215 	 ± 0.2504757121348498
	data : 0.11632661819458008
	model : 0.06931476593017578
			 train-loss:  2.1008112014156497 	 ± 0.24963680847890046
	data : 0.1166844367980957
	model : 0.06856088638305664
			 train-loss:  2.100413436792335 	 ± 0.24883267503491557
	data : 0.11739087104797363
	model : 0.06914091110229492
			 train-loss:  2.101893780199257 	 ± 0.2486392478403892
	data : 0.11691594123840332
	model : 0.06931729316711426
			 train-loss:  2.101785803001199 	 ± 0.24780696495011625
	data : 0.11684813499450683
	model : 0.06936979293823242
			 train-loss:  2.101615221500397 	 ± 0.2469883375272677
	data : 0.11687126159667968
	model : 0.06920266151428223
			 train-loss:  2.1017780232903185 	 ± 0.24617721173672863
	data : 0.11690082550048828
	model : 0.0700307846069336
			 train-loss:  2.1018189795707403 	 ± 0.24536659812708678
	data : 0.11627793312072754
	model : 0.0700911521911621
			 train-loss:  2.1016993094113916 	 ± 0.24456788229067347
	data : 0.1162017822265625
	model : 0.07012395858764649
			 train-loss:  2.1026183792522977 	 ± 0.2440374715127543
	data : 0.11618471145629883
	model : 0.06999344825744629
			 train-loss:  2.1019094990145777 	 ± 0.24340799702582916
	data : 0.11626591682434081
	model : 0.06994047164916992
			 train-loss:  2.1014945338933897 	 ± 0.24268158570165038
	data : 0.11629905700683593
	model : 0.06983652114868164
			 train-loss:  2.101715177487416 	 ± 0.2419231766867034
	data : 0.11622424125671386
	model : 0.06942472457885743
			 train-loss:  2.1012125739568397 	 ± 0.24123859621475208
	data : 0.11665453910827636
	model : 0.06915922164916992
			 train-loss:  2.1006453757016166 	 ± 0.2405844511513049
	data : 0.11679310798645019
	model : 0.06836209297180176
			 train-loss:  2.0978075705468653 	 ± 0.24248622972190118
	data : 0.11756796836853027
	model : 0.06845369338989257
			 train-loss:  2.0963896557411053 	 ± 0.24239643986185413
	data : 0.11761765480041504
	model : 0.0683349609375
			 train-loss:  2.095682732117029 	 ± 0.24181356618704972
	data : 0.11774344444274902
	model : 0.06850218772888184
			 train-loss:  2.0930713415145874 	 ± 0.2433511912447404
	data : 0.11779389381408692
	model : 0.06854257583618165
			 train-loss:  2.092510639894299 	 ± 0.24271372209437786
	data : 0.11782999038696289
	model : 0.0695225715637207
			 train-loss:  2.091372924140005 	 ± 0.24241535021127808
	data : 0.11688036918640136
	model : 0.06960010528564453
			 train-loss:  2.090870859393154 	 ± 0.24177011015350722
	data : 0.11666660308837891
	model : 0.06940321922302246
			 train-loss:  2.091355744236244 	 ± 0.24112610453797825
	data : 0.11693568229675293
	model : 0.06945304870605469
			 train-loss:  2.091361856886319 	 ± 0.2404074091970825
	data : 0.11674237251281738
	model : 0.0693397045135498
			 train-loss:  2.090965751360154 	 ± 0.23975006778842703
	data : 0.11669735908508301
	model : 0.06974296569824219
			 train-loss:  2.091973846800187 	 ± 0.23940284904943512
	data : 0.11625618934631347
	model : 0.06979823112487793
			 train-loss:  2.0912749049259207 	 ± 0.2388757093385872
	data : 0.1161341667175293
	model : 0.0701970100402832
			 train-loss:  2.0929728670175685 	 ± 0.23921299477174626
	data : 0.11563935279846191
	model : 0.06979932785034179
			 train-loss:  2.0928120868054427 	 ± 0.2385299460371213
	data : 0.11588759422302246
	model : 0.069881010055542
			 train-loss:  2.0920017813814096 	 ± 0.23808220111841746
	data : 0.11594715118408203
	model : 0.06932239532470703
			 train-loss:  2.0923418474197386 	 ± 0.23744336814944067
	data : 0.11659412384033203
	model : 0.06933140754699707
			 train-loss:  2.0972153239629487 	 ± 0.24538826627971644
	data : 0.1165379524230957
	model : 0.06908984184265136
			 train-loss:  2.0977779909715815 	 ± 0.24480792837129842
	data : 0.1167755126953125
	model : 0.06976642608642578
			 train-loss:  2.0953942088598616 	 ± 0.24617071121116949
	data : 0.11627531051635742
	model : 0.06981987953186035
			 train-loss:  2.094380588504855 	 ± 0.2458543335668416
	data : 0.11609969139099122
	model : 0.06896109580993652
			 train-loss:  2.0930324521329666 	 ± 0.24583303032922924
	data : 0.11677350997924804
	model : 0.06883835792541504
			 train-loss:  2.0936280146488166 	 ± 0.2452831731194565
	data : 0.11683602333068847
	model : 0.06892781257629395
			 train-loss:  2.095392178048144 	 ± 0.2457571683916873
	data : 0.11673297882080078
	model : 0.06865863800048828
			 train-loss:  2.0971128399906265 	 ± 0.24618162631790938
	data : 0.11675848960876464
	model : 0.06766180992126465
			 train-loss:  2.1004293917313865 	 ± 0.24957749763773146
	data : 0.11766209602355956
	model : 0.06797094345092773
			 train-loss:  2.100927384479626 	 ± 0.24899369837340055
	data : 0.11728978157043457
	model : 0.06793012619018554
			 train-loss:  2.0988202332168497 	 ± 0.2499719103900488
	data : 0.11752123832702636
	model : 0.06804299354553223
			 train-loss:  2.100147080931434 	 ± 0.24995852478029515
	data : 0.11759734153747559
	model : 0.0683713436126709
			 train-loss:  2.100506894766016 	 ± 0.2493414079937091
	data : 0.11741070747375489
	model : 0.06944437026977539
			 train-loss:  2.101030539583277 	 ± 0.24878452575219925
	data : 0.1165132999420166
	model : 0.06998252868652344
			 train-loss:  2.0991496399829264 	 ± 0.24947269343766387
	data : 0.11629581451416016
	model : 0.06927652359008789
			 train-loss:  2.097481812482105 	 ± 0.24987855409543874
	data : 0.11698923110961915
	model : 0.06930890083312988
			 train-loss:  2.097660026202599 	 ± 0.24923914888499138
	data : 0.11709733009338379
	model : 0.0691676139831543
			 train-loss:  2.098062909328876 	 ± 0.24865528689659744
	data : 0.11729168891906738
	model : 0.06892046928405762
			 train-loss:  2.0977539500010383 	 ± 0.24805073305654837
	data : 0.11776251792907715
	model : 0.0681917667388916
			 train-loss:  2.0967510278408343 	 ± 0.24780792448816769
	data : 0.11815581321716309
	model : 0.06813859939575195
			 train-loss:  2.0976855712277547 	 ± 0.24751922109782873
	data : 0.11812496185302734
	model : 0.06785802841186524
			 train-loss:  2.0985982254677014 	 ± 0.2472206036805041
	data : 0.11813583374023437
	model : 0.06765398979187012
			 train-loss:  2.098002168867323 	 ± 0.24673739243263446
	data : 0.11824631690979004
	model : 0.06773595809936524
			 train-loss:  2.095348273090382 	 ± 0.24893364722760625
	data : 0.11799626350402832
	model : 0.06855149269104004
			 train-loss:  2.096146658658981 	 ± 0.24856582106585293
	data : 0.1172952651977539
	model : 0.06949448585510254
			 train-loss:  2.095730087650356 	 ± 0.24801670465044073
	data : 0.11644124984741211
	model : 0.06987981796264649
			 train-loss:  2.094548247828342 	 ± 0.24796877895763983
	data : 0.11607346534729004
	model : 0.06996068954467774
			 train-loss:  2.095012406410255 	 ± 0.24744521771822572
	data : 0.11608481407165527
	model : 0.06909680366516113
			 train-loss:  2.094996213912964 	 ± 0.24683809709036983
	data : 0.11666626930236816
	model : 0.06882004737854004
			 train-loss:  2.0939481101384976 	 ± 0.2466899465043152
	data : 0.1169271469116211
	model : 0.06850566864013671
			 train-loss:  2.093434655550614 	 ± 0.2462002393837953
	data : 0.1172856330871582
	model : 0.06746582984924317
			 train-loss:  2.0929874199024145 	 ± 0.24568870137822127
	data : 0.11833114624023437
	model : 0.06761407852172852
			 train-loss:  2.095735421547523 	 ± 0.2482657744271073
	data : 0.11811747550964355
	model : 0.06864051818847657
			 train-loss:  2.094741749421261 	 ± 0.24808539275382516
	data : 0.11725187301635742
	model : 0.06874442100524902
			 train-loss:  2.095467710494995 	 ± 0.24771643315393663
	data : 0.11717667579650878
	model : 0.06897211074829102
			 train-loss:  2.095019337125299 	 ± 0.2472141327431591
	data : 0.1168053150177002
	model : 0.06918396949768066
			 train-loss:  2.0979249477386475 	 ± 0.25021577258591343
	data : 0.11672163009643555
	model : 0.06892838478088378
			 train-loss:  2.0994387501282312 	 ± 0.25059891779646226
	data : 0.11693544387817383
	model : 0.06881093978881836
			 train-loss:  2.098453879913437 	 ± 0.2504255657447604
	data : 0.11701221466064453
	model : 0.06900110244750976
			 train-loss:  2.100011771778728 	 ± 0.25087977113692256
	data : 0.11679964065551758
	model : 0.06903104782104492
			 train-loss:  2.0993092170468084 	 ± 0.2505102552807357
	data : 0.11671967506408691
	model : 0.06977267265319824
			 train-loss:  2.099047435593495 	 ± 0.24996198719553062
	data : 0.11602306365966797
	model : 0.07087135314941406
			 train-loss:  2.09752981104982 	 ± 0.2503880507476202
	data : 0.11487412452697754
	model : 0.07092757225036621
			 train-loss:  2.0973696550822147 	 ± 0.2498269258241308
	data : 0.11494560241699218
	model : 0.07072172164916993
			 train-loss:  2.096194132891568 	 ± 0.24986480514874607
	data : 0.11516942977905273
	model : 0.06992621421813965
			 train-loss:  2.098660049395324 	 ± 0.25196762675971934
	data : 0.11585798263549804
	model : 0.06904568672180175
			 train-loss:  2.0973318138638057 	 ± 0.25217373911603813
	data : 0.11660566329956054
	model : 0.06807646751403809
			 train-loss:  2.0987766740568015 	 ± 0.25252699351739094
	data : 0.11776762008666992
	model : 0.06748828887939454
			 train-loss:  2.0984626380460605 	 ± 0.25200632418862945
	data : 0.11803550720214843
	model : 0.0669905662536621
			 train-loss:  2.097394796477424 	 ± 0.2519530854000858
	data : 0.1185107707977295
	model : 0.0671076774597168
			 train-loss:  2.1010568558642295 	 ± 0.25732639481884045
	data : 0.11841444969177246
	model : 0.06707320213317872
			 train-loss:  2.1005628497590054 	 ± 0.25686635178878015
	data : 0.11826887130737304
	model : 0.06651420593261718
			 train-loss:  2.0989980190469506 	 ± 0.25738451604908874
	data : 0.11887469291687011
	model : 0.06651186943054199
			 train-loss:  2.0971026129077095 	 ± 0.2584116996264273
	data : 0.11897649765014648
	model : 0.06635851860046386
			 train-loss:  2.098016318030979 	 ± 0.25821978176940175
	data : 0.11900482177734376
	model : 0.06642012596130371
			 train-loss:  2.0976997495213627 	 ± 0.2577049830631386
	data : 0.1188286304473877
	model : 0.06704955101013184
			 train-loss:  2.0972609088338654 	 ± 0.2572354690378012
	data : 0.11810064315795898
	model : 0.06726765632629395
			 train-loss:  2.0990605282681183 	 ± 0.2581423191351992
	data : 0.11760001182556153
	model : 0.06684012413024902
			 train-loss:  2.0998115967481565 	 ± 0.2578451428111791
	data : 0.11812863349914551
	model : 0.06713223457336426
			 train-loss:  2.097994328559713 	 ± 0.25879332107064273
	data : 0.11765313148498535
	model : 0.0674736499786377
			 train-loss:  2.0976562904099287 	 ± 0.2582964350661848
	data : 0.11731100082397461
	model : 0.06708569526672363
			 train-loss:  2.0976859698315713 	 ± 0.2577513331091931
	data : 0.11775612831115723
	model : 0.06693696975708008
			 train-loss:  2.0967246955182373 	 ± 0.25763463929064284
	data : 0.1179577350616455
	model : 0.06757678985595703
			 train-loss:  2.095349491390723 	 ± 0.2579689661902325
	data : 0.1172712802886963
	model : 0.06796064376831054
			 train-loss:  2.09421953856945 	 ± 0.2580229793125985
	data : 0.11714797019958496
	model : 0.06735272407531738
			 train-loss:  2.094292409192477 	 ± 0.2574895801992038
	data : 0.11808657646179199
	model : 0.06756548881530762
			 train-loss:  2.093333825592167 	 ± 0.2573875754645409
	data : 0.1180124282836914
	model : 0.06774935722351075
			 train-loss:  2.093035094531966 	 ± 0.2568994611336626
	data : 0.11777424812316895
	model : 0.06727013587951661
			 train-loss:  2.0947227448713583 	 ± 0.2577187537386755
	data : 0.11824331283569336
	model : 0.06671500205993652
			 train-loss:  2.094239182374915 	 ± 0.2573031547120462
	data : 0.1186790943145752
	model : 0.06710143089294433
			 train-loss:  2.096126806445238 	 ± 0.2584738928692877
	data : 0.11814699172973633
	model : 0.06690716743469238
			 train-loss:  2.096764754669869 	 ± 0.2581441246377909
	data : 0.11841263771057128
	model : 0.06652436256408692
			 train-loss:  2.095431685447693 	 ± 0.2584736409584968
	data : 0.11874427795410156
	model : 0.0672614574432373
			 train-loss:  2.0951676416588594 	 ± 0.2579876076278246
	data : 0.11812009811401367
	model : 0.0674886703491211
			 train-loss:  2.0958506574630738 	 ± 0.25769659757820446
	data : 0.11793513298034668
	model : 0.06773891448974609
			 train-loss:  2.0940654534267713 	 ± 0.2587270818990732
	data : 0.11763858795166016
	model : 0.06809048652648926
			 train-loss:  2.0933394384762596 	 ± 0.25846928384454365
	data : 0.11728672981262207
	model : 0.0685399055480957
			 train-loss:  2.091605332057938 	 ± 0.2594226462890568
	data : 0.11707463264465331
	model : 0.0683671474456787
			 train-loss:  2.091413813782489 	 ± 0.25892938843855556
	data : 0.11719775199890137
	model : 0.06856269836425781
			 train-loss:  2.0914333544525445 	 ± 0.25842137267977405
	data : 0.11684327125549317
	model : 0.06805224418640136
			 train-loss:  2.089146938174963 	 ± 0.26048762649190554
	data : 0.11617302894592285
	model : 0.05883898735046387
#epoch  32    val-loss:  2.498156045612536  train-loss:  2.089146938174963  lr:  0.00125
			 train-loss:  1.9238965511322021 	 ± 0.0
	data : 5.914646625518799
	model : 0.07137775421142578
			 train-loss:  2.192447304725647 	 ± 0.2685507535934448
	data : 3.0251176357269287
	model : 0.0711740255355835
			 train-loss:  2.199300765991211 	 ± 0.21948487730678679
	data : 2.0552796522776284
	model : 0.06937678654988606
			 train-loss:  2.0777722001075745 	 ± 0.2836155593439499
	data : 1.571179449558258
	model : 0.06835097074508667
			 train-loss:  2.0152390241622924 	 ± 0.28282825314455345
	data : 1.2809467315673828
	model : 0.06792154312133789
			 train-loss:  2.0623908638954163 	 ± 0.27888408093403966
	data : 0.12193760871887208
	model : 0.06759624481201172
			 train-loss:  2.032888548714774 	 ± 0.26811890241705877
	data : 0.11798539161682128
	model : 0.06737656593322754
			 train-loss:  2.073747903108597 	 ± 0.273108376829643
	data : 0.11803622245788574
	model : 0.06829338073730469
			 train-loss:  2.1047586070166693 	 ± 0.27201823354962335
	data : 0.1171989917755127
	model : 0.06854972839355469
			 train-loss:  2.0977254629135134 	 ± 0.25892028404297246
	data : 0.11676964759826661
	model : 0.0691983699798584
			 train-loss:  2.122387474233454 	 ± 0.2588963926057317
	data : 0.1161046028137207
	model : 0.06896500587463379
			 train-loss:  2.1060063540935516 	 ± 0.2537587088745063
	data : 0.1162987232208252
	model : 0.06898384094238282
			 train-loss:  2.107032381571256 	 ± 0.24382938773594856
	data : 0.11627249717712403
	model : 0.06864533424377442
			 train-loss:  2.1069643923214505 	 ± 0.23496000503275175
	data : 0.1168644905090332
	model : 0.069317626953125
			 train-loss:  2.0954503059387206 	 ± 0.23104507993145004
	data : 0.11628584861755371
	model : 0.06943349838256836
			 train-loss:  2.0981128066778183 	 ± 0.22394597183982218
	data : 0.11621589660644531
	model : 0.06981658935546875
			 train-loss:  2.113042256411384 	 ± 0.2253173495989368
	data : 0.11584300994873047
	model : 0.06889257431030274
			 train-loss:  2.116192102432251 	 ± 0.2193539028704785
	data : 0.11677417755126954
	model : 0.06908726692199707
			 train-loss:  2.0982657420007804 	 ± 0.22664528614535617
	data : 0.11655802726745605
	model : 0.06822052001953124
			 train-loss:  2.1108220279216767 	 ± 0.22758564860670236
	data : 0.11749320030212403
	model : 0.06731653213500977
			 train-loss:  2.1065553426742554 	 ± 0.22291899898680945
	data : 0.11819653511047364
	model : 0.06719260215759278
			 train-loss:  2.113797453316775 	 ± 0.2203077945269188
	data : 0.11830577850341797
	model : 0.06815838813781738
			 train-loss:  2.117029609887496 	 ± 0.21599795030331276
	data : 0.11746063232421874
	model : 0.06806602478027343
			 train-loss:  2.1276568422714868 	 ± 0.21750569417119747
	data : 0.1175567626953125
	model : 0.06838312149047851
			 train-loss:  2.112699284553528 	 ± 0.22535718916041833
	data : 0.11730318069458008
	model : 0.06914143562316895
			 train-loss:  2.100006502408248 	 ± 0.22991352343694993
	data : 0.1166346549987793
	model : 0.06827096939086914
			 train-loss:  2.0909004785396434 	 ± 0.23034398942767384
	data : 0.1176412582397461
	model : 0.06830711364746093
			 train-loss:  2.0913364589214325 	 ± 0.22620465158873435
	data : 0.11755156517028809
	model : 0.06861934661865235
			 train-loss:  2.0923972088715126 	 ± 0.2223412178783659
	data : 0.11726274490356445
	model : 0.06894106864929199
			 train-loss:  2.088114627202352 	 ± 0.21981728305318726
	data : 0.11708025932312012
	model : 0.0685509204864502
			 train-loss:  2.087857738617928 	 ± 0.21624735763362127
	data : 0.11742749214172363
	model : 0.06858530044555664
			 train-loss:  2.0904619991779327 	 ± 0.21333500839855027
	data : 0.11732091903686523
	model : 0.06844091415405273
			 train-loss:  2.0836272275809087 	 ± 0.2136060178479592
	data : 0.11726531982421876
	model : 0.06735377311706543
			 train-loss:  2.098101044402403 	 ± 0.22627142988164536
	data : 0.11821370124816895
	model : 0.06744756698608398
			 train-loss:  2.095267724990845 	 ± 0.2236266532172307
	data : 0.1179570198059082
	model : 0.06788530349731445
			 train-loss:  2.0944185389412775 	 ± 0.2205560780525258
	data : 0.11776442527770996
	model : 0.06872029304504394
			 train-loss:  2.1089379078633077 	 ± 0.23434907497263843
	data : 0.11684374809265137
	model : 0.06859025955200196
			 train-loss:  2.115081266353005 	 ± 0.23424485226422487
	data : 0.11708278656005859
	model : 0.0694455623626709
			 train-loss:  2.110091875760983 	 ± 0.233258834238066
	data : 0.1162348747253418
	model : 0.06968293190002442
			 train-loss:  2.114472323656082 	 ± 0.23194349804004943
	data : 0.11627511978149414
	model : 0.07013120651245117
			 train-loss:  2.1074568323972747 	 ± 0.23335451540155697
	data : 0.11587438583374024
	model : 0.07018456459045411
			 train-loss:  2.1061621648924693 	 ± 0.23070873610869344
	data : 0.11588683128356933
	model : 0.0703199863433838
			 train-loss:  2.1018513995547625 	 ± 0.2297154068766414
	data : 0.11581406593322754
	model : 0.07021994590759277
			 train-loss:  2.0974442037669094 	 ± 0.2289215454972578
	data : 0.11604046821594238
	model : 0.07009892463684082
			 train-loss:  2.1048685762617323 	 ± 0.23165891802055594
	data : 0.1160660743713379
	model : 0.0695425033569336
			 train-loss:  2.0976859797602114 	 ± 0.23413829557471202
	data : 0.11649255752563477
	model : 0.06949734687805176
			 train-loss:  2.0890804985736278 	 ± 0.23887411077196913
	data : 0.11654515266418457
	model : 0.0695218563079834
			 train-loss:  2.081862715383371 	 ± 0.2414965917089225
	data : 0.11643261909484863
	model : 0.06941652297973633
			 train-loss:  2.0761550372960618 	 ± 0.24226867664211507
	data : 0.11653099060058594
	model : 0.06957468986511231
			 train-loss:  2.0793403840065 	 ± 0.2408680234026565
	data : 0.11647477149963378
	model : 0.06965508460998535
			 train-loss:  2.078854708110585 	 ± 0.23851960628247995
	data : 0.1162139892578125
	model : 0.06958656311035157
			 train-loss:  2.079858795954631 	 ± 0.23632382679930758
	data : 0.11631546020507813
	model : 0.06946439743041992
			 train-loss:  2.0828563244837635 	 ± 0.23507961744310588
	data : 0.1164320945739746
	model : 0.06882600784301758
			 train-loss:  2.07550996321219 	 ± 0.2389548211198321
	data : 0.11695842742919922
	model : 0.06851439476013184
			 train-loss:  2.0719771428541702 	 ± 0.238191519100295
	data : 0.11716537475585938
	model : 0.06855154037475586
			 train-loss:  2.0690393107278005 	 ± 0.2370585758248615
	data : 0.11726593971252441
	model : 0.06806573867797852
			 train-loss:  2.067553819271556 	 ± 0.235232724142678
	data : 0.11763153076171876
	model : 0.06826291084289551
			 train-loss:  2.069935044337963 	 ± 0.23388799841778757
	data : 0.11759257316589355
	model : 0.06904096603393554
			 train-loss:  2.075771596472142 	 ± 0.23611905044119602
	data : 0.11686606407165527
	model : 0.06924347877502442
			 train-loss:  2.0709517101446786 	 ± 0.23705199745330474
	data : 0.11667976379394532
	model : 0.06894092559814453
			 train-loss:  2.0722435478304253 	 ± 0.2353137752112778
	data : 0.11701359748840331
	model : 0.06950111389160156
			 train-loss:  2.069954249166673 	 ± 0.23409220615367857
	data : 0.11659531593322754
	model : 0.06914243698120118
			 train-loss:  2.0710165689861966 	 ± 0.23237749779300437
	data : 0.11682977676391601
	model : 0.06930027008056641
			 train-loss:  2.0794737339019775 	 ± 0.24012819829188264
	data : 0.11658878326416015
	model : 0.06937909126281738
			 train-loss:  2.07446433030642 	 ± 0.24162051812988503
	data : 0.11649489402770996
	model : 0.07041292190551758
			 train-loss:  2.076615024696697 	 ± 0.2404091902290254
	data : 0.11541624069213867
	model : 0.07011680603027344
			 train-loss:  2.0747302592690313 	 ± 0.23909913787012885
	data : 0.11565155982971191
	model : 0.07047820091247559
			 train-loss:  2.07992780734511 	 ± 0.2411175188598349
	data : 0.11537127494812012
	model : 0.07024292945861817
			 train-loss:  2.08130169087562 	 ± 0.23963187800019506
	data : 0.1156693458557129
	model : 0.06935367584228516
			 train-loss:  2.0835741468838282 	 ± 0.23866173138065286
	data : 0.11646194458007812
	model : 0.06778268814086914
			 train-loss:  2.0836506041002947 	 ± 0.23697591827944242
	data : 0.11810297966003418
	model : 0.0671341896057129
			 train-loss:  2.076687380671501 	 ± 0.24252867766759897
	data : 0.11871552467346191
	model : 0.06726117134094238
			 train-loss:  2.075091700031333 	 ± 0.24124205695631742
	data : 0.11869597434997559
	model : 0.06728663444519042
			 train-loss:  2.0729901130134993 	 ± 0.24027836148232343
	data : 0.11868691444396973
	model : 0.06788406372070313
			 train-loss:  2.0745081377029417 	 ± 0.23902810289353502
	data : 0.11813011169433593
	model : 0.0688403606414795
			 train-loss:  2.0758564676109112 	 ± 0.23773728067001287
	data : 0.11726460456848145
	model : 0.06986298561096191
			 train-loss:  2.083960221959399 	 ± 0.24652785819978348
	data : 0.1163442611694336
	model : 0.06969194412231446
			 train-loss:  2.0849256286254296 	 ± 0.24508890294698898
	data : 0.11632380485534669
	model : 0.0689882755279541
			 train-loss:  2.0835410567778574 	 ± 0.24383957380924023
	data : 0.11687211990356446
	model : 0.06923966407775879
			 train-loss:  2.0879796251654623 	 ± 0.245501295904345
	data : 0.11660580635070801
	model : 0.06880655288696289
			 train-loss:  2.0869599492461592 	 ± 0.24415155210076092
	data : 0.11696152687072754
	model : 0.0687483787536621
			 train-loss:  2.0861640630698783 	 ± 0.24276395455680558
	data : 0.11698675155639648
	model : 0.06875848770141602
			 train-loss:  2.0860424142286003 	 ± 0.24129960390261615
	data : 0.11693315505981446
	model : 0.06870741844177246
			 train-loss:  2.081899391753333 	 ± 0.24281063170128722
	data : 0.11700024604797363
	model : 0.0680974006652832
			 train-loss:  2.08056961368112 	 ± 0.24168559966479836
	data : 0.11770744323730468
	model : 0.06810111999511718
			 train-loss:  2.0817032816798187 	 ± 0.24050356104173384
	data : 0.11781229972839355
	model : 0.06808056831359863
			 train-loss:  2.0834115798445954 	 ± 0.23964157617383883
	data : 0.11789908409118652
	model : 0.06840310096740723
			 train-loss:  2.083508547056805 	 ± 0.23827780254448758
	data : 0.11782774925231934
	model : 0.0693901538848877
			 train-loss:  2.0853873276978394 	 ± 0.2375899807947334
	data : 0.11697206497192383
	model : 0.06898708343505859
			 train-loss:  2.084712051020728 	 ± 0.23635221968261289
	data : 0.11716804504394532
	model : 0.06842188835144043
			 train-loss:  2.085853609409961 	 ± 0.23529934877070902
	data : 0.11750974655151367
	model : 0.06808695793151856
			 train-loss:  2.086108377446299 	 ± 0.23402967344221648
	data : 0.11765923500061035
	model : 0.06754527091979981
			 train-loss:  2.0847956993246592 	 ± 0.23310832657442815
	data : 0.11795597076416016
	model : 0.06674585342407227
			 train-loss:  2.085396430593856 	 ± 0.2319374352631458
	data : 0.11869592666625976
	model : 0.06750597953796386
			 train-loss:  2.0915497190073915 	 ± 0.2383019703522204
	data : 0.1179997444152832
	model : 0.06823167800903321
			 train-loss:  2.0884520014127097 	 ± 0.2389725828326838
	data : 0.11736712455749512
	model : 0.06795802116394042
			 train-loss:  2.087660170093025 	 ± 0.23786413317925997
	data : 0.11786298751831055
	model : 0.06844935417175294
			 train-loss:  2.0871833660164656 	 ± 0.2366940171945068
	data : 0.11757688522338867
	model : 0.06830134391784667
			 train-loss:  2.086107698353854 	 ± 0.2357361876598073
	data : 0.11768908500671386
	model : 0.06880650520324708
			 train-loss:  2.0899242651462555 	 ± 0.23760868396727705
	data : 0.11724905967712403
	model : 0.06891322135925293
			 train-loss:  2.0887951485001213 	 ± 0.23669893992057583
	data : 0.11710109710693359
	model : 0.06973700523376465
			 train-loss:  2.0871103312454973 	 ± 0.2361436202220258
	data : 0.11638503074645996
	model : 0.06959767341613769
			 train-loss:  2.0857021056332634 	 ± 0.23542448746958855
	data : 0.11653718948364258
	model : 0.07089042663574219
			 train-loss:  2.0865409339849768 	 ± 0.23444452178475486
	data : 0.11532959938049317
	model : 0.07064685821533204
			 train-loss:  2.084942428270976 	 ± 0.2338942236042564
	data : 0.11553106307983399
	model : 0.07003350257873535
			 train-loss:  2.0852582623373785 	 ± 0.2328108300239756
	data : 0.11614456176757812
	model : 0.06889905929565429
			 train-loss:  2.0871909945924707 	 ± 0.2325731936247299
	data : 0.11703166961669922
	model : 0.06866307258605957
			 train-loss:  2.087907761335373 	 ± 0.23161266396501226
	data : 0.11717934608459472
	model : 0.06809086799621582
			 train-loss:  2.087618452693344 	 ± 0.23056737617382111
	data : 0.11766562461853028
	model : 0.0686607837677002
			 train-loss:  2.08572819666429 	 ± 0.23036383108140268
	data : 0.11692271232604981
	model : 0.06956133842468262
			 train-loss:  2.086136362574122 	 ± 0.22936376145333712
	data : 0.1161797046661377
	model : 0.07020931243896485
			 train-loss:  2.0855120526892796 	 ± 0.22843223682781666
	data : 0.11554832458496093
	model : 0.06994943618774414
			 train-loss:  2.090183637838448 	 ± 0.23273109116985768
	data : 0.11574225425720215
	model : 0.07012424468994141
			 train-loss:  2.088366437376591 	 ± 0.23251191510966646
	data : 0.11574444770812989
	model : 0.06951265335083008
			 train-loss:  2.0906352188276207 	 ± 0.23276272948538454
	data : 0.11638174057006836
	model : 0.06936655044555665
			 train-loss:  2.0921371455850273 	 ± 0.23231626603110483
	data : 0.11635036468505859
	model : 0.06976132392883301
			 train-loss:  2.0896100101307926 	 ± 0.23291711432643245
	data : 0.11599235534667969
	model : 0.0700760841369629
			 train-loss:  2.0929318444203524 	 ± 0.23469486436956227
	data : 0.11578788757324218
	model : 0.07009410858154297
			 train-loss:  2.0933923380715505 	 ± 0.23376019876325982
	data : 0.11579174995422363
	model : 0.07007217407226562
			 train-loss:  2.0972958584626515 	 ± 0.23664683469943537
	data : 0.11599316596984863
	model : 0.07005114555358886
			 train-loss:  2.095293249965699 	 ± 0.23668576868906946
	data : 0.11600522994995117
	model : 0.06994566917419434
			 train-loss:  2.1005565729297575 	 ± 0.24271996687675146
	data : 0.11620450019836426
	model : 0.07002358436584473
			 train-loss:  2.1002968772640074 	 ± 0.2417483048317936
	data : 0.11597623825073242
	model : 0.07013955116271972
			 train-loss:  2.0989569915879156 	 ± 0.24122967395103925
	data : 0.11577486991882324
	model : 0.06939845085144043
			 train-loss:  2.1005438318252563 	 ± 0.2409117291253896
	data : 0.11661486625671387
	model : 0.06929206848144531
			 train-loss:  2.1005083085998657 	 ± 0.2399541544950864
	data : 0.11668848991394043
	model : 0.0690983772277832
			 train-loss:  2.0993825816732694 	 ± 0.23934139029937146
	data : 0.11664333343505859
	model : 0.06816930770874023
			 train-loss:  2.098777682520449 	 ± 0.23850206956568792
	data : 0.11771883964538574
	model : 0.06801328659057618
			 train-loss:  2.098389047984929 	 ± 0.2376165284039934
	data : 0.11777057647705078
	model : 0.06855506896972656
			 train-loss:  2.096622908115387 	 ± 0.23754931638227542
	data : 0.11716890335083008
	model : 0.06808323860168457
			 train-loss:  2.100217290507018 	 ± 0.24016340861203955
	data : 0.1178199291229248
	model : 0.06851010322570801
			 train-loss:  2.106514476465456 	 ± 0.24987246940135166
	data : 0.11749463081359864
	model : 0.06933736801147461
			 train-loss:  2.10727314034799 	 ± 0.24908388311127883
	data : 0.11644792556762695
	model : 0.06929950714111328
			 train-loss:  2.1059927086331953 	 ± 0.24859169176337456
	data : 0.1166043758392334
	model : 0.06931157112121582
			 train-loss:  2.1038597963474417 	 ± 0.24889691782263015
	data : 0.11654419898986816
	model : 0.06984333992004395
			 train-loss:  2.102899613625863 	 ± 0.2482309947331521
	data : 0.11602253913879394
	model : 0.0696223258972168
			 train-loss:  2.1037385089554057 	 ± 0.24751679765001186
	data : 0.11635932922363282
	model : 0.06955127716064453
			 train-loss:  2.1028959846151047 	 ± 0.24681545346682549
	data : 0.11663093566894531
	model : 0.06875696182250976
			 train-loss:  2.100834217860544 	 ± 0.2471158266139473
	data : 0.11737213134765626
	model : 0.06895265579223633
			 train-loss:  2.1012143654482704 	 ± 0.24627247427910656
	data : 0.11716861724853515
	model : 0.06903533935546875
			 train-loss:  2.100042442903451 	 ± 0.24578906638311715
	data : 0.11691393852233886
	model : 0.06916666030883789
			 train-loss:  2.0988141894340515 	 ± 0.2453559459132506
	data : 0.11679725646972657
	model : 0.06938343048095703
			 train-loss:  2.099784430090364 	 ± 0.24476976629815128
	data : 0.11640801429748535
	model : 0.06933236122131348
			 train-loss:  2.1007143962714405 	 ± 0.24417176943065397
	data : 0.11627664566040039
	model : 0.06936516761779785
			 train-loss:  2.100182159193631 	 ± 0.24341214713977233
	data : 0.1161104679107666
	model : 0.06933374404907226
			 train-loss:  2.100944106709467 	 ± 0.24275056508040121
	data : 0.1162674903869629
	model : 0.06915688514709473
			 train-loss:  2.098963791010331 	 ± 0.2431039451502256
	data : 0.1163558006286621
	model : 0.06821942329406738
			 train-loss:  2.09572467046815 	 ± 0.2454435099749596
	data : 0.11750583648681641
	model : 0.06894059181213379
			 train-loss:  2.095599421718777 	 ± 0.24462323297234315
	data : 0.11700305938720704
	model : 0.06862006187438965
			 train-loss:  2.09391161998113 	 ± 0.24467538045386555
	data : 0.11749906539916992
	model : 0.06793336868286133
			 train-loss:  2.0966635737198076 	 ± 0.24618197609539888
	data : 0.11804695129394531
	model : 0.0679853916168213
			 train-loss:  2.0961506217718124 	 ± 0.24545177857245284
	data : 0.11802215576171875
	model : 0.06888270378112793
			 train-loss:  2.095336107646718 	 ± 0.24485434239950635
	data : 0.11719565391540528
	model : 0.06911873817443848
			 train-loss:  2.0931969346938195 	 ± 0.24548824188540144
	data : 0.11684083938598633
	model : 0.06850838661193848
			 train-loss:  2.094240398560801 	 ± 0.24503744942519823
	data : 0.11733136177062989
	model : 0.06845312118530274
			 train-loss:  2.091982980569204 	 ± 0.24586241766371683
	data : 0.1174314022064209
	model : 0.06854557991027832
			 train-loss:  2.0920112117840226 	 ± 0.24507841918772968
	data : 0.11726260185241699
	model : 0.06863365173339844
			 train-loss:  2.090747257576713 	 ± 0.24481442634777648
	data : 0.11706061363220215
	model : 0.0686070442199707
			 train-loss:  2.089478593952251 	 ± 0.24456381880752187
	data : 0.11726646423339844
	model : 0.06951193809509278
			 train-loss:  2.0878807470202445 	 ± 0.24462948532753376
	data : 0.1164057731628418
	model : 0.07059550285339355
			 train-loss:  2.089480560018409 	 ± 0.24470674299247683
	data : 0.11543126106262207
	model : 0.0712658405303955
			 train-loss:  2.0871770941180947 	 ± 0.24569495443635372
	data : 0.11479425430297852
	model : 0.07116332054138183
			 train-loss:  2.0859133560964667 	 ± 0.2454676892902513
	data : 0.11483254432678222
	model : 0.070367431640625
			 train-loss:  2.0889022154052084 	 ± 0.24767540516681655
	data : 0.1155238151550293
	model : 0.07085952758789063
			 train-loss:  2.0896767970287438 	 ± 0.2471228967537992
	data : 0.11523714065551757
	model : 0.07045540809631348
			 train-loss:  2.08893534361598 	 ± 0.24656144357070156
	data : 0.11555256843566894
	model : 0.06957058906555176
			 train-loss:  2.090591276477197 	 ± 0.2467462443518711
	data : 0.11647005081176758
	model : 0.06936640739440918
			 train-loss:  2.0894948215711686 	 ± 0.24641849707898358
	data : 0.11664938926696777
	model : 0.07021842002868653
			 train-loss:  2.0902002407954288 	 ± 0.24585844100811546
	data : 0.1158940315246582
	model : 0.06894102096557617
			 train-loss:  2.0892793802654044 	 ± 0.24542639513796416
	data : 0.11694850921630859
	model : 0.06901578903198242
			 train-loss:  2.087718390581901 	 ± 0.2455526547117545
	data : 0.11692352294921875
	model : 0.0687718391418457
			 train-loss:  2.0910600936690042 	 ± 0.24870685751930388
	data : 0.11702303886413574
	model : 0.06860089302062988
			 train-loss:  2.0911296695643076 	 ± 0.247988688571352
	data : 0.11724696159362794
	model : 0.0685276985168457
			 train-loss:  2.0915141941487105 	 ± 0.2473267682081704
	data : 0.11729116439819336
	model : 0.06933064460754394
			 train-loss:  2.090730016572135 	 ± 0.2468359434585791
	data : 0.11648101806640625
	model : 0.06946778297424316
			 train-loss:  2.0917075377973644 	 ± 0.24647316713407677
	data : 0.11660027503967285
	model : 0.06961612701416016
			 train-loss:  2.0918748466308505 	 ± 0.2457859514661809
	data : 0.11649856567382813
	model : 0.06897039413452148
			 train-loss:  2.0926794315991777 	 ± 0.24532820868446534
	data : 0.11698422431945801
	model : 0.06819095611572265
			 train-loss:  2.091328341201697 	 ± 0.24530516706200062
	data : 0.11773786544799805
	model : 0.06799015998840333
			 train-loss:  2.0905891233020357 	 ± 0.24482265996151867
	data : 0.11807870864868164
	model : 0.0678431510925293
			 train-loss:  2.0914774520621116 	 ± 0.24443614289199955
	data : 0.11798329353332519
	model : 0.06852874755859376
			 train-loss:  2.0909739018796563 	 ± 0.24385780990949663
	data : 0.11757469177246094
	model : 0.06874403953552247
			 train-loss:  2.09100483480047 	 ± 0.24319097707257548
	data : 0.11736383438110351
	model : 0.06939678192138672
			 train-loss:  2.0897936853377717 	 ± 0.24308201870419024
	data : 0.11673083305358886
	model : 0.06936993598937988
			 train-loss:  2.090157272364642 	 ± 0.24247431309029283
	data : 0.1165628433227539
	model : 0.06914830207824707
			 train-loss:  2.088975627576151 	 ± 0.24235513088255828
	data : 0.11684060096740723
	model : 0.06863446235656738
			 train-loss:  2.0903943190600147 	 ± 0.2424794273040701
	data : 0.11720967292785645
	model : 0.0688211441040039
			 train-loss:  2.0909632755086776 	 ± 0.2419587993313699
	data : 0.11702141761779786
	model : 0.06899557113647461
			 train-loss:  2.090924285076283 	 ± 0.2413184399239781
	data : 0.11684975624084473
	model : 0.06923699378967285
			 train-loss:  2.091487645475488 	 ± 0.24080713334885107
	data : 0.11669349670410156
	model : 0.06992158889770508
			 train-loss:  2.089787935087194 	 ± 0.24131594623871191
	data : 0.11601085662841797
	model : 0.06986346244812011
			 train-loss:  2.0913492192824683 	 ± 0.2416519609206154
	data : 0.11618151664733886
	model : 0.07021756172180176
			 train-loss:  2.0920274652965327 	 ± 0.2412082613991229
	data : 0.11595768928527832
	model : 0.0700312614440918
			 train-loss:  2.091475459103732 	 ± 0.24070797723431908
	data : 0.11615357398986817
	model : 0.06919717788696289
			 train-loss:  2.089923512629974 	 ± 0.24106110584160817
	data : 0.11702852249145508
	model : 0.06871347427368164
			 train-loss:  2.090659646355376 	 ± 0.240665003553516
	data : 0.11748738288879394
	model : 0.06878805160522461
			 train-loss:  2.0905921071919087 	 ± 0.24005526374593417
	data : 0.11740870475769043
	model : 0.06902451515197754
			 train-loss:  2.090860042909179 	 ± 0.2394778259341581
	data : 0.11716456413269043
	model : 0.07004685401916504
			 train-loss:  2.092580657508505 	 ± 0.24009919182248185
	data : 0.1161606788635254
	model : 0.0709254264831543
			 train-loss:  2.0914546036720276 	 ± 0.24002440554144983
	data : 0.11526055335998535
	model : 0.07106189727783203
			 train-loss:  2.0901906175992977 	 ± 0.24009294442966253
	data : 0.11522464752197266
	model : 0.0710233211517334
			 train-loss:  2.090023912415646 	 ± 0.23950957906419254
	data : 0.11527767181396484
	model : 0.07096490859985352
			 train-loss:  2.0924204071167067 	 ± 0.24133457439103742
	data : 0.11523218154907226
	model : 0.06991357803344726
			 train-loss:  2.0923397640387216 	 ± 0.24074508327395794
	data : 0.11625847816467286
	model : 0.0688704490661621
			 train-loss:  2.092793102380706 	 ± 0.2402444534440485
	data : 0.1172722339630127
	model : 0.06874089241027832
			 train-loss:  2.0916922856303093 	 ± 0.24017833865896768
	data : 0.11728386878967285
	model : 0.06800255775451661
			 train-loss:  2.0905584505790675 	 ± 0.240149515250556
	data : 0.11800041198730468
	model : 0.06800994873046876
			 train-loss:  2.0911986632989 	 ± 0.23974854541185217
	data : 0.11825251579284668
	model : 0.06821398735046387
			 train-loss:  2.092364446968553 	 ± 0.23976452378782628
	data : 0.11815171241760254
	model : 0.06916460990905762
			 train-loss:  2.093718850044977 	 ± 0.2399930630833787
	data : 0.11726946830749511
	model : 0.06923122406005859
			 train-loss:  2.0939246252249766 	 ± 0.2394422528414215
	data : 0.11735005378723144
	model : 0.06974444389343262
			 train-loss:  2.094045766119687 	 ± 0.23888334420488563
	data : 0.11656875610351562
	model : 0.06981105804443359
			 train-loss:  2.0933884869159107 	 ± 0.23851399862147063
	data : 0.11633062362670898
	model : 0.06889834403991699
			 train-loss:  2.093043982425583 	 ± 0.23800918255532016
	data : 0.11719331741333008
	model : 0.06828012466430664
			 train-loss:  2.0916099808936894 	 ± 0.23837984669345666
	data : 0.11761431694030762
	model : 0.06821370124816895
			 train-loss:  2.0919990379501274 	 ± 0.23789580994805473
	data : 0.11754484176635742
	model : 0.06839609146118164
			 train-loss:  2.092298258834171 	 ± 0.23738776671589448
	data : 0.11747274398803711
	model : 0.06814374923706054
			 train-loss:  2.0914253690920837 	 ± 0.23719146744438946
	data : 0.11771607398986816
	model : 0.06893582344055176
			 train-loss:  2.089853860471891 	 ± 0.23778410286819185
	data : 0.11685872077941895
	model : 0.0688894271850586
			 train-loss:  2.0884767711162566 	 ± 0.23811673587975152
	data : 0.11702218055725097
	model : 0.0680809497833252
			 train-loss:  2.0866785928674414 	 ± 0.23906981955931833
	data : 0.11783304214477539
	model : 0.06734123229980468
			 train-loss:  2.0870987386316866 	 ± 0.2386125267720256
	data : 0.11863155364990234
	model : 0.06742801666259765
			 train-loss:  2.088353919875996 	 ± 0.23881033557083856
	data : 0.11849288940429688
	model : 0.06665472984313965
			 train-loss:  2.0890342194054807 	 ± 0.23849314988534256
	data : 0.11914949417114258
	model : 0.06640477180480957
			 train-loss:  2.0888419983122084 	 ± 0.23797996476337147
	data : 0.1191563606262207
	model : 0.06652278900146484
			 train-loss:  2.0885368949544114 	 ± 0.2374969757075405
	data : 0.11890611648559571
	model : 0.06664581298828125
			 train-loss:  2.0877019870648823 	 ± 0.23730544073686363
	data : 0.11873278617858887
	model : 0.06625351905822754
			 train-loss:  2.0882008854757275 	 ± 0.23690373941853413
	data : 0.1191749095916748
	model : 0.0670386791229248
			 train-loss:  2.0878776603390556 	 ± 0.23643629515012834
	data : 0.11845488548278808
	model : 0.06793146133422852
			 train-loss:  2.087694297147834 	 ± 0.23593806054374997
	data : 0.11794395446777343
	model : 0.06854476928710937
			 train-loss:  2.0884513519543075 	 ± 0.23570661231095139
	data : 0.11730036735534669
	model : 0.06917748451232911
			 train-loss:  2.0868807568632324 	 ± 0.23640634142917866
	data : 0.11678113937377929
	model : 0.06906399726867676
			 train-loss:  2.085301496951877 	 ± 0.23712173696095298
	data : 0.11681060791015625
	model : 0.06856608390808105
			 train-loss:  2.082844807042016 	 ± 0.239567660207099
	data : 0.11736865043640136
	model : 0.06804161071777344
			 train-loss:  2.083541393787303 	 ± 0.23929476438000621
	data : 0.11768369674682617
	model : 0.06738104820251464
			 train-loss:  2.0819540594593953 	 ± 0.24002387765859612
	data : 0.1182222843170166
	model : 0.0664224624633789
			 train-loss:  2.0812677441770013 	 ± 0.23974890672272234
	data : 0.11882963180541992
	model : 0.0663360595703125
			 train-loss:  2.080945989664863 	 ± 0.2392959741902943
	data : 0.11892070770263671
	model : 0.06621375083923339
			 train-loss:  2.079920621097836 	 ± 0.2393181972917104
	data : 0.11905665397644043
	model : 0.06614551544189454
			 train-loss:  2.0818764785925548 	 ± 0.24072562533816536
	data : 0.11915287971496583
	model : 0.06637110710144042
			 train-loss:  2.082418884973803 	 ± 0.24037259464253355
	data : 0.11896748542785644
	model : 0.06708879470825195
			 train-loss:  2.082286382509657 	 ± 0.23988426237567567
	data : 0.11856307983398437
	model : 0.06747236251831054
			 train-loss:  2.081576188895928 	 ± 0.23964496573761737
	data : 0.11813917160034179
	model : 0.06815500259399414
			 train-loss:  2.080568151395829 	 ± 0.23966907156382336
	data : 0.11747746467590332
	model : 0.06866364479064942
			 train-loss:  2.08201335790206 	 ± 0.24024244724107896
	data : 0.11719985008239746
	model : 0.06908211708068848
			 train-loss:  2.082706382604149 	 ± 0.23999892333388495
	data : 0.1168592929840088
	model : 0.06908459663391113
			 train-loss:  2.0836053830891967 	 ± 0.23992729018121212
	data : 0.11664252281188965
	model : 0.06869516372680665
			 train-loss:  2.084604330601231 	 ± 0.23995722027290997
	data : 0.11717920303344727
	model : 0.06842031478881835
			 train-loss:  2.0849339071526587 	 ± 0.2395311308659007
	data : 0.11734352111816407
	model : 0.06855716705322265
			 train-loss:  2.0843160510063172 	 ± 0.23925032248976597
	data : 0.11724128723144531
	model : 0.06840410232543945
			 train-loss:  2.085715043592263 	 ± 0.23979566553787504
	data : 0.11731996536254882
	model : 0.06849336624145508
			 train-loss:  2.08463886096364 	 ± 0.23992598703888507
	data : 0.11704931259155274
	model : 0.06869263648986816
			 train-loss:  2.085724244947019 	 ± 0.24007045296165783
	data : 0.1168868064880371
	model : 0.06862049102783203
			 train-loss:  2.085312424212929 	 ± 0.23968693214458528
	data : 0.1168914794921875
	model : 0.06835103034973145
			 train-loss:  2.085061434670991 	 ± 0.23924993819920365
	data : 0.11676483154296875
	model : 0.06829342842102051
			 train-loss:  2.0860179415903986 	 ± 0.2392702196173287
	data : 0.11580314636230468
	model : 0.059447717666625974
#epoch  33    val-loss:  2.48068810764112  train-loss:  2.0860179415903986  lr:  0.00125
			 train-loss:  2.2455062866210938 	 ± 0.0
	data : 5.792544841766357
	model : 0.07257366180419922
			 train-loss:  2.2194536924362183 	 ± 0.02605259418487549
	data : 2.9646178483963013
	model : 0.07169628143310547
			 train-loss:  2.1531783739725747 	 ± 0.09611101628198458
	data : 2.0168870290120444
	model : 0.06968172391255696
			 train-loss:  2.083611875772476 	 ± 0.14644619695826835
	data : 1.5425832867622375
	model : 0.06949663162231445
			 train-loss:  1.994837260246277 	 ± 0.2206375315537454
	data : 1.2573086738586425
	model : 0.06957921981811524
			 train-loss:  1.9500062863032024 	 ± 0.22498114305840125
	data : 0.12194385528564453
	model : 0.06904916763305664
			 train-loss:  2.0098784310477122 	 ± 0.25474227437496366
	data : 0.11786947250366211
	model : 0.06893305778503418
			 train-loss:  1.9951551407575607 	 ± 0.2414525833332283
	data : 0.11672983169555665
	model : 0.06958250999450684
			 train-loss:  2.0087146361668906 	 ± 0.23085173546882665
	data : 0.11634702682495117
	model : 0.06961631774902344
			 train-loss:  2.013886296749115 	 ± 0.21955406179497527
	data : 0.11647982597351074
	model : 0.06868882179260254
			 train-loss:  1.9918315735730259 	 ± 0.22064884691371042
	data : 0.1172637939453125
	model : 0.06861538887023926
			 train-loss:  1.9947279194990795 	 ± 0.2114734763296441
	data : 0.11722002029418946
	model : 0.06852245330810547
			 train-loss:  2.0014665035101085 	 ± 0.20451370135579805
	data : 0.11734204292297364
	model : 0.06849098205566406
			 train-loss:  2.032176775591714 	 ± 0.2260505774964356
	data : 0.11730794906616211
	model : 0.06847171783447266
			 train-loss:  2.0160823027292887 	 ± 0.22653635795337146
	data : 0.11731142997741699
	model : 0.06941967010498047
			 train-loss:  2.0227413028478622 	 ± 0.22085387843841878
	data : 0.11650657653808594
	model : 0.06857528686523437
			 train-loss:  1.9989458953633028 	 ± 0.2344499439189982
	data : 0.11710290908813477
	model : 0.06870822906494141
			 train-loss:  1.9995641973283556 	 ± 0.22785865238996547
	data : 0.1170849323272705
	model : 0.06980643272399903
			 train-loss:  2.00845331894724 	 ± 0.2249650049604907
	data : 0.11585531234741211
	model : 0.06996707916259766
			 train-loss:  2.020740818977356 	 ± 0.22571544312882508
	data : 0.11561269760131836
	model : 0.06901650428771973
			 train-loss:  2.022494293394543 	 ± 0.2204152561265692
	data : 0.11642327308654785
	model : 0.06940169334411621
			 train-loss:  2.0141584277153015 	 ± 0.21870937785518968
	data : 0.11617584228515625
	model : 0.06858463287353515
			 train-loss:  2.029314232909161 	 ± 0.2254050295804004
	data : 0.1167677402496338
	model : 0.06756181716918945
			 train-loss:  2.041355555256208 	 ± 0.22809055313414772
	data : 0.11768946647644044
	model : 0.0675654411315918
			 train-loss:  2.0392647266387938 	 ± 0.2237167985540141
	data : 0.11777820587158203
	model : 0.06858134269714355
			 train-loss:  2.0409256036464987 	 ± 0.21952949529916876
	data : 0.11710000038146973
	model : 0.06820459365844726
			 train-loss:  2.038923630007991 	 ± 0.2156675018595708
	data : 0.11776885986328126
	model : 0.06912841796875
			 train-loss:  2.051239034959248 	 ± 0.22123828792286715
	data : 0.11687207221984863
	model : 0.06867733001708984
			 train-loss:  2.042315224121357 	 ± 0.22245974249520695
	data : 0.11746454238891602
	model : 0.06779294013977051
			 train-loss:  2.059816141923269 	 ± 0.23816151534207816
	data : 0.11829423904418945
	model : 0.0676802158355713
			 train-loss:  2.054317582038141 	 ± 0.236216480213265
	data : 0.11833291053771973
	model : 0.06851687431335449
			 train-loss:  2.0549575686454773 	 ± 0.23252360750102932
	data : 0.11740527153015137
	model : 0.06870098114013672
			 train-loss:  2.04252244125713 	 ± 0.23953509925832783
	data : 0.11729698181152344
	model : 0.0695988655090332
			 train-loss:  2.0436520611538604 	 ± 0.23607543801515715
	data : 0.11641082763671876
	model : 0.07020263671875
			 train-loss:  2.0533789464405605 	 ± 0.2394913333737974
	data : 0.11573185920715331
	model : 0.07023053169250489
			 train-loss:  2.043661448690626 	 ± 0.24303891763207536
	data : 0.11553874015808105
	model : 0.07046480178833008
			 train-loss:  2.055752535124083 	 ± 0.2504685263593972
	data : 0.11537060737609864
	model : 0.07011976242065429
			 train-loss:  2.046849680574317 	 ± 0.2530142793702192
	data : 0.11565103530883789
	model : 0.06973781585693359
			 train-loss:  2.0394944563890114 	 ± 0.25383175166058974
	data : 0.11598634719848633
	model : 0.06996326446533203
			 train-loss:  2.0367886543273928 	 ± 0.2512077375885863
	data : 0.11590886116027832
	model : 0.07030239105224609
			 train-loss:  2.0405616527650414 	 ± 0.24927012212732644
	data : 0.11577653884887695
	model : 0.07015199661254883
			 train-loss:  2.0488391603742326 	 ± 0.2519233560167646
	data : 0.11597485542297363
	model : 0.07020740509033203
			 train-loss:  2.04570410417956 	 ± 0.24980440057783931
	data : 0.11581888198852539
	model : 0.07019739151000977
			 train-loss:  2.050584977323359 	 ± 0.24901484198246407
	data : 0.11598391532897949
	model : 0.06949782371520996
			 train-loss:  2.0462494638231066 	 ± 0.24790619409982873
	data : 0.11659550666809082
	model : 0.06910347938537598
			 train-loss:  2.053592246511708 	 ± 0.2500953607493804
	data : 0.11704635620117188
	model : 0.06953115463256836
			 train-loss:  2.05743097244425 	 ± 0.24878652523177053
	data : 0.11657977104187012
	model : 0.06939582824707032
			 train-loss:  2.0584829300642014 	 ± 0.24628697161120633
	data : 0.11684298515319824
	model : 0.06903243064880371
			 train-loss:  2.0552340308014228 	 ± 0.24479792788235397
	data : 0.1170424461364746
	model : 0.0690505027770996
			 train-loss:  2.05408207654953 	 ± 0.2424717053944821
	data : 0.11712441444396973
	model : 0.06913113594055176
			 train-loss:  2.0544005258410585 	 ± 0.24009332300052585
	data : 0.1169036865234375
	model : 0.06851868629455567
			 train-loss:  2.056341691659047 	 ± 0.23817729571173604
	data : 0.117510986328125
	model : 0.06855406761169433
			 train-loss:  2.0666379951081186 	 ± 0.24732730850328677
	data : 0.1176518440246582
	model : 0.06913156509399414
			 train-loss:  2.0610912618813693 	 ± 0.2483316657365775
	data : 0.11714887619018555
	model : 0.06893687248229981
			 train-loss:  2.058885429122231 	 ± 0.24659707285530283
	data : 0.117242431640625
	model : 0.06797642707824707
			 train-loss:  2.0557598109756197 	 ± 0.24548226675273271
	data : 0.11804862022399902
	model : 0.06835236549377441
			 train-loss:  2.0530829366884733 	 ± 0.2441425812214941
	data : 0.11773476600646973
	model : 0.06828289031982422
			 train-loss:  2.055798281883371 	 ± 0.24289541757829242
	data : 0.11752424240112305
	model : 0.06800122261047363
			 train-loss:  2.063060754436558 	 ± 0.24709783103498584
	data : 0.11772027015686035
	model : 0.06796979904174805
			 train-loss:  2.061284150679906 	 ± 0.24540973642883832
	data : 0.1175760269165039
	model : 0.06888418197631836
			 train-loss:  2.0541875870501407 	 ± 0.24952014184049354
	data : 0.1166879653930664
	model : 0.06873612403869629
			 train-loss:  2.046221069751247 	 ± 0.2552008856728218
	data : 0.11672706604003906
	model : 0.06822152137756347
			 train-loss:  2.046487776059953 	 ± 0.2531760899709102
	data : 0.11745085716247558
	model : 0.06746830940246581
			 train-loss:  2.0442322306334972 	 ± 0.2518275417381738
	data : 0.11800689697265625
	model : 0.06755914688110351
			 train-loss:  2.04311004785391 	 ± 0.2500441116042959
	data : 0.11812386512756348
	model : 0.06754779815673828
			 train-loss:  2.050958106012055 	 ± 0.25608247682767216
	data : 0.11829743385314942
	model : 0.06756792068481446
			 train-loss:  2.0547041964175095 	 ± 0.25597977668510696
	data : 0.1182939052581787
	model : 0.06819386482238769
			 train-loss:  2.050999304827522 	 ± 0.25589390473982404
	data : 0.11770048141479492
	model : 0.0690197467803955
			 train-loss:  2.0558184264362724 	 ± 0.2571223598317135
	data : 0.1170466423034668
	model : 0.06983838081359864
			 train-loss:  2.064085616384234 	 ± 0.2643546022727516
	data : 0.11632723808288574
	model : 0.0698455810546875
			 train-loss:  2.0645936576413435 	 ± 0.26252076211418773
	data : 0.11626715660095215
	model : 0.06932392120361328
			 train-loss:  2.066019637717141 	 ± 0.26096808359672546
	data : 0.1166884422302246
	model : 0.06931781768798828
			 train-loss:  2.062027718922863 	 ± 0.2613785653229878
	data : 0.11664791107177734
	model : 0.06868481636047363
			 train-loss:  2.061405094894203 	 ± 0.25966098518851677
	data : 0.11728568077087402
	model : 0.06885933876037598
			 train-loss:  2.0619245306650797 	 ± 0.25796280558735935
	data : 0.11708459854125977
	model : 0.06799955368041992
			 train-loss:  2.06661940248389 	 ± 0.2594655200178837
	data : 0.11784591674804687
	model : 0.06766963005065918
			 train-loss:  2.0703050563861796 	 ± 0.2597699504594302
	data : 0.11806931495666503
	model : 0.06740202903747558
			 train-loss:  2.0687174858191075 	 ± 0.25847507184185686
	data : 0.11837196350097656
	model : 0.06812939643859864
			 train-loss:  2.069280497635467 	 ± 0.2568820726466182
	data : 0.11776857376098633
	model : 0.06786651611328125
			 train-loss:  2.0757136076688765 	 ± 0.26159692606773216
	data : 0.11808247566223144
	model : 0.06862554550170899
			 train-loss:  2.0761018535237254 	 ± 0.2600003062684519
	data : 0.11732406616210937
	model : 0.06857438087463379
			 train-loss:  2.078902863874668 	 ± 0.25963679525727507
	data : 0.11753044128417969
	model : 0.06868681907653809
			 train-loss:  2.080294321818524 	 ± 0.2583753979339882
	data : 0.11735353469848633
	model : 0.06873536109924316
			 train-loss:  2.078319646063305 	 ± 0.25746214396033196
	data : 0.11730942726135254
	model : 0.06857786178588868
			 train-loss:  2.0801969920887666 	 ± 0.25652088287269253
	data : 0.11725616455078125
	model : 0.06804404258728028
			 train-loss:  2.081870389539142 	 ± 0.2554913592089733
	data : 0.11773185729980469
	model : 0.06854052543640136
			 train-loss:  2.083952887304898 	 ± 0.254751844385649
	data : 0.1172215461730957
	model : 0.06776466369628906
			 train-loss:  2.0850334709340874 	 ± 0.25350070166248595
	data : 0.11774096488952637
	model : 0.06683855056762696
			 train-loss:  2.0825273856688082 	 ± 0.2531664171426574
	data : 0.11851997375488281
	model : 0.06719326972961426
			 train-loss:  2.084603993097941 	 ± 0.2525170930348553
	data : 0.11847867965698242
	model : 0.06812372207641601
			 train-loss:  2.084723561674684 	 ± 0.2511283655439032
	data : 0.11755228042602539
	model : 0.06833844184875489
			 train-loss:  2.0835829001405965 	 ± 0.24999672555639912
	data : 0.11729302406311035
	model : 0.06843266487121583
			 train-loss:  2.08419438715904 	 ± 0.24871818930060532
	data : 0.11731324195861817
	model : 0.06916327476501465
			 train-loss:  2.083169738028912 	 ± 0.24758894558913944
	data : 0.11673736572265625
	model : 0.0692873477935791
			 train-loss:  2.089867934427763 	 ± 0.25470062485107126
	data : 0.11648755073547364
	model : 0.06862049102783203
			 train-loss:  2.087810941040516 	 ± 0.2541625859896537
	data : 0.11717443466186524
	model : 0.0687283992767334
			 train-loss:  2.093455449822023 	 ± 0.25882670067443975
	data : 0.11712131500244141
	model : 0.06952419281005859
			 train-loss:  2.0943762891146602 	 ± 0.2576624290647728
	data : 0.11647834777832031
	model : 0.06976203918457032
			 train-loss:  2.097096303496698 	 ± 0.25776806026377264
	data : 0.11623206138610839
	model : 0.06955041885375976
			 train-loss:  2.0944201934337614 	 ± 0.2578544630310913
	data : 0.11654462814331054
	model : 0.06993699073791504
			 train-loss:  2.096505277227647 	 ± 0.2574206195481374
	data : 0.11637377738952637
	model : 0.06990957260131836
			 train-loss:  2.098004391380385 	 ± 0.256598317401449
	data : 0.11645374298095704
	model : 0.06993494033813477
			 train-loss:  2.100574938996324 	 ± 0.25666599686637603
	data : 0.11649341583251953
	model : 0.06925382614135742
			 train-loss:  2.0972677847513785 	 ± 0.25762479657462595
	data : 0.11701755523681641
	model : 0.06936101913452149
			 train-loss:  2.098912583078657 	 ± 0.2569431708937992
	data : 0.11694765090942383
	model : 0.0693751335144043
			 train-loss:  2.098815136360672 	 ± 0.25573025221521856
	data : 0.11679234504699706
	model : 0.06854615211486817
			 train-loss:  2.0962466669973927 	 ± 0.25590242610944747
	data : 0.1174476146697998
	model : 0.06812582015991211
			 train-loss:  2.097423349265699 	 ± 0.2550055872768846
	data : 0.1177365779876709
	model : 0.06872100830078125
			 train-loss:  2.096912536052389 	 ± 0.2538886453187904
	data : 0.11731209754943847
	model : 0.06849040985107421
			 train-loss:  2.0954254974018443 	 ± 0.25320837115840933
	data : 0.1174616813659668
	model : 0.06844162940979004
			 train-loss:  2.100371212572665 	 ± 0.2573470072574165
	data : 0.11758332252502442
	model : 0.0692265510559082
			 train-loss:  2.1016865095921924 	 ± 0.2565700606719181
	data : 0.11675062179565429
	model : 0.06864786148071289
			 train-loss:  2.1026417555007257 	 ± 0.25563224569162746
	data : 0.11710572242736816
	model : 0.06862068176269531
			 train-loss:  2.099775024673395 	 ± 0.2563264886495355
	data : 0.11710801124572753
	model : 0.06907992362976074
			 train-loss:  2.1038467355396437 	 ± 0.258885925767509
	data : 0.11655178070068359
	model : 0.06892805099487305
			 train-loss:  2.0981194068645608 	 ± 0.26498379412247475
	data : 0.11670794486999511
	model : 0.0684401512145996
			 train-loss:  2.0985594305217776 	 ± 0.26389151349809903
	data : 0.11723027229309083
	model : 0.07023420333862304
			 train-loss:  2.0990848076545587 	 ± 0.26283239266177
	data : 0.11561708450317383
	model : 0.06995515823364258
			 train-loss:  2.0970423421939883 	 ± 0.26266444684541007
	data : 0.11589322090148926
	model : 0.06889257431030274
			 train-loss:  2.0952208201090494 	 ± 0.26232138378009634
	data : 0.11683554649353027
	model : 0.06820840835571289
			 train-loss:  2.092054661640451 	 ± 0.2635275262769825
	data : 0.11743078231811524
	model : 0.06872258186340333
			 train-loss:  2.092113751857007 	 ± 0.2624460780461039
	data : 0.11723675727844238
	model : 0.06779603958129883
			 train-loss:  2.09194971390856 	 ± 0.26138332660847113
	data : 0.1180680274963379
	model : 0.06805882453918458
			 train-loss:  2.0954386428479226 	 ± 0.26318719298633264
	data : 0.11785702705383301
	model : 0.06881136894226074
			 train-loss:  2.0990773191452026 	 ± 0.26524538497014616
	data : 0.11731696128845215
	model : 0.07005062103271484
			 train-loss:  2.0988839760659235 	 ± 0.2641995704361502
	data : 0.11626372337341309
	model : 0.0705526351928711
			 train-loss:  2.0987183888127485 	 ± 0.2631639230683849
	data : 0.11593790054321289
	model : 0.07074856758117676
			 train-loss:  2.0980808744207025 	 ± 0.2622323581962457
	data : 0.11586065292358398
	model : 0.07084107398986816
			 train-loss:  2.09815635884455 	 ± 0.26121537229651365
	data : 0.11562514305114746
	model : 0.07030620574951171
			 train-loss:  2.0992601458842937 	 ± 0.26051058420770123
	data : 0.11593422889709473
	model : 0.06995220184326172
			 train-loss:  2.09736065191167 	 ± 0.2604165034500121
	data : 0.11620783805847168
	model : 0.06944589614868164
			 train-loss:  2.0955562618645756 	 ± 0.2602489286097943
	data : 0.11647672653198242
	model : 0.06936326026916503
			 train-loss:  2.096647172045887 	 ± 0.2595714769930633
	data : 0.11664505004882812
	model : 0.0692929744720459
			 train-loss:  2.099116486399921 	 ± 0.2601643805184445
	data : 0.11680312156677246
	model : 0.06904244422912598
			 train-loss:  2.097995881681089 	 ± 0.25952341308955534
	data : 0.11704888343811035
	model : 0.06912999153137207
			 train-loss:  2.0961679789949867 	 ± 0.25943829613823827
	data : 0.11703763008117676
	model : 0.06909213066101075
			 train-loss:  2.0950473790621236 	 ± 0.25881984037227607
	data : 0.11697773933410645
	model : 0.06908345222473145
			 train-loss:  2.094294725984767 	 ± 0.25803081260830846
	data : 0.11704077720642089
	model : 0.06895594596862793
			 train-loss:  2.097046704601041 	 ± 0.2591255206982662
	data : 0.11717114448547364
	model : 0.0688654899597168
			 train-loss:  2.0947562890393394 	 ± 0.25960665518610965
	data : 0.11733808517456054
	model : 0.06790199279785156
			 train-loss:  2.095001164057576 	 ± 0.258700651830053
	data : 0.11810512542724609
	model : 0.06784906387329101
			 train-loss:  2.098163263058998 	 ± 0.26050826837079977
	data : 0.1181488037109375
	model : 0.06700296401977539
			 train-loss:  2.0988048581810266 	 ± 0.2597083634624429
	data : 0.11895942687988281
	model : 0.06723666191101074
			 train-loss:  2.0971030617753663 	 ± 0.2596039021723335
	data : 0.11883111000061035
	model : 0.06726546287536621
			 train-loss:  2.0970018016880956 	 ± 0.25871002116742103
	data : 0.11865262985229492
	model : 0.06818475723266601
			 train-loss:  2.0997895288140804 	 ± 0.25999865512748604
	data : 0.1179570198059082
	model : 0.06833477020263672
			 train-loss:  2.099453195422685 	 ± 0.25914466427423805
	data : 0.1177445888519287
	model : 0.06929845809936523
			 train-loss:  2.099358222774557 	 ± 0.2582702585792955
	data : 0.11681323051452637
	model : 0.06903300285339356
			 train-loss:  2.0979587607735755 	 ± 0.25796454967376414
	data : 0.11718449592590333
	model : 0.06993613243103028
			 train-loss:  2.098645352522532 	 ± 0.25723979213053827
	data : 0.11645293235778809
	model : 0.06989665031433105
			 train-loss:  2.098589924787054 	 ± 0.2563874885160319
	data : 0.1162912368774414
	model : 0.07035260200500489
			 train-loss:  2.0963076807950674 	 ± 0.2570770010016802
	data : 0.11593551635742187
	model : 0.07035789489746094
			 train-loss:  2.096354571822422 	 ± 0.2562361549908753
	data : 0.11584625244140626
	model : 0.07041478157043457
			 train-loss:  2.0973028796059743 	 ± 0.2556720833273549
	data : 0.11574664115905761
	model : 0.07033123970031738
			 train-loss:  2.096245158872297 	 ± 0.25518380599867896
	data : 0.11572856903076172
	model : 0.0703458309173584
			 train-loss:  2.096045944171074 	 ± 0.25437668565960053
	data : 0.11579828262329102
	model : 0.06898422241210937
			 train-loss:  2.097972044519558 	 ± 0.2547039204551689
	data : 0.11698241233825683
	model : 0.06847496032714843
			 train-loss:  2.097576155692716 	 ± 0.253945068661539
	data : 0.11751399040222169
	model : 0.06771831512451172
			 train-loss:  2.096769761739287 	 ± 0.25334809102700934
	data : 0.11805357933044433
	model : 0.0679703712463379
			 train-loss:  2.0951700665056707 	 ± 0.2533593936121044
	data : 0.11808891296386718
	model : 0.06801738739013671
			 train-loss:  2.0933196004133046 	 ± 0.25365361526956565
	data : 0.1181182861328125
	model : 0.06820445060729981
			 train-loss:  2.0918903667249795 	 ± 0.25351897605085155
	data : 0.11802244186401367
	model : 0.06855211257934571
			 train-loss:  2.091320955680192 	 ± 0.25284400397759976
	data : 0.1177361011505127
	model : 0.06932892799377441
			 train-loss:  2.0896307905999625 	 ± 0.2529938892090051
	data : 0.11703763008117676
	model : 0.06829514503479003
			 train-loss:  2.0878517056956434 	 ± 0.253252989816841
	data : 0.11810555458068847
	model : 0.06754608154296875
			 train-loss:  2.087022848876126 	 ± 0.25271340391207725
	data : 0.11867904663085938
	model : 0.06836085319519043
			 train-loss:  2.085890319532977 	 ± 0.252377812691998
	data : 0.11772794723510742
	model : 0.06860361099243165
			 train-loss:  2.0831467317683354 	 ± 0.25411115831420356
	data : 0.11753010749816895
	model : 0.06881928443908691
			 train-loss:  2.0824274833385763 	 ± 0.25352969169255557
	data : 0.11733102798461914
	model : 0.06967277526855468
			 train-loss:  2.0841295873417574 	 ± 0.25374952706655
	data : 0.1165494441986084
	model : 0.07033867835998535
			 train-loss:  2.0833800096958006 	 ± 0.2531951756945291
	data : 0.1158111572265625
	model : 0.07103190422058106
			 train-loss:  2.085512530665065 	 ± 0.2539935523926483
	data : 0.11517486572265626
	model : 0.0707252025604248
			 train-loss:  2.0839180505344634 	 ± 0.2541202586153018
	data : 0.1152566909790039
	model : 0.07033743858337402
			 train-loss:  2.0825025062451417 	 ± 0.2540720852462776
	data : 0.11555685997009277
	model : 0.07018880844116211
			 train-loss:  2.084464921951294 	 ± 0.2546641699997014
	data : 0.11531076431274415
	model : 0.0699657917022705
			 train-loss:  2.0846918387846514 	 ± 0.2539574033690099
	data : 0.115775728225708
	model : 0.06925134658813477
			 train-loss:  2.083583936179425 	 ± 0.2536651704026874
	data : 0.11659178733825684
	model : 0.06955943107604981
			 train-loss:  2.0826213808541887 	 ± 0.25327557510736554
	data : 0.11646904945373535
	model : 0.06992521286010742
			 train-loss:  2.083819972070236 	 ± 0.25307284299917215
	data : 0.11610684394836426
	model : 0.06990609169006348
			 train-loss:  2.0832326955265468 	 ± 0.25249116714872366
	data : 0.11625046730041504
	model : 0.06925959587097168
			 train-loss:  2.082457612232609 	 ± 0.2520073518708456
	data : 0.11682295799255371
	model : 0.0685227394104004
			 train-loss:  2.081317593763163 	 ± 0.2517816458416734
	data : 0.11762690544128418
	model : 0.06803522109985352
			 train-loss:  2.0802254227341197 	 ± 0.2515247072017104
	data : 0.11805305480957032
	model : 0.06722674369812012
			 train-loss:  2.0795844210230787 	 ± 0.25099011969354257
	data : 0.11893296241760254
	model : 0.06741604804992676
			 train-loss:  2.080712587768967 	 ± 0.25077820664015354
	data : 0.11883206367492676
	model : 0.06833176612854004
			 train-loss:  2.079264774758329 	 ± 0.2508772255029254
	data : 0.11791577339172363
	model : 0.06913909912109376
			 train-loss:  2.0803850685211427 	 ± 0.2506715956159879
	data : 0.11724143028259278
	model : 0.06933941841125488
			 train-loss:  2.0794271192652114 	 ± 0.2503469935304531
	data : 0.1169510841369629
	model : 0.06990623474121094
			 train-loss:  2.0789613666988553 	 ± 0.24976547542617217
	data : 0.11627225875854492
	model : 0.0699371337890625
			 train-loss:  2.0791410075990777 	 ± 0.24911957253116832
	data : 0.11626706123352051
	model : 0.06905288696289062
			 train-loss:  2.079964233942681 	 ± 0.24872555220405893
	data : 0.11713643074035644
	model : 0.06903700828552246
			 train-loss:  2.0790754668414593 	 ± 0.2483808811689935
	data : 0.11719708442687989
	model : 0.06907806396484376
			 train-loss:  2.08172486977256 	 ± 0.25044185243237765
	data : 0.1171806812286377
	model : 0.06844415664672851
			 train-loss:  2.082933607789659 	 ± 0.2503593395215324
	data : 0.1176076889038086
	model : 0.06872501373291015
			 train-loss:  2.0818713567195792 	 ± 0.25015449071071927
	data : 0.11733107566833496
	model : 0.06965670585632325
			 train-loss:  2.083824709970124 	 ± 0.2510020662876426
	data : 0.11663293838500977
	model : 0.06958026885986328
			 train-loss:  2.083280141583554 	 ± 0.2504802481463256
	data : 0.11666903495788575
	model : 0.0689213752746582
			 train-loss:  2.0812839173307323 	 ± 0.25141303019694966
	data : 0.11755871772766113
	model : 0.06972317695617676
			 train-loss:  2.0819408809719375 	 ± 0.250950867867015
	data : 0.11695480346679688
	model : 0.06895222663879394
			 train-loss:  2.0813438087701797 	 ± 0.2504643666081069
	data : 0.11765422821044921
	model : 0.06800570487976074
			 train-loss:  2.0835696399508423 	 ± 0.2518157309265974
	data : 0.11830086708068847
	model : 0.06796245574951172
			 train-loss:  2.08277560755758 	 ± 0.25144377835331333
	data : 0.11812200546264648
	model : 0.06838603019714355
			 train-loss:  2.085430636781777 	 ± 0.2536463284241475
	data : 0.11767902374267578
	model : 0.06844882965087891
			 train-loss:  2.0834208855441974 	 ± 0.25463900413528123
	data : 0.11761069297790527
	model : 0.067936372756958
			 train-loss:  2.0840572380438083 	 ± 0.25417972664120836
	data : 0.11789255142211914
	model : 0.06888465881347657
			 train-loss:  2.08505586628775 	 ± 0.2539648471126591
	data : 0.11717786788940429
	model : 0.06809992790222168
			 train-loss:  2.0830055014522757 	 ± 0.2550540756423723
	data : 0.11783385276794434
	model : 0.06839451789855958
			 train-loss:  2.0825040271648994 	 ± 0.25454250005354606
	data : 0.11736869812011719
	model : 0.06832389831542969
			 train-loss:  2.0815541379189377 	 ± 0.2543020871918183
	data : 0.11761994361877441
	model : 0.06906247138977051
			 train-loss:  2.081800205366952 	 ± 0.25372082312087363
	data : 0.11698760986328124
	model : 0.06890015602111817
			 train-loss:  2.082746636811026 	 ± 0.2534901746508458
	data : 0.11697373390197754
	model : 0.06989512443542481
			 train-loss:  2.0831944548858785 	 ± 0.2529752606040049
	data : 0.11628828048706055
	model : 0.06919713020324707
			 train-loss:  2.0857718181162372 	 ± 0.2551554494169797
	data : 0.11675539016723632
	model : 0.06923050880432129
			 train-loss:  2.08428425710892 	 ± 0.25548270409436
	data : 0.11672191619873047
	model : 0.06929616928100586
			 train-loss:  2.0822856564854466 	 ± 0.25655920696967865
	data : 0.11681442260742188
	model : 0.06945533752441406
			 train-loss:  2.0829718559980392 	 ± 0.2561623104574737
	data : 0.11678895950317383
	model : 0.06922259330749511
			 train-loss:  2.0828514335342265 	 ± 0.25557752118891297
	data : 0.11687726974487304
	model : 0.06900763511657715
			 train-loss:  2.0836657936419916 	 ± 0.2552726924026312
	data : 0.11729035377502442
	model : 0.06801371574401856
			 train-loss:  2.0823326502760797 	 ± 0.25544870215103255
	data : 0.11805419921875
	model : 0.06802973747253419
			 train-loss:  2.0838272539052096 	 ± 0.2558254114084552
	data : 0.11817340850830078
	model : 0.06784706115722657
			 train-loss:  2.0830263908092794 	 ± 0.25552222300290484
	data : 0.11805553436279297
	model : 0.06718649864196777
			 train-loss:  2.081638964446815 	 ± 0.2557790344914566
	data : 0.11868271827697754
	model : 0.06724381446838379
			 train-loss:  2.0799589702366714 	 ± 0.25642953548388114
	data : 0.11854033470153809
	model : 0.06738228797912597
			 train-loss:  2.0812080215130533 	 ± 0.25653549736469594
	data : 0.11839957237243652
	model : 0.0673309326171875
			 train-loss:  2.0805910963482326 	 ± 0.25613126345718457
	data : 0.11821370124816895
	model : 0.06649913787841796
			 train-loss:  2.0798246343578914 	 ± 0.25582244577738505
	data : 0.11912946701049805
	model : 0.06641440391540528
			 train-loss:  2.079590847838818 	 ± 0.25528253277574786
	data : 0.11894474029541016
	model : 0.06675896644592286
			 train-loss:  2.079995496231213 	 ± 0.25479503702708856
	data : 0.1184957504272461
	model : 0.06699891090393066
			 train-loss:  2.080986779850123 	 ± 0.2546783419638747
	data : 0.11807622909545898
	model : 0.06658272743225098
			 train-loss:  2.080832216014033 	 ± 0.2541348541057325
	data : 0.11821742057800293
	model : 0.06725544929504394
			 train-loss:  2.0823800367710392 	 ± 0.2546683326978232
	data : 0.11753807067871094
	model : 0.0679636001586914
			 train-loss:  2.082424817414119 	 ± 0.2541197972501022
	data : 0.1170119285583496
	model : 0.06833481788635254
			 train-loss:  2.083380950878618 	 ± 0.25399175136010016
	data : 0.11700687408447266
	model : 0.06881961822509766
			 train-loss:  2.083267649014791 	 ± 0.25345435359853513
	data : 0.11671795845031738
	model : 0.0694272518157959
			 train-loss:  2.0849677775768525 	 ± 0.2542481341794529
	data : 0.11630077362060547
	model : 0.06950836181640625
			 train-loss:  2.084535207788823 	 ± 0.25379554542624594
	data : 0.1163210391998291
	model : 0.0695030689239502
			 train-loss:  2.0863280648420632 	 ± 0.25475278394215695
	data : 0.11636734008789062
	model : 0.06904640197753906
			 train-loss:  2.085667763938423 	 ± 0.25442017901224917
	data : 0.11672296524047851
	model : 0.06858663558959961
			 train-loss:  2.0863576479037937 	 ± 0.2541103420169218
	data : 0.11725068092346191
	model : 0.0683211326599121
			 train-loss:  2.0861681883533794 	 ± 0.2535973078158906
	data : 0.11743283271789551
	model : 0.0682192325592041
			 train-loss:  2.0857041075021896 	 ± 0.2531727285686026
	data : 0.11755623817443847
	model : 0.06802530288696289
			 train-loss:  2.0873210247883125 	 ± 0.25389297901241925
	data : 0.11780047416687012
	model : 0.06835803985595704
			 train-loss:  2.087671868104503 	 ± 0.2534288037364084
	data : 0.11716132164001465
	model : 0.06836624145507812
			 train-loss:  2.087736715547374 	 ± 0.2529109694404692
	data : 0.11702561378479004
	model : 0.06836342811584473
			 train-loss:  2.0879622279381267 	 ± 0.25241887787247697
	data : 0.1172558307647705
	model : 0.06826047897338867
			 train-loss:  2.088682781874649 	 ± 0.2521576652946739
	data : 0.11714286804199218
	model : 0.06814484596252442
			 train-loss:  2.087756145338298 	 ± 0.2520660515309198
	data : 0.11702547073364258
	model : 0.06813268661499024
			 train-loss:  2.087522207729278 	 ± 0.2515842067533745
	data : 0.11711750030517579
	model : 0.06813058853149415
			 train-loss:  2.0886857184061562 	 ± 0.25174620079368165
	data : 0.11721186637878418
	model : 0.06815967559814454
			 train-loss:  2.089309133529663 	 ± 0.25143471910862764
	data : 0.11717944145202637
	model : 0.06840872764587402
			 train-loss:  2.089088436141907 	 ± 0.2509576151790311
	data : 0.11716341972351074
	model : 0.06872797012329102
			 train-loss:  2.0895704674342324 	 ± 0.2505755893339977
	data : 0.11705899238586426
	model : 0.06851534843444824
			 train-loss:  2.088752664596196 	 ± 0.25041663190085883
	data : 0.11729354858398437
	model : 0.0690314769744873
			 train-loss:  2.0885922458228166 	 ± 0.24993622479299507
	data : 0.11704430580139161
	model : 0.06916575431823731
			 train-loss:  2.088382780785654 	 ± 0.2494680096944091
	data : 0.11669445037841797
	model : 0.06918015480041503
			 train-loss:  2.0873927413485944 	 ± 0.24948172395207874
	data : 0.1156723976135254
	model : 0.060155487060546874
#epoch  34    val-loss:  2.4700371779893575  train-loss:  2.0873927413485944  lr:  0.00125
			 train-loss:  2.3738434314727783 	 ± 0.0
	data : 5.960367679595947
	model : 0.07259654998779297
			 train-loss:  2.196171522140503 	 ± 0.1776719093322754
	data : 3.048022747039795
	model : 0.0698014497756958
			 train-loss:  2.1041101217269897 	 ± 0.19492427880943564
	data : 2.0714844862620034
	model : 0.06841460863749187
			 train-loss:  2.0298961102962494 	 ± 0.21217861421443993
	data : 1.5835072994232178
	model : 0.06805133819580078
			 train-loss:  1.9839287281036377 	 ± 0.21087392528839274
	data : 1.2905128479003907
	model : 0.06759543418884277
			 train-loss:  2.0295081535975137 	 ± 0.21781627681880106
	data : 0.12243113517761231
	model : 0.06701140403747559
			 train-loss:  1.9954418795449393 	 ± 0.21824136240537584
	data : 0.11861553192138671
	model : 0.06757168769836426
			 train-loss:  1.9806566834449768 	 ± 0.20786015707077124
	data : 0.11808686256408692
	model : 0.06852841377258301
			 train-loss:  1.974060787094964 	 ± 0.19685843201070316
	data : 0.1171194076538086
	model : 0.06919546127319336
			 train-loss:  1.9974506974220276 	 ± 0.19950365704496267
	data : 0.11643767356872559
	model : 0.07017693519592286
			 train-loss:  1.9839251258156516 	 ± 0.1949686714563168
	data : 0.11541538238525391
	model : 0.07033405303955079
			 train-loss:  1.9880995750427246 	 ± 0.18718102797870464
	data : 0.11525001525878906
	model : 0.07052631378173828
			 train-loss:  1.9867318685238178 	 ± 0.17990011410544668
	data : 0.11525359153747558
	model : 0.07032527923583984
			 train-loss:  1.9860511762755257 	 ± 0.1733734598138404
	data : 0.11548590660095215
	model : 0.07022156715393066
			 train-loss:  2.0145801464716593 	 ± 0.19861796519019334
	data : 0.11566724777221679
	model : 0.0695927619934082
			 train-loss:  1.999221809208393 	 ± 0.20130003835731886
	data : 0.11635756492614746
	model : 0.06929373741149902
			 train-loss:  2.0129387729308186 	 ± 0.2028510715055119
	data : 0.11668715476989747
	model : 0.0691460132598877
			 train-loss:  2.0214616788758173 	 ± 0.20024336441559132
	data : 0.1168212890625
	model : 0.06925520896911622
			 train-loss:  2.037326455116272 	 ± 0.20619761276237852
	data : 0.11681842803955078
	model : 0.06939959526062012
			 train-loss:  2.0689692199230194 	 ± 0.243753173232978
	data : 0.1165999412536621
	model : 0.07023072242736816
			 train-loss:  2.0754473266147433 	 ± 0.23963641637090186
	data : 0.11572933197021484
	model : 0.07037019729614258
			 train-loss:  2.0731251293962654 	 ± 0.23436851656357047
	data : 0.11551713943481445
	model : 0.07067198753356933
			 train-loss:  2.0594010508578755 	 ± 0.23808424724571936
	data : 0.11523118019104003
	model : 0.07079405784606933
			 train-loss:  2.0662751346826553 	 ± 0.23539135620142546
	data : 0.11522636413574219
	model : 0.07066531181335449
			 train-loss:  2.068617196083069 	 ± 0.2309207071482587
	data : 0.11544389724731445
	model : 0.06971750259399415
			 train-loss:  2.067624967831832 	 ± 0.22649072495298586
	data : 0.11636672019958497
	model : 0.06975016593933106
			 train-loss:  2.0836311843660145 	 ± 0.2367684394700968
	data : 0.11634607315063476
	model : 0.06939363479614258
			 train-loss:  2.0925316512584686 	 ± 0.23705710426641713
	data : 0.1166961669921875
	model : 0.0692784309387207
			 train-loss:  2.092793633197916 	 ± 0.2329381825923015
	data : 0.11667389869689941
	model : 0.0692708969116211
			 train-loss:  2.1069797396659853 	 ± 0.24142834664568558
	data : 0.11671929359436035
	model : 0.06978273391723633
			 train-loss:  2.1045687390911962 	 ± 0.23786926580342654
	data : 0.11614151000976562
	model : 0.0697354793548584
			 train-loss:  2.0970818921923637 	 ± 0.2378050586552174
	data : 0.11622257232666015
	model : 0.06907978057861328
			 train-loss:  2.0906570969205913 	 ± 0.23697777874783232
	data : 0.1169581413269043
	model : 0.06898174285888672
			 train-loss:  2.1013880617478313 	 ± 0.2414680454769139
	data : 0.11714067459106445
	model : 0.06896986961364746
			 train-loss:  2.099577324731009 	 ± 0.23822759298659035
	data : 0.11712808609008789
	model : 0.06915011405944824
			 train-loss:  2.1053456200493708 	 ± 0.2373615258896018
	data : 0.1170919418334961
	model : 0.06929998397827149
			 train-loss:  2.099115055960578 	 ± 0.2370976494156985
	data : 0.1169062614440918
	model : 0.06997642517089844
			 train-loss:  2.091838290816859 	 ± 0.23810742110652197
	data : 0.1160921573638916
	model : 0.06916403770446777
			 train-loss:  2.0944536404731946 	 ± 0.23558723308082277
	data : 0.11669292449951171
	model : 0.06906323432922364
			 train-loss:  2.0939052045345306 	 ± 0.23264896577910005
	data : 0.11670928001403809
	model : 0.06871204376220703
			 train-loss:  2.094974762055932 	 ± 0.2298938108271389
	data : 0.1170590877532959
	model : 0.06846766471862793
			 train-loss:  2.101168910662333 	 ± 0.2305772533407851
	data : 0.11738595962524415
	model : 0.06833233833312988
			 train-loss:  2.1019961667615314 	 ± 0.22794340743672034
	data : 0.11753277778625489
	model : 0.06835541725158692
			 train-loss:  2.111915474588221 	 ± 0.23453829877884025
	data : 0.11760387420654297
	model : 0.06833319664001465
			 train-loss:  2.1170758406321206 	 ± 0.2344301633038018
	data : 0.11780371665954589
	model : 0.06857891082763672
			 train-loss:  2.110185820123424 	 ± 0.23642975783804362
	data : 0.11767182350158692
	model : 0.06802859306335449
			 train-loss:  2.1113444744272436 	 ± 0.23403299636975214
	data : 0.11829586029052734
	model : 0.06844143867492676
			 train-loss:  2.1089045157035193 	 ± 0.2321856614553756
	data : 0.11772422790527344
	model : 0.06934771537780762
			 train-loss:  2.1126044331764686 	 ± 0.23122946241517747
	data : 0.11686673164367675
	model : 0.06932792663574219
			 train-loss:  2.1067755651474 	 ± 0.2325135102000538
	data : 0.11680097579956054
	model : 0.06952629089355469
			 train-loss:  2.106647500804826 	 ± 0.23022446176273173
	data : 0.1167628288269043
	model : 0.07031855583190919
			 train-loss:  2.098341011084043 	 ± 0.2355905236952371
	data : 0.11584696769714356
	model : 0.06993894577026367
			 train-loss:  2.111149931853672 	 ± 0.2509725680038894
	data : 0.11634936332702636
	model : 0.0701411247253418
			 train-loss:  2.113654538437172 	 ± 0.2493055787114079
	data : 0.11635165214538574
	model : 0.06990637779235839
			 train-loss:  2.117224606600675 	 ± 0.24841791999529636
	data : 0.1164928913116455
	model : 0.06975154876708985
			 train-loss:  2.117987777505602 	 ± 0.2462549617879453
	data : 0.11637659072875976
	model : 0.06893095970153809
			 train-loss:  2.1133667481573006 	 ± 0.24652269513521013
	data : 0.11709260940551758
	model : 0.0687035083770752
			 train-loss:  2.113562886057229 	 ± 0.24439274558283822
	data : 0.11713194847106934
	model : 0.06811079978942872
			 train-loss:  2.114397370209128 	 ± 0.24239609601190176
	data : 0.11740698814392089
	model : 0.06847133636474609
			 train-loss:  2.1126911540826163 	 ± 0.24072466043289525
	data : 0.117073392868042
	model : 0.06842150688171386
			 train-loss:  2.112807502512072 	 ± 0.23874505483488057
	data : 0.11708579063415528
	model : 0.0687802791595459
			 train-loss:  2.1188281255383647 	 ± 0.24143525271229302
	data : 0.11686010360717773
	model : 0.068934965133667
			 train-loss:  2.1155836355118525 	 ± 0.2408700561393438
	data : 0.11669554710388183
	model : 0.06918787956237793
			 train-loss:  2.1207096874713898 	 ± 0.24241959583132908
	data : 0.11648731231689453
	model : 0.06907830238342286
			 train-loss:  2.121888417464036 	 ± 0.24073236306159085
	data : 0.11652584075927734
	model : 0.06892185211181641
			 train-loss:  2.121315190286347 	 ± 0.23894636900337826
	data : 0.11679129600524903
	model : 0.0693404197692871
			 train-loss:  2.1202304968193397 	 ± 0.2373201435905715
	data : 0.11645197868347168
	model : 0.06958770751953125
			 train-loss:  2.115801180110258 	 ± 0.23834232831730343
	data : 0.11637787818908692
	model : 0.06977806091308594
			 train-loss:  2.1221202732860176 	 ± 0.24227892703696027
	data : 0.11646699905395508
	model : 0.07000837326049805
			 train-loss:  2.120156354563577 	 ± 0.24109469429609778
	data : 0.11631488800048828
	model : 0.0702981948852539
			 train-loss:  2.119467864573841 	 ± 0.2394601170890341
	data : 0.11612477302551269
	model : 0.07035984992980956
			 train-loss:  2.1222129348251553 	 ± 0.2389137020056963
	data : 0.11611752510070801
	model : 0.07062397003173829
			 train-loss:  2.120489324608894 	 ± 0.23772198561201427
	data : 0.11583833694458008
	model : 0.07044439315795899
			 train-loss:  2.121989313009623 	 ± 0.23645785609259873
	data : 0.11580743789672851
	model : 0.07016310691833497
			 train-loss:  2.12029083887736 	 ± 0.2353301859142572
	data : 0.1160811424255371
	model : 0.06910271644592285
			 train-loss:  2.124909410351201 	 ± 0.23717387814914154
	data : 0.11685233116149903
	model : 0.0688873291015625
			 train-loss:  2.122650131002649 	 ± 0.23645050278163526
	data : 0.11692004203796387
	model : 0.06860594749450684
			 train-loss:  2.120248264227158 	 ± 0.2358734204840318
	data : 0.11727490425109863
	model : 0.0687025547027588
			 train-loss:  2.119285503520241 	 ± 0.23452998260412314
	data : 0.11724085807800293
	model : 0.06892967224121094
			 train-loss:  2.1242354795336724 	 ± 0.2371759633082294
	data : 0.11706032752990722
	model : 0.06991662979125976
			 train-loss:  2.1231781479753096 	 ± 0.23589700938166963
	data : 0.1162827491760254
	model : 0.07005295753479004
			 train-loss:  2.1217895179260067 	 ± 0.23478706093869994
	data : 0.11617259979248047
	model : 0.06997919082641602
			 train-loss:  2.1204457900610314 	 ± 0.23368540255396955
	data : 0.11606683731079101
	model : 0.0699695110321045
			 train-loss:  2.12042397970245 	 ± 0.23229033833832766
	data : 0.11599340438842773
	model : 0.06936154365539551
			 train-loss:  2.123824979277218 	 ± 0.23301416888138918
	data : 0.11650223731994629
	model : 0.06935443878173828
			 train-loss:  2.120882579060488 	 ± 0.23323842907577058
	data : 0.11641144752502441
	model : 0.06942167282104492
			 train-loss:  2.1214217851901878 	 ± 0.23194801026676407
	data : 0.11630678176879883
	model : 0.06954898834228515
			 train-loss:  2.1253957707773554 	 ± 0.23358610841814692
	data : 0.11634206771850586
	model : 0.06934666633605957
			 train-loss:  2.1273099398345092 	 ± 0.23296318246565925
	data : 0.11643505096435547
	model : 0.06917839050292969
			 train-loss:  2.1227895418802896 	 ± 0.2355577417184755
	data : 0.11669049263000489
	model : 0.06839914321899414
			 train-loss:  2.120033376819485 	 ± 0.23571461052510062
	data : 0.11740946769714355
	model : 0.06760997772216797
			 train-loss:  2.1163671016693115 	 ± 0.23702454046750593
	data : 0.11821064949035645
	model : 0.0674774169921875
			 train-loss:  2.114524237571224 	 ± 0.23640851419085895
	data : 0.1184351921081543
	model : 0.06681284904479981
			 train-loss:  2.116305842044506 	 ± 0.2357744993716522
	data : 0.11909704208374024
	model : 0.06694173812866211
			 train-loss:  2.113861742772554 	 ± 0.23572437530758608
	data : 0.11873822212219239
	model : 0.06769905090332032
			 train-loss:  2.117203132559856 	 ± 0.23674423531185348
	data : 0.11801543235778808
	model : 0.0684330940246582
			 train-loss:  2.1165496897451654 	 ± 0.23560774826752462
	data : 0.11713275909423829
	model : 0.06788301467895508
			 train-loss:  2.1163970633428923 	 ± 0.2344074054992789
	data : 0.11742181777954101
	model : 0.06782622337341308
			 train-loss:  2.118640351777125 	 ± 0.2342754422987322
	data : 0.11745409965515137
	model : 0.0682518482208252
			 train-loss:  2.1179459488391874 	 ± 0.23320349565177045
	data : 0.11724071502685547
	model : 0.06822609901428223
			 train-loss:  2.1164283740638505 	 ± 0.23254186621307438
	data : 0.11729884147644043
	model : 0.06784224510192871
			 train-loss:  2.117136893319149 	 ± 0.2315086766878335
	data : 0.11755051612854003
	model : 0.06824221611022949
			 train-loss:  2.112556187851915 	 ± 0.23498120617223583
	data : 0.11727118492126465
	model : 0.06917757987976074
			 train-loss:  2.1102246458713827 	 ± 0.2350428889240051
	data : 0.11646480560302734
	model : 0.06862049102783203
			 train-loss:  2.1051054125740416 	 ± 0.23967581523747641
	data : 0.11716127395629883
	model : 0.06781063079833985
			 train-loss:  2.106551616821649 	 ± 0.23900245899267436
	data : 0.1179854393005371
	model : 0.06808347702026367
			 train-loss:  2.1021130876006366 	 ± 0.24223249309787712
	data : 0.11796393394470214
	model : 0.06809005737304688
			 train-loss:  2.103512270583047 	 ± 0.24154244830278684
	data : 0.1179347038269043
	model : 0.06807112693786621
			 train-loss:  2.1069122968463723 	 ± 0.2430144007935849
	data : 0.11807518005371094
	model : 0.0688819408416748
			 train-loss:  2.104692047292536 	 ± 0.2430153136240784
	data : 0.11731452941894531
	model : 0.06952490806579589
			 train-loss:  2.10401329049119 	 ± 0.24202289317205933
	data : 0.11649737358093262
	model : 0.06956186294555664
			 train-loss:  2.106874508517129 	 ± 0.24281844686936638
	data : 0.11641921997070312
	model : 0.06981282234191895
			 train-loss:  2.106200448179667 	 ± 0.24184687136135752
	data : 0.11610245704650879
	model : 0.06996393203735352
			 train-loss:  2.1081198328419735 	 ± 0.24164671742834767
	data : 0.1158635139465332
	model : 0.0702451229095459
			 train-loss:  2.1052211688912434 	 ± 0.24257622812474844
	data : 0.11550450325012207
	model : 0.06959948539733887
			 train-loss:  2.1046798609454056 	 ± 0.2415981247884408
	data : 0.11630592346191407
	model : 0.06946396827697754
			 train-loss:  2.1069621866584844 	 ± 0.24181607309510678
	data : 0.11631627082824707
	model : 0.06849217414855957
			 train-loss:  2.1044060991982283 	 ± 0.24237139161002372
	data : 0.11723875999450684
	model : 0.06842021942138672
			 train-loss:  2.103760395731245 	 ± 0.24145277677902757
	data : 0.11731033325195313
	model : 0.06809778213500976
			 train-loss:  2.103885305921237 	 ± 0.24044847976915099
	data : 0.11785883903503418
	model : 0.0689619541168213
			 train-loss:  2.106957696686106 	 ± 0.241806550389618
	data : 0.11701350212097168
	model : 0.06912569999694824
			 train-loss:  2.102712392807007 	 ± 0.24529956661531757
	data : 0.11700358390808105
	model : 0.0692133903503418
			 train-loss:  2.105640035334641 	 ± 0.24643122407767262
	data : 0.11709804534912109
	model : 0.06826128959655761
			 train-loss:  2.1102709039565055 	 ± 0.2507515335797633
	data : 0.11801576614379883
	model : 0.06815013885498047
			 train-loss:  2.109392000198364 	 ± 0.24993820725342422
	data : 0.11787281036376954
	model : 0.06806674003601074
			 train-loss:  2.10524198554811 	 ± 0.2532314130942225
	data : 0.1180272102355957
	model : 0.06807498931884766
			 train-loss:  2.103781870969637 	 ± 0.2527644002451717
	data : 0.11803603172302246
	model : 0.06899681091308593
			 train-loss:  2.1003226013854146 	 ± 0.2547753009802665
	data : 0.11717333793640136
	model : 0.06981010437011718
			 train-loss:  2.100744880447092 	 ± 0.25383084344750506
	data : 0.11638832092285156
	model : 0.07074561119079589
			 train-loss:  2.1005099415779114 	 ± 0.25286676585302215
	data : 0.11568851470947265
	model : 0.07054781913757324
			 train-loss:  2.0989657722356667 	 ± 0.25251431070522457
	data : 0.1158064365386963
	model : 0.06979541778564453
			 train-loss:  2.1000851374683958 	 ± 0.25188203717778435
	data : 0.11643567085266113
	model : 0.06960458755493164
			 train-loss:  2.099510838214616 	 ± 0.25102005910394226
	data : 0.11638445854187011
	model : 0.06901350021362304
			 train-loss:  2.0991381353406764 	 ± 0.25011859765883454
	data : 0.11697020530700683
	model : 0.06725068092346191
			 train-loss:  2.0991244987205224 	 ± 0.2491905606064872
	data : 0.1183511734008789
	model : 0.06669964790344238
			 train-loss:  2.0977935142376842 	 ± 0.24875390025688024
	data : 0.11890659332275391
	model : 0.06760196685791016
			 train-loss:  2.096996939965408 	 ± 0.2480184088860577
	data : 0.11799249649047852
	model : 0.06778030395507813
			 train-loss:  2.0975204403849617 	 ± 0.24719411206105701
	data : 0.11792182922363281
	model : 0.0682981014251709
			 train-loss:  2.100233324140096 	 ± 0.2483565362794634
	data : 0.11740579605102539
	model : 0.06895346641540527
			 train-loss:  2.097327267272132 	 ± 0.2498284789774479
	data : 0.11667160987854004
	model : 0.06926898956298828
			 train-loss:  2.0967760415787393 	 ± 0.24902641122152733
	data : 0.11634163856506348
	model : 0.06917433738708496
			 train-loss:  2.094362221133541 	 ± 0.24979786769326365
	data : 0.11675100326538086
	model : 0.06908946037292481
			 train-loss:  2.09601581596828 	 ± 0.24970162116462224
	data : 0.11678524017333984
	model : 0.0692519187927246
			 train-loss:  2.098697598609659 	 ± 0.2508911263221828
	data : 0.11656265258789063
	model : 0.06863903999328613
			 train-loss:  2.098694738026323 	 ± 0.25002449007410654
	data : 0.11722264289855958
	model : 0.06907501220703124
			 train-loss:  2.099760551158696 	 ± 0.24949708100019238
	data : 0.11686363220214843
	model : 0.06914296150207519
			 train-loss:  2.1001348860409794 	 ± 0.24868813950816915
	data : 0.11671390533447265
	model : 0.06954092979431152
			 train-loss:  2.1011918968445547 	 ± 0.2481776636362417
	data : 0.11638603210449219
	model : 0.06946296691894531
			 train-loss:  2.1022763772298827 	 ± 0.24769506461760357
	data : 0.11661787033081054
	model : 0.06982889175415039
			 train-loss:  2.1016818849245706 	 ± 0.24697466633862034
	data : 0.11645016670227051
	model : 0.06966581344604492
			 train-loss:  2.1024500336868086 	 ± 0.2463352257677316
	data : 0.1165041446685791
	model : 0.06955599784851074
			 train-loss:  2.101805799101528 	 ± 0.24565116900032413
	data : 0.11655225753784179
	model : 0.06910452842712403
			 train-loss:  2.104219453007567 	 ± 0.2466487352132207
	data : 0.1168426513671875
	model : 0.06984934806823731
			 train-loss:  2.1017188600131442 	 ± 0.2477847168862232
	data : 0.11585264205932617
	model : 0.07118463516235352
			 train-loss:  2.1009293163976364 	 ± 0.24717838682607293
	data : 0.11442136764526367
	model : 0.07064762115478515
			 train-loss:  2.099455775358738 	 ± 0.24706691721063823
	data : 0.11498627662658692
	model : 0.06973848342895508
			 train-loss:  2.099725492440971 	 ± 0.2463018619894346
	data : 0.11577963829040527
	model : 0.06945295333862304
			 train-loss:  2.101686954498291 	 ± 0.2467482225941554
	data : 0.11630678176879883
	model : 0.06873965263366699
			 train-loss:  2.0998260367591426 	 ± 0.2470807947765653
	data : 0.11702556610107422
	model : 0.06807479858398438
			 train-loss:  2.0995752163231374 	 ± 0.246327761802107
	data : 0.11789274215698242
	model : 0.06884384155273438
			 train-loss:  2.100703195755526 	 ± 0.24597573498065364
	data : 0.11718559265136719
	model : 0.0697404384613037
			 train-loss:  2.099424256954664 	 ± 0.2457517559068858
	data : 0.11650280952453614
	model : 0.06977458000183105
			 train-loss:  2.101041917420604 	 ± 0.24586040015182142
	data : 0.11653685569763184
	model : 0.06947102546691894
			 train-loss:  2.101293500603699 	 ± 0.2451307242707551
	data : 0.11704096794128419
	model : 0.06875491142272949
			 train-loss:  2.1019171418565694 	 ± 0.244517238752796
	data : 0.11760168075561524
	model : 0.06872682571411133
			 train-loss:  2.1018430762980356 	 ± 0.2437814849769587
	data : 0.11761622428894043
	model : 0.06817283630371093
			 train-loss:  2.1012951535379103 	 ± 0.2431530055023633
	data : 0.11804041862487794
	model : 0.06856632232666016
			 train-loss:  2.1022303366944906 	 ± 0.24272929769217422
	data : 0.11771440505981445
	model : 0.0690892219543457
			 train-loss:  2.1034041699573134 	 ± 0.24248788057938012
	data : 0.11712770462036133
	model : 0.0692986011505127
			 train-loss:  2.101324792469249 	 ± 0.2432801095446616
	data : 0.11707673072814942
	model : 0.06854267120361328
			 train-loss:  2.104391414519639 	 ± 0.24584102258762436
	data : 0.11765975952148437
	model : 0.06907238960266113
			 train-loss:  2.103048230326453 	 ± 0.2457538084463091
	data : 0.11709685325622558
	model : 0.06937389373779297
			 train-loss:  2.102245621598525 	 ± 0.24526848511677082
	data : 0.11684970855712891
	model : 0.0693129539489746
			 train-loss:  2.104018460745099 	 ± 0.24567180114679132
	data : 0.11695942878723145
	model : 0.06958398818969727
			 train-loss:  2.1064138984680176 	 ± 0.246998347088451
	data : 0.11659784317016601
	model : 0.0693741798400879
			 train-loss:  2.104942411861636 	 ± 0.24706369382026144
	data : 0.11688017845153809
	model : 0.06922492980957032
			 train-loss:  2.1073938688989413 	 ± 0.24850212136126976
	data : 0.11693544387817383
	model : 0.06878204345703125
			 train-loss:  2.1081277152125755 	 ± 0.24799535384361615
	data : 0.11719784736633301
	model : 0.0685079574584961
			 train-loss:  2.1095148111854853 	 ± 0.24799312270540377
	data : 0.11751389503479004
	model : 0.06762828826904296
			 train-loss:  2.1088192601998648 	 ± 0.24747831741562148
	data : 0.11804361343383789
	model : 0.0685317039489746
			 train-loss:  2.1076610852341626 	 ± 0.2472824114867765
	data : 0.11721315383911132
	model : 0.06851391792297364
			 train-loss:  2.1090212090984806 	 ± 0.24728010098868433
	data : 0.11737661361694336
	model : 0.06891846656799316
			 train-loss:  2.109046697616577 	 ± 0.2466037865117413
	data : 0.11707034111022949
	model : 0.06823878288269043
			 train-loss:  2.1083608107722323 	 ± 0.2461077211778026
	data : 0.11774997711181641
	model : 0.06922826766967774
			 train-loss:  2.1079566652710375 	 ± 0.24550287930457707
	data : 0.1170572280883789
	model : 0.06938271522521973
			 train-loss:  2.109356782128734 	 ± 0.24558152023047444
	data : 0.11703815460205078
	model : 0.06931190490722657
			 train-loss:  2.1103788333780624 	 ± 0.24532032468713297
	data : 0.1169783592224121
	model : 0.06889047622680664
			 train-loss:  2.1094749030914715 	 ± 0.2449790606933416
	data : 0.11721949577331543
	model : 0.06959357261657714
			 train-loss:  2.1126979425470664 	 ± 0.24829446779348047
	data : 0.11664972305297852
	model : 0.06951913833618165
			 train-loss:  2.112915772513339 	 ± 0.2476583055974413
	data : 0.11663784980773925
	model : 0.06941184997558594
			 train-loss:  2.1130507972227965 	 ± 0.24701614643314437
	data : 0.1166377067565918
	model : 0.0696861743927002
			 train-loss:  2.112665887301167 	 ± 0.24642945760254875
	data : 0.11653385162353516
	model : 0.07036767005920411
			 train-loss:  2.1129555374847175 	 ± 0.24582297634547903
	data : 0.11583080291748046
	model : 0.07008347511291504
			 train-loss:  2.1123362152846816 	 ± 0.24533950643476513
	data : 0.11603188514709473
	model : 0.07009549140930176
			 train-loss:  2.1140211337651964 	 ± 0.24583237286915535
	data : 0.11612944602966309
	model : 0.07009544372558593
			 train-loss:  2.113097552742277 	 ± 0.24554339016767485
	data : 0.11607804298400878
	model : 0.07014703750610352
			 train-loss:  2.1156490557084835 	 ± 0.24751060735426442
	data : 0.1160085678100586
	model : 0.06954102516174317
			 train-loss:  2.117441813753109 	 ± 0.2481637636139028
	data : 0.11675472259521484
	model : 0.06902313232421875
			 train-loss:  2.1149202333622843 	 ± 0.25006946381580714
	data : 0.11709632873535156
	model : 0.06896920204162597
			 train-loss:  2.1141940325498583 	 ± 0.24965377867325247
	data : 0.11707606315612792
	model : 0.06892008781433105
			 train-loss:  2.111774664613145 	 ± 0.2513714240433027
	data : 0.11702556610107422
	model : 0.06883277893066406
			 train-loss:  2.1099093892786764 	 ± 0.25213907397869467
	data : 0.11716341972351074
	model : 0.06886529922485352
			 train-loss:  2.1098382167628245 	 ± 0.2515193091885494
	data : 0.11698737144470214
	model : 0.06910519599914551
			 train-loss:  2.1097961044779012 	 ± 0.250902800388275
	data : 0.11693592071533203
	model : 0.06967253684997558
			 train-loss:  2.1108247745327833 	 ± 0.2507209531143629
	data : 0.11663351058959961
	model : 0.0698019027709961
			 train-loss:  2.109490813560856 	 ± 0.2508398562773637
	data : 0.11651415824890136
	model : 0.06985740661621094
			 train-loss:  2.110285378884578 	 ± 0.25049296177592
	data : 0.1163907527923584
	model : 0.06920733451843261
			 train-loss:  2.1080196018402395 	 ± 0.2520074250883933
	data : 0.11698923110961915
	model : 0.07018427848815918
			 train-loss:  2.1091561511372836 	 ± 0.25193761195849496
	data : 0.11591386795043945
	model : 0.06970973014831543
			 train-loss:  2.1105753274190993 	 ± 0.2521730533735865
	data : 0.11629719734191894
	model : 0.06964297294616699
			 train-loss:  2.111765911228849 	 ± 0.2521657022300077
	data : 0.11655988693237304
	model : 0.06967978477478028
			 train-loss:  2.111067778659317 	 ± 0.25177458022024846
	data : 0.11631855964660645
	model : 0.06971526145935059
			 train-loss:  2.1095085731694394 	 ± 0.2522067195023649
	data : 0.11630635261535645
	model : 0.06938858032226562
			 train-loss:  2.1102334634165896 	 ± 0.2518390733700731
	data : 0.11671009063720703
	model : 0.06931171417236329
			 train-loss:  2.1092665589132973 	 ± 0.2516505475450039
	data : 0.11672863960266114
	model : 0.06955423355102539
			 train-loss:  2.1089647715842283 	 ± 0.251106340333498
	data : 0.11652121543884278
	model : 0.06950035095214843
			 train-loss:  2.1078454387902115 	 ± 0.25106662180771444
	data : 0.11655755043029785
	model : 0.0691288948059082
			 train-loss:  2.107671556669638 	 ± 0.25050321501419104
	data : 0.11700258255004883
	model : 0.06863918304443359
			 train-loss:  2.1071223439691273 	 ± 0.2500621500673245
	data : 0.11733899116516114
	model : 0.06863822937011718
			 train-loss:  2.105932677334005 	 ± 0.25011357197758805
	data : 0.1172360897064209
	model : 0.06823673248291015
			 train-loss:  2.1051244331161363 	 ± 0.24983485218528215
	data : 0.11767487525939942
	model : 0.06820321083068848
			 train-loss:  2.104521406126452 	 ± 0.24943267317896928
	data : 0.11790361404418945
	model : 0.06904382705688476
			 train-loss:  2.1042394825161304 	 ± 0.2489082255481345
	data : 0.11703152656555176
	model : 0.06871213912963867
			 train-loss:  2.103920391095536 	 ± 0.24839771390908322
	data : 0.11742730140686035
	model : 0.06884636878967285
			 train-loss:  2.104926110903422 	 ± 0.24830176303298065
	data : 0.1172417163848877
	model : 0.06896381378173828
			 train-loss:  2.1056531483093193 	 ± 0.24799171864339511
	data : 0.11694526672363281
	model : 0.06811304092407226
			 train-loss:  2.104485102687113 	 ± 0.24806714129273008
	data : 0.11765074729919434
	model : 0.06736092567443848
			 train-loss:  2.1045945595230973 	 ± 0.2475280302572039
	data : 0.11812968254089355
	model : 0.06767325401306153
			 train-loss:  2.1028784445800115 	 ± 0.24834259027036043
	data : 0.11762967109680175
	model : 0.06663722991943359
			 train-loss:  2.1020264516706053 	 ± 0.24813730751748442
	data : 0.11855382919311523
	model : 0.06646804809570313
			 train-loss:  2.100971160512982 	 ± 0.24811633251693072
	data : 0.1186415672302246
	model : 0.06640186309814453
			 train-loss:  2.099038295704743 	 ± 0.24931781018392085
	data : 0.1187018871307373
	model : 0.06658449172973632
			 train-loss:  2.100385916591202 	 ± 0.24962756878456022
	data : 0.11878705024719238
	model : 0.06657781600952148
			 train-loss:  2.099042321372236 	 ± 0.24993648611363495
	data : 0.11881165504455567
	model : 0.0670163631439209
			 train-loss:  2.098829534713258 	 ± 0.24942537932766398
	data : 0.11841330528259278
	model : 0.06634235382080078
			 train-loss:  2.097934706231295 	 ± 0.2492740943112595
	data : 0.11903667449951172
	model : 0.06661996841430665
			 train-loss:  2.0980857590582804 	 ± 0.24875846727197554
	data : 0.11883730888366699
	model : 0.06730303764343262
			 train-loss:  2.0973506239281985 	 ± 0.24849316326678075
	data : 0.11829066276550293
	model : 0.06777782440185547
			 train-loss:  2.097801209992445 	 ± 0.24807017033241377
	data : 0.11795272827148437
	model : 0.06780195236206055
			 train-loss:  2.096355830629667 	 ± 0.24855924392776735
	data : 0.11816191673278809
	model : 0.06786575317382812
			 train-loss:  2.0960477625186016 	 ± 0.24808893482037112
	data : 0.11816692352294922
	model : 0.06840214729309083
			 train-loss:  2.096208396036763 	 ± 0.24758838232297015
	data : 0.1175424575805664
	model : 0.06827154159545898
			 train-loss:  2.0950529585159363 	 ± 0.24773135113864345
	data : 0.11745166778564453
	model : 0.06784634590148926
			 train-loss:  2.0941469938051505 	 ± 0.24762623080602497
	data : 0.11770420074462891
	model : 0.0673762321472168
			 train-loss:  2.093175998999148 	 ± 0.24758537984510992
	data : 0.11778650283813477
	model : 0.06729931831359863
			 train-loss:  2.0932843229634974 	 ± 0.24708746262421308
	data : 0.11789178848266602
	model : 0.06658153533935547
			 train-loss:  2.0938270082357926 	 ± 0.24673363780973476
	data : 0.11867871284484863
	model : 0.06599879264831543
			 train-loss:  2.092889530524131 	 ± 0.24667609086891193
	data : 0.1191901683807373
	model : 0.06582660675048828
			 train-loss:  2.0917635241665513 	 ± 0.24681806403574283
	data : 0.11926732063293458
	model : 0.06644635200500489
			 train-loss:  2.091525040626526 	 ± 0.24635267774621725
	data : 0.11874499320983886
	model : 0.06632962226867675
			 train-loss:  2.0909347310959103 	 ± 0.24603854731828376
	data : 0.11869173049926758
	model : 0.06685256958007812
			 train-loss:  2.0903193084966567 	 ± 0.24574338980590266
	data : 0.11800165176391601
	model : 0.06727356910705566
			 train-loss:  2.090846229918861 	 ± 0.24539984832190473
	data : 0.11766014099121094
	model : 0.06754655838012695
			 train-loss:  2.0908865839477597 	 ± 0.24491714244954665
	data : 0.11759037971496582
	model : 0.06748437881469727
			 train-loss:  2.0898908526289697 	 ± 0.24495103558992573
	data : 0.1180656909942627
	model : 0.06839489936828613
			 train-loss:  2.089515799190849 	 ± 0.2445454977865503
	data : 0.11624927520751953
	model : 0.05939373970031738
#epoch  35    val-loss:  2.5004149989077917  train-loss:  2.089515799190849  lr:  0.000625
			 train-loss:  1.6762441396713257 	 ± 0.0
	data : 5.707967042922974
	model : 0.08446693420410156
			 train-loss:  1.8164092898368835 	 ± 0.14016515016555786
	data : 2.918933153152466
	model : 0.07834446430206299
			 train-loss:  1.9912313222885132 	 ± 0.2724389820522378
	data : 1.9852642218271892
	model : 0.0744935671488444
			 train-loss:  2.0070222318172455 	 ± 0.23751906802923947
	data : 1.51833975315094
	model : 0.07320505380630493
			 train-loss:  2.025270104408264 	 ± 0.21555552778431591
	data : 1.2379123210906982
	model : 0.07246546745300293
			 train-loss:  1.9649600783983867 	 ± 0.2385511492710519
	data : 0.11944150924682617
	model : 0.06967778205871582
			 train-loss:  1.9652400187083654 	 ± 0.22085651331236966
	data : 0.11664981842041015
	model : 0.06903514862060547
			 train-loss:  1.9960775822401047 	 ± 0.2221195327080742
	data : 0.11651678085327148
	model : 0.06956076622009277
			 train-loss:  2.021798703405592 	 ± 0.22169302394986617
	data : 0.11606087684631347
	model : 0.06959161758422852
			 train-loss:  2.0405468344688416 	 ± 0.21770725515360145
	data : 0.11602835655212403
	model : 0.06967425346374512
			 train-loss:  2.0891822576522827 	 ± 0.25834419819843557
	data : 0.11612300872802735
	model : 0.06950597763061524
			 train-loss:  2.08821106950442 	 ± 0.24736671081243708
	data : 0.11611099243164062
	model : 0.06972761154174804
			 train-loss:  2.1222411210720358 	 ± 0.26529206015117646
	data : 0.11580095291137696
	model : 0.06993021965026855
			 train-loss:  2.1617239117622375 	 ± 0.29260608842596697
	data : 0.11590695381164551
	model : 0.06964926719665528
			 train-loss:  2.149199191729228 	 ± 0.2865424843748114
	data : 0.11596636772155762
	model : 0.06953587532043456
			 train-loss:  2.1369370371103287 	 ± 0.281478844108532
	data : 0.11608743667602539
	model : 0.06959881782531738
			 train-loss:  2.1225881436291862 	 ± 0.2790411857957454
	data : 0.11604132652282714
	model : 0.06961579322814941
			 train-loss:  2.1317716572019787 	 ± 0.2738100330203382
	data : 0.11613197326660156
	model : 0.06964578628540039
			 train-loss:  2.1132271164341976 	 ± 0.2778781077577227
	data : 0.11607751846313477
	model : 0.0705474853515625
			 train-loss:  2.0901555180549622 	 ± 0.2889102023400074
	data : 0.1155557632446289
	model : 0.07068681716918945
			 train-loss:  2.094375178927467 	 ± 0.2825783015152436
	data : 0.11538724899291992
	model : 0.06974515914916993
			 train-loss:  2.079284445805983 	 ± 0.28461072224024436
	data : 0.11619219779968262
	model : 0.06888804435729981
			 train-loss:  2.0720400602921196 	 ± 0.28042105284236546
	data : 0.11702704429626465
	model : 0.06869063377380372
			 train-loss:  2.058676371971766 	 ± 0.2818989110734168
	data : 0.1172119140625
	model : 0.06791796684265136
			 train-loss:  2.052827072143555 	 ± 0.27768590102007445
	data : 0.11789817810058593
	model : 0.0678217887878418
			 train-loss:  2.0568154133283176 	 ± 0.2730226782033946
	data : 0.11795310974121094
	model : 0.06868462562561035
			 train-loss:  2.0639606140278004 	 ± 0.27038489616884553
	data : 0.11721453666687012
	model : 0.06871614456176758
			 train-loss:  2.072709151676723 	 ± 0.2693761105890489
	data : 0.11711630821228028
	model : 0.06874523162841797
			 train-loss:  2.084110810838897 	 ± 0.2714797210989182
	data : 0.11708183288574218
	model : 0.06878209114074707
			 train-loss:  2.0834670543670653 	 ± 0.2669392238915142
	data : 0.11704673767089843
	model : 0.0684779167175293
			 train-loss:  2.0829935996763167 	 ± 0.2626112634088041
	data : 0.11714997291564941
	model : 0.06834778785705567
			 train-loss:  2.0879427269101143 	 ± 0.25994006852767537
	data : 0.11720972061157227
	model : 0.06894440650939941
			 train-loss:  2.078379193941752 	 ± 0.2616257935570262
	data : 0.11654014587402343
	model : 0.06896762847900391
			 train-loss:  2.08097782906364 	 ± 0.25818157097161687
	data : 0.11647887229919433
	model : 0.06826801300048828
			 train-loss:  2.0746310472488405 	 ± 0.2571435259555703
	data : 0.11722030639648437
	model : 0.06789264678955079
			 train-loss:  2.0690147115124597 	 ± 0.25571480528341894
	data : 0.11784725189208985
	model : 0.06847701072692872
			 train-loss:  2.060413383148812 	 ± 0.25746095871832175
	data : 0.11760549545288086
	model : 0.06813931465148926
			 train-loss:  2.0599925110214636 	 ± 0.25406362776718777
	data : 0.11799507141113282
	model : 0.06838817596435547
			 train-loss:  2.0579788226347704 	 ± 0.2510922726859915
	data : 0.11797547340393066
	model : 0.06968216896057129
			 train-loss:  2.06862910091877 	 ± 0.2566999315175007
	data : 0.11676363945007324
	model : 0.07081365585327148
			 train-loss:  2.0738801868950447 	 ± 0.2557158959016995
	data : 0.11576976776123046
	model : 0.07093009948730469
			 train-loss:  2.077756498541151 	 ± 0.2538695676483734
	data : 0.11549549102783203
	model : 0.0715207576751709
			 train-loss:  2.0796786713045696 	 ± 0.25120928572129747
	data : 0.11477808952331543
	model : 0.07140536308288574
			 train-loss:  2.085919518362392 	 ± 0.2516875979546662
	data : 0.114691162109375
	model : 0.07094712257385254
			 train-loss:  2.0915087090598212 	 ± 0.2516216644389288
	data : 0.1150238037109375
	model : 0.07043237686157226
			 train-loss:  2.093396308629409 	 ± 0.24919353693881116
	data : 0.11529068946838379
	model : 0.06964273452758789
			 train-loss:  2.088560104370117 	 ± 0.24870079467042366
	data : 0.11601467132568359
	model : 0.06959047317504882
			 train-loss:  2.0896510928869247 	 ± 0.24621015858260945
	data : 0.11631288528442382
	model : 0.06907110214233399
			 train-loss:  2.0883423795505447 	 ± 0.24385348425780293
	data : 0.11672372817993164
	model : 0.06894955635070801
			 train-loss:  2.090672092437744 	 ± 0.24195285055911106
	data : 0.11692132949829101
	model : 0.06906261444091796
			 train-loss:  2.0916302811865712 	 ± 0.23966481148879096
	data : 0.1168302059173584
	model : 0.0685469627380371
			 train-loss:  2.0865221871779513 	 ± 0.24013609743206926
	data : 0.11723008155822753
	model : 0.06863393783569335
			 train-loss:  2.0930037790874265 	 ± 0.24240852637332383
	data : 0.11713910102844238
	model : 0.06848492622375488
			 train-loss:  2.089833433981295 	 ± 0.2412600632335635
	data : 0.11726145744323731
	model : 0.0678098201751709
			 train-loss:  2.086631412939592 	 ± 0.2402119445259271
	data : 0.11779613494873047
	model : 0.0677032470703125
			 train-loss:  2.0953982110534395 	 ± 0.2467762358881667
	data : 0.11812858581542969
	model : 0.06758441925048828
			 train-loss:  2.093597978876348 	 ± 0.2449726566093871
	data : 0.11812925338745117
	model : 0.06695494651794434
			 train-loss:  2.091280972135478 	 ± 0.24348085209785178
	data : 0.11860785484313965
	model : 0.06744823455810547
			 train-loss:  2.09510078874685 	 ± 0.24315511084976357
	data : 0.11823396682739258
	model : 0.06814436912536621
			 train-loss:  2.0901108543078104 	 ± 0.24414763654423727
	data : 0.1176069736480713
	model : 0.06832799911499024
			 train-loss:  2.0972570278605476 	 ± 0.248384691088668
	data : 0.11740946769714355
	model : 0.06903185844421386
			 train-loss:  2.0994420320756975 	 ± 0.24696377001545466
	data : 0.11685285568237305
	model : 0.06998729705810547
			 train-loss:  2.101362111076476 	 ± 0.24546194522164114
	data : 0.11597709655761719
	model : 0.06926703453063965
			 train-loss:  2.099938277155161 	 ± 0.24379880212144767
	data : 0.1167224407196045
	model : 0.06867814064025879
			 train-loss:  2.09573958837069 	 ± 0.2442369393926052
	data : 0.11721229553222656
	model : 0.06790904998779297
			 train-loss:  2.0961096033905493 	 ± 0.24239795464103486
	data : 0.1179110050201416
	model : 0.06779160499572753
			 train-loss:  2.0951060430327457 	 ± 0.2407203201336862
	data : 0.11813368797302246
	model : 0.06646814346313476
			 train-loss:  2.095067290698781 	 ± 0.23894397267042194
	data : 0.11928558349609375
	model : 0.06726703643798829
			 train-loss:  2.0915236110272617 	 ± 0.2389993532281002
	data : 0.11848750114440917
	model : 0.06708998680114746
			 train-loss:  2.0895280207906453 	 ± 0.23786438409593627
	data : 0.11873016357421876
	model : 0.06776385307312012
			 train-loss:  2.0960286486316733 	 ± 0.2423646894824223
	data : 0.11813631057739257
	model : 0.06894412040710449
			 train-loss:  2.1019693828291364 	 ± 0.24582627070194737
	data : 0.11701345443725586
	model : 0.06986632347106933
			 train-loss:  2.1019970178604126 	 ± 0.24413683566048663
	data : 0.11620049476623535
	model : 0.07010083198547364
			 train-loss:  2.106154449888178 	 ± 0.24506958390388137
	data : 0.11598806381225586
	model : 0.07188267707824707
			 train-loss:  2.1128494342168174 	 ± 0.25015035731332547
	data : 0.11410136222839355
	model : 0.07194981575012208
			 train-loss:  2.106621753228338 	 ± 0.2542845731452972
	data : 0.11400585174560547
	model : 0.07121787071228028
			 train-loss:  2.111094566134663 	 ± 0.25561955268898046
	data : 0.11482892036437989
	model : 0.07110114097595215
			 train-loss:  2.114325998685299 	 ± 0.2555536970297879
	data : 0.11487803459167481
	model : 0.07036399841308594
			 train-loss:  2.1130261557011663 	 ± 0.25419048089631086
	data : 0.11543083190917969
	model : 0.06940593719482421
			 train-loss:  2.112041760981083 	 ± 0.2527482822400548
	data : 0.11654806137084961
	model : 0.0693044662475586
			 train-loss:  2.1143465792691267 	 ± 0.25202778868909953
	data : 0.11645631790161133
	model : 0.06913990974426269
			 train-loss:  2.11102110002099 	 ± 0.25226803251611296
	data : 0.11649303436279297
	model : 0.06835613250732422
			 train-loss:  2.1094771523073494 	 ± 0.2511332173803308
	data : 0.11732416152954102
	model : 0.06888813972473144
			 train-loss:  2.106668398493812 	 ± 0.250941984841283
	data : 0.1170797348022461
	model : 0.06806325912475586
			 train-loss:  2.1088287549860336 	 ± 0.25024602671464363
	data : 0.11782636642456054
	model : 0.06801576614379883
			 train-loss:  2.1062848664993465 	 ± 0.24988990564345745
	data : 0.11789107322692871
	model : 0.06797204017639161
			 train-loss:  2.105007515556511 	 ± 0.24873183654660339
	data : 0.11807637214660645
	model : 0.06889042854309083
			 train-loss:  2.1061738417907194 	 ± 0.2475536992387883
	data : 0.11708087921142578
	model : 0.06883978843688965
			 train-loss:  2.104604959487915 	 ± 0.24659859145185176
	data : 0.11702527999877929
	model : 0.07010769844055176
			 train-loss:  2.101954264110989 	 ± 0.24649648998959592
	data : 0.11595616340637208
	model : 0.07040214538574219
			 train-loss:  2.1014430889716516 	 ± 0.24518633437028278
	data : 0.11580605506896972
	model : 0.07052702903747558
			 train-loss:  2.1004512024962385 	 ± 0.24403366453892408
	data : 0.11567368507385253
	model : 0.0704803466796875
			 train-loss:  2.0982683794472807 	 ± 0.24361944634894742
	data : 0.11572027206420898
	model : 0.06953444480895996
			 train-loss:  2.0992919295392136 	 ± 0.2425210896651175
	data : 0.11647553443908691
	model : 0.06845679283142089
			 train-loss:  2.0993720694592124 	 ± 0.24124253730991582
	data : 0.11730961799621582
	model : 0.06834440231323242
			 train-loss:  2.0976716987788677 	 ± 0.2405543655428027
	data : 0.11744918823242187
	model : 0.06853866577148438
			 train-loss:  2.096225116670746 	 ± 0.23973053937421562
	data : 0.11724643707275391
	model : 0.06851310729980468
			 train-loss:  2.0962063366053054 	 ± 0.23850435988439078
	data : 0.11744065284729004
	model : 0.06859912872314453
			 train-loss:  2.097636745433615 	 ± 0.2377188576825898
	data : 0.11749629974365235
	model : 0.06932563781738281
			 train-loss:  2.0980639672279358 	 ± 0.2365654710067151
	data : 0.11691198348999024
	model : 0.06924376487731934
			 train-loss:  2.097141709658179 	 ± 0.2355720413535455
	data : 0.11670832633972168
	model : 0.06893324851989746
			 train-loss:  2.0948256850242615 	 ± 0.23556716026320398
	data : 0.11697640419006347
	model : 0.06876106262207031
			 train-loss:  2.096180268861715 	 ± 0.23481969751720602
	data : 0.11713471412658691
	model : 0.0689927101135254
			 train-loss:  2.101360177764526 	 ± 0.23952815123800195
	data : 0.1167306900024414
	model : 0.06799812316894531
			 train-loss:  2.0997149297169275 	 ± 0.23897453878563682
	data : 0.11739978790283204
	model : 0.06805834770202637
			 train-loss:  2.0999268866934866 	 ± 0.23785454556021632
	data : 0.11764802932739257
	model : 0.06804308891296387
			 train-loss:  2.096718397095939 	 ± 0.23903400494375215
	data : 0.11743817329406739
	model : 0.0680957317352295
			 train-loss:  2.097931002025251 	 ± 0.23825520100030134
	data : 0.11756429672241211
	model : 0.06834650039672852
			 train-loss:  2.097274857923525 	 ± 0.23725777697382072
	data : 0.1173856258392334
	model : 0.06972413063049317
			 train-loss:  2.096970587426966 	 ± 0.23619823316794583
	data : 0.1164400577545166
	model : 0.06972122192382812
			 train-loss:  2.0941424434249467 	 ± 0.23699540107144387
	data : 0.11633920669555664
	model : 0.06897034645080566
			 train-loss:  2.095123895577022 	 ± 0.23616149395668468
	data : 0.11703238487243653
	model : 0.06897354125976562
			 train-loss:  2.0923652110901556 	 ± 0.2369199222806279
	data : 0.11681041717529297
	model : 0.06930537223815918
			 train-loss:  2.0903389025152777 	 ± 0.2368599620733987
	data : 0.11665029525756836
	model : 0.06913743019104004
			 train-loss:  2.0928030003672062 	 ± 0.2372909085769032
	data : 0.11676859855651855
	model : 0.06946167945861817
			 train-loss:  2.092030828369075 	 ± 0.23641095371665394
	data : 0.11646952629089355
	model : 0.07031497955322266
			 train-loss:  2.093804421587887 	 ± 0.23617226542980868
	data : 0.11575560569763184
	model : 0.07044787406921386
			 train-loss:  2.0938119231644325 	 ± 0.23516942026772877
	data : 0.11572322845458985
	model : 0.0703061580657959
			 train-loss:  2.0930150677176083 	 ± 0.23433915313899448
	data : 0.1157641887664795
	model : 0.07009177207946778
			 train-loss:  2.089110904932022 	 ± 0.23721524191717738
	data : 0.11580119132995606
	model : 0.0697540283203125
			 train-loss:  2.0881734583988663 	 ± 0.23645607952554815
	data : 0.116188383102417
	model : 0.06963281631469727
			 train-loss:  2.0858902403565702 	 ± 0.23682054384093246
	data : 0.11643433570861816
	model : 0.06966071128845215
			 train-loss:  2.082144852576217 	 ± 0.2394564890825482
	data : 0.11648797988891602
	model : 0.06986117362976074
			 train-loss:  2.08466106557077 	 ± 0.24011611543006114
	data : 0.1163515567779541
	model : 0.06980624198913574
			 train-loss:  2.080793662071228 	 ± 0.24300030312082713
	data : 0.1164273738861084
	model : 0.06979579925537109
			 train-loss:  2.07978079621754 	 ± 0.24229886595934752
	data : 0.11639723777770997
	model : 0.0700350284576416
			 train-loss:  2.077115790111812 	 ± 0.24318994911151817
	data : 0.11605043411254883
	model : 0.06956791877746582
			 train-loss:  2.076480652205646 	 ± 0.24234384944072881
	data : 0.11646556854248047
	model : 0.06978030204772949
			 train-loss:  2.071843511374422 	 ± 0.2470377692958308
	data : 0.11624341011047364
	model : 0.06968283653259277
			 train-loss:  2.0716550827026365 	 ± 0.24609509567718735
	data : 0.11652660369873047
	model : 0.06986756324768066
			 train-loss:  2.0722664658349887 	 ± 0.24525308827641007
	data : 0.11641654968261719
	model : 0.07027010917663574
			 train-loss:  2.071333660320802 	 ± 0.2445554926876376
	data : 0.11614952087402344
	model : 0.0701789379119873
			 train-loss:  2.070088028907776 	 ± 0.24405433877024926
	data : 0.11627192497253418
	model : 0.06921443939208985
			 train-loss:  2.0689692541734495 	 ± 0.24348407385527113
	data : 0.11722626686096191
	model : 0.06846175193786622
			 train-loss:  2.0674887277461864 	 ± 0.24318526324340756
	data : 0.11762986183166504
	model : 0.06734766960144042
			 train-loss:  2.066288841997876 	 ± 0.24269031526501117
	data : 0.11856546401977539
	model : 0.06608042716979981
			 train-loss:  2.067528513226196 	 ± 0.2422347521333148
	data : 0.11957826614379882
	model : 0.06571416854858399
			 train-loss:  2.068522618300673 	 ± 0.24163580891036138
	data : 0.11971278190612793
	model : 0.06629576683044433
			 train-loss:  2.0714710659260374 	 ± 0.24324368226183254
	data : 0.11921205520629882
	model : 0.06702642440795899
			 train-loss:  2.075028878450394 	 ± 0.24597627975757613
	data : 0.1185786247253418
	model : 0.06769804954528809
			 train-loss:  2.0735043270368103 	 ± 0.2457653707508505
	data : 0.11786723136901855
	model : 0.06842713356018067
			 train-loss:  2.0715590320842368 	 ± 0.2459854250054637
	data : 0.11727967262268066
	model : 0.06931447982788086
			 train-loss:  2.070071004487418 	 ± 0.24576433988453728
	data : 0.11651821136474609
	model : 0.06874241828918456
			 train-loss:  2.0679441971911325 	 ± 0.24622651922910369
	data : 0.11714811325073242
	model : 0.06862874031066894
			 train-loss:  2.068776791671227 	 ± 0.24557931680890596
	data : 0.11744418144226074
	model : 0.0688995361328125
			 train-loss:  2.0679823715392858 	 ± 0.24492373121319208
	data : 0.11726541519165039
	model : 0.06887922286987305
			 train-loss:  2.06523817169423 	 ± 0.2463311335480336
	data : 0.1173255443572998
	model : 0.06820616722106934
			 train-loss:  2.0652162617928274 	 ± 0.24549766698162337
	data : 0.11795759201049805
	model : 0.0689915657043457
			 train-loss:  2.0663534274837314 	 ± 0.24506325599506815
	data : 0.11730923652648925
	model : 0.0693596363067627
			 train-loss:  2.06416007121404 	 ± 0.24570803213662953
	data : 0.11698718070983886
	model : 0.0688314437866211
			 train-loss:  2.065579304631972 	 ± 0.24550917114578674
	data : 0.11752104759216309
	model : 0.06884407997131348
			 train-loss:  2.0674266226981817 	 ± 0.24575090819362438
	data : 0.11750979423522949
	model : 0.0696451187133789
			 train-loss:  2.0654119923223857 	 ± 0.24620257485566482
	data : 0.11671085357666015
	model : 0.06972217559814453
			 train-loss:  2.067006680098447 	 ± 0.24619338501613516
	data : 0.11656913757324219
	model : 0.06908679008483887
			 train-loss:  2.06712473054086 	 ± 0.24540230045004832
	data : 0.11701946258544922
	model : 0.06977744102478027
			 train-loss:  2.064441184966992 	 ± 0.2468855369658946
	data : 0.11630239486694335
	model : 0.0697249412536621
			 train-loss:  2.0670555085892888 	 ± 0.24825480339501818
	data : 0.11637344360351562
	model : 0.06953601837158203
			 train-loss:  2.066852149329608 	 ± 0.24748105789949554
	data : 0.11639018058776855
	model : 0.06942205429077149
			 train-loss:  2.0667474457302935 	 ± 0.24670509861539325
	data : 0.11631894111633301
	model : 0.07033324241638184
			 train-loss:  2.064739482104778 	 ± 0.2472328545052507
	data : 0.11576371192932129
	model : 0.07012128829956055
			 train-loss:  2.0646274741391957 	 ± 0.24646792686778696
	data : 0.11613359451293945
	model : 0.07054386138916016
			 train-loss:  2.0652723871631387 	 ± 0.24584227243967682
	data : 0.11571407318115234
	model : 0.070396089553833
			 train-loss:  2.0671301721795206 	 ± 0.24622501133050817
	data : 0.11602110862731933
	model : 0.07020335197448731
			 train-loss:  2.067396448879707 	 ± 0.24549671707949658
	data : 0.11616191864013672
	model : 0.06977853775024415
			 train-loss:  2.068258765249541 	 ± 0.24500065776453042
	data : 0.11651082038879394
	model : 0.06980929374694825
			 train-loss:  2.070423143455781 	 ± 0.24583871120341333
	data : 0.11632804870605469
	model : 0.06958651542663574
			 train-loss:  2.069856862821979 	 ± 0.24521012933932593
	data : 0.11659879684448242
	model : 0.06965336799621583
			 train-loss:  2.070642649417832 	 ± 0.24469004605781222
	data : 0.11650261878967286
	model : 0.06985735893249512
			 train-loss:  2.072857578125226 	 ± 0.2456483909607056
	data : 0.11640019416809082
	model : 0.06997189521789551
			 train-loss:  2.0738469074754153 	 ± 0.24526227786772226
	data : 0.11624808311462402
	model : 0.06994428634643554
			 train-loss:  2.0756097139670833 	 ± 0.24562182789383857
	data : 0.11629467010498047
	model : 0.06983184814453125
			 train-loss:  2.077818953020628 	 ± 0.24660480957538933
	data : 0.1163447380065918
	model : 0.07004299163818359
			 train-loss:  2.077921728867327 	 ± 0.24589474028748187
	data : 0.11618251800537109
	model : 0.07017545700073242
			 train-loss:  2.0823091357603842 	 ± 0.2518866083265697
	data : 0.11606321334838868
	model : 0.06933879852294922
			 train-loss:  2.080436568941389 	 ± 0.2523775783986141
	data : 0.11662068367004394
	model : 0.06911559104919433
			 train-loss:  2.0838372111320496 	 ± 0.25564879447852845
	data : 0.11674933433532715
	model : 0.0690314769744873
			 train-loss:  2.084346800874182 	 ± 0.25501522588096615
	data : 0.11668400764465332
	model : 0.06807737350463867
			 train-loss:  2.085683585552687 	 ± 0.25491902758780477
	data : 0.11772618293762208
	model : 0.06791195869445801
			 train-loss:  2.083523928786123 	 ± 0.2558337077575322
	data : 0.11774907112121583
	model : 0.06879901885986328
			 train-loss:  2.0830454256799484 	 ± 0.2552023800170037
	data : 0.11714658737182618
	model : 0.06908411979675293
			 train-loss:  2.086024869212788 	 ± 0.25761658570857326
	data : 0.11694164276123047
	model : 0.06901326179504394
			 train-loss:  2.088151384185959 	 ± 0.2584959364031224
	data : 0.11696491241455079
	model : 0.0697824478149414
			 train-loss:  2.090354157275841 	 ± 0.25949588397489476
	data : 0.1160132884979248
	model : 0.0692145824432373
			 train-loss:  2.0924454577591107 	 ± 0.26033152554253275
	data : 0.11666531562805176
	model : 0.06917953491210938
			 train-loss:  2.091663814235378 	 ± 0.2598433820405705
	data : 0.11665139198303223
	model : 0.06842432022094727
			 train-loss:  2.090219090702713 	 ± 0.25988789290909614
	data : 0.11759600639343262
	model : 0.06869473457336425
			 train-loss:  2.0906663113099366 	 ± 0.2592638277806315
	data : 0.11754460334777832
	model : 0.0688443660736084
			 train-loss:  2.0908235026166793 	 ± 0.2585823115928369
	data : 0.11750988960266114
	model : 0.06939969062805176
			 train-loss:  2.088886269816646 	 ± 0.25926158513856723
	data : 0.1169522762298584
	model : 0.0690317153930664
			 train-loss:  2.089678612508272 	 ± 0.2588077541764061
	data : 0.11743378639221191
	model : 0.06888999938964843
			 train-loss:  2.0885172970007853 	 ± 0.25862522999019993
	data : 0.11731410026550293
	model : 0.0679142951965332
			 train-loss:  2.088199275856217 	 ± 0.2579882884910192
	data : 0.11810064315795898
	model : 0.06772990226745605
			 train-loss:  2.0900329857910234 	 ± 0.2585704847922626
	data : 0.11831178665161132
	model : 0.06769013404846191
			 train-loss:  2.0890620965318583 	 ± 0.2582556678238979
	data : 0.1184006690979004
	model : 0.06803784370422364
			 train-loss:  2.088833694580274 	 ± 0.25761226626656636
	data : 0.11801939010620117
	model : 0.06900734901428222
			 train-loss:  2.0902446003592745 	 ± 0.2577084888693583
	data : 0.1171374797821045
	model : 0.06971793174743653
			 train-loss:  2.089171208705999 	 ± 0.2574924567454102
	data : 0.11653733253479004
	model : 0.07011332511901855
			 train-loss:  2.0885469407746284 	 ± 0.2569908126932809
	data : 0.11637287139892578
	model : 0.06957659721374512
			 train-loss:  2.0883720244594555 	 ± 0.2563561096751397
	data : 0.11685051918029785
	model : 0.06970162391662597
			 train-loss:  2.090471066236496 	 ± 0.25742309918285095
	data : 0.11670308113098145
	model : 0.069465970993042
			 train-loss:  2.0910063655815314 	 ± 0.25689351146900813
	data : 0.1169881820678711
	model : 0.06974472999572753
			 train-loss:  2.091837781490666 	 ± 0.2565278028446182
	data : 0.11676478385925293
	model : 0.06934571266174316
			 train-loss:  2.0909620640900335 	 ± 0.2561976845513633
	data : 0.11696434020996094
	model : 0.0699850082397461
			 train-loss:  2.0925764178528503 	 ± 0.2566019258599029
	data : 0.1163604736328125
	model : 0.06997523307800294
			 train-loss:  2.092222860964333 	 ± 0.25602510802806716
	data : 0.11640801429748535
	model : 0.06993751525878907
			 train-loss:  2.0907396373239537 	 ± 0.2562843105737979
	data : 0.1163337230682373
	model : 0.06979846954345703
			 train-loss:  2.090137160918563 	 ± 0.2558107085921371
	data : 0.11657571792602539
	model : 0.06936206817626953
			 train-loss:  2.091219008542024 	 ± 0.2556692767995212
	data : 0.11701602935791015
	model : 0.06933178901672363
			 train-loss:  2.0892729981664266 	 ± 0.2565963879676092
	data : 0.11699581146240234
	model : 0.06937413215637207
			 train-loss:  2.087746254603068 	 ± 0.2569345093170665
	data : 0.11704392433166504
	model : 0.06937651634216309
			 train-loss:  2.091910891058321 	 ± 0.2633339220213166
	data : 0.11698637008666993
	model : 0.0694385051727295
			 train-loss:  2.0904853495786773 	 ± 0.26352693248367104
	data : 0.11676669120788574
	model : 0.06927990913391113
			 train-loss:  2.0902965007253655 	 ± 0.2629219755838903
	data : 0.11678957939147949
	model : 0.06920537948608399
			 train-loss:  2.089754915683069 	 ± 0.2624260149608302
	data : 0.11690597534179688
	model : 0.06905932426452636
			 train-loss:  2.0897354514099833 	 ± 0.2618151654382427
	data : 0.11699905395507812
	model : 0.06918511390686036
			 train-loss:  2.092285042559659 	 ± 0.26387008550706614
	data : 0.11684908866882324
	model : 0.06904797554016114
			 train-loss:  2.0912656531355895 	 ± 0.26368734472075767
	data : 0.11699786186218261
	model : 0.06991000175476074
			 train-loss:  2.0908660582446177 	 ± 0.26314770730706366
	data : 0.11620078086853028
	model : 0.06963553428649902
			 train-loss:  2.0915852311539322 	 ± 0.26276086565581236
	data : 0.11640625
	model : 0.06952862739562989
			 train-loss:  2.091413594375957 	 ± 0.26217530590137617
	data : 0.11635313034057618
	model : 0.06958241462707519
			 train-loss:  2.0900586767973404 	 ± 0.26235233037846933
	data : 0.11645092964172363
	model : 0.06889009475708008
			 train-loss:  2.088815278298146 	 ± 0.26241261529110305
	data : 0.11714468002319336
	model : 0.06799416542053223
			 train-loss:  2.0906711207377002 	 ± 0.263279682613101
	data : 0.11800975799560547
	model : 0.06826133728027343
			 train-loss:  2.0914684741624763 	 ± 0.2629610641488617
	data : 0.11783046722412109
	model : 0.06752943992614746
			 train-loss:  2.0914506334728666 	 ± 0.26237619136301193
	data : 0.11847395896911621
	model : 0.06683712005615235
			 train-loss:  2.0904626434883187 	 ± 0.26221419940678614
	data : 0.11918063163757324
	model : 0.06696081161499023
			 train-loss:  2.088978883978554 	 ± 0.2625851172421485
	data : 0.11884808540344238
	model : 0.06755785942077637
			 train-loss:  2.090188170734205 	 ± 0.2626413645615222
	data : 0.11841902732849122
	model : 0.06748604774475098
			 train-loss:  2.089547601849752 	 ± 0.262245717685564
	data : 0.11826505661010742
	model : 0.06759934425354004
			 train-loss:  2.090793560898822 	 ± 0.262353400957573
	data : 0.11801605224609375
	model : 0.06795687675476074
			 train-loss:  2.0900688604875044 	 ± 0.2620155309974648
	data : 0.11740798950195312
	model : 0.06836938858032227
			 train-loss:  2.0898864032893347 	 ± 0.26146493881437255
	data : 0.11732196807861328
	model : 0.06840667724609376
			 train-loss:  2.0903323433429897 	 ± 0.26099165308356903
	data : 0.1170072078704834
	model : 0.06814765930175781
			 train-loss:  2.0904142173946414 	 ± 0.2604363801341568
	data : 0.11722722053527831
	model : 0.06857781410217285
			 train-loss:  2.0920332726011885 	 ± 0.26105914197696967
	data : 0.11688785552978516
	model : 0.0686182975769043
			 train-loss:  2.092260201098555 	 ± 0.26052868971266385
	data : 0.11683068275451661
	model : 0.06809611320495605
			 train-loss:  2.0917834455956887 	 ± 0.26008161552337156
	data : 0.11703577041625976
	model : 0.06806206703186035
			 train-loss:  2.091125346532389 	 ± 0.2597323201674358
	data : 0.1171226978302002
	model : 0.06829581260681153
			 train-loss:  2.0900355712140453 	 ± 0.2597330673774392
	data : 0.11721000671386719
	model : 0.0684356689453125
			 train-loss:  2.0900610397259394 	 ± 0.2591916910433918
	data : 0.11704797744750976
	model : 0.068609619140625
			 train-loss:  2.0880069035217477 	 ± 0.26060362673911086
	data : 0.11695237159729004
	model : 0.06938343048095703
			 train-loss:  2.0881722273905416 	 ± 0.2600772959705125
	data : 0.11650986671447754
	model : 0.06916341781616211
			 train-loss:  2.0886028524288913 	 ± 0.2596280437950494
	data : 0.11675481796264649
	model : 0.06921415328979492
			 train-loss:  2.0884655826404446 	 ± 0.25910430895451175
	data : 0.11651167869567872
	model : 0.0692258358001709
			 train-loss:  2.0898833960902934 	 ± 0.25952169448858153
	data : 0.11683192253112792
	model : 0.06918892860412598
			 train-loss:  2.0895292608718563 	 ± 0.2590529852444149
	data : 0.11677069664001465
	model : 0.06907944679260254
			 train-loss:  2.0884524511422224 	 ± 0.2590791322348368
	data : 0.11658277511596679
	model : 0.06902971267700195
			 train-loss:  2.0892441599599776 	 ± 0.2588554883545787
	data : 0.11653122901916504
	model : 0.06874127388000488
			 train-loss:  2.0894122334369216 	 ± 0.2583487342095359
	data : 0.11681900024414063
	model : 0.06861577033996583
			 train-loss:  2.090792622566223 	 ± 0.258749986003331
	data : 0.11646075248718261
	model : 0.06778693199157715
			 train-loss:  2.091366829625164 	 ± 0.25839358434254384
	data : 0.11739044189453125
	model : 0.06716914176940918
			 train-loss:  2.092250012216114 	 ± 0.2582597102345994
	data : 0.11810550689697266
	model : 0.06730327606201172
			 train-loss:  2.0920565863371836 	 ± 0.2577670991102956
	data : 0.11794099807739258
	model : 0.06711645126342773
			 train-loss:  2.0932712798982154 	 ± 0.25798368915193387
	data : 0.11783223152160645
	model : 0.06669125556945801
			 train-loss:  2.092475096384684 	 ± 0.25778982574631426
	data : 0.1182746410369873
	model : 0.06685638427734375
			 train-loss:  2.09051919169724 	 ± 0.25917469146420735
	data : 0.11723995208740234
	model : 0.058214855194091794
#epoch  36    val-loss:  2.4393650167866756  train-loss:  2.09051919169724  lr:  0.000625
			 train-loss:  2.5119168758392334 	 ± 0.0
	data : 5.831364154815674
	model : 0.07142376899719238
			 train-loss:  2.17810320854187 	 ± 0.3338136672973633
	data : 2.9840818643569946
	model : 0.07227373123168945
			 train-loss:  2.150200923283895 	 ± 0.2753993186173694
	data : 2.0279902617136636
	model : 0.0701140562693278
			 train-loss:  2.121899724006653 	 ± 0.24348811503392523
	data : 1.5508078336715698
	model : 0.07002341747283936
			 train-loss:  2.132148265838623 	 ± 0.21874482889646962
	data : 1.2637375354766847
	model : 0.06998038291931152
			 train-loss:  2.108672102292379 	 ± 0.20647050172289713
	data : 0.1206273078918457
	model : 0.0696436882019043
			 train-loss:  2.1149545397077287 	 ± 0.19177297117950112
	data : 0.1164698600769043
	model : 0.06888599395751953
			 train-loss:  2.085544526576996 	 ± 0.19553620002978025
	data : 0.11651048660278321
	model : 0.06950998306274414
			 train-loss:  2.0418142212761774 	 ± 0.2220019257922706
	data : 0.11590008735656739
	model : 0.06944942474365234
			 train-loss:  2.0666786193847657 	 ± 0.22342899138126743
	data : 0.11609649658203125
	model : 0.06939158439636231
			 train-loss:  2.042450048706748 	 ± 0.2263901985465621
	data : 0.11622886657714844
	model : 0.06870589256286622
			 train-loss:  2.081386258204778 	 ± 0.2523049563701466
	data : 0.1168088436126709
	model : 0.06879682540893554
			 train-loss:  2.1093960266846876 	 ± 0.26110458329563135
	data : 0.11694722175598145
	model : 0.06846246719360352
			 train-loss:  2.0840906500816345 	 ± 0.26763898177367274
	data : 0.11729593276977539
	model : 0.06812148094177246
			 train-loss:  2.0750298420588176 	 ± 0.260776964055929
	data : 0.11766715049743652
	model : 0.06815853118896484
			 train-loss:  2.066900387406349 	 ± 0.25445167795681
	data : 0.11750226020812989
	model : 0.06889858245849609
			 train-loss:  2.0448776343289543 	 ± 0.26210134459851353
	data : 0.11696548461914062
	model : 0.0687408447265625
			 train-loss:  2.0607864922947354 	 ± 0.26302693476241373
	data : 0.11714859008789062
	model : 0.06928691864013672
			 train-loss:  2.043179750442505 	 ± 0.26668689783377747
	data : 0.11670150756835937
	model : 0.06988177299499512
			 train-loss:  2.0648527264595034 	 ± 0.276569062444858
	data : 0.11609373092651368
	model : 0.07077417373657227
			 train-loss:  2.0511009125482467 	 ± 0.27682176903882544
	data : 0.115260648727417
	model : 0.07078289985656738
			 train-loss:  2.0326960303566675 	 ± 0.2833030712674051
	data : 0.11520476341247558
	model : 0.07030010223388672
			 train-loss:  2.037931131279987 	 ± 0.2781617798503404
	data : 0.11561770439147949
	model : 0.07008132934570313
			 train-loss:  2.0529408951600394 	 ± 0.281659012606289
	data : 0.11592907905578613
	model : 0.06939635276794434
			 train-loss:  2.057164554595947 	 ± 0.27674296844506907
	data : 0.1164067268371582
	model : 0.06756620407104492
			 train-loss:  2.039234771178319 	 ± 0.2857935579094908
	data : 0.11813488006591796
	model : 0.06734933853149414
			 train-loss:  2.0590793512485646 	 ± 0.29814734529896236
	data : 0.11840920448303223
	model : 0.06800785064697265
			 train-loss:  2.0574726504938945 	 ± 0.2928938905345055
	data : 0.11771354675292969
	model : 0.06818747520446777
			 train-loss:  2.059146918099502 	 ± 0.2879360229960468
	data : 0.11753888130187988
	model : 0.0684267520904541
			 train-loss:  2.0757530967394513 	 ± 0.296885114922348
	data : 0.1175309658050537
	model : 0.06880583763122558
			 train-loss:  2.0727943951083767 	 ± 0.2925066464721275
	data : 0.11714448928833007
	model : 0.0680912971496582
			 train-loss:  2.0659584514796734 	 ± 0.29040492040392535
	data : 0.11762151718139649
	model : 0.06717348098754883
			 train-loss:  2.05717573743878 	 ± 0.2902546559566942
	data : 0.1183967113494873
	model : 0.06723213195800781
			 train-loss:  2.0721193972755882 	 ± 0.29856191626178175
	data : 0.1185530662536621
	model : 0.06724672317504883
			 train-loss:  2.069551420211792 	 ± 0.29464656159071384
	data : 0.11840472221374512
	model : 0.06818814277648926
			 train-loss:  2.076099362638262 	 ± 0.2930966877835479
	data : 0.11764187812805176
	model : 0.06843481063842774
			 train-loss:  2.078965928103473 	 ± 0.2896199452398469
	data : 0.11744198799133301
	model : 0.06915030479431153
			 train-loss:  2.0756350222386812 	 ± 0.2865010722044099
	data : 0.11681418418884278
	model : 0.06913409233093262
			 train-loss:  2.06926843447563 	 ± 0.2855143538501401
	data : 0.11659622192382812
	model : 0.06873106956481934
			 train-loss:  2.078576976060867 	 ± 0.28785377192147615
	data : 0.11719512939453125
	model : 0.06904206275939942
			 train-loss:  2.0782919220808074 	 ± 0.2843274057397343
	data : 0.11689496040344238
	model : 0.06899785995483398
			 train-loss:  2.0764530272710893 	 ± 0.2811688198861398
	data : 0.11700177192687988
	model : 0.06913995742797852
			 train-loss:  2.0729235909705936 	 ± 0.2788199894489384
	data : 0.11693472862243652
	model : 0.06915884017944336
			 train-loss:  2.083452492952347 	 ± 0.2841489739785284
	data : 0.11688032150268554
	model : 0.06975836753845215
			 train-loss:  2.0953692303763494 	 ± 0.2918814532276502
	data : 0.11631274223327637
	model : 0.06832346916198731
			 train-loss:  2.0946462491284246 	 ± 0.2887321316182054
	data : 0.11750879287719726
	model : 0.06805753707885742
			 train-loss:  2.0915554457522454 	 ± 0.2864121767201322
	data : 0.11751780509948731
	model : 0.06742115020751953
			 train-loss:  2.086514040827751 	 ± 0.28551265684392907
	data : 0.11810536384582519
	model : 0.06734075546264648
			 train-loss:  2.088528034638385 	 ± 0.2829285270971484
	data : 0.11808533668518066
	model : 0.06745944023132325
			 train-loss:  2.0970944929122926 	 ± 0.286432202409271
	data : 0.11791844367980957
	model : 0.06821885108947753
			 train-loss:  2.091547603700675 	 ± 0.28630946775598437
	data : 0.11733379364013671
	model : 0.06832847595214844
			 train-loss:  2.0934931383683133 	 ± 0.28388333114495273
	data : 0.11742515563964843
	model : 0.06807937622070312
			 train-loss:  2.0915946488110526 	 ± 0.2815254981656986
	data : 0.11735939979553223
	model : 0.06825113296508789
			 train-loss:  2.0920772133050143 	 ± 0.2789287242636694
	data : 0.11739311218261719
	model : 0.06727457046508789
			 train-loss:  2.097720304402438 	 ± 0.27947498476226645
	data : 0.11819205284118653
	model : 0.06725740432739258
			 train-loss:  2.0938302342380797 	 ± 0.27846688826804067
	data : 0.11826648712158203
	model : 0.06815896034240723
			 train-loss:  2.1023873425366584 	 ± 0.2833442074865543
	data : 0.11750035285949707
	model : 0.06914424896240234
			 train-loss:  2.102451293632902 	 ± 0.28089137966640093
	data : 0.11677756309509277
	model : 0.06896705627441406
			 train-loss:  2.099665324566728 	 ± 0.2793078118540759
	data : 0.11679143905639648
	model : 0.06967606544494628
			 train-loss:  2.1017612000306447 	 ± 0.277437936261406
	data : 0.11624732017517089
	model : 0.06974773406982422
			 train-loss:  2.0985901590253486 	 ± 0.2762486300682266
	data : 0.11609883308410644
	model : 0.06968164443969727
			 train-loss:  2.093835817229363 	 ± 0.2765163206165075
	data : 0.11616501808166504
	model : 0.06971688270568847
			 train-loss:  2.0924122579514033 	 ± 0.274541889025424
	data : 0.11604056358337403
	model : 0.06978821754455566
			 train-loss:  2.103907374665141 	 ± 0.28726332421180983
	data : 0.11598401069641114
	model : 0.06998639106750489
			 train-loss:  2.105312862763038 	 ± 0.2852667190985763
	data : 0.11573066711425781
	model : 0.0699995994567871
			 train-loss:  2.103821832122225 	 ± 0.2833524675210456
	data : 0.1156764030456543
	model : 0.07013502120971679
			 train-loss:  2.0995517524320686 	 ± 0.2833614291723788
	data : 0.11559443473815918
	model : 0.06933131217956542
			 train-loss:  2.0980690857943367 	 ± 0.28153187380381306
	data : 0.11646504402160644
	model : 0.06900963783264161
			 train-loss:  2.0964493820632715 	 ± 0.2798033084279039
	data : 0.11673851013183593
	model : 0.0689840316772461
			 train-loss:  2.096081440789359 	 ± 0.27781433667287614
	data : 0.11676874160766601
	model : 0.06803355216979981
			 train-loss:  2.0963869800030346 	 ± 0.2758628044062985
	data : 0.11779799461364746
	model : 0.06793394088745117
			 train-loss:  2.091418210003111 	 ± 0.27712133511068615
	data : 0.11791539192199707
	model : 0.06939001083374023
			 train-loss:  2.10101463696728 	 ± 0.2870101265067042
	data : 0.1166269302368164
	model : 0.07015724182128906
			 train-loss:  2.1018820453334497 	 ± 0.2851605943561363
	data : 0.115989351272583
	model : 0.07070865631103515
			 train-loss:  2.0972287400563556 	 ± 0.28606762174825
	data : 0.11546902656555176
	model : 0.07158432006835938
			 train-loss:  2.0977582586439034 	 ± 0.28421636352803176
	data : 0.1145552635192871
	model : 0.07157611846923828
			 train-loss:  2.093843709338795 	 ± 0.28441952298201517
	data : 0.11450486183166504
	model : 0.07082319259643555
			 train-loss:  2.0969893366862564 	 ± 0.2839353259555651
	data : 0.11519122123718262
	model : 0.07108044624328613
			 train-loss:  2.101709466946276 	 ± 0.2851956930947667
	data : 0.11485757827758789
	model : 0.07037453651428223
			 train-loss:  2.1015109583735465 	 ± 0.28341310681482107
	data : 0.11548161506652832
	model : 0.0695225715637207
			 train-loss:  2.1044389333254023 	 ± 0.28287309921412807
	data : 0.11635966300964355
	model : 0.06952629089355469
			 train-loss:  2.10907764405739 	 ± 0.28422578961674655
	data : 0.11629142761230468
	model : 0.06958889961242676
			 train-loss:  2.1056420228567467 	 ± 0.2842162576261694
	data : 0.11620831489562988
	model : 0.06872591972351075
			 train-loss:  2.105342740104312 	 ± 0.28253258586644175
	data : 0.11710214614868164
	model : 0.06882553100585938
			 train-loss:  2.1022249067530914 	 ± 0.2823156083687088
	data : 0.1169921875
	model : 0.0687859058380127
			 train-loss:  2.100669524004293 	 ± 0.2810355269522204
	data : 0.11709070205688477
	model : 0.06851615905761718
			 train-loss:  2.1020174369044686 	 ± 0.2796951750526749
	data : 0.11731896400451661
	model : 0.06797304153442382
			 train-loss:  2.0994502929123966 	 ± 0.2791303825384875
	data : 0.11800093650817871
	model : 0.0677492618560791
			 train-loss:  2.097328172640854 	 ± 0.2782707917556328
	data : 0.11812334060668946
	model : 0.06792988777160644
			 train-loss:  2.0993721935484144 	 ± 0.2773915871796155
	data : 0.11807928085327149
	model : 0.06886224746704102
			 train-loss:  2.098269019808088 	 ± 0.27606169688601606
	data : 0.11704130172729492
	model : 0.06900014877319335
			 train-loss:  2.0932003454021784 	 ± 0.27878237596037075
	data : 0.11693382263183594
	model : 0.06957468986511231
			 train-loss:  2.092856651993208 	 ± 0.2772990910474818
	data : 0.11640830039978027
	model : 0.06997590065002442
			 train-loss:  2.089561201156454 	 ± 0.27764498177852015
	data : 0.11603984832763672
	model : 0.0699228286743164
			 train-loss:  2.08808694638704 	 ± 0.27654945078814247
	data : 0.1159553050994873
	model : 0.06992449760437011
			 train-loss:  2.089229889214039 	 ± 0.2753307764916402
	data : 0.1160658359527588
	model : 0.0700235366821289
			 train-loss:  2.0880266292807983 	 ± 0.27416147204207963
	data : 0.11601648330688477
	model : 0.0700502872467041
			 train-loss:  2.0868588552183036 	 ± 0.2730014770392234
	data : 0.11591005325317383
	model : 0.07015023231506348
			 train-loss:  2.086749949840584 	 ± 0.2716213217975152
	data : 0.1158381462097168
	model : 0.07028183937072754
			 train-loss:  2.0863816916942595 	 ± 0.27028464037501304
	data : 0.11587071418762207
	model : 0.06959304809570313
			 train-loss:  2.0879512742014215 	 ± 0.26940089258817
	data : 0.11643719673156738
	model : 0.06956138610839843
			 train-loss:  2.085378663212645 	 ± 0.26932091327709673
	data : 0.11631731986999512
	model : 0.06933937072753907
			 train-loss:  2.0837545070833374 	 ± 0.26851183810185936
	data : 0.11646380424499511
	model : 0.0682988166809082
			 train-loss:  2.083804916877013 	 ± 0.26721828736173964
	data : 0.11757078170776367
	model : 0.06812596321105957
			 train-loss:  2.0814504214695524 	 ± 0.26702452902179474
	data : 0.11760954856872559
	model : 0.06972060203552247
			 train-loss:  2.0803494127291553 	 ± 0.26600135509603856
	data : 0.11613607406616211
	model : 0.0692169189453125
			 train-loss:  2.0775409324146876 	 ± 0.2663297292551531
	data : 0.11662182807922364
	model : 0.0690427303314209
			 train-loss:  2.0779584999437684 	 ± 0.2651290404897825
	data : 0.11694850921630859
	model : 0.07009601593017578
			 train-loss:  2.079142714859149 	 ± 0.26419683925994514
	data : 0.11599564552307129
	model : 0.07044863700866699
			 train-loss:  2.07596676458012 	 ± 0.26507521697307773
	data : 0.1157036304473877
	model : 0.06997122764587402
			 train-loss:  2.075782811319506 	 ± 0.2638855359219628
	data : 0.11614484786987304
	model : 0.0701176643371582
			 train-loss:  2.0755273518817767 	 ± 0.26271862069735635
	data : 0.11625857353210449
	model : 0.0703967571258545
			 train-loss:  2.077229931291226 	 ± 0.26217347440923994
	data : 0.11584310531616211
	model : 0.07006096839904785
			 train-loss:  2.0730302888050414 	 ± 0.26481120323295854
	data : 0.11594491004943848
	model : 0.06972455978393555
			 train-loss:  2.0747002736381863 	 ± 0.26425956899563424
	data : 0.11633052825927734
	model : 0.06916661262512207
			 train-loss:  2.072617854537635 	 ± 0.26406401519887107
	data : 0.1169508457183838
	model : 0.06964278221130371
			 train-loss:  2.0715854769079094 	 ± 0.2631681137700596
	data : 0.11642189025878906
	model : 0.06911578178405761
			 train-loss:  2.072924410892745 	 ± 0.26245052787986717
	data : 0.11699175834655762
	model : 0.06867108345031739
			 train-loss:  2.0763257962315023 	 ± 0.26394439855363516
	data : 0.11743736267089844
	model : 0.06879067420959473
			 train-loss:  2.077817216515541 	 ± 0.26334537389066665
	data : 0.11724338531494141
	model : 0.06840314865112304
			 train-loss:  2.0771053408788256 	 ± 0.26237082724448185
	data : 0.11765112876892089
	model : 0.06864461898803711
			 train-loss:  2.079335632871409 	 ± 0.2624425268086665
	data : 0.11754894256591797
	model : 0.06923785209655761
			 train-loss:  2.0781039048016554 	 ± 0.2617273477136929
	data : 0.11703028678894042
	model : 0.06988639831542968
			 train-loss:  2.074862861825574 	 ± 0.26313648872746404
	data : 0.11644740104675293
	model : 0.06975960731506348
			 train-loss:  2.0736647243499755 	 ± 0.26242120979279465
	data : 0.11660857200622558
	model : 0.07056446075439453
			 train-loss:  2.0725578296752203 	 ± 0.2616705878963093
	data : 0.1157599925994873
	model : 0.07011723518371582
			 train-loss:  2.0722279511098787 	 ± 0.260664654740662
	data : 0.11607675552368164
	model : 0.06921062469482422
			 train-loss:  2.0745943300426006 	 ± 0.2610103480229253
	data : 0.11678586006164551
	model : 0.06906218528747558
			 train-loss:  2.0753626952799715 	 ± 0.2601419985912416
	data : 0.11680517196655274
	model : 0.06913170814514161
			 train-loss:  2.076456902577327 	 ± 0.2594373559093071
	data : 0.11676664352416992
	model : 0.06884918212890626
			 train-loss:  2.0763868229989786 	 ± 0.2584464751855761
	data : 0.11706161499023438
	model : 0.06828298568725585
			 train-loss:  2.0805513244686704 	 ± 0.2618406070162993
	data : 0.11748862266540527
	model : 0.06923875808715821
			 train-loss:  2.0783355388426243 	 ± 0.2620936705825494
	data : 0.11660203933715821
	model : 0.0694389820098877
			 train-loss:  2.079820023543799 	 ± 0.2616745087994734
	data : 0.11651825904846191
	model : 0.0693422794342041
			 train-loss:  2.0803971458364416 	 ± 0.2607891264671779
	data : 0.11653528213500977
	model : 0.06922407150268554
			 train-loss:  2.079879922901883 	 ± 0.2598980626592744
	data : 0.11662831306457519
	model : 0.0690760612487793
			 train-loss:  2.081325089844474 	 ± 0.25949565733480334
	data : 0.1167527198791504
	model : 0.06811509132385254
			 train-loss:  2.0792111262031225 	 ± 0.2597350016334696
	data : 0.11775898933410645
	model : 0.06800708770751954
			 train-loss:  2.0831233237287123 	 ± 0.2628479806694486
	data : 0.11796493530273437
	model : 0.06793489456176757
			 train-loss:  2.0811758518218992 	 ± 0.26291204909366866
	data : 0.11794276237487793
	model : 0.06785740852355956
			 train-loss:  2.080663063847427 	 ± 0.26204832861816557
	data : 0.11803627014160156
	model : 0.06870875358581544
			 train-loss:  2.082748075606118 	 ± 0.2622950730833325
	data : 0.117254638671875
	model : 0.0696484088897705
			 train-loss:  2.0828595761652595 	 ± 0.26137972555896094
	data : 0.11628046035766601
	model : 0.06978964805603027
			 train-loss:  2.082772413889567 	 ± 0.26047266139713415
	data : 0.11606616973876953
	model : 0.06943097114562988
			 train-loss:  2.0858771932536158 	 ± 0.26223312937529236
	data : 0.11643238067626953
	model : 0.06947722434997558
			 train-loss:  2.085497936157331 	 ± 0.2613734280436135
	data : 0.11630830764770508
	model : 0.06856083869934082
			 train-loss:  2.0866773679953856 	 ± 0.26087243703776286
	data : 0.11717801094055176
	model : 0.06825718879699708
			 train-loss:  2.0899932239506698 	 ± 0.2630795537329771
	data : 0.11744322776794433
	model : 0.06819753646850586
			 train-loss:  2.089772861275897 	 ± 0.2622089549758175
	data : 0.11758642196655274
	model : 0.06862969398498535
			 train-loss:  2.0921216090520223 	 ± 0.26290141681558593
	data : 0.11711311340332031
	model : 0.06843366622924804
			 train-loss:  2.0909529071769968 	 ± 0.26242009273581784
	data : 0.1173058032989502
	model : 0.06949691772460938
			 train-loss:  2.0901177792172683 	 ± 0.2617566879335163
	data : 0.11630840301513672
	model : 0.06987547874450684
			 train-loss:  2.0916091398475998 	 ± 0.2615469647937163
	data : 0.1159696102142334
	model : 0.06983537673950195
			 train-loss:  2.0911934793769538 	 ± 0.2607470978969037
	data : 0.11626577377319336
	model : 0.0697929859161377
			 train-loss:  2.089587200841596 	 ± 0.2606678924769189
	data : 0.11633148193359374
	model : 0.06984052658081055
			 train-loss:  2.0887779073837476 	 ± 0.2600263560332325
	data : 0.11628842353820801
	model : 0.0688009262084961
			 train-loss:  2.091254411988957 	 ± 0.26103602240787077
	data : 0.11727042198181152
	model : 0.06800370216369629
			 train-loss:  2.0887124900576435 	 ± 0.26215067240313844
	data : 0.11794810295104981
	model : 0.06808676719665527
			 train-loss:  2.0919722970926538 	 ± 0.2645178956019289
	data : 0.11778111457824707
	model : 0.06822295188903808
			 train-loss:  2.09377157241106 	 ± 0.26466422432507664
	data : 0.11759324073791504
	model : 0.0685502052307129
			 train-loss:  2.0933665103793886 	 ± 0.26389075050388733
	data : 0.11734981536865234
	model : 0.06956181526184083
			 train-loss:  2.093027643215509 	 ± 0.26311014758415247
	data : 0.11638116836547852
	model : 0.07033376693725586
			 train-loss:  2.093822588949847 	 ± 0.2624968925934553
	data : 0.11560935974121093
	model : 0.07047114372253419
			 train-loss:  2.0924957409137632 	 ± 0.2622430822701451
	data : 0.11534428596496582
	model : 0.06952886581420899
			 train-loss:  2.0912154465010673 	 ± 0.26196079639535275
	data : 0.11639232635498047
	model : 0.06937875747680664
			 train-loss:  2.091698362884751 	 ± 0.2612442226657392
	data : 0.11662082672119141
	model : 0.06924257278442383
			 train-loss:  2.090547932122282 	 ± 0.26088229042369465
	data : 0.1167604923248291
	model : 0.06825022697448731
			 train-loss:  2.0928156737770354 	 ± 0.261750407624082
	data : 0.11771163940429688
	model : 0.06815462112426758
			 train-loss:  2.092603128337296 	 ± 0.26098938941412947
	data : 0.11783714294433593
	model : 0.06909260749816895
			 train-loss:  2.0942989945411683 	 ± 0.26115286674818433
	data : 0.11686749458312988
	model : 0.06894493103027344
			 train-loss:  2.0925419253912585 	 ± 0.2613940019334968
	data : 0.11686463356018066
	model : 0.06809787750244141
			 train-loss:  2.0959618597529657 	 ± 0.2644420218047682
	data : 0.11765751838684083
	model : 0.0688927173614502
			 train-loss:  2.0956871709382603 	 ± 0.2637012395354749
	data : 0.11696929931640625
	model : 0.06864380836486816
			 train-loss:  2.0944362310157425 	 ± 0.26345667053281674
	data : 0.11730008125305176
	model : 0.06800861358642578
			 train-loss:  2.093945050239563 	 ± 0.2627827448585033
	data : 0.11800975799560547
	model : 0.06827096939086914
			 train-loss:  2.091976209120317 	 ± 0.26332636219856986
	data : 0.1178696632385254
	model : 0.06913547515869141
			 train-loss:  2.0940298980238747 	 ± 0.2639911391632796
	data : 0.11715211868286132
	model : 0.06931710243225098
			 train-loss:  2.093822211362003 	 ± 0.26326304706886305
	data : 0.11703033447265625
	model : 0.06933088302612304
			 train-loss:  2.096264777902784 	 ± 0.26454151085831973
	data : 0.11688995361328125
	model : 0.06963281631469727
			 train-loss:  2.093089375893275 	 ± 0.2672046215062735
	data : 0.11646833419799804
	model : 0.06893043518066407
			 train-loss:  2.0914927925194164 	 ± 0.2673250419977632
	data : 0.11705961227416992
	model : 0.0688845157623291
			 train-loss:  2.092308895928519 	 ± 0.26681562264212577
	data : 0.11710557937622071
	model : 0.06875290870666503
			 train-loss:  2.0907510623254413 	 ± 0.2669142986608518
	data : 0.11730742454528809
	model : 0.06845765113830567
			 train-loss:  2.0892145717921466 	 ± 0.2669982739093577
	data : 0.1175610065460205
	model : 0.06861562728881836
			 train-loss:  2.0880655449789924 	 ± 0.2667314485180619
	data : 0.11741099357604981
	model : 0.06851534843444824
			 train-loss:  2.0869538155935143 	 ± 0.26644288574064884
	data : 0.11740736961364746
	model : 0.06809144020080567
			 train-loss:  2.087607172083727 	 ± 0.2658788722364808
	data : 0.11796517372131347
	model : 0.06816115379333496
			 train-loss:  2.088427645094851 	 ± 0.2654080624100608
	data : 0.11779522895812988
	model : 0.06875448226928711
			 train-loss:  2.0874684380475808 	 ± 0.2650315231376569
	data : 0.11740360260009766
	model : 0.06879653930664062
			 train-loss:  2.0882708994965804 	 ± 0.2645632637812612
	data : 0.11753149032592773
	model : 0.06955780982971191
			 train-loss:  2.089119959252043 	 ± 0.26412919706409294
	data : 0.11694397926330566
	model : 0.06981844902038574
			 train-loss:  2.0872802268713713 	 ± 0.26466457804599625
	data : 0.11656465530395507
	model : 0.06969938278198243
			 train-loss:  2.0879282685759155 	 ± 0.2641307086243007
	data : 0.11687602996826171
	model : 0.06860504150390626
			 train-loss:  2.0865450243359986 	 ± 0.26414900538967107
	data : 0.11768951416015624
	model : 0.0686917781829834
			 train-loss:  2.086156834700169 	 ± 0.2635263026353217
	data : 0.1174501895904541
	model : 0.06882338523864746
			 train-loss:  2.0884786205632344 	 ± 0.264845199602672
	data : 0.11734552383422851
	model : 0.06825261116027832
			 train-loss:  2.0877850334051296 	 ± 0.26435054831490057
	data : 0.11779398918151855
	model : 0.06846795082092286
			 train-loss:  2.086653744933581 	 ± 0.26415980061055744
	data : 0.1174386978149414
	model : 0.06863183975219726
			 train-loss:  2.086567710392439 	 ± 0.26349802763113306
	data : 0.11742925643920898
	model : 0.06801338195800781
			 train-loss:  2.087820755839348 	 ± 0.2634321714752805
	data : 0.11804742813110351
	model : 0.06790852546691895
			 train-loss:  2.087866159813914 	 ± 0.262776834992812
	data : 0.1180905818939209
	model : 0.06880002021789551
			 train-loss:  2.0876572905200543 	 ± 0.2621423162822413
	data : 0.11734037399291992
	model : 0.06830024719238281
			 train-loss:  2.0870453395279758 	 ± 0.26164044892922
	data : 0.11777548789978028
	model : 0.06906318664550781
			 train-loss:  2.088690602311901 	 ± 0.26204895694004116
	data : 0.11717329025268555
	model : 0.06966214179992676
			 train-loss:  2.0900107988497107 	 ± 0.26208822437826496
	data : 0.11673526763916016
	model : 0.06963810920715333
			 train-loss:  2.0897055424532844 	 ± 0.2614878425557057
	data : 0.11690268516540528
	model : 0.06943926811218262
			 train-loss:  2.090601991340158 	 ± 0.26117258533685683
	data : 0.11679821014404297
	model : 0.06991348266601563
			 train-loss:  2.089604600117757 	 ± 0.26093888677751104
	data : 0.11632008552551269
	model : 0.07009725570678711
			 train-loss:  2.0911648490212182 	 ± 0.261284648982489
	data : 0.1161353588104248
	model : 0.0699894905090332
			 train-loss:  2.0915203582672848 	 ± 0.2607124641898909
	data : 0.11632738113403321
	model : 0.06923942565917969
			 train-loss:  2.0908730041359274 	 ± 0.26026305105298075
	data : 0.11665029525756836
	model : 0.06900725364685059
			 train-loss:  2.0906673444891877 	 ± 0.2596656825723634
	data : 0.1171529769897461
	model : 0.06876621246337891
			 train-loss:  2.0903280620843594 	 ± 0.25910251894504854
	data : 0.11735329627990723
	model : 0.06860990524291992
			 train-loss:  2.0913496474239315 	 ± 0.25892604948417974
	data : 0.11734251976013184
	model : 0.06877074241638184
			 train-loss:  2.091115135370299 	 ± 0.2583459729071485
	data : 0.11716160774230958
	model : 0.06962819099426269
			 train-loss:  2.0899459962491638 	 ± 0.25831672140391737
	data : 0.11648969650268555
	model : 0.06987543106079101
			 train-loss:  2.092414761468562 	 ± 0.26026237836903965
	data : 0.1163419246673584
	model : 0.06985330581665039
			 train-loss:  2.09188440852209 	 ± 0.2597822630406442
	data : 0.11625614166259765
	model : 0.06978430747985839
			 train-loss:  2.0902089234356467 	 ± 0.2603663682887568
	data : 0.11618051528930665
	model : 0.07034463882446289
			 train-loss:  2.0896589214151557 	 ± 0.2599014321311337
	data : 0.1157909870147705
	model : 0.07055039405822754
			 train-loss:  2.089060048711785 	 ± 0.2594648466065752
	data : 0.1156278133392334
	model : 0.0705484390258789
			 train-loss:  2.089054741300978 	 ± 0.2588798187726182
	data : 0.11552467346191406
	model : 0.07064528465270996
			 train-loss:  2.0900808824017445 	 ± 0.2587508190579717
	data : 0.11553001403808594
	model : 0.07065348625183106
			 train-loss:  2.088898185108389 	 ± 0.25877600337138357
	data : 0.11556329727172851
	model : 0.07007975578308105
			 train-loss:  2.089585886001587 	 ± 0.25840536854766005
	data : 0.11593255996704102
	model : 0.06966328620910645
			 train-loss:  2.0917920844744793 	 ± 0.25994811481309077
	data : 0.11637763977050782
	model : 0.06962928771972657
			 train-loss:  2.0927415944406116 	 ± 0.25976739309592795
	data : 0.11631598472595214
	model : 0.0696573257446289
			 train-loss:  2.0921420674575004 	 ± 0.2593544461898447
	data : 0.11635537147521972
	model : 0.0696340560913086
			 train-loss:  2.0919331723425585 	 ± 0.2588067726953653
	data : 0.11661605834960938
	model : 0.06958184242248536
			 train-loss:  2.0916670965111774 	 ± 0.2582749241682487
	data : 0.11677069664001465
	model : 0.06963562965393066
			 train-loss:  2.0907758247284662 	 ± 0.25806950654094263
	data : 0.11659231185913085
	model : 0.06962094306945801
			 train-loss:  2.089646755107518 	 ± 0.2580838627006561
	data : 0.11659421920776367
	model : 0.06938390731811524
			 train-loss:  2.0887144287256723 	 ± 0.25792067378764816
	data : 0.11670613288879395
	model : 0.06920790672302246
			 train-loss:  2.089352345874167 	 ± 0.2575531089840957
	data : 0.11661310195922851
	model : 0.06900711059570312
			 train-loss:  2.08976552537147 	 ± 0.2570822458665211
	data : 0.1165781021118164
	model : 0.06884746551513672
			 train-loss:  2.089457772545895 	 ± 0.2565803784620373
	data : 0.11668624877929687
	model : 0.06862053871154786
			 train-loss:  2.0873374828306432 	 ± 0.2581020737824499
	data : 0.11673450469970703
	model : 0.06858248710632324
			 train-loss:  2.089719478823558 	 ± 0.2601566719030363
	data : 0.11679863929748535
	model : 0.06847448348999023
			 train-loss:  2.0892470646104053 	 ± 0.25971411872191474
	data : 0.11673359870910645
	model : 0.06840801239013672
			 train-loss:  2.0893544887502986 	 ± 0.25917780366534104
	data : 0.11670842170715331
	model : 0.0684290885925293
			 train-loss:  2.0905754066601827 	 ± 0.25933021603025475
	data : 0.11694316864013672
	model : 0.06876230239868164
			 train-loss:  2.0914347728421867 	 ± 0.2591374932759923
	data : 0.11688189506530762
	model : 0.06891541481018067
			 train-loss:  2.0914496487550776 	 ± 0.25860384240343004
	data : 0.11675086021423339
	model : 0.06912760734558106
			 train-loss:  2.0916606436987393 	 ± 0.2580943308929192
	data : 0.11680412292480469
	model : 0.06929636001586914
			 train-loss:  2.089714416679071 	 ± 0.2593550042697531
	data : 0.11674227714538574
	model : 0.06945500373840333
			 train-loss:  2.089081812680252 	 ± 0.25901665819402897
	data : 0.11655011177062988
	model : 0.06951169967651367
			 train-loss:  2.0899049453889793 	 ± 0.25881400264311233
	data : 0.11663279533386231
	model : 0.0695030689239502
			 train-loss:  2.0890507443297293 	 ± 0.25864031839595975
	data : 0.11663942337036133
	model : 0.06949787139892578
			 train-loss:  2.0882180464794358 	 ± 0.25845332283281625
	data : 0.11649832725524903
	model : 0.06933703422546386
			 train-loss:  2.0882247924804687 	 ± 0.25793592021006806
	data : 0.11668996810913086
	model : 0.06914567947387695
			 train-loss:  2.087899818838355 	 ± 0.2574728673081888
	data : 0.11673111915588379
	model : 0.06883797645568848
			 train-loss:  2.0875290996498532 	 ± 0.25702861400813154
	data : 0.11681461334228516
	model : 0.06869621276855468
			 train-loss:  2.0890091987466626 	 ± 0.2575939474493873
	data : 0.11707763671875
	model : 0.06855101585388183
			 train-loss:  2.08742916114687 	 ± 0.25831187018040974
	data : 0.11715421676635743
	model : 0.06866798400878907
			 train-loss:  2.0880648454030353 	 ± 0.25800386725465047
	data : 0.1169206142425537
	model : 0.06869254112243653
			 train-loss:  2.0883101420477033 	 ± 0.25752925185608033
	data : 0.11590595245361328
	model : 0.06002330780029297
#epoch  37    val-loss:  2.446956283167789  train-loss:  2.0883101420477033  lr:  0.000625
			 train-loss:  2.4385364055633545 	 ± 0.0
	data : 5.842508554458618
	model : 0.07616186141967773
			 train-loss:  2.255777597427368 	 ± 0.18275880813598633
	data : 2.987715721130371
	model : 0.07294881343841553
			 train-loss:  2.239536682764689 	 ± 0.15097921242901294
	data : 2.030698617299398
	model : 0.07181493441263835
			 train-loss:  2.1584925651550293 	 ± 0.19183453530751154
	data : 1.5518652200698853
	model : 0.07129639387130737
			 train-loss:  2.1269951343536375 	 ± 0.18278058906151093
	data : 1.2647560596466065
	model : 0.07191314697265624
			 train-loss:  2.0992103219032288 	 ± 0.178046619726172
	data : 0.11860842704772949
	model : 0.06974639892578124
			 train-loss:  2.1133581399917603 	 ± 0.1684425907099152
	data : 0.11601362228393555
	model : 0.06966552734375
			 train-loss:  2.056812807917595 	 ± 0.2172738252184969
	data : 0.11586909294128418
	model : 0.06956181526184083
			 train-loss:  2.035110182232327 	 ± 0.2138471952478459
	data : 0.11624269485473633
	model : 0.06959047317504882
			 train-loss:  2.0301135182380676 	 ± 0.20342630233939032
	data : 0.11616659164428711
	model : 0.0687136173248291
			 train-loss:  2.012038165872747 	 ± 0.20220638079562642
	data : 0.11701807975769044
	model : 0.0687169075012207
			 train-loss:  2.049837032953898 	 ± 0.23064351991127782
	data : 0.11699466705322266
	model : 0.06792702674865722
			 train-loss:  2.0275464057922363 	 ± 0.23466330062278942
	data : 0.11779584884643554
	model : 0.06836166381835937
			 train-loss:  2.0270210334232877 	 ± 0.22613514851337138
	data : 0.11740274429321289
	model : 0.06843242645263672
			 train-loss:  2.0443711916605634 	 ± 0.2279086591282553
	data : 0.11734013557434082
	model : 0.06842303276062012
			 train-loss:  2.05190372467041 	 ± 0.22259165618280335
	data : 0.11736555099487304
	model : 0.0693354606628418
			 train-loss:  2.0276317947051106 	 ± 0.23676684452762262
	data : 0.11660265922546387
	model : 0.06950411796569825
			 train-loss:  2.0085933208465576 	 ± 0.24311736812684878
	data : 0.11641945838928222
	model : 0.0694040298461914
			 train-loss:  2.011228599046406 	 ± 0.23689705389248575
	data : 0.11645545959472656
	model : 0.06932806968688965
			 train-loss:  2.025264227390289 	 ± 0.23886645187831862
	data : 0.11655349731445312
	model : 0.06930370330810547
			 train-loss:  2.0348936149052212 	 ± 0.23705416101068616
	data : 0.11665997505187989
	model : 0.06928501129150391
			 train-loss:  2.0429708415811714 	 ± 0.23454305625658864
	data : 0.11673851013183593
	model : 0.06996273994445801
			 train-loss:  2.040485952211463 	 ± 0.22968354230009144
	data : 0.11610479354858398
	model : 0.06975026130676269
			 train-loss:  2.0438597202301025 	 ± 0.22542896297542162
	data : 0.11616072654724122
	model : 0.06954813003540039
			 train-loss:  2.0408658170700074 	 ± 0.22136081767425714
	data : 0.11624579429626465
	model : 0.06952924728393554
			 train-loss:  2.0322926823909464 	 ± 0.22125423984480344
	data : 0.1161318302154541
	model : 0.06961092948913575
			 train-loss:  2.03688234311563 	 ± 0.2183759097375842
	data : 0.11609196662902832
	model : 0.06961603164672851
			 train-loss:  2.0391282396657124 	 ± 0.21475819687861492
	data : 0.11619648933410645
	model : 0.06972246170043946
			 train-loss:  2.057523953503576 	 ± 0.23239183506032143
	data : 0.11619625091552735
	model : 0.0698361873626709
			 train-loss:  2.0620152831077574 	 ± 0.22976238895012552
	data : 0.11595683097839356
	model : 0.06987142562866211
			 train-loss:  2.0670327025075115 	 ± 0.22769071721588277
	data : 0.1160090446472168
	model : 0.06971235275268554
			 train-loss:  2.0671196915209293 	 ± 0.2241053358946499
	data : 0.11614389419555664
	model : 0.06953201293945313
			 train-loss:  2.0763331218199297 	 ± 0.22675466899085578
	data : 0.11632585525512695
	model : 0.06945033073425293
			 train-loss:  2.0797936600797318 	 ± 0.22427791004134015
	data : 0.1164154052734375
	model : 0.06949610710144043
			 train-loss:  2.079192852973938 	 ± 0.22107848024934834
	data : 0.1165475845336914
	model : 0.06953964233398438
			 train-loss:  2.0935484866301217 	 ± 0.2339465406225057
	data : 0.11635699272155761
	model : 0.06864213943481445
			 train-loss:  2.085840885703628 	 ± 0.2353517030371847
	data : 0.11694893836975098
	model : 0.06905069351196289
			 train-loss:  2.079754606673592 	 ± 0.235166674302294
	data : 0.1166473388671875
	model : 0.06902432441711426
			 train-loss:  2.0816458830466638 	 ± 0.2324247252556553
	data : 0.1167759895324707
	model : 0.06899790763854981
			 train-loss:  2.0734766632318498 	 ± 0.23510302322671217
	data : 0.1167424201965332
	model : 0.06842713356018067
			 train-loss:  2.073545185531058 	 ± 0.23221861868982374
	data : 0.11739187240600586
	model : 0.06922626495361328
			 train-loss:  2.0800577714329673 	 ± 0.23319629388789245
	data : 0.11687922477722168
	model : 0.0681617259979248
			 train-loss:  2.083318081012992 	 ± 0.23143528772579644
	data : 0.1177905559539795
	model : 0.06807999610900879
			 train-loss:  2.084498267282139 	 ± 0.22892107740753612
	data : 0.117962646484375
	model : 0.06797928810119629
			 train-loss:  2.086321245299445 	 ± 0.22668597264394563
	data : 0.11822319030761719
	model : 0.06850881576538086
			 train-loss:  2.0828745287397634 	 ± 0.22539748265672563
	data : 0.1177100658416748
	model : 0.06779632568359376
			 train-loss:  2.082211157108875 	 ± 0.22303213071262223
	data : 0.118206787109375
	model : 0.06873884201049804
			 train-loss:  2.0764037842551866 	 ± 0.22425903362451644
	data : 0.11733756065368653
	model : 0.06902613639831542
			 train-loss:  2.073404545686683 	 ± 0.2229294175764625
	data : 0.1169196605682373
	model : 0.06934113502502441
			 train-loss:  2.070522818565369 	 ± 0.2216088624704467
	data : 0.11647310256958007
	model : 0.0691960334777832
			 train-loss:  2.0664379970700133 	 ± 0.22131838042570035
	data : 0.11660351753234863
	model : 0.07012124061584472
			 train-loss:  2.0699270275922923 	 ± 0.22059172097915114
	data : 0.11580162048339844
	model : 0.07002754211425781
			 train-loss:  2.07256318488211 	 ± 0.2193261172862046
	data : 0.11578302383422852
	model : 0.06896991729736328
			 train-loss:  2.0729271924054182 	 ± 0.21730198915892032
	data : 0.11668171882629394
	model : 0.06891803741455078
			 train-loss:  2.071299934387207 	 ± 0.21564924422878895
	data : 0.11687684059143066
	model : 0.06913824081420898
			 train-loss:  2.07453196815082 	 ± 0.21505508512073157
	data : 0.11641459465026856
	model : 0.06930770874023437
			 train-loss:  2.0737786753135814 	 ± 0.21323481498543767
	data : 0.11628947257995606
	model : 0.06859040260314941
			 train-loss:  2.069563764950325 	 ± 0.2137703635020769
	data : 0.11705703735351562
	model : 0.06942296028137207
			 train-loss:  2.071956236483687 	 ± 0.21273273661580216
	data : 0.11641111373901367
	model : 0.06952142715454102
			 train-loss:  2.0753210802872974 	 ± 0.21252993168756218
	data : 0.11647329330444336
	model : 0.06948132514953613
			 train-loss:  2.076370041878497 	 ± 0.21093723270141954
	data : 0.11664948463439942
	model : 0.06920456886291504
			 train-loss:  2.0735284859134304 	 ± 0.2104029550934753
	data : 0.11699728965759278
	model : 0.07046680450439453
			 train-loss:  2.071331366660103 	 ± 0.20944213810563111
	data : 0.11597743034362792
	model : 0.06981940269470215
			 train-loss:  2.0668319761753082 	 ± 0.21084593249235123
	data : 0.11659131050109864
	model : 0.06955895423889161
			 train-loss:  2.072117541386531 	 ± 0.2134480017476103
	data : 0.11661667823791504
	model : 0.0685244083404541
			 train-loss:  2.0767675240834556 	 ± 0.21511670718661524
	data : 0.1174154281616211
	model : 0.06761708259582519
			 train-loss:  2.0691990941318115 	 ± 0.22218253267457255
	data : 0.11816873550415039
	model : 0.06705102920532227
			 train-loss:  2.0733390503069935 	 ± 0.22313101462305018
	data : 0.11846990585327148
	model : 0.06859774589538574
			 train-loss:  2.0691597859064736 	 ± 0.22417314180801864
	data : 0.11684813499450683
	model : 0.06873927116394044
			 train-loss:  2.0700082557541983 	 ± 0.2226777091379842
	data : 0.11691098213195801
	model : 0.06958346366882324
			 train-loss:  2.068977690078843 	 ± 0.22127205281592813
	data : 0.11627168655395508
	model : 0.07066922187805176
			 train-loss:  2.069719700349702 	 ± 0.21981900311705402
	data : 0.11512513160705566
	model : 0.07071833610534668
			 train-loss:  2.0731013425408977 	 ± 0.22018589238542102
	data : 0.1150691032409668
	model : 0.06893649101257324
			 train-loss:  2.068589835553556 	 ± 0.22206415094546644
	data : 0.1167060375213623
	model : 0.06882338523864746
			 train-loss:  2.069510021209717 	 ± 0.22072074254169227
	data : 0.11675047874450684
	model : 0.06895976066589356
			 train-loss:  2.0681784435322412 	 ± 0.21956686216531382
	data : 0.11680088043212891
	model : 0.06799583435058594
			 train-loss:  2.066843741899961 	 ± 0.21844655348942235
	data : 0.11778712272644043
	model : 0.06783595085144042
			 train-loss:  2.0647612565603013 	 ± 0.2178096540186029
	data : 0.11800670623779297
	model : 0.06883764266967773
			 train-loss:  2.0667178178135353 	 ± 0.2171154534182717
	data : 0.1171797275543213
	model : 0.06799960136413574
			 train-loss:  2.0674282878637316 	 ± 0.21584660701362615
	data : 0.11788887977600097
	model : 0.06805653572082519
			 train-loss:  2.067806856131848 	 ± 0.21453680495296268
	data : 0.11789321899414062
	model : 0.06904158592224122
			 train-loss:  2.0739271320947785 	 ± 0.22022449343173606
	data : 0.11699957847595215
	model : 0.06932787895202637
			 train-loss:  2.073612710079515 	 ± 0.2189123361142255
	data : 0.11657228469848632
	model : 0.0694643497467041
			 train-loss:  2.075437693368821 	 ± 0.21823963994391915
	data : 0.1165048599243164
	model : 0.06940722465515137
			 train-loss:  2.076467171837302 	 ± 0.2171571551894367
	data : 0.11633868217468261
	model : 0.06928033828735351
			 train-loss:  2.074632949607317 	 ± 0.21655221475831105
	data : 0.11644883155822754
	model : 0.06891446113586426
			 train-loss:  2.0779749196151207 	 ± 0.21752322630360657
	data : 0.11673913002014161
	model : 0.06856307983398438
			 train-loss:  2.078484516252171 	 ± 0.21633599113545043
	data : 0.11711635589599609
	model : 0.06858043670654297
			 train-loss:  2.076817055766502 	 ± 0.21568514447726855
	data : 0.11699562072753907
	model : 0.06956658363342286
			 train-loss:  2.076139827569326 	 ± 0.2145786814619848
	data : 0.11629195213317871
	model : 0.07012510299682617
			 train-loss:  2.0769191482564904 	 ± 0.21352445542886647
	data : 0.11582670211791993
	model : 0.0703235149383545
			 train-loss:  2.0736473202705383 	 ± 0.2146421729928989
	data : 0.11573424339294433
	model : 0.06965317726135253
			 train-loss:  2.076441998122841 	 ± 0.21516136536091282
	data : 0.11645383834838867
	model : 0.0692403793334961
			 train-loss:  2.076661426970299 	 ± 0.21402429125194522
	data : 0.11689906120300293
	model : 0.06921300888061524
			 train-loss:  2.0808020566639147 	 ± 0.21664680284674454
	data : 0.11686601638793945
	model : 0.06874122619628906
			 train-loss:  2.0839412435889244 	 ± 0.21767659312686077
	data : 0.11718974113464356
	model : 0.0687981128692627
			 train-loss:  2.0823669667096483 	 ± 0.21710028792033914
	data : 0.1170644760131836
	model : 0.06972460746765137
			 train-loss:  2.085818883107633 	 ± 0.21864907107430814
	data : 0.11615443229675293
	model : 0.06994290351867676
			 train-loss:  2.08130465854298 	 ± 0.22208462121917671
	data : 0.11594514846801758
	model : 0.06994256973266602
			 train-loss:  2.08803640127182 	 ± 0.2308997246616084
	data : 0.11598882675170899
	model : 0.06985740661621094
			 train-loss:  2.084208107230687 	 ± 0.23292144218239388
	data : 0.11598753929138184
	model : 0.06963167190551758
			 train-loss:  2.084638853867849 	 ± 0.23181728102191573
	data : 0.11645612716674805
	model : 0.06954479217529297
			 train-loss:  2.0898657689974147 	 ± 0.23665210105249052
	data : 0.11650304794311524
	model : 0.06983227729797363
			 train-loss:  2.0904360837661304 	 ± 0.2355827169319005
	data : 0.11620216369628907
	model : 0.06985058784484863
			 train-loss:  2.0945405971436273 	 ± 0.2381653704404918
	data : 0.11593332290649414
	model : 0.07001123428344727
			 train-loss:  2.0937137187651866 	 ± 0.23719067198822372
	data : 0.11585025787353516
	model : 0.07107453346252442
			 train-loss:  2.0919529622960313 	 ± 0.23677469009283675
	data : 0.11458196640014648
	model : 0.07100520133972169
			 train-loss:  2.0884236020070537 	 ± 0.23848688018131886
	data : 0.11459627151489257
	model : 0.07053236961364746
			 train-loss:  2.0870043115878323 	 ± 0.23784816067917544
	data : 0.11513199806213378
	model : 0.06989407539367676
			 train-loss:  2.084904863617637 	 ± 0.2377769884544183
	data : 0.11602625846862794
	model : 0.06986174583435059
			 train-loss:  2.0849150301099897 	 ± 0.2367035217127584
	data : 0.11602354049682617
	model : 0.06909246444702148
			 train-loss:  2.0842917561531067 	 ± 0.23573591672359168
	data : 0.11690177917480468
	model : 0.0692366123199463
			 train-loss:  2.083803744442695 	 ± 0.23474733937193923
	data : 0.11682515144348145
	model : 0.06937499046325683
			 train-loss:  2.0820921774496113 	 ± 0.2344225965778284
	data : 0.11637482643127442
	model : 0.06992735862731933
			 train-loss:  2.0812911126924596 	 ± 0.2335578036793708
	data : 0.11585035324096679
	model : 0.0697244644165039
			 train-loss:  2.0822328503789573 	 ± 0.23276809394675435
	data : 0.11607770919799805
	model : 0.06878952980041504
			 train-loss:  2.0847171117097902 	 ± 0.2333105238061219
	data : 0.11688404083251953
	model : 0.06890549659729003
			 train-loss:  2.0873645655179427 	 ± 0.23407808844361752
	data : 0.11687054634094238
	model : 0.06892261505126954
			 train-loss:  2.0890988832762263 	 ± 0.2338525970231964
	data : 0.11691117286682129
	model : 0.06809501647949219
			 train-loss:  2.088117895523707 	 ± 0.23312192072934684
	data : 0.11763415336608887
	model : 0.06732864379882812
			 train-loss:  2.087930364057052 	 ± 0.23216569729478279
	data : 0.11831860542297364
	model : 0.06820082664489746
			 train-loss:  2.0874538832023495 	 ± 0.23127163978547108
	data : 0.11746411323547364
	model : 0.06818304061889649
			 train-loss:  2.0880803383462796 	 ± 0.23043350366885887
	data : 0.11755027770996093
	model : 0.06818389892578125
			 train-loss:  2.0873927820113396 	 ± 0.2296290993806491
	data : 0.11765580177307129
	model : 0.06917591094970703
			 train-loss:  2.0840118656158446 	 ± 0.2317867143725059
	data : 0.11664271354675293
	model : 0.07003817558288575
			 train-loss:  2.0814854379684204 	 ± 0.23258664194313663
	data : 0.11591191291809082
	model : 0.07013192176818847
			 train-loss:  2.0824105326584945 	 ± 0.23190174595843582
	data : 0.11595754623413086
	model : 0.07008180618286133
			 train-loss:  2.0824601221829653 	 ± 0.2309947795566165
	data : 0.11605949401855468
	model : 0.06922383308410644
			 train-loss:  2.0809871383415635 	 ± 0.23070040081174423
	data : 0.11690926551818848
	model : 0.06952943801879882
			 train-loss:  2.0781397599440354 	 ± 0.2320757321763472
	data : 0.1167642593383789
	model : 0.06964712142944336
			 train-loss:  2.079977781718014 	 ± 0.23213614340639985
	data : 0.1166903018951416
	model : 0.06954355239868164
			 train-loss:  2.079231321811676 	 ± 0.2314129346169802
	data : 0.1167900562286377
	model : 0.06951093673706055
			 train-loss:  2.08022413755718 	 ± 0.23082333128390017
	data : 0.11663231849670411
	model : 0.07049455642700195
			 train-loss:  2.081844829801303 	 ± 0.230718760163272
	data : 0.1157674789428711
	model : 0.07014427185058594
			 train-loss:  2.0839258547182435 	 ± 0.23112150608369358
	data : 0.11593742370605468
	model : 0.07017431259155274
			 train-loss:  2.088536623646231 	 ± 0.2364198977939285
	data : 0.11592612266540528
	model : 0.07014236450195313
			 train-loss:  2.088266447512773 	 ± 0.23557654238957793
	data : 0.11589202880859376
	model : 0.0704373836517334
			 train-loss:  2.0861953185952227 	 ± 0.23596998102250538
	data : 0.11578445434570313
	model : 0.07019476890563965
			 train-loss:  2.086977008435366 	 ± 0.23529888781631533
	data : 0.11593446731567383
	model : 0.06989045143127441
			 train-loss:  2.0847971848079134 	 ± 0.23586134696085467
	data : 0.1162557601928711
	model : 0.06966724395751953
			 train-loss:  2.084419093233474 	 ± 0.23506604467375677
	data : 0.11644158363342286
	model : 0.06970210075378418
			 train-loss:  2.084950991079841 	 ± 0.2343220206010299
	data : 0.11634573936462403
	model : 0.06848268508911133
			 train-loss:  2.0834164361020067 	 ± 0.23421621551899569
	data : 0.11741061210632324
	model : 0.06786689758300782
			 train-loss:  2.085188543630971 	 ± 0.23436158984660688
	data : 0.1180917739868164
	model : 0.06816802024841309
			 train-loss:  2.087789468929685 	 ± 0.23562829251191997
	data : 0.11768565177917481
	model : 0.06770224571228027
			 train-loss:  2.085607436421799 	 ± 0.23628541442217274
	data : 0.11797595024108887
	model : 0.06683406829833985
			 train-loss:  2.08539718592248 	 ± 0.23549405448510705
	data : 0.11876826286315918
	model : 0.06702327728271484
			 train-loss:  2.084889502944173 	 ± 0.2347778210879942
	data : 0.11861658096313477
	model : 0.06787033081054687
			 train-loss:  2.0849123009099255 	 ± 0.2339888140810084
	data : 0.11779656410217285
	model : 0.06776089668273926
			 train-loss:  2.083448328177134 	 ± 0.23389121236690566
	data : 0.11793866157531738
	model : 0.06911935806274414
			 train-loss:  2.0817622554223267 	 ± 0.23402828888260963
	data : 0.11657180786132812
	model : 0.07067289352416992
			 train-loss:  2.083342145932348 	 ± 0.2340637097192628
	data : 0.11501431465148926
	model : 0.07128267288208008
			 train-loss:  2.0853937996758356 	 ± 0.23466476936243763
	data : 0.11437196731567383
	model : 0.07121381759643555
			 train-loss:  2.087898263683567 	 ± 0.2359441469402815
	data : 0.11446661949157715
	model : 0.07145104408264161
			 train-loss:  2.085824936436069 	 ± 0.2365850357257447
	data : 0.11432385444641113
	model : 0.0708545207977295
			 train-loss:  2.086593353595489 	 ± 0.23601949760666652
	data : 0.11499018669128418
	model : 0.0703664779663086
			 train-loss:  2.087432552295126 	 ± 0.23550001429741418
	data : 0.11565523147583008
	model : 0.07044663429260253
			 train-loss:  2.0877813953387587 	 ± 0.2347942674163968
	data : 0.11559824943542481
	model : 0.07041792869567871
			 train-loss:  2.0883307269534224 	 ± 0.23415658809209594
	data : 0.11573338508605957
	model : 0.07022562026977539
			 train-loss:  2.088645239919424 	 ± 0.2334573892598662
	data : 0.1159477710723877
	model : 0.06998610496520996
			 train-loss:  2.0886343628723427 	 ± 0.23273127765829937
	data : 0.11620206832885742
	model : 0.06904773712158203
			 train-loss:  2.089676714973685 	 ± 0.23238853032066964
	data : 0.11706051826477051
	model : 0.06920561790466309
			 train-loss:  2.090625618864422 	 ± 0.23198918363000692
	data : 0.11691842079162598
	model : 0.06929492950439453
			 train-loss:  2.0913756962229564 	 ± 0.23147899087433885
	data : 0.11674442291259765
	model : 0.0693690299987793
			 train-loss:  2.0921007467038706 	 ± 0.23096319004558888
	data : 0.1167226791381836
	model : 0.06923918724060059
			 train-loss:  2.0946221229541733 	 ± 0.23253302257936947
	data : 0.11676564216613769
	model : 0.06972565650939941
			 train-loss:  2.096968868535436 	 ± 0.23379910644644486
	data : 0.11620583534240722
	model : 0.06877989768981933
			 train-loss:  2.0967713856980916 	 ± 0.23311620687375353
	data : 0.11691865921020508
	model : 0.06871256828308106
			 train-loss:  2.0996261143825463 	 ± 0.23535233301856218
	data : 0.11689038276672363
	model : 0.06867494583129882
			 train-loss:  2.097571135969723 	 ± 0.23617486978836189
	data : 0.1170961856842041
	model : 0.06895370483398437
			 train-loss:  2.0971886386648255 	 ± 0.23553609143582518
	data : 0.11689391136169433
	model : 0.06897177696228027
			 train-loss:  2.096135225406913 	 ± 0.23525404015378965
	data : 0.11686692237854004
	model : 0.0699042797088623
			 train-loss:  2.0960923812292904 	 ± 0.23457380276326417
	data : 0.11604013442993164
	model : 0.06914353370666504
			 train-loss:  2.0954389037757086 	 ± 0.23405664001515072
	data : 0.11687211990356446
	model : 0.0695462703704834
			 train-loss:  2.0956360408238 	 ± 0.23340143526000903
	data : 0.11650648117065429
	model : 0.06959977149963378
			 train-loss:  2.0974625904451716 	 ± 0.23398836629911737
	data : 0.11644620895385742
	model : 0.0700352668762207
			 train-loss:  2.0945642108971114 	 ± 0.23647354333089277
	data : 0.11613759994506836
	model : 0.06987428665161133
			 train-loss:  2.092699545153071 	 ± 0.23710969199293455
	data : 0.116367769241333
	model : 0.07063918113708496
			 train-loss:  2.0926104590879473 	 ± 0.2364494340669508
	data : 0.11554207801818847
	model : 0.0702092170715332
			 train-loss:  2.0926313360532123 	 ± 0.23579188074851873
	data : 0.11588811874389648
	model : 0.07035884857177735
			 train-loss:  2.09137379925554 	 ± 0.23574412523710064
	data : 0.11567339897155762
	model : 0.06922430992126465
			 train-loss:  2.089386236536634 	 ± 0.23661140555770366
	data : 0.11663575172424316
	model : 0.06977782249450684
			 train-loss:  2.088254585943587 	 ± 0.23645740362629392
	data : 0.11633248329162597
	model : 0.0698592185974121
			 train-loss:  2.0869195901829265 	 ± 0.23650450038919338
	data : 0.11632738113403321
	model : 0.07008447647094726
			 train-loss:  2.0864875870781976 	 ± 0.2359372162693386
	data : 0.11624574661254883
	model : 0.06907954216003417
			 train-loss:  2.0868618052492858 	 ± 0.23535716628348785
	data : 0.11720929145812989
	model : 0.07041864395141602
			 train-loss:  2.0886167485446214 	 ± 0.23594411228706202
	data : 0.11601524353027344
	model : 0.07031750679016113
			 train-loss:  2.089663788359216 	 ± 0.23575096134441295
	data : 0.11605224609375
	model : 0.07028894424438477
			 train-loss:  2.0910197868549005 	 ± 0.235860406076985
	data : 0.11613636016845703
	model : 0.06988492012023925
			 train-loss:  2.088506433838292 	 ± 0.2377629981532015
	data : 0.1162865161895752
	model : 0.07069711685180664
			 train-loss:  2.087893733179382 	 ± 0.23729010639625805
	data : 0.11575651168823242
	model : 0.07031927108764649
			 train-loss:  2.089340547720591 	 ± 0.23751451764580708
	data : 0.11616811752319336
	model : 0.06996340751647949
			 train-loss:  2.0908254601177156 	 ± 0.23779025052677508
	data : 0.116314697265625
	model : 0.06992816925048828
			 train-loss:  2.090592551477177 	 ± 0.23719866745478715
	data : 0.11641268730163574
	model : 0.07029223442077637
			 train-loss:  2.0929443469414344 	 ± 0.23884656434842616
	data : 0.11611027717590332
	model : 0.07030816078186035
			 train-loss:  2.0924203268119266 	 ± 0.23834883707775326
	data : 0.11584315299987794
	model : 0.06973624229431152
			 train-loss:  2.091251200830876 	 ± 0.23830588607500042
	data : 0.11636004447937012
	model : 0.06962099075317382
			 train-loss:  2.0920394893848533 	 ± 0.23796069899713126
	data : 0.11643919944763184
	model : 0.06870040893554688
			 train-loss:  2.0930841274596936 	 ± 0.23781677033768367
	data : 0.11703476905822754
	model : 0.068336820602417
			 train-loss:  2.093079014420509 	 ± 0.2372214943361333
	data : 0.11739468574523926
	model : 0.0680610179901123
			 train-loss:  2.095153110537363 	 ± 0.23844169513794816
	data : 0.11762804985046386
	model : 0.0676572322845459
			 train-loss:  2.094701007451161 	 ± 0.2379371097244054
	data : 0.11795234680175781
	model : 0.06683764457702637
			 train-loss:  2.0937608427602084 	 ± 0.2377261678846689
	data : 0.11879119873046876
	model : 0.06781086921691895
			 train-loss:  2.0939649834352383 	 ± 0.23716062596430867
	data : 0.11803746223449707
	model : 0.0670931339263916
			 train-loss:  2.093247454922374 	 ± 0.2368033460139976
	data : 0.11875686645507813
	model : 0.06748290061950683
			 train-loss:  2.095079156380255 	 ± 0.23767922424889915
	data : 0.1187706470489502
	model : 0.06879825592041015
			 train-loss:  2.093411145002946 	 ± 0.238309996060495
	data : 0.11757965087890625
	model : 0.06883893013000489
			 train-loss:  2.09231970172662 	 ± 0.23825449806304294
	data : 0.11764602661132813
	model : 0.06826090812683105
			 train-loss:  2.0921678794057748 	 ± 0.23769391324790415
	data : 0.11815495491027832
	model : 0.06930418014526367
			 train-loss:  2.0914188708577837 	 ± 0.2373744053125899
	data : 0.1173546314239502
	model : 0.06834540367126465
			 train-loss:  2.091352762769184 	 ± 0.23681317640541066
	data : 0.11807785034179688
	model : 0.06789598464965821
			 train-loss:  2.092224696334803 	 ± 0.23659325123273334
	data : 0.11840639114379883
	model : 0.06867537498474122
			 train-loss:  2.091675340849469 	 ± 0.23617270475916813
	data : 0.11756362915039062
	model : 0.06893014907836914
			 train-loss:  2.0902263126640674 	 ± 0.2365674029616434
	data : 0.1172724723815918
	model : 0.06870102882385254
			 train-loss:  2.0909908061803772 	 ± 0.23628142163186808
	data : 0.11724491119384765
	model : 0.06945633888244629
			 train-loss:  2.091886772049798 	 ± 0.23609963007575166
	data : 0.11653327941894531
	model : 0.06952667236328125
			 train-loss:  2.093714120750603 	 ± 0.23708104707474492
	data : 0.11645145416259765
	model : 0.06963958740234374
			 train-loss:  2.092775923943301 	 ± 0.23694007035975456
	data : 0.11640653610229493
	model : 0.07062973976135253
			 train-loss:  2.092020521425221 	 ± 0.23666145687581042
	data : 0.11527385711669921
	model : 0.0707204818725586
			 train-loss:  2.0926056320017032 	 ± 0.23628168788650453
	data : 0.11515746116638184
	model : 0.07067756652832032
			 train-loss:  2.0930830731111416 	 ± 0.23585284598837314
	data : 0.11524419784545899
	model : 0.07027978897094726
			 train-loss:  2.0945896909043595 	 ± 0.23638452175220592
	data : 0.1157522201538086
	model : 0.0701174259185791
			 train-loss:  2.0941740867802916 	 ± 0.23593519255239556
	data : 0.11583600044250489
	model : 0.0686333179473877
			 train-loss:  2.094366068286555 	 ± 0.23542541888210378
	data : 0.11728835105895996
	model : 0.06792211532592773
			 train-loss:  2.0941295454237197 	 ± 0.23492834056518783
	data : 0.11781282424926758
	model : 0.06736264228820801
			 train-loss:  2.0945323378638885 	 ± 0.23448586352035752
	data : 0.11825556755065918
	model : 0.06707983016967774
			 train-loss:  2.09390144096072 	 ± 0.23416096297992753
	data : 0.11852517127990722
	model : 0.06695327758789063
			 train-loss:  2.0929494238736335 	 ± 0.2340867500967536
	data : 0.11851410865783692
	model : 0.06713862419128418
			 train-loss:  2.0919113237264373 	 ± 0.2341004585985319
	data : 0.11847825050354004
	model : 0.0673072338104248
			 train-loss:  2.092868045620296 	 ± 0.2340392237182897
	data : 0.11843109130859375
	model : 0.06790261268615723
			 train-loss:  2.092891227115284 	 ± 0.23353236050389103
	data : 0.11777334213256836
	model : 0.06795248985290528
			 train-loss:  2.0920712105159103 	 ± 0.23336156334931774
	data : 0.11779580116271973
	model : 0.06759891510009766
			 train-loss:  2.0919656180516846 	 ± 0.23286580321585792
	data : 0.11829299926757812
	model : 0.06749811172485351
			 train-loss:  2.090429783376873 	 ± 0.23354730322154293
	data : 0.11807575225830078
	model : 0.06720447540283203
			 train-loss:  2.089551040974069 	 ± 0.23343721060864953
	data : 0.11820645332336426
	model : 0.06687641143798828
			 train-loss:  2.0902980727664495 	 ± 0.23322343862209463
	data : 0.1183244228363037
	model : 0.0665095329284668
			 train-loss:  2.0907633867947863 	 ± 0.23284063930661286
	data : 0.1184159278869629
	model : 0.06655058860778809
			 train-loss:  2.0917613816862346 	 ± 0.23285837051938912
	data : 0.11806540489196778
	model : 0.06685457229614258
			 train-loss:  2.0911267686588495 	 ± 0.2325768620833908
	data : 0.11793017387390137
	model : 0.06749019622802735
			 train-loss:  2.091833573083083 	 ± 0.23234889989571272
	data : 0.11751179695129395
	model : 0.06674060821533204
			 train-loss:  2.093643352203844 	 ± 0.23355529334400177
	data : 0.11819515228271485
	model : 0.06742453575134277
			 train-loss:  2.0933318842541087 	 ± 0.23312239219041106
	data : 0.1174741268157959
	model : 0.06695528030395508
			 train-loss:  2.0954586198791065 	 ± 0.23498291536760832
	data : 0.11803174018859863
	model : 0.06674518585205078
			 train-loss:  2.0940272798303696 	 ± 0.23556000063382534
	data : 0.11824498176574708
	model : 0.0668684959411621
			 train-loss:  2.093276848111834 	 ± 0.2353708525346129
	data : 0.11810932159423829
	model : 0.06790552139282227
			 train-loss:  2.093775614490354 	 ± 0.2350216700435699
	data : 0.11757035255432129
	model : 0.06808576583862305
			 train-loss:  2.0920696895614808 	 ± 0.23606665457875664
	data : 0.11761746406555176
	model : 0.06888599395751953
			 train-loss:  2.093140289668114 	 ± 0.23619031621878858
	data : 0.1171492576599121
	model : 0.06928119659423829
			 train-loss:  2.0923076619106125 	 ± 0.23607997916558504
	data : 0.11682677268981934
	model : 0.06924233436584473
			 train-loss:  2.090508451461792 	 ± 0.23731176527538364
	data : 0.1168372631072998
	model : 0.0692683219909668
			 train-loss:  2.09187061283218 	 ± 0.2378158422438949
	data : 0.11662631034851074
	model : 0.06919565200805664
			 train-loss:  2.090457404416705 	 ± 0.23839721354867516
	data : 0.11667733192443848
	model : 0.06915245056152344
			 train-loss:  2.0929162417475884 	 ± 0.24110610976575883
	data : 0.11665048599243164
	model : 0.06881566047668457
			 train-loss:  2.092597495852493 	 ± 0.24068442812750598
	data : 0.1167862892150879
	model : 0.06796512603759766
			 train-loss:  2.092495020698099 	 ± 0.24021758618332625
	data : 0.11739640235900879
	model : 0.06715540885925293
			 train-loss:  2.092230360955 	 ± 0.23978519965067877
	data : 0.117071533203125
	model : 0.05786242485046387
#epoch  38    val-loss:  2.4869637740285775  train-loss:  2.092230360955  lr:  0.000625
			 train-loss:  2.10050892829895 	 ± 0.0
	data : 5.671478748321533
	model : 0.07425951957702637
			 train-loss:  2.2845739126205444 	 ± 0.18406498432159424
	data : 2.9013562202453613
	model : 0.07168471813201904
			 train-loss:  2.203899542490641 	 ± 0.18868842119727178
	data : 1.9756522178649902
	model : 0.06975515683492024
			 train-loss:  2.163017690181732 	 ± 0.1780911782348066
	data : 1.5116692185401917
	model : 0.0689191222190857
			 train-loss:  2.123008370399475 	 ± 0.17825867970397669
	data : 1.2330989837646484
	model : 0.0689727783203125
			 train-loss:  2.1271876295407615 	 ± 0.16299528145426248
	data : 0.12201867103576661
	model : 0.06803011894226074
			 train-loss:  2.118559922490801 	 ± 0.15237694729734388
	data : 0.11895790100097656
	model : 0.06825070381164551
			 train-loss:  2.0842888057231903 	 ± 0.1689318158923737
	data : 0.11727395057678222
	model : 0.06912717819213868
			 train-loss:  2.0859274864196777 	 ± 0.15933786852826
	data : 0.11649799346923828
	model : 0.06940245628356934
			 train-loss:  2.0682496070861816 	 ± 0.16019446766971387
	data : 0.11633000373840333
	model : 0.06973385810852051
			 train-loss:  2.0592525005340576 	 ± 0.15536670598471805
	data : 0.11614618301391602
	model : 0.06975889205932617
			 train-loss:  2.0357444485028586 	 ± 0.16794689388160938
	data : 0.11625871658325196
	model : 0.06953201293945313
			 train-loss:  2.044591995385977 	 ± 0.1642431235807284
	data : 0.11650738716125489
	model : 0.06891379356384278
			 train-loss:  2.0322926385062083 	 ± 0.16436400111242297
	data : 0.1171454906463623
	model : 0.06918911933898926
			 train-loss:  2.0305519739786786 	 ± 0.1589242229896664
	data : 0.1169461727142334
	model : 0.06898312568664551
			 train-loss:  2.049082174897194 	 ± 0.16979068600271688
	data : 0.11704587936401367
	model : 0.06985259056091309
			 train-loss:  2.040131919524249 	 ± 0.16856682323599956
	data : 0.1160510540008545
	model : 0.07013764381408691
			 train-loss:  2.0370184911621942 	 ± 0.16431969985647427
	data : 0.1157259464263916
	model : 0.07075643539428711
			 train-loss:  2.041596381287826 	 ± 0.1611120349605398
	data : 0.11513276100158691
	model : 0.07079625129699707
			 train-loss:  2.04238538146019 	 ± 0.1570702432761779
	data : 0.11515607833862304
	model : 0.07062726020812989
			 train-loss:  2.0391865911937894 	 ± 0.15395094693525402
	data : 0.11539349555969239
	model : 0.06985483169555665
			 train-loss:  2.0337975404479285 	 ± 0.1524252601197246
	data : 0.11630206108093262
	model : 0.06962499618530274
			 train-loss:  2.03761523184569 	 ± 0.15014644310984418
	data : 0.1164135456085205
	model : 0.06956949234008789
			 train-loss:  2.0285483300685883 	 ± 0.1532821686508825
	data : 0.11629023551940917
	model : 0.06974506378173828
			 train-loss:  2.0266356658935547 	 ± 0.1504772578138632
	data : 0.11608691215515136
	model : 0.06940951347351074
			 train-loss:  2.044495995228107 	 ± 0.17247402419134078
	data : 0.1166527271270752
	model : 0.06856894493103027
			 train-loss:  2.033278875880771 	 ± 0.1786531709858627
	data : 0.1172818660736084
	model : 0.06885347366333008
			 train-loss:  2.0388656045709337 	 ± 0.17781950327981702
	data : 0.11718854904174805
	model : 0.0682558536529541
			 train-loss:  2.0282056783807687 	 ± 0.18360607667122864
	data : 0.11780276298522949
	model : 0.06775894165039062
			 train-loss:  2.028743294874827 	 ± 0.18054325499519555
	data : 0.11808013916015625
	model : 0.0682586669921875
			 train-loss:  2.0213689304167226 	 ± 0.18214232040052347
	data : 0.11752004623413086
	model : 0.06907448768615723
			 train-loss:  2.022448670119047 	 ± 0.17937452808558774
	data : 0.11686973571777344
	model : 0.06799297332763672
			 train-loss:  2.0124690532684326 	 ± 0.1854378135618603
	data : 0.11770210266113282
	model : 0.06858644485473633
			 train-loss:  2.013914753408993 	 ± 0.18287910374276084
	data : 0.11722521781921387
	model : 0.06895647048950196
			 train-loss:  2.013115893091474 	 ± 0.1803077920434868
	data : 0.11688036918640136
	model : 0.06802043914794922
			 train-loss:  2.0066904756757946 	 ± 0.18180436995135332
	data : 0.11754274368286133
	model : 0.06802749633789062
			 train-loss:  2.0220776441934945 	 ± 0.20170039070914306
	data : 0.11757488250732422
	model : 0.06898589134216308
			 train-loss:  2.0273331780182686 	 ± 0.201579772348551
	data : 0.11669087409973145
	model : 0.06800279617309571
			 train-loss:  2.0234966461475077 	 ± 0.20037918536501204
	data : 0.11743497848510742
	model : 0.06789145469665528
			 train-loss:  2.0171696811914446 	 ± 0.20176523837374014
	data : 0.11758413314819335
	model : 0.06797327995300292
			 train-loss:  2.0101902513969234 	 ± 0.2041195763045904
	data : 0.11771483421325683
	model : 0.06795501708984375
			 train-loss:  2.006826556864239 	 ± 0.20282177996053163
	data : 0.11766815185546875
	model : 0.06800398826599122
			 train-loss:  2.003382976665053 	 ± 0.20168801117552598
	data : 0.11793169975280762
	model : 0.06919951438903808
			 train-loss:  2.004952224818143 	 ± 0.19964829535452672
	data : 0.1170271873474121
	model : 0.06917095184326172
			 train-loss:  2.011170408460829 	 ± 0.20168037090310661
	data : 0.11709299087524414
	model : 0.06998295783996582
			 train-loss:  2.009877585846445 	 ± 0.19966458386444458
	data : 0.11623787879943848
	model : 0.06948041915893555
			 train-loss:  2.0045093703777233 	 ± 0.20085654171158676
	data : 0.1166473388671875
	model : 0.06938896179199219
			 train-loss:  2.0104954466223717 	 ± 0.20294585483981933
	data : 0.11662893295288086
	model : 0.06904635429382325
			 train-loss:  2.0066600016185214 	 ± 0.20261436081947518
	data : 0.1169283390045166
	model : 0.06901545524597168
			 train-loss:  2.0066994643211364 	 ± 0.20057817412175924
	data : 0.11691641807556152
	model : 0.06892056465148926
			 train-loss:  2.0101687323813344 	 ± 0.200111319016681
	data : 0.11683158874511719
	model : 0.06847853660583496
			 train-loss:  2.001682224181982 	 ± 0.2072378213146238
	data : 0.11724991798400879
	model : 0.06760139465332031
			 train-loss:  2.01483135178404 	 ± 0.22611492889136744
	data : 0.11799960136413574
	model : 0.0677821159362793
			 train-loss:  2.017588476339976 	 ± 0.2249089570737517
	data : 0.11780390739440919
	model : 0.06783471107482911
			 train-loss:  2.015479889782992 	 ± 0.22339297344589765
	data : 0.11800928115844726
	model : 0.06802291870117187
			 train-loss:  2.016337356397084 	 ± 0.221480719052688
	data : 0.11795158386230468
	model : 0.06893305778503418
			 train-loss:  2.0192360376056873 	 ± 0.22059838911837767
	data : 0.11716618537902831
	model : 0.0698307991027832
			 train-loss:  2.0271113535453535 	 ± 0.22662699554831947
	data : 0.11633000373840333
	model : 0.06980185508728028
			 train-loss:  2.024649741285938 	 ± 0.22547891949023308
	data : 0.11627407073974609
	model : 0.0699429988861084
			 train-loss:  2.0310350894927978 	 ± 0.2289082459135598
	data : 0.11604824066162109
	model : 0.06995921134948731
			 train-loss:  2.0261398886070876 	 ± 0.23016899181913647
	data : 0.11609897613525391
	model : 0.07002496719360352
			 train-loss:  2.027925879724564 	 ± 0.22873097809057782
	data : 0.11601858139038086
	model : 0.06946182250976562
			 train-loss:  2.033984978993734 	 ± 0.23186980078442124
	data : 0.11644988059997559
	model : 0.06943626403808593
			 train-loss:  2.0381558425724506 	 ± 0.23242096007838992
	data : 0.11647758483886719
	model : 0.06945066452026367
			 train-loss:  2.0359107182576106 	 ± 0.23132451391022996
	data : 0.11646270751953125
	model : 0.06857776641845703
			 train-loss:  2.035937791520899 	 ± 0.22956547036812933
	data : 0.1174154281616211
	model : 0.0686368465423584
			 train-loss:  2.0422224304569303 	 ± 0.23349628429349936
	data : 0.11738753318786621
	model : 0.06818180084228516
			 train-loss:  2.041243099114474 	 ± 0.23191162438428573
	data : 0.11766085624694825
	model : 0.06807489395141601
			 train-loss:  2.047626331232596 	 ± 0.23616570896082076
	data : 0.11774907112121583
	model : 0.0679934024810791
			 train-loss:  2.0438506603240967 	 ± 0.2365610066511867
	data : 0.11774053573608398
	model : 0.06882858276367188
			 train-loss:  2.0449451695025807 	 ± 0.23506761100951812
	data : 0.11682748794555664
	model : 0.06872406005859374
			 train-loss:  2.050983491871092 	 ± 0.23891019520428344
	data : 0.11703100204467773
	model : 0.0691291332244873
			 train-loss:  2.0530560800473983 	 ± 0.2379190505461766
	data : 0.11680450439453124
	model : 0.0691676139831543
			 train-loss:  2.0515176653862 	 ± 0.23667130434966127
	data : 0.11671452522277832
	model : 0.06899547576904297
			 train-loss:  2.052635677655538 	 ± 0.23528484571829228
	data : 0.11689233779907227
	model : 0.06899533271789551
			 train-loss:  2.06277271164091 	 ± 0.24967480975440862
	data : 0.11686944961547852
	model : 0.06818218231201172
			 train-loss:  2.0598215840079566 	 ± 0.2493788850185156
	data : 0.11741342544555664
	model : 0.06828441619873046
			 train-loss:  2.0583275021650853 	 ± 0.24812176124785476
	data : 0.11745634078979492
	model : 0.06830029487609864
			 train-loss:  2.0563757042341595 	 ± 0.24714824352712617
	data : 0.11749701499938965
	model : 0.06843352317810059
			 train-loss:  2.056825278699398 	 ± 0.24563121413787306
	data : 0.11735258102416993
	model : 0.0675346851348877
			 train-loss:  2.0566737519370184 	 ± 0.24411402546782604
	data : 0.11811237335205078
	model : 0.06838903427124024
			 train-loss:  2.0575928266455485 	 ± 0.24276192136407973
	data : 0.11745710372924804
	model : 0.06829280853271484
			 train-loss:  2.061266346150134 	 ± 0.24357725516247508
	data : 0.11754517555236817
	model : 0.06920523643493652
			 train-loss:  2.0599340541022166 	 ± 0.24242709485691333
	data : 0.11671533584594726
	model : 0.06938824653625489
			 train-loss:  2.0594647014842313 	 ± 0.24103522259003476
	data : 0.11646461486816406
	model : 0.06941418647766114
			 train-loss:  2.059907264487688 	 ± 0.23966449238503731
	data : 0.11629366874694824
	model : 0.06866374015808105
			 train-loss:  2.061223668613653 	 ± 0.23859564294443392
	data : 0.11696949005126953
	model : 0.06928095817565919
			 train-loss:  2.057542234659195 	 ± 0.23970832330970487
	data : 0.11630415916442871
	model : 0.06860227584838867
			 train-loss:  2.0610977906859325 	 ± 0.2406801934239612
	data : 0.11707482337951661
	model : 0.06849441528320313
			 train-loss:  2.0579277435938517 	 ± 0.2412005417489935
	data : 0.11727700233459473
	model : 0.06843714714050293
			 train-loss:  2.058239363052033 	 ± 0.23988981958905994
	data : 0.11755690574645997
	model : 0.06916804313659668
			 train-loss:  2.054322783065879 	 ± 0.24149020021114143
	data : 0.11687884330749512
	model : 0.06831011772155762
			 train-loss:  2.055703128537824 	 ± 0.24055298638374212
	data : 0.11774311065673829
	model : 0.06809701919555664
			 train-loss:  2.0560321490815343 	 ± 0.2392910653524211
	data : 0.11776928901672364
	model : 0.06824841499328613
			 train-loss:  2.055443473866111 	 ± 0.2380967229574507
	data : 0.11759004592895508
	model : 0.06931080818176269
			 train-loss:  2.054866394648949 	 ± 0.2369201660358793
	data : 0.11674203872680664
	model : 0.06908674240112304
			 train-loss:  2.0536160419896707 	 ± 0.23601393585918495
	data : 0.11692328453063965
	model : 0.06972661018371581
			 train-loss:  2.05081823164103 	 ± 0.23641800970729032
	data : 0.11631755828857422
	model : 0.06976885795593261
			 train-loss:  2.055780179572828 	 ± 0.2402951219406282
	data : 0.11637663841247559
	model : 0.06962027549743652
			 train-loss:  2.057308714389801 	 ± 0.2395738583864274
	data : 0.11648545265197754
	model : 0.06950750350952148
			 train-loss:  2.057163715362549 	 ± 0.23838930868168345
	data : 0.11659188270568847
	model : 0.06923718452453613
			 train-loss:  2.0595257889990712 	 ± 0.23840266248294076
	data : 0.11694016456604003
	model : 0.0685500144958496
			 train-loss:  2.06393314102321 	 ± 0.24138216586867728
	data : 0.11751017570495606
	model : 0.06851177215576172
			 train-loss:  2.066425683406683 	 ± 0.24154714095886157
	data : 0.11749863624572754
	model : 0.06845817565917969
			 train-loss:  2.063967920484997 	 ± 0.2416972831623543
	data : 0.11768116950988769
	model : 0.06839466094970703
			 train-loss:  2.067591174593512 	 ± 0.243402761609613
	data : 0.11762490272521972
	model : 0.06878070831298828
			 train-loss:  2.066576410676831 	 ± 0.2424878692218224
	data : 0.11730246543884278
	model : 0.06966476440429688
			 train-loss:  2.063129323500174 	 ± 0.2439822467929048
	data : 0.11634726524353027
	model : 0.06978602409362793
			 train-loss:  2.0637251718328633 	 ± 0.24293941261155977
	data : 0.11632099151611328
	model : 0.06964378356933594
			 train-loss:  2.0638826825402 	 ± 0.2418382124701684
	data : 0.11630864143371582
	model : 0.06948347091674804
			 train-loss:  2.065564451990901 	 ± 0.24139167540879716
	data : 0.1163642406463623
	model : 0.0694582462310791
			 train-loss:  2.066547006368637 	 ± 0.24053447670006484
	data : 0.11637582778930664
	model : 0.0685704231262207
			 train-loss:  2.066019271327331 	 ± 0.23953291960657772
	data : 0.11738524436950684
	model : 0.06849884986877441
			 train-loss:  2.063912694914299 	 ± 0.23952907383177918
	data : 0.11739702224731445
	model : 0.0692979335784912
			 train-loss:  2.0697241617285687 	 ± 0.2464252625578874
	data : 0.11673917770385742
	model : 0.069618558883667
			 train-loss:  2.0702863438376067 	 ± 0.2454348400613253
	data : 0.11641130447387696
	model : 0.0696955680847168
			 train-loss:  2.06791655630128 	 ± 0.2457129369222192
	data : 0.11637029647827149
	model : 0.07038712501525879
			 train-loss:  2.0695916838565114 	 ± 0.24533956925904313
	data : 0.11577811241149902
	model : 0.07038135528564453
			 train-loss:  2.071093535222927 	 ± 0.2448506668815511
	data : 0.11586146354675293
	model : 0.07024135589599609
			 train-loss:  2.068893551826477 	 ± 0.24500653386488458
	data : 0.11601471900939941
	model : 0.07028074264526367
			 train-loss:  2.0716172033105016 	 ± 0.24580946703755188
	data : 0.1160743236541748
	model : 0.06956562995910645
			 train-loss:  2.0722975398673387 	 ± 0.24491434253054079
	data : 0.1166450023651123
	model : 0.06994600296020508
			 train-loss:  2.071301853753687 	 ± 0.24416453040237285
	data : 0.11638083457946777
	model : 0.06983685493469238
			 train-loss:  2.0719032441416094 	 ± 0.2432694526822844
	data : 0.1163217544555664
	model : 0.06851954460144043
			 train-loss:  2.0706797819137575 	 ± 0.24267714563493845
	data : 0.11749711036682128
	model : 0.06815972328186035
			 train-loss:  2.072490626857394 	 ± 0.24255863940656056
	data : 0.1176724910736084
	model : 0.0689694881439209
			 train-loss:  2.068872184265317 	 ± 0.24499216767689663
	data : 0.11688747406005859
	model : 0.06859359741210938
			 train-loss:  2.070576193742454 	 ± 0.24478768397741177
	data : 0.1172250747680664
	model : 0.06784377098083497
			 train-loss:  2.0701984518258145 	 ± 0.24387449735077266
	data : 0.11784281730651855
	model : 0.06928062438964844
			 train-loss:  2.072094954894139 	 ± 0.24388778076273568
	data : 0.11646718978881836
	model : 0.06939311027526855
			 train-loss:  2.0748291843720064 	 ± 0.24494708749802716
	data : 0.11669540405273438
	model : 0.06836552619934082
			 train-loss:  2.0753164950645333 	 ± 0.2440812283151653
	data : 0.11757616996765137
	model : 0.06888809204101562
			 train-loss:  2.075198941661003 	 ± 0.2431656491153814
	data : 0.11715097427368164
	model : 0.06881585121154785
			 train-loss:  2.0764323250571293 	 ± 0.24267383859984085
	data : 0.11738090515136719
	model : 0.06821689605712891
			 train-loss:  2.0767181634902956 	 ± 0.24179601654604668
	data : 0.11782627105712891
	model : 0.06840763092041016
			 train-loss:  2.0753569541608585 	 ± 0.24142402918691858
	data : 0.1175269603729248
	model : 0.06947860717773438
			 train-loss:  2.077373820499782 	 ± 0.24168850570137704
	data : 0.1166041374206543
	model : 0.06919918060302735
			 train-loss:  2.076538307943206 	 ± 0.24100972156642356
	data : 0.11673412322998047
	model : 0.0702425479888916
			 train-loss:  2.0777854945162217 	 ± 0.24058773600754876
	data : 0.11577987670898438
	model : 0.06938848495483399
			 train-loss:  2.0773989720003945 	 ± 0.23977026324300932
	data : 0.11661596298217773
	model : 0.06911273002624511
			 train-loss:  2.0767535470056195 	 ± 0.23904052059206482
	data : 0.11675605773925782
	model : 0.06882433891296387
			 train-loss:  2.0779692236806304 	 ± 0.23863435031109992
	data : 0.11703305244445801
	model : 0.06866588592529296
			 train-loss:  2.0779733707854797 	 ± 0.23779850576652437
	data : 0.1170731544494629
	model : 0.06848764419555664
			 train-loss:  2.079077273607254 	 ± 0.23733877476726864
	data : 0.11721439361572265
	model : 0.06940631866455078
			 train-loss:  2.0802221002249883 	 ± 0.2369175886030624
	data : 0.11628541946411133
	model : 0.06862339973449708
			 train-loss:  2.0777901051795644 	 ± 0.23791407924992336
	data : 0.11703243255615234
	model : 0.06789870262145996
			 train-loss:  2.0766121984339083 	 ± 0.23753025855534785
	data : 0.11768784523010253
	model : 0.06804418563842773
			 train-loss:  2.077281458957775 	 ± 0.23686545973852136
	data : 0.11755738258361817
	model : 0.06824870109558105
			 train-loss:  2.080935615821173 	 ± 0.24021849299011286
	data : 0.1174731731414795
	model : 0.06816186904907226
			 train-loss:  2.0811836338043213 	 ± 0.23943556607923996
	data : 0.11769204139709473
	model : 0.0683474063873291
			 train-loss:  2.0789244032853484 	 ± 0.24024017950358853
	data : 0.11743035316467285
	model : 0.06929101943969726
			 train-loss:  2.078280066189013 	 ± 0.23957948232839457
	data : 0.11656427383422852
	model : 0.06924810409545898
			 train-loss:  2.079079654481676 	 ± 0.23899865225224148
	data : 0.11666288375854492
	model : 0.06912755966186523
			 train-loss:  2.0774980984724962 	 ± 0.2390233173708914
	data : 0.11676616668701172
	model : 0.07020335197448731
			 train-loss:  2.0780467787096577 	 ± 0.23834830281644176
	data : 0.11587843894958497
	model : 0.07114701271057129
			 train-loss:  2.0756502266113577 	 ± 0.23944933482338926
	data : 0.11508617401123047
	model : 0.07117056846618652
			 train-loss:  2.071666511001101 	 ± 0.24381653973278963
	data : 0.11515321731567382
	model : 0.07088570594787598
			 train-loss:  2.0702409374563 	 ± 0.24369925349435737
	data : 0.11535181999206542
	model : 0.07084288597106933
			 train-loss:  2.0751679850824223 	 ± 0.2507017840226668
	data : 0.11553525924682617
	model : 0.07013001441955566
			 train-loss:  2.074237048625946 	 ± 0.25019264486795056
	data : 0.11617660522460938
	model : 0.06897940635681152
			 train-loss:  2.071965249428838 	 ± 0.2510644015715418
	data : 0.11704344749450683
	model : 0.06837105751037598
			 train-loss:  2.0715227252171364 	 ± 0.2503512878735214
	data : 0.1175765037536621
	model : 0.06865549087524414
			 train-loss:  2.073201866237664 	 ± 0.2504955384626528
	data : 0.11731066703796386
	model : 0.06775131225585937
			 train-loss:  2.0736031554094176 	 ± 0.24978321265111575
	data : 0.11788997650146485
	model : 0.06775197982788086
			 train-loss:  2.073044763911854 	 ± 0.2491277934407208
	data : 0.1180877685546875
	model : 0.06861734390258789
			 train-loss:  2.0733578032757864 	 ± 0.2484088221804854
	data : 0.11748528480529785
	model : 0.0690955638885498
			 train-loss:  2.072471225333071 	 ± 0.24792724674636024
	data : 0.117039155960083
	model : 0.06898016929626465
			 train-loss:  2.072048696024077 	 ± 0.24724856700248352
	data : 0.11727585792541503
	model : 0.06984758377075195
			 train-loss:  2.0711988923112314 	 ± 0.24676193159719614
	data : 0.11667327880859375
	model : 0.06973962783813477
			 train-loss:  2.0730885000789865 	 ± 0.24725836738231385
	data : 0.1165898323059082
	model : 0.06908297538757324
			 train-loss:  2.0756260261201023 	 ± 0.24874447171212602
	data : 0.11711697578430176
	model : 0.06939764022827148
			 train-loss:  2.0755973624628643 	 ± 0.2480206064735751
	data : 0.11689953804016114
	model : 0.06946468353271484
			 train-loss:  2.077845809087588 	 ± 0.24905460406291305
	data : 0.11682090759277344
	model : 0.0687990665435791
			 train-loss:  2.078599931179792 	 ± 0.2485359069500409
	data : 0.11729536056518555
	model : 0.06852741241455078
			 train-loss:  2.07885831287929 	 ± 0.24784822269330956
	data : 0.11747345924377442
	model : 0.06905784606933593
			 train-loss:  2.0793394812128763 	 ± 0.24722506157518426
	data : 0.11700387001037597
	model : 0.06821012496948242
			 train-loss:  2.0795302364112294 	 ± 0.24653868507588889
	data : 0.11771559715270996
	model : 0.06792826652526855
			 train-loss:  2.0782231963082647 	 ± 0.24645939576106352
	data : 0.11803164482116699
	model : 0.06882925033569336
			 train-loss:  2.0775881905795477 	 ± 0.24591597551991118
	data : 0.11740169525146485
	model : 0.06880083084106445
			 train-loss:  2.0777689867549474 	 ± 0.24524385346550268
	data : 0.11753458976745605
	model : 0.06900987625122071
			 train-loss:  2.0786340829417194 	 ± 0.2448406988191651
	data : 0.11736140251159669
	model : 0.07040910720825196
			 train-loss:  2.079266299258222 	 ± 0.2443152350888803
	data : 0.11592082977294922
	model : 0.07066335678100585
			 train-loss:  2.079303792265595 	 ± 0.24364731772653409
	data : 0.11547865867614746
	model : 0.07051267623901367
			 train-loss:  2.077732749607252 	 ± 0.24391199426831664
	data : 0.11579432487487792
	model : 0.06971125602722168
			 train-loss:  2.078729914330147 	 ± 0.24362765633621974
	data : 0.1163475513458252
	model : 0.06976275444030762
			 train-loss:  2.080241818581858 	 ± 0.24384053835022396
	data : 0.11630415916442871
	model : 0.06869215965270996
			 train-loss:  2.0780359787099503 	 ± 0.24504137508825855
	data : 0.11753554344177246
	model : 0.06888842582702637
			 train-loss:  2.0767332531036216 	 ± 0.2450372263980032
	data : 0.11753077507019043
	model : 0.06879510879516601
			 train-loss:  2.076993891801784 	 ± 0.24441424779223048
	data : 0.1173184871673584
	model : 0.06976852416992188
			 train-loss:  2.0788204732694124 	 ± 0.24506018008368077
	data : 0.11655888557434083
	model : 0.06985416412353515
			 train-loss:  2.0804475976534538 	 ± 0.24544470223229195
	data : 0.11654105186462402
	model : 0.07012624740600586
			 train-loss:  2.0793194317569337 	 ± 0.24530069818859687
	data : 0.11630959510803222
	model : 0.06968131065368652
			 train-loss:  2.0804721524678365 	 ± 0.2451851976713936
	data : 0.11673493385314941
	model : 0.06955518722534179
			 train-loss:  2.0791129389989007 	 ± 0.24528038245771852
	data : 0.11695165634155273
	model : 0.06866288185119629
			 train-loss:  2.0808297438499257 	 ± 0.24581647588752878
	data : 0.11776537895202636
	model : 0.06859989166259765
			 train-loss:  2.08089626078703 	 ± 0.24519035055427202
	data : 0.11804108619689942
	model : 0.06849093437194824
			 train-loss:  2.0805108716645218 	 ± 0.24462675606892667
	data : 0.11786646842956543
	model : 0.06877508163452148
			 train-loss:  2.080800282834756 	 ± 0.244042038773342
	data : 0.11761713027954102
	model : 0.06895947456359863
			 train-loss:  2.0789523867506476 	 ± 0.24481289234427955
	data : 0.11735548973083496
	model : 0.06960535049438477
			 train-loss:  2.0803550577163694 	 ± 0.2450004391393201
	data : 0.11683707237243653
	model : 0.0696218490600586
			 train-loss:  2.0811994063913524 	 ± 0.24468176720817011
	data : 0.11670622825622559
	model : 0.06985263824462891
			 train-loss:  2.081251419416749 	 ± 0.24407648179324487
	data : 0.11669712066650391
	model : 0.06990036964416504
			 train-loss:  2.0804517780031477 	 ± 0.2437396734647762
	data : 0.11673150062561036
	model : 0.06989521980285644
			 train-loss:  2.0821517092340134 	 ± 0.24434490015072274
	data : 0.11647305488586426
	model : 0.07025127410888672
			 train-loss:  2.0828455128320833 	 ± 0.24394955881670477
	data : 0.11599946022033691
	model : 0.07025742530822754
			 train-loss:  2.083114856076472 	 ± 0.24338728172007518
	data : 0.1159754753112793
	model : 0.06924257278442383
			 train-loss:  2.083435592444047 	 ± 0.24284231433838507
	data : 0.11678481101989746
	model : 0.0691605567932129
			 train-loss:  2.084241697994562 	 ± 0.24253531372573678
	data : 0.11682438850402832
	model : 0.06917462348937989
			 train-loss:  2.084292056457848 	 ± 0.2419554800278222
	data : 0.11690664291381836
	model : 0.06872549057006835
			 train-loss:  2.0844445075307574 	 ± 0.2413887697499787
	data : 0.11719808578491211
	model : 0.06868953704833984
			 train-loss:  2.0838123808539875 	 ± 0.24099024175381903
	data : 0.11710906028747559
	model : 0.06958146095275879
			 train-loss:  2.0825289701515772 	 ± 0.24114290162627208
	data : 0.11629915237426758
	model : 0.06953864097595215
			 train-loss:  2.081003016709162 	 ± 0.24159996683079918
	data : 0.1162637710571289
	model : 0.06949658393859863
			 train-loss:  2.080835166378556 	 ± 0.24104726803388882
	data : 0.11645479202270508
	model : 0.06887140274047851
			 train-loss:  2.082299760330555 	 ± 0.2414385488005017
	data : 0.11711101531982422
	model : 0.06883826255798339
			 train-loss:  2.082736531893412 	 ± 0.24096413684415546
	data : 0.11718902587890626
	model : 0.06889858245849609
			 train-loss:  2.085822259226153 	 ± 0.24464838148979098
	data : 0.11720376014709473
	model : 0.06815071105957031
			 train-loss:  2.0840958378730563 	 ± 0.24540792867451064
	data : 0.11767492294311524
	model : 0.06826395988464355
			 train-loss:  2.084041506188101 	 ± 0.2448483096583009
	data : 0.11765069961547851
	model : 0.06909403800964356
			 train-loss:  2.0825414619662546 	 ± 0.2452977183725686
	data : 0.11695199012756348
	model : 0.06909265518188476
			 train-loss:  2.082680623995233 	 ± 0.2447508209154495
	data : 0.11700901985168458
	model : 0.06862077713012696
			 train-loss:  2.083175973312275 	 ± 0.24430996325392002
	data : 0.11727805137634277
	model : 0.0693436622619629
			 train-loss:  2.0827142545460586 	 ± 0.2438586244675683
	data : 0.11668038368225098
	model : 0.0684814453125
			 train-loss:  2.0829968308763847 	 ± 0.24335027703433024
	data : 0.11745076179504395
	model : 0.06818246841430664
			 train-loss:  2.0833774222268 	 ± 0.24287570182875626
	data : 0.11759438514709472
	model : 0.06744532585144043
			 train-loss:  2.0821091537981964 	 ± 0.24308333671197363
	data : 0.11802563667297364
	model : 0.06715106964111328
			 train-loss:  2.0812984641953185 	 ± 0.24285331697689005
	data : 0.11836886405944824
	model : 0.06680760383605958
			 train-loss:  2.0811076692321846 	 ± 0.2423372086857998
	data : 0.1189082145690918
	model : 0.0670198917388916
			 train-loss:  2.082968711332463 	 ± 0.24343488747118724
	data : 0.11852431297302246
	model : 0.0666572093963623
			 train-loss:  2.0835679629574653 	 ± 0.24307431861856552
	data : 0.11901435852050782
	model : 0.06729493141174317
			 train-loss:  2.083759903907776 	 ± 0.24256508017071976
	data : 0.11870179176330567
	model : 0.0678987979888916
			 train-loss:  2.085683605280416 	 ± 0.2438012544339316
	data : 0.11806716918945312
	model : 0.06771631240844726
			 train-loss:  2.08498587270663 	 ± 0.24350953404387224
	data : 0.11801648139953613
	model : 0.06817717552185058
			 train-loss:  2.0844710372452044 	 ± 0.24311570411553163
	data : 0.11759800910949707
	model : 0.06879777908325195
			 train-loss:  2.0848181268002124 	 ± 0.24265597902275365
	data : 0.11690373420715332
	model : 0.06816396713256836
			 train-loss:  2.0852836683645086 	 ± 0.2422464773318007
	data : 0.11734662055969239
	model : 0.06796560287475586
			 train-loss:  2.086392822144907 	 ± 0.24233464280034897
	data : 0.11754956245422363
	model : 0.06829261779785156
			 train-loss:  2.086341771758905 	 ± 0.24182627757398095
	data : 0.11706981658935547
	model : 0.06748161315917969
			 train-loss:  2.087240744834166 	 ± 0.24171802345837856
	data : 0.11766982078552246
	model : 0.06738395690917968
			 train-loss:  2.0868303130070367 	 ± 0.24129735823958812
	data : 0.11787352561950684
	model : 0.06761398315429687
			 train-loss:  2.087200149955591 	 ± 0.24086437482216594
	data : 0.11759285926818848
	model : 0.06700406074523926
			 train-loss:  2.0866356162000295 	 ± 0.24052592227192582
	data : 0.11822233200073243
	model : 0.0664820671081543
			 train-loss:  2.085591218599076 	 ± 0.24057973253600362
	data : 0.1190380573272705
	model : 0.06646828651428223
			 train-loss:  2.0856141531076586 	 ± 0.24008650131658504
	data : 0.11918315887451172
	model : 0.06638655662536622
			 train-loss:  2.085402966032223 	 ± 0.23961873667649022
	data : 0.1191901683807373
	model : 0.0667142391204834
			 train-loss:  2.085352693631397 	 ± 0.23913250543267497
	data : 0.11896848678588867
	model : 0.06680479049682617
			 train-loss:  2.086313929635021 	 ± 0.23912368622042104
	data : 0.11890449523925781
	model : 0.06723966598510742
			 train-loss:  2.087430633364185 	 ± 0.23928557826342892
	data : 0.11851992607116699
	model : 0.06812829971313476
			 train-loss:  2.0864604204055297 	 ± 0.23929288222241332
	data : 0.11782913208007813
	model : 0.06764097213745117
			 train-loss:  2.086757302761078 	 ± 0.2388597617693413
	data : 0.11827220916748046
	model : 0.06711864471435547
			 train-loss:  2.0855264587706306 	 ± 0.2391765525891007
	data : 0.11866631507873535
	model : 0.06786203384399414
			 train-loss:  2.0861411832627796 	 ± 0.2389001194994906
	data : 0.11815004348754883
	model : 0.06814742088317871
			 train-loss:  2.0858221591225727 	 ± 0.23848129636142884
	data : 0.11768889427185059
	model : 0.06799135208129883
			 train-loss:  2.085580439079465 	 ± 0.23804243403604738
	data : 0.11785879135131835
	model : 0.06826696395874024
			 train-loss:  2.0840977154526055 	 ± 0.2387475634787441
	data : 0.11761088371276855
	model : 0.06866092681884765
			 train-loss:  2.085861866362393 	 ± 0.2399403248903145
	data : 0.1161524772644043
	model : 0.059707355499267575
#epoch  39    val-loss:  2.5099562908473767  train-loss:  2.085861866362393  lr:  0.000625
			 train-loss:  2.330597162246704 	 ± 0.0
	data : 5.832690715789795
	model : 0.07295703887939453
			 train-loss:  2.1768078804016113 	 ± 0.15378928184509277
	data : 2.982364296913147
	model : 0.07156574726104736
			 train-loss:  2.188878297805786 	 ± 0.12672339468138838
	data : 2.0266474882761636
	model : 0.07065852483113606
			 train-loss:  2.017601191997528 	 ± 0.3163094291857909
	data : 1.5492047667503357
	model : 0.06937885284423828
			 train-loss:  2.0540027618408203 	 ± 0.29213288267789544
	data : 1.2634740352630616
	model : 0.06949396133422851
			 train-loss:  2.1087858279546103 	 ± 0.29346880461510766
	data : 0.12001514434814453
	model : 0.06888151168823242
			 train-loss:  2.075702360698155 	 ± 0.2835270960931492
	data : 0.1166954517364502
	model : 0.0687295913696289
			 train-loss:  2.06531323492527 	 ± 0.26663589402387455
	data : 0.11694931983947754
	model : 0.0688826560974121
			 train-loss:  2.07516356309255 	 ± 0.2529259191795781
	data : 0.11686153411865234
	model : 0.06980657577514648
			 train-loss:  2.042448937892914 	 ± 0.2592423361473292
	data : 0.11597375869750977
	model : 0.06981768608093261
			 train-loss:  2.084755865010348 	 ± 0.28106166543441513
	data : 0.11605992317199706
	model : 0.06890878677368165
			 train-loss:  2.0555983086427054 	 ± 0.28594489356405406
	data : 0.11679701805114746
	model : 0.06905841827392578
			 train-loss:  2.077130070099464 	 ± 0.2846722810919867
	data : 0.11665487289428711
	model : 0.06817708015441895
			 train-loss:  2.123628011771611 	 ± 0.3214912418937431
	data : 0.11735095977783203
	model : 0.06811141967773438
			 train-loss:  2.1134677171707152 	 ± 0.3129080049834141
	data : 0.11737594604492188
	model : 0.06855278015136719
			 train-loss:  2.128774516284466 	 ± 0.30871739074015936
	data : 0.11720175743103027
	model : 0.06926331520080567
			 train-loss:  2.1296424515107097 	 ± 0.2995199824690173
	data : 0.11667208671569824
	model : 0.0689925193786621
			 train-loss:  2.144221987989214 	 ± 0.2972234457583182
	data : 0.11703476905822754
	model : 0.06929426193237305
			 train-loss:  2.1309539079666138 	 ± 0.29472184270599383
	data : 0.11682143211364746
	model : 0.06924562454223633
			 train-loss:  2.132976895570755 	 ± 0.2873946301476411
	data : 0.11692509651184083
	model : 0.06888780593872071
			 train-loss:  2.125001935731797 	 ± 0.28272698106088523
	data : 0.11702928543090821
	model : 0.06909828186035157
			 train-loss:  2.1334012042392385 	 ± 0.27889542720186283
	data : 0.11695270538330078
	model : 0.06905546188354492
			 train-loss:  2.122834749843763 	 ± 0.2772311375876429
	data : 0.11670389175415039
	model : 0.06967082023620605
			 train-loss:  2.1121109078327813 	 ± 0.276224080125947
	data : 0.11608691215515136
	model : 0.06881904602050781
			 train-loss:  2.1141195249557496 	 ± 0.27082204816020594
	data : 0.11680817604064941
	model : 0.06882200241088868
			 train-loss:  2.107743671307197 	 ± 0.26746948248482966
	data : 0.1169745922088623
	model : 0.0688514232635498
			 train-loss:  2.1144292751948037 	 ± 0.26467419032941447
	data : 0.11702795028686523
	model : 0.06821150779724121
			 train-loss:  2.1271032137530193 	 ± 0.2681184999852054
	data : 0.11767683029174805
	model : 0.06825828552246094
			 train-loss:  2.134817990763434 	 ± 0.2665992266479362
	data : 0.11757240295410157
	model : 0.06913142204284668
			 train-loss:  2.124924695491791 	 ± 0.2674778822811226
	data : 0.11673698425292969
	model : 0.06905083656311035
			 train-loss:  2.1248381714667044 	 ± 0.2631287853238566
	data : 0.11663284301757812
	model : 0.06906328201293946
			 train-loss:  2.1358298175036907 	 ± 0.26611729321789634
	data : 0.11662087440490723
	model : 0.07004618644714355
			 train-loss:  2.123334729310238 	 ± 0.27141936655025506
	data : 0.11585888862609864
	model : 0.07018423080444336
			 train-loss:  2.1170360445976257 	 ± 0.2698350902284392
	data : 0.11577730178833008
	model : 0.06995902061462403
			 train-loss:  2.114022081238883 	 ± 0.2665323947951968
	data : 0.11582403182983399
	model : 0.06983428001403809
			 train-loss:  2.114035232199563 	 ± 0.26280449691465724
	data : 0.11595535278320312
	model : 0.06891822814941406
			 train-loss:  2.1044672244303935 	 ± 0.2655093845109912
	data : 0.1167154312133789
	model : 0.06870198249816895
			 train-loss:  2.0984027354340804 	 ± 0.2645767989394464
	data : 0.11711282730102539
	model : 0.06853165626525878
			 train-loss:  2.0980558731617074 	 ± 0.26117151457152693
	data : 0.11720690727233887
	model : 0.0681990623474121
			 train-loss:  2.095503291487694 	 ± 0.2583784187541473
	data : 0.11767106056213379
	model : 0.0684819221496582
			 train-loss:  2.0927427512843435 	 ± 0.25580451978058555
	data : 0.11739397048950195
	model : 0.069183349609375
			 train-loss:  2.0844439580326988 	 ± 0.2582665720175743
	data : 0.11675500869750977
	model : 0.06838421821594239
			 train-loss:  2.0831287378488583 	 ± 0.2553880840573189
	data : 0.11721239089965821
	model : 0.06836190223693847
			 train-loss:  2.08711117235097 	 ± 0.2538162753561073
	data : 0.11750979423522949
	model : 0.06893210411071778
			 train-loss:  2.087369418144226 	 ± 0.25098609611468986
	data : 0.1169255256652832
	model : 0.0685302734375
			 train-loss:  2.0842733046282893 	 ± 0.24911032007231945
	data : 0.11740736961364746
	model : 0.06867213249206543
			 train-loss:  2.0858419220498265 	 ± 0.24667549120210083
	data : 0.11728572845458984
	model : 0.06960997581481934
			 train-loss:  2.092444303135077 	 ± 0.24825372537129753
	data : 0.1165799617767334
	model : 0.06958622932434082
			 train-loss:  2.0931348435732784 	 ± 0.2457540385506838
	data : 0.11653194427490235
	model : 0.06952757835388183
			 train-loss:  2.093178999423981 	 ± 0.24328428237744823
	data : 0.11656808853149414
	model : 0.06884627342224121
			 train-loss:  2.094257347724017 	 ± 0.24100798668682943
	data : 0.11715435981750488
	model : 0.06863651275634766
			 train-loss:  2.0887054204940796 	 ± 0.24195010229749683
	data : 0.11727471351623535
	model : 0.06835398674011231
			 train-loss:  2.0872083609958865 	 ± 0.2398997045092816
	data : 0.11735434532165527
	model : 0.06843357086181641
			 train-loss:  2.09752200267933 	 ± 0.24924639764540782
	data : 0.11746273040771485
	model : 0.06841607093811035
			 train-loss:  2.09836762601679 	 ± 0.24704829075374912
	data : 0.117496919631958
	model : 0.0691453456878662
			 train-loss:  2.1006278353078023 	 ± 0.24540569580361146
	data : 0.11682753562927246
	model : 0.0693366527557373
			 train-loss:  2.0956313610076904 	 ± 0.24610042912121333
	data : 0.11668186187744141
	model : 0.06867952346801758
			 train-loss:  2.094409231481881 	 ± 0.24414406617066864
	data : 0.11734380722045898
	model : 0.06866259574890136
			 train-loss:  2.088543326167737 	 ± 0.24615393835892235
	data : 0.1171457290649414
	model : 0.06802239418029785
			 train-loss:  2.0880536874135336 	 ± 0.244123009363276
	data : 0.11787786483764648
	model : 0.06830205917358398
			 train-loss:  2.083908456270812 	 ± 0.24423356823175862
	data : 0.11754345893859863
	model : 0.06739544868469238
			 train-loss:  2.079308586735879 	 ± 0.24490533943305656
	data : 0.11840295791625977
	model : 0.0685051441192627
			 train-loss:  2.081863467655485 	 ± 0.2437853224365514
	data : 0.11748967170715333
	model : 0.06888127326965332
			 train-loss:  2.089297316968441 	 ± 0.24896622846302166
	data : 0.11711807250976562
	model : 0.06888151168823242
			 train-loss:  2.085726257470938 	 ± 0.24869004387157173
	data : 0.11713786125183105
	model : 0.06928119659423829
			 train-loss:  2.085305651028951 	 ± 0.24682213286049223
	data : 0.11706662178039551
	model : 0.06978368759155273
			 train-loss:  2.079786403855281 	 ± 0.24904295220784864
	data : 0.11655755043029785
	model : 0.0689328670501709
			 train-loss:  2.080940481494455 	 ± 0.24738539775411755
	data : 0.1174201488494873
	model : 0.06767945289611817
			 train-loss:  2.078017452488775 	 ± 0.24676625365244556
	data : 0.11848783493041992
	model : 0.06862049102783203
			 train-loss:  2.081554164205279 	 ± 0.24675240698617215
	data : 0.11771788597106933
	model : 0.06719622611999512
			 train-loss:  2.0757202783101043 	 ± 0.2498231098477287
	data : 0.11876235008239747
	model : 0.06761960983276367
			 train-loss:  2.074830692675379 	 ± 0.2481953774194679
	data : 0.11834311485290527
	model : 0.06813793182373047
			 train-loss:  2.081108183076937 	 ± 0.2521792865781135
	data : 0.11775226593017578
	model : 0.0689049243927002
			 train-loss:  2.0847135057320467 	 ± 0.2523566693218337
	data : 0.11716609001159668
	model : 0.0687406063079834
			 train-loss:  2.082668474515279 	 ± 0.2512851952171718
	data : 0.1171349048614502
	model : 0.06961369514465332
			 train-loss:  2.077042927867488 	 ± 0.2543362185785316
	data : 0.11642065048217773
	model : 0.06971650123596192
			 train-loss:  2.075747349045493 	 ± 0.25293159188667436
	data : 0.11647515296936035
	model : 0.06990518569946289
			 train-loss:  2.0784359329786057 	 ± 0.2524099818142058
	data : 0.11630692481994628
	model : 0.06935930252075195
			 train-loss:  2.0772333809092074 	 ± 0.2510321321380007
	data : 0.11663937568664551
	model : 0.06935129165649415
			 train-loss:  2.075680413842201 	 ± 0.24983983280066463
	data : 0.11666226387023926
	model : 0.06858420372009277
			 train-loss:  2.0732732201799933 	 ± 0.24922458143507373
	data : 0.11750092506408691
	model : 0.06847105026245118
			 train-loss:  2.073376033364273 	 ± 0.24770198609120675
	data : 0.11753625869750976
	model : 0.06771831512451172
			 train-loss:  2.0753938261284888 	 ± 0.24688236811106443
	data : 0.1182429313659668
	model : 0.06839971542358399
			 train-loss:  2.0731228249413625 	 ± 0.24627904039608706
	data : 0.11781940460205079
	model : 0.06841907501220704
			 train-loss:  2.070048785209656 	 ± 0.2464418252500733
	data : 0.11784563064575196
	model : 0.06984076499938965
			 train-loss:  2.073204981726269 	 ± 0.24672677781515728
	data : 0.11645736694335937
	model : 0.0699030876159668
			 train-loss:  2.074001675364615 	 ± 0.2454159459850962
	data : 0.11652588844299316
	model : 0.06973981857299805
			 train-loss:  2.0699979093941776 	 ± 0.2468586409089843
	data : 0.11649727821350098
	model : 0.06990165710449218
			 train-loss:  2.0706202823124573 	 ± 0.24553729907206642
	data : 0.1163449764251709
	model : 0.06896696090698243
			 train-loss:  2.075865576002333 	 ± 0.2491332147928579
	data : 0.11714839935302734
	model : 0.06831235885620117
			 train-loss:  2.0796766595526055 	 ± 0.2503846880911696
	data : 0.11765766143798828
	model : 0.06813225746154786
			 train-loss:  2.078523639751517 	 ± 0.24926297814549342
	data : 0.11789512634277344
	model : 0.06882562637329101
			 train-loss:  2.0796211881022297 	 ± 0.24814264092368632
	data : 0.1172607421875
	model : 0.06799697875976562
			 train-loss:  2.083675778926687 	 ± 0.2498972033164827
	data : 0.11791739463806153
	model : 0.06890830993652344
			 train-loss:  2.0843470611070334 	 ± 0.24866366174105872
	data : 0.11712112426757812
	model : 0.06902289390563965
			 train-loss:  2.081765046964089 	 ± 0.2486420362993463
	data : 0.11686458587646484
	model : 0.06891498565673829
			 train-loss:  2.085509488263081 	 ± 0.2500630197003904
	data : 0.11674084663391113
	model : 0.0693131923675537
			 train-loss:  2.0842621070998057 	 ± 0.2490870635889749
	data : 0.11655216217041016
	model : 0.06980805397033692
			 train-loss:  2.0856432083881264 	 ± 0.24820270683984574
	data : 0.11600079536437988
	model : 0.06955537796020508
			 train-loss:  2.084489593505859 	 ± 0.24722518024700138
	data : 0.11625056266784668
	model : 0.06950011253356933
			 train-loss:  2.082533104584949 	 ± 0.24677504585002452
	data : 0.11650233268737793
	model : 0.06980829238891602
			 train-loss:  2.081296256944245 	 ± 0.24587678579264496
	data : 0.11635422706604004
	model : 0.06966114044189453
			 train-loss:  2.0777421877222153 	 ± 0.24729911411183134
	data : 0.11655497550964355
	model : 0.0698786735534668
			 train-loss:  2.078317561974892 	 ± 0.2461765705907863
	data : 0.1165459156036377
	model : 0.07012062072753907
			 train-loss:  2.0783787681942893 	 ± 0.24500229182472322
	data : 0.11618752479553222
	model : 0.07015118598937989
			 train-loss:  2.07848070927386 	 ± 0.24384611946556883
	data : 0.11616535186767578
	model : 0.07008399963378906
			 train-loss:  2.073696603284818 	 ± 0.24765159677372398
	data : 0.11606864929199219
	model : 0.06979069709777833
			 train-loss:  2.074637778379299 	 ± 0.24669457347698784
	data : 0.1163905143737793
	model : 0.06968951225280762
			 train-loss:  2.0764852609109443 	 ± 0.2463097745960809
	data : 0.11653809547424317
	model : 0.07001247406005859
			 train-loss:  2.0737667798995973 	 ± 0.24682483333198652
	data : 0.1162829875946045
	model : 0.0699958324432373
			 train-loss:  2.0799921912115975 	 ± 0.25423764291165846
	data : 0.11617507934570312
	model : 0.06992497444152831
			 train-loss:  2.083803417427199 	 ± 0.2562654666532108
	data : 0.11610941886901856
	model : 0.07017326354980469
			 train-loss:  2.0820013147539798 	 ± 0.25584086781479937
	data : 0.11569738388061523
	model : 0.07038474082946777
			 train-loss:  2.085093429214076 	 ± 0.25682834607054617
	data : 0.11559405326843261
	model : 0.07020106315612792
			 train-loss:  2.0835311962210614 	 ± 0.2562527119397974
	data : 0.11574792861938477
	model : 0.06927261352539063
			 train-loss:  2.0803349367503463 	 ± 0.25743779728309546
	data : 0.11672863960266114
	model : 0.06913809776306153
			 train-loss:  2.079101333251366 	 ± 0.2566793707146499
	data : 0.11719713211059571
	model : 0.06953244209289551
			 train-loss:  2.0760833752357355 	 ± 0.2576656789114344
	data : 0.11697759628295898
	model : 0.06938166618347168
			 train-loss:  2.0802900911379263 	 ± 0.26061824876769574
	data : 0.1170539379119873
	model : 0.0686572551727295
			 train-loss:  2.0804197430610656 	 ± 0.2595339213649551
	data : 0.11769895553588867
	model : 0.06957192420959472
			 train-loss:  2.0821316675706343 	 ± 0.25913869279514423
	data : 0.1167119026184082
	model : 0.06963028907775878
			 train-loss:  2.0815006885372225 	 ± 0.2581677808849478
	data : 0.11649270057678222
	model : 0.06907858848571777
			 train-loss:  2.081284608298201 	 ± 0.25712725356701355
	data : 0.11676483154296875
	model : 0.06904091835021972
			 train-loss:  2.0813558063199444 	 ± 0.25608956868726074
	data : 0.11676554679870606
	model : 0.06966781616210938
			 train-loss:  2.085272319793701 	 ± 0.2587648699087234
	data : 0.11634879112243653
	model : 0.06875314712524414
			 train-loss:  2.0838267055768815 	 ± 0.2582422511424598
	data : 0.11712369918823243
	model : 0.06898036003112792
			 train-loss:  2.0846581599843783 	 ± 0.25739280377411006
	data : 0.11697163581848144
	model : 0.06893706321716309
			 train-loss:  2.084396910853684 	 ± 0.2564022951435236
	data : 0.11722159385681152
	model : 0.06897468566894531
			 train-loss:  2.081977878430093 	 ± 0.2568686977292512
	data : 0.11705875396728516
	model : 0.06876444816589355
			 train-loss:  2.0820583095917335 	 ± 0.2558804646350913
	data : 0.11745519638061523
	model : 0.06955065727233886
			 train-loss:  2.084291890377307 	 ± 0.25617095710019455
	data : 0.11693844795227051
	model : 0.06919898986816406
			 train-loss:  2.0835895276430882 	 ± 0.25532535154615993
	data : 0.11726951599121094
	model : 0.0694267749786377
			 train-loss:  2.0854156671610093 	 ± 0.2552274841672484
	data : 0.11697287559509277
	model : 0.06940336227416992
			 train-loss:  2.086433749590347 	 ± 0.25454428830746817
	data : 0.11707029342651368
	model : 0.06882243156433106
			 train-loss:  2.087253310062267 	 ± 0.25377717217604556
	data : 0.1174649715423584
	model : 0.06799235343933105
			 train-loss:  2.0877294443985996 	 ± 0.25290296114209665
	data : 0.1181614875793457
	model : 0.06749382019042968
			 train-loss:  2.0901006738634873 	 ± 0.25349110099117544
	data : 0.1183657169342041
	model : 0.0674978256225586
			 train-loss:  2.087238582148068 	 ± 0.2547829418315752
	data : 0.11846389770507812
	model : 0.06759157180786132
			 train-loss:  2.0857630830874547 	 ± 0.2544558452982071
	data : 0.11836476325988769
	model : 0.06833291053771973
			 train-loss:  2.08661224927221 	 ± 0.25374302679998767
	data : 0.1176785945892334
	model : 0.0683438777923584
			 train-loss:  2.0900104544686933 	 ± 0.2560187112852399
	data : 0.11762924194335937
	model : 0.06912794113159179
			 train-loss:  2.0883796055551986 	 ± 0.255849575231376
	data : 0.11691455841064453
	model : 0.06867127418518067
			 train-loss:  2.086805199409698 	 ± 0.2556427840450307
	data : 0.11717743873596191
	model : 0.06831111907958984
			 train-loss:  2.0865133379896483 	 ± 0.2547774957092564
	data : 0.1175816535949707
	model : 0.06736669540405274
			 train-loss:  2.0849192265806527 	 ± 0.25461704152396925
	data : 0.1181920051574707
	model : 0.06836323738098145
			 train-loss:  2.0826944308738184 	 ± 0.2551538881932584
	data : 0.11748099327087402
	model : 0.06766934394836426
			 train-loss:  2.0832380859219297 	 ± 0.2543693722540215
	data : 0.1183661937713623
	model : 0.06718535423278808
			 train-loss:  2.085380784563116 	 ± 0.254836202299552
	data : 0.11879887580871581
	model : 0.06762499809265136
			 train-loss:  2.0859903665196975 	 ± 0.25408785177613685
	data : 0.11829018592834473
	model : 0.06841492652893066
			 train-loss:  2.086452883084615 	 ± 0.2533024012751583
	data : 0.1177863597869873
	model : 0.06837491989135742
			 train-loss:  2.0849437650465807 	 ± 0.25313792182749145
	data : 0.11779317855834961
	model : 0.06909842491149902
			 train-loss:  2.082782620662137 	 ± 0.25369763236576715
	data : 0.11707534790039062
	model : 0.06909651756286621
			 train-loss:  2.0866523716184826 	 ± 0.2573286121323664
	data : 0.11706600189208985
	model : 0.06875081062316894
			 train-loss:  2.0879899116305563 	 ± 0.2570247975322056
	data : 0.1172027587890625
	model : 0.06879153251647949
			 train-loss:  2.08677433998354 	 ± 0.2566380609966357
	data : 0.11699838638305664
	model : 0.06868171691894531
			 train-loss:  2.0900794924833836 	 ± 0.2591025282618103
	data : 0.11695551872253418
	model : 0.0690232276916504
			 train-loss:  2.091206796609672 	 ± 0.25865954721864515
	data : 0.11681833267211914
	model : 0.06932945251464843
			 train-loss:  2.0895320901387855 	 ± 0.25869217666463923
	data : 0.11662569046020507
	model : 0.06958403587341308
			 train-loss:  2.0878312182876297 	 ± 0.2587621299845832
	data : 0.11652908325195313
	model : 0.06978497505187989
			 train-loss:  2.087254299968481 	 ± 0.25805478902640616
	data : 0.11653633117675781
	model : 0.07002472877502441
			 train-loss:  2.087952430944265 	 ± 0.25740365076547955
	data : 0.11640372276306152
	model : 0.06964750289916992
			 train-loss:  2.087757014198068 	 ± 0.25661994475575955
	data : 0.11669888496398925
	model : 0.07002272605895996
			 train-loss:  2.088303584262637 	 ± 0.2559261235573804
	data : 0.11623330116271972
	model : 0.06994042396545411
			 train-loss:  2.0860551799215923 	 ± 0.25675439444643716
	data : 0.11637096405029297
	model : 0.0697472095489502
			 train-loss:  2.085668367328066 	 ± 0.2560230949815179
	data : 0.11645793914794922
	model : 0.06899323463439941
			 train-loss:  2.0848496498831786 	 ± 0.2554673324698642
	data : 0.11712656021118165
	model : 0.06907153129577637
			 train-loss:  2.081623385052481 	 ± 0.2580709524158092
	data : 0.1171189785003662
	model : 0.06945929527282715
			 train-loss:  2.082793657978376 	 ± 0.2577457995899572
	data : 0.11673507690429688
	model : 0.06912989616394043
			 train-loss:  2.082104790845566 	 ± 0.2571371725589998
	data : 0.11692328453063965
	model : 0.06954426765441894
			 train-loss:  2.0792044239885668 	 ± 0.2591374861772221
	data : 0.11657299995422363
	model : 0.07011876106262208
			 train-loss:  2.0794230685596578 	 ± 0.25839438950883953
	data : 0.11602849960327148
	model : 0.0691408634185791
			 train-loss:  2.077296581379203 	 ± 0.2591384369760618
	data : 0.11688227653503418
	model : 0.06898369789123535
			 train-loss:  2.0768689048083533 	 ± 0.2584492667791644
	data : 0.11714849472045899
	model : 0.06932373046875
			 train-loss:  2.07652446319317 	 ± 0.25774534531337556
	data : 0.11693415641784669
	model : 0.06894955635070801
			 train-loss:  2.079583681651524 	 ± 0.26015664910127806
	data : 0.11721453666687012
	model : 0.06806201934814453
			 train-loss:  2.0804220248352396 	 ± 0.25965346439551523
	data : 0.11788554191589355
	model : 0.06905221939086914
			 train-loss:  2.0809866789370606 	 ± 0.25902728213397
	data : 0.11710882186889648
	model : 0.06904711723327636
			 train-loss:  2.081004902218165 	 ± 0.25829876639361665
	data : 0.11720805168151856
	model : 0.06924443244934082
			 train-loss:  2.0810776143100673 	 ± 0.2575780777671766
	data : 0.11695232391357421
	model : 0.06912693977355958
			 train-loss:  2.079203473197089 	 ± 0.2580825355322906
	data : 0.11712770462036133
	model : 0.0692021369934082
			 train-loss:  2.0785390211073733 	 ± 0.2575229548850182
	data : 0.11732048988342285
	model : 0.06897258758544922
			 train-loss:  2.078096797833076 	 ± 0.2568834052598914
	data : 0.11731019020080566
	model : 0.0687932014465332
			 train-loss:  2.085149606720346 	 ± 0.27327925719583684
	data : 0.11732840538024902
	model : 0.0677764892578125
			 train-loss:  2.0838805067798365 	 ± 0.27307584458142425
	data : 0.1181920051574707
	model : 0.06774616241455078
			 train-loss:  2.0837343815210705 	 ± 0.2723440148710573
	data : 0.11818184852600097
	model : 0.06859631538391113
			 train-loss:  2.08500172309978 	 ± 0.2721573640804235
	data : 0.11722164154052735
	model : 0.06880950927734375
			 train-loss:  2.0838608633388174 	 ± 0.2718742850761093
	data : 0.11698555946350098
	model : 0.06835541725158692
			 train-loss:  2.0841410369315048 	 ± 0.2711773177125841
	data : 0.11745448112487793
	model : 0.06914968490600586
			 train-loss:  2.0855875740606318 	 ± 0.27118524398396215
	data : 0.11681737899780273
	model : 0.0691953182220459
			 train-loss:  2.0842900100507236 	 ± 0.27105827867499677
	data : 0.11673192977905274
	model : 0.0689927101135254
			 train-loss:  2.0838448183698803 	 ± 0.2704174076499068
	data : 0.11701078414916992
	model : 0.06884145736694336
			 train-loss:  2.0831869871666036 	 ± 0.2698654585066602
	data : 0.11733784675598144
	model : 0.06938142776489258
			 train-loss:  2.084092544768141 	 ± 0.26945773039326226
	data : 0.11668996810913086
	model : 0.06932907104492188
			 train-loss:  2.0863505497421184 	 ± 0.27058682590726263
	data : 0.11668281555175782
	model : 0.06953353881835937
			 train-loss:  2.087576322677808 	 ± 0.270431592312408
	data : 0.1164743423461914
	model : 0.07016358375549317
			 train-loss:  2.0874898585737967 	 ± 0.26974353586682415
	data : 0.11584367752075195
	model : 0.07050065994262696
			 train-loss:  2.0873673089264613 	 ± 0.2690635067308787
	data : 0.11549882888793946
	model : 0.07097487449645996
			 train-loss:  2.087586056704473 	 ± 0.2684007545548292
	data : 0.11526784896850586
	model : 0.0702183723449707
			 train-loss:  2.0871994752979757 	 ± 0.26778078785179577
	data : 0.11600818634033203
	model : 0.0693845272064209
			 train-loss:  2.087242202162743 	 ± 0.26711117700730047
	data : 0.11669549942016602
	model : 0.06956186294555664
			 train-loss:  2.0884738116715087 	 ± 0.2670145805186977
	data : 0.1164736270904541
	model : 0.06943531036376953
			 train-loss:  2.088991950644125 	 ± 0.2664541121645221
	data : 0.11644301414489747
	model : 0.0690382957458496
			 train-loss:  2.0892608664893166 	 ± 0.2658244887894361
	data : 0.11677055358886719
	model : 0.06980881690979004
			 train-loss:  2.0894117898800793 	 ± 0.26518087631878867
	data : 0.11608433723449707
	model : 0.07061433792114258
			 train-loss:  2.0905715122455506 	 ± 0.26505139014047036
	data : 0.11557536125183106
	model : 0.06942648887634277
			 train-loss:  2.089609754317015 	 ± 0.26476561272407934
	data : 0.11666145324707031
	model : 0.06931076049804688
			 train-loss:  2.091466269631317 	 ± 0.2654659828436802
	data : 0.11693000793457031
	model : 0.06931681632995605
			 train-loss:  2.091401807963848 	 ± 0.2648286985944709
	data : 0.11693010330200196
	model : 0.06843395233154297
			 train-loss:  2.090739956883152 	 ± 0.2643667581604591
	data : 0.11770410537719726
	model : 0.0682673454284668
			 train-loss:  2.092611548446474 	 ± 0.26512086115854533
	data : 0.11778178215026855
	model : 0.06880369186401367
			 train-loss:  2.0933945862602847 	 ± 0.26473516688920556
	data : 0.11732840538024902
	model : 0.0688140869140625
			 train-loss:  2.092121679827852 	 ± 0.2647564950961324
	data : 0.11736412048339843
	model : 0.06883978843688965
			 train-loss:  2.0918917230597125 	 ± 0.26415549032752494
	data : 0.1173257827758789
	model : 0.06889777183532715
			 train-loss:  2.0921922153401598 	 ± 0.2635740693611185
	data : 0.11738801002502441
	model : 0.06934723854064942
			 train-loss:  2.0919019355330355 	 ± 0.26299467657490827
	data : 0.11700739860534667
	model : 0.06878848075866699
			 train-loss:  2.0927353126031383 	 ± 0.26266957807594044
	data : 0.11753764152526855
	model : 0.06929125785827636
			 train-loss:  2.093171403155349 	 ± 0.2621420117329823
	data : 0.11707921028137207
	model : 0.06937260627746582
			 train-loss:  2.093840485319085 	 ± 0.26172572807050004
	data : 0.11697163581848144
	model : 0.06960744857788086
			 train-loss:  2.093505239922162 	 ± 0.26117440652327595
	data : 0.11674265861511231
	model : 0.06856474876403809
			 train-loss:  2.0952168768102473 	 ± 0.26180836548809017
	data : 0.11767244338989258
	model : 0.06899852752685547
			 train-loss:  2.0959827177125403 	 ± 0.2614622359252713
	data : 0.11728148460388184
	model : 0.06763281822204589
			 train-loss:  2.0950791218259313 	 ± 0.2612183089362101
	data : 0.11842031478881836
	model : 0.06715593338012696
			 train-loss:  2.0943906681420024 	 ± 0.2608337389138163
	data : 0.11883225440979003
	model : 0.06699557304382324
			 train-loss:  2.0937620667474612 	 ± 0.2604201052702547
	data : 0.11885886192321778
	model : 0.06704788208007813
			 train-loss:  2.0925108740064835 	 ± 0.2605146509196594
	data : 0.11847405433654785
	model : 0.06631679534912109
			 train-loss:  2.093308419252919 	 ± 0.2602127987235435
	data : 0.1191258430480957
	model : 0.06775445938110351
			 train-loss:  2.0950806865608116 	 ± 0.26100242545536795
	data : 0.11735768318176269
	model : 0.06785416603088379
			 train-loss:  2.09487325475927 	 ± 0.2604481744415257
	data : 0.11711182594299316
	model : 0.06816225051879883
			 train-loss:  2.095813312905324 	 ± 0.2602662525608366
	data : 0.11702260971069336
	model : 0.06815690994262695
			 train-loss:  2.094929961536242 	 ± 0.2600436459825557
	data : 0.1171597957611084
	model : 0.06833457946777344
			 train-loss:  2.094428679127714 	 ± 0.2595915141073175
	data : 0.11667227745056152
	model : 0.067547607421875
			 train-loss:  2.095010114641025 	 ± 0.2591821430480005
	data : 0.117633056640625
	model : 0.0675581455230713
			 train-loss:  2.0945668798659494 	 ± 0.2587134610882112
	data : 0.1178445816040039
	model : 0.06813044548034668
			 train-loss:  2.094428279970446 	 ± 0.25816873140489704
	data : 0.1173370361328125
	model : 0.06835355758666992
			 train-loss:  2.094273765543674 	 ± 0.2576296933213251
	data : 0.11749377250671386
	model : 0.06899447441101074
			 train-loss:  2.095107338691162 	 ± 0.257400671116241
	data : 0.11714887619018555
	model : 0.06909551620483398
			 train-loss:  2.0943403958268307 	 ± 0.2571271350990457
	data : 0.11713781356811523
	model : 0.06927981376647949
			 train-loss:  2.093609775815691 	 ± 0.2568327940273362
	data : 0.11696772575378418
	model : 0.06856694221496581
			 train-loss:  2.0925869033925206 	 ± 0.25678025567155316
	data : 0.11744818687438965
	model : 0.06885900497436523
			 train-loss:  2.09411505262057 	 ± 0.2573314755895375
	data : 0.11720237731933594
	model : 0.06852941513061524
			 train-loss:  2.0925607097594074 	 ± 0.2579235434836908
	data : 0.11744093894958496
	model : 0.06841959953308105
			 train-loss:  2.0919245983943466 	 ± 0.2575794578551336
	data : 0.11741075515747071
	model : 0.06748323440551758
			 train-loss:  2.092069335435153 	 ± 0.257058773572403
	data : 0.11797547340393066
	model : 0.06745743751525879
			 train-loss:  2.090465792867004 	 ± 0.25774645365965565
	data : 0.11796445846557617
	model : 0.067130708694458
			 train-loss:  2.089049051732433 	 ± 0.2581701459181542
	data : 0.11798682212829589
	model : 0.06693005561828613
			 train-loss:  2.09039021265216 	 ± 0.2584986778660879
	data : 0.11839399337768555
	model : 0.06620874404907226
			 train-loss:  2.0904042271949983 	 ± 0.2579749641283274
	data : 0.11893143653869628
	model : 0.06635608673095703
			 train-loss:  2.092565104365349 	 ± 0.2596845593125293
	data : 0.11895079612731933
	model : 0.06622848510742188
			 train-loss:  2.092347924967846 	 ± 0.259185146454205
	data : 0.11891264915466308
	model : 0.06568784713745117
			 train-loss:  2.0927629399299623 	 ± 0.25874914395837906
	data : 0.11927957534790039
	model : 0.06565141677856445
			 train-loss:  2.0926857445819445 	 ± 0.2582360775564631
	data : 0.11908330917358398
	model : 0.06642556190490723
			 train-loss:  2.0918345101295954 	 ± 0.2580758026236702
	data : 0.11858572959899902
	model : 0.06727442741394044
			 train-loss:  2.0905533972465005 	 ± 0.2583669125812338
	data : 0.11811437606811523
	model : 0.0680161952972412
			 train-loss:  2.090379966056253 	 ± 0.2578725701634831
	data : 0.11757292747497558
	model : 0.0685009479522705
			 train-loss:  2.0911129703708724 	 ± 0.25763143821272816
	data : 0.1173776626586914
	model : 0.06870646476745605
			 train-loss:  2.091639353428036 	 ± 0.25726511589919543
	data : 0.11628909111022949
	model : 0.06007084846496582
#epoch  40    val-loss:  2.4468432100195634  train-loss:  2.091639353428036  lr:  0.000625
			 train-loss:  2.442908763885498 	 ± 0.0
	data : 6.148569107055664
	model : 0.0728604793548584
			 train-loss:  2.287857174873352 	 ± 0.155051589012146
	data : 3.1420319080352783
	model : 0.07143855094909668
			 train-loss:  2.2029643058776855 	 ± 0.17447328902373113
	data : 2.1332155068715415
	model : 0.06955393155415852
			 train-loss:  2.2507364749908447 	 ± 0.17227082417634637
	data : 1.6299314498901367
	model : 0.06953680515289307
			 train-loss:  2.128758358955383 	 ± 0.2885419079720658
	data : 1.3270697116851806
	model : 0.06955471038818359
			 train-loss:  2.1136677066485086 	 ± 0.26555413663137495
	data : 0.12046971321105956
	model : 0.06903181076049805
			 train-loss:  2.0708154269627164 	 ± 0.26732520259882603
	data : 0.11649408340454101
	model : 0.06894130706787109
			 train-loss:  2.0530837029218674 	 ± 0.254422516129501
	data : 0.11665048599243164
	model : 0.06960530281066894
			 train-loss:  2.070056292745802 	 ± 0.24462840020760743
	data : 0.11604175567626954
	model : 0.06973142623901367
			 train-loss:  2.0542583584785463 	 ± 0.2368647743192999
	data : 0.11605525016784668
	model : 0.06979827880859375
			 train-loss:  2.06701444495808 	 ± 0.22941589165518844
	data : 0.11598019599914551
	model : 0.0688891887664795
			 train-loss:  2.089752584695816 	 ± 0.2322346515588863
	data : 0.11673345565795898
	model : 0.06888318061828613
			 train-loss:  2.081875021641071 	 ± 0.22478638347866722
	data : 0.11679611206054688
	model : 0.06909923553466797
			 train-loss:  2.0614474500928606 	 ± 0.22878903729503733
	data : 0.11662044525146484
	model : 0.0691497802734375
			 train-loss:  2.1084110418955486 	 ± 0.28237014841769337
	data : 0.1166341781616211
	model : 0.06922149658203125
			 train-loss:  2.107653394341469 	 ± 0.2734194668816284
	data : 0.11667113304138184
	model : 0.06907815933227539
			 train-loss:  2.1232578614178825 	 ± 0.2725007407159191
	data : 0.11672730445861816
	model : 0.06919159889221191
			 train-loss:  2.112222181426154 	 ± 0.26870364716578937
	data : 0.11654338836669922
	model : 0.0691025733947754
			 train-loss:  2.1021678824173775 	 ± 0.26499276192402565
	data : 0.11664724349975586
	model : 0.06901335716247559
			 train-loss:  2.114315938949585 	 ± 0.26365514588416733
	data : 0.11665754318237305
	model : 0.06799535751342774
			 train-loss:  2.119473797934396 	 ± 0.258332951729339
	data : 0.11755309104919434
	model : 0.06891808509826661
			 train-loss:  2.130847204815258 	 ± 0.25771865547679906
	data : 0.11694049835205078
	model : 0.06789236068725586
			 train-loss:  2.130521359650985 	 ± 0.25205845103371716
	data : 0.11777658462524414
	model : 0.06792607307434081
			 train-loss:  2.1316185891628265 	 ± 0.24680746567576045
	data : 0.11748099327087402
	model : 0.06787567138671875
			 train-loss:  2.123508987426758 	 ± 0.24506273375685575
	data : 0.11746354103088379
	model : 0.06895108222961426
			 train-loss:  2.123043408760658 	 ± 0.2403150562827651
	data : 0.11663475036621093
	model : 0.06922597885131836
			 train-loss:  2.1145348283979626 	 ± 0.2397804885733172
	data : 0.11623435020446778
	model : 0.07055039405822754
			 train-loss:  2.10163146683148 	 ± 0.24481975644902088
	data : 0.11521420478820801
	model : 0.07044491767883301
			 train-loss:  2.085995448046717 	 ± 0.25439243752913493
	data : 0.11539621353149414
	model : 0.07054686546325684
			 train-loss:  2.0802780270576475 	 ± 0.2520045767636684
	data : 0.11546621322631836
	model : 0.07023072242736816
			 train-loss:  2.0957706089942687 	 ± 0.2620273249542803
	data : 0.1158076286315918
	model : 0.07005176544189454
			 train-loss:  2.1102901063859463 	 ± 0.27027400262132456
	data : 0.1160616397857666
	model : 0.06978445053100586
			 train-loss:  2.109141064412666 	 ± 0.26622680020638884
	data : 0.11627764701843261
	model : 0.0698671817779541
			 train-loss:  2.116572222288917 	 ± 0.2657337569808147
	data : 0.11624550819396973
	model : 0.06973624229431152
			 train-loss:  2.1194441897528513 	 ± 0.26244487644000064
	data : 0.11626138687133789
	model : 0.06972832679748535
			 train-loss:  2.1111301382382712 	 ± 0.2634072423282998
	data : 0.11625480651855469
	model : 0.0694925308227539
			 train-loss:  2.0980085392256043 	 ± 0.2714894066710798
	data : 0.11630325317382813
	model : 0.06943483352661133
			 train-loss:  2.099124905310179 	 ± 0.2679794121976767
	data : 0.11640629768371583
	model : 0.06947994232177734
			 train-loss:  2.1084557588283834 	 ± 0.2707029196665284
	data : 0.11633491516113281
	model : 0.06955008506774903
			 train-loss:  2.1009196519851683 	 ± 0.2714092708361764
	data : 0.11634612083435059
	model : 0.06985344886779785
			 train-loss:  2.091443905016271 	 ± 0.27469605962206484
	data : 0.11611227989196778
	model : 0.0699150562286377
			 train-loss:  2.0920038790929887 	 ± 0.27142985155564975
	data : 0.11625580787658692
	model : 0.06893391609191894
			 train-loss:  2.0870421736739404 	 ± 0.2701754791334123
	data : 0.117039155960083
	model : 0.06892094612121583
			 train-loss:  2.087891299616207 	 ± 0.2671456918518353
	data : 0.11715903282165527
	model : 0.0684096336364746
			 train-loss:  2.0862516111797755 	 ± 0.26438454713935045
	data : 0.11773428916931153
	model : 0.06844158172607422
			 train-loss:  2.091365821983503 	 ± 0.2637358963636378
	data : 0.11771130561828613
	model : 0.06884360313415527
			 train-loss:  2.0844229028580035 	 ± 0.26513031640378276
	data : 0.11727242469787598
	model : 0.06979007720947265
			 train-loss:  2.0876096288363137 	 ± 0.2632620748125429
	data : 0.11640987396240235
	model : 0.06968879699707031
			 train-loss:  2.092207504778492 	 ± 0.2625018768080468
	data : 0.11653375625610352
	model : 0.07004313468933106
			 train-loss:  2.0902865982055663 	 ± 0.2602112504144001
	data : 0.11618995666503906
	model : 0.06987342834472657
			 train-loss:  2.104175824745029 	 ± 0.27573135574059093
	data : 0.116162109375
	model : 0.06932034492492675
			 train-loss:  2.1007404029369354 	 ± 0.27416713170775164
	data : 0.11662211418151855
	model : 0.06969408988952637
			 train-loss:  2.1006966199515 	 ± 0.2715685160001647
	data : 0.1164048194885254
	model : 0.06993794441223145
			 train-loss:  2.0991627088299505 	 ± 0.26927389583639916
	data : 0.11623516082763671
	model : 0.07017993927001953
			 train-loss:  2.0917712536725133 	 ± 0.2722871843748846
	data : 0.11601495742797852
	model : 0.0701972484588623
			 train-loss:  2.092049238937242 	 ± 0.2698529726040296
	data : 0.11594662666320801
	model : 0.07025537490844727
			 train-loss:  2.094340054612411 	 ± 0.26802416086671893
	data : 0.11600112915039062
	model : 0.06990590095520019
			 train-loss:  2.090424897341893 	 ± 0.2673426728166286
	data : 0.11630597114562988
	model : 0.06951346397399902
			 train-loss:  2.089646236371186 	 ± 0.2651337007099052
	data : 0.11653618812561035
	model : 0.06936187744140625
			 train-loss:  2.089481876293818 	 ± 0.26291800072353555
	data : 0.11662750244140625
	model : 0.07004227638244628
			 train-loss:  2.08374635313378 	 ± 0.26451169651328227
	data : 0.11610016822814942
	model : 0.07022509574890137
			 train-loss:  2.0847007632255554 	 ± 0.262475735029824
	data : 0.11586666107177734
	model : 0.07022032737731934
			 train-loss:  2.0910904880553955 	 ± 0.2652005577842025
	data : 0.1157641887664795
	model : 0.07044863700866699
			 train-loss:  2.0956079121679068 	 ± 0.2655523626801038
	data : 0.11554150581359864
	model : 0.07044878005981445
			 train-loss:  2.095078939657945 	 ± 0.263535713135835
	data : 0.11542959213256836
	model : 0.06991844177246094
			 train-loss:  2.090977430343628 	 ± 0.2636138041518956
	data : 0.11593208312988282
	model : 0.06949820518493652
			 train-loss:  2.0940298607100303 	 ± 0.26281168926930915
	data : 0.1163454532623291
	model : 0.06942744255065918
			 train-loss:  2.095748087939094 	 ± 0.26125093948125266
	data : 0.11636428833007813
	model : 0.06927742958068847
			 train-loss:  2.095354788545249 	 ± 0.259371185226933
	data : 0.11648898124694824
	model : 0.06997799873352051
			 train-loss:  2.094687182562692 	 ± 0.25757157477425385
	data : 0.11581621170043946
	model : 0.0698014259338379
			 train-loss:  2.1025612757239545 	 ± 0.26409998118739825
	data : 0.11605463027954102
	model : 0.07029142379760742
			 train-loss:  2.099228557613161 	 ± 0.2637587239114566
	data : 0.11564092636108399
	model : 0.07035207748413086
			 train-loss:  2.101619436316294 	 ± 0.2627303612188425
	data : 0.11549606323242187
	model : 0.07047829627990723
			 train-loss:  2.1075165755039937 	 ± 0.26576890040561035
	data : 0.1154242992401123
	model : 0.06987075805664063
			 train-loss:  2.1054666980107624 	 ± 0.26457944265644184
	data : 0.11587686538696289
	model : 0.06985468864440918
			 train-loss:  2.1044421023444126 	 ± 0.2629827628312376
	data : 0.11574044227600097
	model : 0.06969652175903321
			 train-loss:  2.098096952809916 	 ± 0.2670610093404743
	data : 0.11605191230773926
	model : 0.06972255706787109
			 train-loss:  2.0988089793767686 	 ± 0.26541710767299315
	data : 0.11620235443115234
	model : 0.06972064971923828
			 train-loss:  2.1002068821387954 	 ± 0.2640207165765663
	data : 0.1161341667175293
	model : 0.06967096328735352
			 train-loss:  2.1014396637678145 	 ± 0.26259410187564464
	data : 0.11629481315612793
	model : 0.07054753303527832
			 train-loss:  2.0991248113137706 	 ± 0.2617881609570306
	data : 0.11539549827575683
	model : 0.07001323699951172
			 train-loss:  2.104789411149374 	 ± 0.26513463639636126
	data : 0.11594228744506836
	model : 0.06976666450500488
			 train-loss:  2.108561903597361 	 ± 0.26573752176733983
	data : 0.1160658359527588
	model : 0.06964073181152344
			 train-loss:  2.1071382065614066 	 ± 0.2644692656688923
	data : 0.11623635292053222
	model : 0.0694542407989502
			 train-loss:  2.1099285812938913 	 ± 0.2641498855575068
	data : 0.11660575866699219
	model : 0.06839632987976074
			 train-loss:  2.106276499670605 	 ± 0.26475937653296117
	data : 0.11785349845886231
	model : 0.06914281845092773
			 train-loss:  2.1030931692013795 	 ± 0.2648835550633958
	data : 0.11697802543640137
	model : 0.06944766044616699
			 train-loss:  2.1017189784483477 	 ± 0.26368594646100363
	data : 0.11673569679260254
	model : 0.0695946216583252
			 train-loss:  2.102578749817409 	 ± 0.2623243974115791
	data : 0.11652231216430664
	model : 0.0703047752380371
			 train-loss:  2.101227594746484 	 ± 0.26117421133305146
	data : 0.11576004028320312
	model : 0.07108168601989746
			 train-loss:  2.1026495564114915 	 ± 0.2600853020455069
	data : 0.11506309509277343
	model : 0.07105674743652343
			 train-loss:  2.1002541534278705 	 ± 0.2596752846472446
	data : 0.1152298927307129
	model : 0.07039504051208496
			 train-loss:  2.1021794926735664 	 ± 0.2589347869378928
	data : 0.1159433364868164
	model : 0.07039422988891601
			 train-loss:  2.1013468567361224 	 ± 0.2576789297353494
	data : 0.11590423583984374
	model : 0.06995120048522949
			 train-loss:  2.1040550018611706 	 ± 0.25766043677763206
	data : 0.11631078720092773
	model : 0.0684730052947998
			 train-loss:  2.101757944871982 	 ± 0.2572909138675242
	data : 0.11754498481750489
	model : 0.06851234436035156
			 train-loss:  2.102572159668834 	 ± 0.2560855271340851
	data : 0.11737914085388183
	model : 0.06831169128417969
			 train-loss:  2.1075861149904678 	 ± 0.25951718855639555
	data : 0.11734623908996582
	model : 0.06834950447082519
			 train-loss:  2.1061564382880626 	 ± 0.25859076954614113
	data : 0.11721482276916503
	model : 0.06826639175415039
			 train-loss:  2.1043084239959717 	 ± 0.25795076033823633
	data : 0.11715536117553711
	model : 0.06828160285949707
			 train-loss:  2.1047451378095268 	 ± 0.25670774956486536
	data : 0.1170318603515625
	model : 0.06750202178955078
			 train-loss:  2.1047896567512963 	 ± 0.25544667060297754
	data : 0.117767333984375
	model : 0.06809091567993164
			 train-loss:  2.1082914107054185 	 ± 0.2566519625340021
	data : 0.11749825477600098
	model : 0.06810288429260254
			 train-loss:  2.10993597140679 	 ± 0.2559598288938666
	data : 0.11764249801635743
	model : 0.0681830883026123
			 train-loss:  2.1077457723163424 	 ± 0.2557153937228348
	data : 0.11766815185546875
	model : 0.06922249794006348
			 train-loss:  2.107490602529274 	 ± 0.25451976169615126
	data : 0.1167985439300537
	model : 0.06932401657104492
			 train-loss:  2.1060058034469034 	 ± 0.2537884478178254
	data : 0.1166067123413086
	model : 0.06954069137573242
			 train-loss:  2.105651686588923 	 ± 0.25263732544094897
	data : 0.11647052764892578
	model : 0.06932005882263184
			 train-loss:  2.103888876941226 	 ± 0.2521421659379082
	data : 0.11666984558105468
	model : 0.06908183097839356
			 train-loss:  2.105907130241394 	 ± 0.25187637036163263
	data : 0.11696443557739258
	model : 0.06894245147705078
			 train-loss:  2.1028736969372175 	 ± 0.2527495756343196
	data : 0.11711139678955078
	model : 0.0700223445892334
			 train-loss:  2.1025063672236035 	 ± 0.2516484596227217
	data : 0.11617341041564941
	model : 0.0701167106628418
			 train-loss:  2.102874365528073 	 ± 0.250562764824517
	data : 0.11595945358276367
	model : 0.06939926147460937
			 train-loss:  2.103665694855807 	 ± 0.24960317164540766
	data : 0.11663827896118165
	model : 0.06920080184936524
			 train-loss:  2.1008889125741046 	 ± 0.25027782216504635
	data : 0.1167381763458252
	model : 0.06902103424072266
			 train-loss:  2.0977629371758164 	 ± 0.25144133518862755
	data : 0.11678862571716309
	model : 0.06833267211914062
			 train-loss:  2.100585606363085 	 ± 0.2522034996084164
	data : 0.1174734115600586
	model : 0.06822686195373535
			 train-loss:  2.101987480107 	 ± 0.2515899450231925
	data : 0.1177337646484375
	model : 0.06820540428161621
			 train-loss:  2.105370248065275 	 ± 0.2532111307139193
	data : 0.11774263381958008
	model : 0.06869745254516602
			 train-loss:  2.1041178832451504 	 ± 0.25252370075748554
	data : 0.1173187255859375
	model : 0.06929049491882325
			 train-loss:  2.102052084670579 	 ± 0.2524941825394961
	data : 0.11693568229675293
	model : 0.06865892410278321
			 train-loss:  2.105333655584054 	 ± 0.254034953717491
	data : 0.11752371788024903
	model : 0.06838173866271972
			 train-loss:  2.1056040002078547 	 ± 0.2530178047721581
	data : 0.11762876510620117
	model : 0.06927337646484374
			 train-loss:  2.107248184181029 	 ± 0.2526544007070427
	data : 0.11691102981567383
	model : 0.06907238960266113
			 train-loss:  2.1081507062911986 	 ± 0.25184236329660803
	data : 0.11688547134399414
	model : 0.0686563491821289
			 train-loss:  2.106726644531129 	 ± 0.2513457783362971
	data : 0.1172116756439209
	model : 0.07038516998291015
			 train-loss:  2.105414405582458 	 ± 0.2507872206790006
	data : 0.11544079780578613
	model : 0.07077670097351074
			 train-loss:  2.1075272783637047 	 ± 0.2509378932526574
	data : 0.11503863334655762
	model : 0.0707634449005127
			 train-loss:  2.1060105663861415 	 ± 0.25055167377303844
	data : 0.11511735916137696
	model : 0.06995534896850586
			 train-loss:  2.1060142021912793 	 ± 0.249586156550502
	data : 0.11599583625793457
	model : 0.07081046104431152
			 train-loss:  2.1064869028921347 	 ± 0.2486901214898185
	data : 0.11513862609863282
	model : 0.07073087692260742
			 train-loss:  2.1053503986560935 	 ± 0.2480875761236373
	data : 0.11519241333007812
	model : 0.07006502151489258
			 train-loss:  2.103628279571246 	 ± 0.24794385313237155
	data : 0.1157686710357666
	model : 0.06996588706970215
			 train-loss:  2.1029344076540935 	 ± 0.24714653746464224
	data : 0.11588034629821778
	model : 0.07028145790100097
			 train-loss:  2.1024249368243746 	 ± 0.24630009580821682
	data : 0.11577448844909669
	model : 0.06970500946044922
			 train-loss:  2.1041079312562943 	 ± 0.24617080206395076
	data : 0.11622834205627441
	model : 0.0699911117553711
			 train-loss:  2.1021503523318437 	 ± 0.24633086536233198
	data : 0.11595864295959472
	model : 0.07102365493774414
			 train-loss:  2.099826286668363 	 ± 0.24693960519998767
	data : 0.11507411003112793
	model : 0.07162370681762695
			 train-loss:  2.1035697400141107 	 ± 0.24994864410290177
	data : 0.11447381973266602
	model : 0.07186341285705566
			 train-loss:  2.1019993007183073 	 ± 0.24974165126065864
	data : 0.11427578926086426
	model : 0.0718581199645996
			 train-loss:  2.1031658894626806 	 ± 0.24923698706604888
	data : 0.11446971893310547
	model : 0.06984562873840332
			 train-loss:  2.1048032423140297 	 ± 0.2491176986421596
	data : 0.1163400650024414
	model : 0.06913838386535645
			 train-loss:  2.1074409726616388 	 ± 0.25022715001104584
	data : 0.11695733070373535
	model : 0.06872472763061524
			 train-loss:  2.107594916390048 	 ± 0.24936358728024033
	data : 0.11722874641418457
	model : 0.06823844909667968
			 train-loss:  2.1063259519379716 	 ± 0.24896834116869818
	data : 0.11765990257263184
	model : 0.0680229663848877
			 train-loss:  2.1077659113766396 	 ± 0.24871938718697745
	data : 0.1178983211517334
	model : 0.06896586418151855
			 train-loss:  2.108761422488154 	 ± 0.2481636563857808
	data : 0.1169672966003418
	model : 0.0693636417388916
			 train-loss:  2.1078189933622204 	 ± 0.24758765218163173
	data : 0.11654210090637207
	model : 0.06945042610168457
			 train-loss:  2.1050611054337263 	 ± 0.24902593936820294
	data : 0.11651263236999512
	model : 0.07021517753601074
			 train-loss:  2.1034240770339965 	 ± 0.24899757386423385
	data : 0.11570782661437988
	model : 0.07005324363708496
			 train-loss:  2.1032737927721037 	 ± 0.24817853441940296
	data : 0.11592726707458496
	model : 0.07000083923339843
			 train-loss:  2.1042068663396334 	 ± 0.24762640234950747
	data : 0.11608395576477051
	model : 0.06941032409667969
			 train-loss:  2.102693205565409 	 ± 0.24752033475875826
	data : 0.1166832447052002
	model : 0.0693120002746582
			 train-loss:  2.10212196622576 	 ± 0.2468165492793363
	data : 0.11671929359436035
	model : 0.06939640045166015
			 train-loss:  2.1012600029668502 	 ± 0.24625150969327694
	data : 0.11649546623229981
	model : 0.06865615844726562
			 train-loss:  2.101178613228676 	 ± 0.24546306460604458
	data : 0.11709952354431152
	model : 0.06875815391540527
			 train-loss:  2.1013967284731043 	 ± 0.24469525175264698
	data : 0.11722650527954101
	model : 0.0684347152709961
			 train-loss:  2.0991131776495826 	 ± 0.2455921386559765
	data : 0.11753830909729004
	model : 0.06842007637023925
			 train-loss:  2.0978582407693445 	 ± 0.24532628218336347
	data : 0.1176109790802002
	model : 0.06844120025634766
			 train-loss:  2.096758808195591 	 ± 0.2449510566232403
	data : 0.11779046058654785
	model : 0.06899881362915039
			 train-loss:  2.094415894206266 	 ± 0.24598093899920317
	data : 0.11718435287475586
	model : 0.06899418830871581
			 train-loss:  2.0955556748825828 	 ± 0.2456466563592992
	data : 0.11731491088867188
	model : 0.06965703964233398
			 train-loss:  2.097981126761875 	 ± 0.24683009960229765
	data : 0.11667709350585938
	model : 0.06954617500305176
			 train-loss:  2.097866606421587 	 ± 0.24608076174157678
	data : 0.11679902076721191
	model : 0.0693861961364746
			 train-loss:  2.097454698158033 	 ± 0.24539063231320657
	data : 0.11682171821594238
	model : 0.06976203918457032
			 train-loss:  2.095354434955551 	 ± 0.24613338753968167
	data : 0.1166140079498291
	model : 0.06973309516906738
			 train-loss:  2.0960023945677064 	 ± 0.24553731997204875
	data : 0.11653523445129395
	model : 0.0689936637878418
			 train-loss:  2.09623816325551 	 ± 0.24482442267590926
	data : 0.1170198917388916
	model : 0.06895828247070312
			 train-loss:  2.097148841654761 	 ± 0.2443842417514057
	data : 0.11682782173156739
	model : 0.06869468688964844
			 train-loss:  2.098614793665269 	 ± 0.2444085218820815
	data : 0.11712808609008789
	model : 0.06846017837524414
			 train-loss:  2.0970688458771733 	 ± 0.2445250238431667
	data : 0.1171783447265625
	model : 0.06820964813232422
			 train-loss:  2.0984725141248037 	 ± 0.24450311900549995
	data : 0.11724424362182617
	model : 0.06898617744445801
			 train-loss:  2.098083681453859 	 ± 0.2438487659076609
	data : 0.1167973518371582
	model : 0.06836404800415039
			 train-loss:  2.096251165044719 	 ± 0.24433877684460945
	data : 0.1174459457397461
	model : 0.06852660179138184
			 train-loss:  2.0952256311689106 	 ± 0.24401492993149487
	data : 0.11718306541442872
	model : 0.0686711311340332
			 train-loss:  2.097804144024849 	 ± 0.24570001912952089
	data : 0.11701140403747559
	model : 0.06889734268188477
			 train-loss:  2.103083175454436 	 ± 0.2548180387075609
	data : 0.11673936843872071
	model : 0.06900115013122558
			 train-loss:  2.1052899883034524 	 ± 0.2557917885468902
	data : 0.11665225028991699
	model : 0.06942582130432129
			 train-loss:  2.1074449829549096 	 ± 0.2566915344183688
	data : 0.11648492813110352
	model : 0.06943669319152831
			 train-loss:  2.10733380317688 	 ± 0.25598183114033884
	data : 0.11638507843017579
	model : 0.06874232292175293
			 train-loss:  2.105553310220413 	 ± 0.2563889620266469
	data : 0.11732745170593262
	model : 0.06861815452575684
			 train-loss:  2.1040120209966386 	 ± 0.2565230896044386
	data : 0.11756038665771484
	model : 0.06860990524291992
			 train-loss:  2.1021381238770616 	 ± 0.25706730934597755
	data : 0.11751832962036132
	model : 0.06904692649841308
			 train-loss:  2.101522807193839 	 ± 0.25650290057155456
	data : 0.11731328964233398
	model : 0.06918997764587402
			 train-loss:  2.104232120513916 	 ± 0.25843514824650454
	data : 0.11742377281188965
	model : 0.06966576576232911
			 train-loss:  2.102335520969924 	 ± 0.2590272346618412
	data : 0.1167104721069336
	model : 0.06977415084838867
			 train-loss:  2.102322217614893 	 ± 0.2583337837839491
	data : 0.11664791107177734
	model : 0.06974077224731445
			 train-loss:  2.1023335444166307 	 ± 0.2576458563284176
	data : 0.11656384468078614
	model : 0.06970210075378418
			 train-loss:  2.1007398518304976 	 ± 0.2578907829895705
	data : 0.11648721694946289
	model : 0.06987485885620118
			 train-loss:  2.098428303944437 	 ± 0.25916691646983037
	data : 0.1162938117980957
	model : 0.07004280090332031
			 train-loss:  2.0977855870861033 	 ± 0.2586393521497967
	data : 0.1161618709564209
	model : 0.06983432769775391
			 train-loss:  2.097392172242204 	 ± 0.25802222513433387
	data : 0.11626019477844238
	model : 0.06979489326477051
			 train-loss:  2.0974937566203775 	 ± 0.2573567550512865
	data : 0.11643977165222168
	model : 0.06977238655090331
			 train-loss:  2.0969611824173287 	 ± 0.25679921406417044
	data : 0.11647052764892578
	model : 0.0697248935699463
			 train-loss:  2.096221786278945 	 ± 0.25634686200117107
	data : 0.11645393371582032
	model : 0.06986727714538574
			 train-loss:  2.0973944864710985 	 ± 0.2562159414010079
	data : 0.11630287170410156
	model : 0.07000350952148438
			 train-loss:  2.0953490643331847 	 ± 0.25716413523213305
	data : 0.1161778450012207
	model : 0.07011213302612304
			 train-loss:  2.0951272508110663 	 ± 0.25653280114075033
	data : 0.11610975265502929
	model : 0.0693575382232666
			 train-loss:  2.0932650338465244 	 ± 0.2572256089057812
	data : 0.11663317680358887
	model : 0.06939401626586914
			 train-loss:  2.0937946021556852 	 ± 0.2566904690162543
	data : 0.11685023307800294
	model : 0.06841378211975098
			 train-loss:  2.091153398675112 	 ± 0.2587612357351332
	data : 0.11781811714172363
	model : 0.06757636070251465
			 train-loss:  2.090466841612712 	 ± 0.2583034038409155
	data : 0.11861772537231445
	model : 0.067622709274292
			 train-loss:  2.0902193144624457 	 ± 0.25769041846410035
	data : 0.11859197616577148
	model : 0.06829357147216797
			 train-loss:  2.0896091046286562 	 ± 0.25720503205867107
	data : 0.11794109344482422
	model : 0.06818180084228516
			 train-loss:  2.0914890539355393 	 ± 0.25797810508612934
	data : 0.11798653602600098
	model : 0.06906604766845703
			 train-loss:  2.0895431412076486 	 ± 0.2588549390722873
	data : 0.11707649230957032
	model : 0.06902513504028321
			 train-loss:  2.0911417180213374 	 ± 0.25924621855306984
	data : 0.11698918342590332
	model : 0.06877870559692383
			 train-loss:  2.0911214844538617 	 ± 0.2586224435373871
	data : 0.11700663566589356
	model : 0.06882052421569824
			 train-loss:  2.0908066986850575 	 ± 0.25804292742749124
	data : 0.11703901290893555
	model : 0.0688706398010254
			 train-loss:  2.091332263038272 	 ± 0.2575399095636626
	data : 0.11714153289794922
	model : 0.06882963180541993
			 train-loss:  2.0918224671440666 	 ± 0.2570270860907494
	data : 0.11722517013549805
	model : 0.06967215538024903
			 train-loss:  2.091694033370828 	 ± 0.2564269602000766
	data : 0.11658544540405273
	model : 0.06977872848510742
			 train-loss:  2.0909438368300317 	 ± 0.2560573969064526
	data : 0.11665425300598145
	model : 0.06973471641540527
			 train-loss:  2.089510693171314 	 ± 0.25631326738978927
	data : 0.11659493446350097
	model : 0.06985001564025879
			 train-loss:  2.08782423762388 	 ± 0.2569038152315133
	data : 0.11640791893005371
	model : 0.06907262802124023
			 train-loss:  2.087917916752674 	 ± 0.25631212126412584
	data : 0.11730341911315918
	model : 0.06911110877990723
			 train-loss:  2.0883525522073842 	 ± 0.2558006284807757
	data : 0.11715397834777833
	model : 0.06907877922058106
			 train-loss:  2.087527989247523 	 ± 0.25550214270832594
	data : 0.11712794303894043
	model : 0.06918482780456543
			 train-loss:  2.0879434907817407 	 ± 0.25499194584922663
	data : 0.11720619201660157
	model : 0.06911568641662598
			 train-loss:  2.087514066696167 	 ± 0.25449111505465577
	data : 0.11695423126220703
	model : 0.06965556144714355
			 train-loss:  2.0865400862370143 	 ± 0.25432532393822166
	data : 0.11624107360839844
	model : 0.06985673904418946
			 train-loss:  2.0856895661568857 	 ± 0.2540666857851594
	data : 0.11600589752197266
	model : 0.06997642517089844
			 train-loss:  2.0838784166515674 	 ± 0.2549286915171699
	data : 0.11590752601623536
	model : 0.07009491920471192
			 train-loss:  2.085958375462464 	 ± 0.2562484319983055
	data : 0.11557717323303222
	model : 0.06949806213378906
			 train-loss:  2.087818449868096 	 ± 0.25718949161528026
	data : 0.11609139442443847
	model : 0.06926403045654297
			 train-loss:  2.0864075385363754 	 ± 0.257491072049586
	data : 0.11638875007629394
	model : 0.06925711631774903
			 train-loss:  2.086342840467781 	 ± 0.2569251261013071
	data : 0.11633257865905762
	model : 0.06932768821716309
			 train-loss:  2.0883665309663404 	 ± 0.2581678503800747
	data : 0.11617956161499024
	model : 0.06905932426452636
			 train-loss:  2.0895896400426675 	 ± 0.25826473857254295
	data : 0.11634860038757325
	model : 0.06961269378662109
			 train-loss:  2.089903696205305 	 ± 0.2577465010740574
	data : 0.11581850051879883
	model : 0.07027764320373535
			 train-loss:  2.08983508178166 	 ± 0.2571901082408021
	data : 0.11498689651489258
	model : 0.07009358406066894
			 train-loss:  2.0894476344873167 	 ± 0.2567027719489872
	data : 0.11543760299682618
	model : 0.07004251480102539
			 train-loss:  2.0895857304462546 	 ± 0.25615995137089503
	data : 0.11552529335021973
	model : 0.07028136253356934
			 train-loss:  2.0877337027818728 	 ± 0.2571705537756858
	data : 0.11530866622924804
	model : 0.07034802436828613
			 train-loss:  2.0869912264194896 	 ± 0.25687401277438454
	data : 0.11544818878173828
	model : 0.0706024169921875
			 train-loss:  2.0868188380184822 	 ± 0.2563428325449815
	data : 0.1155776023864746
	model : 0.0708160400390625
			 train-loss:  2.085138848059288 	 ± 0.2571001009306564
	data : 0.11541237831115722
	model : 0.07093224525451661
			 train-loss:  2.0871728638640974 	 ± 0.25846324704896656
	data : 0.11530003547668458
	model : 0.07111077308654785
			 train-loss:  2.0860415347949233 	 ± 0.2585118103078489
	data : 0.11516056060791016
	model : 0.07115230560302735
			 train-loss:  2.086416032413642 	 ± 0.25803764060403606
	data : 0.11510629653930664
	model : 0.07114720344543457
			 train-loss:  2.087698005047082 	 ± 0.2582664769591422
	data : 0.11502490043640137
	model : 0.0714301586151123
			 train-loss:  2.087174645140151 	 ± 0.25786034598866264
	data : 0.11460442543029785
	model : 0.07125210762023926
			 train-loss:  2.0872134573665666 	 ± 0.2573299304833849
	data : 0.11482753753662109
	model : 0.07104392051696777
			 train-loss:  2.0870337261528267 	 ± 0.2568173567735435
	data : 0.11493921279907227
	model : 0.07109184265136718
			 train-loss:  2.087220267860257 	 ± 0.25630926766745193
	data : 0.11482391357421876
	model : 0.07062773704528809
			 train-loss:  2.088243288722465 	 ± 0.256288509658571
	data : 0.11501431465148926
	model : 0.07017755508422852
			 train-loss:  2.0868996854735773 	 ± 0.2566358708565983
	data : 0.11532049179077149
	model : 0.06945357322692872
			 train-loss:  2.0883117880552047 	 ± 0.2570776615615516
	data : 0.11601619720458985
	model : 0.06928167343139649
			 train-loss:  2.088446689418042 	 ± 0.2565697174373181
	data : 0.116229248046875
	model : 0.0690880298614502
			 train-loss:  2.089374299526215 	 ± 0.2564740971207911
	data : 0.1163515567779541
	model : 0.06840677261352539
			 train-loss:  2.089487288102686 	 ± 0.25596891709516445
	data : 0.1170717716217041
	model : 0.06825518608093262
			 train-loss:  2.087908441112155 	 ± 0.2566822323189487
	data : 0.11728019714355468
	model : 0.06919069290161133
			 train-loss:  2.0890560357466987 	 ± 0.2568213924881787
	data : 0.11649317741394043
	model : 0.06895132064819336
			 train-loss:  2.0888610346110785 	 ± 0.2563341061038384
	data : 0.11702699661254883
	model : 0.06854071617126464
			 train-loss:  2.0884676811741847 	 ± 0.25590779472178915
	data : 0.11772522926330567
	model : 0.06951699256896973
			 train-loss:  2.088792280294001 	 ± 0.2554600785774656
	data : 0.1159388542175293
	model : 0.06116304397583008
#epoch  41    val-loss:  2.4636744825463546  train-loss:  2.088792280294001  lr:  0.0003125
			 train-loss:  2.02881121635437 	 ± 0.0
	data : 5.320293188095093
	model : 0.07120490074157715
			 train-loss:  2.0222561359405518 	 ± 0.006555080413818359
	data : 2.7297168970108032
	model : 0.08495140075683594
			 train-loss:  2.278079112370809 	 ± 0.36182791018948895
	data : 1.8543678919474285
	model : 0.08156371116638184
			 train-loss:  2.1695189476013184 	 ± 0.36543878498102317
	data : 1.418609619140625
	model : 0.07766580581665039
			 train-loss:  2.1952099800109863 	 ± 0.330872363668868
	data : 1.1589062213897705
	model : 0.07609305381774903
			 train-loss:  2.2189388275146484 	 ± 0.3066687548791257
	data : 0.1178666591644287
	model : 0.07577104568481445
			 train-loss:  2.2186933926173618 	 ± 0.28392073373763255
	data : 0.1132420539855957
	model : 0.06998510360717773
			 train-loss:  2.197542756795883 	 ± 0.27141491472452456
	data : 0.11562790870666503
	model : 0.06904535293579102
			 train-loss:  2.177090644836426 	 ± 0.2623494800407198
	data : 0.11649646759033203
	model : 0.06954197883605957
			 train-loss:  2.15391845703125 	 ± 0.25841261056492093
	data : 0.11602044105529785
	model : 0.06867012977600098
			 train-loss:  2.131327683275396 	 ± 0.2565343339536505
	data : 0.11691203117370605
	model : 0.06876902580261231
			 train-loss:  2.113126277923584 	 ± 0.25292273888853933
	data : 0.11692123413085938
	model : 0.06791009902954101
			 train-loss:  2.0998836205555844 	 ± 0.24729246872299826
	data : 0.11784424781799316
	model : 0.06698899269104004
			 train-loss:  2.097563854285649 	 ± 0.23844372585012794
	data : 0.11869206428527831
	model : 0.06723408699035645
			 train-loss:  2.0839942852656046 	 ± 0.2358875040174758
	data : 0.11849980354309082
	model : 0.06816062927246094
			 train-loss:  2.085104174911976 	 ± 0.22843754117378962
	data : 0.1177527904510498
	model : 0.06814584732055665
			 train-loss:  2.0915626147214104 	 ± 0.22311759975646153
	data : 0.11757059097290039
	model : 0.06903247833251953
			 train-loss:  2.101112915409936 	 ± 0.22037778155933602
	data : 0.11685843467712402
	model : 0.07042202949523926
			 train-loss:  2.117501980380008 	 ± 0.22548851003440123
	data : 0.11568059921264648
	model : 0.07060732841491699
			 train-loss:  2.131124585866928 	 ± 0.2276592782525029
	data : 0.11553826332092285
	model : 0.06969976425170898
			 train-loss:  2.121088499114627 	 ± 0.22666091888597142
	data : 0.11635365486145019
	model : 0.06881346702575683
			 train-loss:  2.121442583474246 	 ± 0.22145557103605074
	data : 0.11715106964111328
	model : 0.06888003349304199
			 train-loss:  2.124384905980981 	 ± 0.2170270583391664
	data : 0.11705036163330078
	model : 0.06856336593627929
			 train-loss:  2.121629372239113 	 ± 0.2128681546682628
	data : 0.11716284751892089
	model : 0.06858892440795898
			 train-loss:  2.1326842737197875 	 ± 0.21548410044115737
	data : 0.11702418327331543
	model : 0.06872334480285644
			 train-loss:  2.121768836791699 	 ± 0.2182342015375098
	data : 0.11707687377929688
	model : 0.0689004898071289
			 train-loss:  2.1127075619167752 	 ± 0.2190821966324037
	data : 0.11698055267333984
	model : 0.06884512901306153
			 train-loss:  2.1222356046949113 	 ± 0.2207577567619673
	data : 0.11697754859924317
	model : 0.06864614486694336
			 train-loss:  2.127190219944921 	 ± 0.21849680729490475
	data : 0.11726202964782714
	model : 0.06857614517211914
			 train-loss:  2.12055557568868 	 ± 0.2177751813207001
	data : 0.11735243797302246
	model : 0.06940646171569824
			 train-loss:  2.1027026099543416 	 ± 0.23549524250406964
	data : 0.116436767578125
	model : 0.06949248313903808
			 train-loss:  2.102212868630886 	 ± 0.23180246266318527
	data : 0.11648702621459961
	model : 0.06939835548400879
			 train-loss:  2.105738712079597 	 ± 0.22913301379483056
	data : 0.11652402877807617
	model : 0.06947455406188965
			 train-loss:  2.089416675707873 	 ± 0.24443660960847208
	data : 0.1166337013244629
	model : 0.06934690475463867
			 train-loss:  2.0886511291776384 	 ± 0.2409607034439505
	data : 0.11670851707458496
	model : 0.069390869140625
			 train-loss:  2.091625223557154 	 ± 0.2382410730660425
	data : 0.11676626205444336
	model : 0.07004327774047851
			 train-loss:  2.0831242542009094 	 ± 0.24047116316403466
	data : 0.11606979370117188
	model : 0.06999406814575196
			 train-loss:  2.079171252878089 	 ± 0.23850116054867274
	data : 0.11612672805786133
	model : 0.06994028091430664
			 train-loss:  2.0869768919088902 	 ± 0.24029052037828189
	data : 0.11590242385864258
	model : 0.0699491024017334
			 train-loss:  2.085605910420418 	 ± 0.23742230276134862
	data : 0.11591124534606934
	model : 0.07018756866455078
			 train-loss:  2.096185687111645 	 ± 0.24386831415853144
	data : 0.11587634086608886
	model : 0.069268798828125
			 train-loss:  2.0941833342824663 	 ± 0.24128851316034863
	data : 0.11680889129638672
	model : 0.06974859237670898
			 train-loss:  2.0961453997811605 	 ± 0.2388051028113482
	data : 0.11643290519714355
	model : 0.06880836486816407
			 train-loss:  2.1008900214325297 	 ± 0.23811715750992393
	data : 0.11737275123596191
	model : 0.0688094139099121
			 train-loss:  2.1002102136611938 	 ± 0.2354997232962035
	data : 0.11722283363342285
	model : 0.06752123832702636
			 train-loss:  2.095047442809395 	 ± 0.23548652620938962
	data : 0.11823101043701172
	model : 0.06891188621520997
			 train-loss:  2.0946514403566403 	 ± 0.232983362824678
	data : 0.11680121421813965
	model : 0.06866998672485351
			 train-loss:  2.0953211983044944 	 ± 0.2305893993477344
	data : 0.11692600250244141
	model : 0.06948199272155761
			 train-loss:  2.0953749929155623 	 ± 0.22822462166415072
	data : 0.11632390022277832
	model : 0.06893696784973144
			 train-loss:  2.089096682071686 	 ± 0.2301655720142227
	data : 0.11697163581848144
	model : 0.06983680725097656
			 train-loss:  2.0887418237386965 	 ± 0.22791168888042762
	data : 0.11618690490722657
	model : 0.06935200691223145
			 train-loss:  2.0911406943431268 	 ± 0.2263587936888301
	data : 0.11679286956787109
	model : 0.06926889419555664
			 train-loss:  2.0926859086414553 	 ± 0.22448987231739878
	data : 0.11689109802246093
	model : 0.06942954063415527
			 train-loss:  2.1036253880571434 	 ± 0.23623098048731273
	data : 0.11652173995971679
	model : 0.0699437141418457
			 train-loss:  2.1046555150638926 	 ± 0.2341959460004862
	data : 0.11587214469909668
	model : 0.06991019248962402
			 train-loss:  2.1058059696640288 	 ± 0.2322522598226336
	data : 0.11596498489379883
	model : 0.06901488304138184
			 train-loss:  2.112991117594535 	 ± 0.23640188775675175
	data : 0.11677441596984864
	model : 0.06890296936035156
			 train-loss:  2.1082705875922896 	 ± 0.2370494829206307
	data : 0.11661758422851562
	model : 0.06881151199340821
			 train-loss:  2.103284514556497 	 ± 0.23807976905228964
	data : 0.11682829856872559
	model : 0.06797986030578614
			 train-loss:  2.102647012472153 	 ± 0.23613821154217451
	data : 0.11768813133239746
	model : 0.06785564422607422
			 train-loss:  2.101346826944195 	 ± 0.2344111024698944
	data : 0.11763482093811035
	model : 0.06854472160339356
			 train-loss:  2.099436725339582 	 ± 0.23299110618410998
	data : 0.11715226173400879
	model : 0.06849198341369629
			 train-loss:  2.0962455348363 	 ± 0.23249640941370148
	data : 0.11741509437561035
	model : 0.06857495307922364
			 train-loss:  2.099087793380022 	 ± 0.23177342144930124
	data : 0.11734676361083984
	model : 0.06948790550231934
			 train-loss:  2.10158508007343 	 ± 0.23084974809899877
	data : 0.11654877662658691
	model : 0.07018589973449707
			 train-loss:  2.1014717492190274 	 ± 0.2290960332938777
	data : 0.11592459678649902
	model : 0.06941933631896972
			 train-loss:  2.1032588659827387 	 ± 0.2278429816167238
	data : 0.11646790504455566
	model : 0.06948180198669433
			 train-loss:  2.105649835923139 	 ± 0.22700666876579592
	data : 0.1164250373840332
	model : 0.06938710212707519
			 train-loss:  2.101892139600671 	 ± 0.22747607750230014
	data : 0.11639680862426757
	model : 0.06936626434326172
			 train-loss:  2.1118356909070695 	 ± 0.24047550185438418
	data : 0.11656351089477539
	model : 0.06853189468383789
			 train-loss:  2.1085808293920167 	 ± 0.24032388600652407
	data : 0.11719489097595215
	model : 0.06863470077514648
			 train-loss:  2.1047183589802847 	 ± 0.2408581233914395
	data : 0.1173595905303955
	model : 0.06820979118347167
			 train-loss:  2.1067270595733434 	 ± 0.2398092016300952
	data : 0.11771130561828613
	model : 0.06747946739196778
			 train-loss:  2.105529809320295 	 ± 0.23840291676798347
	data : 0.11832027435302735
	model : 0.06739997863769531
			 train-loss:  2.101731621424357 	 ± 0.23905162447228823
	data : 0.11831612586975097
	model : 0.06732516288757324
			 train-loss:  2.1019550436421444 	 ± 0.2374815911671997
	data : 0.1184502124786377
	model : 0.06794929504394531
			 train-loss:  2.100382348159691 	 ± 0.23633249290080066
	data : 0.11767268180847168
	model : 0.06841859817504883
			 train-loss:  2.100008434210068 	 ± 0.2348355764985754
	data : 0.11740589141845703
	model : 0.06835212707519531
			 train-loss:  2.0979469214813617 	 ± 0.23405375950632953
	data : 0.11749944686889649
	model : 0.06917285919189453
			 train-loss:  2.097096213698387 	 ± 0.23270919707789323
	data : 0.11672515869140625
	model : 0.06966381072998047
			 train-loss:  2.0939341209552906 	 ± 0.2329912321275741
	data : 0.11632294654846191
	model : 0.06990156173706055
			 train-loss:  2.093819089052154 	 ± 0.23156851020463293
	data : 0.11617522239685059
	model : 0.06933469772338867
			 train-loss:  2.09738855476839 	 ± 0.23242777790611474
	data : 0.11655879020690918
	model : 0.069270658493042
			 train-loss:  2.095079922959918 	 ± 0.23199551009467725
	data : 0.11647443771362305
	model : 0.06843743324279786
			 train-loss:  2.0961206898969764 	 ± 0.23082397217943446
	data : 0.11719245910644531
	model : 0.06808953285217285
			 train-loss:  2.099017931971439 	 ± 0.2310274132116497
	data : 0.11745457649230957
	model : 0.0680344581604004
			 train-loss:  2.1018767891259027 	 ± 0.23122080090074892
	data : 0.1172414779663086
	model : 0.06857123374938964
			 train-loss:  2.0986729128794237 	 ± 0.2318373645719832
	data : 0.11688079833984374
	model : 0.06929960250854492
			 train-loss:  2.10027234741811 	 ± 0.23101897749068004
	data : 0.11650424003601074
	model : 0.06918582916259766
			 train-loss:  2.0988014101982118 	 ± 0.23015068096833158
	data : 0.11658854484558105
	model : 0.06988959312438965
			 train-loss:  2.098902315883846 	 ± 0.22888462532794754
	data : 0.1158670425415039
	model : 0.0698470115661621
			 train-loss:  2.0956370986026265 	 ± 0.2297584484992706
	data : 0.11600561141967773
	model : 0.06989798545837403
			 train-loss:  2.0927916739576604 	 ± 0.23014385545936872
	data : 0.11601753234863281
	model : 0.07022361755371094
			 train-loss:  2.098066731970361 	 ± 0.23450066971983724
	data : 0.11574692726135254
	model : 0.06950955390930176
			 train-loss:  2.096683743125514 	 ± 0.23364825255398672
	data : 0.1163848876953125
	model : 0.06897344589233398
			 train-loss:  2.0930558294057846 	 ± 0.23510255508502384
	data : 0.11691160202026367
	model : 0.06907153129577637
			 train-loss:  2.0929282080266893 	 ± 0.2338908892166482
	data : 0.11688871383666992
	model : 0.06918816566467285
			 train-loss:  2.0953014334853814 	 ± 0.23386546904876732
	data : 0.11671171188354493
	model : 0.06812148094177246
			 train-loss:  2.0943981565610326 	 ± 0.23285309014010064
	data : 0.11771702766418457
	model : 0.06908020973205567
			 train-loss:  2.0951390194892885 	 ± 0.23180313820542592
	data : 0.11683354377746583
	model : 0.06913433074951172
			 train-loss:  2.0949293575664556 	 ± 0.23066227219476904
	data : 0.11695623397827148
	model : 0.06893982887268066
			 train-loss:  2.0986836143568453 	 ± 0.23260912319630767
	data : 0.11698327064514161
	model : 0.06820292472839355
			 train-loss:  2.099192742005135 	 ± 0.2315343019769928
	data : 0.11760649681091309
	model : 0.06834826469421387
			 train-loss:  2.096404718664976 	 ± 0.23214929853804367
	data : 0.11745333671569824
	model : 0.06885042190551757
			 train-loss:  2.093757275172642 	 ± 0.23261332474936053
	data : 0.11706342697143554
	model : 0.06934151649475098
			 train-loss:  2.094969148905772 	 ± 0.23184629382250072
	data : 0.11671209335327148
	model : 0.06868996620178222
			 train-loss:  2.0916554638158495 	 ± 0.23326867809620894
	data : 0.11731529235839844
	model : 0.06932015419006347
			 train-loss:  2.092760748333401 	 ± 0.2324675402152252
	data : 0.11687583923339843
	model : 0.07046937942504883
			 train-loss:  2.0913796370182562 	 ± 0.231843424350137
	data : 0.11589808464050293
	model : 0.06970224380493165
			 train-loss:  2.0866250829263167 	 ± 0.2360651500921142
	data : 0.11665854454040528
	model : 0.06852512359619141
			 train-loss:  2.0870174427290222 	 ± 0.23503541506123574
	data : 0.1175544261932373
	model : 0.0697092056274414
			 train-loss:  2.084227883390018 	 ± 0.23582234739913396
	data : 0.11666011810302734
	model : 0.06892695426940917
			 train-loss:  2.0831767394479397 	 ± 0.23503996586298184
	data : 0.11749629974365235
	model : 0.06785688400268555
			 train-loss:  2.085521520229808 	 ± 0.23533054326660605
	data : 0.11836476325988769
	model : 0.06833744049072266
			 train-loss:  2.0829672844513603 	 ± 0.23588693279571968
	data : 0.1177816390991211
	model : 0.06900615692138672
			 train-loss:  2.084505043153105 	 ± 0.2354461887543218
	data : 0.11714868545532227
	model : 0.06847262382507324
			 train-loss:  2.0819753795607476 	 ± 0.23601570312899403
	data : 0.11753406524658203
	model : 0.06928391456604004
			 train-loss:  2.0818951119810847 	 ± 0.23501511254882332
	data : 0.11666288375854492
	model : 0.07003660202026367
			 train-loss:  2.0804000902576605 	 ± 0.23458838083004216
	data : 0.11601405143737793
	model : 0.0698096752166748
			 train-loss:  2.08206836382548 	 ± 0.23431667485956584
	data : 0.11626834869384765
	model : 0.07032036781311035
			 train-loss:  2.0800276186840594 	 ± 0.234414816992506
	data : 0.11583123207092286
	model : 0.06963262557983399
			 train-loss:  2.0807026302228206 	 ± 0.23357017486775106
	data : 0.11653280258178711
	model : 0.06869726181030274
			 train-loss:  2.0842402049196447 	 ± 0.23587761831926618
	data : 0.11723542213439941
	model : 0.06868581771850586
			 train-loss:  2.0844746010918773 	 ± 0.23493895604044024
	data : 0.11726875305175781
	model : 0.06862587928771972
			 train-loss:  2.0836717309951784 	 ± 0.23416804438154754
	data : 0.11721992492675781
	model : 0.06815247535705567
			 train-loss:  2.081704918354277 	 ± 0.23427125557245423
	data : 0.11755857467651368
	model : 0.06850719451904297
			 train-loss:  2.0837385738928487 	 ± 0.23446103413021419
	data : 0.1172861099243164
	model : 0.06939110755920411
			 train-loss:  2.0834267837926745 	 ± 0.23356980545065895
	data : 0.1164931297302246
	model : 0.06931004524230958
			 train-loss:  2.0845239411952883 	 ± 0.2329936238012062
	data : 0.11662521362304687
	model : 0.06932244300842286
			 train-loss:  2.084128721860739 	 ± 0.23213916834349108
	data : 0.11670403480529785
	model : 0.06932477951049805
			 train-loss:  2.084685732390134 	 ± 0.23133863504630517
	data : 0.11680278778076172
	model : 0.06987791061401367
			 train-loss:  2.085039457588485 	 ± 0.23049624485504008
	data : 0.1162712574005127
	model : 0.0707768440246582
			 train-loss:  2.0815493388283524 	 ± 0.233102850619442
	data : 0.11524434089660644
	model : 0.07099838256835937
			 train-loss:  2.079380889437092 	 ± 0.23357403194659185
	data : 0.11523947715759278
	model : 0.07086906433105469
			 train-loss:  2.079989941914876 	 ± 0.23281411117960063
	data : 0.11540155410766602
	model : 0.07060275077819825
			 train-loss:  2.078997962615069 	 ± 0.23224277453783929
	data : 0.11563777923583984
	model : 0.07016267776489257
			 train-loss:  2.079069377732103 	 ± 0.23139511955720998
	data : 0.11592555046081543
	model : 0.06932754516601562
			 train-loss:  2.0808762830236684 	 ± 0.23152320778135532
	data : 0.11687870025634765
	model : 0.06853446960449219
			 train-loss:  2.0795282974517604 	 ± 0.2312317401711259
	data : 0.11732206344604493
	model : 0.06869912147521973
			 train-loss:  2.0809378521783013 	 ± 0.2310029738459544
	data : 0.11723747253417968
	model : 0.06895923614501953
			 train-loss:  2.082365095192659 	 ± 0.23080099770168885
	data : 0.11683316230773926
	model : 0.06938180923461915
			 train-loss:  2.083240356243832 	 ± 0.2302215966807524
	data : 0.11661453247070312
	model : 0.06842041015625
			 train-loss:  2.08230045708743 	 ± 0.2296884512043788
	data : 0.11746439933776856
	model : 0.06884551048278809
			 train-loss:  2.080006573763159 	 ± 0.2305273733370863
	data : 0.1169501781463623
	model : 0.06794137954711914
			 train-loss:  2.0794651664536574 	 ± 0.22982292500297297
	data : 0.11791810989379883
	model : 0.06797547340393066
			 train-loss:  2.080303429740749 	 ± 0.2292568323008936
	data : 0.11809477806091309
	model : 0.06770586967468262
			 train-loss:  2.0808836821796133 	 ± 0.2285832674870741
	data : 0.11811585426330566
	model : 0.06867461204528809
			 train-loss:  2.079359663499368 	 ± 0.22855785827793448
	data : 0.11725473403930664
	model : 0.06897196769714356
			 train-loss:  2.0821871341474902 	 ± 0.23037208744620458
	data : 0.11715135574340821
	model : 0.06982626914978027
			 train-loss:  2.081693048477173 	 ± 0.22968209332310882
	data : 0.11622486114501954
	model : 0.06967792510986329
			 train-loss:  2.0816950719088116 	 ± 0.22892029457809707
	data : 0.11632962226867676
	model : 0.0692112922668457
			 train-loss:  2.0820992275288233 	 ± 0.22822006796192149
	data : 0.11694164276123047
	model : 0.06912274360656738
			 train-loss:  2.08275342143439 	 ± 0.22761597021857446
	data : 0.11713232994079589
	model : 0.06908001899719238
			 train-loss:  2.082404063893603 	 ± 0.2269169041350691
	data : 0.11708874702453613
	model : 0.06916751861572265
			 train-loss:  2.0825197988940825 	 ± 0.22618828955935483
	data : 0.11713566780090331
	model : 0.06938815116882324
			 train-loss:  2.0871521219229088 	 ± 0.23272138035193835
	data : 0.11693153381347657
	model : 0.07013983726501465
			 train-loss:  2.0887352964680668 	 ± 0.23282027929533478
	data : 0.1161008358001709
	model : 0.07010045051574706
			 train-loss:  2.088108353976962 	 ± 0.23221524707590135
	data : 0.11610383987426758
	model : 0.06983447074890137
			 train-loss:  2.088112010895831 	 ± 0.231483863218135
	data : 0.11624197959899903
	model : 0.06962676048278808
			 train-loss:  2.087649403512478 	 ± 0.23083305861827375
	data : 0.11633338928222656
	model : 0.06920647621154785
			 train-loss:  2.089309756059824 	 ± 0.23107147884924226
	data : 0.11682572364807128
	model : 0.06884188652038574
			 train-loss:  2.088636454976635 	 ± 0.23051555782677297
	data : 0.11702160835266114
	model : 0.06892046928405762
			 train-loss:  2.08795329983249 	 ± 0.22997180587585694
	data : 0.11681780815124512
	model : 0.06831588745117187
			 train-loss:  2.0903558076881783 	 ± 0.2313123273193163
	data : 0.1174767017364502
	model : 0.06829476356506348
			 train-loss:  2.089028657566417 	 ± 0.23123575710360064
	data : 0.1173919677734375
	model : 0.06841678619384765
			 train-loss:  2.090709812669869 	 ± 0.23154741131669251
	data : 0.11737599372863769
	model : 0.06834025382995605
			 train-loss:  2.0914745816213642 	 ± 0.23106330134569383
	data : 0.11761798858642578
	model : 0.06830973625183105
			 train-loss:  2.0900028411831175 	 ± 0.23115833416178838
	data : 0.11769766807556152
	model : 0.06835265159606933
			 train-loss:  2.088921850249612 	 ± 0.2308989212932746
	data : 0.11769232749938965
	model : 0.06867570877075195
			 train-loss:  2.086689800374648 	 ± 0.23204022094918267
	data : 0.11747941970825196
	model : 0.06905770301818848
			 train-loss:  2.0873037276909367 	 ± 0.23149917689846575
	data : 0.11691832542419434
	model : 0.06896400451660156
			 train-loss:  2.086321307476177 	 ± 0.23118245788186764
	data : 0.11709189414978027
	model : 0.06909780502319336
			 train-loss:  2.0865823512821526 	 ± 0.23053875388121717
	data : 0.11698503494262695
	model : 0.0698326587677002
			 train-loss:  2.0877080381601707 	 ± 0.23035166304258015
	data : 0.11628189086914062
	model : 0.0694974422454834
			 train-loss:  2.0865050274985175 	 ± 0.2302400849191107
	data : 0.11655426025390625
	model : 0.069171142578125
			 train-loss:  2.0860769924792377 	 ± 0.22965487834066267
	data : 0.11684532165527343
	model : 0.06968479156494141
			 train-loss:  2.0838213694297663 	 ± 0.23095204775591793
	data : 0.11647624969482422
	model : 0.06961817741394043
			 train-loss:  2.085031351346648 	 ± 0.23086431050621778
	data : 0.11640901565551758
	model : 0.06968388557434083
			 train-loss:  2.085286113802947 	 ± 0.23024362472948073
	data : 0.11628265380859375
	model : 0.06965951919555664
			 train-loss:  2.084502155913247 	 ± 0.22984261249504315
	data : 0.11627073287963867
	model : 0.06985583305358886
			 train-loss:  2.0831182108399617 	 ± 0.2299576393436882
	data : 0.11602540016174316
	model : 0.069775390625
			 train-loss:  2.0845738662468207 	 ± 0.23015970633343877
	data : 0.11607775688171387
	model : 0.06936016082763671
			 train-loss:  2.0855665180852503 	 ± 0.22992031856844905
	data : 0.11667943000793457
	model : 0.06904549598693847
			 train-loss:  2.085463302290958 	 ± 0.2292989351266701
	data : 0.11693181991577148
	model : 0.06924266815185547
			 train-loss:  2.085382635528977 	 ± 0.2286809864137664
	data : 0.11665544509887696
	model : 0.06906027793884277
			 train-loss:  2.0842282669518584 	 ± 0.22860525481291427
	data : 0.1168487548828125
	model : 0.06834592819213867
			 train-loss:  2.086513668458092 	 ± 0.2301138512065196
	data : 0.11734709739685059
	model : 0.06795625686645508
			 train-loss:  2.08765835204023 	 ± 0.23003423460742936
	data : 0.11757621765136719
	model : 0.06753244400024414
			 train-loss:  2.091270548956735 	 ± 0.23471001107103154
	data : 0.11824188232421876
	model : 0.06673774719238282
			 train-loss:  2.089002712149369 	 ± 0.2361586143590769
	data : 0.11884818077087403
	model : 0.0659968376159668
			 train-loss:  2.088975454500208 	 ± 0.23553988644741905
	data : 0.11963901519775391
	model : 0.06585602760314942
			 train-loss:  2.08983501419425 	 ± 0.23522585693733006
	data : 0.11985154151916504
	model : 0.0658987045288086
			 train-loss:  2.0896262455480703 	 ± 0.2346335052622746
	data : 0.1199345588684082
	model : 0.06667990684509277
			 train-loss:  2.0897527748776463 	 ± 0.2340345998287809
	data : 0.11914401054382324
	model : 0.06751441955566406
			 train-loss:  2.0887078383030038 	 ± 0.2338870197459117
	data : 0.11869139671325683
	model : 0.06750721931457519
			 train-loss:  2.089196869305202 	 ± 0.23338953462046091
	data : 0.11863870620727539
	model : 0.06837902069091797
			 train-loss:  2.0921321072554226 	 ± 0.23639550727599268
	data : 0.11780471801757812
	model : 0.06827812194824219
			 train-loss:  2.0919100588018242 	 ± 0.23581838881250733
	data : 0.11786799430847168
	model : 0.06818771362304688
			 train-loss:  2.0917580559026057 	 ± 0.23523485809312994
	data : 0.11785755157470704
	model : 0.06824016571044922
			 train-loss:  2.0924850142002107 	 ± 0.23487002038299684
	data : 0.11804671287536621
	model : 0.0692227840423584
			 train-loss:  2.0919376202483675 	 ± 0.23441289876415858
	data : 0.11697778701782227
	model : 0.06936931610107422
			 train-loss:  2.09332282944481 	 ± 0.23465519394903375
	data : 0.11678214073181152
	model : 0.06992011070251465
			 train-loss:  2.0960841460768225 	 ± 0.2373437108784878
	data : 0.11627492904663086
	model : 0.06979913711547851
			 train-loss:  2.097144133904401 	 ± 0.23724246082509082
	data : 0.11630463600158691
	model : 0.06975269317626953
			 train-loss:  2.0971693666969857 	 ± 0.2366633877016659
	data : 0.11614456176757812
	model : 0.06968560218811035
			 train-loss:  2.0954269141826813 	 ± 0.23740277351304387
	data : 0.116448974609375
	model : 0.06873784065246583
			 train-loss:  2.097231703104028 	 ± 0.23824105892425598
	data : 0.11748256683349609
	model : 0.06909365653991699
			 train-loss:  2.096957140817092 	 ± 0.23770050038199858
	data : 0.11710114479064941
	model : 0.06878523826599121
			 train-loss:  2.0953582970149207 	 ± 0.23824965132390324
	data : 0.11743669509887696
	model : 0.068751859664917
			 train-loss:  2.0930316504978 	 ± 0.24004993898931015
	data : 0.11754927635192872
	model : 0.06865320205688477
			 train-loss:  2.0917310206246036 	 ± 0.24022097702689987
	data : 0.11761012077331542
	model : 0.06902828216552734
			 train-loss:  2.092856226102361 	 ± 0.24021045682764663
	data : 0.11722745895385742
	model : 0.06805315017700195
			 train-loss:  2.0934984415349827 	 ± 0.23982828018731697
	data : 0.11801228523254395
	model : 0.0682551383972168
			 train-loss:  2.093376617565333 	 ± 0.23927388339974417
	data : 0.11782193183898926
	model : 0.06736969947814941
			 train-loss:  2.0935349575308866 	 ± 0.2387280215145187
	data : 0.11843109130859375
	model : 0.06746993064880372
			 train-loss:  2.093343844016393 	 ± 0.23819125395095625
	data : 0.11845154762268066
	model : 0.06803059577941895
			 train-loss:  2.094472785149851 	 ± 0.23822030822999532
	data : 0.11778092384338379
	model : 0.06889309883117675
			 train-loss:  2.094555070640844 	 ± 0.23767639431361162
	data : 0.11711711883544922
	model : 0.06832504272460938
			 train-loss:  2.09225943426019 	 ± 0.23954325540399476
	data : 0.11762485504150391
	model : 0.06911888122558593
			 train-loss:  2.091801683469252 	 ± 0.2390942009970186
	data : 0.11704411506652831
	model : 0.06873784065246583
			 train-loss:  2.0921847334814285 	 ± 0.2386202990287094
	data : 0.117340087890625
	model : 0.06781191825866699
			 train-loss:  2.0924391703562693 	 ± 0.23811230404351202
	data : 0.11824154853820801
	model : 0.06767330169677735
			 train-loss:  2.0932341562792858 	 ± 0.23787291812360475
	data : 0.11814517974853515
	model : 0.06768460273742676
			 train-loss:  2.092768852732011 	 ± 0.23744304854374218
	data : 0.1180887222290039
	model : 0.06690487861633301
			 train-loss:  2.09173973136478 	 ± 0.23741496037604776
	data : 0.11854991912841797
	model : 0.06652917861938476
			 train-loss:  2.0910606753509655 	 ± 0.2371080097251465
	data : 0.11867575645446778
	model : 0.06664500236511231
			 train-loss:  2.0887584371188663 	 ± 0.23910334618810006
	data : 0.11860246658325195
	model : 0.06637463569641114
			 train-loss:  2.0910268959246183 	 ± 0.2410140727438786
	data : 0.11916074752807618
	model : 0.06656112670898437
			 train-loss:  2.0890306031339554 	 ± 0.24236903056493356
	data : 0.11904821395874024
	model : 0.06675558090209961
			 train-loss:  2.0888277789820795 	 ± 0.24186104326252597
	data : 0.11889228820800782
	model : 0.06726660728454589
			 train-loss:  2.0869011564172193 	 ± 0.24309928555856283
	data : 0.11842737197875977
	model : 0.06730470657348633
			 train-loss:  2.086016140621284 	 ± 0.2429474521214093
	data : 0.11829605102539062
	model : 0.06704726219177246
			 train-loss:  2.0865182666819493 	 ± 0.2425461588995676
	data : 0.11833481788635254
	model : 0.06719498634338379
			 train-loss:  2.086023439708938 	 ± 0.24214517479347375
	data : 0.11826434135437011
	model : 0.06767616271972657
			 train-loss:  2.087051693936612 	 ± 0.24214084301954572
	data : 0.11781997680664062
	model : 0.06695356369018554
			 train-loss:  2.087155524956978 	 ± 0.24163253069739288
	data : 0.11831831932067871
	model : 0.06715879440307618
			 train-loss:  2.086281041052774 	 ± 0.241496167040322
	data : 0.11792197227478027
	model : 0.06746835708618164
			 train-loss:  2.085417530115913 	 ± 0.2413546650116696
	data : 0.11766481399536133
	model : 0.06737232208251953
			 train-loss:  2.0850334860789728 	 ± 0.24092207116006875
	data : 0.11754794120788574
	model : 0.06668806076049805
			 train-loss:  2.0846673965454103 	 ± 0.2404862321148702
	data : 0.11809020042419434
	model : 0.06713614463806153
			 train-loss:  2.0871045955483845 	 ± 0.24293875695850198
	data : 0.11776852607727051
	model : 0.06735239028930665
			 train-loss:  2.087243194422446 	 ± 0.24244584550405784
	data : 0.11792831420898438
	model : 0.06703743934631348
			 train-loss:  2.0857359072799055 	 ± 0.2430800246085895
	data : 0.11815156936645507
	model : 0.06744484901428223
			 train-loss:  2.0873632299118356 	 ± 0.24390416628316858
	data : 0.11782989501953126
	model : 0.06747674942016602
			 train-loss:  2.088786429288436 	 ± 0.24441900546782785
	data : 0.11793465614318847
	model : 0.06784644126892089
			 train-loss:  2.0884306871794105 	 ± 0.2439852606449669
	data : 0.11773943901062012
	model : 0.06806430816650391
			 train-loss:  2.0873264894794357 	 ± 0.24410599383147033
	data : 0.11743769645690919
	model : 0.0682830810546875
			 train-loss:  2.088472296634028 	 ± 0.24427800347408443
	data : 0.1175729751586914
	model : 0.06829714775085449
			 train-loss:  2.086618867743925 	 ± 0.24552805683365186
	data : 0.11752595901489257
	model : 0.06921963691711426
			 train-loss:  2.0883863220214844 	 ± 0.24661861313966446
	data : 0.11651020050048828
	model : 0.06888084411621094
			 train-loss:  2.08852419720228 	 ± 0.2461365048743072
	data : 0.11672205924987793
	model : 0.06865363121032715
			 train-loss:  2.0871797662878793 	 ± 0.24656936287081607
	data : 0.11686553955078124
	model : 0.06890864372253418
			 train-loss:  2.0864723133946597 	 ± 0.24633771921497946
	data : 0.11643743515014648
	model : 0.0687896728515625
			 train-loss:  2.0868446507791836 	 ± 0.24592364671394495
	data : 0.11679983139038086
	model : 0.06826128959655761
			 train-loss:  2.0886445849549538 	 ± 0.24711165385614156
	data : 0.11747498512268066
	model : 0.06833071708679199
			 train-loss:  2.0892206924036145 	 ± 0.24680006484314804
	data : 0.11632599830627441
	model : 0.05959806442260742
#epoch  42    val-loss:  2.505532095306798  train-loss:  2.0892206924036145  lr:  0.0003125
			 train-loss:  2.1641745567321777 	 ± 0.0
	data : 5.770832300186157
	model : 0.0749201774597168
			 train-loss:  2.275268077850342 	 ± 0.11109352111816406
	data : 2.949898362159729
	model : 0.07177996635437012
			 train-loss:  2.275965134302775 	 ± 0.0907128366422511
	data : 2.0070311228434243
	model : 0.06980403264363606
			 train-loss:  2.2166332602500916 	 ± 0.12935388623683983
	data : 1.5350517630577087
	model : 0.06885409355163574
			 train-loss:  2.2067458629608154 	 ± 0.11737540120605584
	data : 1.2518049240112306
	model : 0.06898393630981445
			 train-loss:  2.170861840248108 	 ± 0.1338623774770169
	data : 0.12098770141601563
	model : 0.0679734230041504
			 train-loss:  2.2005409513201033 	 ± 0.14368144390172768
	data : 0.11854434013366699
	model : 0.06812753677368164
			 train-loss:  2.238273411989212 	 ± 0.1674215720075987
	data : 0.11752796173095703
	model : 0.06857709884643555
			 train-loss:  2.274398194419013 	 ± 0.188030687622821
	data : 0.11713194847106934
	model : 0.06855940818786621
			 train-loss:  2.235556924343109 	 ± 0.2130675571012716
	data : 0.11725072860717774
	model : 0.06824378967285157
			 train-loss:  2.191324407404119 	 ± 0.2466492817221207
	data : 0.11755404472351075
	model : 0.06830286979675293
			 train-loss:  2.1520495216051736 	 ± 0.2696922240120961
	data : 0.117405366897583
	model : 0.06847176551818848
			 train-loss:  2.157530271089994 	 ± 0.2598065511458452
	data : 0.11736259460449219
	model : 0.06870436668395996
			 train-loss:  2.1284493633678983 	 ± 0.2714261311684682
	data : 0.11708908081054688
	model : 0.06940441131591797
			 train-loss:  2.148530109723409 	 ± 0.2727745912062402
	data : 0.11646194458007812
	model : 0.0691798210144043
			 train-loss:  2.1635999903082848 	 ± 0.27048497836272617
	data : 0.11649932861328124
	model : 0.06908621788024902
			 train-loss:  2.1514842369977165 	 ± 0.26684664668626823
	data : 0.116552734375
	model : 0.06875472068786621
			 train-loss:  2.1654870775010853 	 ± 0.26567749855718065
	data : 0.11695528030395508
	model : 0.06886444091796876
			 train-loss:  2.147618582374171 	 ± 0.2694747742029805
	data : 0.1168973445892334
	model : 0.06888689994812011
			 train-loss:  2.140937054157257 	 ± 0.26426130023542027
	data : 0.11672487258911132
	model : 0.0695650577545166
			 train-loss:  2.1381911550249373 	 ± 0.25818482491103206
	data : 0.11624507904052735
	model : 0.06870412826538086
			 train-loss:  2.135549794543873 	 ± 0.25253899382609796
	data : 0.11700100898742676
	model : 0.0691382884979248
			 train-loss:  2.1265712561814683 	 ± 0.2505525645907196
	data : 0.11657485961914063
	model : 0.06846323013305664
			 train-loss:  2.122306083639463 	 ± 0.24612863608026134
	data : 0.11742467880249023
	model : 0.0688359260559082
			 train-loss:  2.128392767906189 	 ± 0.24299234285772878
	data : 0.1174896240234375
	model : 0.06790461540222167
			 train-loss:  2.130707616989429 	 ± 0.2385545421643264
	data : 0.11829352378845215
	model : 0.06878247261047363
			 train-loss:  2.1192017484594277 	 ± 0.24133496674785276
	data : 0.11750712394714355
	model : 0.06853160858154297
			 train-loss:  2.1263042518070767 	 ± 0.23984266609334604
	data : 0.1176530361175537
	model : 0.06917381286621094
			 train-loss:  2.1274769388396164 	 ± 0.23575285010490185
	data : 0.11689252853393554
	model : 0.0688669204711914
			 train-loss:  2.128721022605896 	 ± 0.23188713643266112
	data : 0.1169156551361084
	model : 0.06962828636169434
			 train-loss:  2.11745253685982 	 ± 0.2363185096655564
	data : 0.1161125659942627
	model : 0.06873674392700195
			 train-loss:  2.1131517440080643 	 ± 0.23382608540640865
	data : 0.11700677871704102
	model : 0.06808867454528808
			 train-loss:  2.103666493386933 	 ± 0.23642518972576723
	data : 0.11759738922119141
	model : 0.0673410415649414
			 train-loss:  2.0968377975856556 	 ± 0.23620260495632284
	data : 0.11843934059143066
	model : 0.0671006202697754
			 train-loss:  2.086301704815456 	 ± 0.24077362269646435
	data : 0.11868929862976074
	model : 0.06710715293884277
			 train-loss:  2.0851233899593353 	 ± 0.23750831711504056
	data : 0.11877923011779785
	model : 0.06706447601318359
			 train-loss:  2.0883634380392126 	 ± 0.23508195484998137
	data : 0.11858034133911133
	model : 0.06681032180786133
			 train-loss:  2.0916592478752136 	 ± 0.23283283618325407
	data : 0.11878824234008789
	model : 0.06797080039978028
			 train-loss:  2.089077646915729 	 ± 0.23037872700064155
	data : 0.11774945259094238
	model : 0.06755867004394531
			 train-loss:  2.084779217839241 	 ± 0.22905912362657285
	data : 0.11808080673217773
	model : 0.0681879997253418
			 train-loss:  2.089152943797228 	 ± 0.22793321817339363
	data : 0.11764388084411621
	model : 0.0694697380065918
			 train-loss:  2.0796613210723516 	 ± 0.23326013288776618
	data : 0.11664538383483887
	model : 0.07035961151123046
			 train-loss:  2.082062297089155 	 ± 0.23105638019289373
	data : 0.1158294677734375
	model : 0.06911282539367676
			 train-loss:  2.085446598854932 	 ± 0.22949119633783982
	data : 0.11683034896850586
	model : 0.06973061561584473
			 train-loss:  2.0907108068466185 	 ± 0.22959784834904956
	data : 0.11618986129760742
	model : 0.06869878768920898
			 train-loss:  2.093619494334511 	 ± 0.22792523071797277
	data : 0.11690106391906738
	model : 0.06747016906738282
			 train-loss:  2.0959972396810005 	 ± 0.2260634037205708
	data : 0.11801114082336425
	model : 0.06662559509277344
			 train-loss:  2.0948370223244033 	 ± 0.22383755032551528
	data : 0.11866827011108398
	model : 0.06749587059020996
			 train-loss:  2.088228325454556 	 ± 0.22622362303386034
	data : 0.11790337562561035
	model : 0.0676234245300293
			 train-loss:  2.0937648129463198 	 ± 0.2272786110916973
	data : 0.11805391311645508
	model : 0.06862115859985352
			 train-loss:  2.093336589196149 	 ± 0.22505972892258527
	data : 0.11717605590820312
	model : 0.06954307556152343
			 train-loss:  2.093767897440837 	 ± 0.22290646985627172
	data : 0.1163945198059082
	model : 0.07022948265075683
			 train-loss:  2.0999777069631613 	 ± 0.22528872224021826
	data : 0.11589784622192383
	model : 0.06984643936157227
			 train-loss:  2.09935235314899 	 ± 0.22323939477178256
	data : 0.11623239517211914
	model : 0.06972217559814453
			 train-loss:  2.1017533714121037 	 ± 0.22190318928652805
	data : 0.11626477241516113
	model : 0.06831574440002441
			 train-loss:  2.100505707519395 	 ± 0.22010756001477919
	data : 0.11763529777526856
	model : 0.06830019950866699
			 train-loss:  2.1124898320750187 	 ± 0.23588145792805396
	data : 0.11765117645263672
	model : 0.06856880187988282
			 train-loss:  2.1079984451162406 	 ± 0.23628496887337191
	data : 0.11735210418701172
	model : 0.06909346580505371
			 train-loss:  2.102152355646683 	 ± 0.23846709959468457
	data : 0.11696181297302247
	model : 0.0690701961517334
			 train-loss:  2.1013917446136476 	 ± 0.23654368486339641
	data : 0.11709794998168946
	model : 0.0699552059173584
			 train-loss:  2.1005672157787885 	 ± 0.2346837122971356
	data : 0.11620750427246093
	model : 0.06922779083251954
			 train-loss:  2.1040267482880624 	 ± 0.23434629363100398
	data : 0.11667571067810059
	model : 0.06920437812805176
			 train-loss:  2.09761681443169 	 ± 0.237894677825424
	data : 0.11667556762695312
	model : 0.06892790794372558
			 train-loss:  2.105413807556033 	 ± 0.2440073027805063
	data : 0.1168513298034668
	model : 0.06811904907226562
			 train-loss:  2.102871581224295 	 ± 0.24297571290773876
	data : 0.11740431785583497
	model : 0.06820740699768066
			 train-loss:  2.096901176553784 	 ± 0.24588547466787908
	data : 0.11754260063171387
	model : 0.06898703575134277
			 train-loss:  2.0939099521779303 	 ± 0.2452505117301137
	data : 0.11697273254394532
	model : 0.06908488273620605
			 train-loss:  2.0945771319024704 	 ± 0.24350176685866573
	data : 0.11696724891662598
	model : 0.06921830177307128
			 train-loss:  2.0951325565144634 	 ± 0.24177420826831045
	data : 0.11693859100341797
	model : 0.06983284950256348
			 train-loss:  2.0891945276941573 	 ± 0.24505642722698806
	data : 0.1165247917175293
	model : 0.06957507133483887
			 train-loss:  2.0926235880650266 	 ± 0.24501006585394158
	data : 0.11672992706298828
	model : 0.06948304176330566
			 train-loss:  2.100677369369401 	 ± 0.25258955685710516
	data : 0.11664295196533203
	model : 0.06932849884033203
			 train-loss:  2.0991891328602623 	 ± 0.251171177382836
	data : 0.11660218238830566
	model : 0.06901640892028808
			 train-loss:  2.100285038754747 	 ± 0.2496439616072274
	data : 0.11686487197875976
	model : 0.0693516731262207
			 train-loss:  2.1039533281326293 	 ± 0.2499738302458008
	data : 0.11635966300964355
	model : 0.06874332427978516
			 train-loss:  2.103923852506437 	 ± 0.2483239511365517
	data : 0.11673173904418946
	model : 0.06856889724731445
			 train-loss:  2.1032483407429288 	 ± 0.24677646438642392
	data : 0.11707043647766113
	model : 0.06850833892822265
			 train-loss:  2.098610731271597 	 ± 0.2485436445721221
	data : 0.11716094017028808
	model : 0.06869487762451172
			 train-loss:  2.0956020943726164 	 ± 0.24839090662820348
	data : 0.11679110527038575
	model : 0.06822404861450196
			 train-loss:  2.0982041373848914 	 ± 0.24791469614277958
	data : 0.11734418869018555
	model : 0.06891922950744629
			 train-loss:  2.094323718989337 	 ± 0.24881222224843258
	data : 0.11663174629211426
	model : 0.06917395591735839
			 train-loss:  2.0968709646201713 	 ± 0.24835079513674424
	data : 0.11651239395141602
	model : 0.06865382194519043
			 train-loss:  2.0930320168116006 	 ± 0.2492859487797615
	data : 0.11694302558898925
	model : 0.06882777214050292
			 train-loss:  2.0971339302403584 	 ± 0.25059970761851047
	data : 0.11694183349609374
	model : 0.06899013519287109
			 train-loss:  2.0939358767341165 	 ± 0.2508395927699873
	data : 0.11669411659240722
	model : 0.06849107742309571
			 train-loss:  2.094723892766376 	 ± 0.24948276484089654
	data : 0.11717371940612793
	model : 0.06847710609436035
			 train-loss:  2.1005738932510902 	 ± 0.25390818060187426
	data : 0.11711564064025878
	model : 0.0681765079498291
			 train-loss:  2.1015575121749532 	 ± 0.252628048331453
	data : 0.11755623817443847
	model : 0.06814403533935547
			 train-loss:  2.1020380004068437 	 ± 0.2512452151990384
	data : 0.11739234924316407
	model : 0.06850962638854981
			 train-loss:  2.1024375173780654 	 ± 0.24987393672492453
	data : 0.11702332496643067
	model : 0.06921501159667968
			 train-loss:  2.1048362333695967 	 ± 0.2495369892521782
	data : 0.11647706031799317
	model : 0.0690845012664795
			 train-loss:  2.1016229857569155 	 ± 0.25006288527401493
	data : 0.1164848804473877
	model : 0.06990137100219726
			 train-loss:  2.103471202235068 	 ± 0.2493458013025882
	data : 0.11588950157165527
	model : 0.0703643798828125
			 train-loss:  2.102849100498443 	 ± 0.24808849675197486
	data : 0.1157567024230957
	model : 0.07066450119018555
			 train-loss:  2.1003521919250487 	 ± 0.2479638632816201
	data : 0.11559038162231446
	model : 0.06970658302307128
			 train-loss:  2.1002316176891327 	 ± 0.2466718035032778
	data : 0.11656246185302735
	model : 0.0692624568939209
			 train-loss:  2.100211512182177 	 ± 0.24539708435543958
	data : 0.11716079711914062
	model : 0.06967253684997558
			 train-loss:  2.100648449391735 	 ± 0.244179771171916
	data : 0.11682510375976562
	model : 0.06907663345336915
			 train-loss:  2.100798898273044 	 ± 0.24294797520836575
	data : 0.11722164154052735
	model : 0.06806278228759766
			 train-loss:  2.1001866483688354 	 ± 0.24180693045452897
	data : 0.11825432777404785
	model : 0.06894550323486329
			 train-loss:  2.0988338701795826 	 ± 0.24098687879239736
	data : 0.11738018989562989
	model : 0.06967344284057617
			 train-loss:  2.0975306665196136 	 ± 0.2401600476799566
	data : 0.11672983169555665
	model : 0.06942577362060547
			 train-loss:  2.101884749329206 	 ± 0.24300328986294514
	data : 0.11684722900390625
	model : 0.06939191818237304
			 train-loss:  2.098689397940269 	 ± 0.24399684941725888
	data : 0.11677608489990235
	model : 0.0698178768157959
			 train-loss:  2.103059434890747 	 ± 0.24688778590890528
	data : 0.11625084877014161
	model : 0.0696495532989502
			 train-loss:  2.106214707752444 	 ± 0.24783845239546579
	data : 0.11626105308532715
	model : 0.06952638626098633
			 train-loss:  2.1040461965810473 	 ± 0.2476858939745947
	data : 0.11624813079833984
	model : 0.06954488754272461
			 train-loss:  2.1046455513547966 	 ± 0.24661447532022743
	data : 0.1161959171295166
	model : 0.06976218223571777
			 train-loss:  2.103654177910691 	 ± 0.24569671226712514
	data : 0.11592020988464355
	model : 0.07008094787597656
			 train-loss:  2.1019936312328684 	 ± 0.24519103351422933
	data : 0.11553564071655273
	model : 0.07014007568359375
			 train-loss:  2.1016819681133234 	 ± 0.2441059570128259
	data : 0.11564040184020996
	model : 0.06989598274230957
			 train-loss:  2.1007948017546108 	 ± 0.24319343981885286
	data : 0.11588201522827149
	model : 0.06895675659179687
			 train-loss:  2.0967107863552803 	 ± 0.2459425292925864
	data : 0.1168245792388916
	model : 0.06894087791442871
			 train-loss:  2.0954103396649946 	 ± 0.24525137078562081
	data : 0.11718487739562988
	model : 0.06881332397460938
			 train-loss:  2.093499201277028 	 ± 0.24503384671475442
	data : 0.11743297576904296
	model : 0.06809229850769043
			 train-loss:  2.094441657436305 	 ± 0.24418462660783719
	data : 0.11815524101257324
	model : 0.06844849586486816
			 train-loss:  2.09366040250175 	 ± 0.24328441891851207
	data : 0.1177255630493164
	model : 0.06884918212890626
			 train-loss:  2.096786875846022 	 ± 0.24460044335696082
	data : 0.11728582382202149
	model : 0.06897187232971191
			 train-loss:  2.101809386445695 	 ± 0.2496061524057529
	data : 0.1171414852142334
	model : 0.06896815299987794
			 train-loss:  2.10391981502374 	 ± 0.2496278287163939
	data : 0.11716904640197753
	model : 0.06923494338989258
			 train-loss:  2.103156778438032 	 ± 0.2487346531485314
	data : 0.11680426597595214
	model : 0.0690572738647461
			 train-loss:  2.1088880075783027 	 ± 0.2556096453605872
	data : 0.11703944206237793
	model : 0.06916232109069824
			 train-loss:  2.108709080432489 	 ± 0.2545761325535538
	data : 0.11685981750488281
	model : 0.06905226707458496
			 train-loss:  2.1087955707503903 	 ± 0.2535493523798832
	data : 0.11678862571716309
	model : 0.06907730102539063
			 train-loss:  2.1081166429519653 	 ± 0.2526462602652153
	data : 0.11655068397521973
	model : 0.06951427459716797
			 train-loss:  2.107671288270799 	 ± 0.25169095547762854
	data : 0.11616148948669433
	model : 0.06929707527160645
			 train-loss:  2.1033872375338096 	 ± 0.2552685266457681
	data : 0.11626477241516113
	model : 0.06980390548706054
			 train-loss:  2.101270252838731 	 ± 0.2553861951682444
	data : 0.11599712371826172
	model : 0.06982622146606446
			 train-loss:  2.099952219068542 	 ± 0.25483106994098176
	data : 0.11613836288452148
	model : 0.0698702335357666
			 train-loss:  2.0992101540932286 	 ± 0.2539889359811365
	data : 0.1160822868347168
	model : 0.0699120044708252
			 train-loss:  2.097124717617763 	 ± 0.2541324651498476
	data : 0.11611552238464355
	model : 0.07011518478393555
			 train-loss:  2.0945609401572836 	 ± 0.25486290320854044
	data : 0.11611080169677734
	model : 0.07039084434509277
			 train-loss:  2.0937846119242502 	 ± 0.2540595790655893
	data : 0.11598663330078125
	model : 0.07030096054077148
			 train-loss:  2.0961635824459703 	 ± 0.25459240902640656
	data : 0.1159062385559082
	model : 0.07020821571350097
			 train-loss:  2.093389051931876 	 ± 0.2556730358242677
	data : 0.11614785194396973
	model : 0.0694654941558838
			 train-loss:  2.0911795794963837 	 ± 0.25602165488952944
	data : 0.11693100929260254
	model : 0.06887054443359375
			 train-loss:  2.089856935243537 	 ± 0.25555147939248685
	data : 0.11752214431762695
	model : 0.06940312385559082
			 train-loss:  2.092078847297724 	 ± 0.25594858283031763
	data : 0.11680269241333008
	model : 0.06923580169677734
			 train-loss:  2.0902561506779076 	 ± 0.2559235260419581
	data : 0.11697220802307129
	model : 0.069354248046875
			 train-loss:  2.0892090448311396 	 ± 0.2553065224223682
	data : 0.11699175834655762
	model : 0.06940274238586426
			 train-loss:  2.088599552499487 	 ± 0.2545017645135044
	data : 0.11674408912658692
	model : 0.06984667778015137
			 train-loss:  2.088231752456074 	 ± 0.2536416514166596
	data : 0.11621160507202148
	model : 0.06893110275268555
			 train-loss:  2.089121762689177 	 ± 0.2529756498109912
	data : 0.11693229675292968
	model : 0.06908326148986817
			 train-loss:  2.088728648920854 	 ± 0.2521395585955776
	data : 0.11668877601623535
	model : 0.06907215118408203
			 train-loss:  2.0903087065137664 	 ± 0.2519829775498525
	data : 0.11648554801940918
	model : 0.06911001205444336
			 train-loss:  2.0892954387076914 	 ± 0.2514147848079265
	data : 0.11646838188171386
	model : 0.06892499923706055
			 train-loss:  2.086798607897596 	 ± 0.2523679577523419
	data : 0.11660923957824706
	model : 0.06882901191711426
			 train-loss:  2.086042960753312 	 ± 0.25168072719628487
	data : 0.116817045211792
	model : 0.06890301704406739
			 train-loss:  2.0841608935554556 	 ± 0.25187756660465516
	data : 0.11697654724121094
	model : 0.06799907684326172
			 train-loss:  2.087872320016225 	 ± 0.2550917248846763
	data : 0.11786065101623536
	model : 0.06869339942932129
			 train-loss:  2.085056380720328 	 ± 0.25657411449383505
	data : 0.11725544929504395
	model : 0.06900625228881836
			 train-loss:  2.0823076304636503 	 ± 0.25794976667900366
	data : 0.1170544147491455
	model : 0.06907730102539063
			 train-loss:  2.0826068734811023 	 ± 0.2571318800817176
	data : 0.11681880950927734
	model : 0.06858038902282715
			 train-loss:  2.0831060533399706 	 ± 0.2563700420898851
	data : 0.11744036674499511
	model : 0.06902751922607422
			 train-loss:  2.0833941644237886 	 ± 0.2555667145647402
	data : 0.11690001487731934
	model : 0.06902179718017579
			 train-loss:  2.0846217549764194 	 ± 0.25520432121822356
	data : 0.11684727668762207
	model : 0.06844167709350586
			 train-loss:  2.085079859776102 	 ± 0.2544546084679708
	data : 0.11736230850219727
	model : 0.06842384338378907
			 train-loss:  2.087808756888667 	 ± 0.2559424071346768
	data : 0.11741580963134765
	model : 0.06888308525085449
			 train-loss:  2.085922676812178 	 ± 0.2562353960664922
	data : 0.11681056022644043
	model : 0.06927170753479003
			 train-loss:  2.0862070299685 	 ± 0.2554585696384109
	data : 0.11671347618103027
	model : 0.06928019523620606
			 train-loss:  2.085617152800471 	 ± 0.2547732672551029
	data : 0.11676645278930664
	model : 0.06898212432861328
			 train-loss:  2.0882200885702065 	 ± 0.25612411347612407
	data : 0.11698074340820312
	model : 0.06907176971435547
			 train-loss:  2.091628532468176 	 ± 0.25899641610429336
	data : 0.11683464050292969
	model : 0.06816587448120118
			 train-loss:  2.0935345001337007 	 ± 0.25934968009195825
	data : 0.11763544082641601
	model : 0.0679995059967041
			 train-loss:  2.0910077434597594 	 ± 0.260579478101148
	data : 0.11774816513061523
	model : 0.06785378456115723
			 train-loss:  2.089680716215846 	 ± 0.260352037541176
	data : 0.117927885055542
	model : 0.06855907440185546
			 train-loss:  2.0894456380855537 	 ± 0.25958903977734055
	data : 0.11741180419921875
	model : 0.0683100700378418
			 train-loss:  2.0907169239861623 	 ± 0.25933619042480643
	data : 0.11785516738891602
	model : 0.0692093849182129
			 train-loss:  2.0919821191821577 	 ± 0.2590872832081942
	data : 0.1169424057006836
	model : 0.06855716705322265
			 train-loss:  2.0901761693112992 	 ± 0.25938879329081604
	data : 0.11760921478271484
	model : 0.06836934089660644
			 train-loss:  2.088265355567486 	 ± 0.2598264545054746
	data : 0.11758337020874024
	model : 0.06871905326843261
			 train-loss:  2.088221333054609 	 ± 0.2590706835928226
	data : 0.11710305213928222
	model : 0.06891827583312989
			 train-loss:  2.0866775505804602 	 ± 0.25911305919503835
	data : 0.11689996719360352
	model : 0.06847739219665527
			 train-loss:  2.086914763368409 	 ± 0.25838624672917715
	data : 0.11738181114196777
	model : 0.06837215423583984
			 train-loss:  2.0859696401868546 	 ± 0.25794839390480256
	data : 0.11761374473571777
	model : 0.06786074638366699
			 train-loss:  2.0860023322430523 	 ± 0.25721490565675487
	data : 0.1181302547454834
	model : 0.0677330493927002
			 train-loss:  2.0850027967981024 	 ± 0.25682983019362265
	data : 0.11827011108398437
	model : 0.06773209571838379
			 train-loss:  2.082203573725197 	 ± 0.25880089088480474
	data : 0.11829962730407714
	model : 0.06732234954833985
			 train-loss:  2.082123252266612 	 ± 0.25807919574762295
	data : 0.1186450481414795
	model : 0.0681870460510254
			 train-loss:  2.0834490835666655 	 ± 0.25797188905182256
	data : 0.11780691146850586
	model : 0.06907529830932617
			 train-loss:  2.084576033096946 	 ± 0.25770219539985495
	data : 0.11700677871704102
	model : 0.06904668807983398
			 train-loss:  2.0841319122157254 	 ± 0.2570626968963915
	data : 0.11696887016296387
	model : 0.0680783748626709
			 train-loss:  2.0861896680352467 	 ± 0.25785806921200516
	data : 0.1177785873413086
	model : 0.06870880126953124
			 train-loss:  2.086344174068907 	 ± 0.25716490720729507
	data : 0.11714024543762207
	model : 0.06886658668518067
			 train-loss:  2.086731415181547 	 ± 0.256522711276592
	data : 0.11685781478881836
	model : 0.0686525821685791
			 train-loss:  2.0905628851664964 	 ± 0.2610860911653312
	data : 0.11702084541320801
	model : 0.06769051551818847
			 train-loss:  2.0906000538943283 	 ± 0.26038755765379445
	data : 0.11784439086914063
	model : 0.0688359260559082
			 train-loss:  2.091142379857124 	 ± 0.259799986353038
	data : 0.11691470146179199
	model : 0.06884908676147461
			 train-loss:  2.08916559042754 	 ± 0.2605255418844597
	data : 0.1169848918914795
	model : 0.06779608726501465
			 train-loss:  2.087704863046345 	 ± 0.26061389573241533
	data : 0.11799211502075195
	model : 0.06808247566223144
			 train-loss:  2.0864848322893312 	 ± 0.26047420773291424
	data : 0.11791648864746093
	model : 0.06913490295410156
			 train-loss:  2.086324102555712 	 ± 0.2598045003073678
	data : 0.11704020500183106
	model : 0.06895475387573242
			 train-loss:  2.0875057352639232 	 ± 0.25964731266481644
	data : 0.11712856292724609
	model : 0.0693580150604248
			 train-loss:  2.0874020875114754 	 ± 0.2589812569346558
	data : 0.11674737930297852
	model : 0.07043590545654296
			 train-loss:  2.08879970342685 	 ± 0.25904880268130703
	data : 0.11573300361633301
	model : 0.0698592185974121
			 train-loss:  2.0893383117354647 	 ± 0.25849656180967573
	data : 0.11617832183837891
	model : 0.06895546913146973
			 train-loss:  2.088779986207255 	 ± 0.25795809896446564
	data : 0.11698760986328124
	model : 0.06842937469482421
			 train-loss:  2.0884826947944335 	 ± 0.25733969666809137
	data : 0.11738910675048828
	model : 0.06825461387634277
			 train-loss:  2.0878218837718867 	 ± 0.256860658471394
	data : 0.11755046844482422
	model : 0.06815838813781738
			 train-loss:  2.0877570867538453 	 ± 0.2562193326294138
	data : 0.11758103370666503
	model : 0.06851382255554199
			 train-loss:  2.0885059050659636 	 ± 0.25580047595216227
	data : 0.11727018356323242
	model : 0.06941132545471192
			 train-loss:  2.087793762140935 	 ± 0.25536618812542217
	data : 0.11642074584960938
	model : 0.06917095184326172
			 train-loss:  2.0893984557372596 	 ± 0.2557553666900805
	data : 0.11650209426879883
	model : 0.06892514228820801
			 train-loss:  2.087687460230846 	 ± 0.2562897774367125
	data : 0.11668968200683594
	model : 0.06876754760742188
			 train-loss:  2.0879843182680085 	 ± 0.25569907216271254
	data : 0.11682620048522949
	model : 0.06833786964416504
			 train-loss:  2.0868756319712665 	 ± 0.25557114482572285
	data : 0.1171792984008789
	model : 0.06815032958984375
			 train-loss:  2.0855303012230544 	 ± 0.2556832285110032
	data : 0.11773157119750977
	model : 0.06887402534484863
			 train-loss:  2.0841994342895656 	 ± 0.2557855654410668
	data : 0.1171154499053955
	model : 0.0684129238128662
			 train-loss:  2.0871582567406612 	 ± 0.2587163974416259
	data : 0.11763396263122558
	model : 0.06900749206542969
			 train-loss:  2.0876507089251564 	 ± 0.2581978399503069
	data : 0.11716480255126953
	model : 0.06972041130065917
			 train-loss:  2.085813746633123 	 ± 0.2589571426996275
	data : 0.11658902168273926
	model : 0.06898508071899415
			 train-loss:  2.0863594879519263 	 ± 0.25846726976428563
	data : 0.1171194076538086
	model : 0.06809077262878419
			 train-loss:  2.0870253183472323 	 ± 0.2580420033232274
	data : 0.11806058883666992
	model : 0.06876411437988281
			 train-loss:  2.085770043257241 	 ± 0.25808943193259193
	data : 0.11738200187683105
	model : 0.06822004318237304
			 train-loss:  2.090088203341462 	 ± 0.26512391931929247
	data : 0.11779117584228516
	model : 0.06804952621459961
			 train-loss:  2.0896674852680275 	 ± 0.26458142133758983
	data : 0.11780071258544922
	model : 0.06893672943115234
			 train-loss:  2.088919702762832 	 ± 0.2641997636768177
	data : 0.11683144569396972
	model : 0.06989631652832032
			 train-loss:  2.088684714715415 	 ± 0.26361583278800144
	data : 0.11603007316589356
	model : 0.06990571022033691
			 train-loss:  2.0884726129166067 	 ± 0.26303192479688564
	data : 0.11581459045410156
	model : 0.06914010047912597
			 train-loss:  2.0877355044538324 	 ± 0.26266004951075234
	data : 0.11642885208129883
	model : 0.0685692310333252
			 train-loss:  2.088547536150902 	 ± 0.26234175287874595
	data : 0.11696796417236328
	model : 0.06843080520629882
			 train-loss:  2.0895976698076404 	 ± 0.2622153608091528
	data : 0.11717715263366699
	model : 0.06799287796020508
			 train-loss:  2.08996104018036 	 ± 0.2616827869788897
	data : 0.11760387420654297
	model : 0.06714253425598145
			 train-loss:  2.0881573611072133 	 ± 0.26248362378155726
	data : 0.11851515769958496
	model : 0.06783289909362793
			 train-loss:  2.087476838429769 	 ± 0.26209764946129166
	data : 0.11788430213928222
	model : 0.06780295372009278
			 train-loss:  2.086872050192504 	 ± 0.26167444443786897
	data : 0.11793060302734375
	model : 0.06721806526184082
			 train-loss:  2.0847578626372214 	 ± 0.2630247930332366
	data : 0.118304443359375
	model : 0.06705107688903808
			 train-loss:  2.08478004367728 	 ± 0.26244756319342893
	data : 0.11821522712707519
	model : 0.06719021797180176
			 train-loss:  2.083101695802014 	 ± 0.26309729215090677
	data : 0.11807169914245605
	model : 0.06739687919616699
			 train-loss:  2.083086856033491 	 ± 0.2625248145233999
	data : 0.11818070411682129
	model : 0.06783132553100586
			 train-loss:  2.0833838311108677 	 ± 0.2619946775259077
	data : 0.11773443222045898
	model : 0.06835460662841797
			 train-loss:  2.0861067674283325 	 ± 0.26468484287646565
	data : 0.11724209785461426
	model : 0.0680809497833252
			 train-loss:  2.0863106081925746 	 ± 0.2641344875224436
	data : 0.11758780479431152
	model : 0.06869568824768066
			 train-loss:  2.084856593201303 	 ± 0.2645023194495216
	data : 0.11712050437927246
	model : 0.06855659484863282
			 train-loss:  2.0866215274689046 	 ± 0.2653161829519089
	data : 0.11715865135192871
	model : 0.06869421005249024
			 train-loss:  2.087462202472202 	 ± 0.26506694527936
	data : 0.11726984977722169
	model : 0.06814889907836914
			 train-loss:  2.0876169068903865 	 ± 0.26451781798386353
	data : 0.11775264739990235
	model : 0.06838436126708984
			 train-loss:  2.0885507163881254 	 ± 0.26435269969819164
	data : 0.11731572151184082
	model : 0.06802120208740234
			 train-loss:  2.0905141486283627 	 ± 0.26553241010295425
	data : 0.117486572265625
	model : 0.0673990249633789
			 train-loss:  2.091199781994025 	 ± 0.26519055786763623
	data : 0.1177452564239502
	model : 0.0666952133178711
			 train-loss:  2.0922466842960024 	 ± 0.2651363117039687
	data : 0.11806039810180664
	model : 0.06716957092285156
			 train-loss:  2.091830361480555 	 ± 0.2646668669712526
	data : 0.11769204139709473
	model : 0.0676344394683838
			 train-loss:  2.0915140188279957 	 ± 0.26416756504090577
	data : 0.1173851490020752
	model : 0.06734161376953125
			 train-loss:  2.091421280239449 	 ± 0.2636296460356366
	data : 0.11759920120239258
	model : 0.0676544189453125
			 train-loss:  2.0930117466011824 	 ± 0.2642614854697732
	data : 0.11727495193481445
	model : 0.06799588203430176
			 train-loss:  2.092972941999513 	 ± 0.26372452111011774
	data : 0.1171004295349121
	model : 0.067836332321167
			 train-loss:  2.0915786592583907 	 ± 0.26409708737145066
	data : 0.11733980178833008
	model : 0.06784420013427735
			 train-loss:  2.0909559289293904 	 ± 0.26374574393976824
	data : 0.11740942001342773
	model : 0.06785550117492675
			 train-loss:  2.0922963384643616 	 ± 0.26406066301620756
	data : 0.11766204833984376
	model : 0.06764683723449708
			 train-loss:  2.091198024749756 	 ± 0.2641012846967163
	data : 0.1180544376373291
	model : 0.06796326637268066
			 train-loss:  2.09175596674125 	 ± 0.2637222534907607
	data : 0.11769399642944336
	model : 0.06770896911621094
			 train-loss:  2.092272944866665 	 ± 0.26332588376105104
	data : 0.11764960289001465
	model : 0.06728386878967285
			 train-loss:  2.0914467323438926 	 ± 0.2631320388896716
	data : 0.1181295394897461
	model : 0.06786789894104003
			 train-loss:  2.0921254082927554 	 ± 0.2628353278254561
	data : 0.11753911972045898
	model : 0.06810436248779297
			 train-loss:  2.0909170127382466 	 ± 0.26302546260087933
	data : 0.11707806587219238
	model : 0.0674217700958252
			 train-loss:  2.0899266125634313 	 ± 0.26298721991579005
	data : 0.1168238639831543
	model : 0.05880956649780274
#epoch  43    val-loss:  2.4440117133291146  train-loss:  2.0899266125634313  lr:  0.0003125
			 train-loss:  2.1101200580596924 	 ± 0.0
	data : 5.799320697784424
	model : 0.07174396514892578
			 train-loss:  2.323822855949402 	 ± 0.21370279788970947
	data : 2.9641010761260986
	model : 0.06991946697235107
			 train-loss:  2.384098768234253 	 ± 0.19419653726951158
	data : 2.0150789419809976
	model : 0.06981245676676433
			 train-loss:  2.2888991832733154 	 ± 0.23552728981119336
	data : 1.5402705073356628
	model : 0.06967765092849731
			 train-loss:  2.3046712398529055 	 ± 0.21301059708766562
	data : 1.2554623126983642
	model : 0.0696187973022461
			 train-loss:  2.256223201751709 	 ± 0.22259228207415016
	data : 0.11876459121704101
	model : 0.06827964782714843
			 train-loss:  2.223369700568063 	 ± 0.221235734298724
	data : 0.11695742607116699
	model : 0.06868228912353516
			 train-loss:  2.231989860534668 	 ± 0.20820000953243895
	data : 0.11662869453430176
	model : 0.06869797706604004
			 train-loss:  2.1849000321494207 	 ± 0.2372140363197121
	data : 0.11667985916137695
	model : 0.06887478828430176
			 train-loss:  2.1782190442085265 	 ± 0.2259317806842988
	data : 0.11675887107849121
	model : 0.06873493194580078
			 train-loss:  2.19032084941864 	 ± 0.21879039389324395
	data : 0.11685209274291992
	model : 0.06960411071777343
			 train-loss:  2.1657791336377463 	 ± 0.22473403696325744
	data : 0.11619315147399903
	model : 0.06946439743041992
			 train-loss:  2.150595188140869 	 ± 0.22223182486392717
	data : 0.11629862785339355
	model : 0.0695199966430664
			 train-loss:  2.130402777876173 	 ± 0.2261854922164499
	data : 0.11627640724182128
	model : 0.06947808265686035
			 train-loss:  2.176937699317932 	 ± 0.2794032985134355
	data : 0.1161428451538086
	model : 0.06928515434265137
			 train-loss:  2.1812693253159523 	 ± 0.2710507522139111
	data : 0.11637444496154785
	model : 0.06936860084533691
			 train-loss:  2.1695074053371655 	 ± 0.26713351977257077
	data : 0.11600747108459472
	model : 0.06945614814758301
			 train-loss:  2.156427734427982 	 ± 0.26514934990991595
	data : 0.1160351276397705
	model : 0.06951556205749512
			 train-loss:  2.182239664228339 	 ± 0.28035078176456635
	data : 0.11589360237121582
	model : 0.0687063217163086
			 train-loss:  2.180512565374374 	 ± 0.27335582549632864
	data : 0.1165398120880127
	model : 0.06913108825683593
			 train-loss:  2.1659103688739596 	 ± 0.27464455700682794
	data : 0.11637792587280274
	model : 0.06894869804382324
			 train-loss:  2.179060686718334 	 ± 0.275013747062144
	data : 0.1167119026184082
	model : 0.06883072853088379
			 train-loss:  2.179455041885376 	 ± 0.2689751107529091
	data : 0.11673870086669921
	model : 0.06780743598937988
			 train-loss:  2.175996273756027 	 ± 0.2638338060529526
	data : 0.11766285896301269
	model : 0.0676788330078125
			 train-loss:  2.1504970455169676 	 ± 0.2871045132118878
	data : 0.11789040565490723
	model : 0.06689815521240235
			 train-loss:  2.146682830957266 	 ± 0.2821743452416031
	data : 0.11837196350097656
	model : 0.06707415580749512
			 train-loss:  2.1583450546971075 	 ± 0.28321295662723045
	data : 0.11822175979614258
	model : 0.06756539344787597
			 train-loss:  2.1771874938692366 	 ± 0.2948405736393693
	data : 0.11785025596618652
	model : 0.0683466911315918
			 train-loss:  2.1762204581293565 	 ± 0.2897577071413772
	data : 0.11712427139282226
	model : 0.06897144317626953
			 train-loss:  2.163869873682658 	 ± 0.29254819950789296
	data : 0.11653218269348145
	model : 0.06959328651428223
			 train-loss:  2.1700209571469213 	 ± 0.2897563376188374
	data : 0.11618938446044921
	model : 0.06877145767211915
			 train-loss:  2.1642015911638737 	 ± 0.2870275951008363
	data : 0.1171579360961914
	model : 0.06828160285949707
			 train-loss:  2.1549867861198657 	 ± 0.28741178517890115
	data : 0.11756553649902343
	model : 0.06849861145019531
			 train-loss:  2.1492080723538116 	 ± 0.28509287312162235
	data : 0.1173858642578125
	model : 0.0687706470489502
			 train-loss:  2.149820944241115 	 ± 0.2810133272173609
	data : 0.11706500053405762
	model : 0.06801414489746094
			 train-loss:  2.1437666515509286 	 ± 0.2793883086392506
	data : 0.11758232116699219
	model : 0.06875615119934082
			 train-loss:  2.140497191532238 	 ± 0.27628422553863313
	data : 0.11682572364807128
	model : 0.06865115165710449
			 train-loss:  2.136197419543015 	 ± 0.27387637407341425
	data : 0.11706314086914063
	model : 0.06864347457885742
			 train-loss:  2.1417069160021267 	 ± 0.27246734169861475
	data : 0.11711349487304687
	model : 0.06865925788879394
			 train-loss:  2.1382903307676315 	 ± 0.2698846792926939
	data : 0.11717104911804199
	model : 0.06959099769592285
			 train-loss:  2.1319706178292996 	 ± 0.26955289042242325
	data : 0.11643714904785156
	model : 0.0698615550994873
			 train-loss:  2.157783119451432 	 ± 0.3134429534717389
	data : 0.11624865531921387
	model : 0.06984682083129883
			 train-loss:  2.158688947211864 	 ± 0.3098324468282537
	data : 0.11613454818725585
	model : 0.06929025650024415
			 train-loss:  2.1575287986885416 	 ± 0.30638585175329325
	data : 0.1164515495300293
	model : 0.06905155181884766
			 train-loss:  2.1552759408950806 	 ± 0.30333076907020057
	data : 0.11665482521057129
	model : 0.06871776580810547
			 train-loss:  2.153381775254789 	 ± 0.30028453438947583
	data : 0.11680164337158203
	model : 0.06864490509033203
			 train-loss:  2.1583158690878688 	 ± 0.29895176345004687
	data : 0.11689324378967285
	model : 0.06908168792724609
			 train-loss:  2.149423030515512 	 ± 0.30203827256687377
	data : 0.1166196346282959
	model : 0.06889266967773437
			 train-loss:  2.1437868828676185 	 ± 0.3014798763925982
	data : 0.1169438362121582
	model : 0.06845288276672364
			 train-loss:  2.1365118741989138 	 ± 0.30276339831237725
	data : 0.1174088954925537
	model : 0.0687363624572754
			 train-loss:  2.1383725661857453 	 ± 0.3000690219060754
	data : 0.11734042167663575
	model : 0.06867918968200684
			 train-loss:  2.139355494425847 	 ± 0.29725262946332326
	data : 0.11732449531555175
	model : 0.06884636878967285
			 train-loss:  2.1358927128449925 	 ± 0.29549195747722595
	data : 0.11720447540283203
	model : 0.06961760520935059
			 train-loss:  2.132747921678755 	 ± 0.29363701741299914
	data : 0.11664295196533203
	model : 0.07038421630859375
			 train-loss:  2.129061141881076 	 ± 0.2922139613742684
	data : 0.11588835716247559
	model : 0.07036142349243164
			 train-loss:  2.1431082763842175 	 ± 0.30776111676220624
	data : 0.11581764221191407
	model : 0.0702280044555664
			 train-loss:  2.148157469013281 	 ± 0.30738068952142344
	data : 0.11588964462280274
	model : 0.06953115463256836
			 train-loss:  2.14053691872235 	 ± 0.3101032438064718
	data : 0.11649098396301269
	model : 0.06939635276794434
			 train-loss:  2.136865021818775 	 ± 0.30873310010894645
	data : 0.1164283275604248
	model : 0.06926746368408203
			 train-loss:  2.1339507857958475 	 ± 0.30696677101722564
	data : 0.11641335487365723
	model : 0.06935620307922363
			 train-loss:  2.1383848424817695 	 ± 0.30637153736482414
	data : 0.11632885932922363
	model : 0.06952505111694336
			 train-loss:  2.1453383353448685 	 ± 0.30870537090533734
	data : 0.11614346504211426
	model : 0.06907839775085449
			 train-loss:  2.1436096524435375 	 ± 0.3065478767729759
	data : 0.11645736694335937
	model : 0.06826825141906738
			 train-loss:  2.148004364222288 	 ± 0.30613729675456114
	data : 0.11742959022521973
	model : 0.06806297302246093
			 train-loss:  2.148875188827515 	 ± 0.30385314067462915
	data : 0.11760978698730469
	model : 0.06773347854614258
			 train-loss:  2.1487266800620337 	 ± 0.3015448139136036
	data : 0.11789321899414062
	model : 0.06770477294921876
			 train-loss:  2.15102802105804 	 ± 0.2998694193946196
	data : 0.11797418594360351
	model : 0.06854462623596191
			 train-loss:  2.1453999596483566 	 ± 0.3012001348735545
	data : 0.11725192070007324
	model : 0.06857132911682129
			 train-loss:  2.142172447149304 	 ± 0.3001917079191665
	data : 0.11718339920043945
	model : 0.06884560585021973
			 train-loss:  2.1423257521220616 	 ± 0.29804248883097356
	data : 0.11751027107238769
	model : 0.06911416053771972
			 train-loss:  2.1421774071706854 	 ± 0.2959387577331996
	data : 0.1172863483428955
	model : 0.0688969612121582
			 train-loss:  2.137401095694966 	 ± 0.2966194475728876
	data : 0.1175044059753418
	model : 0.06918249130249024
			 train-loss:  2.134146273952641 	 ± 0.29587261959852024
	data : 0.11741461753845214
	model : 0.07053709030151367
			 train-loss:  2.131533928819605 	 ± 0.29471308622099646
	data : 0.11625871658325196
	model : 0.07067251205444336
			 train-loss:  2.130486911137899 	 ± 0.29288026225737485
	data : 0.11570439338684083
	model : 0.06995716094970703
			 train-loss:  2.128837839553231 	 ± 0.29129733392001134
	data : 0.11639585494995117
	model : 0.07038636207580566
			 train-loss:  2.1313082397758185 	 ± 0.2901998512141051
	data : 0.11616377830505371
	model : 0.06983652114868164
			 train-loss:  2.125605575549297 	 ± 0.2926436931421726
	data : 0.11647915840148926
	model : 0.06846861839294434
			 train-loss:  2.1259485724606093 	 ± 0.2908013974658931
	data : 0.1176600456237793
	model : 0.06828432083129883
			 train-loss:  2.123790806531906 	 ± 0.2896138894441314
	data : 0.11781487464904786
	model : 0.06823415756225586
			 train-loss:  2.120240037823901 	 ± 0.289567492873735
	data : 0.11788220405578613
	model : 0.06804141998291016
			 train-loss:  2.119623437160399 	 ± 0.2878499192204464
	data : 0.11785678863525391
	model : 0.068109130859375
			 train-loss:  2.1183040716561927 	 ± 0.286359968264499
	data : 0.11775784492492676
	model : 0.0682065486907959
			 train-loss:  2.117416989235651 	 ± 0.2847650447789533
	data : 0.11774544715881348
	model : 0.06858887672424316
			 train-loss:  2.114358632704791 	 ± 0.2844693571855239
	data : 0.11737580299377441
	model : 0.06950163841247559
			 train-loss:  2.113348043242166 	 ± 0.28296406476906516
	data : 0.1164973258972168
	model : 0.06951675415039063
			 train-loss:  2.112696992939916 	 ± 0.2813979122551986
	data : 0.11664133071899414
	model : 0.06970815658569336
			 train-loss:  2.1110692728649485 	 ± 0.28020610635558924
	data : 0.11642036437988282
	model : 0.07052984237670898
			 train-loss:  2.107201640525561 	 ± 0.28097975170568895
	data : 0.115440034866333
	model : 0.07027249336242676
			 train-loss:  2.1093117025163437 	 ± 0.28012258574020765
	data : 0.1156951904296875
	model : 0.06934704780578613
			 train-loss:  2.1066016522082656 	 ± 0.2797630489929457
	data : 0.11647329330444336
	model : 0.06939764022827148
			 train-loss:  2.1056057629377944 	 ± 0.2784005832499923
	data : 0.11624951362609863
	model : 0.06924033164978027
			 train-loss:  2.1080175574107836 	 ± 0.27786438891722864
	data : 0.11653528213500977
	model : 0.06824150085449218
			 train-loss:  2.110018456235845 	 ± 0.27705520168934006
	data : 0.1174093246459961
	model : 0.06786103248596191
			 train-loss:  2.107585781498959 	 ± 0.27660056462906984
	data : 0.11772584915161133
	model : 0.06869893074035645
			 train-loss:  2.1102558548251786 	 ± 0.276384149415888
	data : 0.11714687347412109
	model : 0.06885004043579102
			 train-loss:  2.1088941625713074 	 ± 0.2752793029056445
	data : 0.11690354347229004
	model : 0.06804947853088379
			 train-loss:  2.1110129684818033 	 ± 0.27466508415813556
	data : 0.11757540702819824
	model : 0.06949906349182129
			 train-loss:  2.113480422231886 	 ± 0.27436387375549004
	data : 0.11632723808288574
	model : 0.06970868110656739
			 train-loss:  2.114015530347824 	 ± 0.27304052373004234
	data : 0.11609849929809571
	model : 0.06879072189331055
			 train-loss:  2.1142485743702046 	 ± 0.27169547028647173
	data : 0.11683354377746583
	model : 0.06867804527282714
			 train-loss:  2.119393338175381 	 ± 0.27525996175586903
	data : 0.11721773147583008
	model : 0.06929588317871094
			 train-loss:  2.117849449509556 	 ± 0.2743639204580786
	data : 0.11666884422302246
	model : 0.0695457935333252
			 train-loss:  2.117757038428233 	 ± 0.2730432877395867
	data : 0.1164100170135498
	model : 0.06874141693115235
			 train-loss:  2.114291540781657 	 ± 0.27402849843610644
	data : 0.11719536781311035
	model : 0.06877317428588867
			 train-loss:  2.1173551802365287 	 ± 0.27453364924813156
	data : 0.11713294982910157
	model : 0.06859059333801269
			 train-loss:  2.1222367843734884 	 ± 0.27783147808472164
	data : 0.1170741081237793
	model : 0.068916654586792
			 train-loss:  2.1189630130926767 	 ± 0.2786079428581949
	data : 0.11685805320739746
	model : 0.06836404800415039
			 train-loss:  2.116350404713132 	 ± 0.27865288779857855
	data : 0.11764750480651856
	model : 0.069392728805542
			 train-loss:  2.115225834196264 	 ± 0.27763175961785214
	data : 0.11659307479858398
	model : 0.07041788101196289
			 train-loss:  2.113059403660061 	 ± 0.2773107667104092
	data : 0.11572265625
	model : 0.07041854858398437
			 train-loss:  2.1100626875247275 	 ± 0.277869496476101
	data : 0.11576132774353028
	model : 0.07035360336303711
			 train-loss:  2.110207252797827 	 ± 0.276641483769868
	data : 0.11569371223449706
	model : 0.07015609741210938
			 train-loss:  2.111269276393087 	 ± 0.2756567472594982
	data : 0.11574482917785645
	model : 0.06998801231384277
			 train-loss:  2.1117263700651088 	 ± 0.2744990117387465
	data : 0.11600494384765625
	model : 0.06966252326965332
			 train-loss:  2.113423753401329 	 ± 0.27391872640239723
	data : 0.11621060371398925
	model : 0.06938586235046387
			 train-loss:  2.1102474846391597 	 ± 0.27488263191188794
	data : 0.1165247917175293
	model : 0.06954755783081054
			 train-loss:  2.109843044967975 	 ± 0.27375035396064595
	data : 0.1163848876953125
	model : 0.0693892002105713
			 train-loss:  2.1109737257997527 	 ± 0.2728742759275203
	data : 0.11653571128845215
	model : 0.06942744255065918
			 train-loss:  2.1114926785230637 	 ± 0.27179388421252565
	data : 0.11650786399841309
	model : 0.06869077682495117
			 train-loss:  2.1125922429660133 	 ± 0.27093631877754454
	data : 0.11715703010559082
	model : 0.06882023811340332
			 train-loss:  2.1127895302459843 	 ± 0.26983236628707624
	data : 0.11697883605957031
	model : 0.06871500015258789
			 train-loss:  2.1152896600040965 	 ± 0.27014836266055403
	data : 0.1170079231262207
	model : 0.06888713836669921
			 train-loss:  2.116114936528667 	 ± 0.2692124836527266
	data : 0.11681694984436035
	model : 0.06889824867248535
			 train-loss:  2.116797745704651 	 ± 0.2682412548091995
	data : 0.11684303283691407
	model : 0.06983957290649415
			 train-loss:  2.1193335822650363 	 ± 0.2686747488432261
	data : 0.11608295440673828
	model : 0.07000293731689453
			 train-loss:  2.1236537357014935 	 ± 0.27197307854838937
	data : 0.11591944694519044
	model : 0.07004075050354004
			 train-loss:  2.1210173424333334 	 ± 0.27253291945666713
	data : 0.115802001953125
	model : 0.0691451072692871
			 train-loss:  2.119515820067058 	 ± 0.272005529615098
	data : 0.11651339530944824
	model : 0.06923041343688965
			 train-loss:  2.1166257014641396 	 ± 0.27293843013371627
	data : 0.1165550708770752
	model : 0.06827721595764161
			 train-loss:  2.1166633595037094 	 ± 0.2718950237258362
	data : 0.1173243522644043
	model : 0.06845173835754395
			 train-loss:  2.1176119952490837 	 ± 0.2710806887353584
	data : 0.1172935962677002
	model : 0.06836061477661133
			 train-loss:  2.1167687688555037 	 ± 0.27023337881117665
	data : 0.11756396293640137
	model : 0.06962275505065918
			 train-loss:  2.114863632330254 	 ± 0.27011819226709044
	data : 0.11653356552124024
	model : 0.0693753719329834
			 train-loss:  2.1135769429030242 	 ± 0.2695277560076193
	data : 0.1168135166168213
	model : 0.07037625312805176
			 train-loss:  2.110590471940882 	 ± 0.2707676522083253
	data : 0.11582274436950683
	model : 0.06927733421325684
			 train-loss:  2.11059449536957 	 ± 0.26977764327653125
	data : 0.11675024032592773
	model : 0.06853995323181153
			 train-loss:  2.1075795949369236 	 ± 0.2711048952864801
	data : 0.11739301681518555
	model : 0.06746602058410645
			 train-loss:  2.105965427357516 	 ± 0.27079266374380423
	data : 0.11839861869812011
	model : 0.06764163970947265
			 train-loss:  2.107504028933389 	 ± 0.2704328835866173
	data : 0.11823339462280273
	model : 0.06699547767639161
			 train-loss:  2.108131204091065 	 ± 0.26957435501606214
	data : 0.11879024505615235
	model : 0.06803698539733886
			 train-loss:  2.106430897410487 	 ± 0.2693811556338615
	data : 0.11773529052734374
	model : 0.0689732551574707
			 train-loss:  2.1068066358566284 	 ± 0.2684749494636332
	data : 0.11690468788146972
	model : 0.0696570873260498
			 train-loss:  2.1057740814156003 	 ± 0.26782590128310035
	data : 0.11622257232666015
	model : 0.06894397735595703
			 train-loss:  2.1038770560560556 	 ± 0.2678698048938523
	data : 0.11683988571166992
	model : 0.06932110786437988
			 train-loss:  2.10342207346877 	 ± 0.26700708109763727
	data : 0.1167414665222168
	model : 0.06896262168884278
			 train-loss:  2.1040834261446584 	 ± 0.2662173078594492
	data : 0.11717290878295898
	model : 0.06834230422973633
			 train-loss:  2.1053076595873446 	 ± 0.2657312709409349
	data : 0.11767826080322266
	model : 0.06740550994873047
			 train-loss:  2.1067363975832127 	 ± 0.26540781098451094
	data : 0.11836471557617187
	model : 0.06727643013000488
			 train-loss:  2.1074803924560546 	 ± 0.2646774889394589
	data : 0.11832866668701172
	model : 0.06751437187194824
			 train-loss:  2.1068615439711817 	 ± 0.2639084770272643
	data : 0.11812367439270019
	model : 0.0673858642578125
			 train-loss:  2.109407076710149 	 ± 0.2648922725818098
	data : 0.11820201873779297
	model : 0.06699209213256836
			 train-loss:  2.111504215041017 	 ± 0.26528813881925994
	data : 0.11854991912841797
	model : 0.06793084144592285
			 train-loss:  2.1097524568632053 	 ± 0.26531170798920317
	data : 0.11775445938110352
	model : 0.06878571510314942
			 train-loss:  2.111040578349944 	 ± 0.26493715667209733
	data : 0.11713662147521972
	model : 0.06873083114624023
			 train-loss:  2.1133175507569923 	 ± 0.2656037720515818
	data : 0.11714320182800293
	model : 0.06904330253601074
			 train-loss:  2.111222093272361 	 ± 0.26604702058847146
	data : 0.11677007675170899
	model : 0.06991534233093262
			 train-loss:  2.1125028020218957 	 ± 0.2656888213346387
	data : 0.1160698413848877
	model : 0.06987218856811524
			 train-loss:  2.1114840125137904 	 ± 0.2651614174914168
	data : 0.11619348526000976
	model : 0.06979780197143555
			 train-loss:  2.1108280293643475 	 ± 0.26446087825357784
	data : 0.11618390083312988
	model : 0.0697474479675293
			 train-loss:  2.1098116036527648 	 ± 0.2639516022722528
	data : 0.11634016036987305
	model : 0.06972212791442871
			 train-loss:  2.1102929144729803 	 ± 0.263206537006894
	data : 0.11649503707885742
	model : 0.069484281539917
			 train-loss:  2.1088530205510145 	 ± 0.26303714259512545
	data : 0.11660075187683105
	model : 0.06870427131652831
			 train-loss:  2.106918156147003 	 ± 0.2633949130026741
	data : 0.1173696517944336
	model : 0.06950812339782715
			 train-loss:  2.1083145184950394 	 ± 0.2632036968119113
	data : 0.11654205322265625
	model : 0.0693596363067627
			 train-loss:  2.1053448039365104 	 ± 0.2651679248060273
	data : 0.11669354438781739
	model : 0.06885533332824707
			 train-loss:  2.1023495154466456 	 ± 0.26717465688357966
	data : 0.11725153923034667
	model : 0.06908807754516602
			 train-loss:  2.0993324603353227 	 ± 0.2692165307928948
	data : 0.11712851524353027
	model : 0.06902613639831542
			 train-loss:  2.100019691963873 	 ± 0.2685666091202501
	data : 0.11717238426208496
	model : 0.06892094612121583
			 train-loss:  2.1025874726912557 	 ± 0.26984818358101403
	data : 0.11725344657897949
	model : 0.069099760055542
			 train-loss:  2.1011901117904843 	 ± 0.2696741564384234
	data : 0.11699590682983399
	model : 0.0694655418395996
			 train-loss:  2.102662817683331 	 ± 0.26957783910035593
	data : 0.116615629196167
	model : 0.06940250396728516
			 train-loss:  2.1032770017668003 	 ± 0.2689182458433682
	data : 0.11643385887145996
	model : 0.07036404609680176
			 train-loss:  2.1039712463302176 	 ± 0.26829981296481886
	data : 0.11548438072204589
	model : 0.06920881271362304
			 train-loss:  2.1019402033942085 	 ± 0.26887026898276606
	data : 0.11658549308776855
	model : 0.06897883415222168
			 train-loss:  2.102290743453936 	 ± 0.2681454450000499
	data : 0.11669383049011231
	model : 0.06845402717590332
			 train-loss:  2.1029030232779724 	 ± 0.2675102499699835
	data : 0.11735062599182129
	model : 0.06831812858581543
			 train-loss:  2.104169219397427 	 ± 0.26728912686248874
	data : 0.11765546798706054
	model : 0.0682023048400879
			 train-loss:  2.1056527219005137 	 ± 0.26727530852967346
	data : 0.11777963638305664
	model : 0.06870226860046387
			 train-loss:  2.1045389009846582 	 ± 0.26694810403662145
	data : 0.11744627952575684
	model : 0.06900877952575683
			 train-loss:  2.10382161943952 	 ± 0.26638353959946803
	data : 0.11719598770141601
	model : 0.06979808807373047
			 train-loss:  2.102430004994948 	 ± 0.2663096354673403
	data : 0.1164388656616211
	model : 0.07037878036499023
			 train-loss:  2.1011975091663215 	 ± 0.2661010011889292
	data : 0.11593899726867676
	model : 0.0696608543395996
			 train-loss:  2.1017325902762622 	 ± 0.2654756153269313
	data : 0.11650748252868652
	model : 0.06968297958374023
			 train-loss:  2.0997284579921414 	 ± 0.2661491845662454
	data : 0.11645078659057617
	model : 0.06868062019348145
			 train-loss:  2.0971044359668607 	 ± 0.26782152446643004
	data : 0.11728482246398926
	model : 0.06883487701416016
			 train-loss:  2.0948146760144972 	 ± 0.26892376786155997
	data : 0.11712970733642578
	model : 0.0684776782989502
			 train-loss:  2.0939362226648535 	 ± 0.2684764725654969
	data : 0.11740097999572754
	model : 0.06917223930358887
			 train-loss:  2.0933657600766136 	 ± 0.26787949356384877
	data : 0.11675658226013183
	model : 0.06922268867492676
			 train-loss:  2.0922778543673064 	 ± 0.26759191082182154
	data : 0.11658806800842285
	model : 0.07003774642944335
			 train-loss:  2.090169361124488 	 ± 0.2684682933003423
	data : 0.11593317985534668
	model : 0.06897602081298829
			 train-loss:  2.089809654901425 	 ± 0.2678143870847271
	data : 0.11695866584777832
	model : 0.0689061164855957
			 train-loss:  2.090227852213568 	 ± 0.2671825122352769
	data : 0.11697430610656738
	model : 0.06899232864379883
			 train-loss:  2.090281298480083 	 ± 0.2664940421971979
	data : 0.1170461654663086
	model : 0.06906390190124512
			 train-loss:  2.090078377112364 	 ± 0.2658248718578093
	data : 0.11708641052246094
	model : 0.06936955451965332
			 train-loss:  2.0897642167247072 	 ± 0.26518217050858645
	data : 0.11685209274291992
	model : 0.0697113037109375
			 train-loss:  2.0890119475156523 	 ± 0.2647178485846724
	data : 0.11644644737243652
	model : 0.06965184211730957
			 train-loss:  2.0903736352920532 	 ± 0.2647393031514688
	data : 0.11660046577453613
	model : 0.06927008628845215
			 train-loss:  2.092211617896305 	 ± 0.2653367347602036
	data : 0.11688394546508789
	model : 0.06912651062011718
			 train-loss:  2.0929670226573944 	 ± 0.2648869976676899
	data : 0.11698737144470214
	model : 0.0685962200164795
			 train-loss:  2.0930431422902576 	 ± 0.26422944606245224
	data : 0.11757254600524902
	model : 0.06902136802673339
			 train-loss:  2.0928776429431273 	 ± 0.26358504483276357
	data : 0.11715936660766602
	model : 0.0693145751953125
			 train-loss:  2.0954569032039547 	 ± 0.2654781462189944
	data : 0.11673893928527831
	model : 0.0690011978149414
			 train-loss:  2.0957280116922714 	 ± 0.26485483386637937
	data : 0.1168851375579834
	model : 0.06804523468017579
			 train-loss:  2.094991592081582 	 ± 0.26441733921118005
	data : 0.11779561042785644
	model : 0.06828551292419434
			 train-loss:  2.0952340248719 	 ± 0.2637976065998754
	data : 0.11736745834350586
	model : 0.06838603019714355
			 train-loss:  2.0939777110509827 	 ± 0.2637766715943946
	data : 0.1171767234802246
	model : 0.06794614791870117
			 train-loss:  2.093409550877718 	 ± 0.2632687658335034
	data : 0.11771979331970214
	model : 0.06854267120361328
			 train-loss:  2.0955618990665417 	 ± 0.26446624645937494
	data : 0.11726217269897461
	model : 0.06896467208862304
			 train-loss:  2.094027619702475 	 ± 0.26476654857052023
	data : 0.11681451797485351
	model : 0.06814661026000976
			 train-loss:  2.094205372141436 	 ± 0.26415095422548324
	data : 0.11750612258911133
	model : 0.06722607612609863
			 train-loss:  2.093836393558754 	 ± 0.2635817187673172
	data : 0.11827874183654785
	model : 0.06749234199523926
			 train-loss:  2.0927487729300913 	 ± 0.2634386569876149
	data : 0.11808772087097168
	model : 0.06768126487731933
			 train-loss:  2.0906810231297928 	 ± 0.264549291783661
	data : 0.11799149513244629
	model : 0.06817855834960937
			 train-loss:  2.090711748322775 	 ± 0.26393372653682434
	data : 0.11768116950988769
	model : 0.06930027008056641
			 train-loss:  2.0908273276355533 	 ± 0.26332751358827816
	data : 0.11672096252441407
	model : 0.06939234733581542
			 train-loss:  2.0906994381258563 	 ± 0.2627267910250171
	data : 0.11662325859069825
	model : 0.06866683959960937
			 train-loss:  2.0913218812111323 	 ± 0.2622838349580661
	data : 0.11726236343383789
	model : 0.06809878349304199
			 train-loss:  2.090938957314513 	 ± 0.26174539758086146
	data : 0.1179124355316162
	model : 0.06846756935119629
			 train-loss:  2.089181287722154 	 ± 0.26244203211313477
	data : 0.11741461753845214
	model : 0.06824002265930176
			 train-loss:  2.08977901342228 	 ± 0.2619976443545438
	data : 0.11776738166809082
	model : 0.06889386177062988
			 train-loss:  2.089176208586306 	 ± 0.2615604511033738
	data : 0.11724467277526855
	model : 0.06910452842712403
			 train-loss:  2.089862590948028 	 ± 0.2611736390493653
	data : 0.11694817543029785
	model : 0.06871004104614258
			 train-loss:  2.0884676410683563 	 ± 0.2614212802044646
	data : 0.11728453636169434
	model : 0.0674372673034668
			 train-loss:  2.0904931243260703 	 ± 0.2625953656266435
	data : 0.11827130317687988
	model : 0.06677131652832032
			 train-loss:  2.090977438255749 	 ± 0.2621144512569044
	data : 0.11844339370727539
	model : 0.06658105850219727
			 train-loss:  2.090636522759425 	 ± 0.26158668030171606
	data : 0.1184755802154541
	model : 0.06623706817626954
			 train-loss:  2.0895042942281354 	 ± 0.26156924705559914
	data : 0.11865057945251464
	model : 0.0668342113494873
			 train-loss:  2.0907358654721855 	 ± 0.2616591741784352
	data : 0.11784377098083496
	model : 0.0674560546875
			 train-loss:  2.090332820104516 	 ± 0.2611609604785344
	data : 0.11729559898376465
	model : 0.06815218925476074
			 train-loss:  2.0914927034667046 	 ± 0.26118807993156307
	data : 0.11701855659484864
	model : 0.06812138557434082
			 train-loss:  2.091470026764376 	 ± 0.26062479458754406
	data : 0.11715860366821289
	model : 0.06884245872497559
			 train-loss:  2.092202977561132 	 ± 0.26030442379189034
	data : 0.11655759811401367
	model : 0.0690192699432373
			 train-loss:  2.091570211781396 	 ± 0.259927141182561
	data : 0.11650671958923339
	model : 0.06910271644592285
			 train-loss:  2.0925848986240143 	 ± 0.2598375343608412
	data : 0.11653146743774415
	model : 0.06859102249145507
			 train-loss:  2.09400102597172 	 ± 0.2601936478169765
	data : 0.11682729721069336
	model : 0.06869521141052246
			 train-loss:  2.093725129521849 	 ± 0.2596787270958685
	data : 0.11677179336547852
	model : 0.06840858459472657
			 train-loss:  2.092897055529747 	 ± 0.25944598984938677
	data : 0.11714520454406738
	model : 0.06824579238891601
			 train-loss:  2.093046926554277 	 ± 0.25891297060841356
	data : 0.1169966697692871
	model : 0.06818690299987792
			 train-loss:  2.093257109324137 	 ± 0.2583934369211164
	data : 0.11717424392700196
	model : 0.06848349571228027
			 train-loss:  2.0919099170637327 	 ± 0.2587000358113273
	data : 0.1168741226196289
	model : 0.06853089332580567
			 train-loss:  2.0926541789504123 	 ± 0.25842339700845096
	data : 0.11676459312438965
	model : 0.06860933303833008
			 train-loss:  2.0928493851006276 	 ± 0.2579089915245972
	data : 0.1166982650756836
	model : 0.06862974166870117
			 train-loss:  2.093063597796393 	 ± 0.25740160764329206
	data : 0.11696934700012207
	model : 0.06859230995178223
			 train-loss:  2.0923561986611814 	 ± 0.25711331655593056
	data : 0.11682090759277344
	model : 0.06852364540100098
			 train-loss:  2.0923936798320555 	 ± 0.25659086703644113
	data : 0.11678109169006348
	model : 0.06856312751770019
			 train-loss:  2.0935611189135654 	 ± 0.25672474705576614
	data : 0.11662964820861817
	model : 0.06811566352844238
			 train-loss:  2.0941894924448383 	 ± 0.25639689561339174
	data : 0.11708049774169922
	model : 0.06779923439025878
			 train-loss:  2.093407074610392 	 ± 0.25617801352836184
	data : 0.11732625961303711
	model : 0.06803755760192871
			 train-loss:  2.0935197696685792 	 ± 0.25567132858561786
	data : 0.11731724739074707
	model : 0.06780610084533692
			 train-loss:  2.0926807487153436 	 ± 0.2555061402511938
	data : 0.1178253173828125
	model : 0.06793785095214844
			 train-loss:  2.0919517117833335 	 ± 0.2552601258533029
	data : 0.11779885292053223
	model : 0.06854004859924316
			 train-loss:  2.0920376758801607 	 ± 0.2547588146555176
	data : 0.11727361679077149
	model : 0.06895961761474609
			 train-loss:  2.090199101158953 	 ± 0.2559331242157368
	data : 0.11696877479553222
	model : 0.06896982192993165
			 train-loss:  2.090523704360513 	 ± 0.2554831847428995
	data : 0.11684737205505372
	model : 0.06926555633544922
			 train-loss:  2.0894685983657837 	 ± 0.2555397594932463
	data : 0.11543064117431641
	model : 0.06015896797180176
#epoch  44    val-loss:  2.480973984065809  train-loss:  2.0894685983657837  lr:  0.0003125
			 train-loss:  1.8798551559448242 	 ± 0.0
	data : 5.680042028427124
	model : 0.0739598274230957
			 train-loss:  2.086169719696045 	 ± 0.2063145637512207
	data : 2.907897472381592
	model : 0.07184386253356934
			 train-loss:  2.135688543319702 	 ± 0.18243179710980417
	data : 1.978115479151408
	model : 0.07126537958780925
			 train-loss:  2.095534384250641 	 ± 0.17262123252707065
	data : 1.5122089385986328
	model : 0.06979602575302124
			 train-loss:  2.1064637660980225 	 ± 0.15593677400021938
	data : 1.23362193107605
	model : 0.06884455680847168
			 train-loss:  2.16489577293396 	 ± 0.19322282910076855
	data : 0.12161178588867187
	model : 0.06793432235717774
			 train-loss:  2.150345836366926 	 ± 0.18240527780061114
	data : 0.11765785217285156
	model : 0.067570161819458
			 train-loss:  2.0817131400108337 	 ± 0.24917033082227347
	data : 0.1174748420715332
	model : 0.06739068031311035
			 train-loss:  2.152171664767795 	 ± 0.3080627456439891
	data : 0.11791515350341797
	model : 0.0683401107788086
			 train-loss:  2.165698432922363 	 ± 0.2950578770999154
	data : 0.11711969375610351
	model : 0.06942024230957031
			 train-loss:  2.181178938258778 	 ± 0.28555409826397515
	data : 0.11611394882202149
	model : 0.06947550773620606
			 train-loss:  2.184989253679911 	 ± 0.2736891509031729
	data : 0.11617131233215332
	model : 0.06961774826049805
			 train-loss:  2.166013552592351 	 ± 0.2710437183045243
	data : 0.11613545417785645
	model : 0.06930313110351563
			 train-loss:  2.124170822756631 	 ± 0.3016252664813893
	data : 0.11642966270446778
	model : 0.06932024955749512
			 train-loss:  2.112297781308492 	 ± 0.2947646217009661
	data : 0.11657223701477051
	model : 0.06925406455993652
			 train-loss:  2.1426119431853294 	 ± 0.3086098215757773
	data : 0.11679615974426269
	model : 0.06923985481262207
			 train-loss:  2.125925660133362 	 ± 0.3067451391836086
	data : 0.1167724609375
	model : 0.06950221061706544
			 train-loss:  2.1331933670573764 	 ± 0.2996049856413464
	data : 0.11654820442199706
	model : 0.07007980346679688
			 train-loss:  2.1303129509875647 	 ± 0.29187002857834665
	data : 0.11594953536987304
	model : 0.06995391845703125
			 train-loss:  2.119925081729889 	 ± 0.28806068220044445
	data : 0.11605629920959473
	model : 0.06897206306457519
			 train-loss:  2.124847207750593 	 ± 0.28197894289100145
	data : 0.11683130264282227
	model : 0.06866846084594727
			 train-loss:  2.1288222182880747 	 ± 0.2760973590112428
	data : 0.11709389686584473
	model : 0.06853575706481933
			 train-loss:  2.1097827994305156 	 ± 0.28441238189314383
	data : 0.11714067459106445
	model : 0.06843395233154297
			 train-loss:  2.1113692621390023 	 ± 0.27852801903428076
	data : 0.11736350059509278
	model : 0.06858587265014648
			 train-loss:  2.1234475994110107 	 ± 0.2792418533132658
	data : 0.11714048385620117
	model : 0.06856069564819336
			 train-loss:  2.1191555995207567 	 ± 0.27465881891717026
	data : 0.11727352142333984
	model : 0.06881670951843262
			 train-loss:  2.1225754066749856 	 ± 0.270088057237016
	data : 0.11706476211547852
	model : 0.0688669204711914
			 train-loss:  2.116254597902298 	 ± 0.26724709455006784
	data : 0.11689672470092774
	model : 0.06869616508483886
			 train-loss:  2.11478211550877 	 ± 0.26271453297453884
	data : 0.117061185836792
	model : 0.06770014762878418
			 train-loss:  2.124880095322927 	 ± 0.26396098512529353
	data : 0.11810145378112794
	model : 0.0686788558959961
			 train-loss:  2.1220115423202515 	 ± 0.2601435487495057
	data : 0.11711468696594238
	model : 0.06882224082946778
			 train-loss:  2.1293946094810963 	 ± 0.25932534150485514
	data : 0.11708631515502929
	model : 0.06815919876098633
			 train-loss:  2.1226182561932188 	 ± 0.25822697754465873
	data : 0.11774358749389649
	model : 0.06818151473999023
			 train-loss:  2.127771861412946 	 ± 0.2561179988516699
	data : 0.11752920150756836
	model : 0.06908445358276367
			 train-loss:  2.1358164582933696 	 ± 0.2567539171374134
	data : 0.1165236473083496
	model : 0.06903491020202637
			 train-loss:  2.1442950235472784 	 ± 0.25808410211019034
	data : 0.11660075187683105
	model : 0.06806669235229493
			 train-loss:  2.1380539810335315 	 ± 0.25731192292633703
	data : 0.11730217933654785
	model : 0.06841917037963867
			 train-loss:  2.129257459389536 	 ± 0.25948041875045375
	data : 0.11715111732482911
	model : 0.06894631385803222
			 train-loss:  2.125233524884933 	 ± 0.2573304744244237
	data : 0.11666784286499024
	model : 0.0688483715057373
			 train-loss:  2.1252579003572465 	 ± 0.25409352985307176
	data : 0.11689043045043945
	model : 0.06880364418029786
			 train-loss:  2.1139249394579633 	 ± 0.261010041907046
	data : 0.11698217391967773
	model : 0.06974596977233886
			 train-loss:  2.1087695899463834 	 ± 0.25998821390204924
	data : 0.1163294792175293
	model : 0.07022924423217773
			 train-loss:  2.109729370405508 	 ± 0.2570225872745353
	data : 0.11590032577514649
	model : 0.06986541748046875
			 train-loss:  2.103433546694842 	 ± 0.25741725058224324
	data : 0.11630306243896485
	model : 0.07008504867553711
			 train-loss:  2.098513298564487 	 ± 0.256624832347291
	data : 0.11598749160766601
	model : 0.06988215446472168
			 train-loss:  2.0980169747186745 	 ± 0.253841940906765
	data : 0.11616530418395996
	model : 0.0698507308959961
			 train-loss:  2.0953317804539457 	 ± 0.25178647760176354
	data : 0.11610908508300781
	model : 0.06979742050170898
			 train-loss:  2.0880624229709306 	 ± 0.25408525847557767
	data : 0.11635041236877441
	model : 0.06890435218811035
			 train-loss:  2.0809884679560757 	 ± 0.256210346706254
	data : 0.11715087890625
	model : 0.0686955451965332
			 train-loss:  2.0851720786094665 	 ± 0.25532037566753485
	data : 0.11742029190063477
	model : 0.0689389705657959
			 train-loss:  2.0828106613720165 	 ± 0.2533556848192713
	data : 0.11715688705444335
	model : 0.0687638759613037
			 train-loss:  2.078916609287262 	 ± 0.2524441382292627
	data : 0.11734132766723633
	model : 0.0680854320526123
			 train-loss:  2.0761194746449307 	 ± 0.2508634550045624
	data : 0.11783361434936523
	model : 0.06891884803771972
			 train-loss:  2.0939947168032327 	 ± 0.2805384890646161
	data : 0.11704349517822266
	model : 0.06805486679077148
			 train-loss:  2.092937948487022 	 ± 0.2780848903339912
	data : 0.11784610748291016
	model : 0.06819849014282227
			 train-loss:  2.095170104077884 	 ± 0.27608754111163136
	data : 0.11778573989868164
	model : 0.06832876205444335
			 train-loss:  2.0871117721524155 	 ± 0.2802204798993312
	data : 0.11741938591003417
	model : 0.06888933181762695
			 train-loss:  2.0886653723387885 	 ± 0.27804180081282714
	data : 0.11689925193786621
	model : 0.06818490028381348
			 train-loss:  2.0898569660671686 	 ± 0.2758247717131283
	data : 0.11757822036743164
	model : 0.06915278434753418
			 train-loss:  2.0955855786800384 	 ± 0.2770334265476295
	data : 0.11671276092529297
	model : 0.06810145378112793
			 train-loss:  2.0994429568775366 	 ± 0.2763731632450768
	data : 0.11767048835754394
	model : 0.0672065258026123
			 train-loss:  2.100286081913979 	 ± 0.27421436513727143
	data : 0.1186452865600586
	model : 0.06638264656066895
			 train-loss:  2.103668180723039 	 ± 0.2733297694508998
	data : 0.11917986869812011
	model : 0.06704072952270508
			 train-loss:  2.1028149854391813 	 ± 0.2712705154571526
	data : 0.11874618530273437
	model : 0.06617617607116699
			 train-loss:  2.101279150522672 	 ± 0.26945600141465964
	data : 0.11960945129394532
	model : 0.0671605110168457
			 train-loss:  2.1050007036238005 	 ± 0.26908490276116614
	data : 0.11882319450378417
	model : 0.0677840232849121
			 train-loss:  2.1096493326016326 	 ± 0.2697262161807365
	data : 0.11822042465209961
	model : 0.06855497360229493
			 train-loss:  2.111279871533899 	 ± 0.26806804354118713
	data : 0.11768450736999511
	model : 0.06866860389709473
			 train-loss:  2.1103103696436123 	 ± 0.2662384932739607
	data : 0.1175917625427246
	model : 0.06930503845214844
			 train-loss:  2.109371657030923 	 ± 0.2644449346344628
	data : 0.11700906753540039
	model : 0.06938762664794922
			 train-loss:  2.1089277116345686 	 ± 0.2626023118312427
	data : 0.11686625480651855
	model : 0.0698509693145752
			 train-loss:  2.1098061982128353 	 ± 0.26087734705297955
	data : 0.11643104553222657
	model : 0.07080259323120117
			 train-loss:  2.1099017139983505 	 ± 0.2590856220314836
	data : 0.11537446975708007
	model : 0.07075538635253906
			 train-loss:  2.1123999856613778 	 ± 0.2582128568937223
	data : 0.11528224945068359
	model : 0.07003164291381836
			 train-loss:  2.1168661006291707 	 ± 0.25934708778139764
	data : 0.11573529243469238
	model : 0.06979713439941407
			 train-loss:  2.118648194953015 	 ± 0.2580970543320926
	data : 0.11591367721557617
	model : 0.0695908546447754
			 train-loss:  2.1284986115121223 	 ± 0.2704132117621424
	data : 0.11624622344970703
	model : 0.0680429458618164
			 train-loss:  2.133320220005818 	 ± 0.27198513888994147
	data : 0.1178513526916504
	model : 0.06807990074157715
			 train-loss:  2.1334794005261193 	 ± 0.27026188813440044
	data : 0.11788525581359863
	model : 0.0681837558746338
			 train-loss:  2.1301510855555534 	 ± 0.2701917954688236
	data : 0.11769571304321289
	model : 0.06734113693237305
			 train-loss:  2.127860141389164 	 ± 0.2692994639095052
	data : 0.11829757690429688
	model : 0.06738739013671875
			 train-loss:  2.1255845180371913 	 ± 0.26843479574419954
	data : 0.11826086044311523
	model : 0.06802477836608886
			 train-loss:  2.124617665647024 	 ± 0.2669564270513494
	data : 0.11760444641113281
	model : 0.06728534698486328
			 train-loss:  2.122776617606481 	 ± 0.2658921912244144
	data : 0.11846361160278321
	model : 0.06812548637390137
			 train-loss:  2.1202864787157845 	 ± 0.26530694477613787
	data : 0.11787796020507812
	model : 0.06908392906188965
			 train-loss:  2.1187326256618944 	 ± 0.26414871093511594
	data : 0.11703667640686036
	model : 0.06915140151977539
			 train-loss:  2.117653626135026 	 ± 0.2628167801014008
	data : 0.11707210540771484
	model : 0.06934895515441894
			 train-loss:  2.1190483150157062 	 ± 0.26164283284372886
	data : 0.11697063446044922
	model : 0.06932916641235351
			 train-loss:  2.124413874711883 	 ± 0.2649929146847969
	data : 0.11691360473632813
	model : 0.06922640800476074
			 train-loss:  2.121874976158142 	 ± 0.2646029162755642
	data : 0.11690101623535157
	model : 0.06895127296447753
			 train-loss:  2.1215503975585266 	 ± 0.2631630532039403
	data : 0.1173886775970459
	model : 0.06795802116394042
			 train-loss:  2.1196463794811913 	 ± 0.2623583884831556
	data : 0.11822648048400879
	model : 0.06788616180419922
			 train-loss:  2.118380755506536 	 ± 0.2612262654567964
	data : 0.11835079193115235
	model : 0.06867942810058594
			 train-loss:  2.1179139094149813 	 ± 0.2598720495657481
	data : 0.1175527572631836
	model : 0.06788725852966308
			 train-loss:  2.1167942536504647 	 ± 0.25872851507901434
	data : 0.11826596260070801
	model : 0.0682952880859375
			 train-loss:  2.1136693954467773 	 ± 0.25917329801372274
	data : 0.11796479225158692
	model : 0.06917352676391601
			 train-loss:  2.116893792889782 	 ± 0.2597622044815362
	data : 0.1170802116394043
	model : 0.06888856887817382
			 train-loss:  2.1144031930942924 	 ± 0.2595950069357593
	data : 0.11723480224609376
	model : 0.06868495941162109
			 train-loss:  2.113798925370881 	 ± 0.2583498569612083
	data : 0.11747603416442871
	model : 0.06957073211669922
			 train-loss:  2.113840869665146 	 ± 0.25705520084024047
	data : 0.11669635772705078
	model : 0.06923480033874511
			 train-loss:  2.1112488602647685 	 ± 0.2570894710093621
	data : 0.11673784255981445
	model : 0.06941299438476563
			 train-loss:  2.1097628345676496 	 ± 0.2562616659893064
	data : 0.11653347015380859
	model : 0.06970067024230957
			 train-loss:  2.1071696732808087 	 ± 0.2563559356724073
	data : 0.11627049446105957
	model : 0.06993694305419922
			 train-loss:  2.109620127540368 	 ± 0.2563297586654848
	data : 0.11599221229553222
	model : 0.06993708610534669
			 train-loss:  2.113928382737296 	 ± 0.25886199862168957
	data : 0.11603083610534667
	model : 0.07045974731445312
			 train-loss:  2.1152936793723196 	 ± 0.2580176206712884
	data : 0.11564145088195801
	model : 0.06978182792663574
			 train-loss:  2.1125587091267666 	 ± 0.25834821605624725
	data : 0.11639761924743652
	model : 0.06990594863891601
			 train-loss:  2.114580550679454 	 ± 0.2579984531227215
	data : 0.11632633209228516
	model : 0.06965265274047852
			 train-loss:  2.113823084656252 	 ± 0.2569328623459552
	data : 0.1165992259979248
	model : 0.06879820823669433
			 train-loss:  2.114181101322174 	 ± 0.2557896306545982
	data : 0.11745076179504395
	model : 0.06842646598815919
			 train-loss:  2.1147904707504823 	 ± 0.2547150115533406
	data : 0.11775517463684082
	model : 0.06879448890686035
			 train-loss:  2.119671013738428 	 ± 0.2587362371754426
	data : 0.11728014945983886
	model : 0.06784491539001465
			 train-loss:  2.1250957398288017 	 ± 0.263908902662985
	data : 0.11804132461547852
	model : 0.06792302131652832
			 train-loss:  2.1234311649673865 	 ± 0.2633440027900135
	data : 0.11805620193481445
	model : 0.06878705024719238
			 train-loss:  2.1224208904349284 	 ± 0.26241831948303834
	data : 0.11709356307983398
	model : 0.0688093662261963
			 train-loss:  2.1226083810987144 	 ± 0.26129249355296225
	data : 0.11711225509643555
	model : 0.06899847984313964
			 train-loss:  2.1183876278054 	 ± 0.2641150179099074
	data : 0.1169888973236084
	model : 0.06980509757995605
			 train-loss:  2.1164729999283614 	 ± 0.26380766182659277
	data : 0.11622114181518554
	model : 0.06990551948547363
			 train-loss:  2.1185883373773398 	 ± 0.26369995042209987
	data : 0.11606373786926269
	model : 0.06994109153747559
			 train-loss:  2.1176021297772727 	 ± 0.26281918378751196
	data : 0.11613779067993164
	model : 0.07011175155639648
			 train-loss:  2.1143678003106237 	 ± 0.264118100378968
	data : 0.11597614288330078
	model : 0.07014460563659668
			 train-loss:  2.11266173495621 	 ± 0.2637020492150379
	data : 0.11601948738098145
	model : 0.07018847465515136
			 train-loss:  2.111806970301682 	 ± 0.26279754728565474
	data : 0.11612367630004883
	model : 0.07026815414428711
			 train-loss:  2.110921953955004 	 ± 0.2619197110526558
	data : 0.11610164642333984
	model : 0.070369291305542
			 train-loss:  2.109285701751709 	 ± 0.26150546341411157
	data : 0.11601605415344238
	model : 0.07026553153991699
			 train-loss:  2.107868316627684 	 ± 0.26094729548621953
	data : 0.11612625122070312
	model : 0.07100853919982911
			 train-loss:  2.1073744043590517 	 ± 0.2599770363251289
	data : 0.11523771286010742
	model : 0.07101321220397949
			 train-loss:  2.1069863503798842 	 ± 0.25899643262086014
	data : 0.11530575752258301
	model : 0.07082200050354004
			 train-loss:  2.104481861572857 	 ± 0.2595419698213354
	data : 0.11534295082092286
	model : 0.0707507610321045
			 train-loss:  2.1030955461355356 	 ± 0.2590208213621219
	data : 0.115421724319458
	model : 0.07076916694641114
			 train-loss:  2.1040771025737732 	 ± 0.2582728862213346
	data : 0.11533288955688477
	model : 0.06911163330078125
			 train-loss:  2.104011418241443 	 ± 0.2572938182684611
	data : 0.11696853637695312
	model : 0.06811017990112304
			 train-loss:  2.10460865228696 	 ± 0.2564165490408077
	data : 0.11764998435974121
	model : 0.06832571029663086
			 train-loss:  2.10501209835508 	 ± 0.25550034706623537
	data : 0.11755027770996093
	model : 0.06815166473388672
			 train-loss:  2.1030317615579674 	 ± 0.25558243477020876
	data : 0.11761646270751953
	model : 0.06736412048339843
			 train-loss:  2.1024621418293785 	 ± 0.25472705457821176
	data : 0.11834774017333985
	model : 0.0681067943572998
			 train-loss:  2.105101064173845 	 ± 0.2556547373765245
	data : 0.11766262054443359
	model : 0.06894955635070801
			 train-loss:  2.104912195516669 	 ± 0.2547363603424852
	data : 0.1167910099029541
	model : 0.06881208419799804
			 train-loss:  2.103193135570279 	 ± 0.2546204771729351
	data : 0.11680779457092286
	model : 0.06813459396362305
			 train-loss:  2.102010371855327 	 ± 0.25409241606040145
	data : 0.11757559776306152
	model : 0.06884708404541015
			 train-loss:  2.102173907536987 	 ± 0.2531971696995162
	data : 0.11689505577087403
	model : 0.06888532638549805
			 train-loss:  2.1012065729624787 	 ± 0.25256538772179965
	data : 0.11702957153320312
	model : 0.06887774467468262
			 train-loss:  2.099243090702937 	 ± 0.252765985676962
	data : 0.11718816757202148
	model : 0.06897878646850586
			 train-loss:  2.0977271455857487 	 ± 0.25253828482706436
	data : 0.11722135543823242
	model : 0.06983933448791504
			 train-loss:  2.097887128797071 	 ± 0.2516732788895789
	data : 0.11646475791931152
	model : 0.07020673751831055
			 train-loss:  2.094988083186215 	 ± 0.2532276742809925
	data : 0.1160811424255371
	model : 0.07070097923278809
			 train-loss:  2.093117820973299 	 ± 0.253374675507681
	data : 0.1156348705291748
	model : 0.07082138061523438
			 train-loss:  2.0914429603396236 	 ± 0.25333240862270495
	data : 0.11546568870544434
	model : 0.0708693027496338
			 train-loss:  2.089905055577323 	 ± 0.2531731253881127
	data : 0.11540079116821289
	model : 0.07068977355957032
			 train-loss:  2.0889791973431904 	 ± 0.2525807699522011
	data : 0.11561851501464844
	model : 0.07036676406860351
			 train-loss:  2.0867783083820974 	 ± 0.253182019885764
	data : 0.11593680381774903
	model : 0.07002816200256348
			 train-loss:  2.0851495014993766 	 ± 0.2531403207507304
	data : 0.11624002456665039
	model : 0.07050442695617676
			 train-loss:  2.0848783698736453 	 ± 0.2523338506723475
	data : 0.11586489677429199
	model : 0.07035021781921387
			 train-loss:  2.085564049807462 	 ± 0.2516562124198193
	data : 0.11589341163635254
	model : 0.0696333885192871
			 train-loss:  2.084568718940981 	 ± 0.2511470254442594
	data : 0.1164207935333252
	model : 0.06871986389160156
			 train-loss:  2.0828386422915335 	 ± 0.25126568244489556
	data : 0.11717219352722168
	model : 0.06864957809448242
			 train-loss:  2.084761432781341 	 ± 0.2516129253296753
	data : 0.11705188751220703
	model : 0.06839070320129395
			 train-loss:  2.085139087483853 	 ± 0.2508600520926442
	data : 0.1172576904296875
	model : 0.0689124584197998
			 train-loss:  2.0846437928061814 	 ± 0.25014742615836405
	data : 0.11689162254333496
	model : 0.07019948959350586
			 train-loss:  2.084689350426197 	 ± 0.24936515188584543
	data : 0.11588973999023437
	model : 0.07096076011657715
			 train-loss:  2.0828904972313356 	 ± 0.249628702161114
	data : 0.11526713371276856
	model : 0.07082686424255372
			 train-loss:  2.085829174077069 	 ± 0.25163505097994227
	data : 0.11559700965881348
	model : 0.07054338455200196
			 train-loss:  2.0870149325739384 	 ± 0.2513155529695007
	data : 0.11584277153015136
	model : 0.0698925495147705
			 train-loss:  2.085297867292311 	 ± 0.2515053941107529
	data : 0.11647629737854004
	model : 0.06942448616027833
			 train-loss:  2.0819223541201968 	 ± 0.254441015714421
	data : 0.11677660942077636
	model : 0.06961135864257813
			 train-loss:  2.0826570406017533 	 ± 0.253848951517799
	data : 0.11657004356384278
	model : 0.06947031021118164
			 train-loss:  2.083044989380294 	 ± 0.25313713650881803
	data : 0.1164510726928711
	model : 0.0695082664489746
			 train-loss:  2.0807418752284277 	 ± 0.2541314909342722
	data : 0.11631803512573242
	model : 0.0697415828704834
			 train-loss:  2.0792945037932085 	 ± 0.25407205351245066
	data : 0.11590991020202637
	model : 0.0698624610900879
			 train-loss:  2.0802638390484978 	 ± 0.2536369085558527
	data : 0.11589107513427735
	model : 0.0696791172027588
			 train-loss:  2.081198161108452 	 ± 0.2531874316187725
	data : 0.11615657806396484
	model : 0.06976737976074218
			 train-loss:  2.0793564284956734 	 ± 0.2535965437281407
	data : 0.11616959571838378
	model : 0.06975440979003907
			 train-loss:  2.0778017533307818 	 ± 0.25368325224054405
	data : 0.11631650924682617
	model : 0.0697591781616211
			 train-loss:  2.0772365742716294 	 ± 0.2530624349329365
	data : 0.11631665229797364
	model : 0.06980452537536622
			 train-loss:  2.0794061497279577 	 ± 0.2539560547505002
	data : 0.11634726524353027
	model : 0.06956377029418945
			 train-loss:  2.0786606811664323 	 ± 0.25342550776205885
	data : 0.11651954650878907
	model : 0.06890377998352051
			 train-loss:  2.0788578306887784 	 ± 0.2527221369929202
	data : 0.11712522506713867
	model : 0.0682377815246582
			 train-loss:  2.0793325050493303 	 ± 0.25209035629442583
	data : 0.11779332160949707
	model : 0.06811323165893554
			 train-loss:  2.0811753306308938 	 ± 0.25258466188663153
	data : 0.11788434982299804
	model : 0.06723065376281738
			 train-loss:  2.08193184071117 	 ± 0.2520853337876434
	data : 0.11864075660705567
	model : 0.0677189826965332
			 train-loss:  2.082351265032647 	 ± 0.25145097344095285
	data : 0.11821813583374023
	model : 0.06870408058166504
			 train-loss:  2.08101853957543 	 ± 0.25139942747673744
	data : 0.11741023063659668
	model : 0.06890459060668945
			 train-loss:  2.079419305415753 	 ± 0.251638196401823
	data : 0.11726951599121094
	model : 0.06902060508728028
			 train-loss:  2.0817227169223456 	 ± 0.25288057597505703
	data : 0.11737942695617676
	model : 0.07008261680603027
			 train-loss:  2.080792914210139 	 ± 0.2525113695698144
	data : 0.11640195846557617
	model : 0.07001662254333496
			 train-loss:  2.0797793140975376 	 ± 0.25220874611030003
	data : 0.11644659042358399
	model : 0.06986775398254394
			 train-loss:  2.0790415269168303 	 ± 0.251734662997869
	data : 0.11646881103515624
	model : 0.070269775390625
			 train-loss:  2.080380163294204 	 ± 0.2517307260342078
	data : 0.11593818664550781
	model : 0.06983475685119629
			 train-loss:  2.082435890480324 	 ± 0.2526411818994704
	data : 0.1160313606262207
	model : 0.06931800842285156
			 train-loss:  2.0828461433711807 	 ± 0.2520385731085871
	data : 0.11664404869079589
	model : 0.06920394897460938
			 train-loss:  2.0818509275376487 	 ± 0.2517519517946218
	data : 0.11667323112487793
	model : 0.06924309730529785
			 train-loss:  2.0819514424850545 	 ± 0.25109933444778654
	data : 0.11671409606933594
	model : 0.06878643035888672
			 train-loss:  2.082504124839071 	 ± 0.25056503183663337
	data : 0.11729736328125
	model : 0.069130277633667
			 train-loss:  2.0827781988173415 	 ± 0.24994741409112078
	data : 0.11719312667846679
	model : 0.0692021369934082
			 train-loss:  2.080792477192023 	 ± 0.25083518761174717
	data : 0.11695189476013183
	model : 0.06888508796691895
			 train-loss:  2.0809999996302078 	 ± 0.25021126561037765
	data : 0.11724486351013183
	model : 0.06886086463928223
			 train-loss:  2.082675311771141 	 ± 0.25067506775153053
	data : 0.1170933723449707
	model : 0.06850061416625977
			 train-loss:  2.0837071194793237 	 ± 0.2504602913763544
	data : 0.11731610298156739
	model : 0.06924161911010743
			 train-loss:  2.0832236771607517 	 ± 0.24992279906210257
	data : 0.11650624275207519
	model : 0.06909699440002441
			 train-loss:  2.082501950263977 	 ± 0.24950502102477423
	data : 0.11672749519348144
	model : 0.06964049339294434
			 train-loss:  2.0814224659506952 	 ± 0.2493513536989969
	data : 0.11636433601379395
	model : 0.06966533660888671
			 train-loss:  2.081243109585035 	 ± 0.24874637894466803
	data : 0.11654868125915527
	model : 0.07068219184875488
			 train-loss:  2.0819458098247132 	 ± 0.24833385665308336
	data : 0.11578187942504883
	model : 0.06973943710327149
			 train-loss:  2.082798507283716 	 ± 0.24802217997192877
	data : 0.11679577827453613
	model : 0.06997146606445312
			 train-loss:  2.0839419242812367 	 ± 0.24795491258389138
	data : 0.1163515567779541
	model : 0.06980576515197753
			 train-loss:  2.085515550618033 	 ± 0.2483763788821795
	data : 0.11643657684326172
	model : 0.0699498176574707
			 train-loss:  2.0869904123066703 	 ± 0.24867830017260686
	data : 0.11616835594177247
	model : 0.0697141170501709
			 train-loss:  2.0858659457701902 	 ± 0.248606759653304
	data : 0.11623201370239258
	model : 0.06973500251770019
			 train-loss:  2.0863423016653107 	 ± 0.24810642880348055
	data : 0.11609406471252441
	model : 0.06981749534606933
			 train-loss:  2.08779019060589 	 ± 0.24839850304060387
	data : 0.11625256538391113
	model : 0.06920504570007324
			 train-loss:  2.0874329133056353 	 ± 0.24786326175460574
	data : 0.11674504280090332
	model : 0.06910943984985352
			 train-loss:  2.0890625737748056 	 ± 0.24840848520618777
	data : 0.11680593490600585
	model : 0.06905074119567871
			 train-loss:  2.0879149649624535 	 ± 0.2483873539857413
	data : 0.1168440818786621
	model : 0.06894621849060059
			 train-loss:  2.0888426036478203 	 ± 0.24817587843975294
	data : 0.11700162887573243
	model : 0.06888785362243652
			 train-loss:  2.088607193702875 	 ± 0.24762200033507625
	data : 0.11707878112792969
	model : 0.06982378959655762
			 train-loss:  2.087419475670214 	 ± 0.24766121332141172
	data : 0.11630630493164062
	model : 0.06979212760925294
			 train-loss:  2.0870436765081872 	 ± 0.24715162644666458
	data : 0.11645469665527344
	model : 0.07006649971008301
			 train-loss:  2.0862642652397856 	 ± 0.2468512680451999
	data : 0.11617522239685059
	model : 0.07050676345825195
			 train-loss:  2.0859515356690914 	 ± 0.2463303155818596
	data : 0.11570906639099121
	model : 0.07043972015380859
			 train-loss:  2.087034696340561 	 ± 0.24629200425001727
	data : 0.11565628051757812
	model : 0.07007684707641601
			 train-loss:  2.0871220115083373 	 ± 0.2457375634921042
	data : 0.11585416793823242
	model : 0.06938881874084472
			 train-loss:  2.0858461803144164 	 ± 0.2459159785765598
	data : 0.11627521514892578
	model : 0.06871356964111328
			 train-loss:  2.0849512617684267 	 ± 0.2457260198843395
	data : 0.11693873405456542
	model : 0.06819219589233398
			 train-loss:  2.0851219732846533 	 ± 0.24519016362123977
	data : 0.11745576858520508
	model : 0.06812844276428223
			 train-loss:  2.087547490861681 	 ± 0.24732336448401376
	data : 0.11737103462219238
	model : 0.06744184494018554
			 train-loss:  2.0880819337557903 	 ± 0.2469057605199204
	data : 0.11801424026489257
	model : 0.06757912635803223
			 train-loss:  2.088981801717817 	 ± 0.24673245388514997
	data : 0.11791653633117676
	model : 0.06785035133361816
			 train-loss:  2.0880645905670367 	 ± 0.24657832336834276
	data : 0.11744251251220703
	model : 0.06794824600219726
			 train-loss:  2.0889018867734217 	 ± 0.24636397135182045
	data : 0.11719675064086914
	model : 0.06767559051513672
			 train-loss:  2.0871277886888255 	 ± 0.247289454441026
	data : 0.11735420227050782
	model : 0.06811809539794922
			 train-loss:  2.0870483515066502 	 ± 0.246756556234326
	data : 0.11694464683532715
	model : 0.06841588020324707
			 train-loss:  2.0858833137257347 	 ± 0.24686005274054862
	data : 0.11663131713867188
	model : 0.06831831932067871
			 train-loss:  2.086214967551661 	 ± 0.24638153285887038
	data : 0.11700949668884278
	model : 0.06833939552307129
			 train-loss:  2.086767714757186 	 ± 0.24599924775729456
	data : 0.11707863807678223
	model : 0.06872129440307617
			 train-loss:  2.08578565120697 	 ± 0.24593453926104478
	data : 0.11676464080810547
	model : 0.06885123252868652
			 train-loss:  2.0861917345200554 	 ± 0.24549187905713665
	data : 0.11683669090270996
	model : 0.0689809799194336
			 train-loss:  2.084680634208872 	 ± 0.24607084905431917
	data : 0.11681108474731446
	model : 0.06936869621276856
			 train-loss:  2.0852349320379626 	 ± 0.2457015764832202
	data : 0.1165590763092041
	model : 0.06912240982055665
			 train-loss:  2.084422444698701 	 ± 0.24550720134387935
	data : 0.11693792343139649
	model : 0.06904330253601074
			 train-loss:  2.084266448020935 	 ± 0.24500706355805804
	data : 0.1169931411743164
	model : 0.06829452514648438
			 train-loss:  2.0852394460147843 	 ± 0.24496243480287128
	data : 0.11771531105041504
	model : 0.0678755760192871
			 train-loss:  2.087324519787938 	 ± 0.2465895145713177
	data : 0.11806831359863282
	model : 0.06740517616271972
			 train-loss:  2.0870062883008162 	 ± 0.24613139640013992
	data : 0.11808285713195801
	model : 0.06740813255310059
			 train-loss:  2.0851758537722414 	 ± 0.24727828875529745
	data : 0.11799130439758301
	model : 0.06684446334838867
			 train-loss:  2.083801588720205 	 ± 0.24770505406194177
	data : 0.11855626106262207
	model : 0.06709103584289551
			 train-loss:  2.0837455832861305 	 ± 0.24720263014559463
	data : 0.11817426681518554
	model : 0.06698274612426758
			 train-loss:  2.0839349663691964 	 ± 0.2467195937671086
	data : 0.11819267272949219
	model : 0.06684188842773438
			 train-loss:  2.0842201767429227 	 ± 0.24626247045556024
	data : 0.11840009689331055
	model : 0.0670325756072998
			 train-loss:  2.086713991969465 	 ± 0.2488854951269171
	data : 0.11807093620300294
	model : 0.06717405319213868
			 train-loss:  2.0882840700149536 	 ± 0.24961978056234843
	data : 0.11798601150512696
	model : 0.06722207069396972
			 train-loss:  2.088176670302433 	 ± 0.24912782135235026
	data : 0.11805815696716308
	model : 0.0675048828125
			 train-loss:  2.087983973442562 	 ± 0.24865177084698184
	data : 0.11798133850097656
	model : 0.06797127723693848
			 train-loss:  2.087572623618507 	 ± 0.24824577631619704
	data : 0.11781749725341797
	model : 0.06819019317626954
			 train-loss:  2.0874853274953646 	 ± 0.2477605125170158
	data : 0.11775484085083007
	model : 0.06870713233947753
			 train-loss:  2.0873865024716247 	 ± 0.24727924633785403
	data : 0.11726212501525879
	model : 0.06924324035644532
			 train-loss:  2.086249595042318 	 ± 0.24746266893940402
	data : 0.11590147018432617
	model : 0.06078329086303711
#epoch  45    val-loss:  2.4789572452244006  train-loss:  2.086249595042318  lr:  0.0003125
			 train-loss:  2.0506131649017334 	 ± 0.0
	data : 5.470582723617554
	model : 0.08011436462402344
			 train-loss:  2.2891595363616943 	 ± 0.23854637145996094
	data : 2.804612398147583
	model : 0.07626712322235107
			 train-loss:  2.264352718989054 	 ± 0.19790655354011938
	data : 1.909260908762614
	model : 0.07427978515625
			 train-loss:  2.286833882331848 	 ± 0.17575966848606714
	data : 1.4608314633369446
	model : 0.07210075855255127
			 train-loss:  2.195069742202759 	 ± 0.24165222636588798
	data : 1.1923959732055665
	model : 0.07145423889160156
			 train-loss:  2.1476409435272217 	 ± 0.2447664604107661
	data : 0.121482515335083
	model : 0.0687138557434082
			 train-loss:  2.1053090606416975 	 ± 0.24920652072835028
	data : 0.11746878623962402
	model : 0.0674330711364746
			 train-loss:  2.0788278728723526 	 ± 0.24341256513698215
	data : 0.11751980781555176
	model : 0.0674325942993164
			 train-loss:  2.0975208150015936 	 ± 0.23550326562602997
	data : 0.11748623847961426
	model : 0.06737613677978516
			 train-loss:  2.0950170159339905 	 ± 0.223544247066236
	data : 0.11791205406188965
	model : 0.06757335662841797
			 train-loss:  2.0818909731778232 	 ± 0.21714522450783613
	data : 0.11795401573181152
	model : 0.06808266639709473
			 train-loss:  2.0943103035291037 	 ± 0.21194185188268488
	data : 0.11767072677612304
	model : 0.06879849433898926
			 train-loss:  2.10227841597337 	 ± 0.20548942518884797
	data : 0.11706147193908692
	model : 0.06871976852416992
			 train-loss:  2.0934904728616988 	 ± 0.2005336160045676
	data : 0.11724629402160644
	model : 0.06962022781372071
			 train-loss:  2.0615403493245443 	 ± 0.22764920657573476
	data : 0.11653194427490235
	model : 0.06970248222351075
			 train-loss:  2.0324385464191437 	 ± 0.24756590064328973
	data : 0.11645855903625488
	model : 0.06998620033264161
			 train-loss:  2.0283003484501556 	 ± 0.24074393477208453
	data : 0.11620154380798339
	model : 0.07006282806396484
			 train-loss:  2.0193434490097895 	 ± 0.2368577968556048
	data : 0.11629385948181152
	model : 0.0693242073059082
			 train-loss:  2.0176191769148173 	 ± 0.23065648779074746
	data : 0.11688499450683594
	model : 0.0693894386291504
			 train-loss:  2.031462234258652 	 ± 0.23277299905236884
	data : 0.11670141220092774
	model : 0.06861205101013183
			 train-loss:  2.0225142410823276 	 ± 0.23066088757016365
	data : 0.11741862297058106
	model : 0.068503999710083
			 train-loss:  2.0216752724214033 	 ± 0.22539042221272235
	data : 0.1174232006072998
	model : 0.06850004196166992
			 train-loss:  2.019191078517748 	 ± 0.22074391717914346
	data : 0.11746797561645508
	model : 0.06883778572082519
			 train-loss:  2.0144521594047546 	 ± 0.21728798382494877
	data : 0.11706595420837403
	model : 0.06880307197570801
			 train-loss:  2.020430908203125 	 ± 0.21490322401206344
	data : 0.11709709167480468
	model : 0.06958765983581543
			 train-loss:  2.0089779725441566 	 ± 0.21837205549553615
	data : 0.11638078689575196
	model : 0.06946516036987305
			 train-loss:  2.000309458485356 	 ± 0.21880108517727154
	data : 0.11652173995971679
	model : 0.06871438026428223
			 train-loss:  1.9887906525816237 	 ± 0.2230393916234207
	data : 0.11696834564208984
	model : 0.0688720703125
			 train-loss:  1.991024374961853 	 ± 0.21947864971701267
	data : 0.11700429916381835
	model : 0.06885237693786621
			 train-loss:  1.9817117174466452 	 ± 0.22154056935187794
	data : 0.11690740585327149
	model : 0.06871266365051269
			 train-loss:  1.9862629098276938 	 ± 0.219359046778129
	data : 0.11695742607116699
	model : 0.06896963119506835
			 train-loss:  1.9934959448873997 	 ± 0.2196281228656198
	data : 0.11680855751037597
	model : 0.06970834732055664
			 train-loss:  1.9966770988522153 	 ± 0.21702219138937734
	data : 0.11645879745483398
	model : 0.07071123123168946
			 train-loss:  2.002214210874894 	 ± 0.21615999815270062
	data : 0.11546730995178223
	model : 0.07023067474365234
			 train-loss:  2.010122077805655 	 ± 0.21798235911103153
	data : 0.11595840454101562
	model : 0.06939315795898438
			 train-loss:  2.022328770822949 	 ± 0.22674110527045382
	data : 0.11675353050231933
	model : 0.06841015815734863
			 train-loss:  2.0327422071147607 	 ± 0.2322194127003566
	data : 0.11761736869812012
	model : 0.06797356605529785
			 train-loss:  2.0235510154774317 	 ± 0.23586530119592525
	data : 0.11775331497192383
	model : 0.06719555854797363
			 train-loss:  2.0221170156429977 	 ± 0.23298950403351643
	data : 0.11841955184936523
	model : 0.06687517166137695
			 train-loss:  2.043498969078064 	 ± 0.26600251059024604
	data : 0.11863656044006347
	model : 0.06772904396057129
			 train-loss:  2.0414795235889716 	 ± 0.2630488041590317
	data : 0.11774835586547852
	model : 0.06856083869934082
			 train-loss:  2.043850200516837 	 ± 0.2603413252239495
	data : 0.11690926551818848
	model : 0.06906161308288575
			 train-loss:  2.0392848863158117 	 ± 0.25899179556607
	data : 0.1166409969329834
	model : 0.06830348968505859
			 train-loss:  2.035822957754135 	 ± 0.2570362453011181
	data : 0.11738076210021972
	model : 0.0681394100189209
			 train-loss:  2.039922663900587 	 ± 0.25561493656142753
	data : 0.117439603805542
	model : 0.06736030578613281
			 train-loss:  2.042140465715657 	 ± 0.25325860668764255
	data : 0.11830182075500488
	model : 0.06730947494506836
			 train-loss:  2.0402824548964804 	 ± 0.2508665861891904
	data : 0.11833381652832031
	model : 0.06703882217407227
			 train-loss:  2.046174523731073 	 ± 0.25150465697536545
	data : 0.11849417686462402
	model : 0.06767435073852539
			 train-loss:  2.03916465749546 	 ± 0.25361844809172485
	data : 0.11801257133483886
	model : 0.0686800479888916
			 train-loss:  2.0389885878562928 	 ± 0.2510724793631907
	data : 0.11723461151123046
	model : 0.06890740394592285
			 train-loss:  2.0395922076468374 	 ± 0.24863543690178475
	data : 0.11684937477111816
	model : 0.06903219223022461
			 train-loss:  2.044240165215272 	 ± 0.24846030142576045
	data : 0.11680302619934083
	model : 0.06931233406066895
			 train-loss:  2.03773865834722 	 ± 0.2505309872870639
	data : 0.1165421485900879
	model : 0.06895661354064941
			 train-loss:  2.0350952678256564 	 ± 0.24894534465669832
	data : 0.11678953170776367
	model : 0.06908020973205567
			 train-loss:  2.0403213067488237 	 ± 0.24964336284940083
	data : 0.11676969528198242
	model : 0.06874670982360839
			 train-loss:  2.053697999034609 	 ± 0.2665527968023988
	data : 0.11709518432617187
	model : 0.06872062683105469
			 train-loss:  2.065293834920515 	 ± 0.27808963717819013
	data : 0.1170536994934082
	model : 0.06891827583312989
			 train-loss:  2.0662124198058556 	 ± 0.27576910745246536
	data : 0.11688737869262696
	model : 0.06852889060974121
			 train-loss:  2.06648786997391 	 ± 0.27343014083055067
	data : 0.11721487045288086
	model : 0.06842775344848633
			 train-loss:  2.0683235923449197 	 ± 0.27150837421735874
	data : 0.11734952926635742
	model : 0.06913876533508301
			 train-loss:  2.0771049476060712 	 ± 0.2777319880696322
	data : 0.11678242683410645
	model : 0.06919522285461426
			 train-loss:  2.082448467131584 	 ± 0.27862643203300397
	data : 0.11657867431640626
	model : 0.06861796379089355
			 train-loss:  2.0807052131683106 	 ± 0.27674688374068984
	data : 0.1169090747833252
	model : 0.0694183349609375
			 train-loss:  2.0775919053703547 	 ± 0.2756860102630633
	data : 0.11624631881713868
	model : 0.06914286613464356
			 train-loss:  2.0771994535739604 	 ± 0.273575144982732
	data : 0.11634988784790039
	model : 0.0693368911743164
			 train-loss:  2.083012246724331 	 ± 0.2755097572203198
	data : 0.11627445220947266
	model : 0.06907758712768555
			 train-loss:  2.078519095235796 	 ± 0.2758716034814998
	data : 0.11679472923278808
	model : 0.06878695487976075
			 train-loss:  2.0773152849253487 	 ± 0.2740128500986366
	data : 0.11712570190429687
	model : 0.0685539722442627
			 train-loss:  2.0819092522496763 	 ± 0.27464520826468053
	data : 0.11726822853088378
	model : 0.06781935691833496
			 train-loss:  2.087248458181109 	 ± 0.27625967930462636
	data : 0.11795001029968262
	model : 0.06873927116394044
			 train-loss:  2.0813941720505835 	 ± 0.27864596850302004
	data : 0.11707262992858887
	model : 0.06820878982543946
			 train-loss:  2.0825703971915774 	 ± 0.2768816022465589
	data : 0.11760821342468261
	model : 0.06883358955383301
			 train-loss:  2.0851598439151293 	 ± 0.2758550609945116
	data : 0.11725358963012696
	model : 0.06903810501098633
			 train-loss:  2.0856465906710238 	 ± 0.2740163963874288
	data : 0.1170379638671875
	model : 0.07035374641418457
			 train-loss:  2.0885794099171955 	 ± 0.2733502479884921
	data : 0.11599364280700683
	model : 0.0698472499847412
			 train-loss:  2.0867565650688973 	 ± 0.27200441672827264
	data : 0.11646203994750977
	model : 0.07115068435668945
			 train-loss:  2.0817674847392293 	 ± 0.273710157757128
	data : 0.11534528732299805
	model : 0.07140159606933594
			 train-loss:  2.0770354530750175 	 ± 0.2751017382935156
	data : 0.11510405540466309
	model : 0.0717360496520996
			 train-loss:  2.0802478141422514 	 ± 0.2748233651972747
	data : 0.11478891372680664
	model : 0.07102794647216797
			 train-loss:  2.081293497979641 	 ± 0.2732584243037342
	data : 0.11518335342407227
	model : 0.07057280540466308
			 train-loss:  2.0776173788824197 	 ± 0.2735496676574279
	data : 0.11539764404296875
	model : 0.06984100341796876
			 train-loss:  2.0787208385583833 	 ± 0.27205788767941863
	data : 0.11595697402954101
	model : 0.06868505477905273
			 train-loss:  2.0853486477610574 	 ± 0.2769942696187988
	data : 0.11700496673583985
	model : 0.06891841888427734
			 train-loss:  2.0850496732053303 	 ± 0.2753540298206139
	data : 0.11688294410705566
	model : 0.0692723274230957
			 train-loss:  2.0860023484510535 	 ± 0.2738687298154947
	data : 0.11658611297607421
	model : 0.06911110877990723
			 train-loss:  2.0860265340915944 	 ± 0.2722719052938909
	data : 0.11678485870361328
	model : 0.06839714050292969
			 train-loss:  2.0889101398402246 	 ± 0.272020227612413
	data : 0.11732096672058105
	model : 0.06838107109069824
			 train-loss:  2.0912984785708515 	 ± 0.27138609764343574
	data : 0.11737060546875
	model : 0.0679295539855957
			 train-loss:  2.0921208603998247 	 ± 0.26996739932335606
	data : 0.11780829429626465
	model : 0.06774687767028809
			 train-loss:  2.0892511354552377 	 ± 0.2698250097407439
	data : 0.11814031600952149
	model : 0.06741118431091309
			 train-loss:  2.086714426239768 	 ± 0.26941532153104264
	data : 0.11852388381958008
	model : 0.06739668846130371
			 train-loss:  2.085169763668724 	 ± 0.26835196415844637
	data : 0.11875925064086915
	model : 0.06787986755371093
			 train-loss:  2.086540114495062 	 ± 0.2672287577776988
	data : 0.11823334693908691
	model : 0.0677159309387207
			 train-loss:  2.08258175342641 	 ± 0.2685306260919897
	data : 0.11853218078613281
	model : 0.06793212890625
			 train-loss:  2.084726945977462 	 ± 0.2679220637305124
	data : 0.11821370124816895
	model : 0.06850013732910157
			 train-loss:  2.085933501521746 	 ± 0.26678230712434253
	data : 0.11779546737670898
	model : 0.06931710243225098
			 train-loss:  2.088461018100227 	 ± 0.2665564477552071
	data : 0.11683788299560546
	model : 0.06971955299377441
			 train-loss:  2.087848534389418 	 ± 0.26526157690979296
	data : 0.11639223098754883
	model : 0.06995501518249511
			 train-loss:  2.0869075926867398 	 ± 0.2640828015683043
	data : 0.11607789993286133
	model : 0.0700192928314209
			 train-loss:  2.086564692258835 	 ± 0.26278121948298555
	data : 0.11605820655822754
	model : 0.06970529556274414
			 train-loss:  2.0833314928678 	 ± 0.26346845062875873
	data : 0.11635241508483887
	model : 0.07060251235961915
			 train-loss:  2.0830821313110053 	 ± 0.26218573462051353
	data : 0.11551141738891602
	model : 0.06976776123046875
			 train-loss:  2.079804076731784 	 ± 0.26300194498401314
	data : 0.11630229949951172
	model : 0.06980090141296387
			 train-loss:  2.078568643102279 	 ± 0.2620346068819825
	data : 0.11625556945800782
	model : 0.0698115348815918
			 train-loss:  2.07622169199444 	 ± 0.2618798594296346
	data : 0.11623787879943848
	model : 0.07036991119384765
			 train-loss:  2.073113457211908 	 ± 0.2625804402447946
	data : 0.11569776535034179
	model : 0.06958098411560058
			 train-loss:  2.072319725963557 	 ± 0.26147827863056167
	data : 0.11658267974853516
	model : 0.07124075889587403
			 train-loss:  2.070831192864312 	 ± 0.26071998310785804
	data : 0.11508302688598633
	model : 0.07126269340515137
			 train-loss:  2.0720843700093963 	 ± 0.25984783140779905
	data : 0.11522946357727051
	model : 0.07130451202392578
			 train-loss:  2.07096871137619 	 ± 0.2589261300660501
	data : 0.11522932052612304
	model : 0.07121829986572266
			 train-loss:  2.072301236358849 	 ± 0.25813576018459966
	data : 0.11518044471740722
	model : 0.07108011245727539
			 train-loss:  2.0697901344725063 	 ± 0.2583390192252474
	data : 0.11517720222473145
	model : 0.07017979621887208
			 train-loss:  2.06745778986838 	 ± 0.2583751127990534
	data : 0.11609916687011719
	model : 0.06921024322509765
			 train-loss:  2.067913622186895 	 ± 0.25728502590394864
	data : 0.11694154739379883
	model : 0.06919288635253906
			 train-loss:  2.070323023588761 	 ± 0.257452452501054
	data : 0.11698594093322753
	model : 0.06814360618591309
			 train-loss:  2.0728521943092346 	 ± 0.25777120046991103
	data : 0.11802134513854981
	model : 0.06832141876220703
			 train-loss:  2.071426800173572 	 ± 0.25712596164372586
	data : 0.11790213584899903
	model : 0.06840667724609376
			 train-loss:  2.0724689950377253 	 ± 0.2562821803286642
	data : 0.11769514083862305
	model : 0.06917619705200195
			 train-loss:  2.0726020746872207 	 ± 0.25520718703008577
	data : 0.11699719429016113
	model : 0.06905698776245117
			 train-loss:  2.0713192145029704 	 ± 0.2545266082539084
	data : 0.11697707176208497
	model : 0.06999311447143555
			 train-loss:  2.070507708659842 	 ± 0.25362850009116933
	data : 0.11625051498413086
	model : 0.06938295364379883
			 train-loss:  2.0739819407463074 	 ± 0.25546163523285287
	data : 0.11679272651672364
	model : 0.06923589706420899
			 train-loss:  2.0728106382416516 	 ± 0.2547497802852385
	data : 0.11695008277893067
	model : 0.06895232200622559
			 train-loss:  2.0715377705712474 	 ± 0.2541129033475528
	data : 0.11709518432617187
	model : 0.06861443519592285
			 train-loss:  2.0713521432876587 	 ± 0.2531028514954031
	data : 0.11726617813110352
	model : 0.0686110496520996
			 train-loss:  2.070049064499991 	 ± 0.25251709678149303
	data : 0.11716070175170898
	model : 0.06963186264038086
			 train-loss:  2.071860782743439 	 ± 0.25234177450840495
	data : 0.11612162590026856
	model : 0.06984419822692871
			 train-loss:  2.0734584238380194 	 ± 0.2519981380087319
	data : 0.11599106788635254
	model : 0.07033920288085938
			 train-loss:  2.0693963054538695 	 ± 0.25519187468057536
	data : 0.11549229621887207
	model : 0.07068514823913574
			 train-loss:  2.0717570836727437 	 ± 0.25561866108130094
	data : 0.1154752254486084
	model : 0.0697434425354004
			 train-loss:  2.072043426164234 	 ± 0.25466207680089886
	data : 0.11619582176208496
	model : 0.06833066940307617
			 train-loss:  2.0718963940938315 	 ± 0.25370119527851276
	data : 0.11750292778015137
	model : 0.0675544261932373
			 train-loss:  2.0739505380616152 	 ± 0.25384508935379546
	data : 0.11816911697387696
	model : 0.06736874580383301
			 train-loss:  2.0727264836652957 	 ± 0.25328981210373586
	data : 0.11835951805114746
	model : 0.06815013885498047
			 train-loss:  2.071718698960763 	 ± 0.2526194682034254
	data : 0.11753678321838379
	model : 0.06809930801391602
			 train-loss:  2.072247604236883 	 ± 0.25176401857255765
	data : 0.11751279830932618
	model : 0.06902413368225098
			 train-loss:  2.074734991484315 	 ± 0.25251514983067275
	data : 0.11664586067199707
	model : 0.06945271492004394
			 train-loss:  2.0746961063232976 	 ± 0.25159898805994696
	data : 0.11633539199829102
	model : 0.06965789794921876
			 train-loss:  2.0786718112959277 	 ± 0.2550056864061821
	data : 0.11608562469482422
	model : 0.06890106201171875
			 train-loss:  2.0778847285679407 	 ± 0.25426270959473024
	data : 0.11674456596374512
	model : 0.06906204223632813
			 train-loss:  2.0753452651044157 	 ± 0.2551349852000571
	data : 0.11673240661621094
	model : 0.06893610954284668
			 train-loss:  2.0787664626685665 	 ± 0.2574602897234666
	data : 0.11697721481323242
	model : 0.06823577880859374
			 train-loss:  2.078986243768172 	 ± 0.25657186682482536
	data : 0.11747145652770996
	model : 0.068107271194458
			 train-loss:  2.0798932106958494 	 ± 0.25590937143386405
	data : 0.117549467086792
	model : 0.06737051010131836
			 train-loss:  2.0765425049025437 	 ± 0.258175666845161
	data : 0.1181309700012207
	model : 0.06824307441711426
			 train-loss:  2.0751482231976235 	 ± 0.25783719513474873
	data : 0.11764001846313477
	model : 0.06840543746948242
			 train-loss:  2.0797227304808947 	 ± 0.26283642675927527
	data : 0.1174461841583252
	model : 0.0693115234375
			 train-loss:  2.082765836973448 	 ± 0.2645326130521677
	data : 0.1166569709777832
	model : 0.06915011405944824
			 train-loss:  2.0816120549336374 	 ± 0.2640168085897437
	data : 0.11678662300109863
	model : 0.06974444389343262
			 train-loss:  2.0817506527900695 	 ± 0.2631407194740093
	data : 0.11641817092895508
	model : 0.0697235107421875
			 train-loss:  2.0826802672139855 	 ± 0.2625149567637714
	data : 0.11634793281555175
	model : 0.06908540725708008
			 train-loss:  2.08080534950683 	 ± 0.26266239367091054
	data : 0.11677627563476563
	model : 0.0692103385925293
			 train-loss:  2.080597236265544 	 ± 0.26181518536316256
	data : 0.11679091453552246
	model : 0.06940345764160157
			 train-loss:  2.083689811167779 	 ± 0.2637524874621712
	data : 0.11688122749328614
	model : 0.06959490776062012
			 train-loss:  2.0816974478383217 	 ± 0.264060354021297
	data : 0.11670260429382324
	model : 0.06946673393249511
			 train-loss:  2.0815828358515716 	 ± 0.26321651373393723
	data : 0.11683554649353027
	model : 0.06995096206665039
			 train-loss:  2.0803012316394005 	 ± 0.26286474087324047
	data : 0.11655631065368652
	model : 0.06971135139465331
			 train-loss:  2.0801323169394386 	 ± 0.2620401176198115
	data : 0.11677384376525879
	model : 0.06874113082885742
			 train-loss:  2.07862278125571 	 ± 0.2619030393695848
	data : 0.11732597351074218
	model : 0.06865229606628417
			 train-loss:  2.079272549599409 	 ± 0.26121183767874623
	data : 0.11751189231872558
	model : 0.06872425079345704
			 train-loss:  2.0796013952042003 	 ± 0.2604325777334136
	data : 0.1172882080078125
	model : 0.06878705024719238
			 train-loss:  2.0793156454592574 	 ± 0.25965284523310095
	data : 0.11712894439697266
	model : 0.0690495491027832
			 train-loss:  2.0800871841746607 	 ± 0.2590413421092796
	data : 0.1167452335357666
	model : 0.06985278129577636
			 train-loss:  2.0801528597750316 	 ± 0.2582517355379368
	data : 0.11617536544799804
	model : 0.06980276107788086
			 train-loss:  2.0815665497924343 	 ± 0.25810368127769995
	data : 0.11585931777954102
	model : 0.0691032886505127
			 train-loss:  2.0801196960081536 	 ± 0.2579953655692549
	data : 0.11662096977233886
	model : 0.06831350326538085
			 train-loss:  2.0789216828203485 	 ± 0.2576844684058193
	data : 0.11736507415771484
	model : 0.06912612915039062
			 train-loss:  2.0804890202624455 	 ± 0.25771356736461215
	data : 0.11648073196411132
	model : 0.06867418289184571
			 train-loss:  2.0818112806455624 	 ± 0.25752089940633294
	data : 0.11686491966247559
	model : 0.06875829696655274
			 train-loss:  2.0824333366225747 	 ± 0.2568896819955538
	data : 0.11693758964538574
	model : 0.06945862770080566
			 train-loss:  2.08178909061945 	 ± 0.25627514077042
	data : 0.11618132591247558
	model : 0.06947798728942871
			 train-loss:  2.0805579936781595 	 ± 0.25603568618915123
	data : 0.11645755767822266
	model : 0.06879544258117676
			 train-loss:  2.082707564954813 	 ± 0.2568464504698266
	data : 0.11730737686157226
	model : 0.06935515403747558
			 train-loss:  2.0810003198426346 	 ± 0.25708987003439926
	data : 0.11678261756896972
	model : 0.07006287574768066
			 train-loss:  2.082634905406407 	 ± 0.2572594406116921
	data : 0.11607613563537597
	model : 0.06974997520446777
			 train-loss:  2.08290873196992 	 ± 0.25655312311585904
	data : 0.11638436317443848
	model : 0.07004575729370117
			 train-loss:  2.0845382590751864 	 ± 0.2567391409226847
	data : 0.1159273624420166
	model : 0.07004423141479492
			 train-loss:  2.08634474706114 	 ± 0.25714256583494777
	data : 0.11582460403442382
	model : 0.06995649337768554
			 train-loss:  2.0854939142418973 	 ± 0.2566744201890578
	data : 0.11567997932434082
	model : 0.06922321319580078
			 train-loss:  2.0869760771592456 	 ± 0.25672743649708457
	data : 0.11658310890197754
	model : 0.06858668327331544
			 train-loss:  2.0861457191119537 	 ± 0.25625953185282235
	data : 0.11716108322143555
	model : 0.06924223899841309
			 train-loss:  2.087424098135351 	 ± 0.25613263956694815
	data : 0.11670498847961426
	model : 0.06809587478637695
			 train-loss:  2.0886813376119227 	 ± 0.2559943673919684
	data : 0.11782293319702149
	model : 0.06802668571472167
			 train-loss:  2.0907077355229338 	 ± 0.2567652793718387
	data : 0.11814508438110352
	model : 0.06802487373352051
			 train-loss:  2.0926950035868463 	 ± 0.25748533557473674
	data : 0.11803112030029297
	model : 0.06880578994750977
			 train-loss:  2.0968095980664736 	 ± 0.26281986731089546
	data : 0.11735444068908692
	model : 0.06794934272766114
			 train-loss:  2.0984327506254066 	 ± 0.26304931394960396
	data : 0.11800546646118164
	model : 0.06823554039001464
			 train-loss:  2.0957328207949373 	 ± 0.2649340342674919
	data : 0.11780710220336914
	model : 0.06806302070617676
			 train-loss:  2.0954248223985945 	 ± 0.264265966165186
	data : 0.11822466850280762
	model : 0.06812629699707032
			 train-loss:  2.0946676128789 	 ± 0.26377510603699306
	data : 0.11828064918518066
	model : 0.06848258972167968
			 train-loss:  2.0941044610208244 	 ± 0.263198184028239
	data : 0.11791167259216309
	model : 0.06927657127380371
			 train-loss:  2.0947067302962146 	 ± 0.26264380217834304
	data : 0.11730637550354003
	model : 0.06982302665710449
			 train-loss:  2.0952174972376056 	 ± 0.26205808106693884
	data : 0.11662683486938477
	model : 0.07016348838806152
			 train-loss:  2.0952445452975246 	 ± 0.26138207112026507
	data : 0.11603255271911621
	model : 0.0691296100616455
			 train-loss:  2.0942223237111017 	 ± 0.2610994884914451
	data : 0.11687641143798828
	model : 0.06897158622741699
			 train-loss:  2.0929481983184814 	 ± 0.26103962128179176
	data : 0.11708211898803711
	model : 0.06903162002563476
			 train-loss:  2.0924708958204628 	 ± 0.260461972791968
	data : 0.11692562103271484
	model : 0.06842145919799805
			 train-loss:  2.0921137712218543 	 ± 0.2598517572989804
	data : 0.11738743782043456
	model : 0.06766262054443359
			 train-loss:  2.0947574305174936 	 ± 0.2618538400743388
	data : 0.1181227684020996
	model : 0.06855311393737792
			 train-loss:  2.093005582690239 	 ± 0.26236486347266597
	data : 0.11732678413391114
	model : 0.06851463317871094
			 train-loss:  2.093221819222863 	 ± 0.26172926650672107
	data : 0.11726980209350586
	model : 0.06841073036193848
			 train-loss:  2.093617307667685 	 ± 0.2611408197371405
	data : 0.11746888160705567
	model : 0.06905708312988282
			 train-loss:  2.0941951950195388 	 ± 0.2606262702888994
	data : 0.11703648567199706
	model : 0.0690009593963623
			 train-loss:  2.0934085664795896 	 ± 0.26022815981974123
	data : 0.11698942184448242
	model : 0.06884379386901855
			 train-loss:  2.094622259023713 	 ± 0.2601708326878912
	data : 0.11718668937683105
	model : 0.06841645240783692
			 train-loss:  2.0961825830265157 	 ± 0.26049831232774767
	data : 0.1175919532775879
	model : 0.06841101646423339
			 train-loss:  2.0953701365973063 	 ± 0.26012981746406966
	data : 0.11760091781616211
	model : 0.0684586524963379
			 train-loss:  2.095933686655301 	 ± 0.25963038759639895
	data : 0.11763873100280761
	model : 0.06992411613464355
			 train-loss:  2.0967078887674795 	 ± 0.2592490791438223
	data : 0.11620635986328125
	model : 0.07002711296081543
			 train-loss:  2.094910903204055 	 ± 0.25993255057803405
	data : 0.11606535911560059
	model : 0.07052440643310547
			 train-loss:  2.0934864892778804 	 ± 0.2601361152637827
	data : 0.11564068794250489
	model : 0.0696136474609375
			 train-loss:  2.094368859844388 	 ± 0.2598381736856482
	data : 0.11648068428039551
	model : 0.06959333419799804
			 train-loss:  2.095509001346821 	 ± 0.2597585112351943
	data : 0.1163747787475586
	model : 0.06894965171813965
			 train-loss:  2.0935022558007286 	 ± 0.26080057463233947
	data : 0.11705956459045411
	model : 0.06862883567810059
			 train-loss:  2.091587531289389 	 ± 0.26169666073448716
	data : 0.11730356216430664
	model : 0.06864838600158692
			 train-loss:  2.0912439536165306 	 ± 0.2611387776711765
	data : 0.11732125282287598
	model : 0.06971025466918945
			 train-loss:  2.08991215086203 	 ± 0.26127059634528754
	data : 0.1163595199584961
	model : 0.06972079277038574
			 train-loss:  2.0888070387577793 	 ± 0.2611785036621866
	data : 0.11652297973632812
	model : 0.06964435577392578
			 train-loss:  2.08829756033475 	 ± 0.2606900769921443
	data : 0.11661744117736816
	model : 0.06938281059265136
			 train-loss:  2.0873303440484134 	 ± 0.26049047270045916
	data : 0.11686496734619141
	model : 0.06929998397827149
			 train-loss:  2.0879755014747516 	 ± 0.26007656383434324
	data : 0.11683955192565917
	model : 0.06910967826843262
			 train-loss:  2.0886771909825437 	 ± 0.25969972766793475
	data : 0.11692919731140136
	model : 0.06891107559204102
			 train-loss:  2.087741781777865 	 ± 0.25949134317744355
	data : 0.1170729160308838
	model : 0.06896762847900391
			 train-loss:  2.086892232298851 	 ± 0.2592221014457156
	data : 0.11707820892333984
	model : 0.0690910816192627
			 train-loss:  2.086676367653741 	 ± 0.25866558797820954
	data : 0.11700096130371093
	model : 0.06839146614074706
			 train-loss:  2.0872352882824114 	 ± 0.25822881727251884
	data : 0.11767048835754394
	model : 0.06780691146850586
			 train-loss:  2.086943808106074 	 ± 0.2576966614935256
	data : 0.11822695732116699
	model : 0.06749415397644043
			 train-loss:  2.087630574117627 	 ± 0.25733902166354783
	data : 0.11838865280151367
	model : 0.06679167747497558
			 train-loss:  2.0865701030956085 	 ± 0.25727533026635624
	data : 0.118870210647583
	model : 0.06653814315795899
			 train-loss:  2.0875237221303196 	 ± 0.25712071193676694
	data : 0.11880645751953126
	model : 0.06653318405151368
			 train-loss:  2.0868245647067116 	 ± 0.25678258175629304
	data : 0.11861629486083984
	model : 0.06628851890563965
			 train-loss:  2.0876415361618172 	 ± 0.25652925954523853
	data : 0.118560791015625
	model : 0.0660707950592041
			 train-loss:  2.0867407910301963 	 ± 0.25634558256780243
	data : 0.1188581943511963
	model : 0.06599984169006348
			 train-loss:  2.087912435715015 	 ± 0.25642169119329705
	data : 0.11900105476379394
	model : 0.06650118827819824
			 train-loss:  2.0874270981930674 	 ± 0.25598321605313773
	data : 0.11858072280883789
	model : 0.06692705154418946
			 train-loss:  2.0866174687773493 	 ± 0.25574164863769633
	data : 0.11826920509338379
	model : 0.06745333671569824
			 train-loss:  2.0867514731008794 	 ± 0.2552098418461922
	data : 0.11783390045166016
	model : 0.06784873008728028
			 train-loss:  2.0857245125690427 	 ± 0.2551633806087313
	data : 0.1172377586364746
	model : 0.06835393905639649
			 train-loss:  2.08631620696399 	 ± 0.25479257301382524
	data : 0.11673617362976074
	model : 0.06834821701049805
			 train-loss:  2.0876726443568865 	 ± 0.25512447906831465
	data : 0.11720438003540039
	model : 0.06790337562561036
			 train-loss:  2.0890229099519027 	 ± 0.2554525301883717
	data : 0.11760969161987304
	model : 0.06799073219299316
			 train-loss:  2.0876055389396417 	 ± 0.25587203244653306
	data : 0.11762027740478516
	model : 0.06824917793273926
			 train-loss:  2.087615855436757 	 ± 0.2553450544476528
	data : 0.11766400337219238
	model : 0.06843113899230957
			 train-loss:  2.0890105889468895 	 ± 0.2557471068087263
	data : 0.11758961677551269
	model : 0.06875510215759277
			 train-loss:  2.0897257381555985 	 ± 0.25546899609333457
	data : 0.11709952354431152
	model : 0.06933765411376953
			 train-loss:  2.088401612712116 	 ± 0.2557902773760239
	data : 0.11663403511047363
	model : 0.06945309638977051
			 train-loss:  2.08967729184309 	 ± 0.25605488180819247
	data : 0.11651377677917481
	model : 0.06901421546936035
			 train-loss:  2.0909207477684943 	 ± 0.25628429180981505
	data : 0.11683464050292969
	model : 0.06884064674377441
			 train-loss:  2.091381945763247 	 ± 0.25587224749713733
	data : 0.11686062812805176
	model : 0.06860604286193847
			 train-loss:  2.091975263118744 	 ± 0.2555315615666261
	data : 0.11704087257385254
	model : 0.06839537620544434
			 train-loss:  2.0912321445001547 	 ± 0.255292558334393
	data : 0.11728978157043457
	model : 0.06822943687438965
			 train-loss:  2.092040798493794 	 ± 0.25510742138613796
	data : 0.11733446121215821
	model : 0.06852517127990723
			 train-loss:  2.0908414087747866 	 ± 0.25531367970630725
	data : 0.1168287754058838
	model : 0.06803302764892578
			 train-loss:  2.090137153159915 	 ± 0.25505670509150813
	data : 0.11755728721618652
	model : 0.06802706718444824
			 train-loss:  2.090598679523842 	 ± 0.25466235108832275
	data : 0.11758589744567871
	model : 0.0675821304321289
			 train-loss:  2.0898337946273386 	 ± 0.25445779383020206
	data : 0.11682748794555664
	model : 0.05881896018981934
#epoch  46    val-loss:  2.4524104030508744  train-loss:  2.0898337946273386  lr:  0.0003125
			 train-loss:  2.1299970149993896 	 ± 0.0
	data : 5.65331768989563
	model : 0.07646393775939941
			 train-loss:  1.961997389793396 	 ± 0.16799962520599365
	data : 2.8928974866867065
	model : 0.07480955123901367
			 train-loss:  2.030102491378784 	 ± 0.1676082509011663
	data : 1.9675041039784749
	model : 0.07320912679036458
			 train-loss:  2.0817298889160156 	 ± 0.17048624242739716
	data : 1.504417896270752
	model : 0.07131987810134888
			 train-loss:  2.0542482137680054 	 ± 0.16209076766236144
	data : 1.2274726390838624
	model : 0.07098550796508789
			 train-loss:  2.0723297794659934 	 ± 0.1533924027614513
	data : 0.1200345516204834
	model : 0.06964402198791504
			 train-loss:  2.0619423730032787 	 ± 0.1442750846208614
	data : 0.11677789688110352
	model : 0.06899986267089844
			 train-loss:  2.0496766418218613 	 ± 0.13880390724675462
	data : 0.1166910171508789
	model : 0.06804347038269043
			 train-loss:  2.053450597657098 	 ± 0.13130019666832182
	data : 0.11763315200805664
	model : 0.06874022483825684
			 train-loss:  2.045590364933014 	 ± 0.1267746694214456
	data : 0.11711239814758301
	model : 0.0678591251373291
			 train-loss:  2.075292879884893 	 ± 0.1530788571175285
	data : 0.11791458129882812
	model : 0.06784076690673828
			 train-loss:  2.1136976778507233 	 ± 0.194176694524514
	data : 0.11804728507995606
	model : 0.06791343688964843
			 train-loss:  2.0974127054214478 	 ± 0.1949016152936715
	data : 0.11801486015319824
	model : 0.06873135566711426
			 train-loss:  2.0747582146099637 	 ± 0.20480538479735508
	data : 0.11725859642028809
	model : 0.06808686256408691
			 train-loss:  2.0669473012288413 	 ± 0.2000076025614778
	data : 0.1178314208984375
	model : 0.06905326843261719
			 train-loss:  2.07232329249382 	 ± 0.19477261164228826
	data : 0.11694979667663574
	model : 0.06910309791564942
			 train-loss:  2.05388044609743 	 ± 0.20284732284709206
	data : 0.1167215347290039
	model : 0.06909713745117188
			 train-loss:  2.0934958259264627 	 ± 0.25600883059634033
	data : 0.11687746047973632
	model : 0.06934943199157714
			 train-loss:  2.0829824585663643 	 ± 0.2531414156127848
	data : 0.1166612148284912
	model : 0.07003068923950195
			 train-loss:  2.084764856100082 	 ± 0.2468540243988708
	data : 0.11605916023254395
	model : 0.06982145309448243
			 train-loss:  2.0827505872363137 	 ± 0.24107321990293012
	data : 0.11637511253356933
	model : 0.06949987411499023
			 train-loss:  2.090102970600128 	 ± 0.2379282577549817
	data : 0.11678786277770996
	model : 0.06916203498840331
			 train-loss:  2.08771929015284 	 ± 0.23296686518223844
	data : 0.116961669921875
	model : 0.0691309928894043
			 train-loss:  2.0826936264832816 	 ± 0.22933181140104963
	data : 0.11707625389099122
	model : 0.06966252326965332
			 train-loss:  2.079092903137207 	 ± 0.22538971036501712
	data : 0.11656384468078614
	model : 0.07002506256103516
			 train-loss:  2.0840629247518687 	 ± 0.22240544776750631
	data : 0.11599063873291016
	model : 0.06987390518188477
			 train-loss:  2.066881360831084 	 ± 0.23517554298750423
	data : 0.11610217094421386
	model : 0.06988062858581542
			 train-loss:  2.068896715130125 	 ± 0.23117511004773617
	data : 0.11609554290771484
	model : 0.06971092224121093
			 train-loss:  2.0580396282261817 	 ± 0.23430672992973584
	data : 0.11621932983398438
	model : 0.06928443908691406
			 train-loss:  2.06534229516983 	 ± 0.23370108035171752
	data : 0.11656036376953124
	model : 0.06889276504516602
			 train-loss:  2.0639631325198757 	 ± 0.23002487888366413
	data : 0.1170799732208252
	model : 0.06937775611877442
			 train-loss:  2.07953729853034 	 ± 0.24244000558223852
	data : 0.11668953895568848
	model : 0.06970853805541992
			 train-loss:  2.087653813940106 	 ± 0.24311339057059447
	data : 0.11642947196960449
	model : 0.06969895362854003
			 train-loss:  2.0772336616235623 	 ± 0.24687829012733456
	data : 0.11651620864868165
	model : 0.06888446807861329
			 train-loss:  2.078829973084586 	 ± 0.243503865317786
	data : 0.11728224754333497
	model : 0.06912789344787598
			 train-loss:  2.0785478121704526 	 ± 0.24010385192252579
	data : 0.11709094047546387
	model : 0.06844587326049804
			 train-loss:  2.0713034997115263 	 ± 0.24079251994132364
	data : 0.11773686408996582
	model : 0.06844873428344726
			 train-loss:  2.0741792446688603 	 ± 0.23824610702880508
	data : 0.11775727272033691
	model : 0.06796231269836425
			 train-loss:  2.077492179014744 	 ± 0.23605690464024603
	data : 0.11805791854858398
	model : 0.06790456771850586
			 train-loss:  2.0810619086027144 	 ± 0.23415116159554691
	data : 0.11798334121704102
	model : 0.06738643646240235
			 train-loss:  2.081862278100921 	 ± 0.23133342154376507
	data : 0.11843829154968262
	model : 0.06824088096618652
			 train-loss:  2.081236177966708 	 ± 0.22859801769866991
	data : 0.11771340370178222
	model : 0.06822881698608399
			 train-loss:  2.084009971729545 	 ± 0.22663829886954864
	data : 0.11775946617126465
	model : 0.06792421340942383
			 train-loss:  2.0905697481198744 	 ± 0.22813998477537542
	data : 0.1180999755859375
	model : 0.06782536506652832
			 train-loss:  2.0918157233132257 	 ± 0.2257422018353788
	data : 0.11807889938354492
	model : 0.06740851402282715
			 train-loss:  2.0859573172486345 	 ± 0.22670722758349382
	data : 0.11824445724487305
	model : 0.06712536811828614
			 train-loss:  2.081965504808629 	 ± 0.22591064966325144
	data : 0.11837191581726074
	model : 0.06706976890563965
			 train-loss:  2.0754679491122565 	 ± 0.22793998404238972
	data : 0.1181410312652588
	model : 0.06717286109924317
			 train-loss:  2.0784335866266366 	 ± 0.2265357744154436
	data : 0.11804170608520508
	model : 0.06814579963684082
			 train-loss:  2.082167329788208 	 ± 0.22577685667498584
	data : 0.11756997108459473
	model : 0.06906046867370605
			 train-loss:  2.074733722443674 	 ± 0.22964886546132068
	data : 0.1168372631072998
	model : 0.06930365562438964
			 train-loss:  2.073519915342331 	 ± 0.22759511692057485
	data : 0.11667828559875489
	model : 0.06933484077453614
			 train-loss:  2.0712351529103405 	 ± 0.2260390115312022
	data : 0.1167717456817627
	model : 0.06930041313171387
			 train-loss:  2.072829665961089 	 ± 0.22423694459669283
	data : 0.11678595542907715
	model : 0.06913118362426758
			 train-loss:  2.0658794099634346 	 ± 0.22798357950985965
	data : 0.11685042381286621
	model : 0.06953468322753906
			 train-loss:  2.061451956629753 	 ± 0.2283122628834011
	data : 0.11658921241760253
	model : 0.06946048736572266
			 train-loss:  2.068882379615516 	 ± 0.2330317917603846
	data : 0.11648421287536621
	model : 0.06949701309204101
			 train-loss:  2.0668884782955566 	 ± 0.23150411422456862
	data : 0.11638646125793457
	model : 0.07042641639709472
			 train-loss:  2.0644960605492026 	 ± 0.23025583995006782
	data : 0.11550893783569335
	model : 0.07078866958618164
			 train-loss:  2.063107661406199 	 ± 0.2285778950654751
	data : 0.11514158248901367
	model : 0.07030396461486817
			 train-loss:  2.077994858632322 	 ± 0.2543404222473611
	data : 0.11566200256347656
	model : 0.06971325874328613
			 train-loss:  2.0787382471945977 	 ± 0.2523477537612027
	data : 0.11633753776550293
	model : 0.06889982223510742
			 train-loss:  2.0775194622221447 	 ± 0.25052086132861556
	data : 0.11706414222717285
	model : 0.06855993270874024
			 train-loss:  2.085005749017 	 ± 0.25555991246889165
	data : 0.11745147705078125
	model : 0.06834754943847657
			 train-loss:  2.0859174434955303 	 ± 0.2536913128584896
	data : 0.11750884056091308
	model : 0.06859159469604492
			 train-loss:  2.08364988637693 	 ± 0.2524249583020004
	data : 0.11732816696166992
	model : 0.06905965805053711
			 train-loss:  2.081140158781365 	 ± 0.25136239900486507
	data : 0.11689438819885253
	model : 0.06901941299438477
			 train-loss:  2.0767353843240177 	 ± 0.25209884886286615
	data : 0.11700167655944824
	model : 0.06903162002563476
			 train-loss:  2.0782298005145527 	 ± 0.25056859864485187
	data : 0.1169576644897461
	model : 0.06935548782348633
			 train-loss:  2.079516523224967 	 ± 0.24900188686930172
	data : 0.1168203353881836
	model : 0.06937909126281738
			 train-loss:  2.086662806255717 	 ± 0.25436888211714437
	data : 0.11689090728759766
	model : 0.06865043640136718
			 train-loss:  2.089539948436949 	 ± 0.2537569749230258
	data : 0.11753740310668945
	model : 0.06848220825195313
			 train-loss:  2.0876020291080213 	 ± 0.25254882722900907
	data : 0.11749677658081055
	model : 0.06828489303588867
			 train-loss:  2.084776725317981 	 ± 0.25199546994673744
	data : 0.1176980972290039
	model : 0.06805787086486817
			 train-loss:  2.0854191700617473 	 ± 0.250370864366787
	data : 0.1179306983947754
	model : 0.06770405769348145
			 train-loss:  2.0829126254508368 	 ± 0.24966370788986117
	data : 0.11810922622680664
	model : 0.06864361763000489
			 train-loss:  2.0842367317769432 	 ± 0.24830567562315153
	data : 0.11736669540405273
	model : 0.06968779563903808
			 train-loss:  2.0845470902247305 	 ± 0.24672386906616892
	data : 0.11660771369934082
	model : 0.06960206031799317
			 train-loss:  2.0874967288367356 	 ± 0.24653753501369127
	data : 0.11665186882019044
	model : 0.06928749084472656
			 train-loss:  2.0847115024924276 	 ± 0.2462393918442524
	data : 0.11679902076721191
	model : 0.0685314655303955
			 train-loss:  2.0819491663096863 	 ± 0.2459587613571191
	data : 0.11736469268798828
	model : 0.06843376159667969
			 train-loss:  2.079776118441326 	 ± 0.2452355047694752
	data : 0.1173820972442627
	model : 0.06750359535217285
			 train-loss:  2.080308641295835 	 ± 0.24380139999453915
	data : 0.11828122138977051
	model : 0.06703653335571289
			 train-loss:  2.080036075342269 	 ± 0.24235857791497517
	data : 0.11864657402038574
	model : 0.06645431518554687
			 train-loss:  2.0861316400415757 	 ± 0.24732113761756394
	data : 0.11919970512390136
	model : 0.06640033721923828
			 train-loss:  2.0853876014088475 	 ± 0.2459746891627177
	data : 0.11929793357849121
	model : 0.06539912223815918
			 train-loss:  2.0847898817610466 	 ± 0.24461976564954382
	data : 0.12014179229736328
	model : 0.06634268760681153
			 train-loss:  2.087182391773571 	 ± 0.24424749764674367
	data : 0.11934542655944824
	model : 0.06739611625671386
			 train-loss:  2.085805333062504 	 ± 0.2432147448521805
	data : 0.11863188743591309
	model : 0.06810531616210938
			 train-loss:  2.079967909389072 	 ± 0.24805014175899326
	data : 0.11787500381469726
	model : 0.06899991035461425
			 train-loss:  2.078558465936682 	 ± 0.24704558110420996
	data : 0.11703801155090332
	model : 0.06954245567321778
			 train-loss:  2.0765260315459706 	 ± 0.24646304979602074
	data : 0.11657581329345704
	model : 0.0693845272064209
			 train-loss:  2.0766534818116056 	 ± 0.24513744646109267
	data : 0.1165766716003418
	model : 0.06932053565979004
			 train-loss:  2.0740782643886324 	 ± 0.24509149192617877
	data : 0.11658859252929688
	model : 0.06960291862487793
			 train-loss:  2.0724503328925685 	 ± 0.24430849441914923
	data : 0.11651806831359864
	model : 0.06960115432739258
			 train-loss:  2.0699812956154346 	 ± 0.2442212897959294
	data : 0.11648235321044922
	model : 0.06947102546691894
			 train-loss:  2.07321131475193 	 ± 0.2450116765862482
	data : 0.11669135093688965
	model : 0.06930384635925294
			 train-loss:  2.0779665501750246 	 ± 0.24821674576949893
	data : 0.11676616668701172
	model : 0.06909718513488769
			 train-loss:  2.0751148257592713 	 ± 0.24856826529641673
	data : 0.11705784797668457
	model : 0.06886053085327148
			 train-loss:  2.0721952641010284 	 ± 0.2490224505435141
	data : 0.11721792221069335
	model : 0.06902050971984863
			 train-loss:  2.0778431573716722 	 ± 0.25414182797307633
	data : 0.11709489822387695
	model : 0.0692439079284668
			 train-loss:  2.0782968051293316 	 ± 0.25293405818618775
	data : 0.11673436164855958
	model : 0.06940770149230957
			 train-loss:  2.081240807922141 	 ± 0.2534532790137729
	data : 0.11672005653381348
	model : 0.06948432922363282
			 train-loss:  2.08055693255021 	 ± 0.2523272829990025
	data : 0.11641054153442383
	model : 0.06962051391601562
			 train-loss:  2.0788709754035586 	 ± 0.2517107481280141
	data : 0.11636505126953126
	model : 0.06937198638916016
			 train-loss:  2.0787316425791325 	 ± 0.25052468807843575
	data : 0.11665124893188476
	model : 0.06906623840332031
			 train-loss:  2.0761360943874467 	 ± 0.25077910811103915
	data : 0.11690092086791992
	model : 0.06914772987365722
			 train-loss:  2.0780745678477817 	 ± 0.2504194809713263
	data : 0.11676855087280273
	model : 0.06902165412902832
			 train-loss:  2.081639300792589 	 ± 0.25200592608124744
	data : 0.11694135665893554
	model : 0.06880593299865723
			 train-loss:  2.080374963717027 	 ± 0.2512048811126922
	data : 0.11716465950012207
	model : 0.06926484107971191
			 train-loss:  2.0773998520395778 	 ± 0.2520099811612957
	data : 0.1169426441192627
	model : 0.07017607688903808
			 train-loss:  2.0764815966997827 	 ± 0.2510688752061218
	data : 0.11618833541870117
	model : 0.0705146312713623
			 train-loss:  2.0790610830340763 	 ± 0.25144176875993074
	data : 0.1159731388092041
	model : 0.07067527770996093
			 train-loss:  2.0783202418109825 	 ± 0.25046036662079785
	data : 0.11578855514526368
	model : 0.07103681564331055
			 train-loss:  2.0776499457981275 	 ± 0.24947170857419917
	data : 0.11545987129211426
	model : 0.07089958190917969
			 train-loss:  2.082695019656214 	 ± 0.2542177876636304
	data : 0.11546669006347657
	model : 0.0696556568145752
			 train-loss:  2.0860969877650595 	 ± 0.2557671433968346
	data : 0.11660418510437012
	model : 0.0692373275756836
			 train-loss:  2.0826932646460454 	 ± 0.25732845829311124
	data : 0.11682243347167968
	model : 0.06933059692382812
			 train-loss:  2.081960298434025 	 ± 0.25636863393495396
	data : 0.11698341369628906
	model : 0.06857643127441407
			 train-loss:  2.082869816819827 	 ± 0.2554909172189748
	data : 0.11760282516479492
	model : 0.06847987174987794
			 train-loss:  2.0823032865839557 	 ± 0.25450865549368507
	data : 0.11775803565979004
	model : 0.0695457935333252
			 train-loss:  2.079941684105357 	 ± 0.25479119497837316
	data : 0.11671719551086426
	model : 0.06953816413879395
			 train-loss:  2.078323272185597 	 ± 0.2543822110441291
	data : 0.11683363914489746
	model : 0.06936774253845215
			 train-loss:  2.075091767695642 	 ± 0.2558767171485274
	data : 0.11690354347229004
	model : 0.06998176574707031
			 train-loss:  2.073608949661255 	 ± 0.255385504952303
	data : 0.11632037162780762
	model : 0.06993675231933594
			 train-loss:  2.0785725078885515 	 ± 0.2603531004981072
	data : 0.1163106918334961
	model : 0.06990141868591308
			 train-loss:  2.077740011252756 	 ± 0.2594943758708164
	data : 0.11637578010559083
	model : 0.06987967491149902
			 train-loss:  2.0761924739927053 	 ± 0.25906641406953346
	data : 0.11646513938903809
	model : 0.07040767669677735
			 train-loss:  2.0795801425164986 	 ± 0.26089097066443573
	data : 0.11592493057250977
	model : 0.06950321197509765
			 train-loss:  2.0810251290981587 	 ± 0.26040330030114994
	data : 0.11665267944335937
	model : 0.0700164794921875
			 train-loss:  2.079173768749674 	 ± 0.26026491448765976
	data : 0.11633353233337403
	model : 0.06901769638061524
			 train-loss:  2.0776022573312125 	 ± 0.25990033526927336
	data : 0.1172865390777588
	model : 0.06907572746276855
			 train-loss:  2.078456673407017 	 ± 0.2591074424441702
	data : 0.1171299934387207
	model : 0.06870098114013672
			 train-loss:  2.078568688969114 	 ± 0.2581420454800948
	data : 0.1174628734588623
	model : 0.06913237571716309
			 train-loss:  2.077951960210447 	 ± 0.25728325515588496
	data : 0.11703691482543946
	model : 0.06862773895263671
			 train-loss:  2.0761948403190162 	 ± 0.2571473436305712
	data : 0.11725239753723145
	model : 0.06962146759033203
			 train-loss:  2.0757350886825225 	 ± 0.25626322485756137
	data : 0.11649928092956544
	model : 0.0697317123413086
			 train-loss:  2.07481141625971 	 ± 0.25556183038794983
	data : 0.11651453971862794
	model : 0.06910295486450195
			 train-loss:  2.0773907899856567 	 ± 0.2564373549812569
	data : 0.11701908111572265
	model : 0.06952857971191406
			 train-loss:  2.0786082037857603 	 ± 0.2559226705516271
	data : 0.1166337013244629
	model : 0.06858239173889161
			 train-loss:  2.077483463794627 	 ± 0.25536053990275887
	data : 0.11738905906677247
	model : 0.06762170791625977
			 train-loss:  2.080622539553844 	 ± 0.25717537268503
	data : 0.11822504997253418
	model : 0.0665863037109375
			 train-loss:  2.078057827649417 	 ± 0.25809048939636203
	data : 0.11906647682189941
	model : 0.0662912368774414
			 train-loss:  2.078098732564184 	 ± 0.2571932457815395
	data : 0.11939568519592285
	model : 0.0654714584350586
			 train-loss:  2.0794056168917954 	 ± 0.256784178576646
	data : 0.1202301025390625
	model : 0.06652708053588867
			 train-loss:  2.0789677198619056 	 ± 0.2559575897472722
	data : 0.11932849884033203
	model : 0.06743569374084472
			 train-loss:  2.0777932510894983 	 ± 0.25547994314501876
	data : 0.11849122047424317
	model : 0.06836204528808594
			 train-loss:  2.077685781427332 	 ± 0.2546187063764348
	data : 0.11745848655700683
	model : 0.0692744255065918
			 train-loss:  2.0774881679739727 	 ± 0.25377423021943585
	data : 0.11653027534484864
	model : 0.06948394775390625
			 train-loss:  2.0745593372980755 	 ± 0.2554410829214069
	data : 0.11640448570251465
	model : 0.06929492950439453
			 train-loss:  2.0765846391387335 	 ± 0.2557993428415165
	data : 0.11666579246520996
	model : 0.06921377182006835
			 train-loss:  2.074648075982144 	 ± 0.25606466632883984
	data : 0.11668472290039063
	model : 0.06904234886169433
			 train-loss:  2.073386518004673 	 ± 0.25569995968397347
	data : 0.1169173240661621
	model : 0.06930837631225586
			 train-loss:  2.075220025979079 	 ± 0.2558754694596533
	data : 0.1167367935180664
	model : 0.06961221694946289
			 train-loss:  2.075601140914425 	 ± 0.25509257634020505
	data : 0.11655378341674805
	model : 0.06965603828430175
			 train-loss:  2.077066325224363 	 ± 0.254927129699983
	data : 0.11649479866027831
	model : 0.06946735382080078
			 train-loss:  2.0788720261519122 	 ± 0.25511282380971845
	data : 0.11684060096740723
	model : 0.06969084739685058
			 train-loss:  2.0789463504960266 	 ± 0.25430592849076195
	data : 0.11656718254089356
	model : 0.06983389854431152
			 train-loss:  2.0781468527871856 	 ± 0.2537040782080157
	data : 0.11636967658996582
	model : 0.06979489326477051
			 train-loss:  2.076586829125881 	 ± 0.2536738603832623
	data : 0.1161984920501709
	model : 0.06923294067382812
			 train-loss:  2.075283744320366 	 ± 0.2534214279564649
	data : 0.11658658981323242
	model : 0.06933155059814453
			 train-loss:  2.077044760003502 	 ± 0.2536242788639559
	data : 0.11656079292297364
	model : 0.06925110816955567
			 train-loss:  2.0777684552537883 	 ± 0.2530128184132744
	data : 0.11669692993164063
	model : 0.06880931854248047
			 train-loss:  2.0770006855813468 	 ± 0.2524306471180576
	data : 0.11717314720153808
	model : 0.06916770935058594
			 train-loss:  2.0763377680923 	 ± 0.25180769189193725
	data : 0.11681408882141113
	model : 0.06879959106445313
			 train-loss:  2.075309115001954 	 ± 0.25139557298460374
	data : 0.11725502014160157
	model : 0.0683664321899414
			 train-loss:  2.074160785018327 	 ± 0.2510780564790925
	data : 0.11737666130065919
	model : 0.06847310066223145
			 train-loss:  2.077259730015482 	 ± 0.25351277887398077
	data : 0.11754770278930664
	model : 0.06831116676330566
			 train-loss:  2.0778350554979763 	 ± 0.2528716050011058
	data : 0.11771578788757324
	model : 0.06737403869628907
			 train-loss:  2.0777232064920312 	 ± 0.252130960766843
	data : 0.11871304512023925
	model : 0.06781148910522461
			 train-loss:  2.0779114724599825 	 ± 0.2514046386981166
	data : 0.11826667785644532
	model : 0.06852216720581054
			 train-loss:  2.078587690758151 	 ± 0.25082866446207686
	data : 0.11782803535461425
	model : 0.06847820281982422
			 train-loss:  2.078095589069962 	 ± 0.25018593187179616
	data : 0.11784367561340332
	model : 0.06856856346130372
			 train-loss:  2.076920819693598 	 ± 0.24994404372329482
	data : 0.11764345169067383
	model : 0.06861538887023926
			 train-loss:  2.0775001328332086 	 ± 0.24934601869486878
	data : 0.11755390167236328
	model : 0.06818838119506836
			 train-loss:  2.0769370570778847 	 ± 0.24874819271218526
	data : 0.11792821884155273
	model : 0.06785130500793457
			 train-loss:  2.077269285412158 	 ± 0.24808367434341314
	data : 0.1181117057800293
	model : 0.06780695915222168
			 train-loss:  2.0774703153063743 	 ± 0.2474002855511497
	data : 0.11814165115356445
	model : 0.06796655654907227
			 train-loss:  2.076186640968536 	 ± 0.24730199194538177
	data : 0.1180506706237793
	model : 0.0682988166809082
			 train-loss:  2.076040634844038 	 ± 0.24662182163914068
	data : 0.11778182983398437
	model : 0.06918106079101563
			 train-loss:  2.0765616445910204 	 ± 0.2460389180263409
	data : 0.11694893836975098
	model : 0.06916837692260742
			 train-loss:  2.075926491847405 	 ± 0.24551080895587954
	data : 0.1169743537902832
	model : 0.06919126510620117
			 train-loss:  2.0765980390903076 	 ± 0.24500665350422268
	data : 0.1166923999786377
	model : 0.06814804077148437
			 train-loss:  2.076693624905918 	 ± 0.24434338894893465
	data : 0.11753926277160645
	model : 0.06858615875244141
			 train-loss:  2.0778253007579495 	 ± 0.24416514097666972
	data : 0.11712417602539063
	model : 0.06865200996398926
			 train-loss:  2.0760431783173674 	 ± 0.24471135915538772
	data : 0.11722192764282227
	model : 0.06893424987792969
			 train-loss:  2.0757948963399877 	 ± 0.24407966251586088
	data : 0.11702136993408203
	model : 0.06884212493896484
			 train-loss:  2.076571873527892 	 ± 0.24366141401426883
	data : 0.11721773147583008
	model : 0.0696488857269287
			 train-loss:  2.0782379921151217 	 ± 0.24408734467408225
	data : 0.11657843589782715
	model : 0.06882753372192382
			 train-loss:  2.0791963934898376 	 ± 0.2438004570492766
	data : 0.11723222732543945
	model : 0.06874289512634277
			 train-loss:  2.0776182999785657 	 ± 0.244132418380807
	data : 0.11719880104064942
	model : 0.06874427795410157
			 train-loss:  2.0798532931754985 	 ± 0.24544714274802937
	data : 0.11730260848999023
	model : 0.06853914260864258
			 train-loss:  2.0803472038377753 	 ± 0.2449060865274199
	data : 0.11750192642211914
	model : 0.06875443458557129
			 train-loss:  2.079766947584054 	 ± 0.24440704519941134
	data : 0.11741766929626465
	model : 0.06970996856689453
			 train-loss:  2.0808760221187885 	 ± 0.244268500636077
	data : 0.11661443710327149
	model : 0.06982645988464356
			 train-loss:  2.0815239603422127 	 ± 0.24381251421926053
	data : 0.11650328636169434
	model : 0.06887798309326172
			 train-loss:  2.080453240931942 	 ± 0.24365445901149083
	data : 0.1171255111694336
	model : 0.06916418075561523
			 train-loss:  2.0796783355751423 	 ± 0.24328163471234596
	data : 0.11691427230834961
	model : 0.06907091140747071
			 train-loss:  2.0783245120216254 	 ± 0.24341618455124756
	data : 0.11700496673583985
	model : 0.06807780265808105
			 train-loss:  2.0783151292800905 	 ± 0.2428069175829047
	data : 0.11796107292175292
	model : 0.06731762886047363
			 train-loss:  2.0807529361686896 	 ± 0.24464355757396475
	data : 0.11872739791870117
	model : 0.06820144653320312
			 train-loss:  2.080785088019796 	 ± 0.2440376786230939
	data : 0.11793427467346192
	model : 0.06866474151611328
			 train-loss:  2.0820529038095708 	 ± 0.2441018297852541
	data : 0.11750526428222656
	model : 0.06807503700256348
			 train-loss:  2.083023357625101 	 ± 0.24389505527387056
	data : 0.11796503067016602
	model : 0.06867976188659668
			 train-loss:  2.0828079153851764 	 ± 0.2433189202958739
	data : 0.11737728118896484
	model : 0.06933813095092774
			 train-loss:  2.082925661096295 	 ± 0.24273347642158855
	data : 0.11666669845581054
	model : 0.06936559677124024
			 train-loss:  2.0823041839876035 	 ± 0.24231068749870843
	data : 0.1165384292602539
	model : 0.06898736953735352
			 train-loss:  2.0808179980287185 	 ± 0.24267138099828747
	data : 0.11702151298522949
	model : 0.0689310073852539
			 train-loss:  2.0801608682249153 	 ± 0.24227556659304916
	data : 0.11696486473083496
	model : 0.06858558654785156
			 train-loss:  2.080678709915706 	 ± 0.24181394489154492
	data : 0.11744165420532227
	model : 0.06775550842285157
			 train-loss:  2.079544432355329 	 ± 0.24179958443027805
	data : 0.11832194328308106
	model : 0.0677835464477539
			 train-loss:  2.078659029501789 	 ± 0.24157123602294692
	data : 0.11846084594726562
	model : 0.067680025100708
			 train-loss:  2.078549134339525 	 ± 0.24100881196968602
	data : 0.11854796409606934
	model : 0.06832160949707031
			 train-loss:  2.0801233198041116 	 ± 0.24154015776032536
	data : 0.11803207397460938
	model : 0.06896882057189942
			 train-loss:  2.0812612389409266 	 ± 0.24155204685800102
	data : 0.11718153953552246
	model : 0.06984467506408691
			 train-loss:  2.0807593133714466 	 ± 0.24110460245325832
	data : 0.11631121635437011
	model : 0.06942286491394042
			 train-loss:  2.082996289301578 	 ± 0.24278471903407345
	data : 0.11647396087646485
	model : 0.06947560310363769
			 train-loss:  2.084509256782882 	 ± 0.2432504077484128
	data : 0.11637978553771973
	model : 0.06961679458618164
			 train-loss:  2.0874134155168926 	 ± 0.24645327512671508
	data : 0.11618828773498535
	model : 0.06932129859924316
			 train-loss:  2.0892119906165383 	 ± 0.2473288631047836
	data : 0.11648035049438477
	model : 0.06903605461120606
			 train-loss:  2.0891470596261694 	 ± 0.24677054044884134
	data : 0.11681785583496093
	model : 0.06938085556030274
			 train-loss:  2.0887345285029024 	 ± 0.2462904888259457
	data : 0.11664042472839356
	model : 0.06929669380187989
			 train-loss:  2.089541553381847 	 ± 0.24603165975898292
	data : 0.11666278839111328
	model : 0.06839804649353028
			 train-loss:  2.0908060217542306 	 ± 0.24620702192594232
	data : 0.11761369705200195
	model : 0.06801042556762696
			 train-loss:  2.0924381918377346 	 ± 0.24687084903622988
	data : 0.11786813735961914
	model : 0.06744036674499512
			 train-loss:  2.092922623706075 	 ± 0.24643122510946866
	data : 0.11829657554626465
	model : 0.06682558059692383
			 train-loss:  2.0915127157639826 	 ± 0.2467996654683418
	data : 0.11876654624938965
	model : 0.06677761077880859
			 train-loss:  2.090512102110344 	 ± 0.24671887667608966
	data : 0.11874661445617676
	model : 0.06734666824340821
			 train-loss:  2.089603541199297 	 ± 0.24656156526226425
	data : 0.11823334693908691
	model : 0.06728940010070801
			 train-loss:  2.090480193366175 	 ± 0.2463823870421999
	data : 0.11847105026245117
	model : 0.06804275512695312
			 train-loss:  2.090042153994242 	 ± 0.24593825160785035
	data : 0.11772618293762208
	model : 0.06838712692260743
			 train-loss:  2.088270149354277 	 ± 0.24688104384836718
	data : 0.11746382713317871
	model : 0.06765160560607911
			 train-loss:  2.089284193873917 	 ± 0.24683440435937232
	data : 0.11807599067687988
	model : 0.06705646514892578
			 train-loss:  2.0888219990281978 	 ± 0.24640743664995896
	data : 0.11840753555297852
	model : 0.06767964363098145
			 train-loss:  2.0912781918302494 	 ± 0.24873670991833433
	data : 0.11782069206237793
	model : 0.06679458618164062
			 train-loss:  2.0928669147572276 	 ± 0.24940116194368525
	data : 0.11859636306762696
	model : 0.06675868034362793
			 train-loss:  2.0920491198447184 	 ± 0.24919133702333052
	data : 0.11836228370666504
	model : 0.06707696914672852
			 train-loss:  2.090818238859417 	 ± 0.24938822074352857
	data : 0.11793303489685059
	model : 0.06759510040283204
			 train-loss:  2.0903279212728205 	 ± 0.24898087190444396
	data : 0.11740574836730958
	model : 0.06741833686828613
			 train-loss:  2.089586549003919 	 ± 0.24872583101196566
	data : 0.11741271018981933
	model : 0.06743073463439941
			 train-loss:  2.089850369330758 	 ± 0.24824291321235195
	data : 0.11762371063232421
	model : 0.06688323020935058
			 train-loss:  2.0901283075001613 	 ± 0.24776705641419638
	data : 0.11844348907470703
	model : 0.06668806076049805
			 train-loss:  2.0896926137154974 	 ± 0.24734960127202127
	data : 0.11880173683166503
	model : 0.06702656745910644
			 train-loss:  2.088171508957128 	 ± 0.24797847420222077
	data : 0.11879940032958984
	model : 0.06694474220275878
			 train-loss:  2.0889771359307425 	 ± 0.24779163627141335
	data : 0.11911735534667969
	model : 0.06754093170166016
			 train-loss:  2.086996546605738 	 ± 0.24922313001226476
	data : 0.11850161552429199
	model : 0.06800885200500488
			 train-loss:  2.086672263589465 	 ± 0.24877011776840807
	data : 0.11816301345825195
	model : 0.06863436698913575
			 train-loss:  2.0872802320987947 	 ± 0.24845185935603847
	data : 0.11746020317077636
	model : 0.0677558422088623
			 train-loss:  2.087406916790698 	 ± 0.24796048404306215
	data : 0.11809248924255371
	model : 0.0675271987915039
			 train-loss:  2.0867486023902893 	 ± 0.24768200403920904
	data : 0.11816763877868652
	model : 0.06776261329650879
			 train-loss:  2.0871546624666193 	 ± 0.2472714873418538
	data : 0.1179133415222168
	model : 0.06735806465148926
			 train-loss:  2.0857896043194666 	 ± 0.24772619173609253
	data : 0.11808724403381347
	model : 0.06670155525207519
			 train-loss:  2.086269108674272 	 ± 0.24735327915958333
	data : 0.11895289421081542
	model : 0.06708593368530273
			 train-loss:  2.084790646091221 	 ± 0.2479834362960748
	data : 0.11853885650634766
	model : 0.06745400428771972
			 train-loss:  2.085822880969328 	 ± 0.24804286722977606
	data : 0.117946195602417
	model : 0.06703295707702636
			 train-loss:  2.0871911547146738 	 ± 0.24852029101643613
	data : 0.11718721389770508
	model : 0.05872831344604492
#epoch  47    val-loss:  2.4148079533325997  train-loss:  2.0871911547146738  lr:  0.00015625
			 train-loss:  2.627326250076294 	 ± 0.0
	data : 5.779635190963745
	model : 0.07238459587097168
			 train-loss:  2.341774821281433 	 ± 0.28555142879486084
	data : 2.957767128944397
	model : 0.07043564319610596
			 train-loss:  2.2486223379770913 	 ± 0.2677956608113525
	data : 2.0110508600870767
	model : 0.06893332799275716
			 train-loss:  2.3628918528556824 	 ± 0.30489088712817775
	data : 1.5380223393440247
	model : 0.06906837224960327
			 train-loss:  2.2978214740753176 	 ± 0.3021644903385017
	data : 1.2536531925201415
	model : 0.06864423751831054
			 train-loss:  2.254757364590963 	 ± 0.29216217581965415
	data : 0.12162423133850098
	model : 0.06747589111328126
			 train-loss:  2.2300869056156705 	 ± 0.2771577546690212
	data : 0.11826982498168945
	model : 0.06720261573791504
			 train-loss:  2.2055141925811768 	 ± 0.2672846720870011
	data : 0.118403959274292
	model : 0.06745052337646484
			 train-loss:  2.177840987841288 	 ± 0.26387430050914235
	data : 0.118430757522583
	model : 0.06774225234985351
			 train-loss:  2.161657154560089 	 ± 0.25499790187241356
	data : 0.11812324523925781
	model : 0.06838445663452149
			 train-loss:  2.1639651493592695 	 ± 0.2432404818267801
	data : 0.11742362976074219
	model : 0.0689976692199707
			 train-loss:  2.1638423105080924 	 ± 0.23288538736740022
	data : 0.11696267127990723
	model : 0.06907691955566406
			 train-loss:  2.152614162518428 	 ± 0.22710457904920336
	data : 0.11687521934509278
	model : 0.06963462829589843
			 train-loss:  2.1631228157452176 	 ± 0.22209923039474827
	data : 0.11638274192810058
	model : 0.06940217018127441
			 train-loss:  2.1543609062830607 	 ± 0.217058344851638
	data : 0.11659755706787109
	model : 0.06846852302551269
			 train-loss:  2.146459050476551 	 ± 0.2123823665718917
	data : 0.11745181083679199
	model : 0.0677495002746582
			 train-loss:  2.1572779557284187 	 ± 0.21053678782099927
	data : 0.11803174018859863
	model : 0.0672837257385254
			 train-loss:  2.1358504229121738 	 ± 0.22286445147900238
	data : 0.11854543685913085
	model : 0.06718683242797852
			 train-loss:  2.139804344428213 	 ± 0.21756799331728194
	data : 0.11830787658691407
	model : 0.06710329055786132
			 train-loss:  2.1241042852401733 	 ± 0.22282815226527017
	data : 0.11857776641845703
	model : 0.06797585487365723
			 train-loss:  2.120294979640416 	 ± 0.21812428181713653
	data : 0.11781244277954102
	model : 0.0687718391418457
			 train-loss:  2.1016000671820207 	 ± 0.22968467851968194
	data : 0.1171346664428711
	model : 0.06989035606384278
			 train-loss:  2.102078432622163 	 ± 0.22464725235406377
	data : 0.11611056327819824
	model : 0.06903548240661621
			 train-loss:  2.0956574082374573 	 ± 0.22206283248080383
	data : 0.11717476844787597
	model : 0.0682873249053955
			 train-loss:  2.096763505935669 	 ± 0.2176437188559628
	data : 0.11778960227966309
	model : 0.06849913597106934
			 train-loss:  2.089202431532053 	 ± 0.21673983952100773
	data : 0.11760783195495605
	model : 0.06762690544128418
			 train-loss:  2.0765097273720636 	 ± 0.22231739370051215
	data : 0.11848793029785157
	model : 0.06698465347290039
			 train-loss:  2.0740175374916623 	 ± 0.21869508776006172
	data : 0.11896486282348633
	model : 0.06782112121582032
			 train-loss:  2.084249426578653 	 ± 0.2216070381657215
	data : 0.11819238662719726
	model : 0.06869783401489257
			 train-loss:  2.082050383090973 	 ± 0.21820386804085606
	data : 0.11741871833801269
	model : 0.06798124313354492
			 train-loss:  2.0863679339808803 	 ± 0.21595431112730945
	data : 0.11800665855407715
	model : 0.0688239574432373
			 train-loss:  2.0727659948170185 	 ± 0.22564192085815485
	data : 0.11720137596130371
	model : 0.06937632560729981
			 train-loss:  2.0739106156609277 	 ± 0.22229112579345642
	data : 0.11673011779785156
	model : 0.0694584846496582
			 train-loss:  2.0616787987596847 	 ± 0.22999431148814728
	data : 0.11669645309448243
	model : 0.06935000419616699
			 train-loss:  2.0761609520230975 	 ± 0.2419027561950907
	data : 0.11672701835632324
	model : 0.06988248825073243
			 train-loss:  2.0755191412236957 	 ± 0.23854955468798886
	data : 0.11613655090332031
	model : 0.0699185848236084
			 train-loss:  2.084091885669811 	 ± 0.2408601318563592
	data : 0.11603274345397949
	model : 0.06964807510375977
			 train-loss:  2.085895409709529 	 ± 0.23792284298316385
	data : 0.11634364128112792
	model : 0.0696493148803711
			 train-loss:  2.083667889619485 	 ± 0.23525382176274198
	data : 0.11601862907409669
	model : 0.06953692436218262
			 train-loss:  2.0831239581108094 	 ± 0.23231937120176968
	data : 0.116162109375
	model : 0.06939735412597656
			 train-loss:  2.0796112083807223 	 ± 0.23054168677142864
	data : 0.1163680076599121
	model : 0.06893725395202636
			 train-loss:  2.071952442328135 	 ± 0.23299985059886613
	data : 0.11686172485351562
	model : 0.06911520957946778
			 train-loss:  2.0743013509484225 	 ± 0.2307772231528388
	data : 0.11678338050842285
	model : 0.06928067207336426
			 train-loss:  2.0762311193076046 	 ± 0.2284903652446881
	data : 0.11689434051513672
	model : 0.06946163177490235
			 train-loss:  2.0629896508322823 	 ± 0.24240973181064532
	data : 0.11669631004333496
	model : 0.06958613395690919
			 train-loss:  2.061902396056963 	 ± 0.23987127471525874
	data : 0.11664533615112305
	model : 0.07007255554199218
			 train-loss:  2.06978935130099 	 ± 0.24325993855597614
	data : 0.11621522903442383
	model : 0.06956162452697753
			 train-loss:  2.0697020615140596 	 ± 0.24071338774055479
	data : 0.11659646034240723
	model : 0.06881942749023437
			 train-loss:  2.0661071587582023 	 ± 0.23954278490234895
	data : 0.11719784736633301
	model : 0.06906514167785645
			 train-loss:  2.065609872341156 	 ± 0.23716080679160695
	data : 0.1170644760131836
	model : 0.06848464012145997
			 train-loss:  2.0591513619703403 	 ± 0.23922378243621725
	data : 0.11760478019714356
	model : 0.06757645606994629
			 train-loss:  2.063056535445727 	 ± 0.2385482103209251
	data : 0.11833562850952148
	model : 0.06809678077697753
			 train-loss:  2.066872369568303 	 ± 0.2378838275150863
	data : 0.11787190437316894
	model : 0.06868014335632325
			 train-loss:  2.0617973296730607 	 ± 0.23854946196964819
	data : 0.11736221313476562
	model : 0.06885414123535157
			 train-loss:  2.058215763352134 	 ± 0.23783163246062491
	data : 0.11701068878173829
	model : 0.06947932243347169
			 train-loss:  2.0550249963998795 	 ± 0.23688345377560843
	data : 0.11640558242797852
	model : 0.07028260231018066
			 train-loss:  2.056001669482181 	 ± 0.23491006026336836
	data : 0.11551008224487305
	model : 0.06933827400207519
			 train-loss:  2.0536070219401656 	 ± 0.2335768976665682
	data : 0.11639990806579589
	model : 0.06925373077392578
			 train-loss:  2.054786918526989 	 ± 0.23176323580748087
	data : 0.11663861274719238
	model : 0.0688845157623291
			 train-loss:  2.056056640545527 	 ± 0.23003060729754793
	data : 0.11703495979309082
	model : 0.06865534782409669
			 train-loss:  2.0623376545358876 	 ± 0.23326745285029452
	data : 0.1173130989074707
	model : 0.06851863861083984
			 train-loss:  2.060415008375722 	 ± 0.23186538082869657
	data : 0.11747827529907226
	model : 0.06954846382141114
			 train-loss:  2.0592278601631286 	 ± 0.23020767734415196
	data : 0.1165802001953125
	model : 0.06983842849731445
			 train-loss:  2.062475237995386 	 ± 0.22985187188460376
	data : 0.11632070541381836
	model : 0.07011523246765136
			 train-loss:  2.0622192052694466 	 ± 0.2280861244906235
	data : 0.11612091064453126
	model : 0.07048707008361817
			 train-loss:  2.061249391599135 	 ± 0.22648660815438595
	data : 0.1156198501586914
	model : 0.07032256126403809
			 train-loss:  2.064131218995621 	 ± 0.22600596079536198
	data : 0.11589217185974121
	model : 0.06932058334350585
			 train-loss:  2.0678257118253147 	 ± 0.22636704402959382
	data : 0.11669678688049316
	model : 0.06915483474731446
			 train-loss:  2.074253446813943 	 ± 0.23088714391126608
	data : 0.11673417091369628
	model : 0.06889944076538086
			 train-loss:  2.0718924352100916 	 ± 0.2300694456495477
	data : 0.11687946319580078
	model : 0.06879687309265137
			 train-loss:  2.072399384538892 	 ± 0.22848286420431443
	data : 0.11717028617858886
	model : 0.06895108222961426
			 train-loss:  2.0731020073095956 	 ± 0.22696785906501143
	data : 0.11702384948730468
	model : 0.07001032829284667
			 train-loss:  2.0748888335815847 	 ± 0.2259172637877964
	data : 0.11604452133178711
	model : 0.06903119087219238
			 train-loss:  2.073521771946469 	 ± 0.22468939834560592
	data : 0.1168898105621338
	model : 0.06889424324035645
			 train-loss:  2.071106448173523 	 ± 0.22415148545840283
	data : 0.11702241897583007
	model : 0.0687211036682129
			 train-loss:  2.0670213275834133 	 ± 0.22546485403653288
	data : 0.11713581085205078
	model : 0.06878643035888672
			 train-loss:  2.0620015627377994 	 ± 0.22823072615110132
	data : 0.11713809967041015
	model : 0.0682680606842041
			 train-loss:  2.0605917496558948 	 ± 0.22710019020956915
	data : 0.11777572631835938
	model : 0.069340181350708
			 train-loss:  2.0638362637049035 	 ± 0.2274703291355486
	data : 0.11698403358459472
	model : 0.06945114135742188
			 train-loss:  2.0620364531874658 	 ± 0.22660951517548325
	data : 0.1169766902923584
	model : 0.06960473060607911
			 train-loss:  2.0576272628925465 	 ± 0.22863327701730832
	data : 0.1168337345123291
	model : 0.07066931724548339
			 train-loss:  2.0554923051741065 	 ± 0.22804582700107928
	data : 0.1156832218170166
	model : 0.07115092277526855
			 train-loss:  2.055359607719513 	 ± 0.22667107902914574
	data : 0.11519265174865723
	model : 0.07118401527404786
			 train-loss:  2.0619576289540245 	 ± 0.23319825563913085
	data : 0.11499505043029785
	model : 0.07112293243408203
			 train-loss:  2.061247963063857 	 ± 0.23191366820961817
	data : 0.11485252380371094
	model : 0.07107200622558593
			 train-loss:  2.0611450478088025 	 ± 0.23056334275532833
	data : 0.11483225822448731
	model : 0.06988000869750977
			 train-loss:  2.061443753626155 	 ± 0.22925117266456674
	data : 0.11599907875061036
	model : 0.0695002555847168
			 train-loss:  2.0619892369617117 	 ± 0.22800166400075525
	data : 0.11632952690124512
	model : 0.0684535026550293
			 train-loss:  2.0646681919526517 	 ± 0.22810571838514157
	data : 0.11731576919555664
	model : 0.06847381591796875
			 train-loss:  2.0677658213509456 	 ± 0.22870956560261718
	data : 0.11746892929077149
	model : 0.06855564117431641
			 train-loss:  2.0696782017802144 	 ± 0.2281718637216833
	data : 0.1173327922821045
	model : 0.06892485618591308
			 train-loss:  2.0682671121929004 	 ± 0.2273272983312141
	data : 0.1170581340789795
	model : 0.06828694343566895
			 train-loss:  2.0645497370791692 	 ± 0.2288959657107515
	data : 0.11754169464111328
	model : 0.06917757987976074
			 train-loss:  2.067343462020793 	 ± 0.22926369648696332
	data : 0.11696381568908691
	model : 0.06926493644714356
			 train-loss:  2.0737354240919412 	 ± 0.23632421545651697
	data : 0.1168785572052002
	model : 0.06908221244812011
			 train-loss:  2.074121399472157 	 ± 0.23512023691003608
	data : 0.11701626777648926
	model : 0.0690117359161377
			 train-loss:  2.0756954347964416 	 ± 0.2344130148666482
	data : 0.1171875
	model : 0.06904692649841308
			 train-loss:  2.0773914991592872 	 ± 0.23381143316138608
	data : 0.11718063354492188
	model : 0.06915512084960937
			 train-loss:  2.0787963349409777 	 ± 0.23304290516083598
	data : 0.11681165695190429
	model : 0.06895051002502442
			 train-loss:  2.0850145161151885 	 ± 0.2399871197038709
	data : 0.11699304580688477
	model : 0.06812653541564942
			 train-loss:  2.082509629797227 	 ± 0.24010628345025456
	data : 0.11773576736450195
	model : 0.06821904182434083
			 train-loss:  2.0835822131119524 	 ± 0.23916942767304303
	data : 0.1176539421081543
	model : 0.06924309730529785
			 train-loss:  2.0796576275408847 	 ± 0.2412834385064498
	data : 0.11668262481689454
	model : 0.06921730041503907
			 train-loss:  2.085475844832567 	 ± 0.24727440697612646
	data : 0.11674971580505371
	model : 0.06837615966796876
			 train-loss:  2.0863574198314123 	 ± 0.24625825602263032
	data : 0.11771740913391113
	model : 0.06909046173095704
			 train-loss:  2.086929832989315 	 ± 0.2451640830921326
	data : 0.11704077720642089
	model : 0.06871767044067383
			 train-loss:  2.0906258732358984 	 ± 0.24696503862143276
	data : 0.11718811988830566
	model : 0.06839494705200196
			 train-loss:  2.089056095591298 	 ± 0.246354747581772
	data : 0.11764898300170898
	model : 0.06790118217468262
			 train-loss:  2.0901969102544524 	 ± 0.24550850101238
	data : 0.11793956756591797
	model : 0.069342041015625
			 train-loss:  2.0886720754883505 	 ± 0.24490796833829415
	data : 0.11668896675109863
	model : 0.06950674057006836
			 train-loss:  2.088743904689411 	 ± 0.24380344730370027
	data : 0.1166388988494873
	model : 0.06982836723327637
			 train-loss:  2.087872763829572 	 ± 0.24288606782188815
	data : 0.11631617546081544
	model : 0.07026147842407227
			 train-loss:  2.0904405507366213 	 ± 0.24333115219670381
	data : 0.11594586372375489
	model : 0.07053933143615723
			 train-loss:  2.093259136927755 	 ± 0.24410731985403847
	data : 0.1158268928527832
	model : 0.06937007904052735
			 train-loss:  2.0922499138375987 	 ± 0.24328242088339624
	data : 0.1166646957397461
	model : 0.06976637840270997
			 train-loss:  2.088906164827018 	 ± 0.24487115975546309
	data : 0.11653299331665039
	model : 0.07025718688964844
			 train-loss:  2.086518263205504 	 ± 0.24517510220103717
	data : 0.11611390113830566
	model : 0.07048449516296387
			 train-loss:  2.0875092967081876 	 ± 0.24436924553975128
	data : 0.11578431129455566
	model : 0.07029228210449219
			 train-loss:  2.0884173437326896 	 ± 0.24354015502780957
	data : 0.11592926979064941
	model : 0.07105250358581543
			 train-loss:  2.08852548400561 	 ± 0.24252615049851448
	data : 0.11523513793945313
	model : 0.07086405754089356
			 train-loss:  2.0896880666086495 	 ± 0.24185743441708316
	data : 0.11536965370178223
	model : 0.07031750679016113
			 train-loss:  2.0890921510633875 	 ± 0.24095335684146027
	data : 0.11584281921386719
	model : 0.06898713111877441
			 train-loss:  2.0884611412761656 	 ± 0.24007306561304798
	data : 0.11715226173400879
	model : 0.06916351318359375
			 train-loss:  2.08560943122833 	 ± 0.24118570563023078
	data : 0.11702380180358887
	model : 0.06934762001037598
			 train-loss:  2.0908450365066527 	 ± 0.2471926657684634
	data : 0.11678538322448731
	model : 0.06917529106140137
			 train-loss:  2.0915097386117965 	 ± 0.24632192054575627
	data : 0.11692495346069336
	model : 0.06919298171997071
			 train-loss:  2.090670384759978 	 ± 0.24553106837838345
	data : 0.11692280769348144
	model : 0.06914443969726562
			 train-loss:  2.090318726375699 	 ± 0.2446021878615345
	data : 0.11675662994384765
	model : 0.06864166259765625
			 train-loss:  2.088741194370181 	 ± 0.24430507935152238
	data : 0.11706109046936035
	model : 0.06843380928039551
			 train-loss:  2.0884248247513404 	 ± 0.24339015634407513
	data : 0.11730146408081055
	model : 0.06852149963378906
			 train-loss:  2.087512761581945 	 ± 0.24268231407952845
	data : 0.11706809997558594
	model : 0.06844391822814941
			 train-loss:  2.087447492462216 	 ± 0.24176246942550952
	data : 0.11730828285217285
	model : 0.06932263374328614
			 train-loss:  2.087324401489774 	 ± 0.24085602494345837
	data : 0.11657729148864746
	model : 0.07025265693664551
			 train-loss:  2.0888525203092776 	 ± 0.24060190511843035
	data : 0.11586132049560546
	model : 0.06947140693664551
			 train-loss:  2.0920707481878775 	 ± 0.24258669386462245
	data : 0.11652278900146484
	model : 0.06944293975830078
			 train-loss:  2.094276495716151 	 ± 0.24304817303082343
	data : 0.11651887893676757
	model : 0.06960649490356445
			 train-loss:  2.092366494401528 	 ± 0.24318176574308678
	data : 0.11631054878234863
	model : 0.06892857551574708
			 train-loss:  2.091563694718955 	 ± 0.24248120397031325
	data : 0.11708707809448242
	model : 0.06792621612548828
			 train-loss:  2.0910291620295682 	 ± 0.24168898069607697
	data : 0.11790714263916016
	model : 0.06880345344543456
			 train-loss:  2.088850128650665 	 ± 0.2421906719306819
	data : 0.11715598106384277
	model : 0.06938905715942383
			 train-loss:  2.0894297150009913 	 ± 0.24142772859976755
	data : 0.11670527458190919
	model : 0.06927108764648438
			 train-loss:  2.0952731438086065 	 ± 0.25038253123285686
	data : 0.11677618026733398
	model : 0.06919326782226562
			 train-loss:  2.094606865536083 	 ± 0.2496318250163684
	data : 0.11679835319519043
	model : 0.06933479309082032
			 train-loss:  2.095555650691191 	 ± 0.24902213830287273
	data : 0.11671380996704102
	model : 0.06918940544128419
			 train-loss:  2.0952589158354136 	 ± 0.24818750099719863
	data : 0.11682677268981934
	model : 0.06869254112243653
			 train-loss:  2.093205452781834 	 ± 0.24856902697454936
	data : 0.11729397773742675
	model : 0.06857771873474121
			 train-loss:  2.092661300484015 	 ± 0.24780935268872367
	data : 0.11747994422912597
	model : 0.06939234733581542
			 train-loss:  2.089773380273097 	 ± 0.24944045104039836
	data : 0.11669821739196777
	model : 0.0699842929840088
			 train-loss:  2.0899811087038693 	 ± 0.2486148375915913
	data : 0.11641545295715332
	model : 0.0696197509765625
			 train-loss:  2.090716303984324 	 ± 0.24794719503121776
	data : 0.11675243377685547
	model : 0.06850857734680176
			 train-loss:  2.090218933212836 	 ± 0.24719987935239185
	data : 0.11759238243103028
	model : 0.06787075996398925
			 train-loss:  2.094215172686075 	 ± 0.25123139712255244
	data : 0.11815910339355469
	model : 0.06773324012756347
			 train-loss:  2.0926224406248606 	 ± 0.25117777964515403
	data : 0.11821727752685547
	model : 0.06708426475524902
			 train-loss:  2.0923564588868775 	 ± 0.250382555394281
	data : 0.11880731582641602
	model : 0.06656289100646973
			 train-loss:  2.090220836670168 	 ± 0.25097677006519997
	data : 0.11933941841125488
	model : 0.06758074760437012
			 train-loss:  2.089846338217075 	 ± 0.25021450778621857
	data : 0.11853070259094238
	model : 0.06838774681091309
			 train-loss:  2.0909169092299833 	 ± 0.2497745430856453
	data : 0.11774725914001465
	model : 0.06762723922729492
			 train-loss:  2.0901947195016883 	 ± 0.24914724664241694
	data : 0.11860699653625488
	model : 0.06738395690917968
			 train-loss:  2.0877485807586766 	 ± 0.250258573798825
	data : 0.11860146522521972
	model : 0.06843905448913574
			 train-loss:  2.0872053511440756 	 ± 0.2495693109071659
	data : 0.11786088943481446
	model : 0.06830120086669922
			 train-loss:  2.087519021508116 	 ± 0.24882467891291168
	data : 0.1178464412689209
	model : 0.06793909072875977
			 train-loss:  2.0883746022059593 	 ± 0.24829295665680148
	data : 0.11826124191284179
	model : 0.06783995628356934
			 train-loss:  2.0858383171397485 	 ± 0.2496262794425969
	data : 0.11811132431030273
	model : 0.06786675453186035
			 train-loss:  2.0855955154430577 	 ± 0.24888336555887197
	data : 0.11816163063049316
	model : 0.06779990196228028
			 train-loss:  2.0833754539489746 	 ± 0.24975151718298458
	data : 0.11824345588684082
	model : 0.06782097816467285
			 train-loss:  2.083692396979734 	 ± 0.24903139783440836
	data : 0.11853857040405273
	model : 0.06804237365722657
			 train-loss:  2.0837821318003944 	 ± 0.24828736715154584
	data : 0.11818170547485352
	model : 0.06818485260009766
			 train-loss:  2.0820614220131013 	 ± 0.2485440266488785
	data : 0.11814665794372559
	model : 0.06828465461730956
			 train-loss:  2.081809195541066 	 ± 0.24782916217429335
	data : 0.11809868812561035
	model : 0.06726837158203125
			 train-loss:  2.080151114744299 	 ± 0.24803754525767424
	data : 0.1188204288482666
	model : 0.06727967262268067
			 train-loss:  2.0813632457576996 	 ± 0.24781569139646123
	data : 0.11875309944152831
	model : 0.06708621978759766
			 train-loss:  2.0813509877337966 	 ± 0.24709429879774958
	data : 0.11900372505187988
	model : 0.06791219711303711
			 train-loss:  2.082349859910204 	 ± 0.2467271412832948
	data : 0.11830754280090332
	model : 0.06870641708374023
			 train-loss:  2.084453253910459 	 ± 0.2475678252972815
	data : 0.1175004482269287
	model : 0.06972208023071289
			 train-loss:  2.0856252588544573 	 ± 0.24734309403255592
	data : 0.11658883094787598
	model : 0.06982989311218261
			 train-loss:  2.0852921578017147 	 ± 0.24667877452920828
	data : 0.11650962829589843
	model : 0.0696976661682129
			 train-loss:  2.084715226275773 	 ± 0.24610000351381026
	data : 0.11649408340454101
	model : 0.0695620059967041
			 train-loss:  2.086001844888323 	 ± 0.24600398637096005
	data : 0.11639518737792968
	model : 0.06970138549804687
			 train-loss:  2.08484453528953 	 ± 0.245801300243508
	data : 0.1162879467010498
	model : 0.06957931518554687
			 train-loss:  2.085474556684494 	 ± 0.24526245538244706
	data : 0.11641387939453125
	model : 0.06951274871826171
			 train-loss:  2.0861979867871954 	 ± 0.24477649905834983
	data : 0.11644840240478516
	model : 0.06994380950927734
			 train-loss:  2.08462204513969 	 ± 0.24502215904609878
	data : 0.11626057624816895
	model : 0.07050075531005859
			 train-loss:  2.083038896810813 	 ± 0.24528340933370493
	data : 0.11600751876831054
	model : 0.07051949501037598
			 train-loss:  2.082015295391497 	 ± 0.24500757734360806
	data : 0.11606440544128419
	model : 0.0704577922821045
			 train-loss:  2.081889602300283 	 ± 0.24435044587002147
	data : 0.11602401733398438
	model : 0.06954431533813477
			 train-loss:  2.080824136734009 	 ± 0.24412322582499557
	data : 0.1167226791381836
	model : 0.0688246726989746
			 train-loss:  2.0822135165413433 	 ± 0.2442058627131899
	data : 0.11747546195983886
	model : 0.06831369400024415
			 train-loss:  2.0806449886332166 	 ± 0.24449818048872493
	data : 0.11769781112670899
	model : 0.06811628341674805
			 train-loss:  2.0796767719208247 	 ± 0.24421160240263262
	data : 0.11776952743530274
	model : 0.06835103034973145
			 train-loss:  2.0787599645162884 	 ± 0.2438939870708773
	data : 0.1176680564880371
	model : 0.06912736892700196
			 train-loss:  2.0769105250922797 	 ± 0.2445868391783343
	data : 0.11693863868713379
	model : 0.07053871154785156
			 train-loss:  2.077256832892696 	 ± 0.2439960075070532
	data : 0.11566739082336426
	model : 0.07021493911743164
			 train-loss:  2.078098984579966 	 ± 0.24364267940475642
	data : 0.11620550155639649
	model : 0.07023682594299316
			 train-loss:  2.077276489783808 	 ± 0.2432824096654635
	data : 0.11621856689453125
	model : 0.06940655708312989
			 train-loss:  2.076942428564414 	 ± 0.24270241244270124
	data : 0.11687812805175782
	model : 0.069724702835083
			 train-loss:  2.0758095292412504 	 ± 0.24259885183589805
	data : 0.11666698455810547
	model : 0.06898856163024902
			 train-loss:  2.0771842868194965 	 ± 0.24274653954123251
	data : 0.11728081703186036
	model : 0.06854133605957032
			 train-loss:  2.0789025088753363 	 ± 0.2433307985804766
	data : 0.1176724910736084
	model : 0.06782617568969726
			 train-loss:  2.077269009010277 	 ± 0.24380456965153433
	data : 0.11821908950805664
	model : 0.0685460090637207
			 train-loss:  2.0795189476013185 	 ± 0.24525669685267176
	data : 0.11753816604614258
	model : 0.06846013069152831
			 train-loss:  2.0795842082939338 	 ± 0.2446475857139472
	data : 0.1176109790802002
	model : 0.06841330528259278
			 train-loss:  2.081040413072794 	 ± 0.24491298247699356
	data : 0.1177292823791504
	model : 0.06923971176147461
			 train-loss:  2.0809834637665396 	 ± 0.2443103445757945
	data : 0.11716175079345703
	model : 0.06921215057373047
			 train-loss:  2.080453481159958 	 ± 0.2438277617946757
	data : 0.11733446121215821
	model : 0.06915059089660644
			 train-loss:  2.0812470447726366 	 ± 0.24349627387322378
	data : 0.11740326881408691
	model : 0.06903924942016601
			 train-loss:  2.0838203268143736 	 ± 0.24568289060945214
	data : 0.11758589744567871
	model : 0.06902461051940918
			 train-loss:  2.0864664391043104 	 ± 0.24801387390086188
	data : 0.11752538681030274
	model : 0.06904435157775879
			 train-loss:  2.0855844393372536 	 ± 0.2477421773491926
	data : 0.11715383529663086
	model : 0.07060256004333496
			 train-loss:  2.086168358200475 	 ± 0.24729221664172468
	data : 0.11554088592529296
	model : 0.07111124992370606
			 train-loss:  2.0864073180017018 	 ± 0.24672690940769887
	data : 0.11523561477661133
	model : 0.07116112709045411
			 train-loss:  2.08593554123883 	 ± 0.24623648197771245
	data : 0.11506829261779786
	model : 0.07106375694274902
			 train-loss:  2.086224574525401 	 ± 0.2456909240019107
	data : 0.11509361267089843
	model : 0.07106957435607911
			 train-loss:  2.0866102954031716 	 ± 0.2451778383554437
	data : 0.11514158248901367
	model : 0.06974472999572753
			 train-loss:  2.0861356782021923 	 ± 0.24470238069032466
	data : 0.11647686958312989
	model : 0.06933622360229492
			 train-loss:  2.0856283997380456 	 ± 0.24424540094008862
	data : 0.11674633026123046
	model : 0.0696554183959961
			 train-loss:  2.084522718632663 	 ± 0.24421809034051353
	data : 0.11655502319335938
	model : 0.06942811012268066
			 train-loss:  2.084654176290134 	 ± 0.2436623857971504
	data : 0.11684956550598144
	model : 0.06862387657165528
			 train-loss:  2.085105934274306 	 ± 0.2431939536198459
	data : 0.11754770278930664
	model : 0.0690399169921875
			 train-loss:  2.085489465765757 	 ± 0.24270415193635927
	data : 0.11708383560180664
	model : 0.06890740394592285
			 train-loss:  2.085559515519576 	 ± 0.24215414224231036
	data : 0.117100191116333
	model : 0.06863408088684082
			 train-loss:  2.086803672540242 	 ± 0.24230938759415824
	data : 0.11716728210449219
	model : 0.06896858215332032
			 train-loss:  2.0870493757832156 	 ± 0.24179062089141617
	data : 0.11680140495300292
	model : 0.0696946620941162
			 train-loss:  2.0881482774366713 	 ± 0.24180286101060663
	data : 0.11620817184448243
	model : 0.06965198516845703
			 train-loss:  2.0873680050883974 	 ± 0.24154372463993587
	data : 0.11608257293701171
	model : 0.06959996223449708
			 train-loss:  2.088696083492703 	 ± 0.24182463998889175
	data : 0.1161797046661377
	model : 0.0693666934967041
			 train-loss:  2.0893489913602847 	 ± 0.24148771010112371
	data : 0.1164179801940918
	model : 0.06854238510131835
			 train-loss:  2.0874213011779448 	 ± 0.24269163084225268
	data : 0.11689352989196777
	model : 0.06823420524597168
			 train-loss:  2.0861764022132805 	 ± 0.24288412114425662
	data : 0.11742420196533203
	model : 0.06793761253356934
			 train-loss:  2.087429973756382 	 ± 0.24309128936349622
	data : 0.11782383918762207
	model : 0.06772909164428711
			 train-loss:  2.086189762405727 	 ± 0.24328723289080254
	data : 0.11799306869506836
	model : 0.06784019470214844
			 train-loss:  2.08687162399292 	 ± 0.24298021472388678
	data : 0.11805787086486816
	model : 0.06867861747741699
			 train-loss:  2.087390107327494 	 ± 0.24258401268067578
	data : 0.11736860275268554
	model : 0.06891188621520997
			 train-loss:  2.0901819933125902 	 ± 0.24576979983066502
	data : 0.1171022891998291
	model : 0.06889796257019043
			 train-loss:  2.089525223797203 	 ± 0.24544890827234858
	data : 0.11711282730102539
	model : 0.06923623085021972
			 train-loss:  2.0873361181705556 	 ± 0.24720472302766341
	data : 0.1167829990386963
	model : 0.06844797134399414
			 train-loss:  2.0874309176105563 	 ± 0.2466847088831644
	data : 0.11729087829589843
	model : 0.06819353103637696
			 train-loss:  2.088632343187614 	 ± 0.24685467050865825
	data : 0.1174018383026123
	model : 0.06782865524291992
			 train-loss:  2.0892913091082534 	 ± 0.24654432366086215
	data : 0.11742172241210938
	model : 0.0676347255706787
			 train-loss:  2.0890485422381797 	 ± 0.24605650457901906
	data : 0.11765389442443848
	model : 0.06696968078613282
			 train-loss:  2.088495474557082 	 ± 0.2456921729328485
	data : 0.1182629108428955
	model : 0.06714415550231934
			 train-loss:  2.0888172279255026 	 ± 0.24523257164394677
	data : 0.1180023193359375
	model : 0.06677331924438476
			 train-loss:  2.0878362867457807 	 ± 0.2451987093381998
	data : 0.11838912963867188
	model : 0.06644558906555176
			 train-loss:  2.0869944439012817 	 ± 0.24504386341392048
	data : 0.11870284080505371
	model : 0.06662578582763672
			 train-loss:  2.0876555393953793 	 ± 0.24475825887296015
	data : 0.11856269836425781
	model : 0.06718292236328124
			 train-loss:  2.0874688158229904 	 ± 0.2442756552733365
	data : 0.11802330017089843
	model : 0.06782231330871583
			 train-loss:  2.088515259386078 	 ± 0.24432829998559513
	data : 0.11791062355041504
	model : 0.06770539283752441
			 train-loss:  2.0891694655785193 	 ± 0.24404900566845092
	data : 0.1180222511291504
	model : 0.06841568946838379
			 train-loss:  2.0895451711070154 	 ± 0.24362803904423883
	data : 0.11751112937927247
	model : 0.06806473731994629
			 train-loss:  2.0880429079254945 	 ± 0.24428658275132073
	data : 0.117852783203125
	model : 0.0675851821899414
			 train-loss:  2.0881501288414 	 ± 0.24380339078680938
	data : 0.11837148666381836
	model : 0.06714000701904296
			 train-loss:  2.0885733309020083 	 ± 0.2434092340775483
	data : 0.1185225486755371
	model : 0.06763410568237305
			 train-loss:  2.089972678158018 	 ± 0.24393533008236568
	data : 0.11839118003845214
	model : 0.06720108985900879
			 train-loss:  2.0910438996529863 	 ± 0.24404594594128337
	data : 0.11851892471313477
	model : 0.06773200035095214
			 train-loss:  2.0903774579678935 	 ± 0.24379563226327827
	data : 0.11804766654968261
	model : 0.06798534393310547
			 train-loss:  2.0902672781663783 	 ± 0.2433234682399666
	data : 0.11772313117980956
	model : 0.06803584098815918
			 train-loss:  2.087631757836789 	 ± 0.24646755818176097
	data : 0.11659679412841797
	model : 0.05907883644104004
#epoch  48    val-loss:  2.446264254419427  train-loss:  2.087631757836789  lr:  0.00015625
			 train-loss:  1.9241507053375244 	 ± 0.0
	data : 5.280562400817871
	model : 0.07179832458496094
			 train-loss:  2.0190906524658203 	 ± 0.0949399471282959
	data : 2.838305115699768
	model : 0.06830823421478271
			 train-loss:  2.0619331995646157 	 ± 0.09838714376198132
	data : 1.932212511698405
	model : 0.06741547584533691
			 train-loss:  2.078702747821808 	 ± 0.09002041873642588
	data : 1.4788740873336792
	model : 0.06760174036026001
			 train-loss:  2.0771980762481688 	 ± 0.08057292831815928
	data : 1.2065368175506592
	model : 0.06794114112854004
			 train-loss:  2.002289811770121 	 ± 0.1829377974896615
	data : 0.17376132011413575
	model : 0.06776604652404786
			 train-loss:  2.0545924391065324 	 ± 0.2123646288837244
	data : 0.11761999130249023
	model : 0.06871938705444336
			 train-loss:  2.0727369487285614 	 ± 0.20436720934820743
	data : 0.11678509712219239
	model : 0.06952333450317383
			 train-loss:  2.085116227467855 	 ± 0.19583479462880643
	data : 0.11620531082153321
	model : 0.06928315162658691
			 train-loss:  2.0981837272644044 	 ± 0.18987621248321626
	data : 0.11645207405090333
	model : 0.06951260566711426
			 train-loss:  2.0933175520463423 	 ± 0.1816926784454583
	data : 0.11635785102844239
	model : 0.06846852302551269
			 train-loss:  2.0847174922625222 	 ± 0.1762804061576301
	data : 0.11740822792053222
	model : 0.06850161552429199
			 train-loss:  2.111256562746488 	 ± 0.1927077604195696
	data : 0.11751642227172851
	model : 0.06855940818786621
			 train-loss:  2.1026000806263516 	 ± 0.18830252336670483
	data : 0.11745185852050781
	model : 0.0689939022064209
			 train-loss:  2.102474006017049 	 ± 0.18191813217255193
	data : 0.11712589263916015
	model : 0.06810708045959472
			 train-loss:  2.103194087743759 	 ± 0.17616355086318314
	data : 0.11801633834838868
	model : 0.06888022422790527
			 train-loss:  2.1071324769188378 	 ± 0.1716282771456408
	data : 0.11717734336853028
	model : 0.06901364326477051
			 train-loss:  2.0870020257102118 	 ± 0.18630298559152847
	data : 0.11703634262084961
	model : 0.06890373229980469
			 train-loss:  2.08472026021857 	 ± 0.18159223422869805
	data : 0.11722230911254883
	model : 0.06898841857910157
			 train-loss:  2.08214750289917 	 ± 0.17734913329313973
	data : 0.11715707778930665
	model : 0.07023425102233886
			 train-loss:  2.10540855498541 	 ± 0.20193191334177066
	data : 0.1159477710723877
	model : 0.06942214965820312
			 train-loss:  2.0971643437038767 	 ± 0.20087391038609717
	data : 0.11679091453552246
	model : 0.06915607452392578
			 train-loss:  2.094991253769916 	 ± 0.19672279183987798
	data : 0.11693034172058106
	model : 0.06832923889160156
			 train-loss:  2.0983549108107886 	 ± 0.19325524348529544
	data : 0.11758527755737305
	model : 0.06748785972595214
			 train-loss:  2.0884844970703127 	 ± 0.1954274473736344
	data : 0.11839108467102051
	model : 0.06610245704650879
			 train-loss:  2.0817663302788367 	 ± 0.19455413790245188
	data : 0.1195220947265625
	model : 0.06697707176208496
			 train-loss:  2.090446260240343 	 ± 0.19598031252342596
	data : 0.1186640739440918
	model : 0.06725716590881348
			 train-loss:  2.087111907345908 	 ± 0.19322717732418987
	data : 0.11855483055114746
	model : 0.06803240776062011
			 train-loss:  2.0890707887452225 	 ± 0.19014917893610508
	data : 0.11787624359130859
	model : 0.06888246536254883
			 train-loss:  2.0889694929122924 	 ± 0.18695396262074715
	data : 0.11709003448486328
	model : 0.06888203620910645
			 train-loss:  2.081552497802242 	 ± 0.18834718401331624
	data : 0.11704187393188477
	model : 0.06853408813476562
			 train-loss:  2.0938186272978783 	 ± 0.19756081206487178
	data : 0.11735520362854004
	model : 0.06830439567565919
			 train-loss:  2.1041910431601782 	 ± 0.20320020477395542
	data : 0.11756515502929688
	model : 0.06831750869750977
			 train-loss:  2.107258509187137 	 ± 0.20096370425345914
	data : 0.11759490966796875
	model : 0.06832141876220703
			 train-loss:  2.113092763083322 	 ± 0.200972193639686
	data : 0.11763539314270019
	model : 0.06919045448303222
			 train-loss:  2.103876938422521 	 ± 0.20552489616948716
	data : 0.11681971549987794
	model : 0.06921343803405762
			 train-loss:  2.102830355231826 	 ± 0.2028257389504053
	data : 0.11690244674682618
	model : 0.06915082931518554
			 train-loss:  2.1024158346025565 	 ± 0.20015506905146158
	data : 0.11696653366088867
	model : 0.0683668613433838
			 train-loss:  2.1109665571114955 	 ± 0.2044827231800159
	data : 0.1175394058227539
	model : 0.06811509132385254
			 train-loss:  2.113560089468956 	 ± 0.20255908883734572
	data : 0.11777400970458984
	model : 0.06822619438171387
			 train-loss:  2.1097743075068403 	 ± 0.20150120115424255
	data : 0.11788268089294433
	model : 0.06855764389038085
			 train-loss:  2.109151860078176 	 ± 0.199127816680572
	data : 0.1175806999206543
	model : 0.06845574378967285
			 train-loss:  2.1050331093544186 	 ± 0.19860071242641156
	data : 0.11763381958007812
	model : 0.06946892738342285
			 train-loss:  2.100471756675027 	 ± 0.1985962832195259
	data : 0.11678056716918946
	model : 0.06884965896606446
			 train-loss:  2.098384009467231 	 ± 0.19686495575608795
	data : 0.11728525161743164
	model : 0.0680323600769043
			 train-loss:  2.106842922127765 	 ± 0.20281318332788123
	data : 0.11795587539672851
	model : 0.06728248596191407
			 train-loss:  2.1098278684819 	 ± 0.20166275995073846
	data : 0.11852211952209472
	model : 0.06690850257873535
			 train-loss:  2.119286745786667 	 ± 0.20982308833642765
	data : 0.11882205009460449
	model : 0.0671755313873291
			 train-loss:  2.126758808992347 	 ± 0.21402608980304086
	data : 0.11875395774841309
	model : 0.0685234546661377
			 train-loss:  2.1179932618141173 	 ± 0.22058089179121185
	data : 0.11753330230712891
	model : 0.06946020126342774
			 train-loss:  2.1090791786418244 	 ± 0.22732121749599132
	data : 0.11682705879211426
	model : 0.07021565437316894
			 train-loss:  2.107058983582717 	 ± 0.225586630251024
	data : 0.11622810363769531
	model : 0.07000260353088379
			 train-loss:  2.1032575391373545 	 ± 0.2251235276599668
	data : 0.11634254455566406
	model : 0.06958146095275879
			 train-loss:  2.1062942301785506 	 ± 0.22412231510438416
	data : 0.11667900085449219
	model : 0.06827564239501953
			 train-loss:  2.1014279387213968 	 ± 0.22493618524372488
	data : 0.1177748203277588
	model : 0.06811790466308594
			 train-loss:  2.098814153245517 	 ± 0.22375999507854522
	data : 0.11781325340270996
	model : 0.06816177368164063
			 train-loss:  2.1069602192493906 	 ± 0.23001349950537425
	data : 0.11778097152709961
	model : 0.06902661323547363
			 train-loss:  2.107187885662605 	 ± 0.2280284814332678
	data : 0.1170578956604004
	model : 0.06903729438781739
			 train-loss:  2.1085819975804476 	 ± 0.22633693742704056
	data : 0.11703023910522461
	model : 0.0698624610900879
			 train-loss:  2.10892218152682 	 ± 0.22445808117563068
	data : 0.11642050743103027
	model : 0.06976218223571777
			 train-loss:  2.1080802522721838 	 ± 0.222706164737095
	data : 0.11645827293395997
	model : 0.06979351043701172
			 train-loss:  2.1123852518297013 	 ± 0.22344704469955048
	data : 0.11645030975341797
	model : 0.06975507736206055
			 train-loss:  2.110241659103878 	 ± 0.22230824091736576
	data : 0.11643266677856445
	model : 0.06886200904846192
			 train-loss:  2.111797794699669 	 ± 0.22091018475246063
	data : 0.11718935966491699
	model : 0.0679936408996582
			 train-loss:  2.1190497948573186 	 ± 0.22675179547091695
	data : 0.11792902946472168
	model : 0.06803498268127442
			 train-loss:  2.1193253632747764 	 ± 0.225038389402469
	data : 0.11789870262145996
	model : 0.06791996955871582
			 train-loss:  2.1180589092311575 	 ± 0.22358953377653643
	data : 0.11795368194580078
	model : 0.0670102596282959
			 train-loss:  2.1158169034649346 	 ± 0.22269683552350084
	data : 0.11873250007629395
	model : 0.06792874336242676
			 train-loss:  2.112689140914143 	 ± 0.2225766514330774
	data : 0.11787958145141601
	model : 0.06894841194152831
			 train-loss:  2.105246307168688 	 ± 0.22946667442020988
	data : 0.11703534126281738
	model : 0.068965482711792
			 train-loss:  2.10085349183687 	 ± 0.23079019356848918
	data : 0.11713175773620606
	model : 0.06911072731018067
			 train-loss:  2.10418609281381 	 ± 0.2308958128079009
	data : 0.11710786819458008
	model : 0.06917548179626465
			 train-loss:  2.1039109703612655 	 ± 0.22932076410011293
	data : 0.11707978248596192
	model : 0.06934900283813476
			 train-loss:  2.105910389809995 	 ± 0.22840576728451856
	data : 0.11693720817565918
	model : 0.0689481258392334
			 train-loss:  2.106362385749817 	 ± 0.22691126788424765
	data : 0.11712627410888672
	model : 0.06893119812011719
			 train-loss:  2.1110590178715554 	 ± 0.22905374213381607
	data : 0.11688008308410644
	model : 0.06869740486145019
			 train-loss:  2.1074292566869164 	 ± 0.22975107785277218
	data : 0.11708483695983887
	model : 0.07055902481079102
			 train-loss:  2.10691442550757 	 ± 0.22831826206493658
	data : 0.11539649963378906
	model : 0.06960086822509766
			 train-loss:  2.107694327076779 	 ± 0.22697314457981754
	data : 0.11632404327392579
	model : 0.07036123275756836
			 train-loss:  2.1036416962742805 	 ± 0.2284082523755054
	data : 0.11573200225830078
	model : 0.07067313194274902
			 train-loss:  2.110445409645269 	 ± 0.23500955907983032
	data : 0.11569805145263672
	model : 0.07071847915649414
			 train-loss:  2.1160682454341795 	 ± 0.23899138337021977
	data : 0.11544322967529297
	model : 0.06973819732666016
			 train-loss:  2.12119318921882 	 ± 0.24203813775884866
	data : 0.11633338928222656
	model : 0.06963067054748535
			 train-loss:  2.1191398047265553 	 ± 0.24131931146014055
	data : 0.11638469696044922
	model : 0.06828532218933106
			 train-loss:  2.1182431725894704 	 ± 0.24003629735897072
	data : 0.11755151748657226
	model : 0.06718573570251465
			 train-loss:  2.1207434881565184 	 ± 0.23974744654985036
	data : 0.11843643188476563
	model : 0.06748838424682617
			 train-loss:  2.1208232052024756 	 ± 0.2383667517509495
	data : 0.11835680007934571
	model : 0.06737813949584961
			 train-loss:  2.1171085373921827 	 ± 0.23952772919626614
	data : 0.11854467391967774
	model : 0.06761474609375
			 train-loss:  2.1150665765398005 	 ± 0.23894729916428434
	data : 0.11839752197265625
	model : 0.06831068992614746
			 train-loss:  2.1131527105967205 	 ± 0.23830109198814398
	data : 0.11788411140441894
	model : 0.06899619102478027
			 train-loss:  2.1107055040506215 	 ± 0.2381225876612365
	data : 0.11722078323364257
	model : 0.06880269050598145
			 train-loss:  2.1088592034319174 	 ± 0.23747892505390053
	data : 0.11727848052978515
	model : 0.06894631385803222
			 train-loss:  2.1046884123997023 	 ± 0.2395625481255262
	data : 0.1170745849609375
	model : 0.06969971656799316
			 train-loss:  2.104135953365488 	 ± 0.23834442491806346
	data : 0.11630620956420898
	model : 0.07003993988037109
			 train-loss:  2.1032942658976506 	 ± 0.23722706087564455
	data : 0.11587591171264648
	model : 0.0700192928314209
			 train-loss:  2.1022717567781606 	 ± 0.23619861954935661
	data : 0.11576423645019532
	model : 0.06994404792785644
			 train-loss:  2.097696401409267 	 ± 0.2392159852368586
	data : 0.11591205596923829
	model : 0.06961722373962402
			 train-loss:  2.0981378494476783 	 ± 0.2380320761973543
	data : 0.11630091667175294
	model : 0.06956539154052735
			 train-loss:  2.0964668423238426 	 ± 0.23740386559299118
	data : 0.1162635326385498
	model : 0.06948823928833008
			 train-loss:  2.0938844299316406 	 ± 0.23760725150358214
	data : 0.11636142730712891
	model : 0.06875300407409668
			 train-loss:  2.0936098806928882 	 ± 0.23644399222276427
	data : 0.11719760894775391
	model : 0.06886067390441894
			 train-loss:  2.092218282175999 	 ± 0.2356973842606672
	data : 0.11707653999328613
	model : 0.06936211585998535
			 train-loss:  2.09055636692973 	 ± 0.2351502181399647
	data : 0.11653661727905273
	model : 0.06920347213745118
			 train-loss:  2.0914916762938867 	 ± 0.23420939596856524
	data : 0.11684050559997558
	model : 0.06878647804260254
			 train-loss:  2.089732391493661 	 ± 0.23378090245613564
	data : 0.11701035499572754
	model : 0.06966586112976074
			 train-loss:  2.0897422612838024 	 ± 0.23267557118715584
	data : 0.11610932350158691
	model : 0.06963238716125489
			 train-loss:  2.091883252714282 	 ± 0.232632429040197
	data : 0.11621928215026855
	model : 0.06935596466064453
			 train-loss:  2.0913096257933863 	 ± 0.2316289359820652
	data : 0.11659717559814453
	model : 0.06949739456176758
			 train-loss:  2.0937569261690894 	 ± 0.23196246738867174
	data : 0.11634469032287598
	model : 0.07018027305603028
			 train-loss:  2.0930201151154257 	 ± 0.23103378662079524
	data : 0.11598739624023438
	model : 0.07033824920654297
			 train-loss:  2.0897107607609517 	 ± 0.23259501516953687
	data : 0.11588950157165527
	model : 0.0704308032989502
			 train-loss:  2.0868253878184726 	 ± 0.2335412560514947
	data : 0.11568970680236816
	model : 0.07061347961425782
			 train-loss:  2.0896169126561257 	 ± 0.2343749611151283
	data : 0.11550579071044922
	model : 0.07056975364685059
			 train-loss:  2.091242468147947 	 ± 0.2339836761937373
	data : 0.11549663543701172
	model : 0.06966028213500977
			 train-loss:  2.094228974632595 	 ± 0.2351362992292389
	data : 0.11620564460754394
	model : 0.06942801475524903
			 train-loss:  2.09197900007511 	 ± 0.23536062447988265
	data : 0.11642522811889648
	model : 0.0693197250366211
			 train-loss:  2.092262669506236 	 ± 0.23437256572117535
	data : 0.11645631790161133
	model : 0.06922626495361328
			 train-loss:  2.0919056847944097 	 ± 0.2334092909685727
	data : 0.1166015625
	model : 0.0692293643951416
			 train-loss:  2.095988355764822 	 ± 0.23661980203446958
	data : 0.11664137840270997
	model : 0.06958427429199218
			 train-loss:  2.095977026224136 	 ± 0.23563185601032077
	data : 0.11616454124450684
	model : 0.0697568416595459
			 train-loss:  2.098534000806572 	 ± 0.23632199066900733
	data : 0.1160268783569336
	model : 0.06987853050231933
			 train-loss:  2.098031143673131 	 ± 0.23541645821879634
	data : 0.11593041419982911
	model : 0.06978793144226074
			 train-loss:  2.098343763894182 	 ± 0.23448295361729277
	data : 0.11593751907348633
	model : 0.06898999214172363
			 train-loss:  2.0983095015248945 	 ± 0.23353585301067542
	data : 0.11691832542419434
	model : 0.06831893920898438
			 train-loss:  2.0948258781433107 	 ± 0.23581242765282212
	data : 0.11765575408935547
	model : 0.06819229125976563
			 train-loss:  2.095367111856975 	 ± 0.23495273653188337
	data : 0.117832612991333
	model : 0.06830263137817383
			 train-loss:  2.093551883547325 	 ± 0.2349112546461522
	data : 0.11776385307312012
	model : 0.0685645580291748
			 train-loss:  2.0905718822032213 	 ± 0.23638948763560824
	data : 0.11771221160888672
	model : 0.06988139152526855
			 train-loss:  2.0900348304778107 	 ± 0.23554984608731688
	data : 0.11641516685485839
	model : 0.07135486602783203
			 train-loss:  2.0932700615662796 	 ± 0.23750187151684243
	data : 0.1153780460357666
	model : 0.07147531509399414
			 train-loss:  2.0936315059661865 	 ± 0.2366295281530225
	data : 0.11538124084472656
	model : 0.07116961479187012
			 train-loss:  2.091810682506272 	 ± 0.23665091866514967
	data : 0.11561169624328613
	model : 0.07090396881103515
			 train-loss:  2.089822955597612 	 ± 0.23686307619744443
	data : 0.11581096649169922
	model : 0.07028441429138184
			 train-loss:  2.087311439549745 	 ± 0.2377485125855182
	data : 0.11621193885803223
	model : 0.06964001655578614
			 train-loss:  2.0864706613399364 	 ± 0.23706619746537363
	data : 0.11645793914794922
	model : 0.0689466953277588
			 train-loss:  2.0892406482906902 	 ± 0.2383757026594449
	data : 0.11686997413635254
	model : 0.0680307388305664
			 train-loss:  2.0892661548879023 	 ± 0.2375043112298356
	data : 0.11758184432983398
	model : 0.06819067001342774
			 train-loss:  2.0865213179933852 	 ± 0.2388131441594763
	data : 0.11742563247680664
	model : 0.06776652336120606
			 train-loss:  2.085757775272397 	 ± 0.23812154742543615
	data : 0.11807575225830078
	model : 0.06788606643676758
			 train-loss:  2.0864541360310147 	 ± 0.2374115869667372
	data : 0.1181248664855957
	model : 0.0682950496673584
			 train-loss:  2.0872175642784607 	 ± 0.23674059699631017
	data : 0.11777348518371582
	model : 0.06925759315490723
			 train-loss:  2.085888425229301 	 ± 0.23643288993939504
	data : 0.11694746017456055
	model : 0.06916294097900391
			 train-loss:  2.088363534920699 	 ± 0.2374437083425001
	data : 0.11692705154418945
	model : 0.06977581977844238
			 train-loss:  2.0898618292477398 	 ± 0.23729519360199575
	data : 0.11628236770629882
	model : 0.06982707977294922
			 train-loss:  2.08854430461752 	 ± 0.2370034522109902
	data : 0.1161203384399414
	model : 0.07002677917480468
			 train-loss:  2.088928010365734 	 ± 0.23623559075333264
	data : 0.11596570014953614
	model : 0.06994142532348632
			 train-loss:  2.0856049328434225 	 ± 0.23883020832511206
	data : 0.1160975456237793
	model : 0.0699422836303711
			 train-loss:  2.0833316622553646 	 ± 0.23961244644179439
	data : 0.11612186431884766
	model : 0.06990232467651367
			 train-loss:  2.08449782461128 	 ± 0.23922806090012014
	data : 0.11629056930541992
	model : 0.06978816986083984
			 train-loss:  2.0836615125338236 	 ± 0.23864774170374067
	data : 0.11654825210571289
	model : 0.06938481330871582
			 train-loss:  2.0839053345042347 	 ± 0.23787494928871655
	data : 0.11692471504211426
	model : 0.06948132514953613
			 train-loss:  2.0853195292385003 	 ± 0.23772719042292556
	data : 0.11683764457702636
	model : 0.06948795318603515
			 train-loss:  2.0869517365312267 	 ± 0.2378019898926017
	data : 0.11676964759826661
	model : 0.0696408748626709
			 train-loss:  2.0854186973014435 	 ± 0.23778595839862932
	data : 0.11653718948364258
	model : 0.06958484649658203
			 train-loss:  2.0853735285420574 	 ± 0.2370183285379753
	data : 0.11650810241699219
	model : 0.0698664665222168
			 train-loss:  2.0854527789812822 	 ± 0.23625949330322155
	data : 0.11621489524841308
	model : 0.06985177993774414
			 train-loss:  2.0868671054293397 	 ± 0.2361674529501676
	data : 0.11625528335571289
	model : 0.06891369819641113
			 train-loss:  2.0865755888480173 	 ± 0.2354472366543925
	data : 0.11700878143310547
	model : 0.0692636489868164
			 train-loss:  2.0839220187948935 	 ± 0.23706391016372358
	data : 0.11672406196594239
	model : 0.06889209747314454
			 train-loss:  2.083477358520031 	 ± 0.2363884299443868
	data : 0.11720213890075684
	model : 0.06830973625183105
			 train-loss:  2.0835144386528444 	 ± 0.23565362765063216
	data : 0.11767354011535644
	model : 0.06843595504760742
			 train-loss:  2.082049685495871 	 ± 0.23565921155425268
	data : 0.11755118370056153
	model : 0.0689004898071289
			 train-loss:  2.083577633635398 	 ± 0.235738766822715
	data : 0.11730995178222656
	model : 0.0679628849029541
			 train-loss:  2.0859992511388734 	 ± 0.23704383046181784
	data : 0.11797752380371093
	model : 0.06832294464111328
			 train-loss:  2.0872552156448365 	 ± 0.23687113529382595
	data : 0.11758370399475097
	model : 0.06896982192993165
			 train-loss:  2.0899053086717445 	 ± 0.23859742031716377
	data : 0.11709074974060059
	model : 0.06823506355285644
			 train-loss:  2.0889766159171828 	 ± 0.23818272071641333
	data : 0.11767158508300782
	model : 0.06798505783081055
			 train-loss:  2.0876596797080267 	 ± 0.23808182517208087
	data : 0.1178051471710205
	model : 0.06810102462768555
			 train-loss:  2.085634596954436 	 ± 0.23882318706638828
	data : 0.11772446632385254
	model : 0.06827917098999023
			 train-loss:  2.084196808758904 	 ± 0.23885218958165788
	data : 0.11763515472412109
	model : 0.06853313446044922
			 train-loss:  2.0844295275838753 	 ± 0.23817209628948777
	data : 0.1174513816833496
	model : 0.06910276412963867
			 train-loss:  2.0849538334580355 	 ± 0.2375776773140958
	data : 0.11698241233825683
	model : 0.06975102424621582
			 train-loss:  2.0843515995609967 	 ± 0.23702167346898767
	data : 0.11641287803649902
	model : 0.06970996856689453
			 train-loss:  2.0833317281185897 	 ± 0.23671997832313757
	data : 0.11649808883666993
	model : 0.06952061653137206
			 train-loss:  2.0825824546813965 	 ± 0.23624949924312183
	data : 0.11663188934326171
	model : 0.0692744255065918
			 train-loss:  2.081980064511299 	 ± 0.23571212276281528
	data : 0.11690454483032227
	model : 0.06933116912841797
			 train-loss:  2.0838308657630016 	 ± 0.23632432638898798
	data : 0.11691980361938477
	model : 0.06946687698364258
			 train-loss:  2.0854165192400473 	 ± 0.2366018983954582
	data : 0.11675267219543457
	model : 0.0690910816192627
			 train-loss:  2.08548590058055 	 ± 0.23594188942370742
	data : 0.11713232994079589
	model : 0.06911487579345703
			 train-loss:  2.085694434907701 	 ± 0.23530212370043224
	data : 0.11710643768310547
	model : 0.06879034042358398
			 train-loss:  2.0864106620872875 	 ± 0.23484788809650292
	data : 0.1172910213470459
	model : 0.06879839897155762
			 train-loss:  2.0874785420658823 	 ± 0.2346420587004539
	data : 0.11734557151794434
	model : 0.06874704360961914
			 train-loss:  2.0910913110430775 	 ± 0.23902200840049084
	data : 0.11738619804382325
	model : 0.0694498062133789
			 train-loss:  2.090482242729353 	 ± 0.23851396092547078
	data : 0.11664514541625977
	model : 0.0695307731628418
			 train-loss:  2.0907817337964034 	 ± 0.23790314373415528
	data : 0.11651768684387206
	model : 0.06997084617614746
			 train-loss:  2.0924413127283894 	 ± 0.23833410036186367
	data : 0.11622347831726074
	model : 0.06923766136169433
			 train-loss:  2.0925744548838408 	 ± 0.23770292480580077
	data : 0.11678357124328613
	model : 0.06932673454284669
			 train-loss:  2.091709247294893 	 ± 0.2373649500847272
	data : 0.1167999267578125
	model : 0.06844010353088378
			 train-loss:  2.0922616259761586 	 ± 0.23685729047509882
	data : 0.11759772300720214
	model : 0.06831583976745606
			 train-loss:  2.0923730423575955 	 ± 0.23623812525338864
	data : 0.11769747734069824
	model : 0.06800131797790528
			 train-loss:  2.0908919504175634 	 ± 0.23650169409458957
	data : 0.11787872314453125
	model : 0.06874685287475586
			 train-loss:  2.089190679912766 	 ± 0.23705389410040165
	data : 0.11742377281188965
	model : 0.067787504196167
			 train-loss:  2.0868041935362345 	 ± 0.2387402063973015
	data : 0.11845407485961915
	model : 0.06790432929992676
			 train-loss:  2.085742878545191 	 ± 0.23858013526670185
	data : 0.11832633018493652
	model : 0.06783571243286132
			 train-loss:  2.08547966724787 	 ± 0.23799584323657147
	data : 0.1184840202331543
	model : 0.06775064468383789
			 train-loss:  2.0844165713203195 	 ± 0.23785166641895616
	data : 0.1186147689819336
	model : 0.06774959564208985
			 train-loss:  2.0851321105424523 	 ± 0.2374586107343832
	data : 0.11846795082092285
	model : 0.06866683959960937
			 train-loss:  2.086172300757784 	 ± 0.23730774154025955
	data : 0.11762032508850098
	model : 0.06994185447692872
			 train-loss:  2.085692853783842 	 ± 0.23680685920487504
	data : 0.11644835472106933
	model : 0.06937193870544434
			 train-loss:  2.085388791561127 	 ± 0.23625304111665185
	data : 0.11677274703979493
	model : 0.06885838508605957
			 train-loss:  2.085218168609771 	 ± 0.23567696710380406
	data : 0.11718745231628418
	model : 0.0688467025756836
			 train-loss:  2.0858793223258294 	 ± 0.23527967680063724
	data : 0.11707949638366699
	model : 0.06789436340332031
			 train-loss:  2.085377046627364 	 ± 0.23480799561505866
	data : 0.11769342422485352
	model : 0.06756205558776855
			 train-loss:  2.0857563632376053 	 ± 0.2342941187145138
	data : 0.11819543838500976
	model : 0.06816349029541016
			 train-loss:  2.0866306275856203 	 ± 0.2340553027263967
	data : 0.11760225296020507
	model : 0.06879363059997559
			 train-loss:  2.089515614856794 	 ± 0.23711220829766066
	data : 0.11691036224365234
	model : 0.06869077682495117
			 train-loss:  2.088793984933752 	 ± 0.23676543021265373
	data : 0.11698327064514161
	model : 0.06962752342224121
			 train-loss:  2.087227731370009 	 ± 0.237268122711934
	data : 0.11624879837036133
	model : 0.06953573226928711
			 train-loss:  2.0864958044444544 	 ± 0.2369350788069422
	data : 0.11623644828796387
	model : 0.06974987983703614
			 train-loss:  2.086754527546111 	 ± 0.23639986595697016
	data : 0.11637415885925292
	model : 0.06901302337646484
			 train-loss:  2.0846533877024718 	 ± 0.23779643586359783
	data : 0.11710500717163086
	model : 0.0690335750579834
			 train-loss:  2.0837332038384564 	 ± 0.2376111842576399
	data : 0.11715269088745117
	model : 0.06923189163208007
			 train-loss:  2.0824061962360507 	 ± 0.2378388740945003
	data : 0.1168339729309082
	model : 0.06915955543518067
			 train-loss:  2.0822717619833546 	 ± 0.2372906364981293
	data : 0.11686515808105469
	model : 0.06911392211914062
			 train-loss:  2.0815448650093966 	 ± 0.23697684910771696
	data : 0.11671962738037109
	model : 0.06916222572326661
			 train-loss:  2.0795039269659252 	 ± 0.23831408516522376
	data : 0.11685385704040527
	model : 0.06938276290893555
			 train-loss:  2.079334946821362 	 ± 0.23777731018946888
	data : 0.11656103134155274
	model : 0.06916689872741699
			 train-loss:  2.0791172576606822 	 ± 0.23725299515716733
	data : 0.11670098304748536
	model : 0.0691025733947754
			 train-loss:  2.07743538841265 	 ± 0.2380096837035739
	data : 0.11693816184997559
	model : 0.06904888153076172
			 train-loss:  2.077921541712501 	 ± 0.23757709363224078
	data : 0.11703281402587891
	model : 0.0696329116821289
			 train-loss:  2.0780305306835953 	 ± 0.23704449178053852
	data : 0.11632738113403321
	model : 0.06952333450317383
			 train-loss:  2.079958184882327 	 ± 0.2382397648840035
	data : 0.11673712730407715
	model : 0.06935806274414062
			 train-loss:  2.0796756129628338 	 ± 0.2377422776242417
	data : 0.11704673767089843
	model : 0.06935315132141114
			 train-loss:  2.07957442051598 	 ± 0.23721582247756776
	data : 0.1168982982635498
	model : 0.06853857040405273
			 train-loss:  2.077738486395942 	 ± 0.2382777330711563
	data : 0.11743278503417968
	model : 0.0685053825378418
			 train-loss:  2.078862539434855 	 ± 0.23834710410633234
	data : 0.11753177642822266
	model : 0.06812453269958496
			 train-loss:  2.0796583644093922 	 ± 0.23812226873473322
	data : 0.11758756637573242
	model : 0.06806120872497559
			 train-loss:  2.080016557584729 	 ± 0.237660778371536
	data : 0.11767077445983887
	model : 0.06711101531982422
			 train-loss:  2.0794222932715605 	 ± 0.2373110083533746
	data : 0.11834192276000977
	model : 0.06738262176513672
			 train-loss:  2.078523774250694 	 ± 0.23718461231008459
	data : 0.1180002212524414
	model : 0.0671121597290039
			 train-loss:  2.077237325829345 	 ± 0.2374734593686238
	data : 0.11801662445068359
	model : 0.06712841987609863
			 train-loss:  2.0775634517957426 	 ± 0.23701294602807116
	data : 0.11786055564880371
	model : 0.06699190139770508
			 train-loss:  2.078054174844799 	 ± 0.23662186985109032
	data : 0.11781210899353027
	model : 0.06800169944763183
			 train-loss:  2.0779075474820585 	 ± 0.23612633393620264
	data : 0.11724190711975098
	model : 0.06795625686645508
			 train-loss:  2.078685678827002 	 ± 0.23592386864223194
	data : 0.11756095886230469
	model : 0.0685335636138916
			 train-loss:  2.0794587807130007 	 ± 0.23572161624873336
	data : 0.11710495948791504
	model : 0.06802506446838379
			 train-loss:  2.07987513773552 	 ± 0.2353107341090757
	data : 0.11777572631835938
	model : 0.06772956848144532
			 train-loss:  2.0778775049858735 	 ± 0.2368211285048858
	data : 0.11810946464538574
	model : 0.06739301681518554
			 train-loss:  2.076288044702059 	 ± 0.23759390601593847
	data : 0.11829590797424316
	model : 0.06771616935729981
			 train-loss:  2.077737482388814 	 ± 0.23815490773466175
	data : 0.11813678741455078
	model : 0.06689615249633789
			 train-loss:  2.079209531491228 	 ± 0.2387519204732639
	data : 0.11902594566345215
	model : 0.06748852729797364
			 train-loss:  2.078671024357977 	 ± 0.23840473918810054
	data : 0.1182168960571289
	model : 0.06757316589355469
			 train-loss:  2.079965390786222 	 ± 0.23876424842750588
	data : 0.1179814338684082
	model : 0.06742677688598633
			 train-loss:  2.0801051273697713 	 ± 0.2382844316684468
	data : 0.11793274879455566
	model : 0.06696510314941406
			 train-loss:  2.079681756058518 	 ± 0.23788958109266004
	data : 0.11793727874755859
	model : 0.06745495796203613
			 train-loss:  2.0796148743086715 	 ± 0.23740788142487554
	data : 0.11741633415222168
	model : 0.06721882820129395
			 train-loss:  2.0788994231204754 	 ± 0.23719239872695067
	data : 0.118007230758667
	model : 0.0674856185913086
			 train-loss:  2.078358733365613 	 ± 0.23686618067603324
	data : 0.11782126426696778
	model : 0.06773190498352051
			 train-loss:  2.079255782456762 	 ± 0.23681180033334276
	data : 0.11758942604064941
	model : 0.0674527645111084
			 train-loss:  2.0802553124427794 	 ± 0.23686341095672564
	data : 0.11807231903076172
	model : 0.06730308532714843
			 train-loss:  2.0802386229731646 	 ± 0.23639124788133026
	data : 0.11810250282287597
	model : 0.06769986152648926
			 train-loss:  2.0809462236033545 	 ± 0.23618795088352337
	data : 0.11771116256713868
	model : 0.06791749000549316
			 train-loss:  2.0800027781324424 	 ± 0.2361960151754358
	data : 0.11777005195617676
	model : 0.06810526847839356
			 train-loss:  2.081637521428386 	 ± 0.23716034905123282
	data : 0.11772036552429199
	model : 0.06876254081726074
			 train-loss:  2.083145574494904 	 ± 0.2379119907056214
	data : 0.11697945594787598
	model : 0.06866211891174316
			 train-loss:  2.085281648673117 	 ± 0.23988441090808352
	data : 0.11624422073364257
	model : 0.0599301815032959
#epoch  49    val-loss:  2.423621416091919  train-loss:  2.085281648673117  lr:  0.00015625
			 train-loss:  1.9738703966140747 	 ± 0.0
	data : 5.56089448928833
	model : 0.07339024543762207
			 train-loss:  2.0841874480247498 	 ± 0.11031705141067505
	data : 2.8680431842803955
	model : 0.07202839851379395
			 train-loss:  1.9357616901397705 	 ± 0.22841551121167605
	data : 1.9504177570343018
	model : 0.07183170318603516
			 train-loss:  1.9990575313568115 	 ± 0.22616216497833383
	data : 1.4916154146194458
	model : 0.0709802508354187
			 train-loss:  1.993839144706726 	 ± 0.20255464973944398
	data : 1.2167076587677002
	model : 0.07055172920227051
			 train-loss:  2.170094390710195 	 ± 0.4353388111512771
	data : 0.1279381275177002
	model : 0.0694662094116211
			 train-loss:  2.178916811943054 	 ± 0.4036243582230643
	data : 0.11637759208679199
	model : 0.06836142539978027
			 train-loss:  2.1861003190279007 	 ± 0.37803407802786015
	data : 0.1175187110900879
	model : 0.06743092536926269
			 train-loss:  2.172116345829434 	 ± 0.3586018872208049
	data : 0.11808247566223144
	model : 0.06781249046325684
			 train-loss:  2.120217967033386 	 ± 0.3741346785068353
	data : 0.11777420043945312
	model : 0.06797976493835449
			 train-loss:  2.1516324173320425 	 ± 0.37029754893825473
	data : 0.1175529956817627
	model : 0.06825313568115235
			 train-loss:  2.1700363159179688 	 ± 0.3597489728991281
	data : 0.11741127967834472
	model : 0.06833152770996094
			 train-loss:  2.189649691948524 	 ± 0.3522502024934746
	data : 0.11724395751953125
	model : 0.06904444694519044
			 train-loss:  2.192816598074777 	 ± 0.33962878880794195
	data : 0.11675143241882324
	model : 0.06944584846496582
			 train-loss:  2.1808568954467775 	 ± 0.3311500406039164
	data : 0.11645712852478027
	model : 0.07001514434814453
			 train-loss:  2.1721783578395844 	 ± 0.3223915829856123
	data : 0.11603837013244629
	model : 0.070277738571167
			 train-loss:  2.1546115174013027 	 ± 0.3205619019463796
	data : 0.1156301498413086
	model : 0.0710218906402588
			 train-loss:  2.1634653011957803 	 ± 0.3136617112449378
	data : 0.11502285003662109
	model : 0.07003374099731445
			 train-loss:  2.169708879370438 	 ± 0.30644291815163793
	data : 0.1159024715423584
	model : 0.06934809684753418
			 train-loss:  2.169210159778595 	 ± 0.2986915209449732
	data : 0.11650347709655762
	model : 0.06792445182800293
			 train-loss:  2.1781611896696544 	 ± 0.2942288780989657
	data : 0.11772313117980956
	model : 0.06777663230895996
			 train-loss:  2.172064033421603 	 ± 0.28881877630882313
	data : 0.11796736717224121
	model : 0.0677915096282959
			 train-loss:  2.1587588787078857 	 ± 0.28928202095482547
	data : 0.11787004470825195
	model : 0.06780061721801758
			 train-loss:  2.147528181473414 	 ± 0.28826759712279737
	data : 0.11797556877136231
	model : 0.06804547309875489
			 train-loss:  2.14724347114563 	 ± 0.28244685285591564
	data : 0.11775832176208496
	model : 0.06875782012939453
			 train-loss:  2.153328143633329 	 ± 0.2786278689149981
	data : 0.11704063415527344
	model : 0.06861815452575684
			 train-loss:  2.158837150644373 	 ± 0.27485860552418284
	data : 0.11720261573791504
	model : 0.06849513053894044
			 train-loss:  2.14372627224241 	 ± 0.2810947888082618
	data : 0.1173248291015625
	model : 0.06943435668945312
			 train-loss:  2.1342309137870528 	 ± 0.2807386351068126
	data : 0.1164628505706787
	model : 0.06847872734069824
			 train-loss:  2.128571383158366 	 ± 0.2776975336370294
	data : 0.11745920181274414
	model : 0.06774625778198243
			 train-loss:  2.1186275366813905 	 ± 0.27855827458930504
	data : 0.11807975769042969
	model : 0.06803197860717773
			 train-loss:  2.1073296442627907 	 ± 0.28129484735353366
	data : 0.11785326004028321
	model : 0.06823759078979492
			 train-loss:  2.1053595037171333 	 ± 0.27722412614768516
	data : 0.11767315864562988
	model : 0.06800642013549804
			 train-loss:  2.11990591357736 	 ± 0.28561436014861136
	data : 0.11783666610717773
	model : 0.06865277290344238
			 train-loss:  2.1247785908835275 	 ± 0.28293478722137505
	data : 0.11710557937622071
	model : 0.06871771812438965
			 train-loss:  2.131340503692627 	 ± 0.2816655446397814
	data : 0.1171999454498291
	model : 0.06780204772949219
			 train-loss:  2.130707193065334 	 ± 0.27785916552402334
	data : 0.11783409118652344
	model : 0.06786069869995118
			 train-loss:  2.1320381541001168 	 ± 0.27429825085121423
	data : 0.11781826019287109
	model : 0.06716713905334473
			 train-loss:  2.1221931255780735 	 ± 0.27747692873842894
	data : 0.11833457946777344
	model : 0.06750879287719727
			 train-loss:  2.1190591037273405 	 ± 0.2746846764164909
	data : 0.11793293952941894
	model : 0.06829853057861328
			 train-loss:  2.1185887034346416 	 ± 0.2713304956058921
	data : 0.11736836433410644
	model : 0.06839199066162109
			 train-loss:  2.1124994357426963 	 ± 0.2709014977106609
	data : 0.11739501953125
	model : 0.0684598445892334
			 train-loss:  2.1085400193236596 	 ± 0.268959784508028
	data : 0.11718826293945313
	model : 0.06911611557006836
			 train-loss:  2.1119925542311235 	 ± 0.26684798869987847
	data : 0.11668572425842286
	model : 0.06823844909667968
			 train-loss:  2.1151866065131295 	 ± 0.26471558153276176
	data : 0.11748042106628417
	model : 0.06791625022888184
			 train-loss:  2.105481134808582 	 ± 0.2697958710833368
	data : 0.11756210327148438
	model : 0.0686373233795166
			 train-loss:  2.1083361975690154 	 ± 0.26761176409690407
	data : 0.11708989143371581
	model : 0.06854887008666992
			 train-loss:  2.1033075923720994 	 ± 0.26704407503469596
	data : 0.11716842651367188
	model : 0.06880006790161133
			 train-loss:  2.100588012714775 	 ± 0.2649758357683645
	data : 0.11699423789978028
	model : 0.0687558650970459
			 train-loss:  2.1035398268699645 	 ± 0.26312524927086045
	data : 0.11727237701416016
	model : 0.06892824172973633
			 train-loss:  2.099154163809384 	 ± 0.2623719722678962
	data : 0.11711502075195312
	model : 0.06897368431091308
			 train-loss:  2.096052254621799 	 ± 0.26077948113859245
	data : 0.11712541580200195
	model : 0.06810851097106933
			 train-loss:  2.1004768295108147 	 ± 0.26027063559816244
	data : 0.11798319816589356
	model : 0.06733107566833496
			 train-loss:  2.0967891216278076 	 ± 0.25924332482378803
	data : 0.11862211227416992
	model : 0.06816391944885254
			 train-loss:  2.0953465244986793 	 ± 0.25709440452112564
	data : 0.11762409210205078
	model : 0.06827249526977539
			 train-loss:  2.0962277395384654 	 ± 0.25487237874427016
	data : 0.11757111549377441
	model : 0.06823530197143554
			 train-loss:  2.092319505256519 	 ± 0.2543140619255574
	data : 0.11762681007385253
	model : 0.06911444664001465
			 train-loss:  2.094699839065815 	 ± 0.25275186683998796
	data : 0.11686601638793945
	model : 0.06983380317687989
			 train-loss:  2.0967222149089233 	 ± 0.2510736050138886
	data : 0.11612811088562011
	model : 0.06988759040832519
			 train-loss:  2.09871324300766 	 ± 0.24944179667730088
	data : 0.1161494255065918
	model : 0.0700235366821289
			 train-loss:  2.100528107314813 	 ± 0.24778784078701524
	data : 0.1159907341003418
	model : 0.06978673934936523
			 train-loss:  2.096558324752315 	 ± 0.24772932845912068
	data : 0.1160649299621582
	model : 0.0696873664855957
			 train-loss:  2.0982707719954234 	 ± 0.24612498762565962
	data : 0.11618247032165527
	model : 0.06873040199279785
			 train-loss:  2.1014120243489742 	 ± 0.24546412431806397
	data : 0.1171121597290039
	model : 0.06865100860595703
			 train-loss:  2.09601188989786 	 ± 0.24737017933695538
	data : 0.11718535423278809
	model : 0.06821489334106445
			 train-loss:  2.09897618040894 	 ± 0.2466495696530421
	data : 0.11767840385437012
	model : 0.0677940845489502
			 train-loss:  2.099433744131629 	 ± 0.2448302026275017
	data : 0.11797089576721191
	model : 0.06788740158081055
			 train-loss:  2.097636482294868 	 ± 0.24346817184932543
	data : 0.11798214912414551
	model : 0.06923365592956543
			 train-loss:  2.1010409058004185 	 ± 0.24332240776318018
	data : 0.1168025016784668
	model : 0.06932568550109863
			 train-loss:  2.104152059555054 	 ± 0.24295651302462035
	data : 0.11658649444580078
	model : 0.06986913681030274
			 train-loss:  2.1036601536710497 	 ± 0.24127458798520043
	data : 0.11606125831604004
	model : 0.07040190696716309
			 train-loss:  2.103173464536667 	 ± 0.2396283048784487
	data : 0.11555395126342774
	model : 0.07033352851867676
			 train-loss:  2.1011537754372376 	 ± 0.23859761923661693
	data : 0.11552486419677735
	model : 0.07003941535949706
			 train-loss:  2.103154446627643 	 ± 0.23759568913326767
	data : 0.11593918800354004
	model : 0.07023720741271973
			 train-loss:  2.0959206708272298 	 ± 0.24407223914362616
	data : 0.11590466499328614
	model : 0.07012500762939453
			 train-loss:  2.098787044223986 	 ± 0.24372860483177924
	data : 0.11602587699890136
	model : 0.07018160820007324
			 train-loss:  2.1020422693970917 	 ± 0.24379805243874134
	data : 0.11596155166625977
	model : 0.07029294967651367
			 train-loss:  2.105042393390949 	 ± 0.24365657912710678
	data : 0.11577262878417968
	model : 0.0710413932800293
			 train-loss:  2.1036201896546762 	 ± 0.2424351375955429
	data : 0.11497888565063477
	model : 0.07048802375793457
			 train-loss:  2.1027999833226203 	 ± 0.24102542879839897
	data : 0.11543893814086914
	model : 0.07026505470275879
			 train-loss:  2.1024282817487365 	 ± 0.2395560677263425
	data : 0.11567554473876954
	model : 0.07013397216796875
			 train-loss:  2.102751596671779 	 ± 0.23810865990214078
	data : 0.11578083038330078
	model : 0.07008624076843262
			 train-loss:  2.1006986965616066 	 ± 0.2373988911000528
	data : 0.11590213775634765
	model : 0.06838064193725586
			 train-loss:  2.100193922008787 	 ± 0.23602637633192372
	data : 0.11753058433532715
	model : 0.06859841346740722
			 train-loss:  2.0994594167260563 	 ± 0.23473042969334218
	data : 0.11734991073608399
	model : 0.06792173385620118
			 train-loss:  2.097757124623587 	 ± 0.23388888112414433
	data : 0.11792049407958985
	model : 0.06772689819335938
			 train-loss:  2.101248484918441 	 ± 0.23478401206445682
	data : 0.11804075241088867
	model : 0.06769499778747559
			 train-loss:  2.100783696228808 	 ± 0.2334864514837425
	data : 0.11797785758972168
	model : 0.06831912994384766
			 train-loss:  2.0988357562697337 	 ± 0.23288902493012487
	data : 0.11748261451721191
	model : 0.06849908828735352
			 train-loss:  2.097852263185713 	 ± 0.23177736582489566
	data : 0.11735720634460449
	model : 0.06850852966308593
			 train-loss:  2.099100858300597 	 ± 0.2308045027825412
	data : 0.11731014251708985
	model : 0.06884675025939942
			 train-loss:  2.096800699182179 	 ± 0.23059303017438715
	data : 0.11715154647827149
	model : 0.06853265762329101
			 train-loss:  2.0964404113831057 	 ± 0.22937596557409723
	data : 0.11757407188415528
	model : 0.06883435249328614
			 train-loss:  2.101779880675864 	 ± 0.23389108784831544
	data : 0.1173020362854004
	model : 0.06908192634582519
			 train-loss:  2.101708689488863 	 ± 0.2326578493145909
	data : 0.11685256958007813
	model : 0.0690908432006836
			 train-loss:  2.098987065255642 	 ± 0.23295817590799126
	data : 0.11685843467712402
	model : 0.06905064582824708
			 train-loss:  2.0999473321069146 	 ± 0.23194515518317851
	data : 0.11681056022644043
	model : 0.0694185733795166
			 train-loss:  2.0963489267290853 	 ± 0.23346433716610193
	data : 0.1163703441619873
	model : 0.06943311691284179
			 train-loss:  2.0972566038671165 	 ± 0.2324559639338357
	data : 0.11627755165100098
	model : 0.0691309928894043
			 train-loss:  2.093607431650162 	 ± 0.2341233579670238
	data : 0.11682267189025879
	model : 0.07002387046813965
			 train-loss:  2.0978175047600622 	 ± 0.23673510077934498
	data : 0.11595740318298339
	model : 0.06908955574035644
			 train-loss:  2.0988973928432837 	 ± 0.23582163550069554
	data : 0.11683187484741211
	model : 0.06810193061828614
			 train-loss:  2.0999042860512596 	 ± 0.23489430392840746
	data : 0.117669677734375
	model : 0.06791563034057617
			 train-loss:  2.097381896697558 	 ± 0.23515980547165632
	data : 0.11780195236206055
	model : 0.06803526878356933
			 train-loss:  2.0973173255012147 	 ± 0.23403824428341805
	data : 0.11778302192687988
	model : 0.06792154312133789
			 train-loss:  2.0967803991065836 	 ± 0.23299664231207898
	data : 0.11791901588439942
	model : 0.06895914077758789
			 train-loss:  2.0968188192242776 	 ± 0.23190565449596187
	data : 0.11705961227416992
	model : 0.06903624534606934
			 train-loss:  2.096905725973624 	 ± 0.2308312708851252
	data : 0.11722440719604492
	model : 0.06919751167297364
			 train-loss:  2.094068598309788 	 ± 0.23165397416717656
	data : 0.11703481674194335
	model : 0.06916875839233398
			 train-loss:  2.0931908986785195 	 ± 0.2307805928625328
	data : 0.11695523262023926
	model : 0.06837620735168456
			 train-loss:  2.0925805772747004 	 ± 0.22982784694289007
	data : 0.11760597229003907
	model : 0.06745686531066894
			 train-loss:  2.094674620245184 	 ± 0.22986074342137353
	data : 0.11836962699890137
	model : 0.06801862716674804
			 train-loss:  2.096390442510622 	 ± 0.22956071014896115
	data : 0.11777129173278808
	model : 0.06786074638366699
			 train-loss:  2.0963425855887565 	 ± 0.22855221337561504
	data : 0.11787071228027343
	model : 0.06768460273742676
			 train-loss:  2.0955871094828065 	 ± 0.22769925777497343
	data : 0.11818137168884277
	model : 0.06766505241394043
			 train-loss:  2.093757226549346 	 ± 0.2275633312659579
	data : 0.11829657554626465
	model : 0.06842927932739258
			 train-loss:  2.0948678143004065 	 ± 0.2269042469874402
	data : 0.11757130622863769
	model : 0.0678330898284912
			 train-loss:  2.0945614111625543 	 ± 0.22596504925655
	data : 0.11804203987121582
	model : 0.06795291900634766
			 train-loss:  2.09484497238608 	 ± 0.22503469547486266
	data : 0.11792821884155273
	model : 0.06776227951049804
			 train-loss:  2.096541122595469 	 ± 0.22485765094345042
	data : 0.11797518730163574
	model : 0.06867384910583496
			 train-loss:  2.0962251592273553 	 ± 0.22395330747704115
	data : 0.1170921802520752
	model : 0.06880350112915039
			 train-loss:  2.09850775804676 	 ± 0.22444245994550804
	data : 0.11701416969299316
	model : 0.06979880332946778
			 train-loss:  2.097390086670232 	 ± 0.22386886967441968
	data : 0.11610703468322754
	model : 0.06970033645629883
			 train-loss:  2.096591040011375 	 ± 0.2231403857771388
	data : 0.11623826026916503
	model : 0.06995410919189453
			 train-loss:  2.0994844398498533 	 ± 0.22456935961121272
	data : 0.11582159996032715
	model : 0.06948561668395996
			 train-loss:  2.1014283895492554 	 ± 0.2247298726152891
	data : 0.11648740768432617
	model : 0.0695070743560791
			 train-loss:  2.1008673784301037 	 ± 0.22393192399948536
	data : 0.11647539138793946
	model : 0.06986088752746582
			 train-loss:  2.1028584856539965 	 ± 0.22418125998617974
	data : 0.1162038803100586
	model : 0.07000060081481933
			 train-loss:  2.1017152468363443 	 ± 0.22368491612355212
	data : 0.11610870361328125
	model : 0.07005929946899414
			 train-loss:  2.103507641645578 	 ± 0.22375096233289804
	data : 0.11617732048034668
	model : 0.07059669494628906
			 train-loss:  2.1023932094792372 	 ± 0.22325719723771634
	data : 0.1156723976135254
	model : 0.07044920921325684
			 train-loss:  2.0996369556947188 	 ± 0.22463608398705465
	data : 0.1157607078552246
	model : 0.06989092826843261
			 train-loss:  2.0970410045824552 	 ± 0.22576869676224623
	data : 0.11629281044006348
	model : 0.06989364624023438
			 train-loss:  2.096349682380904 	 ± 0.22506595541225938
	data : 0.11633892059326172
	model : 0.06894550323486329
			 train-loss:  2.098082990999575 	 ± 0.22512673869820254
	data : 0.1173391342163086
	model : 0.06830878257751465
			 train-loss:  2.099325024029788 	 ± 0.22476130354719434
	data : 0.11788253784179688
	model : 0.06883888244628907
			 train-loss:  2.0974747795258124 	 ± 0.22497663143286653
	data : 0.11747136116027831
	model : 0.06906266212463379
			 train-loss:  2.0930050836093184 	 ± 0.2301841087911082
	data : 0.11730031967163086
	model : 0.06887979507446289
			 train-loss:  2.0919507038679055 	 ± 0.22968882378521724
	data : 0.11744246482849122
	model : 0.06975669860839843
			 train-loss:  2.091438227891922 	 ± 0.22894677584424458
	data : 0.11653127670288085
	model : 0.06997060775756836
			 train-loss:  2.0898403208306493 	 ± 0.22891557401018062
	data : 0.11619105339050292
	model : 0.06950497627258301
			 train-loss:  2.0883713339416072 	 ± 0.22877407311704562
	data : 0.11641569137573242
	model : 0.06946196556091308
			 train-loss:  2.0883097715311116 	 ± 0.22797394075201421
	data : 0.11639695167541504
	model : 0.06967315673828126
			 train-loss:  2.0891423556539745 	 ± 0.22739904848577097
	data : 0.1162184715270996
	model : 0.06890554428100586
			 train-loss:  2.0912787240127035 	 ± 0.22805905014208178
	data : 0.11689348220825195
	model : 0.06925392150878906
			 train-loss:  2.090862189253716 	 ± 0.22733202321167564
	data : 0.11675543785095215
	model : 0.06866497993469238
			 train-loss:  2.089868040311904 	 ± 0.22687569651615305
	data : 0.11751804351806641
	model : 0.0686450481414795
			 train-loss:  2.0877656646676965 	 ± 0.22754017559117057
	data : 0.11743474006652832
	model : 0.06790380477905274
			 train-loss:  2.0860420937506143 	 ± 0.2277426498118978
	data : 0.11803388595581055
	model : 0.06818194389343261
			 train-loss:  2.084743518034617 	 ± 0.22753504121568965
	data : 0.11783404350280761
	model : 0.0672980785369873
			 train-loss:  2.0822403146731143 	 ± 0.22884325819862827
	data : 0.11863679885864258
	model : 0.06808333396911621
			 train-loss:  2.079281819494147 	 ± 0.23096830761492196
	data : 0.11799964904785157
	model : 0.06792869567871093
			 train-loss:  2.0794495492199667 	 ± 0.23022155929514734
	data : 0.1183042049407959
	model : 0.06859354972839356
			 train-loss:  2.0800200329198466 	 ± 0.22958134038781047
	data : 0.11768116950988769
	model : 0.0690502643585205
			 train-loss:  2.077955799718057 	 ± 0.2302688557232824
	data : 0.1171638011932373
	model : 0.06908154487609863
			 train-loss:  2.075644979110131 	 ± 0.23132560029246166
	data : 0.11722970008850098
	model : 0.06896967887878418
			 train-loss:  2.076292952154852 	 ± 0.2307297014539484
	data : 0.11711125373840332
	model : 0.06966128349304199
			 train-loss:  2.0770579078529456 	 ± 0.23019801665673706
	data : 0.11618342399597167
	model : 0.06936783790588379
			 train-loss:  2.07585383811087 	 ± 0.22997155304148326
	data : 0.11634640693664551
	model : 0.06934709548950195
			 train-loss:  2.0768476739525794 	 ± 0.2295940282921623
	data : 0.11643948554992675
	model : 0.07019286155700684
			 train-loss:  2.0750657009041826 	 ± 0.2299871165208317
	data : 0.11555309295654297
	model : 0.06921014785766602
			 train-loss:  2.074866263218868 	 ± 0.2292901458310545
	data : 0.11655073165893555
	model : 0.06863489151000976
			 train-loss:  2.0751713236416776 	 ± 0.22861869411219463
	data : 0.1171755313873291
	model : 0.06908788681030273
			 train-loss:  2.0759944690436853 	 ± 0.22816277681482158
	data : 0.11700234413146973
	model : 0.06836791038513183
			 train-loss:  2.0740049232136117 	 ± 0.22889278745718725
	data : 0.11755719184875488
	model : 0.06831731796264648
			 train-loss:  2.073894607015403 	 ± 0.22820670945416
	data : 0.11748099327087402
	model : 0.06904363632202148
			 train-loss:  2.0711616306247826 	 ± 0.2302310506223665
	data : 0.1169154167175293
	model : 0.06891975402832032
			 train-loss:  2.073457243187087 	 ± 0.23145385187326584
	data : 0.11712231636047363
	model : 0.06876039505004883
			 train-loss:  2.071106135492494 	 ± 0.23277146066541834
	data : 0.11713118553161621
	model : 0.06937894821166993
			 train-loss:  2.070544171333313 	 ± 0.2322007812840803
	data : 0.11661410331726074
	model : 0.06943225860595703
			 train-loss:  2.0696500100587545 	 ± 0.23181418570629445
	data : 0.11661200523376465
	model : 0.0692934513092041
			 train-loss:  2.0722794103067974 	 ± 0.23368277222862802
	data : 0.11680369377136231
	model : 0.0694554328918457
			 train-loss:  2.0734585227304803 	 ± 0.2335189919472553
	data : 0.11653566360473633
	model : 0.06944026947021484
			 train-loss:  2.074992833466365 	 ± 0.23371988210913355
	data : 0.11656394004821777
	model : 0.06858038902282715
			 train-loss:  2.073472500528608 	 ± 0.23391243289204613
	data : 0.11729984283447266
	model : 0.0676501750946045
			 train-loss:  2.071620026095347 	 ± 0.2345307787849495
	data : 0.1180419921875
	model : 0.06799769401550293
			 train-loss:  2.0721523162335327 	 ± 0.2339739127119924
	data : 0.11760025024414063
	model : 0.06735477447509766
			 train-loss:  2.0717113406470653 	 ± 0.2333895065406075
	data : 0.11834053993225098
	model : 0.06747674942016602
			 train-loss:  2.0708762154232856 	 ± 0.2330032181475206
	data : 0.11833748817443848
	model : 0.06861233711242676
			 train-loss:  2.0715668631924524 	 ± 0.23253874465798263
	data : 0.11742324829101562
	model : 0.0688044548034668
			 train-loss:  2.0727935726471367 	 ± 0.2324787771002698
	data : 0.11726326942443847
	model : 0.06784553527832031
			 train-loss:  2.0735816477419258 	 ± 0.2320815286629404
	data : 0.11811847686767578
	model : 0.06848969459533691
			 train-loss:  2.0732877638822047 	 ± 0.2314805130021287
	data : 0.11756997108459473
	model : 0.06876115798950196
			 train-loss:  2.074264748588852 	 ± 0.2312286488371127
	data : 0.11722121238708497
	model : 0.06910891532897949
			 train-loss:  2.075648196323498 	 ± 0.23136516938214166
	data : 0.11686086654663086
	model : 0.06996717453002929
			 train-loss:  2.0770983087119235 	 ± 0.23158382835298555
	data : 0.11613860130310058
	model : 0.0709108829498291
			 train-loss:  2.077817276199871 	 ± 0.2311718374748411
	data : 0.11523351669311524
	model : 0.07000508308410644
			 train-loss:  2.076445652449385 	 ± 0.23131790606631572
	data : 0.11621079444885254
	model : 0.06891613006591797
			 train-loss:  2.077088602636226 	 ± 0.23087351312864177
	data : 0.11710319519042969
	model : 0.06823158264160156
			 train-loss:  2.076354736403415 	 ± 0.23048606671110625
	data : 0.11788501739501953
	model : 0.06842250823974609
			 train-loss:  2.0754386305184886 	 ± 0.2302284717989698
	data : 0.11760420799255371
	model : 0.0680119514465332
			 train-loss:  2.076551945259174 	 ± 0.23014304129451996
	data : 0.11801590919494628
	model : 0.06891765594482421
			 train-loss:  2.076138317276159 	 ± 0.22961758224358608
	data : 0.11719026565551757
	model : 0.06960115432739258
			 train-loss:  2.0758295391023776 	 ± 0.22906518953637658
	data : 0.11657485961914063
	model : 0.06971025466918945
			 train-loss:  2.077020127956684 	 ± 0.2290780981928659
	data : 0.11644110679626465
	model : 0.06919326782226562
			 train-loss:  2.077261342077839 	 ± 0.2285177943892406
	data : 0.11712203025817872
	model : 0.0699319839477539
			 train-loss:  2.078671281107791 	 ± 0.22879016187519777
	data : 0.11656537055969238
	model : 0.06991133689880372
			 train-loss:  2.0779136052035323 	 ± 0.2284593227100397
	data : 0.11638855934143066
	model : 0.06997561454772949
			 train-loss:  2.078507882865829 	 ± 0.22803795578810898
	data : 0.11626501083374023
	model : 0.06926417350769043
			 train-loss:  2.0793007093667986 	 ± 0.22774193494366965
	data : 0.1167372703552246
	model : 0.06940484046936035
			 train-loss:  2.0810796259647577 	 ± 0.22856346175276185
	data : 0.11642804145812988
	model : 0.0691227912902832
			 train-loss:  2.0797479117270745 	 ± 0.22877740708806732
	data : 0.11672430038452149
	model : 0.06824522018432617
			 train-loss:  2.077992783391417 	 ± 0.2295724938906531
	data : 0.11769495010375977
	model : 0.06827282905578613
			 train-loss:  2.0763618075380137 	 ± 0.23018509075902016
	data : 0.11785612106323243
	model : 0.06945748329162597
			 train-loss:  2.0750967456073295 	 ± 0.23033278012253716
	data : 0.1169008731842041
	model : 0.06926789283752441
			 train-loss:  2.0773562216064305 	 ± 0.23203926900569521
	data : 0.1171903133392334
	model : 0.06872425079345704
			 train-loss:  2.0791199195788104 	 ± 0.2328581218190163
	data : 0.11752753257751465
	model : 0.06949429512023926
			 train-loss:  2.0803061689321813 	 ± 0.2329238194618116
	data : 0.1166759967803955
	model : 0.06925044059753419
			 train-loss:  2.0798029637222655 	 ± 0.23247922131162221
	data : 0.11701407432556152
	model : 0.06866230964660644
			 train-loss:  2.0793123233885993 	 ± 0.23203347985604703
	data : 0.11738781929016114
	model : 0.06798930168151855
			 train-loss:  2.079014752148452 	 ± 0.23152314638600227
	data : 0.11791033744812011
	model : 0.06848287582397461
			 train-loss:  2.0782063907047488 	 ± 0.23127472992575634
	data : 0.11739745140075683
	model : 0.06932206153869629
			 train-loss:  2.079330029062262 	 ± 0.23131049850285534
	data : 0.11642398834228515
	model : 0.06859288215637208
			 train-loss:  2.0805237983988825 	 ± 0.23142616236087918
	data : 0.11681532859802246
	model : 0.06883358955383301
			 train-loss:  2.082371729473735 	 ± 0.2324644899124127
	data : 0.11675515174865722
	model : 0.06984777450561523
			 train-loss:  2.0820652367892087 	 ± 0.23196929045173162
	data : 0.11579351425170899
	model : 0.06977887153625488
			 train-loss:  2.081580324656403 	 ± 0.23154388503772616
	data : 0.11589312553405762
	model : 0.06903138160705566
			 train-loss:  2.0823131963747357 	 ± 0.23126433443643488
	data : 0.11679744720458984
	model : 0.06924209594726563
			 train-loss:  2.0820151583789146 	 ± 0.23077768761523004
	data : 0.11658749580383301
	model : 0.06925864219665527
			 train-loss:  2.082520745017312 	 ± 0.23037412624100473
	data : 0.11647882461547851
	model : 0.0689615249633789
			 train-loss:  2.0816379095094777 	 ± 0.23022502002448114
	data : 0.11690187454223633
	model : 0.06822781562805176
			 train-loss:  2.081099668064633 	 ± 0.22984522971490304
	data : 0.11755409240722656
	model : 0.06817402839660644
			 train-loss:  2.0810813433386284 	 ± 0.22932946515449842
	data : 0.1173630714416504
	model : 0.06861715316772461
			 train-loss:  2.0807233452796936 	 ± 0.22887944014222722
	data : 0.11712069511413574
	model : 0.06806726455688476
			 train-loss:  2.082191416422526 	 ± 0.2294248123364416
	data : 0.11765036582946778
	model : 0.06791348457336426
			 train-loss:  2.0821610615316746 	 ± 0.22891712542003223
	data : 0.11763572692871094
	model : 0.06839103698730468
			 train-loss:  2.083216915046591 	 ± 0.2289632081589426
	data : 0.11724619865417481
	model : 0.06815576553344727
			 train-loss:  2.085317731949321 	 ± 0.23064273062790766
	data : 0.11735167503356933
	model : 0.0673293113708496
			 train-loss:  2.084429853868276 	 ± 0.2305287630928301
	data : 0.11774153709411621
	model : 0.06715807914733887
			 train-loss:  2.083905049510624 	 ± 0.23016412179763535
	data : 0.11761817932128907
	model : 0.0669947624206543
			 train-loss:  2.084286224790466 	 ± 0.2297381323403899
	data : 0.11744203567504882
	model : 0.06673593521118164
			 train-loss:  2.084787073320356 	 ± 0.2293688237563588
	data : 0.1175018310546875
	model : 0.0663536548614502
			 train-loss:  2.084590854051287 	 ± 0.22889559964828557
	data : 0.11791558265686035
	model : 0.06677618026733398
			 train-loss:  2.08513429073187 	 ± 0.22855656488389192
	data : 0.11760616302490234
	model : 0.06712799072265625
			 train-loss:  2.084574971807764 	 ± 0.22823018547554766
	data : 0.11741490364074707
	model : 0.06738505363464356
			 train-loss:  2.0833321531950415 	 ± 0.2285416415898033
	data : 0.1175124168395996
	model : 0.06748623847961426
			 train-loss:  2.083600412944198 	 ± 0.22809620803916686
	data : 0.11776432991027833
	model : 0.06771731376647949
			 train-loss:  2.0825898532106093 	 ± 0.22814755603475248
	data : 0.11766066551208496
	model : 0.0679936408996582
			 train-loss:  2.0826913148289443 	 ± 0.22767514029720856
	data : 0.11758904457092285
	model : 0.0674717903137207
			 train-loss:  2.083181660870711 	 ± 0.22732675003524377
	data : 0.11813688278198242
	model : 0.0669865608215332
			 train-loss:  2.0832817242847934 	 ± 0.22685992389136841
	data : 0.11853694915771484
	model : 0.0671036720275879
			 train-loss:  2.0837935844728768 	 ± 0.22653013094868613
	data : 0.11819400787353515
	model : 0.06706995964050293
			 train-loss:  2.081568430480643 	 ± 0.2286983619824541
	data : 0.1183852195739746
	model : 0.06668295860290527
			 train-loss:  2.0823188521822944 	 ± 0.22852882906162514
	data : 0.11894516944885254
	model : 0.06680464744567871
			 train-loss:  2.0816476286674033 	 ± 0.2283028530594051
	data : 0.11876845359802246
	model : 0.06746530532836914
			 train-loss:  2.081371105783354 	 ± 0.22787945889156613
	data : 0.11813907623291016
	model : 0.06708617210388183
			 train-loss:  2.0818767914405236 	 ± 0.22755596091706132
	data : 0.11839003562927246
	model : 0.06684484481811523
			 train-loss:  2.08308641660598 	 ± 0.22789104148842898
	data : 0.11840744018554687
	model : 0.06733431816101074
			 train-loss:  2.084085132223535 	 ± 0.22797613592777993
	data : 0.1177248477935791
	model : 0.06704902648925781
			 train-loss:  2.0831616525650025 	 ± 0.22798591389294776
	data : 0.11821064949035645
	model : 0.06657986640930176
			 train-loss:  2.082304577428506 	 ± 0.2279345068241916
	data : 0.11851563453674316
	model : 0.06678438186645508
			 train-loss:  2.082485702302721 	 ± 0.22749990448915874
	data : 0.11839461326599121
	model : 0.06684160232543945
			 train-loss:  2.082743021810479 	 ± 0.22708659651148153
	data : 0.11821374893188477
	model : 0.06651468276977539
			 train-loss:  2.083272821321262 	 ± 0.22679574812402292
	data : 0.11844410896301269
	model : 0.0671966552734375
			 train-loss:  2.083034091837266 	 ± 0.226382588223707
	data : 0.11757912635803222
	model : 0.06734442710876465
			 train-loss:  2.0834934320300817 	 ± 0.22605903624276394
	data : 0.11658244132995606
	model : 0.058800268173217776
#epoch  50    val-loss:  2.4430024749354313  train-loss:  2.0834934320300817  lr:  0.00015625
			 train-loss:  2.142914295196533 	 ± 0.0
	data : 5.351368188858032
	model : 0.0731053352355957
			 train-loss:  2.015503764152527 	 ± 0.12741053104400635
	data : 2.7814940214157104
	model : 0.06939136981964111
			 train-loss:  2.1309683322906494 	 ± 0.19361412294713962
	data : 1.8941218852996826
	model : 0.0687100092569987
			 train-loss:  2.1250191926956177 	 ± 0.16799106595056207
	data : 1.4499563574790955
	model : 0.06814819574356079
			 train-loss:  2.0324661254882814 	 ± 0.23841367324318422
	data : 1.1837876796722413
	model : 0.06790270805358886
			 train-loss:  2.037645777066549 	 ± 0.217948870785752
	data : 0.13718819618225098
	model : 0.06694140434265136
			 train-loss:  2.060534749712263 	 ± 0.20942583908981413
	data : 0.11833295822143555
	model : 0.06786155700683594
			 train-loss:  2.1063534915447235 	 ± 0.23037422607668465
	data : 0.11759891510009765
	model : 0.067543363571167
			 train-loss:  2.0986511442396374 	 ± 0.2182887372806025
	data : 0.1180570125579834
	model : 0.06727147102355957
			 train-loss:  2.075136959552765 	 ± 0.21877209025513086
	data : 0.11829843521118164
	model : 0.06822547912597657
			 train-loss:  2.0672561905600806 	 ± 0.21007444393416977
	data : 0.11731181144714356
	model : 0.06793656349182128
			 train-loss:  2.059020350376765 	 ± 0.20297730339500047
	data : 0.11748318672180176
	model : 0.06697149276733398
			 train-loss:  2.044777650099534 	 ± 0.20115870735226585
	data : 0.11838841438293457
	model : 0.06725592613220215
			 train-loss:  2.041749494416373 	 ± 0.19414862170740577
	data : 0.1181112289428711
	model : 0.06814107894897461
			 train-loss:  2.0397525628407798 	 ± 0.18771415269909572
	data : 0.11745147705078125
	model : 0.0677109718322754
			 train-loss:  2.040866196155548 	 ± 0.18180461522116168
	data : 0.11805753707885742
	model : 0.06826939582824706
			 train-loss:  2.062045686385211 	 ± 0.19566747741258464
	data : 0.11765308380126953
	model : 0.0688901424407959
			 train-loss:  2.071519520547655 	 ± 0.19412517875970106
	data : 0.11705136299133301
	model : 0.06945924758911133
			 train-loss:  2.0817504807522424 	 ± 0.19386926434378818
	data : 0.11661872863769532
	model : 0.06958050727844238
			 train-loss:  2.076556998491287 	 ± 0.1903115865527997
	data : 0.1164433479309082
	model : 0.06972851753234863
			 train-loss:  2.0578126793815974 	 ± 0.20376653179288803
	data : 0.11631627082824707
	model : 0.06995272636413574
			 train-loss:  2.0556796464053066 	 ± 0.19932144098143598
	data : 0.1161574363708496
	model : 0.06971154212951661
			 train-loss:  2.049345057943593 	 ± 0.19719148472704173
	data : 0.11646857261657714
	model : 0.06895575523376465
			 train-loss:  2.033322920401891 	 ± 0.207770545111202
	data : 0.11710925102233886
	model : 0.06895313262939454
			 train-loss:  2.0414315605163575 	 ± 0.20741228629788586
	data : 0.11711239814758301
	model : 0.06799135208129883
			 train-loss:  2.03670504459968 	 ± 0.20475288876940256
	data : 0.1180117130279541
	model : 0.06849217414855957
			 train-loss:  2.05117220348782 	 ± 0.21403920199291943
	data : 0.1174394130706787
	model : 0.06942791938781738
			 train-loss:  2.047900216920035 	 ± 0.2108688418674964
	data : 0.1163930892944336
	model : 0.07012972831726075
			 train-loss:  2.0415817211414207 	 ± 0.2098814533184969
	data : 0.11579041481018067
	model : 0.06991229057312012
			 train-loss:  2.0426751136779786 	 ± 0.20643777111302233
	data : 0.11594953536987304
	model : 0.07002849578857422
			 train-loss:  2.0425387428652857 	 ± 0.20308220927718978
	data : 0.11577873229980469
	model : 0.0693586826324463
			 train-loss:  2.051392950117588 	 ± 0.2058734217949698
	data : 0.11646771430969238
	model : 0.06888031959533691
			 train-loss:  2.066229314515085 	 ± 0.21941575354781395
	data : 0.11683402061462403
	model : 0.0688509464263916
			 train-loss:  2.0627551920273723 	 ± 0.2170842893510979
	data : 0.11688928604125977
	model : 0.0683053970336914
			 train-loss:  2.0652448313576834 	 ± 0.21445252534758683
	data : 0.11735334396362304
	model : 0.06907868385314941
			 train-loss:  2.072812100251516 	 ± 0.21614026314609502
	data : 0.1167947769165039
	model : 0.06811952590942383
			 train-loss:  2.065244574804564 	 ± 0.21798080207030557
	data : 0.11766524314880371
	model : 0.06730751991271973
			 train-loss:  2.0669707530423214 	 ± 0.2153496397302884
	data : 0.11867756843566894
	model : 0.0673288345336914
			 train-loss:  2.069852190139966 	 ± 0.2133116373182755
	data : 0.11856703758239746
	model : 0.06807646751403809
			 train-loss:  2.064994585514069 	 ± 0.21280170308382992
	data : 0.11787185668945313
	model : 0.06812186241149902
			 train-loss:  2.068588716227834 	 ± 0.2114161164091637
	data : 0.117901611328125
	model : 0.06917219161987305
			 train-loss:  2.0705053238641646 	 ± 0.2092442929789497
	data : 0.11699624061584472
	model : 0.07007355690002441
			 train-loss:  2.0611597216406534 	 ± 0.21548376410001915
	data : 0.11601238250732422
	model : 0.07042407989501953
			 train-loss:  2.05499452623454 	 ± 0.21682335131271802
	data : 0.11568942070007324
	model : 0.0703826904296875
			 train-loss:  2.0505858500798544 	 ± 0.21638588021516347
	data : 0.11583762168884278
	model : 0.07027812004089355
			 train-loss:  2.052199988261513 	 ± 0.21429467101278313
	data : 0.11591939926147461
	model : 0.06973085403442383
			 train-loss:  2.053955319079947 	 ± 0.21233669579271697
	data : 0.1163217544555664
	model : 0.06880955696105957
			 train-loss:  2.0684168711304665 	 ± 0.23232945887527884
	data : 0.11712465286254883
	model : 0.06751036643981934
			 train-loss:  2.0774823183916054 	 ± 0.23836979365435093
	data : 0.11823511123657227
	model : 0.06748833656311035
			 train-loss:  2.0845680689811705 	 ± 0.24113054672882125
	data : 0.11826162338256836
	model : 0.06661720275878906
			 train-loss:  2.0836087839276183 	 ± 0.23885115593281894
	data : 0.11893563270568848
	model : 0.06700420379638672
			 train-loss:  2.0797477800112505 	 ± 0.23814499077028722
	data : 0.11860995292663574
	model : 0.0677295207977295
			 train-loss:  2.0766462290062093 	 ± 0.23694556232014774
	data : 0.11798858642578125
	model : 0.06781001091003418
			 train-loss:  2.0684047076437206 	 ± 0.2422878708582929
	data : 0.11806073188781738
	model : 0.06756181716918945
			 train-loss:  2.064019296386025 	 ± 0.2422283974213675
	data : 0.11827192306518555
	model : 0.06763358116149902
			 train-loss:  2.0666580902678624 	 ± 0.24085226527588263
	data : 0.11821012496948242
	model : 0.06685004234313965
			 train-loss:  2.067287409514712 	 ± 0.23877662375305997
	data : 0.11893606185913086
	model : 0.06710104942321778
			 train-loss:  2.0647373816062666 	 ± 0.23749088804508126
	data : 0.11860551834106445
	model : 0.06811227798461914
			 train-loss:  2.0640752153881525 	 ± 0.23552364646511353
	data : 0.11759390830993652
	model : 0.06826109886169433
			 train-loss:  2.067532495657603 	 ± 0.23505760854518992
	data : 0.1173405647277832
	model : 0.0691110610961914
			 train-loss:  2.0653284280026547 	 ± 0.2337472617941488
	data : 0.11648440361022949
	model : 0.07023906707763672
			 train-loss:  2.067276591254819 	 ± 0.23235327323919064
	data : 0.11557612419128419
	model : 0.07015643119812012
			 train-loss:  2.067153039432707 	 ± 0.23050387620484625
	data : 0.11575894355773926
	model : 0.0692563533782959
			 train-loss:  2.0603466648608446 	 ± 0.2349902831211829
	data : 0.11656932830810547
	model : 0.0693892002105713
			 train-loss:  2.0628128877052894 	 ± 0.23400887240315404
	data : 0.11647286415100097
	model : 0.06947441101074218
			 train-loss:  2.0607740824872796 	 ± 0.2328103097411416
	data : 0.11643505096435547
	model : 0.0686370849609375
			 train-loss:  2.0668235661378547 	 ± 0.23623511212728374
	data : 0.11709837913513184
	model : 0.06859378814697266
			 train-loss:  2.0766911804676056 	 ± 0.24801232641172238
	data : 0.11717486381530762
	model : 0.06839127540588379
			 train-loss:  2.075658388759779 	 ± 0.24635583191039342
	data : 0.11717915534973145
	model : 0.0676088809967041
			 train-loss:  2.0772769740649633 	 ± 0.24495907066419262
	data : 0.11777634620666504
	model : 0.06736202239990234
			 train-loss:  2.078885530082273 	 ± 0.24359993408082187
	data : 0.11795940399169921
	model : 0.06816167831420898
			 train-loss:  2.079984368549453 	 ± 0.24207948486932468
	data : 0.11732969284057618
	model : 0.0676041603088379
			 train-loss:  2.074767911270873 	 ± 0.24445638884668802
	data : 0.11790046691894532
	model : 0.06865668296813965
			 train-loss:  2.0677049917143746 	 ± 0.25018586246539937
	data : 0.11712989807128907
	model : 0.06862378120422363
			 train-loss:  2.064772758483887 	 ± 0.24978919970177235
	data : 0.11733179092407227
	model : 0.06882920265197753
			 train-loss:  2.063905268907547 	 ± 0.24825410870386744
	data : 0.11712536811828614
	model : 0.0684356689453125
			 train-loss:  2.062532212827113 	 ± 0.2469271006895472
	data : 0.11739511489868164
	model : 0.06910247802734375
			 train-loss:  2.058861270928994 	 ± 0.2474447932894744
	data : 0.11694726943969727
	model : 0.06902275085449219
			 train-loss:  2.0545359699031973 	 ± 0.24882346732675517
	data : 0.11704454421997071
	model : 0.06969738006591797
			 train-loss:  2.053853526711464 	 ± 0.24733781855377063
	data : 0.11651806831359864
	model : 0.06960549354553222
			 train-loss:  2.051756618935385 	 ± 0.2465207882689821
	data : 0.11662044525146484
	model : 0.0697319507598877
			 train-loss:  2.052012349047312 	 ± 0.24502381157896094
	data : 0.11652626991271972
	model : 0.06966381072998047
			 train-loss:  2.058651230421411 	 ± 0.2508534712778204
	data : 0.11647439002990723
	model : 0.06875252723693848
			 train-loss:  2.0626858941146304 	 ± 0.2520504790673171
	data : 0.11728172302246094
	model : 0.06824078559875488
			 train-loss:  2.064637729700874 	 ± 0.25120121495653314
	data : 0.11742577552795411
	model : 0.06838402748107911
			 train-loss:  2.0611907742744267 	 ± 0.2517503391638403
	data : 0.11727633476257324
	model : 0.0684091567993164
			 train-loss:  2.0602288780541254 	 ± 0.2504582177576851
	data : 0.11713833808898926
	model : 0.06830635070800781
			 train-loss:  2.064815479246053 	 ± 0.25267903927047947
	data : 0.11726064682006836
	model : 0.06922154426574707
			 train-loss:  2.061649977491143 	 ± 0.2530041770201792
	data : 0.11638860702514649
	model : 0.06991539001464844
			 train-loss:  2.0613967405425178 	 ± 0.25160601437755575
	data : 0.11585588455200195
	model : 0.0689394474029541
			 train-loss:  2.0644105122639584 	 ± 0.2518479193572374
	data : 0.11673917770385742
	model : 0.06882843971252442
			 train-loss:  2.064061422710833 	 ± 0.2504975769962791
	data : 0.11669120788574219
	model : 0.06877803802490234
			 train-loss:  2.0646925677535353 	 ± 0.24922071125190373
	data : 0.11657767295837403
	model : 0.06796112060546874
			 train-loss:  2.059792735475175 	 ± 0.2523548793532053
	data : 0.1175732135772705
	model : 0.06805768013000488
			 train-loss:  2.061067419303091 	 ± 0.25132721876945346
	data : 0.11769595146179199
	model : 0.06845955848693848
			 train-loss:  2.060330310215553 	 ± 0.2501180013764783
	data : 0.117303466796875
	model : 0.0685361385345459
			 train-loss:  2.059700368605938 	 ± 0.24890193180249806
	data : 0.11743416786193847
	model : 0.06863560676574706
			 train-loss:  2.064165794119543 	 ± 0.2515038549701179
	data : 0.1173008918762207
	model : 0.06927762031555176
			 train-loss:  2.065174517005381 	 ± 0.2504295809294013
	data : 0.11662263870239258
	model : 0.06915912628173829
			 train-loss:  2.0636432790756225 	 ± 0.24963963932954705
	data : 0.11668415069580078
	model : 0.06971993446350097
			 train-loss:  2.064086642595801 	 ± 0.24844028949120775
	data : 0.11626358032226562
	model : 0.06938419342041016
			 train-loss:  2.0658367100883934 	 ± 0.24784428678770667
	data : 0.11660871505737305
	model : 0.06848640441894531
			 train-loss:  2.0695210313334047 	 ± 0.24942931791650538
	data : 0.11754441261291504
	model : 0.06879158020019531
			 train-loss:  2.065988976221818 	 ± 0.2508021754316284
	data : 0.11722612380981445
	model : 0.06848006248474121
			 train-loss:  2.0661703745524087 	 ± 0.24961187722424252
	data : 0.11748456954956055
	model : 0.06833901405334472
			 train-loss:  2.065925953523168 	 ± 0.24844429723528952
	data : 0.11752495765686036
	model : 0.06934938430786133
			 train-loss:  2.06594838828684 	 ± 0.24728072519157288
	data : 0.11636853218078613
	model : 0.06942005157470703
			 train-loss:  2.064399532697819 	 ± 0.2466541346797657
	data : 0.11628742218017578
	model : 0.0684272289276123
			 train-loss:  2.06301094304531 	 ± 0.24594380767288046
	data : 0.11705899238586426
	model : 0.06902956962585449
			 train-loss:  2.06069482348182 	 ± 0.246014599270181
	data : 0.11664276123046875
	model : 0.06915812492370606
			 train-loss:  2.06342639257242 	 ± 0.24657390539008633
	data : 0.11657109260559081
	model : 0.06857333183288575
			 train-loss:  2.0647160911134312 	 ± 0.24584644423671753
	data : 0.11712565422058105
	model : 0.06891751289367676
			 train-loss:  2.0606412613286382 	 ± 0.248526210533298
	data : 0.1167677879333496
	model : 0.06986289024353028
			 train-loss:  2.0592934834329704 	 ± 0.24784822318852984
	data : 0.1162353515625
	model : 0.06959519386291504
			 train-loss:  2.058229370739149 	 ± 0.24702968455257837
	data : 0.1163564682006836
	model : 0.06937756538391113
			 train-loss:  2.0566373549658676 	 ± 0.24655439178343827
	data : 0.11672420501708984
	model : 0.0691685676574707
			 train-loss:  2.0559013022316828 	 ± 0.2456264423139539
	data : 0.11706504821777344
	model : 0.06959042549133301
			 train-loss:  2.0560468101905562 	 ± 0.24458850189539724
	data : 0.11678466796875
	model : 0.06937146186828613
			 train-loss:  2.054067028670752 	 ± 0.2445062812893544
	data : 0.1166928768157959
	model : 0.0692528247833252
			 train-loss:  2.0540313641230266 	 ± 0.24348568460302086
	data : 0.11678647994995117
	model : 0.06941170692443847
			 train-loss:  2.0534240352220774 	 ± 0.24256871074739483
	data : 0.11653008460998535
	model : 0.06953682899475097
			 train-loss:  2.052623796658438 	 ± 0.24173285666569666
	data : 0.11637773513793945
	model : 0.07015738487243653
			 train-loss:  2.051903607399483 	 ± 0.240879581081074
	data : 0.11570086479187011
	model : 0.07060179710388184
			 train-loss:  2.0502492391294047 	 ± 0.2406069165910189
	data : 0.11512794494628906
	model : 0.07017874717712402
			 train-loss:  2.052290307044983 	 ± 0.24071795451567438
	data : 0.11542043685913086
	model : 0.07002782821655273
			 train-loss:  2.0560611779727633 	 ± 0.24343928412963933
	data : 0.11567921638488769
	model : 0.06993236541748046
			 train-loss:  2.058123773477209 	 ± 0.24358179646521688
	data : 0.11585679054260253
	model : 0.06938052177429199
			 train-loss:  2.0582952657714486 	 ± 0.24263613626977473
	data : 0.11621341705322266
	model : 0.06906437873840332
			 train-loss:  2.058795765388844 	 ± 0.24176017916516498
	data : 0.11657843589782715
	model : 0.06893138885498047
			 train-loss:  2.056269361422612 	 ± 0.24253196664431004
	data : 0.11674485206604004
	model : 0.06880192756652832
			 train-loss:  2.0553381088125797 	 ± 0.24183770156882173
	data : 0.11655921936035156
	model : 0.06952900886535644
			 train-loss:  2.055398047873468 	 ± 0.2409208848785089
	data : 0.1157963752746582
	model : 0.06958599090576172
			 train-loss:  2.053595802837745 	 ± 0.24090497649426235
	data : 0.1158592700958252
	model : 0.06983327865600586
			 train-loss:  2.055287078245362 	 ± 0.24079564779676715
	data : 0.1157911777496338
	model : 0.07058286666870117
			 train-loss:  2.055912558237712 	 ± 0.24001139108706018
	data : 0.11522483825683594
	model : 0.07091574668884278
			 train-loss:  2.0573655840228584 	 ± 0.23972259293994597
	data : 0.11501917839050294
	model : 0.07030601501464843
			 train-loss:  2.0589785506255436 	 ± 0.23958564422065376
	data : 0.11569418907165527
	model : 0.06943349838256836
			 train-loss:  2.0579124568165215 	 ± 0.23904191636396266
	data : 0.11656908988952637
	model : 0.06923308372497558
			 train-loss:  2.0601054747327625 	 ± 0.23956969420692864
	data : 0.11662025451660156
	model : 0.06907215118408203
			 train-loss:  2.0611502391951424 	 ± 0.2390301380144446
	data : 0.11664619445800781
	model : 0.06898260116577148
			 train-loss:  2.0610105670090264 	 ± 0.23818673848961297
	data : 0.11661305427551269
	model : 0.06899213790893555
			 train-loss:  2.063577908865163 	 ± 0.23929638568712747
	data : 0.11650648117065429
	model : 0.07002062797546386
			 train-loss:  2.0628622586910543 	 ± 0.2386106606058628
	data : 0.11561975479125977
	model : 0.06976590156555176
			 train-loss:  2.065191985004478 	 ± 0.23940721671019027
	data : 0.11591219902038574
	model : 0.06974954605102539
			 train-loss:  2.064296474950067 	 ± 0.23882213682236633
	data : 0.11585764884948731
	model : 0.06978516578674317
			 train-loss:  2.066069837302378 	 ± 0.23895889489113
	data : 0.11606507301330567
	model : 0.06981873512268066
			 train-loss:  2.0685031616768867 	 ± 0.2399528802140219
	data : 0.11608772277832032
	model : 0.06968994140625
			 train-loss:  2.068320734275354 	 ± 0.23915108292457138
	data : 0.11619076728820801
	model : 0.07028512954711914
			 train-loss:  2.0676569074592335 	 ± 0.23848398643671131
	data : 0.11562151908874511
	model : 0.07063751220703125
			 train-loss:  2.0681794452667237 	 ± 0.23777327757013703
	data : 0.11530685424804688
	model : 0.07055878639221191
			 train-loss:  2.068298313001923 	 ± 0.2369891125983504
	data : 0.11536626815795899
	model : 0.07053165435791016
			 train-loss:  2.0666056761616156 	 ± 0.23712224376170968
	data : 0.11539974212646484
	model : 0.07038960456848145
			 train-loss:  2.071192674387514 	 ± 0.24301774840533666
	data : 0.11557345390319824
	model : 0.07088432312011719
			 train-loss:  2.0732805434759563 	 ± 0.24360027125062855
	data : 0.11496973037719727
	model : 0.0706787109375
			 train-loss:  2.071998696942483 	 ± 0.24333369783077624
	data : 0.11509132385253906
	model : 0.07078862190246582
			 train-loss:  2.0727898379166922 	 ± 0.24275243373589284
	data : 0.1148911952972412
	model : 0.07081594467163085
			 train-loss:  2.0730185136673556 	 ± 0.24199495740846153
	data : 0.1148231029510498
	model : 0.07110462188720704
			 train-loss:  2.0719732242294504 	 ± 0.24158323522432687
	data : 0.11452250480651856
	model : 0.07016563415527344
			 train-loss:  2.070994958937543 	 ± 0.2411360746933865
	data : 0.11559715270996093
	model : 0.07001409530639649
			 train-loss:  2.074780452251434 	 ± 0.24507479227551468
	data : 0.11576323509216309
	model : 0.06975393295288086
			 train-loss:  2.0740149946686643 	 ± 0.24450429057234174
	data : 0.11589493751525878
	model : 0.06932716369628907
			 train-loss:  2.074496371510588 	 ± 0.24382499621245549
	data : 0.11639742851257324
	model : 0.06898550987243653
			 train-loss:  2.0749457489493435 	 ± 0.243143199138797
	data : 0.11659445762634277
	model : 0.06904468536376954
			 train-loss:  2.073690900715386 	 ± 0.24292962543224988
	data : 0.1164924144744873
	model : 0.06920061111450196
			 train-loss:  2.0721976547530203 	 ± 0.24294612969778148
	data : 0.1164407730102539
	model : 0.06956973075866699
			 train-loss:  2.071236192461956 	 ± 0.24252791668893542
	data : 0.11618051528930665
	model : 0.06975159645080567
			 train-loss:  2.0702311485827325 	 ± 0.24214717601269453
	data : 0.115913724899292
	model : 0.07001171112060547
			 train-loss:  2.0698778281609216 	 ± 0.24146859625079303
	data : 0.11573185920715331
	model : 0.07014002799987792
			 train-loss:  2.069858728075874 	 ± 0.24075325945593448
	data : 0.11565909385681153
	model : 0.07003107070922851
			 train-loss:  2.0687003226841196 	 ± 0.24051602823325166
	data : 0.11596746444702148
	model : 0.06967396736145019
			 train-loss:  2.071505669961896 	 ± 0.24258516678400943
	data : 0.11628379821777343
	model : 0.06987347602844238
			 train-loss:  2.072462011908376 	 ± 0.242202025228289
	data : 0.11608715057373047
	model : 0.07009100914001465
			 train-loss:  2.07282395101007 	 ± 0.24154765024548566
	data : 0.11583933830261231
	model : 0.07038884162902832
			 train-loss:  2.0725317720709175 	 ± 0.24088320503274932
	data : 0.11574206352233887
	model : 0.06968164443969727
			 train-loss:  2.0736152805600847 	 ± 0.24061883346791432
	data : 0.11626958847045898
	model : 0.06962080001831054
			 train-loss:  2.072763768786734 	 ± 0.240198559615154
	data : 0.11640582084655762
	model : 0.06867246627807617
			 train-loss:  2.0740284226034995 	 ± 0.24010595805915474
	data : 0.11726465225219726
	model : 0.06734929084777833
			 train-loss:  2.0741571690259355 	 ± 0.2394366799087466
	data : 0.11841983795166015
	model : 0.06692514419555665
			 train-loss:  2.073813958540975 	 ± 0.23881082893000272
	data : 0.11866569519042969
	model : 0.06761202812194825
			 train-loss:  2.0725219196743434 	 ± 0.23877309629501664
	data : 0.118017578125
	model : 0.0679295539855957
			 train-loss:  2.0721240280741484 	 ± 0.23817242076162193
	data : 0.11774797439575195
	model : 0.06889300346374512
			 train-loss:  2.0723053950529833 	 ± 0.23752973275535502
	data : 0.11688437461853027
	model : 0.06996183395385742
			 train-loss:  2.072341683132401 	 ± 0.23688036125984527
	data : 0.11610116958618164
	model : 0.07007460594177246
			 train-loss:  2.0736500556054325 	 ± 0.23689789745566056
	data : 0.11599144935607911
	model : 0.07001399993896484
			 train-loss:  2.073141421498479 	 ± 0.23635748674856139
	data : 0.11598596572875977
	model : 0.06982426643371582
			 train-loss:  2.072199736231117 	 ± 0.23606898459039471
	data : 0.11629080772399902
	model : 0.06902046203613281
			 train-loss:  2.0720861346963892 	 ± 0.23544203565587604
	data : 0.11688928604125977
	model : 0.06871380805969238
			 train-loss:  2.072109801338074 	 ± 0.23481524815450525
	data : 0.1170041561126709
	model : 0.06857056617736816
			 train-loss:  2.070725407549944 	 ± 0.2349612200599676
	data : 0.11716084480285645
	model : 0.06867880821228027
			 train-loss:  2.0721150178658334 	 ± 0.23511949119461986
	data : 0.11716055870056152
	model : 0.06826777458190918
			 train-loss:  2.0730492925144617 	 ± 0.2348565307499601
	data : 0.1175508975982666
	model : 0.06956949234008789
			 train-loss:  2.0747602973133326 	 ± 0.235434641819535
	data : 0.11659731864929199
	model : 0.0699127197265625
			 train-loss:  2.0775709640176805 	 ± 0.23803159858582557
	data : 0.1164595603942871
	model : 0.06995606422424316
			 train-loss:  2.0810058110768033 	 ± 0.2421653002638054
	data : 0.11637439727783203
	model : 0.06996421813964844
			 train-loss:  2.081225134164859 	 ± 0.2415628819387703
	data : 0.11645712852478027
	model : 0.07029519081115723
			 train-loss:  2.0825723488720094 	 ± 0.24167919026557377
	data : 0.11595354080200196
	model : 0.0695845603942871
			 train-loss:  2.0811055910768848 	 ± 0.24193802853553342
	data : 0.1164675235748291
	model : 0.06874494552612305
			 train-loss:  2.081952231700974 	 ± 0.24161869341696482
	data : 0.11719369888305664
	model : 0.0677877426147461
			 train-loss:  2.084045746218619 	 ± 0.24280449588214048
	data : 0.11805667877197265
	model : 0.06776041984558105
			 train-loss:  2.082750517129898 	 ± 0.24288495046943312
	data : 0.11789374351501465
	model : 0.06817469596862794
			 train-loss:  2.082856867443863 	 ± 0.2422846739614401
	data : 0.11770806312561036
	model : 0.06745734214782714
			 train-loss:  2.0838988535475025 	 ± 0.2421352776782589
	data : 0.11836938858032227
	model : 0.06789598464965821
			 train-loss:  2.081795066448268 	 ± 0.24338182621853455
	data : 0.11772093772888184
	model : 0.06813244819641114
			 train-loss:  2.0838282809538 	 ± 0.24450673041946275
	data : 0.1177297592163086
	model : 0.06814875602722167
			 train-loss:  2.084090693404035 	 ± 0.24393843832011416
	data : 0.11765937805175782
	model : 0.06839656829833984
			 train-loss:  2.0811169321097216 	 ± 0.24704243628705916
	data : 0.1174163818359375
	model : 0.06954035758972169
			 train-loss:  2.0811802638325716 	 ± 0.24644666932856277
	data : 0.11667456626892089
	model : 0.0698091983795166
			 train-loss:  2.0810890071667156 	 ± 0.24585704151347446
	data : 0.1166463851928711
	model : 0.07045063972473145
			 train-loss:  2.080899674356269 	 ± 0.24528336106739737
	data : 0.11590752601623536
	model : 0.07042012214660645
			 train-loss:  2.0805408398310345 	 ± 0.24475363864151348
	data : 0.11599440574645996
	model : 0.07003121376037598
			 train-loss:  2.0792613888238844 	 ± 0.24487589859183134
	data : 0.11644172668457031
	model : 0.0693131923675537
			 train-loss:  2.077575298975099 	 ± 0.2455223154188086
	data : 0.11706728935241699
	model : 0.06955904960632324
			 train-loss:  2.0783210058167505 	 ± 0.24518581869358091
	data : 0.11693320274353028
	model : 0.06993036270141602
			 train-loss:  2.076934262413845 	 ± 0.2454481220089693
	data : 0.11662254333496094
	model : 0.06966476440429688
			 train-loss:  2.07670135220816 	 ± 0.2449003495484586
	data : 0.116827392578125
	model : 0.06874990463256836
			 train-loss:  2.0776509482551506 	 ± 0.2447292099926757
	data : 0.11736979484558105
	model : 0.06919636726379394
			 train-loss:  2.0757765846867717 	 ± 0.2457137440634597
	data : 0.11681938171386719
	model : 0.06899552345275879
			 train-loss:  2.075942917701301 	 ± 0.2451617772590117
	data : 0.11689267158508301
	model : 0.0687471866607666
			 train-loss:  2.0765915994774806 	 ± 0.24478884774047158
	data : 0.1170539379119873
	model : 0.06840047836303711
			 train-loss:  2.0771572448990563 	 ± 0.2443752836289534
	data : 0.11739988327026367
	model : 0.06921839714050293
			 train-loss:  2.0779316414535316 	 ± 0.24409217126096813
	data : 0.11663289070129394
	model : 0.06847500801086426
			 train-loss:  2.077306981559272 	 ± 0.243718771205868
	data : 0.11741304397583008
	model : 0.06769390106201172
			 train-loss:  2.0768915158216195 	 ± 0.24325048148848855
	data : 0.11815695762634278
	model : 0.06682429313659669
			 train-loss:  2.0770133079162667 	 ± 0.2427137186705063
	data : 0.1186638355255127
	model : 0.0670785903930664
			 train-loss:  2.07617310418023 	 ± 0.2425000171412926
	data : 0.11844706535339355
	model : 0.06653733253479004
			 train-loss:  2.078874430825225 	 ± 0.24533225613520526
	data : 0.11906061172485352
	model : 0.0667388916015625
			 train-loss:  2.079963794363753 	 ± 0.24533847642824838
	data : 0.11854205131530762
	model : 0.06715888977050781
			 train-loss:  2.081161856651306 	 ± 0.24546445308631115
	data : 0.11804327964782715
	model : 0.0679995059967041
			 train-loss:  2.0803021145699847 	 ± 0.2452717120764269
	data : 0.11751909255981445
	model : 0.06823077201843261
			 train-loss:  2.0798176920932274 	 ± 0.24484769472944992
	data : 0.11721787452697754
	model : 0.06858830451965332
			 train-loss:  2.0809957785007756 	 ± 0.24496955371042256
	data : 0.11678152084350586
	model : 0.06907625198364258
			 train-loss:  2.080531713777575 	 ± 0.2445427679344173
	data : 0.11646971702575684
	model : 0.06894860267639161
			 train-loss:  2.081914317966019 	 ± 0.24492447381659277
	data : 0.11648077964782715
	model : 0.06805367469787597
			 train-loss:  2.0824417726606383 	 ± 0.24453315006159393
	data : 0.11713962554931641
	model : 0.06791510581970214
			 train-loss:  2.0829165534770233 	 ± 0.2441203719237698
	data : 0.11738009452819824
	model : 0.06760997772216797
			 train-loss:  2.0851359271397025 	 ± 0.2459669823523814
	data : 0.1175379753112793
	model : 0.06670022010803223
			 train-loss:  2.0844903223625217 	 ± 0.2456478155376776
	data : 0.1181910514831543
	model : 0.06650643348693848
			 train-loss:  2.0856986256206738 	 ± 0.24583597606513521
	data : 0.11849031448364258
	model : 0.06662054061889648
			 train-loss:  2.0868352646608233 	 ± 0.2459470336222458
	data : 0.11831378936767578
	model : 0.06655297279357911
			 train-loss:  2.0873645693063736 	 ± 0.24557048065969095
	data : 0.11825108528137207
	model : 0.06653847694396972
			 train-loss:  2.089389914793592 	 ± 0.24706096246628956
	data : 0.11824336051940917
	model : 0.06704716682434082
			 train-loss:  2.088651315732436 	 ± 0.24681645770837768
	data : 0.11797232627868652
	model : 0.06733121871948242
			 train-loss:  2.0897532544508883 	 ± 0.2469038760811126
	data : 0.11785750389099121
	model : 0.06797475814819336
			 train-loss:  2.091446733377019 	 ± 0.24780753486224652
	data : 0.11742887496948243
	model : 0.06806697845458984
			 train-loss:  2.0920296401393657 	 ± 0.24746885356334797
	data : 0.11738796234130859
	model : 0.06780405044555664
			 train-loss:  2.0943572661740992 	 ± 0.24963825534357767
	data : 0.11778559684753417
	model : 0.06817688941955566
			 train-loss:  2.094807427421755 	 ± 0.24923243076935964
	data : 0.11736927032470704
	model : 0.06777443885803222
			 train-loss:  2.0931965348220642 	 ± 0.2500145843309103
	data : 0.11766219139099121
	model : 0.06747288703918457
			 train-loss:  2.093402933882901 	 ± 0.24953321225313538
	data : 0.11817445755004882
	model : 0.06730365753173828
			 train-loss:  2.091642632007599 	 ± 0.2505779793430673
	data : 0.11830415725708007
	model : 0.06744484901428223
			 train-loss:  2.0906173332753886 	 ± 0.2506032251108695
	data : 0.11795430183410645
	model : 0.0669281005859375
			 train-loss:  2.0910441350369227 	 ± 0.2501968911465903
	data : 0.11840872764587403
	model : 0.06737790107727051
			 train-loss:  2.090261740175632 	 ± 0.2500106382002856
	data : 0.11773276329040527
	model : 0.06729693412780761
			 train-loss:  2.0900167358203197 	 ± 0.24954843646942812
	data : 0.11762447357177734
	model : 0.06679301261901856
			 train-loss:  2.088848189279145 	 ± 0.24975397090529078
	data : 0.11808571815490723
	model : 0.06659717559814453
			 train-loss:  2.0865332074463367 	 ± 0.2519919970417294
	data : 0.11743931770324707
	model : 0.057899999618530276
#epoch  51    val-loss:  2.4700664344586825  train-loss:  2.0865332074463367  lr:  0.00015625
			 train-loss:  1.7937848567962646 	 ± 0.0
	data : 5.539575815200806
	model : 0.07203817367553711
			 train-loss:  2.197527527809143 	 ± 0.4037426710128784
	data : 2.8848929405212402
	model : 0.06882405281066895
			 train-loss:  2.17452343304952 	 ± 0.3312559026128114
	data : 1.96294109026591
	model : 0.06915680567423503
			 train-loss:  2.1960611939430237 	 ± 0.2892913413754178
	data : 1.501281201839447
	model : 0.06929963827133179
			 train-loss:  2.1504806518554687 	 ± 0.2743390738189007
	data : 1.2242222785949708
	model : 0.06933836936950684
			 train-loss:  2.103654623031616 	 ± 0.2714436547982624
	data : 0.13948240280151367
	model : 0.06818742752075195
			 train-loss:  2.0554222890308926 	 ± 0.27769381499209284
	data : 0.11717982292175293
	model : 0.06896090507507324
			 train-loss:  2.0208274722099304 	 ± 0.27541284007900996
	data : 0.1166919231414795
	model : 0.06821446418762207
			 train-loss:  2.013889273007711 	 ± 0.26040221872055874
	data : 0.11723489761352539
	model : 0.0674746036529541
			 train-loss:  1.9938870072364807 	 ± 0.2542227363690688
	data : 0.1180039882659912
	model : 0.06756072044372559
			 train-loss:  2.0001332651485098 	 ± 0.2431953438622699
	data : 0.11810083389282226
	model : 0.06756486892700195
			 train-loss:  1.976576805114746 	 ± 0.24559984847812036
	data : 0.11812262535095215
	model : 0.06770138740539551
			 train-loss:  1.9980780344742994 	 ± 0.24744085130815513
	data : 0.11797885894775391
	model : 0.06858372688293457
			 train-loss:  1.993478434426444 	 ± 0.23901600723421396
	data : 0.11718225479125977
	model : 0.06838679313659668
			 train-loss:  2.030047067006429 	 ± 0.268406004731674
	data : 0.11727204322814941
	model : 0.0683894157409668
			 train-loss:  2.0243946611881256 	 ± 0.2608034073551188
	data : 0.11712737083435058
	model : 0.06905264854431152
			 train-loss:  2.0569927552167107 	 ± 0.28463925531609113
	data : 0.11680989265441895
	model : 0.06901540756225585
			 train-loss:  2.066997316148546 	 ± 0.27967833861276703
	data : 0.11797661781311035
	model : 0.06884188652038574
			 train-loss:  2.0528256830416227 	 ± 0.27877977998557
	data : 0.11816301345825195
	model : 0.06934528350830078
			 train-loss:  2.066316431760788 	 ± 0.27801126445061924
	data : 0.1178673267364502
	model : 0.06859664916992188
			 train-loss:  2.05548103650411 	 ± 0.27560459039505875
	data : 0.1187096118927002
	model : 0.06777853965759277
			 train-loss:  2.039779641411521 	 ± 0.2787157491726958
	data : 0.11932315826416015
	model : 0.06757259368896484
			 train-loss:  2.0373164570849873 	 ± 0.27283410763025434
	data : 0.11835722923278809
	model : 0.06675558090209961
			 train-loss:  2.0495451192061105 	 ± 0.2734525045933942
	data : 0.11925015449523926
	model : 0.06686940193176269
			 train-loss:  2.057267217636108 	 ± 0.27058521888227016
	data : 0.11893491744995117
	model : 0.06762857437133789
			 train-loss:  2.047139722567338 	 ± 0.27011941935845907
	data : 0.11819577217102051
	model : 0.06838979721069335
			 train-loss:  2.038473619355096 	 ± 0.2687280163403391
	data : 0.11757726669311523
	model : 0.0678332805633545
			 train-loss:  2.049254838909422 	 ± 0.2697665387599838
	data : 0.11793303489685059
	model : 0.06786322593688965
			 train-loss:  2.043864866782879 	 ± 0.26660455368885644
	data : 0.11770620346069335
	model : 0.06773719787597657
			 train-loss:  2.047248601913452 	 ± 0.2627560884526983
	data : 0.11787018775939942
	model : 0.06758074760437012
			 train-loss:  2.0606086254119873 	 ± 0.2686416711903687
	data : 0.11795554161071778
	model : 0.06750993728637696
			 train-loss:  2.068684846162796 	 ± 0.2682071442573262
	data : 0.11802401542663574
	model : 0.06796021461486816
			 train-loss:  2.0633396452123467 	 ± 0.26583735459757224
	data : 0.11775360107421876
	model : 0.06882195472717285
			 train-loss:  2.0728975113700416 	 ± 0.2675922775439488
	data : 0.11698484420776367
	model : 0.06932954788208008
			 train-loss:  2.070052828107561 	 ± 0.2642629136321482
	data : 0.11652550697326661
	model : 0.06891794204711914
			 train-loss:  2.070171892642975 	 ± 0.2605676988948818
	data : 0.11681971549987794
	model : 0.06904559135437012
			 train-loss:  2.0655967931489685 	 ± 0.2584841325357291
	data : 0.1165933609008789
	model : 0.06931447982788086
			 train-loss:  2.06937669452868 	 ± 0.2560945637812029
	data : 0.11645255088806153
	model : 0.06919903755187988
			 train-loss:  2.0675223912948217 	 ± 0.25304828471701746
	data : 0.11645288467407226
	model : 0.06901307106018066
			 train-loss:  2.073736923933029 	 ± 0.25286121639905873
	data : 0.11669187545776367
	model : 0.06930160522460938
			 train-loss:  2.0703861946012916 	 ± 0.25065595433182686
	data : 0.11657323837280273
	model : 0.06850471496582031
			 train-loss:  2.068570892016093 	 ± 0.24792660454578713
	data : 0.11728091239929199
	model : 0.06846370697021484
			 train-loss:  2.0585100179494815 	 ± 0.25355356829730935
	data : 0.11734819412231445
	model : 0.06862349510192871
			 train-loss:  2.059332704002207 	 ± 0.2507137649333607
	data : 0.11742162704467773
	model : 0.0686941146850586
			 train-loss:  2.0590480830934315 	 ± 0.24791959461499483
	data : 0.11736712455749512
	model : 0.06808209419250488
			 train-loss:  2.0622977510742517 	 ± 0.24617709914696104
	data : 0.11784577369689941
	model : 0.06804475784301758
			 train-loss:  2.0710775370293475 	 ± 0.2507182255546234
	data : 0.11778316497802735
	model : 0.06819109916687012
			 train-loss:  2.072649416824182 	 ± 0.24832676238195034
	data : 0.11764860153198242
	model : 0.06808886528015137
			 train-loss:  2.080356089436278 	 ± 0.25151250855382906
	data : 0.11768250465393067
	model : 0.06740250587463378
			 train-loss:  2.0730701422691347 	 ± 0.2541545541591908
	data : 0.11821198463439941
	model : 0.06746964454650879
			 train-loss:  2.075398928978864 	 ± 0.2521886996278139
	data : 0.11813216209411621
	model : 0.0684974193572998
			 train-loss:  2.0733849658415866 	 ± 0.2501658215155621
	data : 0.1173734188079834
	model : 0.06851186752319335
			 train-loss:  2.080345034599304 	 ± 0.2528263061246629
	data : 0.11738877296447754
	model : 0.06824288368225098
			 train-loss:  2.077735671290645 	 ± 0.2511937129159404
	data : 0.11755647659301757
	model : 0.06899409294128418
			 train-loss:  2.08217961137945 	 ± 0.2510327957021501
	data : 0.11689181327819824
	model : 0.06970958709716797
			 train-loss:  2.087378902094705 	 ± 0.25175175663426713
	data : 0.11633162498474121
	model : 0.06947674751281738
			 train-loss:  2.084721933331406 	 ± 0.250324522703286
	data : 0.11649503707885742
	model : 0.06909270286560058
			 train-loss:  2.0816462800420563 	 ± 0.24924120986376438
	data : 0.11677608489990235
	model : 0.06956291198730469
			 train-loss:  2.081939590179314 	 ± 0.24713006517859373
	data : 0.11647329330444336
	model : 0.06996850967407227
			 train-loss:  2.080348227421443 	 ± 0.2453666541867313
	data : 0.1162684440612793
	model : 0.06931734085083008
			 train-loss:  2.0790264000658127 	 ± 0.24356244535649163
	data : 0.11679720878601074
	model : 0.06989736557006836
			 train-loss:  2.079965497216871 	 ± 0.24170155921102382
	data : 0.11629300117492676
	model : 0.07069487571716308
			 train-loss:  2.0798113251489307 	 ± 0.2397786926992088
	data : 0.11558060646057129
	model : 0.0705655574798584
			 train-loss:  2.0797645207494497 	 ± 0.23789833652864667
	data : 0.11566805839538574
	model : 0.07019920349121093
			 train-loss:  2.077405960743244 	 ± 0.236814138596176
	data : 0.11593685150146485
	model : 0.07091970443725586
			 train-loss:  2.079295470859065 	 ± 0.23550645688593802
	data : 0.11521263122558593
	model : 0.0704965591430664
			 train-loss:  2.0810427719087743 	 ± 0.23417297653930336
	data : 0.11558785438537597
	model : 0.0690384864807129
			 train-loss:  2.0835669023149155 	 ± 0.23336115662238063
	data : 0.11686153411865234
	model : 0.06820530891418457
			 train-loss:  2.079801212186399 	 ± 0.23373587661801004
	data : 0.11758599281311036
	model : 0.06817741394042968
			 train-loss:  2.082105801786695 	 ± 0.23284858706125106
	data : 0.1174546241760254
	model : 0.0672530174255371
			 train-loss:  2.083762680980521 	 ± 0.23161820212972542
	data : 0.11845259666442871
	model : 0.06722068786621094
			 train-loss:  2.087112835711903 	 ± 0.23172994100947555
	data : 0.11837091445922851
	model : 0.06853952407836914
			 train-loss:  2.0917607382552266 	 ± 0.23349214524645806
	data : 0.11718440055847168
	model : 0.06843137741088867
			 train-loss:  2.088315211437844 	 ± 0.23377013307362376
	data : 0.11739940643310547
	model : 0.06839895248413086
			 train-loss:  2.0910395860671995 	 ± 0.23338610251967168
	data : 0.11738591194152832
	model : 0.06929688453674317
			 train-loss:  2.0902419576519415 	 ± 0.23194846486670262
	data : 0.11669726371765136
	model : 0.06868128776550293
			 train-loss:  2.088610673879648 	 ± 0.23087579014389242
	data : 0.11738448143005371
	model : 0.0682898998260498
			 train-loss:  2.088197729526422 	 ± 0.22941966103775832
	data : 0.11802029609680176
	model : 0.0692746639251709
			 train-loss:  2.0850872933110103 	 ± 0.22961221873479246
	data : 0.11704187393188477
	model : 0.0684736728668213
			 train-loss:  2.0880258083343506 	 ± 0.2296625864779884
	data : 0.11766347885131836
	model : 0.06894850730895996
			 train-loss:  2.0864446810734125 	 ± 0.22867822163742793
	data : 0.11704106330871582
	model : 0.0687873363494873
			 train-loss:  2.089088878980497 	 ± 0.2285220668480634
	data : 0.11711287498474121
	model : 0.06885275840759278
			 train-loss:  2.0882210731506348 	 ± 0.22727715109007074
	data : 0.11681933403015136
	model : 0.06790390014648437
			 train-loss:  2.0850049399194264 	 ± 0.2278123694119592
	data : 0.11772966384887695
	model : 0.06846022605895996
			 train-loss:  2.080281944835887 	 ± 0.23056813893122818
	data : 0.11736521720886231
	model : 0.06811671257019043
			 train-loss:  2.07783942167149 	 ± 0.23032718234055663
	data : 0.11761841773986817
	model : 0.06877589225769043
			 train-loss:  2.072473454749447 	 ± 0.2343439426880913
	data : 0.11709537506103515
	model : 0.06957855224609374
			 train-loss:  2.0744795365767046 	 ± 0.23375873331650807
	data : 0.11619095802307129
	model : 0.06951365470886231
			 train-loss:  2.0780768153372775 	 ± 0.2348785514903986
	data : 0.11625156402587891
	model : 0.0698173999786377
			 train-loss:  2.0755186518033346 	 ± 0.2348135233874864
	data : 0.11599349975585938
	model : 0.06879549026489258
			 train-loss:  2.0769949417847853 	 ± 0.23393938159800376
	data : 0.11699504852294922
	model : 0.06875977516174317
			 train-loss:  2.0757020465705707 	 ± 0.23299116351017823
	data : 0.1170241355895996
	model : 0.06695046424865722
			 train-loss:  2.0727925890235492 	 ± 0.2334094013494931
	data : 0.11866278648376465
	model : 0.06702246665954589
			 train-loss:  2.072729709300589 	 ± 0.23216533424605718
	data : 0.11868171691894532
	model : 0.06700162887573242
			 train-loss:  2.076655018957038 	 ± 0.2340549581701518
	data : 0.11883707046508789
	model : 0.06789793968200683
			 train-loss:  2.0742572570840516 	 ± 0.23400269310725094
	data : 0.1179351806640625
	model : 0.06783194541931152
			 train-loss:  2.085076265728351 	 ± 0.2557921882208725
	data : 0.11800384521484375
	model : 0.06860823631286621
			 train-loss:  2.086197935805029 	 ± 0.254723446262024
	data : 0.11751770973205566
	model : 0.06947178840637207
			 train-loss:  2.0847522899357958 	 ± 0.25383744642186823
	data : 0.11655383110046387
	model : 0.06953306198120117
			 train-loss:  2.0833453822135923 	 ± 0.2529527115064312
	data : 0.1166189193725586
	model : 0.06938314437866211
			 train-loss:  2.080422590274622 	 ± 0.2533886934012439
	data : 0.11679339408874512
	model : 0.0693730354309082
			 train-loss:  2.0777092552652547 	 ± 0.2536137651957261
	data : 0.1167564868927002
	model : 0.06965632438659668
			 train-loss:  2.078318291497462 	 ± 0.2524545717647495
	data : 0.11634769439697265
	model : 0.06960396766662598
			 train-loss:  2.0765113154282937 	 ± 0.2519063362595713
	data : 0.11649270057678222
	model : 0.06967525482177735
			 train-loss:  2.0778057541166035 	 ± 0.2510512122814254
	data : 0.11646494865417481
	model : 0.06888971328735352
			 train-loss:  2.07500296831131 	 ± 0.25150936054425427
	data : 0.1170539379119873
	model : 0.06904687881469726
			 train-loss:  2.075685963452419 	 ± 0.2504300681714254
	data : 0.11691751480102539
	model : 0.0689765453338623
			 train-loss:  2.0771302768477686 	 ± 0.24971529584388377
	data : 0.11695876121520996
	model : 0.06908245086669922
			 train-loss:  2.074782756490445 	 ± 0.24976150968302782
	data : 0.11668753623962402
	model : 0.06914691925048828
			 train-loss:  2.078670917857777 	 ± 0.25191576668004806
	data : 0.11661419868469239
	model : 0.06994013786315918
			 train-loss:  2.0761704466364406 	 ± 0.25214596366178804
	data : 0.11587238311767578
	model : 0.0697723388671875
			 train-loss:  2.081569328904152 	 ± 0.2573817207290264
	data : 0.1159595012664795
	model : 0.06970558166503907
			 train-loss:  2.080027093929527 	 ± 0.2567596129940832
	data : 0.11602973937988281
	model : 0.06897592544555664
			 train-loss:  2.0812074688442967 	 ± 0.2559387548092974
	data : 0.11681995391845704
	model : 0.06882448196411133
			 train-loss:  2.080441006370213 	 ± 0.2549549206676421
	data : 0.1168245792388916
	model : 0.06903514862060547
			 train-loss:  2.0807913007407355 	 ± 0.25388139083183364
	data : 0.11687226295471191
	model : 0.06926479339599609
			 train-loss:  2.080622968510685 	 ± 0.25280060051286424
	data : 0.1165745735168457
	model : 0.06912403106689453
			 train-loss:  2.0806660288471286 	 ± 0.2517275632956018
	data : 0.11678647994995117
	model : 0.06985902786254883
			 train-loss:  2.08001342140326 	 ± 0.25076787687804153
	data : 0.1160573959350586
	model : 0.06984910964965821
			 train-loss:  2.0816781143347423 	 ± 0.25038023772684304
	data : 0.11612238883972167
	model : 0.06968150138854981
			 train-loss:  2.081121304803643 	 ± 0.2494180557133806
	data : 0.11633310317993165
	model : 0.06963248252868652
			 train-loss:  2.0793394221634163 	 ± 0.2491658914669461
	data : 0.11657242774963379
	model : 0.06893744468688964
			 train-loss:  2.0779780391755143 	 ± 0.2486061278045283
	data : 0.11711196899414063
	model : 0.06795501708984375
			 train-loss:  2.0773232550390306 	 ± 0.2477081238202664
	data : 0.11820683479309083
	model : 0.06732678413391113
			 train-loss:  2.0726018600463867 	 ± 0.2522550194814717
	data : 0.11868858337402344
	model : 0.06738557815551757
			 train-loss:  2.0754292427547396 	 ± 0.2532327701581254
	data : 0.11865768432617188
	model : 0.06738052368164063
			 train-loss:  2.0755700070088303 	 ± 0.2522387694285535
	data : 0.11862049102783204
	model : 0.06792335510253907
			 train-loss:  2.0723930904641747 	 ± 0.25378951247389164
	data : 0.11815271377563477
	model : 0.06890959739685058
			 train-loss:  2.0716782872991044 	 ± 0.2529332361319611
	data : 0.11721487045288086
	model : 0.06859350204467773
			 train-loss:  2.0731946028195893 	 ± 0.2525464378962361
	data : 0.11748113632202148
	model : 0.06779403686523437
			 train-loss:  2.0749778874957836 	 ± 0.2524009684623373
	data : 0.11826491355895996
	model : 0.06777534484863282
			 train-loss:  2.0709632264845297 	 ± 0.25560715694355296
	data : 0.1182408332824707
	model : 0.06790261268615723
			 train-loss:  2.070068561941161 	 ± 0.2548517882848122
	data : 0.11822648048400879
	model : 0.06789464950561523
			 train-loss:  2.0718366672743613 	 ± 0.2547165500194002
	data : 0.118255615234375
	model : 0.06881093978881836
			 train-loss:  2.0718846568354854 	 ± 0.25377200989223714
	data : 0.1174039363861084
	model : 0.06989073753356934
			 train-loss:  2.0764874921125522 	 ± 0.2584314694054607
	data : 0.11641573905944824
	model : 0.06989812850952148
			 train-loss:  2.0737800180476946 	 ± 0.2594152382976383
	data : 0.11617770195007324
	model : 0.07007532119750977
			 train-loss:  2.0746631345887114 	 ± 0.2586802221887202
	data : 0.11604175567626954
	model : 0.07002024650573731
			 train-loss:  2.074724133923757 	 ± 0.2577490342763934
	data : 0.11601142883300782
	model : 0.0706632137298584
			 train-loss:  2.0789016910961697 	 ± 0.26150689824883133
	data : 0.11522607803344727
	model : 0.06963601112365722
			 train-loss:  2.079896335060715 	 ± 0.260843547119879
	data : 0.11620025634765625
	model : 0.06899938583374024
			 train-loss:  2.0780640168928763 	 ± 0.2608325088071032
	data : 0.1169351577758789
	model : 0.06903462409973145
			 train-loss:  2.0769914046867743 	 ± 0.2602329894395056
	data : 0.11679511070251465
	model : 0.06957540512084961
			 train-loss:  2.07684418062369 	 ± 0.259333804458058
	data : 0.11649060249328613
	model : 0.0688619613647461
			 train-loss:  2.078219829756638 	 ± 0.2589646856420913
	data : 0.11721386909484863
	model : 0.06872396469116211
			 train-loss:  2.0775613850110197 	 ± 0.2581980624743383
	data : 0.11707706451416015
	model : 0.06922283172607421
			 train-loss:  2.0771355272150362 	 ± 0.25736978346790057
	data : 0.116737699508667
	model : 0.06922340393066406
			 train-loss:  2.076308973737665 	 ± 0.25669451156787476
	data : 0.11668386459350585
	model : 0.06871576309204101
			 train-loss:  2.077727996262928 	 ± 0.25641345535160415
	data : 0.11695151329040528
	model : 0.06881823539733886
			 train-loss:  2.0783372116088867 	 ± 0.2556654872591106
	data : 0.1170358657836914
	model : 0.0692380428314209
			 train-loss:  2.080593661756705 	 ± 0.25631171787818946
	data : 0.11659679412841797
	model : 0.07004723548889161
			 train-loss:  2.0793770167388415 	 ± 0.255904283240098
	data : 0.11575465202331543
	model : 0.06989579200744629
			 train-loss:  2.0805120647343154 	 ± 0.2554502090804414
	data : 0.11593470573425294
	model : 0.07012448310852051
			 train-loss:  2.080640816843355 	 ± 0.25462445500844627
	data : 0.11582803726196289
	model : 0.07001495361328125
			 train-loss:  2.078317358416896 	 ± 0.25543432504020536
	data : 0.1158970832824707
	model : 0.07060208320617675
			 train-loss:  2.07931741919273 	 ± 0.25491854622358234
	data : 0.11538305282592773
	model : 0.07079935073852539
			 train-loss:  2.08062020456715 	 ± 0.2546258603229338
	data : 0.11531648635864258
	model : 0.0709679126739502
			 train-loss:  2.083348581308051 	 ± 0.2561107165190516
	data : 0.11512956619262696
	model : 0.07069215774536133
			 train-loss:  2.0846369963771894 	 ± 0.255817217037361
	data : 0.11526522636413575
	model : 0.07074322700500488
			 train-loss:  2.0855097569525243 	 ± 0.25525388397022725
	data : 0.11514439582824706
	model : 0.07073311805725098
			 train-loss:  2.084988476326747 	 ± 0.2545453516050888
	data : 0.11518220901489258
	model : 0.06986327171325683
			 train-loss:  2.0851811957948003 	 ± 0.25377028357777853
	data : 0.11605958938598633
	model : 0.06975803375244141
			 train-loss:  2.0854142289951536 	 ± 0.25300803558976465
	data : 0.1161879539489746
	model : 0.06973738670349121
			 train-loss:  2.0835602116294023 	 ± 0.25334371021076174
	data : 0.11625289916992188
	model : 0.06980528831481933
			 train-loss:  2.084906234885707 	 ± 0.2531623571956598
	data : 0.11638789176940918
	model : 0.06985492706298828
			 train-loss:  2.0864951876272637 	 ± 0.2532225796506995
	data : 0.11642160415649414
	model : 0.07010383605957031
			 train-loss:  2.085060724241291 	 ± 0.25313887189501416
	data : 0.1162492275238037
	model : 0.07014050483703613
			 train-loss:  2.0835474658580053 	 ± 0.2531408420904439
	data : 0.11615219116210937
	model : 0.06930809020996094
			 train-loss:  2.0850687280914486 	 ± 0.25315984033853917
	data : 0.11691250801086425
	model : 0.06975884437561035
			 train-loss:  2.084457014588749 	 ± 0.25253939100000755
	data : 0.11637568473815918
	model : 0.06983137130737305
			 train-loss:  2.0855627450329517 	 ± 0.252212277395002
	data : 0.11619000434875489
	model : 0.06979932785034179
			 train-loss:  2.0853708206221113 	 ± 0.2514905566096221
	data : 0.11619091033935547
	model : 0.06979198455810547
			 train-loss:  2.08703627889556 	 ± 0.2517121228600689
	data : 0.11619625091552735
	model : 0.07059731483459472
			 train-loss:  2.0867280439398757 	 ± 0.2510205114577925
	data : 0.11555633544921876
	model : 0.07018303871154785
			 train-loss:  2.0903880255562917 	 ± 0.2549157578544648
	data : 0.11599888801574706
	model : 0.07008585929870606
			 train-loss:  2.0901645015586507 	 ± 0.2542077319177171
	data : 0.11619105339050292
	model : 0.06909046173095704
			 train-loss:  2.089513897895813 	 ± 0.2536355170863538
	data : 0.11709818840026856
	model : 0.06834344863891602
			 train-loss:  2.0891240024834534 	 ± 0.2529752415066621
	data : 0.11775522232055664
	model : 0.06851963996887207
			 train-loss:  2.0900501958484754 	 ± 0.25257007958079664
	data : 0.11740298271179199
	model : 0.06837353706359864
			 train-loss:  2.088340669208103 	 ± 0.2529038774513581
	data : 0.1176377773284912
	model : 0.06745309829711914
			 train-loss:  2.087307690256867 	 ± 0.2525847723154682
	data : 0.11840996742248536
	model : 0.06824269294738769
			 train-loss:  2.087195344023652 	 ± 0.25189443702417963
	data : 0.11772089004516602
	model : 0.06936450004577636
			 train-loss:  2.0904468903776077 	 ± 0.2550064439423841
	data : 0.1167076587677002
	model : 0.06951508522033692
			 train-loss:  2.0890517979860306 	 ± 0.25501184600089943
	data : 0.11660981178283691
	model : 0.0701570987701416
			 train-loss:  2.088182025342374 	 ± 0.25459520670291996
	data : 0.11596856117248536
	model : 0.0701263427734375
			 train-loss:  2.0890315976194156 	 ± 0.2541726963169602
	data : 0.11601510047912597
	model : 0.07029137611389161
			 train-loss:  2.088359798339599 	 ± 0.2536577011598048
	data : 0.11587004661560059
	model : 0.06993598937988281
			 train-loss:  2.0875400431612703 	 ± 0.2532304231053928
	data : 0.11626753807067872
	model : 0.06939692497253418
			 train-loss:  2.0847174812246254 	 ± 0.25550758688185876
	data : 0.11674132347106933
	model : 0.0688262939453125
			 train-loss:  2.0858951549780995 	 ± 0.2553481022247264
	data : 0.11728029251098633
	model : 0.06881732940673828
			 train-loss:  2.084082282650533 	 ± 0.25590176692629796
	data : 0.11731758117675781
	model : 0.06874599456787109
			 train-loss:  2.082075900708636 	 ± 0.256736297531447
	data : 0.11729907989501953
	model : 0.0686007022857666
			 train-loss:  2.082984887874188 	 ± 0.25637988821221286
	data : 0.1174276351928711
	model : 0.06900668144226074
			 train-loss:  2.0831767511122004 	 ± 0.25573215271287275
	data : 0.11710119247436523
	model : 0.0689772605895996
			 train-loss:  2.0835448234509197 	 ± 0.25512710041981435
	data : 0.11702065467834473
	model : 0.06979789733886718
			 train-loss:  2.0842813636575426 	 ± 0.2546831996804143
	data : 0.11633167266845704
	model : 0.06942949295043946
			 train-loss:  2.084586710494182 	 ± 0.2540719387933143
	data : 0.11654486656188964
	model : 0.06996870040893555
			 train-loss:  2.086393842191407 	 ± 0.25469565155551926
	data : 0.11604328155517578
	model : 0.0703115463256836
			 train-loss:  2.0867905754539833 	 ± 0.25411623386177606
	data : 0.11576194763183593
	model : 0.06951370239257812
			 train-loss:  2.085914664268494 	 ± 0.2537811298562223
	data : 0.11646838188171386
	model : 0.06972589492797851
			 train-loss:  2.0851361911688278 	 ± 0.25338832598625727
	data : 0.11619329452514648
	model : 0.07000260353088379
			 train-loss:  2.0856404393026144 	 ± 0.25286142751884516
	data : 0.11622772216796876
	model : 0.06960358619689941
			 train-loss:  2.088005630253571 	 ± 0.2544679655123099
	data : 0.11643919944763184
	model : 0.0694005012512207
			 train-loss:  2.086979386853237 	 ± 0.2542642696104528
	data : 0.11659517288208007
	model : 0.06943621635437011
			 train-loss:  2.089885953577553 	 ± 0.2570182288026015
	data : 0.11657028198242188
	model : 0.06926131248474121
			 train-loss:  2.0900148500516575 	 ± 0.2564002811422337
	data : 0.11672720909118653
	model : 0.06838803291320801
			 train-loss:  2.092252004549699 	 ± 0.2577877334275431
	data : 0.11742792129516602
	model : 0.06838026046752929
			 train-loss:  2.091014262002248 	 ± 0.2577831414420895
	data : 0.11747941970825196
	model : 0.06799077987670898
			 train-loss:  2.0913102244646353 	 ± 0.25720111713698623
	data : 0.11783733367919921
	model : 0.06807899475097656
			 train-loss:  2.0910242279370626 	 ± 0.2566213126515999
	data : 0.11773467063903809
	model : 0.06821489334106445
			 train-loss:  2.0900017486364355 	 ± 0.25644090723629637
	data : 0.11763715744018555
	model : 0.0683128833770752
			 train-loss:  2.0895257001777865 	 ± 0.2559288150045561
	data : 0.11765589714050292
	model : 0.06785616874694825
			 train-loss:  2.090953373013528 	 ± 0.2561721250017586
	data : 0.11833772659301758
	model : 0.06846137046813965
			 train-loss:  2.091489056003428 	 ± 0.2556924409721912
	data : 0.11795635223388672
	model : 0.06965079307556152
			 train-loss:  2.0927116288695227 	 ± 0.25572328836009856
	data : 0.11701884269714355
	model : 0.06954121589660645
			 train-loss:  2.0918348321208247 	 ± 0.25545436863099974
	data : 0.1172523021697998
	model : 0.07030997276306153
			 train-loss:  2.0921363314175934 	 ± 0.254903601478727
	data : 0.11648449897766114
	model : 0.06986813545227051
			 train-loss:  2.091142097197541 	 ± 0.2547396643097799
	data : 0.11664137840270997
	model : 0.06932220458984376
			 train-loss:  2.0904109526986945 	 ± 0.2543865591433822
	data : 0.11695981025695801
	model : 0.06876707077026367
			 train-loss:  2.089732833883979 	 ± 0.2540060623896954
	data : 0.11731457710266113
	model : 0.06852426528930664
			 train-loss:  2.0894932763069463 	 ± 0.2534556440813102
	data : 0.11740078926086425
	model : 0.06853437423706055
			 train-loss:  2.088790941345799 	 ± 0.2530996026358462
	data : 0.1172670841217041
	model : 0.06861963272094726
			 train-loss:  2.0871016690549293 	 ± 0.25378269287925964
	data : 0.11717863082885742
	model : 0.06788558959960937
			 train-loss:  2.0871947492871965 	 ± 0.2532193950212088
	data : 0.11788926124572754
	model : 0.06732254028320313
			 train-loss:  2.0883738072713216 	 ± 0.25327156131909384
	data : 0.11839513778686524
	model : 0.06716933250427246
			 train-loss:  2.0870435222060277 	 ± 0.2534971851991224
	data : 0.11846470832824707
	model : 0.06643757820129395
			 train-loss:  2.0869865758828654 	 ± 0.25293965381100586
	data : 0.1192084789276123
	model : 0.06725969314575195
			 train-loss:  2.0875369169210134 	 ± 0.2525205216132101
	data : 0.11854310035705566
	model : 0.06787128448486328
			 train-loss:  2.088919501637788 	 ± 0.25283193674972254
	data : 0.11785378456115722
	model : 0.06773390769958496
			 train-loss:  2.088417927078579 	 ± 0.25239585772901446
	data : 0.11794700622558593
	model : 0.06802120208740234
			 train-loss:  2.088722073154532 	 ± 0.25189119010411004
	data : 0.1177682876586914
	model : 0.06863322257995605
			 train-loss:  2.090217475233407 	 ± 0.25237324019678276
	data : 0.11719927787780762
	model : 0.06850013732910157
			 train-loss:  2.0902553157233372 	 ± 0.25183174395327207
	data : 0.11734933853149414
	model : 0.06811823844909667
			 train-loss:  2.0909007991481032 	 ± 0.25148615092336873
	data : 0.11779565811157226
	model : 0.06826372146606445
			 train-loss:  2.0919064552225963 	 ± 0.2514215777663343
	data : 0.11769208908081055
	model : 0.06790547370910645
			 train-loss:  2.0912043714927413 	 ± 0.2511190861825274
	data : 0.11784100532531738
	model : 0.06753463745117187
			 train-loss:  2.090655864039554 	 ± 0.2507303715150114
	data : 0.1178248405456543
	model : 0.06714863777160644
			 train-loss:  2.0913079626419964 	 ± 0.2504043882321881
	data : 0.11829218864440919
	model : 0.06696257591247559
			 train-loss:  2.0914804107474483 	 ± 0.24989414241265104
	data : 0.11826295852661133
	model : 0.06670022010803223
			 train-loss:  2.0925536255041757 	 ± 0.24992431659383768
	data : 0.11839537620544434
	model : 0.06618404388427734
			 train-loss:  2.0922396024727723 	 ± 0.2494527037986951
	data : 0.11880478858947754
	model : 0.06641845703125
			 train-loss:  2.090904080178127 	 ± 0.249798656683504
	data : 0.11868362426757813
	model : 0.06671643257141113
			 train-loss:  2.0895047256485424 	 ± 0.2502328197593106
	data : 0.11830310821533203
	model : 0.067177152633667
			 train-loss:  2.089614571117964 	 ± 0.24972539173357508
	data : 0.11795873641967773
	model : 0.06750822067260742
			 train-loss:  2.088141812110434 	 ± 0.2502747890058241
	data : 0.11789226531982422
	model : 0.06825289726257325
			 train-loss:  2.0877970990126693 	 ± 0.24982385553508787
	data : 0.1175908088684082
	model : 0.06797671318054199
			 train-loss:  2.087628125179152 	 ± 0.24933171204196053
	data : 0.11786928176879882
	model : 0.0674222469329834
			 train-loss:  2.0899262995489183 	 ± 0.25143625155656524
	data : 0.11835298538208008
	model : 0.06751561164855957
			 train-loss:  2.0896355425976365 	 ± 0.2509726241625574
	data : 0.11827807426452637
	model : 0.06780304908752441
			 train-loss:  2.089912878036499 	 ± 0.25050840478552955
	data : 0.11786856651306152
	model : 0.06793293952941895
			 train-loss:  2.0900946157387055 	 ± 0.2500253991975781
	data : 0.11774101257324218
	model : 0.06786766052246093
			 train-loss:  2.091286199433463 	 ± 0.25024192648356075
	data : 0.11767454147338867
	model : 0.06828255653381347
			 train-loss:  2.090963112978125 	 ± 0.249799545304399
	data : 0.11706008911132812
	model : 0.06821246147155761
			 train-loss:  2.0909569038180855 	 ± 0.24930734853569175
	data : 0.11727657318115234
	model : 0.06811714172363281
			 train-loss:  2.0894062028211704 	 ± 0.250042395091627
	data : 0.11723299026489258
	model : 0.06793456077575684
			 train-loss:  2.0863368017598987 	 ± 0.2543214260366832
	data : 0.11619129180908203
	model : 0.059496164321899414
#epoch  52    val-loss:  2.4059834982219495  train-loss:  2.0863368017598987  lr:  0.00015625
			 train-loss:  1.9180816411972046 	 ± 0.0
	data : 5.725440740585327
	model : 0.07887744903564453
			 train-loss:  1.83793044090271 	 ± 0.08015120029449463
	data : 2.9240357875823975
	model : 0.07285690307617188
			 train-loss:  2.0083919366200766 	 ± 0.24979402111240043
	data : 1.9896288712819417
	model : 0.07080928484598796
			 train-loss:  2.022335708141327 	 ± 0.2176719458964275
	data : 1.521870493888855
	model : 0.06983780860900879
			 train-loss:  2.014444279670715 	 ± 0.1953303852386948
	data : 1.24128155708313
	model : 0.06965703964233398
			 train-loss:  1.9820498029390972 	 ± 0.19246292312587782
	data : 0.119622802734375
	model : 0.06721053123474122
			 train-loss:  1.9668878146580286 	 ± 0.1820153321609497
	data : 0.11875572204589843
	model : 0.06702895164489746
			 train-loss:  2.019860178232193 	 ± 0.22052410892550353
	data : 0.1184420108795166
	model : 0.06685957908630372
			 train-loss:  2.058549854490492 	 ± 0.23495229219403532
	data : 0.11854090690612792
	model : 0.0674746036529541
			 train-loss:  2.0971190690994264 	 ± 0.25113856829738124
	data : 0.11794843673706054
	model : 0.06761403083801269
			 train-loss:  2.114876638759266 	 ± 0.24594756382929783
	data : 0.11785507202148438
	model : 0.06806712150573731
			 train-loss:  2.0946964224179587 	 ± 0.24480401601043597
	data : 0.11758790016174317
	model : 0.06887626647949219
			 train-loss:  2.062463090969966 	 ± 0.2603592413793048
	data : 0.1168971061706543
	model : 0.0697568416595459
			 train-loss:  2.050818622112274 	 ± 0.2543771377076682
	data : 0.11634964942932129
	model : 0.06978440284729004
			 train-loss:  2.045253109931946 	 ± 0.2466323754174776
	data : 0.1163562297821045
	model : 0.0695150375366211
			 train-loss:  2.0529086515307426 	 ± 0.24063440698261965
	data : 0.11653685569763184
	model : 0.06962461471557617
			 train-loss:  2.0686621876323925 	 ± 0.24180473234094096
	data : 0.11646943092346192
	model : 0.06872458457946777
			 train-loss:  2.0715297129419117 	 ± 0.23528919941811136
	data : 0.11733598709106445
	model : 0.06908159255981446
			 train-loss:  2.066872565369857 	 ± 0.22986446562341029
	data : 0.11695418357849122
	model : 0.06881389617919922
			 train-loss:  2.067155474424362 	 ± 0.22404756111515356
	data : 0.11709733009338379
	model : 0.0687939167022705
			 train-loss:  2.0866693258285522 	 ± 0.23542041010917245
	data : 0.11706795692443847
	model : 0.06822614669799805
			 train-loss:  2.10858046466654 	 ± 0.2509693423695982
	data : 0.11765666007995605
	model : 0.0687718391418457
			 train-loss:  2.096662754597871 	 ± 0.25173757344466086
	data : 0.11705403327941895
	model : 0.06829195022583008
			 train-loss:  2.0983073761065802 	 ± 0.24656342776218793
	data : 0.11731338500976562
	model : 0.06861133575439453
			 train-loss:  2.106364731788635 	 ± 0.24478538863162888
	data : 0.11687579154968261
	model : 0.06904759407043456
			 train-loss:  2.087294509777656 	 ± 0.2582771953879922
	data : 0.11662931442260742
	model : 0.06877846717834472
			 train-loss:  2.0906115152217724 	 ± 0.25401287687732055
	data : 0.1167635440826416
	model : 0.06866321563720704
			 train-loss:  2.0993892593043193 	 ± 0.2535714544165152
	data : 0.11713666915893554
	model : 0.06784539222717285
			 train-loss:  2.111346890186441 	 ± 0.2570697797488547
	data : 0.11802663803100585
	model : 0.06770086288452148
			 train-loss:  2.115183079242706 	 ± 0.25359183080804076
	data : 0.11827321052551269
	model : 0.06725606918334961
			 train-loss:  2.114573167216393 	 ± 0.2494904777269757
	data : 0.11851768493652344
	model : 0.0673365592956543
			 train-loss:  2.113478120416403 	 ± 0.24563692637990164
	data : 0.11846213340759278
	model : 0.06716046333312989
			 train-loss:  2.1192720333735147 	 ± 0.24409693197146676
	data : 0.11834540367126464
	model : 0.06710238456726074
			 train-loss:  2.1085329757017246 	 ± 0.24826732509232194
	data : 0.11844301223754883
	model : 0.06691937446594239
			 train-loss:  2.105520166669573 	 ± 0.24532475571450202
	data : 0.11883625984191895
	model : 0.06729846000671387
			 train-loss:  2.1092603935135736 	 ± 0.2429034310772422
	data : 0.11845264434814454
	model : 0.06816296577453614
			 train-loss:  2.1166688107155465 	 ± 0.24368683456389556
	data : 0.11757969856262207
	model : 0.068873929977417
			 train-loss:  2.1106592981438888 	 ± 0.24322167368516623
	data : 0.11708860397338867
	model : 0.0696723461151123
			 train-loss:  2.107255348792443 	 ± 0.24099842961545787
	data : 0.11627035140991211
	model : 0.06981015205383301
			 train-loss:  2.114978963136673 	 ± 0.2428059956491733
	data : 0.11612615585327149
	model : 0.06961121559143066
			 train-loss:  2.1193521546154486 	 ± 0.24141628549608998
	data : 0.11626877784729003
	model : 0.06948151588439941
			 train-loss:  2.129705849147978 	 ± 0.24756679717210633
	data : 0.11640892028808594
	model : 0.06940617561340331
			 train-loss:  2.1183648525282393 	 ± 0.2554720153802109
	data : 0.11637778282165527
	model : 0.06972618103027343
			 train-loss:  2.113490489396182 	 ± 0.2545668644343318
	data : 0.11613140106201172
	model : 0.06979308128356934
			 train-loss:  2.115476973851522 	 ± 0.2520670991427832
	data : 0.1161414623260498
	model : 0.06991643905639648
			 train-loss:  2.107702620651411 	 ± 0.2547084431377139
	data : 0.11620397567749023
	model : 0.07012529373168945
			 train-loss:  2.1013695153784244 	 ± 0.25561889697590584
	data : 0.11616568565368653
	model : 0.06937284469604492
			 train-loss:  2.096869265039762 	 ± 0.2548168069950364
	data : 0.1168868064880371
	model : 0.06923475265502929
			 train-loss:  2.0938273108735377 	 ± 0.2530822744006464
	data : 0.11704072952270508
	model : 0.06919903755187988
			 train-loss:  2.098613810539246 	 ± 0.2527691500894231
	data : 0.11689872741699218
	model : 0.06939072608947754
			 train-loss:  2.098172804888557 	 ± 0.2502981790845655
	data : 0.11680855751037597
	model : 0.06924443244934082
			 train-loss:  2.0985085918353152 	 ± 0.2478913813674764
	data : 0.11694445610046386
	model : 0.06974706649780274
			 train-loss:  2.094397837260984 	 ± 0.24732450783290946
	data : 0.11665773391723633
	model : 0.06939134597778321
			 train-loss:  2.092819341906795 	 ± 0.24529309530554377
	data : 0.11707682609558105
	model : 0.06948919296264648
			 train-loss:  2.0942377177151767 	 ± 0.24327630967689326
	data : 0.11706452369689942
	model : 0.06936039924621581
			 train-loss:  2.092180241431509 	 ± 0.24157678605571983
	data : 0.11716513633728028
	model : 0.06960639953613282
			 train-loss:  2.0929374924877235 	 ± 0.23951535955224554
	data : 0.11676836013793945
	model : 0.06975002288818359
			 train-loss:  2.098336772672061 	 ± 0.2409153122823119
	data : 0.11655683517456054
	model : 0.06910595893859864
			 train-loss:  2.098402877985421 	 ± 0.23886546245161758
	data : 0.11692380905151367
	model : 0.06909661293029785
			 train-loss:  2.102849958340327 	 ± 0.2393169005669774
	data : 0.11685972213745117
	model : 0.06817808151245117
			 train-loss:  2.100973334468779 	 ± 0.23779189891643537
	data : 0.11772918701171875
	model : 0.06707878112792968
			 train-loss:  2.1007582660644286 	 ± 0.23587240785516186
	data : 0.11883020401000977
	model : 0.06725139617919922
			 train-loss:  2.096326120316036 	 ± 0.2365810807172443
	data : 0.11865057945251464
	model : 0.06805176734924316
			 train-loss:  2.093402937054634 	 ± 0.23586945972532566
	data : 0.11814870834350585
	model : 0.06842012405395508
			 train-loss:  2.0959298390608567 	 ± 0.23491943962485215
	data : 0.11787123680114746
	model : 0.06926722526550293
			 train-loss:  2.097226623332862 	 ± 0.23336726739782285
	data : 0.11709976196289062
	model : 0.06999931335449219
			 train-loss:  2.0940331701022474 	 ± 0.23306762740421794
	data : 0.11649165153503419
	model : 0.06983370780944824
			 train-loss:  2.096635730827556 	 ± 0.23232628000062788
	data : 0.11669435501098632
	model : 0.0695490837097168
			 train-loss:  2.098055238309114 	 ± 0.23093346845267693
	data : 0.1167119026184082
	model : 0.06823182106018066
			 train-loss:  2.0955058472497123 	 ± 0.23025391231410192
	data : 0.1177760124206543
	model : 0.06831755638122558
			 train-loss:  2.0900415568284587 	 ± 0.2331528267600792
	data : 0.11767182350158692
	model : 0.06853947639465333
			 train-loss:  2.0929750170972614 	 ± 0.23284373668565458
	data : 0.11747565269470214
	model : 0.06767325401306153
			 train-loss:  2.088552765650292 	 ± 0.2342681628607932
	data : 0.11807484626770019
	model : 0.0681229591369629
			 train-loss:  2.0868195955817765 	 ± 0.2331506216870422
	data : 0.11769318580627441
	model : 0.06907882690429687
			 train-loss:  2.0839357550938926 	 ± 0.23291596457840236
	data : 0.11694560050964356
	model : 0.06831088066101074
			 train-loss:  2.087263946470461 	 ± 0.23316688684920664
	data : 0.11747579574584961
	model : 0.06747584342956543
			 train-loss:  2.091402591048897 	 ± 0.23444080622322433
	data : 0.11811037063598633
	model : 0.067498779296875
			 train-loss:  2.089775278018071 	 ± 0.23337041729195973
	data : 0.11833434104919434
	model : 0.06739182472229004
			 train-loss:  2.0881894962697087 	 ± 0.2323112336891691
	data : 0.11850557327270508
	model : 0.06738581657409667
			 train-loss:  2.086787734925747 	 ± 0.23119068490387995
	data : 0.1184514045715332
	model : 0.06813278198242187
			 train-loss:  2.0821332857932573 	 ± 0.2335002755621747
	data : 0.11784482002258301
	model : 0.06806721687316894
			 train-loss:  2.0816306559050957 	 ± 0.23211621057480647
	data : 0.11801033020019532
	model : 0.06809239387512207
			 train-loss:  2.0852405694593865 	 ± 0.23301799175893204
	data : 0.1178515911102295
	model : 0.06739983558654786
			 train-loss:  2.0846555133660636 	 ± 0.2316881463768134
	data : 0.11836733818054199
	model : 0.06650595664978028
			 train-loss:  2.0839671962401445 	 ± 0.23040762229359663
	data : 0.11910324096679688
	model : 0.06565585136413574
			 train-loss:  2.08413044935049 	 ± 0.2290690709779805
	data : 0.11991348266601562
	model : 0.06639933586120605
			 train-loss:  2.0854087958390686 	 ± 0.22805710708495858
	data : 0.11903853416442871
	model : 0.06739802360534668
			 train-loss:  2.083734921433709 	 ± 0.2272944836424993
	data : 0.11823601722717285
	model : 0.06803226470947266
			 train-loss:  2.08235172877151 	 ± 0.22638609746440114
	data : 0.11788592338562012
	model : 0.06798000335693359
			 train-loss:  2.079637806945377 	 ± 0.2265761050283356
	data : 0.11794905662536621
	model : 0.0687859058380127
			 train-loss:  2.077701673402891 	 ± 0.22607513495105808
	data : 0.11713652610778809
	model : 0.06888871192932129
			 train-loss:  2.0762475793776303 	 ± 0.22527057723096902
	data : 0.11720433235168456
	model : 0.06872754096984864
			 train-loss:  2.073632877360108 	 ± 0.22545540895328325
	data : 0.1172029972076416
	model : 0.06860356330871582
			 train-loss:  2.07466659266898 	 ± 0.2244744350778215
	data : 0.11729168891906738
	model : 0.06935291290283203
			 train-loss:  2.079641832803425 	 ± 0.22844068646001212
	data : 0.11657791137695313
	model : 0.06932835578918457
			 train-loss:  2.08110203469793 	 ± 0.22769301692427985
	data : 0.11663565635681153
	model : 0.06852488517761231
			 train-loss:  2.0776205812532877 	 ± 0.2290703043436245
	data : 0.11743059158325195
	model : 0.0686518669128418
			 train-loss:  2.0747852434917373 	 ± 0.22960304891941333
	data : 0.11749262809753418
	model : 0.06903581619262696
			 train-loss:  2.0776144290211227 	 ± 0.2301509938295739
	data : 0.11702938079833984
	model : 0.06887383460998535
			 train-loss:  2.0746665132045745 	 ± 0.2308681782308186
	data : 0.1171884536743164
	model : 0.06889925003051758
			 train-loss:  2.073222794155083 	 ± 0.23017563787461842
	data : 0.117059326171875
	model : 0.06961688995361329
			 train-loss:  2.07565413386214 	 ± 0.23034421441632916
	data : 0.11631102561950683
	model : 0.06934847831726074
			 train-loss:  2.0752797022606564 	 ± 0.2292545021672214
	data : 0.11633882522583008
	model : 0.06917667388916016
			 train-loss:  2.0784265227042713 	 ± 0.230374087725338
	data : 0.11665830612182618
	model : 0.06949729919433593
			 train-loss:  2.0765356290908086 	 ± 0.23008394198326151
	data : 0.11630778312683106
	model : 0.06959104537963867
			 train-loss:  2.0736615635314077 	 ± 0.2308820607148813
	data : 0.11632776260375977
	model : 0.06987590789794922
			 train-loss:  2.0737130307705605 	 ± 0.22980125088754316
	data : 0.11617913246154785
	model : 0.0700368881225586
			 train-loss:  2.074366918316594 	 ± 0.2288348665034594
	data : 0.1160013198852539
	model : 0.0692070484161377
			 train-loss:  2.0734216434146284 	 ± 0.22799447903528616
	data : 0.11665668487548828
	model : 0.06922588348388672
			 train-loss:  2.0723751826719803 	 ± 0.22721858917875626
	data : 0.11678166389465332
	model : 0.06899657249450683
			 train-loss:  2.0716237656705014 	 ± 0.22633001698708927
	data : 0.11691784858703613
	model : 0.06880149841308594
			 train-loss:  2.067142197063991 	 ± 0.23021138719622228
	data : 0.11693735122680664
	model : 0.0688586711883545
			 train-loss:  2.067394258701696 	 ± 0.2292060126173629
	data : 0.11700215339660644
	model : 0.06987814903259278
			 train-loss:  2.065369779603523 	 ± 0.22921101997055857
	data : 0.11603369712829589
	model : 0.06934900283813476
			 train-loss:  2.0671738624572753 	 ± 0.2290237535423665
	data : 0.1166316032409668
	model : 0.06945242881774902
			 train-loss:  2.071070529263595 	 ± 0.2318315546101634
	data : 0.11646771430969238
	model : 0.06849231719970703
			 train-loss:  2.072217845509195 	 ± 0.23116919802383376
	data : 0.11724486351013183
	model : 0.06754984855651855
			 train-loss:  2.0723318568730758 	 ± 0.23019088685229047
	data : 0.11810755729675293
	model : 0.06721620559692383
			 train-loss:  2.06811040890317 	 ± 0.2337635598123826
	data : 0.11845002174377442
	model : 0.06778812408447266
			 train-loss:  2.071847656369209 	 ± 0.23633048631755138
	data : 0.1179661750793457
	model : 0.06726841926574707
			 train-loss:  2.073082877584725 	 ± 0.23574054327620073
	data : 0.11849617958068848
	model : 0.06828575134277344
			 train-loss:  2.075361809769615 	 ± 0.2361069667805153
	data : 0.11766934394836426
	model : 0.06955313682556152
			 train-loss:  2.074551885690146 	 ± 0.23531533180556144
	data : 0.11656980514526367
	model : 0.06978645324707031
			 train-loss:  2.074923970045582 	 ± 0.23440088613483148
	data : 0.11626091003417968
	model : 0.06976523399353027
			 train-loss:  2.0760164098739624 	 ± 0.23377812106777282
	data : 0.11617832183837891
	model : 0.07049803733825684
			 train-loss:  2.075957685235947 	 ± 0.232849507762691
	data : 0.11564350128173828
	model : 0.0706094741821289
			 train-loss:  2.076527662164583 	 ± 0.2320191951800795
	data : 0.11550483703613282
	model : 0.07009754180908204
			 train-loss:  2.0791448717936873 	 ± 0.23298553645972409
	data : 0.11588444709777831
	model : 0.06995015144348145
			 train-loss:  2.075725567433261 	 ± 0.23528280366461674
	data : 0.11597352027893067
	model : 0.06969771385192872
			 train-loss:  2.07511826478518 	 ± 0.23447759867002912
	data : 0.11620488166809081
	model : 0.06959419250488282
			 train-loss:  2.076196483073344 	 ± 0.2339042190334574
	data : 0.11636557579040527
	model : 0.06926589012145996
			 train-loss:  2.078346973115748 	 ± 0.2343128860940886
	data : 0.11669316291809081
	model : 0.06944727897644043
			 train-loss:  2.0762483535852647 	 ± 0.23467228754617103
	data : 0.11651105880737304
	model : 0.0696711540222168
			 train-loss:  2.0762078406205817 	 ± 0.2337954717422199
	data : 0.11648993492126465
	model : 0.06981096267700196
			 train-loss:  2.0763178207256177 	 ± 0.23293143227599195
	data : 0.11631689071655274
	model : 0.069561767578125
			 train-loss:  2.0769029908320484 	 ± 0.23217306147904468
	data : 0.11652412414550781
	model : 0.06970720291137696
			 train-loss:  2.076538632385922 	 ± 0.23136318456662003
	data : 0.11642112731933593
	model : 0.07005543708801269
			 train-loss:  2.0779615657916968 	 ± 0.23112425604761763
	data : 0.11614832878112794
	model : 0.06998972892761231
			 train-loss:  2.079815729058904 	 ± 0.23131915116867943
	data : 0.1161308765411377
	model : 0.06918501853942871
			 train-loss:  2.078017050027847 	 ± 0.23146499735527687
	data : 0.11692233085632324
	model : 0.06948723793029785
			 train-loss:  2.0758165230987764 	 ± 0.23210772860354756
	data : 0.11661224365234375
	model : 0.06939907073974609
			 train-loss:  2.0737917691888943 	 ± 0.2325352681453585
	data : 0.11668539047241211
	model : 0.06848835945129395
			 train-loss:  2.070689503963177 	 ± 0.2346510931973225
	data : 0.11750450134277343
	model : 0.06826825141906738
			 train-loss:  2.078311157723268 	 ± 0.25096928139801716
	data : 0.11773219108581542
	model : 0.0691108226776123
			 train-loss:  2.079031048972031 	 ± 0.25025152136345163
	data : 0.11686310768127442
	model : 0.0681464672088623
			 train-loss:  2.0796108449975104 	 ± 0.24949072857046042
	data : 0.11771984100341797
	model : 0.06822357177734376
			 train-loss:  2.0799189741108695 	 ± 0.24866854602939953
	data : 0.11753287315368652
	model : 0.06877431869506836
			 train-loss:  2.0796879167492324 	 ± 0.2478428587378559
	data : 0.11686296463012695
	model : 0.06895880699157715
			 train-loss:  2.0781490818766137 	 ± 0.24771817238244234
	data : 0.11669669151306153
	model : 0.06902508735656739
			 train-loss:  2.0754749099413554 	 ± 0.24903960684531046
	data : 0.11669111251831055
	model : 0.06902785301208496
			 train-loss:  2.0775127418783326 	 ± 0.24946524075485083
	data : 0.11683158874511719
	model : 0.06892375946044922
			 train-loss:  2.077518782333324 	 ± 0.2486432883449711
	data : 0.11709532737731934
	model : 0.06887898445129395
			 train-loss:  2.076709658491845 	 ± 0.24803008153468462
	data : 0.11697573661804199
	model : 0.06800570487976074
			 train-loss:  2.0748683252891937 	 ± 0.24827040812234447
	data : 0.11785683631896973
	model : 0.06803331375122071
			 train-loss:  2.0759425540124217 	 ± 0.2478270379586717
	data : 0.11788349151611328
	model : 0.06928272247314453
			 train-loss:  2.0773226901506767 	 ± 0.24762829869535358
	data : 0.11656050682067871
	model : 0.06941323280334473
			 train-loss:  2.0769751337683124 	 ± 0.24687658170086746
	data : 0.11665840148925781
	model : 0.0693422794342041
			 train-loss:  2.0760723375066927 	 ± 0.24635393423508223
	data : 0.1170680046081543
	model : 0.07014598846435546
			 train-loss:  2.078290983565948 	 ± 0.2471564258360692
	data : 0.11619095802307129
	model : 0.06989517211914062
			 train-loss:  2.077899671345949 	 ± 0.24643225515274939
	data : 0.11648297309875488
	model : 0.06942410469055176
			 train-loss:  2.077031513919001 	 ± 0.24591106133078272
	data : 0.11708869934082031
	model : 0.06943020820617676
			 train-loss:  2.0751267472902932 	 ± 0.24633938879514405
	data : 0.11690316200256348
	model : 0.0687182903289795
			 train-loss:  2.0790713630570954 	 ± 0.2506621720634216
	data : 0.11751303672790528
	model : 0.06884407997131348
			 train-loss:  2.077741710151114 	 ± 0.2504727244198741
	data : 0.11731433868408203
	model : 0.06903362274169922
			 train-loss:  2.076497971650326 	 ± 0.25022000951582335
	data : 0.11701974868774415
	model : 0.06906285285949706
			 train-loss:  2.0757446109530435 	 ± 0.2496528195380154
	data : 0.1169651985168457
	model : 0.06831679344177247
			 train-loss:  2.077836027402364 	 ± 0.25035855353048875
	data : 0.11770105361938477
	model : 0.06906375885009766
			 train-loss:  2.078481205162548 	 ± 0.24975153273336223
	data : 0.11693735122680664
	model : 0.06805129051208496
			 train-loss:  2.081377065393346 	 ± 0.2518245211460413
	data : 0.11798300743103027
	model : 0.06810369491577148
			 train-loss:  2.0792084700921003 	 ± 0.252660504856679
	data : 0.1180023193359375
	model : 0.06818299293518067
			 train-loss:  2.0775670796109917 	 ± 0.2528280456197574
	data : 0.11778378486633301
	model : 0.06894750595092773
			 train-loss:  2.0744497131469637 	 ± 0.25536670592444555
	data : 0.11722121238708497
	model : 0.06830482482910157
			 train-loss:  2.075797016910046 	 ± 0.25523993563459707
	data : 0.1178360939025879
	model : 0.06914458274841309
			 train-loss:  2.0748993096680475 	 ± 0.25477918125368887
	data : 0.11696805953979492
	model : 0.06910586357116699
			 train-loss:  2.0747564213616507 	 ± 0.2540571896086842
	data : 0.11692590713500976
	model : 0.06890244483947754
			 train-loss:  2.0758046487515625 	 ± 0.2537136356941083
	data : 0.11706681251525879
	model : 0.06898684501647949
			 train-loss:  2.0755175729255892 	 ± 0.2530245794101254
	data : 0.1168027400970459
	model : 0.06977996826171876
			 train-loss:  2.075474830825677 	 ± 0.252313475949844
	data : 0.11605448722839355
	model : 0.06997251510620117
			 train-loss:  2.076167317742076 	 ± 0.25177727021720914
	data : 0.11589775085449219
	model : 0.07005867958068848
			 train-loss:  2.0749593165185716 	 ± 0.25159655393295854
	data : 0.11602411270141602
	model : 0.07009162902832031
			 train-loss:  2.0740052119144416 	 ± 0.2512268983170016
	data : 0.115897798538208
	model : 0.06913790702819825
			 train-loss:  2.074242041006193 	 ± 0.2505560234866062
	data : 0.11690759658813477
	model : 0.06861801147460937
			 train-loss:  2.0758533536410724 	 ± 0.25081427738795864
	data : 0.1175410270690918
	model : 0.06850123405456543
			 train-loss:  2.0753432635379876 	 ± 0.25022695025699093
	data : 0.11777772903442382
	model : 0.06870999336242675
			 train-loss:  2.0758458504805692 	 ± 0.24964284962402564
	data : 0.1174278736114502
	model : 0.06890053749084472
			 train-loss:  2.074992678498709 	 ± 0.2492411527187337
	data : 0.11728754043579101
	model : 0.06883635520935058
			 train-loss:  2.0749221460067013 	 ± 0.24857570044652955
	data : 0.1172337532043457
	model : 0.06943435668945312
			 train-loss:  2.0749920720749713 	 ± 0.24791555749812869
	data : 0.11661558151245117
	model : 0.06929664611816407
			 train-loss:  2.075942840525713 	 ± 0.24760224520347843
	data : 0.11683826446533203
	model : 0.06937608718872071
			 train-loss:  2.0748210455241956 	 ± 0.24743089181453662
	data : 0.11678361892700195
	model : 0.06934380531311035
			 train-loss:  2.075841448069867 	 ± 0.24718281495322958
	data : 0.1170722484588623
	model : 0.07027196884155273
			 train-loss:  2.0768370951215425 	 ± 0.2469219695592077
	data : 0.11621880531311035
	model : 0.07018733024597168
			 train-loss:  2.0783529417502447 	 ± 0.2471754959721533
	data : 0.11623048782348633
	model : 0.07029585838317871
			 train-loss:  2.077608301467502 	 ± 0.24675456590196151
	data : 0.11593518257141114
	model : 0.06918115615844726
			 train-loss:  2.080444371394622 	 ± 0.2492708754643867
	data : 0.11695389747619629
	model : 0.06911740303039551
			 train-loss:  2.0817001340340595 	 ± 0.2492517850793852
	data : 0.11686859130859376
	model : 0.06819801330566407
			 train-loss:  2.081971235081629 	 ± 0.2486473302398097
	data : 0.11775264739990235
	model : 0.06748943328857422
			 train-loss:  2.081416068655072 	 ± 0.24814101270128774
	data : 0.11835474967956543
	model : 0.06753435134887695
			 train-loss:  2.08116925182055 	 ± 0.24754112216333396
	data : 0.11838126182556152
	model : 0.0683359146118164
			 train-loss:  2.0819620478153227 	 ± 0.2471746360338162
	data : 0.11768240928649902
	model : 0.06849956512451172
			 train-loss:  2.083539648435602 	 ± 0.24756637235694556
	data : 0.11761817932128907
	model : 0.06939210891723632
			 train-loss:  2.0834921976127245 	 ± 0.24695374032470138
	data : 0.11673870086669921
	model : 0.07028861045837402
			 train-loss:  2.082965622394543 	 ± 0.24645838661285782
	data : 0.1160555362701416
	model : 0.07010798454284668
			 train-loss:  2.0824654809400145 	 ± 0.24595682841034192
	data : 0.11621732711791992
	model : 0.07003617286682129
			 train-loss:  2.0845477889223796 	 ± 0.24715220083425465
	data : 0.11616935729980468
	model : 0.0700502872467041
			 train-loss:  2.08374737477997 	 ± 0.24681778898206116
	data : 0.11608514785766602
	model : 0.06993203163146973
			 train-loss:  2.082510930904444 	 ± 0.2468595909073936
	data : 0.1163665771484375
	model : 0.06958093643188476
			 train-loss:  2.080620863689826 	 ± 0.24776229695711025
	data : 0.11661229133605958
	model : 0.06965436935424804
			 train-loss:  2.0800027750325545 	 ± 0.24732954759942302
	data : 0.11641535758972169
	model : 0.06961550712585449
			 train-loss:  2.0800462399210247 	 ± 0.246740765119071
	data : 0.11649775505065918
	model : 0.06860947608947754
			 train-loss:  2.081095806795274 	 ± 0.24662482363491023
	data : 0.11726984977722169
	model : 0.06878390312194824
			 train-loss:  2.079097368244855 	 ± 0.24774902936203577
	data : 0.11704812049865723
	model : 0.06894726753234863
			 train-loss:  2.0784764899894106 	 ± 0.24733204066520548
	data : 0.11692519187927246
	model : 0.06852645874023437
			 train-loss:  2.0771542002107495 	 ± 0.24750697456454676
	data : 0.11759414672851562
	model : 0.06767215728759765
			 train-loss:  2.0770785980446393 	 ± 0.2469331827976594
	data : 0.11834950447082519
	model : 0.06860227584838867
			 train-loss:  2.0773166048306004 	 ± 0.2463856319669243
	data : 0.1176562786102295
	model : 0.06770977973937989
			 train-loss:  2.0771345548366074 	 ± 0.2458318282029203
	data : 0.11844916343688965
	model : 0.06768808364868165
			 train-loss:  2.0757149124364243 	 ± 0.2461572856679347
	data : 0.11838932037353515
	model : 0.06818790435791015
			 train-loss:  2.0738148297349066 	 ± 0.2471917789179974
	data : 0.11791419982910156
	model : 0.06839237213134766
			 train-loss:  2.0738370396874166 	 ± 0.24662955856712565
	data : 0.11784791946411133
	model : 0.06833586692810059
			 train-loss:  2.0726168425374443 	 ± 0.24673561072281805
	data : 0.1178206443786621
	model : 0.06922621726989746
			 train-loss:  2.0735274488861495 	 ± 0.24655118811535265
	data : 0.1170386791229248
	model : 0.0691070556640625
			 train-loss:  2.0725178665109816 	 ± 0.2464572451485761
	data : 0.1172297477722168
	model : 0.06829700469970704
			 train-loss:  2.0740653020995006 	 ± 0.2469898660291676
	data : 0.11771411895751953
	model : 0.06817693710327148
			 train-loss:  2.0748832586076524 	 ± 0.24674426618963552
	data : 0.11760187149047852
	model : 0.06756424903869629
			 train-loss:  2.07438530964134 	 ± 0.2463110426634547
	data : 0.11810932159423829
	model : 0.06661405563354492
			 train-loss:  2.075272863656939 	 ± 0.24612983734354832
	data : 0.11867690086364746
	model : 0.06644926071166993
			 train-loss:  2.0741208378683056 	 ± 0.24620207477737607
	data : 0.11894545555114747
	model : 0.06690840721130371
			 train-loss:  2.0750810553413293 	 ± 0.2460914165457344
	data : 0.11839842796325684
	model : 0.06668987274169921
			 train-loss:  2.0735834152802175 	 ± 0.24659948633992293
	data : 0.11842575073242187
	model : 0.06697320938110352
			 train-loss:  2.0737484172309117 	 ± 0.24607786603304113
	data : 0.11840300559997559
	model : 0.06775622367858887
			 train-loss:  2.075909908475547 	 ± 0.2477348349031905
	data : 0.11770424842834473
	model : 0.0675821304321289
			 train-loss:  2.077900185605487 	 ± 0.2490545041504895
	data : 0.11765513420104981
	model : 0.06749553680419922
			 train-loss:  2.07888681256873 	 ± 0.2489776663169369
	data : 0.11807012557983398
	model : 0.06842026710510254
			 train-loss:  2.07699947255723 	 ± 0.25011919528696847
	data : 0.11735701560974121
	model : 0.06847996711730957
			 train-loss:  2.0765499048313854 	 ± 0.24968384983387615
	data : 0.1170766830444336
	model : 0.06848502159118652
			 train-loss:  2.0756055926471824 	 ± 0.24957849580387942
	data : 0.11714253425598145
	model : 0.06886954307556152
			 train-loss:  2.075979983105379 	 ± 0.24912030258337461
	data : 0.11692094802856445
	model : 0.06900372505187988
			 train-loss:  2.0765935496805104 	 ± 0.24877872584795235
	data : 0.1166717529296875
	model : 0.0688809871673584
			 train-loss:  2.0769014030694963 	 ± 0.24830551099575715
	data : 0.11699113845825196
	model : 0.06895036697387695
			 train-loss:  2.075942071641629 	 ± 0.2482351110213699
	data : 0.11690692901611328
	model : 0.06820850372314453
			 train-loss:  2.0773546621819174 	 ± 0.2486904376427045
	data : 0.11747031211853028
	model : 0.06786942481994629
			 train-loss:  2.0776343880366888 	 ± 0.24821634778995758
	data : 0.11770315170288086
	model : 0.06724152565002442
			 train-loss:  2.078158246200593 	 ± 0.24784175517449894
	data : 0.11814422607421875
	model : 0.06678614616394044
			 train-loss:  2.0792430960402197 	 ± 0.24791527289518564
	data : 0.11829771995544433
	model : 0.06632380485534668
			 train-loss:  2.0802001996738153 	 ± 0.2478640124781522
	data : 0.11878294944763183
	model : 0.0665278434753418
			 train-loss:  2.0801252676890445 	 ± 0.2473645465216288
	data : 0.11860318183898926
	model : 0.06627998352050782
			 train-loss:  2.081291948114672 	 ± 0.2475453302504742
	data : 0.1186478614807129
	model : 0.06702566146850586
			 train-loss:  2.082579524162783 	 ± 0.2478784756404312
	data : 0.1179732322692871
	model : 0.06769447326660157
			 train-loss:  2.082332319736481 	 ± 0.2474129748581266
	data : 0.1175508975982666
	model : 0.06806154251098633
			 train-loss:  2.085480340923446 	 ± 0.2518865067145873
	data : 0.11720194816589355
	model : 0.06862063407897949
			 train-loss:  2.0855568216906653 	 ± 0.2513891552485528
	data : 0.11682496070861817
	model : 0.06927103996276855
			 train-loss:  2.085203177844112 	 ± 0.2509546471583399
	data : 0.11660332679748535
	model : 0.06900205612182617
			 train-loss:  2.0852613913731313 	 ± 0.25046186634643514
	data : 0.11695213317871093
	model : 0.06893768310546874
			 train-loss:  2.0836233742096844 	 ± 0.2513297623751178
	data : 0.11720132827758789
	model : 0.06852631568908692
			 train-loss:  2.086130469571799 	 ± 0.2540132164977942
	data : 0.11668429374694825
	model : 0.059292030334472653
#epoch  53    val-loss:  2.4587761477420202  train-loss:  2.086130469571799  lr:  7.8125e-05
			 train-loss:  1.934912919998169 	 ± 0.0
	data : 5.583889007568359
	model : 0.07225275039672852
			 train-loss:  1.8838198781013489 	 ± 0.05109304189682007
	data : 2.8576416969299316
	model : 0.07217514514923096
			 train-loss:  1.8044230143229167 	 ± 0.11978337353690613
	data : 1.9445512294769287
	model : 0.07153018315633138
			 train-loss:  1.862722396850586 	 ± 0.14476704197169332
	data : 1.4873015880584717
	model : 0.0708622932434082
			 train-loss:  1.9084760189056396 	 ± 0.15855463688462096
	data : 1.213124418258667
	model : 0.0709728717803955
			 train-loss:  1.9348395665486653 	 ± 0.15628444351984866
	data : 0.1192007064819336
	model : 0.07050471305847168
			 train-loss:  1.9385677576065063 	 ± 0.14497918021836828
	data : 0.11621885299682617
	model : 0.0701362133026123
			 train-loss:  1.9212494194507599 	 ± 0.14314700910263137
	data : 0.11573090553283691
	model : 0.07000617980957032
			 train-loss:  1.9246224959691365 	 ± 0.1352970888142691
	data : 0.11574983596801758
	model : 0.07011137008666993
			 train-loss:  1.9372447371482848 	 ± 0.13382324450152855
	data : 0.11566643714904785
	model : 0.06965856552124024
			 train-loss:  1.9963773055510088 	 ± 0.22637846001340892
	data : 0.1161759376525879
	model : 0.06997199058532715
			 train-loss:  1.9953962067763011 	 ± 0.21676529857060167
	data : 0.11579327583312989
	model : 0.06899280548095703
			 train-loss:  2.0307922821778517 	 ± 0.2416761902232403
	data : 0.11672277450561523
	model : 0.06812744140625
			 train-loss:  2.0557646495955333 	 ± 0.24968474901068555
	data : 0.1177762508392334
	model : 0.06832466125488282
			 train-loss:  2.062419549624125 	 ± 0.2425001787300266
	data : 0.11771745681762695
	model : 0.06851282119750976
			 train-loss:  2.052351698279381 	 ± 0.23801547240008789
	data : 0.11753726005554199
	model : 0.06810035705566406
			 train-loss:  2.0386964503456566 	 ± 0.23728123524344732
	data : 0.11797294616699219
	model : 0.06896457672119141
			 train-loss:  2.0250661108228893 	 ± 0.23734540018036085
	data : 0.11724247932434081
	model : 0.06984877586364746
			 train-loss:  2.0344845119275545 	 ± 0.2344454430202134
	data : 0.11654505729675294
	model : 0.06966218948364258
			 train-loss:  2.032328176498413 	 ± 0.2287023793765018
	data : 0.11669526100158692
	model : 0.06886630058288574
			 train-loss:  2.0324340093703497 	 ± 0.22319117055629958
	data : 0.11739611625671387
	model : 0.06901798248291016
			 train-loss:  2.020162593234669 	 ± 0.22519403421742218
	data : 0.11722283363342285
	model : 0.06816220283508301
			 train-loss:  2.015822109968766 	 ± 0.2211830545535085
	data : 0.11794981956481934
	model : 0.06835703849792481
			 train-loss:  2.009906157851219 	 ± 0.2183769551980388
	data : 0.11757421493530273
	model : 0.06840395927429199
			 train-loss:  2.0086863613128663 	 ± 0.21404827601590973
	data : 0.11749134063720704
	model : 0.06867341995239258
			 train-loss:  2.0012628252689657 	 ± 0.21314832217905366
	data : 0.11722655296325683
	model : 0.06797995567321777
			 train-loss:  2.0020842684639826 	 ± 0.2092058242430769
	data : 0.11789541244506836
	model : 0.06902422904968261
			 train-loss:  2.0104487410613467 	 ± 0.20998334661945028
	data : 0.11694889068603516
	model : 0.06875786781311036
			 train-loss:  2.0033867852441194 	 ± 0.20968774770754822
	data : 0.11712841987609864
	model : 0.06875276565551758
			 train-loss:  2.0090012073516847 	 ± 0.20836854405875285
	data : 0.11730394363403321
	model : 0.06917328834533691
			 train-loss:  2.01489931537259 	 ± 0.2075102830989008
	data : 0.11688265800476075
	model : 0.06925525665283203
			 train-loss:  2.0187105759978294 	 ± 0.20534160160863926
	data : 0.11672458648681641
	model : 0.06910500526428223
			 train-loss:  2.0306861183860083 	 ± 0.21325261199132245
	data : 0.11675171852111817
	model : 0.06923890113830566
			 train-loss:  2.033082555322086 	 ± 0.21054368487632313
	data : 0.11673717498779297
	model : 0.06842203140258789
			 train-loss:  2.0298405613218033 	 ± 0.2083733870497642
	data : 0.11736493110656739
	model : 0.0685279369354248
			 train-loss:  2.0317225952943168 	 ± 0.205760404113318
	data : 0.1173166275024414
	model : 0.06909465789794922
			 train-loss:  2.0369882680274345 	 ± 0.20540514720662975
	data : 0.11677842140197754
	model : 0.06914453506469727
			 train-loss:  2.038455671385715 	 ± 0.20288087363875326
	data : 0.11674823760986328
	model : 0.06894350051879883
			 train-loss:  2.043794952906095 	 ± 0.20294962149933077
	data : 0.11685428619384766
	model : 0.06997809410095215
			 train-loss:  2.0490865856409073 	 ± 0.20310314829676793
	data : 0.11596641540527344
	model : 0.06911292076110839
			 train-loss:  2.0438978119594293 	 ± 0.20327740774755618
	data : 0.11667909622192382
	model : 0.06904549598693847
			 train-loss:  2.0373576141539074 	 ± 0.2051623651505301
	data : 0.11683864593505859
	model : 0.06810522079467773
			 train-loss:  2.0361150115035302 	 ± 0.2029225771653215
	data : 0.1177222728729248
	model : 0.0673750400543213
			 train-loss:  2.0403891612182963 	 ± 0.20255186423653762
	data : 0.11833429336547852
	model : 0.06723766326904297
			 train-loss:  2.0536116732491387 	 ± 0.21865103050671014
	data : 0.118434476852417
	model : 0.06805977821350098
			 train-loss:  2.0605719633724378 	 ± 0.2212442464629998
	data : 0.11791901588439942
	model : 0.0682159423828125
			 train-loss:  2.0682239507107023 	 ± 0.22494662039066143
	data : 0.11766638755798339
	model : 0.06899557113647461
			 train-loss:  2.0747563764452934 	 ± 0.22705155018325512
	data : 0.1167985439300537
	model : 0.06983675956726074
			 train-loss:  2.069702586349176 	 ± 0.22743411054933954
	data : 0.11615424156188965
	model : 0.06904048919677734
			 train-loss:  2.072458572387695 	 ± 0.2259732874516875
	data : 0.11681165695190429
	model : 0.0688211441040039
			 train-loss:  2.072179219302009 	 ± 0.2237556146457199
	data : 0.1169367790222168
	model : 0.06867151260375977
			 train-loss:  2.0657220414051642 	 ± 0.22634091278380625
	data : 0.11723370552062988
	model : 0.06859207153320312
			 train-loss:  2.0624976945373246 	 ± 0.22539790271486185
	data : 0.11733298301696778
	model : 0.06863217353820801
			 train-loss:  2.0675763620270624 	 ± 0.2263413738676768
	data : 0.11711616516113281
	model : 0.06932873725891113
			 train-loss:  2.0671988378871573 	 ± 0.22429144368258289
	data : 0.11654319763183593
	model : 0.0697209358215332
			 train-loss:  2.083782985806465 	 ± 0.25403776602013955
	data : 0.11610236167907714
	model : 0.06885557174682617
			 train-loss:  2.0864386119340597 	 ± 0.2525825051666109
	data : 0.1167940616607666
	model : 0.06921625137329102
			 train-loss:  2.0958979684731056 	 ± 0.2603810381629979
	data : 0.11664104461669922
	model : 0.0689629077911377
			 train-loss:  2.1013696133080177 	 ± 0.26150643892118614
	data : 0.11675338745117188
	model : 0.06898212432861328
			 train-loss:  2.0998830060164133 	 ± 0.2595693498413869
	data : 0.1165287971496582
	model : 0.068790864944458
			 train-loss:  2.0985796901046254 	 ± 0.2576308146816091
	data : 0.11670799255371093
	model : 0.06966328620910645
			 train-loss:  2.0970478000179416 	 ± 0.25582463147245227
	data : 0.11600522994995117
	model : 0.0692774772644043
			 train-loss:  2.1018836630715265 	 ± 0.25662680736853843
	data : 0.11637563705444336
	model : 0.06911396980285645
			 train-loss:  2.1008941773325205 	 ± 0.25473511699429796
	data : 0.11676497459411621
	model : 0.06918206214904785
			 train-loss:  2.1034068235984216 	 ± 0.2535660258957815
	data : 0.11688833236694336
	model : 0.06822352409362793
			 train-loss:  2.101555155985283 	 ± 0.25208017675092315
	data : 0.11749005317687988
	model : 0.06840553283691406
			 train-loss:  2.097250947311743 	 ± 0.25262367547565784
	data : 0.11723184585571289
	model : 0.0686251163482666
			 train-loss:  2.093914698151981 	 ± 0.25224186452371555
	data : 0.11691532135009766
	model : 0.06909523010253907
			 train-loss:  2.091379053350808 	 ± 0.2512788262305971
	data : 0.11675400733947754
	model : 0.06879839897155762
			 train-loss:  2.0919422541345867 	 ± 0.2495213818424442
	data : 0.1170569896697998
	model : 0.06891493797302246
			 train-loss:  2.094542985231104 	 ± 0.24871162374718528
	data : 0.1171426773071289
	model : 0.0688638687133789
			 train-loss:  2.0959483401642904 	 ± 0.2472621419284182
	data : 0.11722455024719239
	model : 0.0690112590789795
			 train-loss:  2.0953323498164136 	 ± 0.24561834661264678
	data : 0.11715211868286132
	model : 0.06889777183532715
			 train-loss:  2.093505438920614 	 ± 0.24445197681672076
	data : 0.11693549156188965
	model : 0.06834359169006347
			 train-loss:  2.09733713944753 	 ± 0.24504382218427317
	data : 0.11757330894470215
	model : 0.06831278800964355
			 train-loss:  2.0986637739758742 	 ± 0.24369732566022986
	data : 0.11772446632385254
	model : 0.06808962821960449
			 train-loss:  2.099737661225455 	 ± 0.24229064067239522
	data : 0.11789093017578126
	model : 0.06787514686584473
			 train-loss:  2.0960228886359777 	 ± 0.24292940069937818
	data : 0.11804227828979492
	model : 0.06871504783630371
			 train-loss:  2.0972353401063364 	 ± 0.24162436816868194
	data : 0.11727533340454102
	model : 0.06949315071105958
			 train-loss:  2.096670262515545 	 ± 0.24016199075573738
	data : 0.11633062362670898
	model : 0.0704808235168457
			 train-loss:  2.097856169865455 	 ± 0.2389104864229933
	data : 0.11541781425476075
	model : 0.0705216407775879
			 train-loss:  2.0943839622706903 	 ± 0.23949676184639512
	data : 0.11516661643981933
	model : 0.0705939769744873
			 train-loss:  2.0928211226520768 	 ± 0.23846994071008226
	data : 0.11501932144165039
	model : 0.0692784309387207
			 train-loss:  2.0935485462347665 	 ± 0.23713884688759754
	data : 0.11617789268493653
	model : 0.06928439140319824
			 train-loss:  2.093185914264006 	 ± 0.2357632129904187
	data : 0.11621918678283691
	model : 0.06831164360046386
			 train-loss:  2.0894170237141987 	 ± 0.23695009848878687
	data : 0.11713409423828125
	model : 0.0691380500793457
			 train-loss:  2.0893528064092 	 ± 0.23558513320012703
	data : 0.11658134460449218
	model : 0.06912417411804199
			 train-loss:  2.087559513070367 	 ± 0.23483920567005478
	data : 0.11647567749023438
	model : 0.06878695487976075
			 train-loss:  2.0930064233501304 	 ± 0.23904111138105053
	data : 0.11687469482421875
	model : 0.06884417533874512
			 train-loss:  2.0905983593728807 	 ± 0.23879247790742272
	data : 0.11684985160827636
	model : 0.06880393028259277
			 train-loss:  2.0910825978268632 	 ± 0.23752123613594506
	data : 0.11675043106079101
	model : 0.06815261840820312
			 train-loss:  2.0912046134471893 	 ± 0.23622970038791638
	data : 0.1175002098083496
	model : 0.06838622093200683
			 train-loss:  2.0859280209387503 	 ± 0.24034543201066452
	data : 0.11752772331237793
	model : 0.06942424774169922
			 train-loss:  2.0886424640391734 	 ± 0.2404924911063453
	data : 0.11656303405761718
	model : 0.06937737464904785
			 train-loss:  2.087560048856233 	 ± 0.23945346956532193
	data : 0.11654100418090821
	model : 0.06946916580200195
			 train-loss:  2.0837057096262774 	 ± 0.241147272559928
	data : 0.11661500930786133
	model : 0.06906557083129883
			 train-loss:  2.0867849539235697 	 ± 0.2417907140444163
	data : 0.11686897277832031
	model : 0.06888890266418457
			 train-loss:  2.0808422346504365 	 ± 0.24757187689563945
	data : 0.11710615158081054
	model : 0.06874566078186035
			 train-loss:  2.078969777232469 	 ± 0.24701482197959873
	data : 0.11720709800720215
	model : 0.06828575134277344
			 train-loss:  2.078732800483704 	 ± 0.2457879547390449
	data : 0.11761412620544434
	model : 0.06814875602722167
			 train-loss:  2.0791339048064583 	 ± 0.24460104524289064
	data : 0.11761388778686524
	model : 0.06856880187988282
			 train-loss:  2.0787696370891497 	 ± 0.2434265961571113
	data : 0.1171712875366211
	model : 0.06852192878723144
			 train-loss:  2.0804525764243116 	 ± 0.2428375894845587
	data : 0.11708793640136719
	model : 0.0685232162475586
			 train-loss:  2.0780241512335262 	 ± 0.24292075119505144
	data : 0.11716432571411133
	model : 0.07014145851135253
			 train-loss:  2.0805355616978236 	 ± 0.24311403427946487
	data : 0.1155052661895752
	model : 0.07099728584289551
			 train-loss:  2.079389711595931 	 ± 0.24224926578772288
	data : 0.11476550102233887
	model : 0.07082228660583496
			 train-loss:  2.083339428233209 	 ± 0.24451969101026763
	data : 0.11496243476867676
	model : 0.07122774124145508
			 train-loss:  2.085771494441562 	 ± 0.24468176990693155
	data : 0.11465344429016114
	model : 0.07125868797302246
			 train-loss:  2.0829975134735808 	 ± 0.24525694057929087
	data : 0.11467447280883789
	model : 0.07023334503173828
			 train-loss:  2.083596746488051 	 ± 0.24421973645241776
	data : 0.11607327461242675
	model : 0.0704688549041748
			 train-loss:  2.0843444310867034 	 ± 0.24324359473064308
	data : 0.1158109188079834
	model : 0.06963653564453125
			 train-loss:  2.085578715162618 	 ± 0.24250416357919294
	data : 0.11659126281738282
	model : 0.06844611167907715
			 train-loss:  2.0870014154805547 	 ± 0.24189778569599915
	data : 0.11748828887939453
	model : 0.06770048141479493
			 train-loss:  2.0872593501157928 	 ± 0.24085010113847322
	data : 0.11806726455688477
	model : 0.06668014526367187
			 train-loss:  2.087748744176782 	 ± 0.2398575637064871
	data : 0.11873388290405273
	model : 0.06732988357543945
			 train-loss:  2.0868096289963556 	 ± 0.23903370296210907
	data : 0.11813821792602539
	model : 0.0677828311920166
			 train-loss:  2.0892080241798334 	 ± 0.23940765758706048
	data : 0.11762747764587403
	model : 0.06855344772338867
			 train-loss:  2.0861738889904347 	 ± 0.2406395576495164
	data : 0.11699142456054687
	model : 0.06930809020996094
			 train-loss:  2.0828353867811313 	 ± 0.24235502960098526
	data : 0.11630301475524903
	model : 0.07038264274597168
			 train-loss:  2.083002739151319 	 ± 0.2413500090067012
	data : 0.11532940864562988
	model : 0.0696444034576416
			 train-loss:  2.0838602249287375 	 ± 0.24053410806554282
	data : 0.11610379219055175
	model : 0.07017130851745605
			 train-loss:  2.0810821554699883 	 ± 0.24148759866358857
	data : 0.11568474769592285
	model : 0.07008485794067383
			 train-loss:  2.0816936677064355 	 ± 0.24059876543970485
	data : 0.11569318771362305
	model : 0.06995930671691894
			 train-loss:  2.0789423671460923 	 ± 0.2415615794082462
	data : 0.11582741737365723
	model : 0.07000298500061035
			 train-loss:  2.0777124195098877 	 ± 0.2409829128804044
	data : 0.11586947441101074
	model : 0.06924266815185547
			 train-loss:  2.076153778840625 	 ± 0.24065647599336493
	data : 0.11659855842590332
	model : 0.06921820640563965
			 train-loss:  2.0810913786174745 	 ± 0.2460312539171745
	data : 0.11678361892700195
	model : 0.06932892799377441
			 train-loss:  2.0781000843271613 	 ± 0.2473759333254788
	data : 0.11680288314819336
	model : 0.06892485618591308
			 train-loss:  2.086058172144631 	 ± 0.2623487419198276
	data : 0.11723856925964356
	model : 0.06781535148620606
			 train-loss:  2.0891039454019986 	 ± 0.2636173813783779
	data : 0.11807870864868164
	model : 0.06859450340270996
			 train-loss:  2.0879432463463936 	 ± 0.26294252826255426
	data : 0.11733775138854981
	model : 0.06875081062316894
			 train-loss:  2.0913109111063406 	 ± 0.2647653438840246
	data : 0.11726126670837403
	model : 0.06885061264038086
			 train-loss:  2.0902710113310277 	 ± 0.2640385543223831
	data : 0.11715154647827149
	model : 0.06909008026123047
			 train-loss:  2.0896727033515474 	 ± 0.2631419715160096
	data : 0.11677684783935546
	model : 0.06992292404174805
			 train-loss:  2.087646382826346 	 ± 0.2632128065476083
	data : 0.11619381904602051
	model : 0.06965861320495606
			 train-loss:  2.085095066358061 	 ± 0.2639134454157393
	data : 0.11631212234497071
	model : 0.06875629425048828
			 train-loss:  2.0815241180197166 	 ± 0.2662257269103117
	data : 0.11705079078674316
	model : 0.06777381896972656
			 train-loss:  2.082525978917661 	 ± 0.26551846042956057
	data : 0.11810474395751953
	model : 0.06737346649169922
			 train-loss:  2.0860153805437704 	 ± 0.26771838875198845
	data : 0.11836180686950684
	model : 0.06651773452758789
			 train-loss:  2.0848242793764387 	 ± 0.26712990671875486
	data : 0.11910037994384766
	model : 0.06704940795898437
			 train-loss:  2.084175506382124 	 ± 0.26629161854590416
	data : 0.11867322921752929
	model : 0.06766743659973144
			 train-loss:  2.0836305106189887 	 ± 0.26543121736127984
	data : 0.1181304931640625
	model : 0.06839137077331543
			 train-loss:  2.082381642781771 	 ± 0.2649198387805323
	data : 0.11724720001220704
	model : 0.06813316345214844
			 train-loss:  2.082168194154898 	 ± 0.2640107147197304
	data : 0.11762080192565919
	model : 0.06940302848815919
			 train-loss:  2.082846458204861 	 ± 0.2632246235939922
	data : 0.11658973693847656
	model : 0.069110107421875
			 train-loss:  2.084060439508255 	 ± 0.2627286174439643
	data : 0.1169238567352295
	model : 0.06918916702270508
			 train-loss:  2.0835764562191605 	 ± 0.2618987563771867
	data : 0.11671314239501954
	model : 0.06935935020446778
			 train-loss:  2.083370143497312 	 ± 0.26102444945839937
	data : 0.11669712066650391
	model : 0.06984004974365235
			 train-loss:  2.0843119613276233 	 ± 0.260399248690561
	data : 0.11618638038635254
	model : 0.06864209175109863
			 train-loss:  2.082735753854116 	 ± 0.2602419969894998
	data : 0.11725420951843261
	model : 0.06863102912902833
			 train-loss:  2.079367212901842 	 ± 0.26263937508492907
	data : 0.1171809196472168
	model : 0.06870040893554688
			 train-loss:  2.0793502534690655 	 ± 0.2617740870710212
	data : 0.1174271583557129
	model : 0.06836948394775391
			 train-loss:  2.078845313951081 	 ± 0.2609914691397927
	data : 0.11769399642944336
	model : 0.0688316822052002
			 train-loss:  2.0794677780820177 	 ± 0.26025662978616204
	data : 0.11728663444519043
	model : 0.06960997581481934
			 train-loss:  2.07923316801748 	 ± 0.259432070856407
	data : 0.11651396751403809
	model : 0.06965370178222656
			 train-loss:  2.0799158215522766 	 ± 0.25873884428434324
	data : 0.11651897430419922
	model : 0.06965403556823731
			 train-loss:  2.0794626694575995 	 ± 0.2579756138530285
	data : 0.11624526977539062
	model : 0.06991825103759766
			 train-loss:  2.0793103492712675 	 ± 0.2571650218568337
	data : 0.11596589088439942
	model : 0.06980328559875489
			 train-loss:  2.081306333062034 	 ± 0.2575798451297037
	data : 0.11611504554748535
	model : 0.06896591186523438
			 train-loss:  2.0784886449575426 	 ± 0.2592201086868115
	data : 0.11679244041442871
	model : 0.06852836608886718
			 train-loss:  2.077398199472368 	 ± 0.25878167538289504
	data : 0.11721267700195312
	model : 0.06823792457580566
			 train-loss:  2.081023709273633 	 ± 0.2620511568470997
	data : 0.11759710311889648
	model : 0.06799631118774414
			 train-loss:  2.079351293528738 	 ± 0.26211185657272845
	data : 0.11789135932922364
	model : 0.06721863746643067
			 train-loss:  2.080997342016639 	 ± 0.26215520486060656
	data : 0.11865468025207519
	model : 0.06900219917297364
			 train-loss:  2.081274752183394 	 ± 0.2613837313295929
	data : 0.11691923141479492
	model : 0.06843762397766114
			 train-loss:  2.0821875721575265 	 ± 0.26085889791991285
	data : 0.11748995780944824
	model : 0.06793889999389649
			 train-loss:  2.0816563617683457 	 ± 0.26016675036987974
	data : 0.11780505180358887
	model : 0.06737942695617676
			 train-loss:  2.0824910515830632 	 ± 0.2596154672424767
	data : 0.11838479042053222
	model : 0.06734514236450195
			 train-loss:  2.0860633793667223 	 ± 0.26295495710801386
	data : 0.11834273338317872
	model : 0.06659269332885742
			 train-loss:  2.086239450118121 	 ± 0.2621904107835884
	data : 0.1191481590270996
	model : 0.06762661933898925
			 train-loss:  2.084197459862246 	 ± 0.26277491133691655
	data : 0.11822052001953125
	model : 0.06831526756286621
			 train-loss:  2.085120931614277 	 ± 0.26228805834501073
	data : 0.11753988265991211
	model : 0.06914634704589843
			 train-loss:  2.0865858731242275 	 ± 0.2622336509052777
	data : 0.11678609848022461
	model : 0.07006053924560547
			 train-loss:  2.086952979537262 	 ± 0.26152359870454095
	data : 0.11602563858032226
	model : 0.06994810104370117
			 train-loss:  2.086023919241769 	 ± 0.2610631246713616
	data : 0.11602506637573243
	model : 0.06899700164794922
			 train-loss:  2.0847560580481184 	 ± 0.2608601623154599
	data : 0.1168858528137207
	model : 0.06913447380065918
			 train-loss:  2.0856109103240534 	 ± 0.2603693302089547
	data : 0.11682825088500977
	model : 0.06903204917907715
			 train-loss:  2.0855578217613564 	 ± 0.2596378862571097
	data : 0.11705994606018066
	model : 0.06896185874938965
			 train-loss:  2.083725065492385 	 ± 0.26006370522074435
	data : 0.11704950332641602
	model : 0.0682948112487793
			 train-loss:  2.082168585062027 	 ± 0.2601750221789583
	data : 0.11768455505371093
	model : 0.06982393264770508
			 train-loss:  2.082160917434903 	 ± 0.25945533158926953
	data : 0.11626019477844238
	model : 0.06888875961303711
			 train-loss:  2.0821653466958265 	 ± 0.25874156727327374
	data : 0.11717877388000489
	model : 0.06878604888916015
			 train-loss:  2.082427589619746 	 ± 0.2580579069402996
	data : 0.11716108322143555
	model : 0.06883459091186524
			 train-loss:  2.0809813835050748 	 ± 0.2580982502352457
	data : 0.11722736358642578
	model : 0.06953625679016114
			 train-loss:  2.0797727269095345 	 ± 0.2579213520035725
	data : 0.11670932769775391
	model : 0.06871147155761718
			 train-loss:  2.080542831651626 	 ± 0.2574402597616955
	data : 0.11756696701049804
	model : 0.06949310302734375
			 train-loss:  2.0786460064311716 	 ± 0.25805094635727893
	data : 0.11679935455322266
	model : 0.0698401927947998
			 train-loss:  2.0796452538764223 	 ± 0.2577262227466034
	data : 0.11658225059509278
	model : 0.0707254409790039
			 train-loss:  2.0804598754045194 	 ± 0.2572860680678557
	data : 0.11553244590759278
	model : 0.07065877914428711
			 train-loss:  2.078926886382856 	 ± 0.2574720968285208
	data : 0.1155313491821289
	model : 0.06994810104370117
			 train-loss:  2.0797215587805704 	 ± 0.25703071516431375
	data : 0.11613950729370118
	model : 0.06923785209655761
			 train-loss:  2.0803548662612834 	 ± 0.2565098576377065
	data : 0.11664962768554688
	model : 0.06903514862060547
			 train-loss:  2.078748649883764 	 ± 0.25681069903282794
	data : 0.11667289733886718
	model : 0.0672952651977539
			 train-loss:  2.0804262136675646 	 ± 0.25720599185888976
	data : 0.11859750747680664
	model : 0.0672792911529541
			 train-loss:  2.082286992439857 	 ± 0.2578514897739251
	data : 0.11856765747070312
	model : 0.06802325248718262
			 train-loss:  2.0835885940765846 	 ± 0.2578343109967171
	data : 0.11779680252075195
	model : 0.06849040985107421
			 train-loss:  2.0823185909823114 	 ± 0.25779295450190115
	data : 0.11749353408813476
	model : 0.06759319305419922
			 train-loss:  2.0832029117478266 	 ± 0.25744052420484387
	data : 0.11815023422241211
	model : 0.0676797866821289
			 train-loss:  2.0812835795196456 	 ± 0.2582091782580588
	data : 0.11808147430419921
	model : 0.06768913269042968
			 train-loss:  2.080588690638542 	 ± 0.25774931828984077
	data : 0.11824898719787598
	model : 0.06703705787658691
			 train-loss:  2.079395615639378 	 ± 0.2576603883389189
	data : 0.11866884231567383
	model : 0.06731901168823243
			 train-loss:  2.080743483977743 	 ± 0.2577312252695514
	data : 0.1183502197265625
	model : 0.06831574440002441
			 train-loss:  2.079637573857613 	 ± 0.25757565752481265
	data : 0.11751117706298828
	model : 0.06854453086853027
			 train-loss:  2.081332833743563 	 ± 0.2580763464951553
	data : 0.11726255416870117
	model : 0.06864924430847168
			 train-loss:  2.0800458879005617 	 ± 0.25810148550926654
	data : 0.11709909439086914
	model : 0.06966395378112793
			 train-loss:  2.0820887893149 	 ± 0.25913037803539013
	data : 0.11652636528015137
	model : 0.06959972381591797
			 train-loss:  2.0807797995166504 	 ± 0.259185522861209
	data : 0.11662425994873046
	model : 0.06963658332824707
			 train-loss:  2.078753279378781 	 ± 0.2602004468199741
	data : 0.11670808792114258
	model : 0.06981720924377441
			 train-loss:  2.0798288298565804 	 ± 0.2600402754850383
	data : 0.11662821769714356
	model : 0.0706148624420166
			 train-loss:  2.0793869234266733 	 ± 0.25949904463239704
	data : 0.11566996574401855
	model : 0.07051587104797363
			 train-loss:  2.0792586927730325 	 ± 0.2588900567072858
	data : 0.11548905372619629
	model : 0.07058396339416503
			 train-loss:  2.0797641997067435 	 ± 0.2583831043053262
	data : 0.11552481651306153
	model : 0.07045979499816894
			 train-loss:  2.0802748550271764 	 ± 0.2578830660842451
	data : 0.11554765701293945
	model : 0.07032213211059571
			 train-loss:  2.0803315383251584 	 ± 0.2572811600198427
	data : 0.11553244590759278
	model : 0.0692929744720459
			 train-loss:  2.080244263937307 	 ± 0.2566853094996409
	data : 0.11662726402282715
	model : 0.06917057037353516
			 train-loss:  2.082657099873931 	 ± 0.2585227189323103
	data : 0.11673135757446289
	model : 0.06917347908020019
			 train-loss:  2.083208683998354 	 ± 0.25805371992715126
	data : 0.11652569770812989
	model : 0.06928372383117676
			 train-loss:  2.0852696480007347 	 ± 0.2592450173295895
	data : 0.11650009155273437
	model : 0.06985507011413575
			 train-loss:  2.0854706916634895 	 ± 0.2586694889355117
	data : 0.1158144474029541
	model : 0.06999492645263672
			 train-loss:  2.0856123154813595 	 ± 0.25808944404324824
	data : 0.11585965156555175
	model : 0.06993861198425293
			 train-loss:  2.085796529890725 	 ± 0.2575193650412283
	data : 0.11594815254211426
	model : 0.06991133689880372
			 train-loss:  2.0861214603389704 	 ± 0.2569841138656229
	data : 0.11606688499450683
	model : 0.06991147994995117
			 train-loss:  2.0848728466461592 	 ± 0.25708129757782294
	data : 0.11600604057312011
	model : 0.06973013877868653
			 train-loss:  2.084988401404449 	 ± 0.25651261777137724
	data : 0.11636347770690918
	model : 0.0697758674621582
			 train-loss:  2.083609265751309 	 ± 0.2567729263381267
	data : 0.11634063720703125
	model : 0.06976852416992188
			 train-loss:  2.083590848783476 	 ± 0.25620436377651695
	data : 0.11632742881774902
	model : 0.06976613998413086
			 train-loss:  2.083551222532331 	 ± 0.2556401081964094
	data : 0.1163745403289795
	model : 0.06952085494995117
			 train-loss:  2.0836096066131926 	 ± 0.25508039459607706
	data : 0.11661195755004883
	model : 0.06933250427246093
			 train-loss:  2.0835478862820755 	 ± 0.2545245473668533
	data : 0.116542387008667
	model : 0.06906547546386718
			 train-loss:  2.08347001231235 	 ± 0.2539733644302384
	data : 0.1166799545288086
	model : 0.06880178451538085
			 train-loss:  2.084846259711625 	 ± 0.25428108749328593
	data : 0.11693120002746582
	model : 0.06848273277282715
			 train-loss:  2.0847189945393594 	 ± 0.25373984866115823
	data : 0.11690831184387207
	model : 0.06838369369506836
			 train-loss:  2.0848424613731615 	 ± 0.2532017409554445
	data : 0.11682443618774414
	model : 0.0683072566986084
			 train-loss:  2.084081539740929 	 ± 0.25292696549363114
	data : 0.11699376106262208
	model : 0.06803936958312988
			 train-loss:  2.082720844796363 	 ± 0.25324509317323174
	data : 0.11720261573791504
	model : 0.06790575981140137
			 train-loss:  2.0843682369943393 	 ± 0.25396671757694894
	data : 0.11737093925476075
	model : 0.06778974533081054
			 train-loss:  2.0843187927696776 	 ± 0.2534314947258765
	data : 0.11774721145629882
	model : 0.06769313812255859
			 train-loss:  2.0840933473170304 	 ± 0.25292232930002145
	data : 0.1180004596710205
	model : 0.06797080039978028
			 train-loss:  2.084336627976166 	 ± 0.25242055212392894
	data : 0.11773142814636231
	model : 0.06845030784606934
			 train-loss:  2.0838212112585706 	 ± 0.25202012347832253
	data : 0.11748294830322266
	model : 0.06840319633483886
			 train-loss:  2.0818613312551095 	 ± 0.25332285642716285
	data : 0.11752586364746094
	model : 0.06889162063598633
			 train-loss:  2.082458038468006 	 ± 0.25296858388592325
	data : 0.11736316680908203
	model : 0.06917266845703125
			 train-loss:  2.0835531498669595 	 ± 0.25302170147378883
	data : 0.11707892417907714
	model : 0.06920356750488281
			 train-loss:  2.0840730202979727 	 ± 0.25263269563355617
	data : 0.11698079109191895
	model : 0.06922025680541992
			 train-loss:  2.0844762699944632 	 ± 0.25219526688937755
	data : 0.11663470268249512
	model : 0.06944236755371094
			 train-loss:  2.0834800387785686 	 ± 0.2521647534086685
	data : 0.11642961502075196
	model : 0.06914033889770507
			 train-loss:  2.0839129860101924 	 ± 0.25174538012307507
	data : 0.1162651538848877
	model : 0.06906642913818359
			 train-loss:  2.084322068479753 	 ± 0.25131956581290915
	data : 0.1163243293762207
	model : 0.06829776763916015
			 train-loss:  2.084618307500479 	 ± 0.25085778300667605
	data : 0.11703100204467773
	model : 0.0676124095916748
			 train-loss:  2.0851148076057435 	 ± 0.25047812352004273
	data : 0.11764192581176758
	model : 0.06758308410644531
			 train-loss:  2.086813956617834 	 ± 0.25141819717618685
	data : 0.11743879318237305
	model : 0.0675445556640625
			 train-loss:  2.0869651488841527 	 ± 0.250930288662388
	data : 0.11753716468811035
	model : 0.06690220832824707
			 train-loss:  2.0883727172617856 	 ± 0.25142873133579563
	data : 0.11808505058288574
	model : 0.06743593215942383
			 train-loss:  2.0894748859518155 	 ± 0.25154494972473207
	data : 0.11750965118408203
	model : 0.06709966659545899
			 train-loss:  2.0902397244584328 	 ± 0.25134699006833017
	data : 0.11782393455505372
	model : 0.06683611869812012
			 train-loss:  2.0893748472444713 	 ± 0.251235495254522
	data : 0.11726770401000977
	model : 0.05786242485046387
#epoch  54    val-loss:  2.502927541732788  train-loss:  2.0893748472444713  lr:  7.8125e-05
			 train-loss:  1.888548493385315 	 ± 0.0
	data : 5.589311122894287
	model : 0.07702517509460449
			 train-loss:  1.8879631757736206 	 ± 0.0005853176116943359
	data : 2.8595036268234253
	model : 0.07305359840393066
			 train-loss:  1.908072789510091 	 ± 0.028443303729779042
	data : 1.945217212041219
	model : 0.07105414072672527
			 train-loss:  1.8915584087371826 	 ± 0.03774838357117003
	data : 1.4885873794555664
	model : 0.07065820693969727
			 train-loss:  1.8972873210906982 	 ± 0.0356543702343205
	data : 1.2141708374023437
	model : 0.06972532272338867
			 train-loss:  1.9079299767812092 	 ± 0.04031987555460505
	data : 0.11997036933898926
	model : 0.06746125221252441
			 train-loss:  1.9403542109898158 	 ± 0.08775782804077935
	data : 0.11800823211669922
	model : 0.06752057075500488
			 train-loss:  1.9359393119812012 	 ± 0.08291680268452524
	data : 0.11796865463256836
	model : 0.0679201602935791
			 train-loss:  1.9476553334130182 	 ± 0.08490822544183488
	data : 0.11764440536499024
	model : 0.06802401542663575
			 train-loss:  1.9452225327491761 	 ± 0.08088097887972935
	data : 0.11767301559448243
	model : 0.06787257194519043
			 train-loss:  1.930171088738875 	 ± 0.09062278651486527
	data : 0.11792840957641601
	model : 0.06878657341003418
			 train-loss:  1.9009272456169128 	 ± 0.13013585701887412
	data : 0.11718316078186035
	model : 0.0689208984375
			 train-loss:  1.9175590918614314 	 ± 0.1376664172407194
	data : 0.11706318855285644
	model : 0.06883621215820312
			 train-loss:  1.9401630674089705 	 ± 0.15569374218716944
	data : 0.11693096160888672
	model : 0.06798858642578125
			 train-loss:  1.9615276018778482 	 ± 0.17033705389267087
	data : 0.11745195388793946
	model : 0.06896953582763672
			 train-loss:  1.9443561732769012 	 ± 0.1778318358344305
	data : 0.11677861213684082
	model : 0.06897273063659667
			 train-loss:  1.985237345975988 	 ± 0.23770620842205217
	data : 0.11670522689819336
	model : 0.06942129135131836
			 train-loss:  2.033969905641344 	 ± 0.30616625762729544
	data : 0.11630206108093262
	model : 0.0696258544921875
			 train-loss:  2.043224209233334 	 ± 0.30057572841498115
	data : 0.11634883880615235
	model : 0.07066397666931153
			 train-loss:  2.038608509302139 	 ± 0.2936550171848397
	data : 0.11554360389709473
	model : 0.0705836296081543
			 train-loss:  2.075514787719363 	 ± 0.3307089214120985
	data : 0.11567130088806152
	model : 0.07036762237548828
			 train-loss:  2.087051050229506 	 ± 0.32740172971730114
	data : 0.11576662063598633
	model : 0.06963109970092773
			 train-loss:  2.079635739326477 	 ± 0.3220886305641864
	data : 0.11639595031738281
	model : 0.06967353820800781
			 train-loss:  2.074358051021894 	 ± 0.3163213280030202
	data : 0.11620960235595704
	model : 0.06858158111572266
			 train-loss:  2.073366627693176 	 ± 0.30996839408690025
	data : 0.11716661453247071
	model : 0.06773571968078614
			 train-loss:  2.069611567717332 	 ± 0.3045283517396343
	data : 0.11782946586608886
	model : 0.06786880493164063
			 train-loss:  2.0651727694052235 	 ± 0.29969162542980826
	data : 0.11786627769470215
	model : 0.06811513900756835
			 train-loss:  2.0834470561572482 	 ± 0.3092313491363778
	data : 0.11757936477661132
	model : 0.06805634498596191
			 train-loss:  2.07102278594313 	 ± 0.31088389595999627
	data : 0.11765942573547364
	model : 0.0689396858215332
			 train-loss:  2.0705427447954814 	 ± 0.3056695159055881
	data : 0.11685762405395508
	model : 0.06892075538635253
			 train-loss:  2.0818402113453036 	 ± 0.3069997397567427
	data : 0.11689677238464355
	model : 0.06895670890808106
			 train-loss:  2.0694057047367096 	 ± 0.30999466051320507
	data : 0.11676106452941895
	model : 0.06884422302246093
			 train-loss:  2.067802971059626 	 ± 0.30539624886205113
	data : 0.1168886661529541
	model : 0.06880712509155273
			 train-loss:  2.0735765485202564 	 ± 0.3026941593578434
	data : 0.11691479682922364
	model : 0.06846208572387695
			 train-loss:  2.0704650708607266 	 ± 0.29888977321588917
	data : 0.11719002723693847
	model : 0.06930055618286132
			 train-loss:  2.057398580842548 	 ± 0.3046788911667905
	data : 0.11633639335632324
	model : 0.06913352012634277
			 train-loss:  2.050362947824839 	 ± 0.30348366258111836
	data : 0.11667218208312988
	model : 0.06936473846435547
			 train-loss:  2.046721006694593 	 ± 0.3002821107537167
	data : 0.11658663749694824
	model : 0.06950411796569825
			 train-loss:  2.056230765122634 	 ± 0.3021487394124998
	data : 0.11655473709106445
	model : 0.07030773162841797
			 train-loss:  2.0521395415067674 	 ± 0.2994399781703748
	data : 0.11574196815490723
	model : 0.07097110748291016
			 train-loss:  2.056606964367192 	 ± 0.2971122364495296
	data : 0.1153493881225586
	model : 0.07095909118652344
			 train-loss:  2.063279398850032 	 ± 0.2966466850941799
	data : 0.11528735160827637
	model : 0.07085037231445312
			 train-loss:  2.0714938834656116 	 ± 0.297971185304795
	data : 0.11528563499450684
	model : 0.06992483139038086
			 train-loss:  2.067556305365129 	 ± 0.29569517961869785
	data : 0.11611976623535156
	model : 0.06968126296997071
			 train-loss:  2.0750038570827907 	 ± 0.2965352090629767
	data : 0.11657767295837403
	model : 0.06841726303100586
			 train-loss:  2.0728708168734675 	 ± 0.2936431237928422
	data : 0.11758294105529785
	model : 0.06840901374816895
			 train-loss:  2.0791635792306127 	 ± 0.29362089474732
	data : 0.11761388778686524
	model : 0.06832199096679688
			 train-loss:  2.0847550506393113 	 ± 0.2930640797599575
	data : 0.11763520240783691
	model : 0.06924052238464355
			 train-loss:  2.08122164375928 	 ± 0.291089412883054
	data : 0.11678080558776856
	model : 0.06913995742797852
			 train-loss:  2.072324426174164 	 ± 0.2948173144912576
	data : 0.11669740676879883
	model : 0.06973404884338379
			 train-loss:  2.0891421004837634 	 ± 0.31520580197738285
	data : 0.11613235473632813
	model : 0.06895599365234376
			 train-loss:  2.0917988534157095 	 ± 0.31273631941601254
	data : 0.11672616004943848
	model : 0.06904797554016114
			 train-loss:  2.0871395929804386 	 ± 0.3115886706760498
	data : 0.11678152084350586
	model : 0.06899800300598144
			 train-loss:  2.087937112207766 	 ± 0.30874470520047087
	data : 0.11682262420654296
	model : 0.06890335083007812
			 train-loss:  2.086838804591786 	 ± 0.3060315037821185
	data : 0.1170353889465332
	model : 0.06905503273010254
			 train-loss:  2.0812810638121197 	 ± 0.3060747165498733
	data : 0.11693205833435058
	model : 0.07011981010437011
			 train-loss:  2.0798364898614716 	 ± 0.3035705081353911
	data : 0.11613759994506836
	model : 0.07051639556884766
			 train-loss:  2.0738694462282905 	 ± 0.3042954040606534
	data : 0.11575956344604492
	model : 0.07065744400024414
			 train-loss:  2.073413517515538 	 ± 0.3017255887816647
	data : 0.11556520462036132
	model : 0.07040615081787109
			 train-loss:  2.0738696495691937 	 ± 0.29922115690703893
	data : 0.11572413444519043
	model : 0.07032318115234375
			 train-loss:  2.0699628689250007 	 ± 0.29829736474256635
	data : 0.11583819389343261
	model : 0.07017679214477539
			 train-loss:  2.0756894619234147 	 ± 0.29924331433891294
	data : 0.11598997116088867
	model : 0.06909399032592774
			 train-loss:  2.0685048179020957 	 ± 0.3022012111025018
	data : 0.11683139801025391
	model : 0.06806206703186035
			 train-loss:  2.066190220415592 	 ± 0.3003932817249613
	data : 0.1179286003112793
	model : 0.0680994987487793
			 train-loss:  2.070223800952618 	 ± 0.29981517815680964
	data : 0.11782984733581543
	model : 0.06857943534851074
			 train-loss:  2.0720542922164453 	 ± 0.29790095628904406
	data : 0.11742806434631348
	model : 0.06768722534179687
			 train-loss:  2.0670515120919073 	 ± 0.29844976665019674
	data : 0.118068265914917
	model : 0.06834268569946289
			 train-loss:  2.073664575815201 	 ± 0.3011518918768187
	data : 0.1176079273223877
	model : 0.06913304328918457
			 train-loss:  2.0684808112572934 	 ± 0.3020022097492996
	data : 0.1165811538696289
	model : 0.06854491233825684
			 train-loss:  2.0672608699117387 	 ± 0.3000084849897935
	data : 0.1172184944152832
	model : 0.06805577278137206
			 train-loss:  2.0716289852706478 	 ± 0.3001217152292164
	data : 0.11762833595275879
	model : 0.06898064613342285
			 train-loss:  2.069869857695368 	 ± 0.29839862779584914
	data : 0.11695542335510253
	model : 0.06913866996765136
			 train-loss:  2.0670265824827427 	 ± 0.2973281934093513
	data : 0.1168677806854248
	model : 0.0683929443359375
			 train-loss:  2.065310705352474 	 ± 0.29567606314249795
	data : 0.1176870346069336
	model : 0.069065523147583
			 train-loss:  2.0676546176274617 	 ± 0.29438958364093637
	data : 0.11704297065734863
	model : 0.06881461143493653
			 train-loss:  2.067147598454827 	 ± 0.29247935851228035
	data : 0.11726346015930175
	model : 0.06795692443847656
			 train-loss:  2.0680440850072093 	 ± 0.29067901909249955
	data : 0.11794147491455079
	model : 0.06783418655395508
			 train-loss:  2.0668609279852648 	 ± 0.2889962305055201
	data : 0.118048095703125
	model : 0.06846909523010254
			 train-loss:  2.0688220896298373 	 ± 0.28768319566348854
	data : 0.11745309829711914
	model : 0.06843552589416504
			 train-loss:  2.071133615076542 	 ± 0.2866168341971348
	data : 0.117588472366333
	model : 0.06866536140441895
			 train-loss:  2.06950855107955 	 ± 0.2852127074857221
	data : 0.11721181869506836
	model : 0.06941356658935546
			 train-loss:  2.067353862087901 	 ± 0.2841308115276669
	data : 0.1164280891418457
	model : 0.06881842613220215
			 train-loss:  2.0696488001260414 	 ± 0.28317757035830077
	data : 0.11700959205627441
	model : 0.06903371810913086
			 train-loss:  2.067660222450892 	 ± 0.2820693489792906
	data : 0.11694865226745606
	model : 0.06915297508239746
			 train-loss:  2.0698920011520388 	 ± 0.28115026443863306
	data : 0.11673808097839355
	model : 0.06914339065551758
			 train-loss:  2.0721828008806984 	 ± 0.28030768397721084
	data : 0.11683955192565917
	model : 0.06926817893981933
			 train-loss:  2.0736991112259613 	 ± 0.27904658685619366
	data : 0.11679601669311523
	model : 0.0697472095489502
			 train-loss:  2.069931822744283 	 ± 0.2796728260451681
	data : 0.11633715629577637
	model : 0.06960077285766601
			 train-loss:  2.066535569308849 	 ± 0.2799162150762131
	data : 0.11646003723144531
	model : 0.06962070465087891
			 train-loss:  2.0637610024876065 	 ± 0.27958476223669676
	data : 0.11661758422851562
	model : 0.06887121200561523
			 train-loss:  2.0641418512050924 	 ± 0.27806781249378315
	data : 0.11728482246398926
	model : 0.06901211738586426
			 train-loss:  2.0608942754890607 	 ± 0.27828224712394845
	data : 0.1170511245727539
	model : 0.06913280487060547
			 train-loss:  2.0590144113827775 	 ± 0.27736875785766024
	data : 0.11683220863342285
	model : 0.06926069259643555
			 train-loss:  2.0596457757848374 	 ± 0.27595662497502865
	data : 0.11687002182006836
	model : 0.06896185874938965
			 train-loss:  2.0607989248476533 	 ± 0.27472796546309286
	data : 0.11700019836425782
	model : 0.06975440979003907
			 train-loss:  2.0597765147686005 	 ± 0.27347496773249363
	data : 0.11629271507263184
	model : 0.06877169609069825
			 train-loss:  2.0580077294221857 	 ± 0.27261307317962497
	data : 0.11730222702026367
	model : 0.06876902580261231
			 train-loss:  2.058930747363032 	 ± 0.27137093101778514
	data : 0.11738061904907227
	model : 0.06871814727783203
			 train-loss:  2.060796759345315 	 ± 0.27062808029757063
	data : 0.11745553016662598
	model : 0.06901030540466309
			 train-loss:  2.060095137357712 	 ± 0.26936201910259755
	data : 0.1171292781829834
	model : 0.06913490295410156
			 train-loss:  2.0618393055283195 	 ± 0.26859213386587744
	data : 0.11693758964538574
	model : 0.0698272705078125
			 train-loss:  2.0623280896860012 	 ± 0.26731740005885263
	data : 0.11637821197509765
	model : 0.0698209285736084
			 train-loss:  2.062502908475191 	 ± 0.26602243686736426
	data : 0.11644139289855956
	model : 0.06958541870117188
			 train-loss:  2.068754192728263 	 ± 0.2722362340517271
	data : 0.11644597053527832
	model : 0.06926198005676269
			 train-loss:  2.0663734345209033 	 ± 0.27202243687010885
	data : 0.11672892570495605
	model : 0.06923308372497558
			 train-loss:  2.068298832425531 	 ± 0.27145419547082317
	data : 0.11680378913879394
	model : 0.06936068534851074
			 train-loss:  2.067402265896307 	 ± 0.2703403767051043
	data : 0.11650652885437011
	model : 0.06950483322143555
			 train-loss:  2.0672104480089963 	 ± 0.2690932055806893
	data : 0.11644740104675293
	model : 0.07024893760681153
			 train-loss:  2.0700134695123094 	 ± 0.26943529682213885
	data : 0.11582002639770508
	model : 0.07060022354125976
			 train-loss:  2.067746913433075 	 ± 0.2692496691090276
	data : 0.11577939987182617
	model : 0.07068262100219727
			 train-loss:  2.0664510200689503 	 ± 0.2683784643039425
	data : 0.11574888229370117
	model : 0.07069597244262696
			 train-loss:  2.0688158390777454 	 ± 0.26833683094157695
	data : 0.11575288772583008
	model : 0.07055368423461914
			 train-loss:  2.0655163505436045 	 ± 0.2694192781866934
	data : 0.11576018333435059
	model : 0.07012948989868165
			 train-loss:  2.0621134032282913 	 ± 0.27066319901755886
	data : 0.11606674194335938
	model : 0.06950011253356933
			 train-loss:  2.0647142897481503 	 ± 0.2709108744605219
	data : 0.11635842323303222
	model : 0.06906003952026367
			 train-loss:  2.062850602741899 	 ± 0.27048001600766713
	data : 0.11694121360778809
	model : 0.06907973289489747
			 train-loss:  2.0625148675380607 	 ± 0.26934591144347275
	data : 0.11698851585388184
	model : 0.06886944770812989
			 train-loss:  2.0623653642201827 	 ± 0.26820706209454526
	data : 0.11718602180480957
	model : 0.06902294158935547
			 train-loss:  2.0628208472949114 	 ± 0.2671235911943239
	data : 0.11709012985229492
	model : 0.06963157653808594
			 train-loss:  2.0605024725198744 	 ± 0.26720777757122544
	data : 0.11646909713745117
	model : 0.06980357170104981
			 train-loss:  2.0603405650982185 	 ± 0.2661072330120039
	data : 0.11620087623596191
	model : 0.06952972412109375
			 train-loss:  2.061659819767123 	 ± 0.26541141069355006
	data : 0.11649699211120605
	model : 0.06964144706726075
			 train-loss:  2.060800741358501 	 ± 0.2645005590968042
	data : 0.11621699333190919
	model : 0.06929473876953125
			 train-loss:  2.0621497409958995 	 ± 0.26385636853313466
	data : 0.11648168563842773
	model : 0.06925139427185059
			 train-loss:  2.06396780872345 	 ± 0.26357747942920406
	data : 0.11663870811462403
	model : 0.06980957984924316
			 train-loss:  2.0630432282175337 	 ± 0.2627328873392848
	data : 0.11605978012084961
	model : 0.06994295120239258
			 train-loss:  2.0646214400689433 	 ± 0.26229539240201416
	data : 0.11592283248901367
	model : 0.07003841400146485
			 train-loss:  2.061909551732242 	 ± 0.26305015235032947
	data : 0.11599369049072265
	model : 0.0700775146484375
			 train-loss:  2.0605957840764244 	 ± 0.2624498246797811
	data : 0.11596660614013672
	model : 0.07019586563110351
			 train-loss:  2.065547768886273 	 ± 0.26741993721132423
	data : 0.11597218513488769
	model : 0.06937947273254394
			 train-loss:  2.0635808997481835 	 ± 0.26733954715671704
	data : 0.11660494804382324
	model : 0.06951804161071777
			 train-loss:  2.0621963486526953 	 ± 0.2667960183427478
	data : 0.11643681526184083
	model : 0.06867656707763672
			 train-loss:  2.0599981134099172 	 ± 0.26698835556260736
	data : 0.11733450889587402
	model : 0.06901636123657226
			 train-loss:  2.060759515015047 	 ± 0.2661351636970393
	data : 0.11695208549499511
	model : 0.06867823600769044
			 train-loss:  2.0583679128576207 	 ± 0.26658905081596385
	data : 0.11715278625488282
	model : 0.06862592697143555
			 train-loss:  2.059096958707361 	 ± 0.2657421759703201
	data : 0.11749129295349121
	model : 0.06851887702941895
			 train-loss:  2.057567383251051 	 ± 0.26537072819480884
	data : 0.11760334968566895
	model : 0.06932177543640136
			 train-loss:  2.058045462421749 	 ± 0.2644666981718713
	data : 0.11694250106811524
	model : 0.06913547515869141
			 train-loss:  2.0568795693006448 	 ± 0.26386935092148117
	data : 0.11717681884765625
	model : 0.06955742835998535
			 train-loss:  2.0580450168677737 	 ± 0.2632840626632196
	data : 0.11679620742797851
	model : 0.06899185180664062
			 train-loss:  2.0597847378845757 	 ± 0.26315509721836244
	data : 0.11716480255126953
	model : 0.06811232566833496
			 train-loss:  2.0600810680590884 	 ± 0.262250464833621
	data : 0.11806831359863282
	model : 0.06804375648498535
			 train-loss:  2.0617294903401726 	 ± 0.2620691056608775
	data : 0.11785740852355957
	model : 0.06802186965942383
			 train-loss:  2.061842126150926 	 ± 0.2611610315735503
	data : 0.11798233985900879
	model : 0.06786584854125977
			 train-loss:  2.0639650714808497 	 ± 0.26150276799662026
	data : 0.11800560951232911
	model : 0.06839532852172851
			 train-loss:  2.067447361064284 	 ± 0.2639576397582084
	data : 0.11733417510986328
	model : 0.06930055618286132
			 train-loss:  2.06591496256744 	 ± 0.2637091372496739
	data : 0.1164255142211914
	model : 0.0694282054901123
			 train-loss:  2.0679724611140586 	 ± 0.263997959065078
	data : 0.11635046005249024
	model : 0.06954917907714844
			 train-loss:  2.0695496893569127 	 ± 0.2638092933829124
	data : 0.11615481376647949
	model : 0.06967449188232422
			 train-loss:  2.0694551396369936 	 ± 0.26293099156966
	data : 0.11615114212036133
	model : 0.07006011009216309
			 train-loss:  2.069168731076828 	 ± 0.2620823884107208
	data : 0.1159550666809082
	model : 0.07010502815246582
			 train-loss:  2.0685099912317177 	 ± 0.26134424357494784
	data : 0.11588053703308106
	model : 0.07005372047424316
			 train-loss:  2.0708401452482135 	 ± 0.262068130453898
	data : 0.11598358154296876
	model : 0.06989421844482421
			 train-loss:  2.070359025682722 	 ± 0.26128365567476286
	data : 0.11606459617614746
	model : 0.06978235244750977
			 train-loss:  2.0707198604460686 	 ± 0.26047793316871903
	data : 0.11623601913452149
	model : 0.0689208984375
			 train-loss:  2.0725740851500096 	 ± 0.2606659526940366
	data : 0.11715621948242187
	model : 0.06900768280029297
			 train-loss:  2.0711780665027106 	 ± 0.2604188559521334
	data : 0.1170577049255371
	model : 0.06880264282226563
			 train-loss:  2.0716901662983473 	 ± 0.2596727276092327
	data : 0.11701393127441406
	model : 0.06883368492126465
			 train-loss:  2.0732620669610844 	 ± 0.25960784966957234
	data : 0.11722631454467773
	model : 0.06874032020568847
			 train-loss:  2.076635291427374 	 ± 0.2622674463638065
	data : 0.11726016998291015
	model : 0.06941447257995606
			 train-loss:  2.080551448816098 	 ± 0.26610296592455573
	data : 0.11651501655578614
	model : 0.06928958892822265
			 train-loss:  2.079703094046793 	 ± 0.265498695811188
	data : 0.1167792797088623
	model : 0.06858525276184083
			 train-loss:  2.083904061580728 	 ± 0.27002981868009485
	data : 0.11758184432983398
	model : 0.06859259605407715
			 train-loss:  2.083449875436178 	 ± 0.2692677423238594
	data : 0.11733517646789551
	model : 0.06873621940612792
			 train-loss:  2.084843041680076 	 ± 0.269042750696552
	data : 0.11720671653747558
	model : 0.06912951469421387
			 train-loss:  2.083460664892771 	 ± 0.26881827099433503
	data : 0.11687421798706055
	model : 0.06917781829833984
			 train-loss:  2.0837375625164922 	 ± 0.2680359607824413
	data : 0.11677560806274415
	model : 0.06954455375671387
			 train-loss:  2.081424281001091 	 ± 0.2689038862531202
	data : 0.11662983894348145
	model : 0.06938767433166504
			 train-loss:  2.080762757352118 	 ± 0.268244204850204
	data : 0.11683359146118164
	model : 0.06915135383605957
			 train-loss:  2.0814230301800896 	 ± 0.2675917903800243
	data : 0.11695489883422852
	model : 0.06901388168334961
			 train-loss:  2.0814634680050856 	 ± 0.2668087319293627
	data : 0.11723365783691406
	model : 0.06909718513488769
			 train-loss:  2.081742281137511 	 ± 0.26605697716449056
	data : 0.11719698905944824
	model : 0.06981854438781739
			 train-loss:  2.0828364584487296 	 ± 0.26567474131440444
	data : 0.11642279624938964
	model : 0.06993002891540527
			 train-loss:  2.081671070778507 	 ± 0.26535330141602254
	data : 0.11629571914672851
	model : 0.07007794380187989
			 train-loss:  2.0816550036839074 	 ± 0.26459414782761287
	data : 0.11627373695373536
	model : 0.07004656791687011
			 train-loss:  2.0810381323099136 	 ± 0.26396755750754514
	data : 0.11627988815307617
	model : 0.07007837295532227
			 train-loss:  2.0790104610098283 	 ± 0.2645918003924201
	data : 0.11607804298400878
	model : 0.07007498741149902
			 train-loss:  2.0792324234930315 	 ± 0.26386404297016836
	data : 0.11615452766418458
	model : 0.07007932662963867
			 train-loss:  2.0790859653963056 	 ± 0.2631332154469619
	data : 0.11634626388549804
	model : 0.0698091983795166
			 train-loss:  2.07714560230573 	 ± 0.26368231655302493
	data : 0.1165229320526123
	model : 0.06973237991333008
			 train-loss:  2.0794841907301 	 ± 0.26481814269803267
	data : 0.11644277572631836
	model : 0.06954483985900879
			 train-loss:  2.0809749843000054 	 ± 0.264850132539406
	data : 0.11658592224121093
	model : 0.06943221092224121
			 train-loss:  2.08112037442421 	 ± 0.2641327897570385
	data : 0.11647119522094726
	model : 0.06965169906616211
			 train-loss:  2.0829553895670436 	 ± 0.26458113919280113
	data : 0.11620774269104003
	model : 0.0701866626739502
			 train-loss:  2.083892087034277 	 ± 0.26417082664050934
	data : 0.11572890281677246
	model : 0.0702277660369873
			 train-loss:  2.084159083904759 	 ± 0.26348476055024267
	data : 0.11569914817810059
	model : 0.07011375427246094
			 train-loss:  2.084480505576108 	 ± 0.2628158719439578
	data : 0.11597065925598145
	model : 0.0705106258392334
			 train-loss:  2.08557320845888 	 ± 0.2625415305102975
	data : 0.1158372402191162
	model : 0.07086668014526368
			 train-loss:  2.085388875512219 	 ± 0.2618582527267834
	data : 0.11565284729003907
	model : 0.07114081382751465
			 train-loss:  2.085523925956927 	 ± 0.26117484225817367
	data : 0.1154165267944336
	model : 0.07169580459594727
			 train-loss:  2.087700402549424 	 ± 0.2622121380582603
	data : 0.11506190299987792
	model : 0.07137656211853027
			 train-loss:  2.0877974747369685 	 ± 0.26153184339976226
	data : 0.11526012420654297
	model : 0.07006769180297852
			 train-loss:  2.0911786216528303 	 ± 0.2650273199079477
	data : 0.11635193824768067
	model : 0.0689666748046875
			 train-loss:  2.0910882181728008 	 ± 0.26434636080172136
	data : 0.1172177791595459
	model : 0.06822390556335449
			 train-loss:  2.089848694434533 	 ± 0.2642323029755642
	data : 0.11796789169311524
	model : 0.0674941062927246
			 train-loss:  2.090270289352962 	 ± 0.2636231245008822
	data : 0.11847925186157227
	model : 0.06796159744262695
			 train-loss:  2.0912945415767923 	 ± 0.2633438750086649
	data : 0.11811513900756836
	model : 0.06882085800170898
			 train-loss:  2.0903979979380214 	 ± 0.2629792591151171
	data : 0.11731534004211426
	model : 0.06941361427307129
			 train-loss:  2.0935830692550046 	 ± 0.26611878763234126
	data : 0.11680612564086915
	model : 0.06911516189575195
			 train-loss:  2.091461359858513 	 ± 0.2671346881015674
	data : 0.11692147254943848
	model : 0.06944160461425782
			 train-loss:  2.0929473519918336 	 ± 0.26729673875871557
	data : 0.11676588058471679
	model : 0.06976518630981446
			 train-loss:  2.0941455334720045 	 ± 0.267174866400807
	data : 0.11653323173522949
	model : 0.06986560821533203
			 train-loss:  2.091550188111554 	 ± 0.26905651245686035
	data : 0.11662030220031738
	model : 0.06987195014953614
			 train-loss:  2.0933126853961572 	 ± 0.2695684432262186
	data : 0.11654553413391114
	model : 0.07046127319335938
			 train-loss:  2.0940623597400942 	 ± 0.2691232469954831
	data : 0.11616888046264648
	model : 0.0705296516418457
			 train-loss:  2.0936104209677686 	 ± 0.26854721019840044
	data : 0.1160888671875
	model : 0.07007431983947754
			 train-loss:  2.095135255712242 	 ± 0.26879022453986495
	data : 0.1162961483001709
	model : 0.06966419219970703
			 train-loss:  2.094323906760949 	 ± 0.26839728612607466
	data : 0.11660151481628418
	model : 0.06915183067321777
			 train-loss:  2.0946718642586157 	 ± 0.2678014405194872
	data : 0.11716136932373047
	model : 0.06911168098449708
			 train-loss:  2.0962659847168696 	 ± 0.26815520523717357
	data : 0.11712527275085449
	model : 0.06936216354370117
			 train-loss:  2.0945073953737015 	 ± 0.2687301160500703
	data : 0.11683015823364258
	model : 0.06932663917541504
			 train-loss:  2.092794603334283 	 ± 0.26924753645580674
	data : 0.11688899993896484
	model : 0.06959047317504882
			 train-loss:  2.091808062204173 	 ± 0.26899854870114814
	data : 0.11654095649719239
	model : 0.07018499374389649
			 train-loss:  2.0918370234632047 	 ± 0.268369644320295
	data : 0.11607565879821777
	model : 0.0701366901397705
			 train-loss:  2.0938271106675614 	 ± 0.26932288128608234
	data : 0.11600346565246582
	model : 0.0697962760925293
			 train-loss:  2.0939009139935174 	 ± 0.2687009046291667
	data : 0.11620378494262695
	model : 0.0700185775756836
			 train-loss:  2.094210935078458 	 ± 0.26811978081926413
	data : 0.11589388847351074
	model : 0.06983623504638672
			 train-loss:  2.0937989507246453 	 ± 0.26757295463904646
	data : 0.11605648994445801
	model : 0.06976966857910157
			 train-loss:  2.0956108194507963 	 ± 0.2682984014583043
	data : 0.1160041332244873
	model : 0.06975836753845215
			 train-loss:  2.0941288715059105 	 ± 0.26858479712260275
	data : 0.11612539291381836
	model : 0.07009048461914062
			 train-loss:  2.0927531848665817 	 ± 0.2687521737807091
	data : 0.11589698791503907
	model : 0.06987099647521973
			 train-loss:  2.0935935791548306 	 ± 0.2684370779127064
	data : 0.11637039184570312
	model : 0.0699930191040039
			 train-loss:  2.0927227366665555 	 ± 0.26814863427419433
	data : 0.11610269546508789
	model : 0.06909418106079102
			 train-loss:  2.093870642994131 	 ± 0.26809799715717514
	data : 0.11700601577758789
	model : 0.06902642250061035
			 train-loss:  2.0937157493167455 	 ± 0.2675116054075286
	data : 0.11702661514282227
	model : 0.06853809356689453
			 train-loss:  2.0937683371316016 	 ± 0.26692027501021565
	data : 0.1173975944519043
	model : 0.06865420341491699
			 train-loss:  2.093064862726018 	 ± 0.2665415808131173
	data : 0.11712827682495117
	model : 0.06864781379699707
			 train-loss:  2.093992273012797 	 ± 0.26632321789077473
	data : 0.11732807159423828
	model : 0.06945924758911133
			 train-loss:  2.093286921884295 	 ± 0.2659544349935189
	data : 0.11644024848937988
	model : 0.0694310188293457
			 train-loss:  2.093151428906814 	 ± 0.2653835642167356
	data : 0.11653985977172851
	model : 0.06926255226135254
			 train-loss:  2.0929026763676566 	 ± 0.26483538859316336
	data : 0.11658611297607421
	model : 0.06904683113098145
			 train-loss:  2.0907025851052383 	 ± 0.266371165447659
	data : 0.11677889823913574
	model : 0.06904439926147461
			 train-loss:  2.091479972708379 	 ± 0.2660625505547361
	data : 0.11667718887329101
	model : 0.06886024475097656
			 train-loss:  2.090511588459341 	 ± 0.26590461160004975
	data : 0.11665892601013184
	model : 0.06879673004150391
			 train-loss:  2.0915863397273613 	 ± 0.26584709917595756
	data : 0.11678948402404785
	model : 0.0688812255859375
			 train-loss:  2.090539861028477 	 ± 0.26576787580221645
	data : 0.11672272682189941
	model : 0.06892619132995606
			 train-loss:  2.091542523118514 	 ± 0.26565352301822054
	data : 0.11655259132385254
	model : 0.06876101493835449
			 train-loss:  2.0909942739150105 	 ± 0.2652291668382929
	data : 0.11661376953125
	model : 0.06876945495605469
			 train-loss:  2.0918132201398265 	 ± 0.2649750822825149
	data : 0.11661257743835449
	model : 0.06885328292846679
			 train-loss:  2.091451210776965 	 ± 0.26448169355199436
	data : 0.11632866859436035
	model : 0.06899185180664062
			 train-loss:  2.092673114721211 	 ± 0.2646103677707079
	data : 0.11646990776062012
	model : 0.06919035911560059
			 train-loss:  2.0932358237337474 	 ± 0.26420753976011724
	data : 0.11655025482177735
	model : 0.06941847801208496
			 train-loss:  2.0934133814195546 	 ± 0.26367781020125314
	data : 0.11650476455688477
	model : 0.06952495574951172
			 train-loss:  2.092246152826997 	 ± 0.26376526251411037
	data : 0.11649823188781738
	model : 0.06948904991149903
			 train-loss:  2.092225962755631 	 ± 0.2632266045862222
	data : 0.11659412384033203
	model : 0.0694037914276123
			 train-loss:  2.0915471446223375 	 ± 0.2629058393650306
	data : 0.11658740043640137
	model : 0.0693394660949707
			 train-loss:  2.0915193350208914 	 ± 0.2623734641104063
	data : 0.11657443046569824
	model : 0.06919035911560059
			 train-loss:  2.0906536420506816 	 ± 0.2621971824743759
	data : 0.11644582748413086
	model : 0.0690382480621338
			 train-loss:  2.091635852932451 	 ± 0.26212692297786616
	data : 0.11646618843078613
	model : 0.06893320083618164
			 train-loss:  2.0920372014045716 	 ± 0.2616787930331135
	data : 0.1165781021118164
	model : 0.06876788139343262
			 train-loss:  2.091337299916849 	 ± 0.2613913623882091
	data : 0.11653265953063965
	model : 0.06870388984680176
			 train-loss:  2.0925359744874257 	 ± 0.26156252334351604
	data : 0.11651768684387206
	model : 0.06864080429077149
			 train-loss:  2.0916748339008437 	 ± 0.261402778648349
	data : 0.11677865982055664
	model : 0.06866073608398438
			 train-loss:  2.092924998501154 	 ± 0.261644430462116
	data : 0.11682310104370117
	model : 0.06861200332641601
			 train-loss:  2.0923206525690414 	 ± 0.26130846797828455
	data : 0.11658854484558105
	model : 0.06857705116271973
			 train-loss:  2.0925680212676525 	 ± 0.26082751426634454
	data : 0.11572980880737305
	model : 0.059720182418823244
#epoch  55    val-loss:  2.4875345041877344  train-loss:  2.0925680212676525  lr:  7.8125e-05
			 train-loss:  2.5013396739959717 	 ± 0.0
	data : 5.683366775512695
	model : 0.07706165313720703
			 train-loss:  2.1198887825012207 	 ± 0.381450891494751
	data : 2.90778911113739
	model : 0.07357597351074219
			 train-loss:  2.248800039291382 	 ± 0.3608869804650729
	data : 1.978330930074056
	model : 0.0721591313680013
			 train-loss:  2.245901882648468 	 ± 0.31257760226490805
	data : 1.5126763582229614
	model : 0.07152402400970459
			 train-loss:  2.233640718460083 	 ± 0.28065129705590414
	data : 1.2332891464233398
	model : 0.07122631072998047
			 train-loss:  2.1821446220080056 	 ± 0.2808858579535633
	data : 0.11996912956237793
	model : 0.06986861228942871
			 train-loss:  2.2269823040281023 	 ± 0.28229132262488243
	data : 0.11677336692810059
	model : 0.06967525482177735
			 train-loss:  2.231496348977089 	 ± 0.2643292987700239
	data : 0.11606059074401856
	model : 0.0695981502532959
			 train-loss:  2.219456924332513 	 ± 0.25152779020344085
	data : 0.11626315116882324
	model : 0.06964001655578614
			 train-loss:  2.1729992985725404 	 ± 0.27634110306186166
	data : 0.11637554168701172
	model : 0.06967582702636718
			 train-loss:  2.149771126833829 	 ± 0.27352818174068244
	data : 0.11616215705871583
	model : 0.0697366714477539
			 train-loss:  2.152370552221934 	 ± 0.2620251663962803
	data : 0.11594591140747071
	model : 0.06990160942077636
			 train-loss:  2.1433499593001146 	 ± 0.25367758837656124
	data : 0.11587576866149903
	model : 0.0691309928894043
			 train-loss:  2.13044730254582 	 ± 0.2488371873505142
	data : 0.11650986671447754
	model : 0.06932244300842286
			 train-loss:  2.1559123198191323 	 ± 0.25859328855850466
	data : 0.11630620956420898
	model : 0.06918768882751465
			 train-loss:  2.1434956714510918 	 ± 0.2549581939896553
	data : 0.11641955375671387
	model : 0.06905274391174317
			 train-loss:  2.122882555512821 	 ± 0.26072656853286386
	data : 0.1166450023651123
	model : 0.06905403137207031
			 train-loss:  2.119693511062198 	 ± 0.2537216170816112
	data : 0.11689739227294922
	model : 0.07000303268432617
			 train-loss:  2.1791204841513383 	 ± 0.35292306722309014
	data : 0.11617097854614258
	model : 0.06903886795043945
			 train-loss:  2.1963770568370817 	 ± 0.35211497202697134
	data : 0.11688284873962403
	model : 0.06819429397583007
			 train-loss:  2.19567410718827 	 ± 0.3436434065768632
	data : 0.11766371726989747
	model : 0.06731853485107422
			 train-loss:  2.1791233908046377 	 ± 0.3442026701389793
	data : 0.11848654747009277
	model : 0.06683559417724609
			 train-loss:  2.1732166435407554 	 ± 0.3377749886792268
	data : 0.11903700828552247
	model : 0.06668806076049805
			 train-loss:  2.161047418912252 	 ± 0.33577400534470037
	data : 0.11915431022644044
	model : 0.06659865379333496
			 train-loss:  2.153820858001709 	 ± 0.33088936480471054
	data : 0.11943879127502441
	model : 0.0674746036529541
			 train-loss:  2.1491616780941305 	 ± 0.3252989433309998
	data : 0.11868896484375
	model : 0.06779327392578124
			 train-loss:  2.1399951025291726 	 ± 0.3226218284597781
	data : 0.11830391883850097
	model : 0.06839323043823242
			 train-loss:  2.130329804761069 	 ± 0.32076442106323905
	data : 0.11758499145507813
	model : 0.06838226318359375
			 train-loss:  2.1287275511642982 	 ± 0.31529949456037065
	data : 0.11746959686279297
	model : 0.06913938522338867
			 train-loss:  2.1174887299537657 	 ± 0.31585281518173103
	data : 0.11680097579956054
	model : 0.06897802352905273
			 train-loss:  2.1214834605493853 	 ± 0.311486076279767
	data : 0.11685395240783691
	model : 0.0694420337677002
			 train-loss:  2.121021043509245 	 ± 0.30659128769442234
	data : 0.11639127731323243
	model : 0.06938567161560058
			 train-loss:  2.121810613256512 	 ± 0.3019432666965352
	data : 0.11635217666625977
	model : 0.06965975761413574
			 train-loss:  2.113290751681608 	 ± 0.30146920047055403
	data : 0.11625466346740723
	model : 0.06967134475708008
			 train-loss:  2.1283584799085347 	 ± 0.30984873898014337
	data : 0.11617503166198731
	model : 0.06961994171142578
			 train-loss:  2.1275532378090753 	 ± 0.30555211591124914
	data : 0.1162905216217041
	model : 0.06973886489868164
			 train-loss:  2.1185589416607007 	 ± 0.30618801533340306
	data : 0.11625351905822753
	model : 0.06974754333496094
			 train-loss:  2.1151616981154993 	 ± 0.30283822952373673
	data : 0.11649560928344727
	model : 0.06981134414672852
			 train-loss:  2.1140289337207108 	 ± 0.29901202187857967
	data : 0.1164024829864502
	model : 0.06990022659301758
			 train-loss:  2.1298026710748674 	 ± 0.311250068773995
	data : 0.11652579307556152
	model : 0.06935367584228516
			 train-loss:  2.125844048290718 	 ± 0.3084486813940182
	data : 0.1169651985168457
	model : 0.06928901672363282
			 train-loss:  2.121363478047507 	 ± 0.30610199743102023
	data : 0.11707286834716797
	model : 0.06916952133178711
			 train-loss:  2.1210541974666506 	 ± 0.302528373872049
	data : 0.1170048713684082
	model : 0.06907072067260742
			 train-loss:  2.1187702498652716 	 ± 0.29944556309018794
	data : 0.11718196868896484
	model : 0.06887245178222656
			 train-loss:  2.1301705174975925 	 ± 0.3056035657634065
	data : 0.11711554527282715
	model : 0.06962070465087891
			 train-loss:  2.116379553857057 	 ± 0.316104137252936
	data : 0.11654281616210938
	model : 0.0698089599609375
			 train-loss:  2.1108499816123474 	 ± 0.314964018711571
	data : 0.11628398895263672
	model : 0.06987986564636231
			 train-loss:  2.107797590394815 	 ± 0.3123676057807379
	data : 0.1164515495300293
	model : 0.06961007118225097
			 train-loss:  2.111632461450538 	 ± 0.3103032776495333
	data : 0.11645712852478027
	model : 0.06982159614562988
			 train-loss:  2.104380893707275 	 ± 0.3113503462976452
	data : 0.11629261970520019
	model : 0.06989455223083496
			 train-loss:  2.095911135860518 	 ± 0.314046354198039
	data : 0.11614289283752441
	model : 0.06983599662780762
			 train-loss:  2.0905970335006714 	 ± 0.31331884667704735
	data : 0.11624102592468262
	model : 0.06969995498657226
			 train-loss:  2.1032886280203766 	 ± 0.3235621142561499
	data : 0.1162226676940918
	model : 0.07011704444885254
			 train-loss:  2.100287909860964 	 ± 0.32129569132195124
	data : 0.11603894233703613
	model : 0.07010884284973144
			 train-loss:  2.0913536201823844 	 ± 0.3250605453637901
	data : 0.11609530448913574
	model : 0.07013812065124511
			 train-loss:  2.0879151672124863 	 ± 0.3231528389929653
	data : 0.11618328094482422
	model : 0.06921606063842774
			 train-loss:  2.0859280552780417 	 ± 0.320650610175385
	data : 0.1171595573425293
	model : 0.0684929370880127
			 train-loss:  2.0919525705534836 	 ± 0.32111199420898123
	data : 0.11777515411376953
	model : 0.06809010505676269
			 train-loss:  2.0908078622009794 	 ± 0.31849840981490735
	data : 0.11803441047668457
	model : 0.06811041831970215
			 train-loss:  2.0853987634181976 	 ± 0.3185542220709883
	data : 0.11803698539733887
	model : 0.06802225112915039
			 train-loss:  2.082563292784769 	 ± 0.31669485636536077
	data : 0.11794242858886719
	model : 0.0682765007019043
			 train-loss:  2.0883685638827663 	 ± 0.31738577801694673
	data : 0.11769523620605468
	model : 0.06854958534240722
			 train-loss:  2.0922540350565835 	 ± 0.3163396764651722
	data : 0.1173919677734375
	model : 0.06873431205749511
			 train-loss:  2.095794891938567 	 ± 0.31511435677874106
	data : 0.1172724723815918
	model : 0.06878604888916015
			 train-loss:  2.0958435957248396 	 ± 0.31268124762451954
	data : 0.11707720756530762
	model : 0.06891322135925293
			 train-loss:  2.1084334218140803 	 ± 0.32648269091778676
	data : 0.11713638305664062
	model : 0.0698124885559082
			 train-loss:  2.111011752441748 	 ± 0.32471339958137135
	data : 0.11623878479003906
	model : 0.06948280334472656
			 train-loss:  2.1083389152498806 	 ± 0.3230586221243478
	data : 0.11659355163574218
	model : 0.06952834129333496
			 train-loss:  2.1155670743057695 	 ± 0.32620094606581923
	data : 0.11649317741394043
	model : 0.06942706108093262
			 train-loss:  2.116805282660893 	 ± 0.32402583914266386
	data : 0.11661324501037598
	model : 0.06906695365905761
			 train-loss:  2.12044413828514 	 ± 0.3231731150661451
	data : 0.11698150634765625
	model : 0.06875905990600586
			 train-loss:  2.1206118812163672 	 ± 0.32092412272319004
	data : 0.1172060489654541
	model : 0.06975355148315429
			 train-loss:  2.1166759223154146 	 ± 0.3204634875316454
	data : 0.11629877090454102
	model : 0.06959881782531738
			 train-loss:  2.110921254029145 	 ± 0.32206604094590513
	data : 0.11642689704895019
	model : 0.06890807151794434
			 train-loss:  2.1172211074829104 	 ± 0.3244694710282848
	data : 0.11712050437927246
	model : 0.06920356750488281
			 train-loss:  2.114829017927772 	 ± 0.32299276407712907
	data : 0.11680326461791993
	model : 0.0692418098449707
			 train-loss:  2.1090080072353414 	 ± 0.3248763777973733
	data : 0.11674494743347168
	model : 0.06927499771118165
			 train-loss:  2.111277169142014 	 ± 0.32340068730073446
	data : 0.11685996055603028
	model : 0.07028427124023437
			 train-loss:  2.1128799387171298 	 ± 0.3216589461512147
	data : 0.11594204902648926
	model : 0.07094883918762207
			 train-loss:  2.115268297493458 	 ± 0.3203463875596753
	data : 0.11515274047851562
	model : 0.07086200714111328
			 train-loss:  2.112024720804191 	 ± 0.31968192832900855
	data : 0.11507272720336914
	model : 0.07077207565307617
			 train-loss:  2.1125226122577017 	 ± 0.3177582660719209
	data : 0.11524405479431152
	model : 0.06971936225891114
			 train-loss:  2.111600716430021 	 ± 0.3159485669646918
	data : 0.11593875885009766
	model : 0.06882896423339843
			 train-loss:  2.11111233489854 	 ± 0.31409380585158503
	data : 0.11690349578857422
	model : 0.0703272819519043
			 train-loss:  2.113748178762548 	 ± 0.31317387708981304
	data : 0.11547231674194336
	model : 0.07004899978637695
			 train-loss:  2.1148532864659333 	 ± 0.311514436609141
	data : 0.11579794883728027
	model : 0.07024407386779785
			 train-loss:  2.113988367990516 	 ± 0.3098227926364874
	data : 0.11548538208007812
	model : 0.0712240219116211
			 train-loss:  2.118482994762334 	 ± 0.31089694563675974
	data : 0.11454524993896484
	model : 0.07116861343383789
			 train-loss:  2.121976979662863 	 ± 0.31087807253530925
	data : 0.11458473205566407
	model : 0.06970067024230957
			 train-loss:  2.120638506942325 	 ± 0.30940391887542174
	data : 0.11601605415344238
	model : 0.06981611251831055
			 train-loss:  2.1212489434650967 	 ± 0.3077536926087109
	data : 0.11594572067260742
	model : 0.06956706047058106
			 train-loss:  2.1230190720247184 	 ± 0.30654198525553944
	data : 0.11615900993347168
	model : 0.06948070526123047
			 train-loss:  2.1203828075880646 	 ± 0.30593621907197704
	data : 0.11630749702453613
	model : 0.06954026222229004
			 train-loss:  2.1177636676646294 	 ± 0.30535099120199644
	data : 0.11639552116394043
	model : 0.06970028877258301
			 train-loss:  2.116478537258349 	 ± 0.3039950804324057
	data : 0.11638846397399902
	model : 0.06988487243652344
			 train-loss:  2.114513068149487 	 ± 0.30301380324194316
	data : 0.11625466346740723
	model : 0.06994400024414063
			 train-loss:  2.1150623830323365 	 ± 0.30149587373478837
	data : 0.11630353927612305
	model : 0.0699958324432373
			 train-loss:  2.118687039735366 	 ± 0.3020705422601439
	data : 0.1162804126739502
	model : 0.07000761032104492
			 train-loss:  2.1168598478490654 	 ± 0.30108489717243514
	data : 0.11620073318481446
	model : 0.0699608325958252
			 train-loss:  2.1167797303199767 	 ± 0.299576750792365
	data : 0.11638736724853516
	model : 0.07005553245544434
			 train-loss:  2.120145592359033 	 ± 0.2999842592721525
	data : 0.11629233360290528
	model : 0.06914219856262208
			 train-loss:  2.121592834884045 	 ± 0.29886425189962046
	data : 0.117071533203125
	model : 0.06910457611083984
			 train-loss:  2.1200053911764645 	 ± 0.2978417295277749
	data : 0.11697950363159179
	model : 0.06815924644470214
			 train-loss:  2.120841825237641 	 ± 0.29652787232077976
	data : 0.11773800849914551
	model : 0.06764965057373047
			 train-loss:  2.118070891925267 	 ± 0.2964622767697092
	data : 0.11822686195373536
	model : 0.06704387664794922
			 train-loss:  2.1177460686215817 	 ± 0.2950793288089066
	data : 0.11869454383850098
	model : 0.06719441413879394
			 train-loss:  2.1151819284831253 	 ± 0.29488130732011286
	data : 0.11860766410827636
	model : 0.06700615882873535
			 train-loss:  2.113324232675411 	 ± 0.29414130492069596
	data : 0.11897063255310059
	model : 0.06765708923339844
			 train-loss:  2.1119840221667507 	 ± 0.2931200090700796
	data : 0.11831154823303222
	model : 0.06810173988342286
			 train-loss:  2.1132715431126683 	 ± 0.29209406913016284
	data : 0.11790122985839843
	model : 0.06863713264465332
			 train-loss:  2.1165200192649087 	 ± 0.29276456715109034
	data : 0.11741724014282226
	model : 0.06887631416320801
			 train-loss:  2.117472341018064 	 ± 0.2916272996996067
	data : 0.11725587844848633
	model : 0.0692298412322998
			 train-loss:  2.113854079119927 	 ± 0.2928483258318246
	data : 0.11683940887451172
	model : 0.06945266723632812
			 train-loss:  2.113289318586651 	 ± 0.2916228760738986
	data : 0.1165964126586914
	model : 0.0692831039428711
			 train-loss:  2.1164108006850535 	 ± 0.29225873094836646
	data : 0.11669025421142579
	model : 0.06856369972229004
			 train-loss:  2.1156988842733977 	 ± 0.2910963982112884
	data : 0.1175222396850586
	model : 0.06909828186035157
			 train-loss:  2.113627242226886 	 ± 0.29070724365119244
	data : 0.11694226264953614
	model : 0.06899046897888184
			 train-loss:  2.1173061960834567 	 ± 0.2921952586860401
	data : 0.11711711883544922
	model : 0.06904683113098145
			 train-loss:  2.116900516157391 	 ± 0.29099832718438756
	data : 0.11711807250976562
	model : 0.06925048828125
			 train-loss:  2.1166884938875836 	 ± 0.28979252750364126
	data : 0.11687040328979492
	model : 0.07005915641784669
			 train-loss:  2.117536899472071 	 ± 0.28874216328664315
	data : 0.11600980758666993
	model : 0.0692972183227539
			 train-loss:  2.1183953226589765 	 ± 0.2877113540384692
	data : 0.11668334007263184
	model : 0.06926383972167968
			 train-loss:  2.1162751447863695 	 ± 0.28749476799340956
	data : 0.11649465560913086
	model : 0.06918148994445801
			 train-loss:  2.11598444657941 	 ± 0.28635131812764597
	data : 0.1166679859161377
	model : 0.06898670196533203
			 train-loss:  2.1174569368362426 	 ± 0.2856745718318217
	data : 0.11682381629943847
	model : 0.06895623207092286
			 train-loss:  2.1154267598712253 	 ± 0.28544257776626447
	data : 0.1170544147491455
	model : 0.06884384155273438
			 train-loss:  2.1151795443587416 	 ± 0.2843301090492389
	data : 0.11724176406860351
	model : 0.06881628036499024
			 train-loss:  2.1139918649569154 	 ± 0.2835333563473896
	data : 0.11739168167114258
	model : 0.06809344291687011
			 train-loss:  2.1132714683695357 	 ± 0.28254982775017107
	data : 0.1179539680480957
	model : 0.06733331680297852
			 train-loss:  2.1136079870737516 	 ± 0.2814869498517731
	data : 0.11876296997070312
	model : 0.06729421615600586
			 train-loss:  2.11311761204523 	 ± 0.2804662495675158
	data : 0.11867527961730957
	model : 0.06729435920715332
			 train-loss:  2.1113769529443798 	 ± 0.28011125326373515
	data : 0.11866092681884766
	model : 0.06640844345092774
			 train-loss:  2.111788782858311 	 ± 0.2790963269281951
	data : 0.1193875789642334
	model : 0.06720795631408691
			 train-loss:  2.1124868028199493 	 ± 0.278169475978582
	data : 0.11868252754211425
	model : 0.06816220283508301
			 train-loss:  2.1131429963641697 	 ± 0.2772413824540883
	data : 0.11767549514770508
	model : 0.06829442977905273
			 train-loss:  2.1118888872511246 	 ± 0.2766043080280525
	data : 0.11740732192993164
	model : 0.06926641464233399
			 train-loss:  2.1090747466052537 	 ± 0.2775401109587822
	data : 0.11650433540344238
	model : 0.06933083534240722
			 train-loss:  2.108376142771348 	 ± 0.27665357044793165
	data : 0.11668834686279297
	model : 0.06907925605773926
			 train-loss:  2.108766940857867 	 ± 0.27569484309917625
	data : 0.11677765846252441
	model : 0.06811332702636719
			 train-loss:  2.1065912689481463 	 ± 0.2759034214333865
	data : 0.11760358810424805
	model : 0.06711788177490234
			 train-loss:  2.105533740199204 	 ± 0.2752079069646065
	data : 0.11862273216247558
	model : 0.06621346473693848
			 train-loss:  2.106254837882351 	 ± 0.27437079563780925
	data : 0.11950669288635254
	model : 0.06707606315612794
			 train-loss:  2.10595505387633 	 ± 0.27343311101122214
	data : 0.11854801177978516
	model : 0.06741013526916503
			 train-loss:  2.1066003193457923 	 ± 0.2725912704732966
	data : 0.11848335266113282
	model : 0.06825079917907714
			 train-loss:  2.1041975588634094 	 ± 0.2731755748662048
	data : 0.11768302917480469
	model : 0.06910958290100097
			 train-loss:  2.1025953554127312 	 ± 0.27292121279376974
	data : 0.11692242622375489
	model : 0.07002830505371094
			 train-loss:  2.0990970045530877 	 ± 0.27525641857078675
	data : 0.11595959663391113
	model : 0.06922760009765624
			 train-loss:  2.101372208949682 	 ± 0.2757083902883241
	data : 0.11674833297729492
	model : 0.06933040618896484
			 train-loss:  2.100675617288423 	 ± 0.2749122831375951
	data : 0.11659088134765624
	model : 0.06947793960571289
			 train-loss:  2.0973159082730612 	 ± 0.2770465256702867
	data : 0.11678686141967773
	model : 0.06877098083496094
			 train-loss:  2.0982064419234825 	 ± 0.27634294839807955
	data : 0.11732969284057618
	model : 0.06879262924194336
			 train-loss:  2.096676727658824 	 ± 0.2760731151381951
	data : 0.11736950874328614
	model : 0.0695432186126709
			 train-loss:  2.0942421388002783 	 ± 0.2768016557187111
	data : 0.11671991348266601
	model : 0.06945905685424805
			 train-loss:  2.0935202364797716 	 ± 0.276045946339715
	data : 0.11693873405456542
	model : 0.06942033767700195
			 train-loss:  2.0920132513969176 	 ± 0.27578882755896655
	data : 0.11677618026733398
	model : 0.06937456130981445
			 train-loss:  2.0902868792032585 	 ± 0.2757424026172901
	data : 0.11685786247253419
	model : 0.06933584213256835
			 train-loss:  2.0916678746035147 	 ± 0.2754035140536249
	data : 0.11706743240356446
	model : 0.068550443649292
			 train-loss:  2.093101506746268 	 ± 0.2751176710436407
	data : 0.11780920028686523
	model : 0.06764354705810546
			 train-loss:  2.092341590977315 	 ± 0.2744174510423235
	data : 0.11837658882141114
	model : 0.06761698722839356
			 train-loss:  2.094505947828293 	 ± 0.27491654666379967
	data : 0.11859383583068847
	model : 0.06837902069091797
			 train-loss:  2.0967284744570716 	 ± 0.2754995682699607
	data : 0.1177746295928955
	model : 0.06815915107727051
			 train-loss:  2.0957077535582176 	 ± 0.27495315015345934
	data : 0.11784272193908692
	model : 0.06901068687438965
			 train-loss:  2.0933049354085163 	 ± 0.27580926103512426
	data : 0.11717829704284669
	model : 0.06916623115539551
			 train-loss:  2.0964852106280443 	 ± 0.2779487524210093
	data : 0.11722269058227539
	model : 0.06821842193603515
			 train-loss:  2.0956001794699466 	 ± 0.27733689208476037
	data : 0.11787528991699218
	model : 0.06828522682189941
			 train-loss:  2.096659386732492 	 ± 0.27683482550699595
	data : 0.11793222427368164
	model : 0.06831164360046386
			 train-loss:  2.0956105472085005 	 ± 0.27633534697183154
	data : 0.11769814491271972
	model : 0.06816697120666504
			 train-loss:  2.096034506956736 	 ± 0.27556616226028957
	data : 0.11770524978637695
	model : 0.06867752075195313
			 train-loss:  2.0934903431220873 	 ± 0.2767215304867416
	data : 0.11707792282104493
	model : 0.06876215934753419
			 train-loss:  2.0940967454629784 	 ± 0.2760190405116864
	data : 0.11695370674133301
	model : 0.06872634887695313
			 train-loss:  2.091792312281871 	 ± 0.2768460685348555
	data : 0.11705117225646973
	model : 0.06900873184204101
			 train-loss:  2.0922963473685954 	 ± 0.27611878834078724
	data : 0.11697406768798828
	model : 0.0691312313079834
			 train-loss:  2.0930543255943785 	 ± 0.2754990050801252
	data : 0.11676239967346191
	model : 0.06838974952697754
			 train-loss:  2.0911107022186806 	 ± 0.275893156572203
	data : 0.11745891571044922
	model : 0.06947593688964844
			 train-loss:  2.0919522871289935 	 ± 0.2753276548084654
	data : 0.11671085357666015
	model : 0.06926169395446777
			 train-loss:  2.0916356403719294 	 ± 0.27457631339137595
	data : 0.116981840133667
	model : 0.0682218074798584
			 train-loss:  2.0899136746670566 	 ± 0.27475093577846366
	data : 0.11794834136962891
	model : 0.06728930473327636
			 train-loss:  2.0916644301307334 	 ± 0.27496639249789373
	data : 0.11880006790161132
	model : 0.06740827560424804
			 train-loss:  2.0939999858760303 	 ± 0.27596212113361135
	data : 0.11859598159790039
	model : 0.06724023818969727
			 train-loss:  2.09143565164672 	 ± 0.27732486086093716
	data : 0.11861047744750977
	model : 0.06742329597473144
			 train-loss:  2.0900677534756738 	 ± 0.27716596578410835
	data : 0.11835579872131348
	model : 0.06825056076049804
			 train-loss:  2.092420988685482 	 ± 0.2782107221522727
	data : 0.11755647659301757
	model : 0.06894712448120117
			 train-loss:  2.0913029329372885 	 ± 0.27785924020354624
	data : 0.11705756187438965
	model : 0.06895999908447266
			 train-loss:  2.0907692267842917 	 ± 0.27719719874528514
	data : 0.11725740432739258
	model : 0.06888198852539062
			 train-loss:  2.0897869883356868 	 ± 0.2767678931820814
	data : 0.11722455024719239
	model : 0.06886525154113769
			 train-loss:  2.0883359466829607 	 ± 0.2767275876701816
	data : 0.11735067367553711
	model : 0.06910505294799804
			 train-loss:  2.0872366434750074 	 ± 0.2763936031742368
	data : 0.11725397109985351
	model : 0.06929011344909668
			 train-loss:  2.087383726175795 	 ± 0.27566487136975193
	data : 0.11728215217590332
	model : 0.07005715370178223
			 train-loss:  2.0869701810614774 	 ± 0.2749930972951355
	data : 0.11638917922973632
	model : 0.0697291374206543
			 train-loss:  2.086051517411282 	 ± 0.2745591055382421
	data : 0.11679234504699706
	model : 0.06965484619140624
			 train-loss:  2.0866006712638896 	 ± 0.2739440214541196
	data : 0.11670069694519043
	model : 0.0686068058013916
			 train-loss:  2.0842781520138183 	 ± 0.2751085932596222
	data : 0.11762123107910157
	model : 0.06845064163208008
			 train-loss:  2.0841566460120244 	 ± 0.2744001162815325
	data : 0.11755571365356446
	model : 0.06854143142700195
			 train-loss:  2.0856554133375895 	 ± 0.2744828585700124
	data : 0.11780705451965331
	model : 0.06889610290527344
			 train-loss:  2.086793643388993 	 ± 0.2742367890940224
	data : 0.11731343269348145
	model : 0.06887702941894532
			 train-loss:  2.088283324119996 	 ± 0.2743261696227537
	data : 0.11738834381103516
	model : 0.06879730224609375
			 train-loss:  2.087768063932506 	 ± 0.27372409407758264
	data : 0.11746125221252442
	model : 0.06813225746154786
			 train-loss:  2.0877729660332807 	 ± 0.27303200532061206
	data : 0.11794676780700683
	model : 0.06809148788452149
			 train-loss:  2.089514816825713 	 ± 0.2734458104834812
	data : 0.11802530288696289
	model : 0.06814250946044922
			 train-loss:  2.0881917840242386 	 ± 0.27339912391515625
	data : 0.11804785728454589
	model : 0.06908149719238281
			 train-loss:  2.0870361636527144 	 ± 0.27320742416985455
	data : 0.11693077087402344
	model : 0.06994757652282715
			 train-loss:  2.0883343998748476 	 ± 0.2731511471266387
	data : 0.11618552207946778
	model : 0.07070450782775879
			 train-loss:  2.0910063353665356 	 ± 0.27511112469739535
	data : 0.11554474830627441
	model : 0.0706261157989502
			 train-loss:  2.0922839197458005 	 ± 0.2750390181797597
	data : 0.11544532775878906
	model : 0.07057042121887207
			 train-loss:  2.090889026479023 	 ± 0.2750897721580302
	data : 0.11550230979919433
	model : 0.06896181106567383
			 train-loss:  2.089155303621755 	 ± 0.2755416834841794
	data : 0.11722636222839355
	model : 0.0683448314666748
			 train-loss:  2.0899076346613934 	 ± 0.2750873259721672
	data : 0.11785197257995605
	model : 0.06820807456970215
			 train-loss:  2.088708416200601 	 ± 0.27496711819928743
	data : 0.11797208786010742
	model : 0.06823620796203614
			 train-loss:  2.0895690045288307 	 ± 0.27458916195607824
	data : 0.11772432327270507
	model : 0.068336820602417
			 train-loss:  2.0913690334274655 	 ± 0.2751678501574607
	data : 0.11792364120483398
	model : 0.0682157039642334
			 train-loss:  2.092816284482513 	 ± 0.27531499824710376
	data : 0.11795754432678222
	model : 0.06905188560485839
			 train-loss:  2.095201759405856 	 ± 0.27684201704204364
	data : 0.11702394485473633
	model : 0.06922006607055664
			 train-loss:  2.0949249653749065 	 ± 0.276220791227153
	data : 0.11688027381896973
	model : 0.06905593872070312
			 train-loss:  2.0988781691711638 	 ± 0.28154949628482207
	data : 0.11720395088195801
	model : 0.06880660057067871
			 train-loss:  2.098878374210624 	 ± 0.2808939668985189
	data : 0.11742000579833985
	model : 0.07022967338562011
			 train-loss:  2.098869435765125 	 ± 0.28024302571655585
	data : 0.11618938446044921
	model : 0.07012295722961426
			 train-loss:  2.098501134577984 	 ± 0.2796489499175678
	data : 0.11627306938171386
	model : 0.06922450065612792
			 train-loss:  2.098041731830037 	 ± 0.2790888772760506
	data : 0.11711235046386718
	model : 0.0693857192993164
			 train-loss:  2.0981626799117485 	 ± 0.27845668527103057
	data : 0.11692137718200683
	model : 0.06952781677246093
			 train-loss:  2.0956327785145152 	 ± 0.2803343849463728
	data : 0.1165285587310791
	model : 0.06849713325500488
			 train-loss:  2.095743036917432 	 ± 0.27970420618434855
	data : 0.11759762763977051
	model : 0.06750378608703614
			 train-loss:  2.0937604726971806 	 ± 0.2806255325386987
	data : 0.11851739883422852
	model : 0.06765341758728027
			 train-loss:  2.0930710507080694 	 ± 0.2801839834857518
	data : 0.11831583976745605
	model : 0.06751003265380859
			 train-loss:  2.093724096992186 	 ± 0.27972791633886884
	data : 0.11840519905090333
	model : 0.06701841354370117
			 train-loss:  2.0939221567577784 	 ± 0.2791213474280836
	data : 0.1188124656677246
	model : 0.06739397048950195
			 train-loss:  2.0985724920720124 	 ± 0.2871058374218199
	data : 0.11825146675109863
	model : 0.0681180477142334
			 train-loss:  2.09777285138941 	 ± 0.2867248598872576
	data : 0.11758275032043457
	model : 0.06883411407470703
			 train-loss:  2.0991691999268114 	 ± 0.28686786546385146
	data : 0.11683125495910644
	model : 0.06888971328735352
			 train-loss:  2.099436933817301 	 ± 0.28626937798601
	data : 0.11669797897338867
	model : 0.06924614906311036
			 train-loss:  2.0984922326129416 	 ± 0.2860038901252135
	data : 0.11660141944885254
	model : 0.06929025650024415
			 train-loss:  2.0988889483662394 	 ± 0.2854475757041104
	data : 0.1164696216583252
	model : 0.06933732032775879
			 train-loss:  2.097351312123496 	 ± 0.28578885445504715
	data : 0.11649899482727051
	model : 0.06864237785339355
			 train-loss:  2.0958891876777352 	 ± 0.2860431840225007
	data : 0.11727375984191894
	model : 0.06833410263061523
			 train-loss:  2.095365658784524 	 ± 0.28554317225987663
	data : 0.11763567924499511
	model : 0.06844267845153809
			 train-loss:  2.09764212750374 	 ± 0.2870550586374607
	data : 0.11714081764221192
	model : 0.06787624359130859
			 train-loss:  2.0995447453805958 	 ± 0.2879273188685265
	data : 0.11773180961608887
	model : 0.06763482093811035
			 train-loss:  2.1004207898795855 	 ± 0.2876342504349721
	data : 0.11758651733398437
	model : 0.06815838813781738
			 train-loss:  2.1003617909776064 	 ± 0.28703077779438246
	data : 0.11687860488891602
	model : 0.06834168434143066
			 train-loss:  2.099399025470143 	 ± 0.2868145039188657
	data : 0.11674270629882813
	model : 0.0675318717956543
			 train-loss:  2.1000563825170198 	 ± 0.2863967095893286
	data : 0.11779379844665527
	model : 0.06786966323852539
			 train-loss:  2.0994982353384564 	 ± 0.28593267973154474
	data : 0.11729302406311035
	model : 0.06773285865783692
			 train-loss:  2.0979281831378778 	 ± 0.2863804058298888
	data : 0.1173736572265625
	model : 0.06708121299743652
			 train-loss:  2.099773282376827 	 ± 0.2872282966982567
	data : 0.11803321838378907
	model : 0.06674246788024903
			 train-loss:  2.0991307150145047 	 ± 0.28681407259263575
	data : 0.11809000968933106
	model : 0.06725955009460449
			 train-loss:  2.0987580790811657 	 ± 0.2862873188545052
	data : 0.11753959655761718
	model : 0.06700401306152344
			 train-loss:  2.0973581051438805 	 ± 0.28654395576017977
	data : 0.11815447807312011
	model : 0.0672140121459961
			 train-loss:  2.096384632925273 	 ± 0.2863706361526325
	data : 0.11828999519348145
	model : 0.06735019683837891
			 train-loss:  2.0965737426473248 	 ± 0.28580814648262387
	data : 0.11828050613403321
	model : 0.06747961044311523
			 train-loss:  2.096194186363833 	 ± 0.28529627897490234
	data : 0.11840786933898925
	model : 0.06772322654724121
			 train-loss:  2.0975070967674254 	 ± 0.2854778461246601
	data : 0.11816825866699218
	model : 0.06821770668029785
			 train-loss:  2.0981659096075718 	 ± 0.2850989610321169
	data : 0.11766815185546875
	model : 0.06843719482421876
			 train-loss:  2.0976574042486766 	 ± 0.2846467552536571
	data : 0.11752691268920898
	model : 0.06881489753723144
			 train-loss:  2.097234590722638 	 ± 0.2841629348846359
	data : 0.11698441505432129
	model : 0.0688779354095459
			 train-loss:  2.0962517482089247 	 ± 0.28403355186004114
	data : 0.11704216003417969
	model : 0.06844573020935059
			 train-loss:  2.0968427167219272 	 ± 0.2836324976522445
	data : 0.11726870536804199
	model : 0.06827917098999023
			 train-loss:  2.0974890696816146 	 ± 0.2832660904354437
	data : 0.11623616218566894
	model : 0.05923848152160645
#epoch  56    val-loss:  2.423143656630265  train-loss:  2.0974890696816146  lr:  7.8125e-05
			 train-loss:  1.9284789562225342 	 ± 0.0
	data : 5.866170406341553
	model : 0.0757608413696289
			 train-loss:  2.0420972108840942 	 ± 0.11361825466156006
	data : 2.999730706214905
	model : 0.0725330114364624
			 train-loss:  2.1145475705464682 	 ± 0.1382178754429906
	data : 2.0385515689849854
	model : 0.07142114639282227
			 train-loss:  2.1701477766036987 	 ± 0.1536303503862439
	data : 1.5580416917800903
	model : 0.07034403085708618
			 train-loss:  2.1147269725799562 	 ± 0.1765437331722438
	data : 1.2701918601989746
	model : 0.07012348175048828
			 train-loss:  2.0726674993832908 	 ± 0.18659601066626655
	data : 0.12039203643798828
	model : 0.06897797584533691
			 train-loss:  2.0723387002944946 	 ± 0.1727562145877024
	data : 0.11689858436584473
	model : 0.06994218826293945
			 train-loss:  2.088695153594017 	 ± 0.16729272524713815
	data : 0.11589946746826171
	model : 0.06926560401916504
			 train-loss:  2.069957865609063 	 ± 0.16639078385240336
	data : 0.11643657684326172
	model : 0.0689246654510498
			 train-loss:  2.093369889259338 	 ± 0.17277270999357783
	data : 0.11681537628173828
	model : 0.06906070709228515
			 train-loss:  2.0811368877237495 	 ± 0.16921347171490467
	data : 0.11674342155456544
	model : 0.06883606910705567
			 train-loss:  2.060016284386317 	 ± 0.1765048946376144
	data : 0.11690788269042969
	model : 0.06713871955871582
			 train-loss:  2.0750015057050266 	 ± 0.17734768195137846
	data : 0.1186143398284912
	model : 0.06748800277709961
			 train-loss:  2.116199723311833 	 ± 0.226429735900536
	data : 0.11830863952636719
	model : 0.06830434799194336
			 train-loss:  2.1143673499425253 	 ± 0.21885932293817809
	data : 0.11755309104919434
	model : 0.06813750267028809
			 train-loss:  2.1164577826857567 	 ± 0.2120642335955252
	data : 0.1176645278930664
	model : 0.06771502494812012
			 train-loss:  2.1141409663593067 	 ± 0.20594114296730182
	data : 0.11807160377502442
	model : 0.06893162727355957
			 train-loss:  2.102481782436371 	 ± 0.20583116254950917
	data : 0.11700735092163086
	model : 0.06988844871520997
			 train-loss:  2.098805672244022 	 ± 0.20094750943817674
	data : 0.11633152961730957
	model : 0.07028498649597167
			 train-loss:  2.14400400519371 	 ± 0.2778053332805599
	data : 0.11609277725219727
	model : 0.07027239799499511
			 train-loss:  2.1501908132008145 	 ± 0.27251843390471964
	data : 0.11605849266052246
	model : 0.07090306282043457
			 train-loss:  2.1478933583606374 	 ± 0.2664608795433724
	data : 0.11538753509521485
	model : 0.07003827095031738
			 train-loss:  2.133675082870152 	 ± 0.26900165203176396
	data : 0.11603169441223145
	model : 0.06958532333374023
			 train-loss:  2.126383071144422 	 ± 0.2656497677416275
	data : 0.11633081436157226
	model : 0.06939096450805664
			 train-loss:  2.1256923913955688 	 ± 0.26030454483451654
	data : 0.11627473831176757
	model : 0.06868433952331543
			 train-loss:  2.1310818057793837 	 ± 0.2566680858280058
	data : 0.11686849594116211
	model : 0.06866717338562012
			 train-loss:  2.126000408773069 	 ± 0.2531993228454791
	data : 0.11710567474365234
	model : 0.06905417442321778
			 train-loss:  2.137271170105253 	 ± 0.25544093163554454
	data : 0.11699624061584472
	model : 0.06824979782104493
			 train-loss:  2.133612275123596 	 ± 0.25174375365840274
	data : 0.11764717102050781
	model : 0.06822547912597657
			 train-loss:  2.121943465868632 	 ± 0.2553646264251476
	data : 0.1178274154663086
	model : 0.06897625923156739
			 train-loss:  2.1296531846446376 	 ± 0.25473653391427353
	data : 0.1171459674835205
	model : 0.06906957626342773
			 train-loss:  2.1199565045535564 	 ± 0.2564715598865722
	data : 0.11716604232788086
	model : 0.06929340362548828
			 train-loss:  2.125124458110694 	 ± 0.25424210190717156
	data : 0.11681833267211914
	model : 0.07010188102722167
			 train-loss:  2.1168655823258793 	 ± 0.25492900173132177
	data : 0.11612834930419921
	model : 0.07016448974609375
			 train-loss:  2.1135955231530326 	 ± 0.25198322353054287
	data : 0.1158484935760498
	model : 0.07085075378417968
			 train-loss:  2.1187231971157923 	 ± 0.2503038877304757
	data : 0.11495494842529297
	model : 0.0707326889038086
			 train-loss:  2.118961994712417 	 ± 0.24690239126319
	data : 0.11498603820800782
	model : 0.06966910362243653
			 train-loss:  2.1147168498290214 	 ± 0.24499662428763164
	data : 0.1159012794494629
	model : 0.06946840286254882
			 train-loss:  2.1081449068509617 	 ± 0.24520506167695158
	data : 0.11610345840454102
	model : 0.06964964866638183
			 train-loss:  2.1150308430194853 	 ± 0.245909765513613
	data : 0.11599874496459961
	model : 0.06885848045349122
			 train-loss:  2.1195330154605028 	 ± 0.24455567394307423
	data : 0.11682291030883789
	model : 0.06800661087036133
			 train-loss:  2.1115097375143144 	 ± 0.24702789851258725
	data : 0.11755218505859374
	model : 0.06804852485656739
			 train-loss:  2.1160847419916196 	 ± 0.24593238122315658
	data : 0.11771607398986816
	model : 0.06736469268798828
			 train-loss:  2.1177661906589162 	 ± 0.24337152891900324
	data : 0.11843638420104981
	model : 0.06715188026428223
			 train-loss:  2.1276734246148004 	 ± 0.24946388522895072
	data : 0.11871438026428223
	model : 0.06666445732116699
			 train-loss:  2.114885470141535 	 ± 0.26122458833356504
	data : 0.11938033103942872
	model : 0.06758875846862793
			 train-loss:  2.114186210835234 	 ± 0.2584741756126613
	data : 0.11850857734680176
	model : 0.06843876838684082
			 train-loss:  2.1073012202978134 	 ± 0.2600865102951458
	data : 0.11779065132141113
	model : 0.06903934478759766
			 train-loss:  2.1058028425489153 	 ± 0.2576281221096826
	data : 0.11722245216369628
	model : 0.06868538856506348
			 train-loss:  2.1052656507492067 	 ± 0.2550665491439055
	data : 0.1174731731414795
	model : 0.0694084644317627
			 train-loss:  2.113294844533883 	 ± 0.25885647868171385
	data : 0.11674342155456544
	model : 0.06938471794128417
			 train-loss:  2.1088871910021854 	 ± 0.258280631318275
	data : 0.11668872833251953
	model : 0.06858196258544921
			 train-loss:  2.1128807382763557 	 ± 0.2574481387522541
	data : 0.11735377311706544
	model : 0.06895685195922852
			 train-loss:  2.10941270104161 	 ± 0.2562998071422967
	data : 0.11695828437805175
	model : 0.06909399032592774
			 train-loss:  2.1114386081695558 	 ± 0.25439510042830793
	data : 0.11668391227722168
	model : 0.06895532608032226
			 train-loss:  2.1151299391474043 	 ± 0.2535954152223719
	data : 0.11676783561706543
	model : 0.06799526214599609
			 train-loss:  2.109197269406235 	 ± 0.2552516127441552
	data : 0.11765842437744141
	model : 0.06846351623535156
			 train-loss:  2.1112311091916314 	 ± 0.25350706560953373
	data : 0.11719279289245606
	model : 0.06833510398864746
			 train-loss:  2.1159133183754095 	 ± 0.2538663422424539
	data : 0.11749229431152344
	model : 0.06842546463012696
			 train-loss:  2.113638969262441 	 ± 0.2523473232752999
	data : 0.11744294166564942
	model : 0.06836256980895997
			 train-loss:  2.111712588638556 	 ± 0.2507147926576704
	data : 0.11747560501098633
	model : 0.06944379806518555
			 train-loss:  2.113533769884417 	 ± 0.24909112551935056
	data : 0.11656012535095214
	model : 0.06978034973144531
			 train-loss:  2.118275033103095 	 ± 0.24991050251311508
	data : 0.11617817878723144
	model : 0.06999707221984863
			 train-loss:  2.1174927726387978 	 ± 0.24802811840729477
	data : 0.11600208282470703
	model : 0.06980757713317871
			 train-loss:  2.112742834824782 	 ± 0.24902907410576713
	data : 0.11631426811218262
	model : 0.07006340026855469
			 train-loss:  2.1100340449448787 	 ± 0.24809835053621532
	data : 0.116168212890625
	model : 0.07001705169677734
			 train-loss:  2.110239155256926 	 ± 0.24624554776373508
	data : 0.11617813110351563
	model : 0.07025995254516601
			 train-loss:  2.1060345541028416 	 ± 0.24683926362712016
	data : 0.11607561111450196
	model : 0.07004079818725586
			 train-loss:  2.1043126963186953 	 ± 0.2454550673230312
	data : 0.11604132652282714
	model : 0.07010817527770996
			 train-loss:  2.104694240433829 	 ± 0.2437161184305031
	data : 0.11587071418762207
	model : 0.06997117996215821
			 train-loss:  2.1045567787868875 	 ± 0.2419964543579431
	data : 0.11596007347106933
	model : 0.06914710998535156
			 train-loss:  2.0999984509415097 	 ± 0.24336018966196726
	data : 0.11662888526916504
	model : 0.0680619716644287
			 train-loss:  2.097157466901492 	 ± 0.24288684083715237
	data : 0.11749057769775391
	model : 0.0671717643737793
			 train-loss:  2.1036193161397367 	 ± 0.2474771750626308
	data : 0.11847143173217774
	model : 0.06729202270507813
			 train-loss:  2.1051104847590127 	 ± 0.24615624716753118
	data : 0.11853532791137696
	model : 0.06709408760070801
			 train-loss:  2.1008006977407554 	 ± 0.24736348327181598
	data : 0.11862835884094239
	model : 0.06788196563720703
			 train-loss:  2.1026300408623437 	 ± 0.24626889414433994
	data : 0.11798787117004395
	model : 0.06859474182128907
			 train-loss:  2.1025203481698647 	 ± 0.2446870481701295
	data : 0.11726441383361816
	model : 0.06939539909362794
			 train-loss:  2.0968502141252348 	 ± 0.2482370118175019
	data : 0.11643471717834472
	model : 0.06944246292114258
			 train-loss:  2.0953209936618804 	 ± 0.24705482501552029
	data : 0.11649999618530274
	model : 0.06963491439819336
			 train-loss:  2.1025339556329046 	 ± 0.2538596073860555
	data : 0.11614904403686524
	model : 0.06957879066467285
			 train-loss:  2.1032213844904084 	 ± 0.2523827777732851
	data : 0.11617436408996581
	model : 0.06975483894348145
			 train-loss:  2.0995420519127905 	 ± 0.25306067654613307
	data : 0.11601700782775878
	model : 0.06988592147827148
			 train-loss:  2.1048816045125327 	 ± 0.256210317482666
	data : 0.11585407257080078
	model : 0.06983842849731445
			 train-loss:  2.103406510633581 	 ± 0.2550572943671109
	data : 0.11575770378112793
	model : 0.07004747390747071
			 train-loss:  2.1024410253347354 	 ± 0.2537262554603113
	data : 0.11572251319885254
	model : 0.0701838493347168
			 train-loss:  2.1027278324653365 	 ± 0.2522778651863316
	data : 0.11563758850097657
	model : 0.06941657066345215
			 train-loss:  2.1034021106633274 	 ± 0.25091920463953765
	data : 0.11641936302185059
	model : 0.0693502426147461
			 train-loss:  2.10662158955349 	 ± 0.25132678011260995
	data : 0.11644716262817383
	model : 0.06847133636474609
			 train-loss:  2.1054875718222723 	 ± 0.2501554897667005
	data : 0.11738615036010742
	model : 0.06816954612731933
			 train-loss:  2.106013303274637 	 ± 0.24882720246979734
	data : 0.11781744956970215
	model : 0.06782755851745606
			 train-loss:  2.1054875358291296 	 ± 0.24752200522358914
	data : 0.11811747550964355
	model : 0.067665433883667
			 train-loss:  2.1066116594499156 	 ± 0.24642364485869928
	data : 0.11824445724487305
	model : 0.0677058219909668
			 train-loss:  2.1050797396517815 	 ± 0.24555418262736625
	data : 0.11829619407653809
	model : 0.06783814430236816
			 train-loss:  2.102353136163009 	 ± 0.24568472339776745
	data : 0.11804418563842774
	model : 0.06712961196899414
			 train-loss:  2.102954243620237 	 ± 0.24447198087889746
	data : 0.1187563419342041
	model : 0.06644415855407715
			 train-loss:  2.103084979598055 	 ± 0.24321192458982005
	data : 0.11925063133239747
	model : 0.06749324798583985
			 train-loss:  2.105307408741542 	 ± 0.2429558593057482
	data : 0.11837577819824219
	model : 0.06727209091186523
			 train-loss:  2.1011899890321675 	 ± 0.24513816494525817
	data : 0.1184279441833496
	model : 0.06789078712463378
			 train-loss:  2.100161888599396 	 ± 0.2441238103362472
	data : 0.11784548759460449
	model : 0.06882100105285645
			 train-loss:  2.1025490713591624 	 ± 0.2440824352863516
	data : 0.11701664924621583
	model : 0.06953616142272949
			 train-loss:  2.100419949082767 	 ± 0.24382371461029573
	data : 0.1164555549621582
	model : 0.06947331428527832
			 train-loss:  2.0972887439635195 	 ± 0.24468933925340225
	data : 0.11643648147583008
	model : 0.06954941749572754
			 train-loss:  2.095609276340558 	 ± 0.24410590945110233
	data : 0.11657648086547852
	model : 0.06987442970275878
			 train-loss:  2.0971492392676216 	 ± 0.24344779254795307
	data : 0.11632356643676758
	model : 0.06983485221862792
			 train-loss:  2.0997702659301036 	 ± 0.24378070813917607
	data : 0.11633124351501464
	model : 0.06926479339599609
			 train-loss:  2.1011968271754613 	 ± 0.24308299054416319
	data : 0.11685805320739746
	model : 0.06913619041442871
			 train-loss:  2.104721505332876 	 ± 0.24468656660062427
	data : 0.11685991287231445
	model : 0.0687870979309082
			 train-loss:  2.101991004900101 	 ± 0.24520898073866124
	data : 0.11704463958740234
	model : 0.06864652633666993
			 train-loss:  2.1021035595373676 	 ± 0.24409467830544637
	data : 0.11708192825317383
	model : 0.06882977485656738
			 train-loss:  2.1010682217709653 	 ± 0.2432351680257622
	data : 0.11689152717590331
	model : 0.06881141662597656
			 train-loss:  2.1011723833424703 	 ± 0.24214934875909802
	data : 0.11700263023376464
	model : 0.06871490478515625
			 train-loss:  2.1006571676878805 	 ± 0.2411371639025635
	data : 0.11715455055236816
	model : 0.06895632743835449
			 train-loss:  2.10075987431041 	 ± 0.2400796977103156
	data : 0.11698441505432129
	model : 0.06865172386169434
			 train-loss:  2.099519567904265 	 ± 0.2394001504150785
	data : 0.11736254692077637
	model : 0.06856842041015625
			 train-loss:  2.0989429025814452 	 ± 0.23844622404604723
	data : 0.11747641563415527
	model : 0.06944894790649414
			 train-loss:  2.0981557939806557 	 ± 0.237576334121773
	data : 0.11661434173583984
	model : 0.06870031356811523
			 train-loss:  2.0963254126451782 	 ± 0.23739455080702657
	data : 0.11743721961975098
	model : 0.06890525817871093
			 train-loss:  2.0938207452036752 	 ± 0.23795555851954864
	data : 0.11734375953674317
	model : 0.06920881271362304
			 train-loss:  2.0905959486961363 	 ± 0.23955898846502618
	data : 0.11703219413757324
	model : 0.06996474266052247
			 train-loss:  2.089680178106324 	 ± 0.23877784664661905
	data : 0.11630077362060547
	model : 0.06991415023803711
			 train-loss:  2.090502323674374 	 ± 0.23796914055041488
	data : 0.11637635231018066
	model : 0.07084908485412597
			 train-loss:  2.090386015612905 	 ± 0.23700329387556704
	data : 0.11559128761291504
	model : 0.0709160327911377
			 train-loss:  2.08786392019641 	 ± 0.2376972268262343
	data : 0.11542310714721679
	model : 0.0711442470550537
			 train-loss:  2.088869171142578 	 ± 0.2370090241349196
	data : 0.115364408493042
	model : 0.07039170265197754
			 train-loss:  2.08941342149462 	 ± 0.23614504846435164
	data : 0.11600785255432129
	model : 0.07022643089294434
			 train-loss:  2.0870970863056932 	 ± 0.23664622326766496
	data : 0.11602258682250977
	model : 0.07003269195556641
			 train-loss:  2.086205318570137 	 ± 0.23593414438604585
	data : 0.11619725227355956
	model : 0.06991705894470215
			 train-loss:  2.0834006376044694 	 ± 0.237150348084294
	data : 0.11614313125610351
	model : 0.06915407180786133
			 train-loss:  2.0814942204035245 	 ± 0.23722670807434226
	data : 0.11671161651611328
	model : 0.06957416534423828
			 train-loss:  2.078970192042926 	 ± 0.23806535411111068
	data : 0.1163933277130127
	model : 0.06950030326843262
			 train-loss:  2.0812641779581704 	 ± 0.23861082658458355
	data : 0.1164785385131836
	model : 0.06957917213439942
			 train-loss:  2.081100494341743 	 ± 0.23771953956727412
	data : 0.11628589630126954
	model : 0.06952567100524902
			 train-loss:  2.0811140857525725 	 ± 0.23683091715022395
	data : 0.11651325225830078
	model : 0.06907191276550292
			 train-loss:  2.0828294277191164 	 ± 0.23678617241888214
	data : 0.11681075096130371
	model : 0.06927800178527832
			 train-loss:  2.0804792730247272 	 ± 0.23748908326169754
	data : 0.1165956974029541
	model : 0.06950688362121582
			 train-loss:  2.077761399484899 	 ± 0.2387341363101177
	data : 0.11639323234558105
	model : 0.06962828636169434
			 train-loss:  2.077648477277894 	 ± 0.2378712569291592
	data : 0.11623811721801758
	model : 0.07060122489929199
			 train-loss:  2.078159479786166 	 ± 0.23709006704153096
	data : 0.1152623176574707
	model : 0.07079911231994629
			 train-loss:  2.0785125851631165 	 ± 0.23627847710126515
	data : 0.11502089500427246
	model : 0.06940131187438965
			 train-loss:  2.0807544508724347 	 ± 0.23692870985228973
	data : 0.11637868881225585
	model : 0.06926798820495605
			 train-loss:  2.082313018785396 	 ± 0.23681723550539113
	data : 0.1165647029876709
	model : 0.06914234161376953
			 train-loss:  2.0832778040345734 	 ± 0.23626763108340462
	data : 0.1165811538696289
	model : 0.06813387870788574
			 train-loss:  2.082607029212846 	 ± 0.23558242558508463
	data : 0.11767797470092774
	model : 0.06944851875305176
			 train-loss:  2.082348820258831 	 ± 0.23478911329596663
	data : 0.11653070449829102
	model : 0.07020535469055175
			 train-loss:  2.0823935009028816 	 ± 0.23398427799124955
	data : 0.11587705612182617
	model : 0.07039341926574708
			 train-loss:  2.0834787686665854 	 ± 0.2335554801253722
	data : 0.115787935256958
	model : 0.07002267837524415
			 train-loss:  2.0849492710989876 	 ± 0.23344691711558532
	data : 0.11625313758850098
	model : 0.06978874206542969
			 train-loss:  2.08581813069798 	 ± 0.23290220274012294
	data : 0.11629009246826172
	model : 0.06925020217895508
			 train-loss:  2.0855521821975707 	 ± 0.2321472629526043
	data : 0.1169189453125
	model : 0.06920599937438965
			 train-loss:  2.0876572590000584 	 ± 0.23280925874344946
	data : 0.11693787574768066
	model : 0.06825146675109864
			 train-loss:  2.087018939225297 	 ± 0.2321747106997927
	data : 0.11771373748779297
	model : 0.06852207183837891
			 train-loss:  2.087184677716174 	 ± 0.2314237471033519
	data : 0.11757545471191407
	model : 0.06906280517578126
			 train-loss:  2.088556256387141 	 ± 0.23129419838770537
	data : 0.11720051765441894
	model : 0.0689465045928955
			 train-loss:  2.0897741033184913 	 ± 0.231041705129398
	data : 0.11720113754272461
	model : 0.06866412162780762
			 train-loss:  2.0889214880955524 	 ± 0.23054449888394163
	data : 0.11727685928344726
	model : 0.06953396797180175
			 train-loss:  2.088176942175361 	 ± 0.22999718354991489
	data : 0.11643218994140625
	model : 0.06975755691528321
			 train-loss:  2.0888843181767043 	 ± 0.2294394523633512
	data : 0.11613330841064454
	model : 0.06963214874267579
			 train-loss:  2.0890789369367204 	 ± 0.22872988898085136
	data : 0.11618142127990723
	model : 0.06894860267639161
			 train-loss:  2.087906589359045 	 ± 0.22849268700770983
	data : 0.11676807403564453
	model : 0.068538236618042
			 train-loss:  2.087001128966764 	 ± 0.2280697403158631
	data : 0.1174044132232666
	model : 0.06764683723449708
			 train-loss:  2.086179463951676 	 ± 0.2276036416661194
	data : 0.11823477745056152
	model : 0.06758952140808105
			 train-loss:  2.0885851017536563 	 ± 0.22896094519614651
	data : 0.11839265823364258
	model : 0.06742510795593262
			 train-loss:  2.088511391383846 	 ± 0.22826376607558407
	data : 0.11865358352661133
	model : 0.06748766899108886
			 train-loss:  2.0900858590097138 	 ± 0.22846249255488182
	data : 0.11859774589538574
	model : 0.06804633140563965
			 train-loss:  2.0904825652938293 	 ± 0.22783030778931343
	data : 0.11805305480957032
	model : 0.06860642433166504
			 train-loss:  2.0878990574511227 	 ± 0.22957308206620827
	data : 0.11741924285888672
	model : 0.06791243553161622
			 train-loss:  2.087918242528325 	 ± 0.22888894333563678
	data : 0.1178903579711914
	model : 0.06806120872497559
			 train-loss:  2.0854732962049676 	 ± 0.23040054602226334
	data : 0.1176304817199707
	model : 0.06886544227600097
			 train-loss:  2.0863644445643708 	 ± 0.23001382770378995
	data : 0.1169970989227295
	model : 0.06885814666748047
			 train-loss:  2.084573853782743 	 ± 0.23052553789197627
	data : 0.1167670726776123
	model : 0.06915550231933594
			 train-loss:  2.0849200313867526 	 ± 0.22989900135583366
	data : 0.11689214706420899
	model : 0.06930766105651856
			 train-loss:  2.0846384295149347 	 ± 0.2292633387691491
	data : 0.11666536331176758
	model : 0.06927895545959473
			 train-loss:  2.0843695162356584 	 ± 0.2286309477615497
	data : 0.11668257713317871
	model : 0.06917877197265625
			 train-loss:  2.083708348274231 	 ± 0.22814354121054622
	data : 0.11683273315429688
	model : 0.06933913230895997
			 train-loss:  2.0824293263933877 	 ± 0.22812282154679736
	data : 0.11670317649841308
	model : 0.06926231384277344
			 train-loss:  2.079976236752871 	 ± 0.2297936388902767
	data : 0.11668009757995605
	model : 0.06951570510864258
			 train-loss:  2.079395568102933 	 ± 0.22927742732997955
	data : 0.11666617393493653
	model : 0.0695727825164795
			 train-loss:  2.08036726346895 	 ± 0.22900333640811246
	data : 0.116583251953125
	model : 0.0696134090423584
			 train-loss:  2.0822233829233383 	 ± 0.2297125765507294
	data : 0.11647882461547851
	model : 0.06952223777770997
			 train-loss:  2.084672621600536 	 ± 0.23142193340700534
	data : 0.11662406921386718
	model : 0.06964073181152344
			 train-loss:  2.0884524341467974 	 ± 0.23632137189938307
	data : 0.11661157608032227
	model : 0.06945104598999023
			 train-loss:  2.0906001511818726 	 ± 0.23744919652809587
	data : 0.1166576862335205
	model : 0.06852426528930664
			 train-loss:  2.0912983786800634 	 ± 0.236991377103758
	data : 0.11746730804443359
	model : 0.06753945350646973
			 train-loss:  2.091433341438706 	 ± 0.2363570820776914
	data : 0.1182248592376709
	model : 0.0673872947692871
			 train-loss:  2.0924443384652496 	 ± 0.23612160814185248
	data : 0.11844539642333984
	model : 0.06711907386779785
			 train-loss:  2.0925304653810306 	 ± 0.2354923500939061
	data : 0.11835236549377441
	model : 0.06764402389526367
			 train-loss:  2.091386171731543 	 ± 0.23538590458208994
	data : 0.11810417175292968
	model : 0.06853313446044922
			 train-loss:  2.0921142574340577 	 ± 0.23497452695227544
	data : 0.11739258766174317
	model : 0.06947717666625977
			 train-loss:  2.092528143054561 	 ± 0.23442442146151485
	data : 0.11656551361083985
	model : 0.06887373924255372
			 train-loss:  2.0912290474507196 	 ± 0.2344946507837753
	data : 0.11704096794128419
	model : 0.06825189590454102
			 train-loss:  2.089042140170932 	 ± 0.23582793808107586
	data : 0.11779041290283203
	model : 0.06739788055419922
			 train-loss:  2.0897346142042488 	 ± 0.23541181916862275
	data : 0.11846504211425782
	model : 0.06738557815551757
			 train-loss:  2.0908491445570876 	 ± 0.235314260960785
	data : 0.11859312057495117
	model : 0.0674060344696045
			 train-loss:  2.0901332525106575 	 ± 0.23492182445071866
	data : 0.11866354942321777
	model : 0.06819825172424317
			 train-loss:  2.0886689923247514 	 ± 0.2352122065682695
	data : 0.1179274082183838
	model : 0.06915426254272461
			 train-loss:  2.088487462949027 	 ± 0.23462822599263725
	data : 0.117026948928833
	model : 0.06988463401794434
			 train-loss:  2.090289243543991 	 ± 0.23539735663208797
	data : 0.1163442611694336
	model : 0.06999263763427735
			 train-loss:  2.0891444641142036 	 ± 0.23535706197463258
	data : 0.11623129844665528
	model : 0.0708838939666748
			 train-loss:  2.0893349415063858 	 ± 0.23478330846543152
	data : 0.11521968841552735
	model : 0.07090291976928711
			 train-loss:  2.089031279383607 	 ± 0.234237911716261
	data : 0.11512165069580078
	model : 0.07090682983398437
			 train-loss:  2.0877744956771926 	 ± 0.23433578271122155
	data : 0.11510281562805176
	model : 0.07101011276245117
			 train-loss:  2.0885536817494286 	 ± 0.23402006504244405
	data : 0.1149688720703125
	model : 0.07100906372070312
			 train-loss:  2.0855733243858112 	 ± 0.23727639292024044
	data : 0.11498017311096191
	model : 0.06945786476135254
			 train-loss:  2.085040872852977 	 ± 0.2368191018768989
	data : 0.11667380332946778
	model : 0.06949853897094727
			 train-loss:  2.085290387996192 	 ± 0.23627060946733713
	data : 0.11674432754516602
	model : 0.06940779685974122
			 train-loss:  2.0853171567410085 	 ± 0.2356995297259325
	data : 0.11681485176086426
	model : 0.06953229904174804
			 train-loss:  2.0848511319894056 	 ± 0.23522783966168376
	data : 0.11666922569274903
	model : 0.06936750411987305
			 train-loss:  2.08516596607044 	 ± 0.23470834359000942
	data : 0.1166569709777832
	model : 0.06929073333740235
			 train-loss:  2.0841104592595783 	 ± 0.23464553796578583
	data : 0.11673736572265625
	model : 0.06937127113342285
			 train-loss:  2.0841159249933976 	 ± 0.23408885891374714
	data : 0.11639699935913086
	model : 0.0693507194519043
			 train-loss:  2.0830560630222537 	 ± 0.2340430142348537
	data : 0.11638002395629883
	model : 0.06906309127807617
			 train-loss:  2.0826246727240476 	 ± 0.23357743953497365
	data : 0.11682939529418945
	model : 0.06830081939697266
			 train-loss:  2.082559153298351 	 ± 0.23303302069881807
	data : 0.11767740249633789
	model : 0.06955580711364746
			 train-loss:  2.0818105847336525 	 ± 0.2327482032830996
	data : 0.11655826568603515
	model : 0.06984872817993164
			 train-loss:  2.079929608989645 	 ± 0.233841006026053
	data : 0.11668543815612793
	model : 0.06992502212524414
			 train-loss:  2.0810911402724304 	 ± 0.23392529693880124
	data : 0.11661672592163086
	model : 0.0698878288269043
			 train-loss:  2.0827995932430303 	 ± 0.23474116104560186
	data : 0.11648902893066407
	model : 0.07065539360046387
			 train-loss:  2.0820504470502947 	 ± 0.23446565824531707
	data : 0.11566519737243652
	model : 0.06932277679443359
			 train-loss:  2.083516395633871 	 ± 0.2349359371600899
	data : 0.11669745445251464
	model : 0.06885085105895997
			 train-loss:  2.082624745584721 	 ± 0.23477660163091818
	data : 0.11703705787658691
	model : 0.06872615814208985
			 train-loss:  2.0827460455464886 	 ± 0.23425416937857765
	data : 0.11711297035217286
	model : 0.06812796592712403
			 train-loss:  2.084594564587546 	 ± 0.23534553021911336
	data : 0.11750411987304688
	model : 0.06808195114135743
			 train-loss:  2.0846345653491363 	 ± 0.23482037752225224
	data : 0.11759891510009765
	model : 0.06881637573242187
			 train-loss:  2.085310812526279 	 ± 0.2345164766451155
	data : 0.11687002182006836
	model : 0.06787672042846679
			 train-loss:  2.0871164202690125 	 ± 0.2355592776373447
	data : 0.1174098014831543
	model : 0.06764116287231445
			 train-loss:  2.0887699909672337 	 ± 0.23635076242155093
	data : 0.11763873100280761
	model : 0.06815805435180664
			 train-loss:  2.0875728423135325 	 ± 0.23652062055602385
	data : 0.11726326942443847
	model : 0.06771149635314941
			 train-loss:  2.0862466430039386 	 ± 0.23685169024551503
	data : 0.11749696731567383
	model : 0.06688427925109863
			 train-loss:  2.086606268779091 	 ± 0.2363988841132332
	data : 0.11827082633972168
	model : 0.06700749397277832
			 train-loss:  2.0866928322490677 	 ± 0.23589029633296069
	data : 0.1181872844696045
	model : 0.06667461395263671
			 train-loss:  2.0848989841239205 	 ± 0.23695509895352942
	data : 0.118491792678833
	model : 0.06680421829223633
			 train-loss:  2.0840986035924103 	 ± 0.23676013784534602
	data : 0.11841363906860351
	model : 0.06673641204833984
			 train-loss:  2.085195320793706 	 ± 0.23684606770670635
	data : 0.11854619979858398
	model : 0.06761345863342286
			 train-loss:  2.0847167050584834 	 ± 0.2364549771899126
	data : 0.11785950660705566
	model : 0.06838221549987793
			 train-loss:  2.084499567747116 	 ± 0.23597695932057175
	data : 0.11734089851379395
	model : 0.06887359619140625
			 train-loss:  2.085910543107785 	 ± 0.2364741168542602
	data : 0.1168135166168213
	model : 0.06819519996643067
			 train-loss:  2.084405356595496 	 ± 0.23711177514192633
	data : 0.11745920181274414
	model : 0.06851801872253419
			 train-loss:  2.0845969995195395 	 ± 0.236633675691286
	data : 0.11709127426147461
	model : 0.06826086044311523
			 train-loss:  2.0859769200285276 	 ± 0.2371018369152753
	data : 0.11712908744812012
	model : 0.06785979270935058
			 train-loss:  2.0863939404982252 	 ± 0.23669759567745377
	data : 0.11746134757995605
	model : 0.06737937927246093
			 train-loss:  2.0857411839745263 	 ± 0.2364253124741723
	data : 0.11800570487976074
	model : 0.06781568527221679
			 train-loss:  2.0851975895249795 	 ± 0.2360898338316459
	data : 0.11744680404663085
	model : 0.06761865615844727
			 train-loss:  2.0858237269471904 	 ± 0.2358076360403761
	data : 0.11773381233215333
	model : 0.0671311855316162
			 train-loss:  2.0838956745303405 	 ± 0.23724528079716026
	data : 0.11804304122924805
	model : 0.0673891544342041
			 train-loss:  2.0835527351232077 	 ± 0.23682342568991024
	data : 0.11784625053405762
	model : 0.06726694107055664
			 train-loss:  2.0827865426839605 	 ± 0.2366488608300397
	data : 0.11785602569580078
	model : 0.06696500778198242
			 train-loss:  2.082038560701955 	 ± 0.23646364813829193
	data : 0.11813511848449706
	model : 0.0669938087463379
			 train-loss:  2.082373747863923 	 ± 0.23604737095968842
	data : 0.1182161808013916
	model : 0.0675549030303955
			 train-loss:  2.081960825920105 	 ± 0.23566489666478269
	data : 0.11794300079345703
	model : 0.06743969917297363
			 train-loss:  2.0832382958248794 	 ± 0.23606071073385462
	data : 0.11784505844116211
	model : 0.06811542510986328
			 train-loss:  2.0834842787848578 	 ± 0.2356241009831739
	data : 0.11733193397521972
	model : 0.06797261238098144
			 train-loss:  2.0824770724820527 	 ± 0.23570091314688682
	data : 0.11751084327697754
	model : 0.0679811954498291
			 train-loss:  2.083148628238618 	 ± 0.2354788738803086
	data : 0.1173250675201416
	model : 0.06807971000671387
			 train-loss:  2.0843739878897574 	 ± 0.23582669586088412
	data : 0.117366361618042
	model : 0.06761317253112793
			 train-loss:  2.0821627266705036 	 ± 0.2379996949970516
	data : 0.11705999374389649
	model : 0.05839228630065918
#epoch  57    val-loss:  2.4360291706888297  train-loss:  2.0821627266705036  lr:  7.8125e-05
			 train-loss:  2.527031421661377 	 ± 0.0
	data : 5.371747016906738
	model : 0.07738494873046875
			 train-loss:  2.1665430068969727 	 ± 0.3604884147644043
	data : 2.7564902305603027
	model : 0.07152211666107178
			 train-loss:  2.310046672821045 	 ± 0.3575209118175666
	data : 1.877686341603597
	model : 0.07079243659973145
			 train-loss:  2.1868580877780914 	 ± 0.3760215209862371
	data : 1.4374255537986755
	model : 0.06997495889663696
			 train-loss:  2.1257862091064452 	 ± 0.3578167755343223
	data : 1.173593235015869
	model : 0.06915774345397949
			 train-loss:  2.0979123711586 	 ± 0.33253389539694056
	data : 0.12306618690490723
	model : 0.06739268302917481
			 train-loss:  2.1885491609573364 	 ± 0.37956815903227176
	data : 0.11825528144836425
	model : 0.06818189620971679
			 train-loss:  2.1454644799232483 	 ± 0.37290349570756
	data : 0.11745610237121581
	model : 0.069195556640625
			 train-loss:  2.118103133307563 	 ± 0.359993588471957
	data : 0.11647591590881348
	model : 0.06913766860961915
			 train-loss:  2.0959300637245177 	 ± 0.3479377106601641
	data : 0.11650443077087402
	model : 0.06960868835449219
			 train-loss:  2.097572922706604 	 ± 0.3317862665542674
	data : 0.11606535911560059
	model : 0.06905431747436523
			 train-loss:  2.073679765065511 	 ± 0.3273962872735951
	data : 0.11643838882446289
	model : 0.06830019950866699
			 train-loss:  2.05065210048969 	 ± 0.32450940337611556
	data : 0.11725530624389649
	model : 0.06784262657165527
			 train-loss:  2.0553572859082903 	 ± 0.31316493184692556
	data : 0.11789541244506836
	model : 0.06767578125
			 train-loss:  2.0715253512064615 	 ± 0.30853494388736574
	data : 0.11810822486877441
	model : 0.067252779006958
			 train-loss:  2.0677497386932373 	 ± 0.29909534787815806
	data : 0.11843657493591309
	model : 0.06822466850280762
			 train-loss:  2.0724135707406437 	 ± 0.2907641853851135
	data : 0.11778278350830078
	model : 0.06902179718017579
			 train-loss:  2.0690185096528797 	 ± 0.2829185086951325
	data : 0.1169365406036377
	model : 0.06852903366088867
			 train-loss:  2.0423688386615955 	 ± 0.2976806823099422
	data : 0.11741428375244141
	model : 0.06887569427490234
			 train-loss:  2.030172199010849 	 ± 0.29497373026444257
	data : 0.11722450256347657
	model : 0.06958589553833008
			 train-loss:  2.028905783380781 	 ± 0.2879205934275064
	data : 0.11670112609863281
	model : 0.06941699981689453
			 train-loss:  2.037824484434995 	 ± 0.28425441565907644
	data : 0.11678915023803711
	model : 0.06923694610595703
			 train-loss:  2.041741645854452 	 ± 0.27861277037678206
	data : 0.11697382926940918
	model : 0.06913065910339355
			 train-loss:  2.0549497455358505 	 ± 0.2800056022536674
	data : 0.11690645217895508
	model : 0.06939258575439453
			 train-loss:  2.0689245653152466 	 ± 0.28276157078093167
	data : 0.11638174057006836
	model : 0.06927709579467774
			 train-loss:  2.066814197943761 	 ± 0.2774712404050751
	data : 0.11663303375244141
	model : 0.06918954849243164
			 train-loss:  2.061519145965576 	 ± 0.2736197617482309
	data : 0.11663947105407715
	model : 0.0693861961364746
			 train-loss:  2.0479511320590973 	 ± 0.27778477637181404
	data : 0.11649274826049805
	model : 0.0694051742553711
			 train-loss:  2.0309014361480187 	 ± 0.2874768179947018
	data : 0.11636638641357422
	model : 0.06952357292175293
			 train-loss:  2.0241641839345297 	 ± 0.2849640035322655
	data : 0.11638097763061524
	model : 0.0696418285369873
			 train-loss:  2.0241231226151988 	 ± 0.2803302238940894
	data : 0.11618900299072266
	model : 0.06888337135314941
			 train-loss:  2.0301859974861145 	 ± 0.2779725959610607
	data : 0.11691889762878419
	model : 0.06900410652160645
			 train-loss:  2.0271839734279746 	 ± 0.2742547632319817
	data : 0.1167679786682129
	model : 0.06821317672729492
			 train-loss:  2.029510904760922 	 ± 0.27052196126506445
	data : 0.11744637489318847
	model : 0.06832098960876465
			 train-loss:  2.030050720487322 	 ± 0.266647934771123
	data : 0.11727848052978515
	model : 0.06840000152587891
			 train-loss:  2.0283760759565563 	 ± 0.2631050078888682
	data : 0.11714363098144531
	model : 0.06924715042114257
			 train-loss:  2.0261038219606555 	 ± 0.2598830363969273
	data : 0.11648058891296387
	model : 0.06851058006286621
			 train-loss:  2.0189681617837203 	 ± 0.26008805882040603
	data : 0.11730260848999023
	model : 0.06939244270324707
			 train-loss:  2.010550557038723 	 ± 0.26192331194002705
	data : 0.11653203964233398
	model : 0.06854453086853027
			 train-loss:  2.0131836146116258 	 ± 0.25915075245751945
	data : 0.11742138862609863
	model : 0.06848468780517578
			 train-loss:  2.021821135427894 	 ± 0.2617352784766085
	data : 0.11760087013244629
	model : 0.06759448051452636
			 train-loss:  2.020225629920051 	 ± 0.2588023323800079
	data : 0.11837649345397949
	model : 0.06820378303527833
			 train-loss:  2.015362082525741 	 ± 0.25771006228496335
	data : 0.11759748458862304
	model : 0.06836977005004882
			 train-loss:  2.015744531696493 	 ± 0.25477705113696947
	data : 0.11749958992004395
	model : 0.06855249404907227
			 train-loss:  2.0128764814800686 	 ± 0.2526475861059864
	data : 0.11719117164611817
	model : 0.06831426620483398
			 train-loss:  2.0133334683335344 	 ± 0.24990513056438735
	data : 0.11749906539916992
	model : 0.06990327835083007
			 train-loss:  2.0190913296760398 	 ± 0.25029748819994135
	data : 0.11584467887878418
	model : 0.06990089416503906
			 train-loss:  2.0196509088079133 	 ± 0.2477062083205494
	data : 0.1159785270690918
	model : 0.06977005004882812
			 train-loss:  2.032711980294208 	 ± 0.261332260424586
	data : 0.11609697341918945
	model : 0.06930785179138184
			 train-loss:  2.0349945569038392 	 ± 0.25919868243263516
	data : 0.11647858619689941
	model : 0.06983966827392578
			 train-loss:  2.034720825214012 	 ± 0.2566522373504907
	data : 0.11597332954406739
	model : 0.06912713050842285
			 train-loss:  2.0336725482573876 	 ± 0.25428266977488856
	data : 0.11667561531066895
	model : 0.06949014663696289
			 train-loss:  2.038263424387518 	 ± 0.2540386618758205
	data : 0.11645512580871582
	model : 0.06965970993041992
			 train-loss:  2.042168785024572 	 ± 0.25327630142121155
	data : 0.11646380424499511
	model : 0.07060766220092773
			 train-loss:  2.0449345805428245 	 ± 0.25178487095598606
	data : 0.11572771072387696
	model : 0.07040371894836425
			 train-loss:  2.053043003593172 	 ± 0.25667024645889286
	data : 0.11584396362304687
	model : 0.07043132781982422
			 train-loss:  2.0584639373578524 	 ± 0.2576227435068354
	data : 0.11588239669799805
	model : 0.0697404384613037
			 train-loss:  2.0512262562225603 	 ± 0.2611724836883954
	data : 0.1164475440979004
	model : 0.06967296600341796
			 train-loss:  2.0462747286942045 	 ± 0.26168104385180396
	data : 0.11648588180541992
	model : 0.06995592117309571
			 train-loss:  2.0433710694313048 	 ± 0.26044793892311746
	data : 0.1161534309387207
	model : 0.06896457672119141
			 train-loss:  2.0474757171068036 	 ± 0.26025371915751083
	data : 0.11698775291442871
	model : 0.06893081665039062
			 train-loss:  2.05311769054782 	 ± 0.2618802979838696
	data : 0.11700806617736817
	model : 0.06916337013244629
			 train-loss:  2.055012237458002 	 ± 0.26022151266862437
	data : 0.11662960052490234
	model : 0.06889462471008301
			 train-loss:  2.0593742057681084 	 ± 0.2604915957441747
	data : 0.11688752174377441
	model : 0.06853008270263672
			 train-loss:  2.058225780266982 	 ± 0.25864327434107715
	data : 0.11728911399841309
	model : 0.06947388648986816
			 train-loss:  2.0555722135486025 	 ± 0.2575664078703262
	data : 0.11637349128723144
	model : 0.06960020065307618
			 train-loss:  2.0572637550866424 	 ± 0.25600614264037314
	data : 0.11628212928771972
	model : 0.0705388069152832
			 train-loss:  2.054977387189865 	 ± 0.2548049747343602
	data : 0.1153195858001709
	model : 0.07073259353637695
			 train-loss:  2.056091334508813 	 ± 0.2531185582522404
	data : 0.11511874198913574
	model : 0.07033658027648926
			 train-loss:  2.054859946455274 	 ± 0.25151214459650323
	data : 0.1154977798461914
	model : 0.07027616500854492
			 train-loss:  2.054567446171398 	 ± 0.24974664167419128
	data : 0.1156191349029541
	model : 0.06972804069519042
			 train-loss:  2.0521926151381598 	 ± 0.24881220911814622
	data : 0.11628546714782714
	model : 0.06871795654296875
			 train-loss:  2.049388201269385 	 ± 0.24824529869589326
	data : 0.11738462448120117
	model : 0.06877083778381347
			 train-loss:  2.0453135773942277 	 ± 0.24900790162557207
	data : 0.1173006534576416
	model : 0.06903996467590331
			 train-loss:  2.0432378816604615 	 ± 0.24798595232392354
	data : 0.11707229614257812
	model : 0.06919841766357422
			 train-loss:  2.0438140238586224 	 ± 0.24639958716791682
	data : 0.11696004867553711
	model : 0.06973681449890137
			 train-loss:  2.0449504651032484 	 ± 0.24499476137263007
	data : 0.11624479293823242
	model : 0.06994619369506835
			 train-loss:  2.042934919014955 	 ± 0.24406089717850188
	data : 0.11608467102050782
	model : 0.0697472095489502
			 train-loss:  2.039555285550371 	 ± 0.24434122656681817
	data : 0.11617789268493653
	model : 0.07002019882202148
			 train-loss:  2.0399169251322746 	 ± 0.24283056631005845
	data : 0.11586108207702636
	model : 0.06913871765136718
			 train-loss:  2.03621725977203 	 ± 0.2435851023449709
	data : 0.11651678085327148
	model : 0.06909432411193847
			 train-loss:  2.0326781912547784 	 ± 0.24418158373478333
	data : 0.11676058769226075
	model : 0.06816105842590332
			 train-loss:  2.033055052699813 	 ± 0.24273014361898754
	data : 0.11754436492919922
	model : 0.06841187477111817
			 train-loss:  2.0324955454894473 	 ± 0.24133483335605752
	data : 0.11738510131835937
	model : 0.06836309432983398
			 train-loss:  2.031871602114509 	 ± 0.2399791606280084
	data : 0.11744475364685059
	model : 0.06889533996582031
			 train-loss:  2.0332743428474247 	 ± 0.23893011391306143
	data : 0.1170271396636963
	model : 0.06875748634338379
			 train-loss:  2.0311937510282143 	 ± 0.23833527265424911
	data : 0.11704883575439454
	model : 0.0690727710723877
			 train-loss:  2.0315164666284216 	 ± 0.23699634221136434
	data : 0.1168097972869873
	model : 0.06899886131286621
			 train-loss:  2.034935223922301 	 ± 0.23783336323759133
	data : 0.11685066223144532
	model : 0.06813712120056152
			 train-loss:  2.035662419266171 	 ± 0.23660785335440848
	data : 0.11766495704650878
	model : 0.06823849678039551
			 train-loss:  2.0373042743284624 	 ± 0.23581918360252643
	data : 0.1174661636352539
	model : 0.06835532188415527
			 train-loss:  2.0414533265258954 	 ± 0.2378502786806074
	data : 0.11726608276367187
	model : 0.06934652328491211
			 train-loss:  2.040027763253899 	 ± 0.23696288969639331
	data : 0.1163785457611084
	model : 0.06973462104797364
			 train-loss:  2.041338792506685 	 ± 0.2360379288215286
	data : 0.1160848617553711
	model : 0.07074856758117676
			 train-loss:  2.038536446972897 	 ± 0.23635912697160102
	data : 0.11527128219604492
	model : 0.07104377746582032
			 train-loss:  2.037845073888699 	 ± 0.2352214124510941
	data : 0.11504616737365722
	model : 0.07030348777770996
			 train-loss:  2.0397952991662565 	 ± 0.23478465452145023
	data : 0.11585946083068847
	model : 0.0698432445526123
			 train-loss:  2.0466474404140396 	 ± 0.24313715130692512
	data : 0.11611366271972656
	model : 0.06860146522521973
			 train-loss:  2.046917129044581 	 ± 0.24192080120110102
	data : 0.11704120635986329
	model : 0.06844687461853027
			 train-loss:  2.0477541196346283 	 ± 0.24085217890810073
	data : 0.11703367233276367
	model : 0.06828951835632324
			 train-loss:  2.047705354076801 	 ± 0.239657371498741
	data : 0.11725964546203613
	model : 0.06898622512817383
			 train-loss:  2.0473166318500744 	 ± 0.23851168242195303
	data : 0.11652255058288574
	model : 0.0690093994140625
			 train-loss:  2.044799723671478 	 ± 0.23870832835381778
	data : 0.11665611267089844
	model : 0.07003264427185059
			 train-loss:  2.0435433834791183 	 ± 0.2378998521028216
	data : 0.11590313911437988
	model : 0.07011017799377442
			 train-loss:  2.0461822112401324 	 ± 0.23828873602249317
	data : 0.11594209671020508
	model : 0.07034673690795898
			 train-loss:  2.0480888755816333 	 ± 0.2379654611865049
	data : 0.11582484245300292
	model : 0.07011637687683106
			 train-loss:  2.049008453003714 	 ± 0.23704001176514358
	data : 0.1160810947418213
	model : 0.07016196250915527
			 train-loss:  2.047015466071941 	 ± 0.2368389999380683
	data : 0.11611976623535156
	model : 0.07010111808776856
			 train-loss:  2.045899383518674 	 ± 0.2360352283127614
	data : 0.11609573364257812
	model : 0.07007927894592285
			 train-loss:  2.050291186029261 	 ± 0.23939201083575906
	data : 0.11622676849365235
	model : 0.06894464492797851
			 train-loss:  2.055314514014098 	 ± 0.24406548791250857
	data : 0.11715917587280274
	model : 0.06908106803894043
			 train-loss:  2.0556880331465175 	 ± 0.2430053331083724
	data : 0.11691627502441407
	model : 0.06816792488098145
			 train-loss:  2.054120634509399 	 ± 0.24249570340578352
	data : 0.11770048141479492
	model : 0.06796307563781738
			 train-loss:  2.056101045064759 	 ± 0.24234588728837053
	data : 0.11794261932373047
	model : 0.06760125160217285
			 train-loss:  2.056821158657903 	 ± 0.24141237827820436
	data : 0.11802878379821777
	model : 0.06848464012145997
			 train-loss:  2.056522771202285 	 ± 0.24039085293476656
	data : 0.11725883483886719
	model : 0.06866354942321777
			 train-loss:  2.056363047697605 	 ± 0.23936751872976988
	data : 0.11715235710144042
	model : 0.06969494819641113
			 train-loss:  2.058350182185739 	 ± 0.23931828419729403
	data : 0.11615200042724609
	model : 0.068760347366333
			 train-loss:  2.0561556886224186 	 ± 0.23949993329081848
	data : 0.11677937507629395
	model : 0.06904735565185546
			 train-loss:  2.0595189144213992 	 ± 0.24130532417348496
	data : 0.11655669212341309
	model : 0.06885066032409667
			 train-loss:  2.0610122375251834 	 ± 0.24086227570259536
	data : 0.11685800552368164
	model : 0.06899609565734863
			 train-loss:  2.0590919129184035 	 ± 0.2408013938758939
	data : 0.11685504913330078
	model : 0.06889047622680664
			 train-loss:  2.0621619486227267 	 ± 0.24220600847844764
	data : 0.11692256927490234
	model : 0.0699575424194336
			 train-loss:  2.060589368304899 	 ± 0.24185705668826019
	data : 0.11609420776367188
	model : 0.06940660476684571
			 train-loss:  2.0612985010147096 	 ± 0.2410170802324275
	data : 0.1164548397064209
	model : 0.06948347091674804
			 train-loss:  2.0622306560713146 	 ± 0.24028487558836525
	data : 0.11634640693664551
	model : 0.06917819976806641
			 train-loss:  2.0617072732429804 	 ± 0.23940909741067304
	data : 0.11652202606201172
	model : 0.0682215690612793
			 train-loss:  2.0603424943983555 	 ± 0.2389675333267601
	data : 0.11718516349792481
	model : 0.06818685531616211
			 train-loss:  2.059207801670991 	 ± 0.2383854176754686
	data : 0.11736617088317872
	model : 0.06880097389221192
			 train-loss:  2.057971341793354 	 ± 0.23788167516607162
	data : 0.11705303192138672
	model : 0.0681070327758789
			 train-loss:  2.056114039348282 	 ± 0.2379163060373708
	data : 0.1176307201385498
	model : 0.06821713447570801
			 train-loss:  2.0586046892585177 	 ± 0.23872156660543736
	data : 0.11774811744689942
	model : 0.06912860870361329
			 train-loss:  2.058724633733133 	 ± 0.23782641632805784
	data : 0.11701903343200684
	model : 0.06831889152526856
			 train-loss:  2.0599855513715033 	 ± 0.23738315597152568
	data : 0.11757612228393555
	model : 0.06833581924438477
			 train-loss:  2.058207718531291 	 ± 0.23739604480511606
	data : 0.11741347312927246
	model : 0.06933937072753907
			 train-loss:  2.0596580224878647 	 ± 0.23712117105331082
	data : 0.1165205955505371
	model : 0.07018833160400391
			 train-loss:  2.0596011694330367 	 ± 0.23625511065019528
	data : 0.11552634239196777
	model : 0.07014741897583007
			 train-loss:  2.0594599419745845 	 ± 0.23540336143135654
	data : 0.11561264991760253
	model : 0.0706913948059082
			 train-loss:  2.0595784067249983 	 ± 0.234559186642143
	data : 0.11515116691589355
	model : 0.07069735527038574
			 train-loss:  2.058834993839264 	 ± 0.23388425846098881
	data : 0.11525201797485352
	model : 0.0704655647277832
			 train-loss:  2.059548144644879 	 ± 0.23320611434679395
	data : 0.11536617279052734
	model : 0.06967177391052246
			 train-loss:  2.0594589408014863 	 ± 0.2323859293237117
	data : 0.11623606681823731
	model : 0.0697741985321045
			 train-loss:  2.0596842115575615 	 ± 0.23158752408804503
	data : 0.11628074645996093
	model : 0.06988563537597656
			 train-loss:  2.0582541815108724 	 ± 0.23141470228061178
	data : 0.11614227294921875
	model : 0.06980929374694825
			 train-loss:  2.0596923737690367 	 ± 0.23126020882468915
	data : 0.11615529060363769
	model : 0.06967320442199706
			 train-loss:  2.0582571625709534 	 ± 0.23111393195031277
	data : 0.1163403034210205
	model : 0.06963543891906739
			 train-loss:  2.0590850893332036 	 ± 0.23054363781948134
	data : 0.1162867546081543
	model : 0.06961612701416016
			 train-loss:  2.060779201823312 	 ± 0.2306797289372671
	data : 0.1163508415222168
	model : 0.06984295845031738
			 train-loss:  2.060094848575208 	 ± 0.230055029110674
	data : 0.11617088317871094
	model : 0.0696976661682129
			 train-loss:  2.060570507844289 	 ± 0.22936039868647362
	data : 0.1163203239440918
	model : 0.06988663673400879
			 train-loss:  2.063912676659641 	 ± 0.23223548601250654
	data : 0.11605801582336425
	model : 0.0698854923248291
			 train-loss:  2.0636329831261384 	 ± 0.23149580770030692
	data : 0.11601357460021973
	model : 0.06991839408874512
			 train-loss:  2.0637217775668972 	 ± 0.23074064219860643
	data : 0.11599612236022949
	model : 0.06896982192993165
			 train-loss:  2.0669904614423777 	 ± 0.2335170628098544
	data : 0.11689920425415039
	model : 0.06908760070800782
			 train-loss:  2.0658457694515104 	 ± 0.23319562603823543
	data : 0.11675386428833008
	model : 0.06866536140441895
			 train-loss:  2.0654214055110245 	 ± 0.23250703669904182
	data : 0.11716184616088868
	model : 0.06831207275390624
			 train-loss:  2.0654557783892202 	 ± 0.2317657831980739
	data : 0.11739311218261719
	model : 0.06827898025512695
			 train-loss:  2.064398472822165 	 ± 0.23141071072376979
	data : 0.11739811897277833
	model : 0.06919503211975098
			 train-loss:  2.066837267305866 	 ± 0.23270981574838379
	data : 0.11648526191711425
	model : 0.06933326721191406
			 train-loss:  2.068731054663658 	 ± 0.23320728802821591
	data : 0.1162574291229248
	model : 0.0696444034576416
			 train-loss:  2.0694268327322067 	 ± 0.23264844148974234
	data : 0.11600847244262695
	model : 0.06985244750976563
			 train-loss:  2.0702171517007146 	 ± 0.23214597090961583
	data : 0.11597118377685547
	model : 0.06975178718566895
			 train-loss:  2.0680032924640397 	 ± 0.23314183997289484
	data : 0.11609816551208496
	model : 0.06950173377990723
			 train-loss:  2.0678150530268504 	 ± 0.2324423792219167
	data : 0.11641545295715332
	model : 0.06944465637207031
			 train-loss:  2.0657407363255818 	 ± 0.23325450887634164
	data : 0.11649279594421387
	model : 0.06997528076171874
			 train-loss:  2.0641459736479333 	 ± 0.2334513827486477
	data : 0.1160883903503418
	model : 0.07012934684753418
			 train-loss:  2.0629758270914684 	 ± 0.23323914302657212
	data : 0.11592826843261719
	model : 0.0702782154083252
			 train-loss:  2.065250106510662 	 ± 0.23439382944312984
	data : 0.11579232215881348
	model : 0.07051806449890137
			 train-loss:  2.066735735306373 	 ± 0.23449129486822046
	data : 0.11551589965820312
	model : 0.07062559127807617
			 train-loss:  2.0674136435284334 	 ± 0.23396663200305204
	data : 0.11563658714294434
	model : 0.07018280029296875
			 train-loss:  2.0673151817935254 	 ± 0.23328504834689803
	data : 0.11602025032043457
	model : 0.07011003494262695
			 train-loss:  2.0681401636711385 	 ± 0.23285594059109027
	data : 0.11603469848632812
	model : 0.07006220817565918
			 train-loss:  2.0703312351524485 	 ± 0.23395342346948814
	data : 0.11607093811035156
	model : 0.06998143196105958
			 train-loss:  2.07070104623663 	 ± 0.23333087982826214
	data : 0.11623635292053222
	model : 0.06959414482116699
			 train-loss:  2.071042001588004 	 ± 0.2327067306911197
	data : 0.11641087532043456
	model : 0.06869096755981445
			 train-loss:  2.069963064383377 	 ± 0.23248323972098722
	data : 0.11717348098754883
	model : 0.06864767074584961
			 train-loss:  2.0698925014269554 	 ± 0.23182746723689082
	data : 0.11731724739074707
	model : 0.06863617897033691
			 train-loss:  2.0692916231208973 	 ± 0.23131352924729884
	data : 0.11719069480895997
	model : 0.06866445541381835
			 train-loss:  2.067948235479813 	 ± 0.23136176819158394
	data : 0.11722564697265625
	model : 0.06802268028259277
			 train-loss:  2.067497722307841 	 ± 0.23079692098126273
	data : 0.11783804893493652
	model : 0.06907186508178711
			 train-loss:  2.0663786533787762 	 ± 0.23064765651641408
	data : 0.11675333976745605
	model : 0.06935563087463378
			 train-loss:  2.0667707337127936 	 ± 0.23007361313711927
	data : 0.1163867473602295
	model : 0.06932668685913086
			 train-loss:  2.0674500797615676 	 ± 0.2296271028590582
	data : 0.11636624336242676
	model : 0.06913232803344727
			 train-loss:  2.0677488478629487 	 ± 0.2290379288891376
	data : 0.11651453971862794
	model : 0.06990690231323242
			 train-loss:  2.0689712157120574 	 ± 0.2290190904678622
	data : 0.11595826148986817
	model : 0.07023124694824219
			 train-loss:  2.0698230747253663 	 ± 0.22869631332374818
	data : 0.11571750640869141
	model : 0.07005600929260254
			 train-loss:  2.0689340632229567 	 ± 0.22840603621255332
	data : 0.11585450172424316
	model : 0.07024741172790527
			 train-loss:  2.069868896869903 	 ± 0.22815618097582221
	data : 0.11567792892456055
	model : 0.07063460350036621
			 train-loss:  2.07016976422103 	 ± 0.22758918324562433
	data : 0.11528058052062988
	model : 0.07066316604614258
			 train-loss:  2.0687747992967305 	 ± 0.2277981598516996
	data : 0.11527996063232422
	model : 0.07009172439575195
			 train-loss:  2.0712132666123475 	 ± 0.22967385375957747
	data : 0.11578512191772461
	model : 0.07067532539367676
			 train-loss:  2.072849885871013 	 ± 0.230188915722816
	data : 0.11508765220642089
	model : 0.0702286720275879
			 train-loss:  2.073869382779215 	 ± 0.23002598332498414
	data : 0.11545381546020508
	model : 0.06935734748840332
			 train-loss:  2.0732222033530165 	 ± 0.22960846528884762
	data : 0.11617588996887207
	model : 0.06955657005310059
			 train-loss:  2.0717392499630267 	 ± 0.2299485232873793
	data : 0.11593527793884277
	model : 0.06981463432312011
			 train-loss:  2.074072463780033 	 ± 0.23166377384397133
	data : 0.11584029197692872
	model : 0.06924810409545898
			 train-loss:  2.0758962915633536 	 ± 0.23248148604290078
	data : 0.11644325256347657
	model : 0.06856508255004883
			 train-loss:  2.0784471703298166 	 ± 0.2346413199244548
	data : 0.1170572280883789
	model : 0.06903610229492188
			 train-loss:  2.0779610806374094 	 ± 0.23415094920129692
	data : 0.11655139923095703
	model : 0.06891903877258301
			 train-loss:  2.080493162870407 	 ± 0.23628036381237183
	data : 0.1166193962097168
	model : 0.06856942176818848
			 train-loss:  2.0806043492027775 	 ± 0.23569711393697085
	data : 0.11694073677062988
	model : 0.06806492805480957
			 train-loss:  2.0809975543824755 	 ± 0.23517906092706156
	data : 0.11755967140197754
	model : 0.06840343475341797
			 train-loss:  2.083723702454215 	 ± 0.23777714789163773
	data : 0.11744394302368164
	model : 0.06860690116882324
			 train-loss:  2.084240220341028 	 ± 0.23730778254259463
	data : 0.11735067367553711
	model : 0.06812772750854493
			 train-loss:  2.0858735980057137 	 ± 0.23787503573813965
	data : 0.11781229972839355
	model : 0.06840710639953614
			 train-loss:  2.084952662870722 	 ± 0.23766302897671612
	data : 0.11753983497619629
	model : 0.06886124610900879
			 train-loss:  2.084972321123317 	 ± 0.23708843658092643
	data : 0.11715049743652343
	model : 0.06933417320251464
			 train-loss:  2.0847044948201914 	 ± 0.2365492131088655
	data : 0.11659684181213378
	model : 0.06937117576599121
			 train-loss:  2.085017151239386 	 ± 0.23602570464322575
	data : 0.11649665832519532
	model : 0.06975440979003907
			 train-loss:  2.085725140003931 	 ± 0.23568542016484817
	data : 0.11611495018005372
	model : 0.06973090171813964
			 train-loss:  2.0865953539220077 	 ± 0.23546419176574598
	data : 0.11606740951538086
	model : 0.06976814270019531
			 train-loss:  2.085643622110475 	 ± 0.23531464598739435
	data : 0.11617493629455566
	model : 0.06901202201843262
			 train-loss:  2.084088853827105 	 ± 0.2358505545840649
	data : 0.11709580421447754
	model : 0.06821022033691407
			 train-loss:  2.0835771978458513 	 ± 0.23541731795159437
	data : 0.11784400939941406
	model : 0.068475341796875
			 train-loss:  2.0838348948678305 	 ± 0.23489944933850512
	data : 0.11765952110290527
	model : 0.06837997436523438
			 train-loss:  2.0840843833155103 	 ± 0.23438361997291596
	data : 0.1177905559539795
	model : 0.06815929412841797
			 train-loss:  2.0843353035263203 	 ± 0.23387201853913495
	data : 0.11774907112121583
	model : 0.06948766708374024
			 train-loss:  2.0830690817001765 	 ± 0.23407934843001765
	data : 0.11656413078308106
	model : 0.06993098258972168
			 train-loss:  2.083757458212169 	 ± 0.23376536598340875
	data : 0.11620020866394043
	model : 0.06917042732238769
			 train-loss:  2.0825406670570374 	 ± 0.2339275560535765
	data : 0.1167935848236084
	model : 0.06845793724060059
			 train-loss:  2.0843643494860618 	 ± 0.2349599328283547
	data : 0.11752152442932129
	model : 0.06824946403503418
			 train-loss:  2.0838426738171965 	 ± 0.2345583887849199
	data : 0.11766223907470703
	model : 0.06707448959350586
			 train-loss:  2.0823412080516728 	 ± 0.2350986996174783
	data : 0.11861658096313477
	model : 0.06656718254089355
			 train-loss:  2.08262671317373 	 ± 0.2346120812847963
	data : 0.11916952133178711
	model : 0.0666541576385498
			 train-loss:  2.0840668212042917 	 ± 0.23508030539779223
	data : 0.11888008117675782
	model : 0.06711206436157227
			 train-loss:  2.084100037549449 	 ± 0.23456016880911487
	data : 0.11814274787902831
	model : 0.06742415428161622
			 train-loss:  2.0860364374085143 	 ± 0.23584639132819257
	data : 0.11793222427368164
	model : 0.06731944084167481
			 train-loss:  2.0858563153367293 	 ± 0.2353442633107411
	data : 0.11788382530212402
	model : 0.06766710281372071
			 train-loss:  2.0874736871261264 	 ± 0.23609633891071624
	data : 0.11740198135375976
	model : 0.06811838150024414
			 train-loss:  2.088765360998071 	 ± 0.23639203768189299
	data : 0.1173311710357666
	model : 0.06811375617980957
			 train-loss:  2.088379626150255 	 ± 0.23595234170909543
	data : 0.11735997200012208
	model : 0.06793975830078125
			 train-loss:  2.0878890624334074 	 ± 0.23556130057863053
	data : 0.1171950340270996
	model : 0.06852331161499023
			 train-loss:  2.089505055943272 	 ± 0.2363404927198559
	data : 0.11659450531005859
	model : 0.06867709159851074
			 train-loss:  2.0890341869786253 	 ± 0.23594445172613032
	data : 0.11638078689575196
	model : 0.06793351173400879
			 train-loss:  2.0874369717658836 	 ± 0.2367062486863717
	data : 0.11682653427124023
	model : 0.06758837699890137
			 train-loss:  2.086771635685937 	 ± 0.23642432510134623
	data : 0.11721372604370117
	model : 0.06686806678771973
			 train-loss:  2.086380423372808 	 ± 0.23600154779271287
	data : 0.1181058406829834
	model : 0.06681032180786133
			 train-loss:  2.0859953866285434 	 ± 0.2355798096506406
	data : 0.11834206581115722
	model : 0.06679048538208007
			 train-loss:  2.085161501892441 	 ± 0.2354381760135977
	data : 0.11841473579406739
	model : 0.06712956428527832
			 train-loss:  2.0844130595525105 	 ± 0.23523190965940227
	data : 0.11799850463867187
	model : 0.0669020175933838
			 train-loss:  2.0858959498741814 	 ± 0.23586479334306976
	data : 0.11824197769165039
	model : 0.06721539497375488
			 train-loss:  2.0851879395729256 	 ± 0.23563345210546893
	data : 0.11801266670227051
	model : 0.06756486892700195
			 train-loss:  2.086076923849161 	 ± 0.23555441928871446
	data : 0.11756219863891601
	model : 0.06777801513671874
			 train-loss:  2.087816775822249 	 ± 0.23663065082217472
	data : 0.1174440860748291
	model : 0.06813230514526367
			 train-loss:  2.08626001902989 	 ± 0.23739597655013706
	data : 0.11737222671508789
	model : 0.06900234222412109
			 train-loss:  2.0853584035625303 	 ± 0.23733293041168313
	data : 0.116680908203125
	model : 0.06955075263977051
			 train-loss:  2.0857596928291473 	 ± 0.23693562371331958
	data : 0.11608943939208985
	model : 0.06936635971069335
			 train-loss:  2.087459853579921 	 ± 0.23796237078819651
	data : 0.11620254516601562
	model : 0.06922607421875
			 train-loss:  2.0867226703099937 	 ± 0.23776763617122088
	data : 0.1160810947418213
	model : 0.06832432746887207
			 train-loss:  2.087037776470184 	 ± 0.2373437142550095
	data : 0.11666908264160156
	model : 0.06771221160888671
			 train-loss:  2.0873681107365276 	 ± 0.23692802376500446
	data : 0.11692004203796387
	model : 0.06755185127258301
			 train-loss:  2.0883772387391044 	 ± 0.2369973293564341
	data : 0.11710491180419921
	model : 0.06744160652160644
			 train-loss:  2.08941094064901 	 ± 0.23709702498504123
	data : 0.11724972724914551
	model : 0.06686267852783204
			 train-loss:  2.0896317081188593 	 ± 0.23665589184680944
	data : 0.11782546043395996
	model : 0.06685447692871094
			 train-loss:  2.0881807495565976 	 ± 0.23732071346576225
	data : 0.1178095817565918
	model : 0.06659493446350098
			 train-loss:  2.087090671993792 	 ± 0.23749552686215794
	data : 0.11712775230407715
	model : 0.057496166229248045
#epoch  58    val-loss:  2.434024823339362  train-loss:  2.087090671993792  lr:  7.8125e-05
			 train-loss:  2.2133100032806396 	 ± 0.0
	data : 5.556540250778198
	model : 0.07278037071228027
			 train-loss:  2.2270350456237793 	 ± 0.013725042343139648
	data : 2.8450520038604736
	model : 0.07137453556060791
			 train-loss:  2.3475704193115234 	 ± 0.17083072658327275
	data : 1.93519123395284
	model : 0.06947620709737141
			 train-loss:  2.3447402715682983 	 ± 0.14802493731622424
	data : 1.4812074899673462
	model : 0.06857019662857056
			 train-loss:  2.2643665075302124 	 ± 0.20825194701749455
	data : 1.2088640689849854
	model : 0.06798367500305176
			 train-loss:  2.2722255984942117 	 ± 0.19091766397511928
	data : 0.12159228324890137
	model : 0.06729907989501953
			 train-loss:  2.196727820805141 	 ± 0.25581626576728045
	data : 0.11799907684326172
	model : 0.06749982833862304
			 train-loss:  2.128208041191101 	 ± 0.300210656024603
	data : 0.11796531677246094
	model : 0.06842517852783203
			 train-loss:  2.1170389122433133 	 ± 0.2847988498163312
	data : 0.11724114418029785
	model : 0.06880273818969726
			 train-loss:  2.1005282521247866 	 ± 0.27468666412656495
	data : 0.11707506179809571
	model : 0.06961817741394043
			 train-loss:  2.0948534336957065 	 ± 0.262517536479537
	data : 0.11635375022888184
	model : 0.06956992149353028
			 train-loss:  2.1408582826455436 	 ± 0.2940296066870621
	data : 0.11651444435119629
	model : 0.06851181983947754
			 train-loss:  2.1420051959844737 	 ± 0.2825224455745985
	data : 0.11725411415100098
	model : 0.0683225154876709
			 train-loss:  2.1423035945211137 	 ± 0.27224756654846466
	data : 0.11740589141845703
	model : 0.06795215606689453
			 train-loss:  2.1148253281911216 	 ± 0.2823973442022508
	data : 0.11763014793395996
	model : 0.06790180206298828
			 train-loss:  2.0966508612036705 	 ± 0.2823449353465959
	data : 0.11769089698791504
	model : 0.0680006504058838
			 train-loss:  2.0791793570798984 	 ± 0.28268955539293633
	data : 0.11778221130371094
	model : 0.06883597373962402
			 train-loss:  2.0943189329571195 	 ± 0.2817272921194698
	data : 0.11714963912963867
	model : 0.0688873291015625
			 train-loss:  2.1160177557091964 	 ± 0.2892541875298895
	data : 0.11696815490722656
	model : 0.06966943740844726
			 train-loss:  2.119530177116394 	 ± 0.28234551657869605
	data : 0.1162421703338623
	model : 0.06971502304077148
			 train-loss:  2.1376867407844182 	 ± 0.2872560954982454
	data : 0.11626801490783692
	model : 0.06991324424743653
			 train-loss:  2.1155681772665544 	 ± 0.29839433991754166
	data : 0.11583924293518066
	model : 0.0700791358947754
			 train-loss:  2.1250044211097388 	 ± 0.29517258114735273
	data : 0.11584935188293458
	model : 0.07004728317260742
			 train-loss:  2.1296698302030563 	 ± 0.28982268044413845
	data : 0.11606683731079101
	model : 0.0704124927520752
			 train-loss:  2.1434075021743775 	 ± 0.29183328426420296
	data : 0.11577472686767579
	model : 0.07081413269042969
			 train-loss:  2.1648297905921936 	 ± 0.3055550455128429
	data : 0.11527581214904785
	model : 0.07068557739257812
			 train-loss:  2.1962340893568815 	 ± 0.3399234379151944
	data : 0.1154554843902588
	model : 0.0708855152130127
			 train-loss:  2.1857503993170604 	 ± 0.33821404584384185
	data : 0.11504011154174805
	model : 0.07098975181579589
			 train-loss:  2.184277024762384 	 ± 0.3324230509157189
	data : 0.11485528945922852
	model : 0.07031826972961426
			 train-loss:  2.174259086449941 	 ± 0.3312582006638269
	data : 0.11536130905151368
	model : 0.07018928527832032
			 train-loss:  2.17159475434211 	 ± 0.32619812029487727
	data : 0.11550259590148926
	model : 0.07019462585449218
			 train-loss:  2.163820616900921 	 ± 0.32396543698036434
	data : 0.11559066772460938
	model : 0.06974115371704101
			 train-loss:  2.171321254788023 	 ± 0.32182836777300916
	data : 0.11599168777465821
	model : 0.06974349021911622
			 train-loss:  2.155589485869688 	 ± 0.32968824850223233
	data : 0.11592607498168946
	model : 0.06969552040100098
			 train-loss:  2.1574394941329955 	 ± 0.3251232914361387
	data : 0.11603975296020508
	model : 0.06910347938537598
			 train-loss:  2.1620395415359073 	 ± 0.3217289474117743
	data : 0.11649966239929199
	model : 0.06844973564147949
			 train-loss:  2.1537495207142188 	 ± 0.3212258413562102
	data : 0.11714119911193847
	model : 0.0680091381072998
			 train-loss:  2.144491151759499 	 ± 0.3219350286320371
	data : 0.11770381927490234
	model : 0.06815547943115234
			 train-loss:  2.1450273257035475 	 ± 0.3177980422296221
	data : 0.1176116943359375
	model : 0.06763877868652343
			 train-loss:  2.1293992191553115 	 ± 0.32862741305450127
	data : 0.1180109977722168
	model : 0.0673309326171875
			 train-loss:  2.1367144613731197 	 ± 0.32787564586147616
	data : 0.1183701992034912
	model : 0.06705665588378906
			 train-loss:  2.1354410960560752 	 ± 0.32405144204745484
	data : 0.11853699684143067
	model : 0.06748690605163574
			 train-loss:  2.13342756726021 	 ± 0.3205269723613208
	data : 0.11823043823242188
	model : 0.06718235015869141
			 train-loss:  2.132873825051568 	 ± 0.31688449161894844
	data : 0.11844992637634277
	model : 0.06805810928344727
			 train-loss:  2.13540624777476 	 ± 0.3137937196333241
	data : 0.11763038635253906
	model : 0.06778335571289062
			 train-loss:  2.1397173586099045 	 ± 0.31170864260693193
	data : 0.11776299476623535
	model : 0.06891431808471679
			 train-loss:  2.1383072746560927 	 ± 0.30852302808489307
	data : 0.11675186157226562
	model : 0.06904888153076172
			 train-loss:  2.1376929258306823 	 ± 0.3053213824529421
	data : 0.11658344268798829
	model : 0.06927714347839356
			 train-loss:  2.1354975384108874 	 ± 0.3025723401899701
	data : 0.11649036407470703
	model : 0.06941637992858887
			 train-loss:  2.1341713166236875 	 ± 0.2996751657616754
	data : 0.11640839576721192
	model : 0.07053790092468262
			 train-loss:  2.1337437372581634 	 ± 0.2967380322909988
	data : 0.11554994583129882
	model : 0.07018547058105469
			 train-loss:  2.1400090020436506 	 ± 0.29725755621487987
	data : 0.11586995124816894
	model : 0.06986722946166993
			 train-loss:  2.1356763097475158 	 ± 0.29609289373236564
	data : 0.11618833541870117
	model : 0.0697716236114502
			 train-loss:  2.1382930477460227 	 ± 0.29395641256968535
	data : 0.11630806922912598
	model : 0.06957087516784669
			 train-loss:  2.1446191635998813 	 ± 0.2949582018968033
	data : 0.11640877723693847
	model : 0.06972904205322265
			 train-loss:  2.1420660806553706 	 ± 0.2929253593313712
	data : 0.11613478660583496
	model : 0.06991424560546874
			 train-loss:  2.143049802696496 	 ± 0.29043777698526485
	data : 0.11593384742736816
	model : 0.07026610374450684
			 train-loss:  2.141894593321044 	 ± 0.28805518259869334
	data : 0.11566886901855469
	model : 0.06964449882507324
			 train-loss:  2.148875666876971 	 ± 0.2905100195065188
	data : 0.11614418029785156
	model : 0.06877360343933106
			 train-loss:  2.1482618272304537 	 ± 0.28811751316208317
	data : 0.11684637069702149
	model : 0.06827197074890137
			 train-loss:  2.148824095726013 	 ± 0.2857793249669876
	data : 0.11735191345214843
	model : 0.06803407669067382
			 train-loss:  2.1510080810516112 	 ± 0.28397803556217993
	data : 0.11770496368408204
	model : 0.06723322868347167
			 train-loss:  2.1498967446978132 	 ± 0.2818511007267565
	data : 0.11833715438842773
	model : 0.06789255142211914
			 train-loss:  2.144751414656639 	 ± 0.2826069370113942
	data : 0.11785821914672852
	model : 0.06874642372131348
			 train-loss:  2.1495556794680084 	 ± 0.2830461872985792
	data : 0.11724228858947754
	model : 0.0680318832397461
			 train-loss:  2.1497696168494946 	 ± 0.2808990091155585
	data : 0.11785922050476075
	model : 0.06819462776184082
			 train-loss:  2.145721453339306 	 ± 0.2807279133512185
	data : 0.11764492988586425
	model : 0.06909866333007812
			 train-loss:  2.144745143020854 	 ± 0.27877066016464574
	data : 0.11676149368286133
	model : 0.0691957950592041
			 train-loss:  2.1430677572886148 	 ± 0.27708866954405786
	data : 0.11665596961975097
	model : 0.0690622329711914
			 train-loss:  2.1352543132645745 	 ± 0.2826548141612019
	data : 0.1168093204498291
	model : 0.07005786895751953
			 train-loss:  2.132140596147994 	 ± 0.2818637017579475
	data : 0.11597557067871093
	model : 0.07008991241455079
			 train-loss:  2.131923364268409 	 ± 0.27990545591001653
	data : 0.11600990295410156
	model : 0.06989331245422363
			 train-loss:  2.130124653855415 	 ± 0.278400363957244
	data : 0.11608505249023438
	model : 0.0688845157623291
			 train-loss:  2.1259249915947787 	 ± 0.27883128747254177
	data : 0.11715950965881347
	model : 0.06875967979431152
			 train-loss:  2.1263090721766154 	 ± 0.2769858805776715
	data : 0.1172210693359375
	model : 0.0685995101928711
			 train-loss:  2.1291409652484092 	 ± 0.2762483669316704
	data : 0.11730289459228516
	model : 0.06864032745361329
			 train-loss:  2.1229261113451674 	 ± 0.27974548310264635
	data : 0.11716184616088868
	model : 0.06867847442626954
			 train-loss:  2.1252379539685373 	 ± 0.2786857890847341
	data : 0.11733264923095703
	model : 0.06970281600952148
			 train-loss:  2.1233525230914734 	 ± 0.27741653935101496
	data : 0.11618094444274903
	model : 0.07007827758789062
			 train-loss:  2.1230356469750404 	 ± 0.2756916203460936
	data : 0.11587061882019042
	model : 0.07008628845214844
			 train-loss:  2.124062427768001 	 ± 0.2741384098672386
	data : 0.11555461883544922
	model : 0.07000946998596191
			 train-loss:  2.1242001361963228 	 ± 0.27246452544284977
	data : 0.11562776565551758
	model : 0.07004623413085938
			 train-loss:  2.1206602217203163 	 ± 0.27270870627706884
	data : 0.11543049812316894
	model : 0.0690915584564209
			 train-loss:  2.1251279711723328 	 ± 0.2741193645971511
	data : 0.11641254425048828
	model : 0.06820878982543946
			 train-loss:  2.1265218650593476 	 ± 0.2728014227313601
	data : 0.11737074851989746
	model : 0.06731085777282715
			 train-loss:  2.1250132017357406 	 ± 0.27156716621943433
	data : 0.11836748123168946
	model : 0.06650371551513672
			 train-loss:  2.1243815613889145 	 ± 0.27006545622266037
	data : 0.11921772956848145
	model : 0.06669049263000489
			 train-loss:  2.1251684535633433 	 ± 0.2686268977235262
	data : 0.1191070556640625
	model : 0.06784582138061523
			 train-loss:  2.1219869463631276 	 ± 0.2687756573885806
	data : 0.11790547370910645
	model : 0.06914806365966797
			 train-loss:  2.1209812376234267 	 ± 0.2674466347904993
	data : 0.11672391891479492
	model : 0.0702712059020996
			 train-loss:  2.1256256129715467 	 ± 0.26959785564073596
	data : 0.11564512252807617
	model : 0.07010622024536133
			 train-loss:  2.1256939665130945 	 ± 0.2681294394266624
	data : 0.1154249668121338
	model : 0.06986193656921387
			 train-loss:  2.1244182637942735 	 ± 0.2669645508651225
	data : 0.11562399864196778
	model : 0.06877470016479492
			 train-loss:  2.1207350543204773 	 ± 0.2679058046181034
	data : 0.11675014495849609
	model : 0.06863069534301758
			 train-loss:  2.121524971409848 	 ± 0.26660206768971634
	data : 0.1170562744140625
	model : 0.06844210624694824
			 train-loss:  2.126426291962465 	 ± 0.26947812563468015
	data : 0.11741600036621094
	model : 0.06903085708618165
			 train-loss:  2.127930872219125 	 ± 0.26849047916810137
	data : 0.11703295707702636
	model : 0.06881041526794433
			 train-loss:  2.128575154713222 	 ± 0.2671924756848632
	data : 0.11718688011169434
	model : 0.06933856010437012
			 train-loss:  2.125918815834354 	 ± 0.2671370265024774
	data : 0.11658921241760253
	model : 0.06905522346496581
			 train-loss:  2.126020768880844 	 ± 0.26579992111971107
	data : 0.11663308143615722
	model : 0.06883597373962402
			 train-loss:  2.1252334412961904 	 ± 0.2645979696990086
	data : 0.11694235801696777
	model : 0.06924223899841309
			 train-loss:  2.1226400651183783 	 ± 0.2645845388019271
	data : 0.11651897430419922
	model : 0.06857538223266602
			 train-loss:  2.1195228956278087 	 ± 0.2651724462407919
	data : 0.11713533401489258
	model : 0.068865966796875
			 train-loss:  2.1211646634798784 	 ± 0.26441999382657644
	data : 0.11701755523681641
	model : 0.06801152229309082
			 train-loss:  2.1245500882466635 	 ± 0.26541289069359575
	data : 0.11750249862670899
	model : 0.06822552680969238
			 train-loss:  2.1216190074974635 	 ± 0.2658599557891286
	data : 0.11724715232849121
	model : 0.06835751533508301
			 train-loss:  2.1170673058411786 	 ± 0.26873229783566704
	data : 0.1172663688659668
	model : 0.06939916610717774
			 train-loss:  2.113563457021007 	 ± 0.2699296313522046
	data : 0.11638221740722657
	model : 0.06921234130859374
			 train-loss:  2.113808523624315 	 ± 0.2687006388855526
	data : 0.11646819114685059
	model : 0.06911811828613282
			 train-loss:  2.1130168036981063 	 ± 0.26760417191479274
	data : 0.11677742004394531
	model : 0.06872668266296386
			 train-loss:  2.112511333044585 	 ± 0.26644876580995913
	data : 0.11694650650024414
	model : 0.06841659545898438
			 train-loss:  2.1102792099118233 	 ± 0.2662970237242246
	data : 0.11739630699157715
	model : 0.06819429397583007
			 train-loss:  2.1089109034664864 	 ± 0.26551127998185925
	data : 0.11756439208984375
	model : 0.06830334663391113
			 train-loss:  2.1102594398615655 	 ± 0.2647325970208534
	data : 0.11733131408691407
	model : 0.06907825469970703
			 train-loss:  2.108175984672878 	 ± 0.26451611871172503
	data : 0.11665763854980468
	model : 0.06933026313781739
			 train-loss:  2.109119538603158 	 ± 0.26356779347630577
	data : 0.11650724411010742
	model : 0.06970329284667968
			 train-loss:  2.1070079487613125 	 ± 0.2634225891542356
	data : 0.11604852676391601
	model : 0.06979889869689941
			 train-loss:  2.1039545000609703 	 ± 0.26437521154851057
	data : 0.11606035232543946
	model : 0.0696908950805664
			 train-loss:  2.1027388472517 	 ± 0.26359303406696843
	data : 0.11618647575378419
	model : 0.06962084770202637
			 train-loss:  2.1038963854312898 	 ± 0.2627959747415966
	data : 0.11611828804016114
	model : 0.06971087455749511
			 train-loss:  2.104919691716344 	 ± 0.2619477523827028
	data : 0.11620030403137208
	model : 0.06862077713012696
			 train-loss:  2.1052905887853903 	 ± 0.2609038883324726
	data : 0.1171748161315918
	model : 0.06772994995117188
			 train-loss:  2.1039403231163334 	 ± 0.2602688033786372
	data : 0.11802592277526855
	model : 0.06722612380981445
			 train-loss:  2.1053833663463593 	 ± 0.25971078697675315
	data : 0.11850500106811523
	model : 0.06652970314025879
			 train-loss:  2.1063240308761597 	 ± 0.25888185857462714
	data : 0.11908960342407227
	model : 0.06659178733825684
			 train-loss:  2.106049979489947 	 ± 0.2578707067741575
	data : 0.11890902519226074
	model : 0.06748814582824707
			 train-loss:  2.1031921642033136 	 ± 0.2588489062145317
	data : 0.1180816650390625
	model : 0.06845879554748535
			 train-loss:  2.1022428143769503 	 ± 0.25805766387002727
	data : 0.11707921028137207
	model : 0.06836175918579102
			 train-loss:  2.101621629655823 	 ± 0.257151547962273
	data : 0.11738166809082032
	model : 0.06919398307800292
			 train-loss:  2.1023078294900746 	 ± 0.25627912960684995
	data : 0.11672863960266114
	model : 0.06917915344238282
			 train-loss:  2.099502228598558 	 ± 0.25729537004373987
	data : 0.11674141883850098
	model : 0.06843628883361816
			 train-loss:  2.098721340750203 	 ± 0.25647469146836543
	data : 0.11733870506286621
	model : 0.06836462020874023
			 train-loss:  2.10052354084818 	 ± 0.25634627465923177
	data : 0.11751804351806641
	model : 0.06876654624938965
			 train-loss:  2.100324530210068 	 ± 0.2553982799997223
	data : 0.11721773147583008
	model : 0.06897101402282715
			 train-loss:  2.0987374897356386 	 ± 0.255112944462732
	data : 0.11702685356140137
	model : 0.06808490753173828
			 train-loss:  2.0994817781097748 	 ± 0.25432037098261584
	data : 0.11788902282714844
	model : 0.0689199447631836
			 train-loss:  2.1006455621580136 	 ± 0.2537536998446655
	data : 0.11727523803710938
	model : 0.06838250160217285
			 train-loss:  2.100443294946698 	 ± 0.2528437148050516
	data : 0.11767134666442872
	model : 0.06860442161560058
			 train-loss:  2.1008041891262685 	 ± 0.2519682325550692
	data : 0.11738119125366211
	model : 0.06760621070861816
			 train-loss:  2.100797918013164 	 ± 0.25106674419156416
	data : 0.11834130287170411
	model : 0.06758489608764648
			 train-loss:  2.1026262856544333 	 ± 0.25110847601921843
	data : 0.11811656951904297
	model : 0.068355131149292
			 train-loss:  2.1020209411500206 	 ± 0.2503259524374773
	data : 0.11725282669067383
	model : 0.06881632804870605
			 train-loss:  2.10438690402291 	 ± 0.2510373754936
	data : 0.1167829990386963
	model : 0.06876673698425292
			 train-loss:  2.103476806647248 	 ± 0.25040081926741325
	data : 0.1167022705078125
	model : 0.06932315826416016
			 train-loss:  2.1006469356602637 	 ± 0.2518359168492128
	data : 0.11639456748962403
	model : 0.06975383758544922
			 train-loss:  2.1057890318844414 	 ± 0.25849739576968234
	data : 0.11634721755981445
	model : 0.06898012161254882
			 train-loss:  2.105982213604207 	 ± 0.25762722730590026
	data : 0.11703500747680665
	model : 0.06913061141967773
			 train-loss:  2.103190708804775 	 ± 0.25897650005409056
	data : 0.11686954498291016
	model : 0.0685194492340088
			 train-loss:  2.1060864893382027 	 ± 0.2604990630799632
	data : 0.11748318672180176
	model : 0.06874957084655761
			 train-loss:  2.110786867141724 	 ± 0.2658934046304658
	data : 0.11715612411499024
	model : 0.06904268264770508
			 train-loss:  2.1110875685483417 	 ± 0.2650370889596645
	data : 0.11666707992553711
	model : 0.06893744468688964
			 train-loss:  2.111403283319975 	 ± 0.2641923043187307
	data : 0.11688861846923829
	model : 0.06884775161743165
			 train-loss:  2.1102865973329235 	 ± 0.2636871678401821
	data : 0.11717386245727539
	model : 0.06965818405151367
			 train-loss:  2.1103491070982696 	 ± 0.2628307836421689
	data : 0.11642980575561523
	model : 0.06922516822814942
			 train-loss:  2.1113128539054626 	 ± 0.26225441787337833
	data : 0.11684136390686035
	model : 0.06941542625427247
			 train-loss:  2.111602462255038 	 ± 0.26143737179187304
	data : 0.1167259693145752
	model : 0.0694366455078125
			 train-loss:  2.113263277491187 	 ± 0.26142771117554825
	data : 0.11676602363586426
	model : 0.06848068237304687
			 train-loss:  2.1120007596438444 	 ± 0.26107879792961214
	data : 0.11747827529907226
	model : 0.06832160949707031
			 train-loss:  2.110126002779547 	 ± 0.2613212022270732
	data : 0.11737360954284667
	model : 0.06932897567749023
			 train-loss:  2.1110020466148853 	 ± 0.26073739837573595
	data : 0.11645655632019043
	model : 0.06959528923034668
			 train-loss:  2.111143848910835 	 ± 0.2599325823385992
	data : 0.11611051559448242
	model : 0.06952452659606934
			 train-loss:  2.110247751812876 	 ± 0.25937841308858606
	data : 0.11607804298400878
	model : 0.07028069496154785
			 train-loss:  2.1084010761939673 	 ± 0.2596475919637446
	data : 0.1153796672821045
	model : 0.07026739120483398
			 train-loss:  2.106099044404379 	 ± 0.2605179219044533
	data : 0.11553244590759278
	model : 0.06983137130737305
			 train-loss:  2.1081596851348876 	 ± 0.261064436864524
	data : 0.1160036563873291
	model : 0.06952962875366211
			 train-loss:  2.1066079225884864 	 ± 0.2610390480920439
	data : 0.1164130687713623
	model : 0.06961293220520019
			 train-loss:  2.105423127819678 	 ± 0.26070361140661363
	data : 0.11626462936401367
	model : 0.06981201171875
			 train-loss:  2.1050435517515456 	 ± 0.25997282988993414
	data : 0.11617770195007324
	model : 0.06913971900939941
			 train-loss:  2.103528254836269 	 ± 0.259945580523144
	data : 0.11679677963256836
	model : 0.06922755241394044
			 train-loss:  2.10485705277499 	 ± 0.2597549387141971
	data : 0.11665186882019044
	model : 0.06850676536560059
			 train-loss:  2.1046864240490204 	 ± 0.25900386255991964
	data : 0.11736035346984863
	model : 0.06834502220153808
			 train-loss:  2.105929759352706 	 ± 0.2587611433584227
	data : 0.11756463050842285
	model : 0.06748266220092773
			 train-loss:  2.106986652908987 	 ± 0.2583842503959938
	data : 0.11835918426513672
	model : 0.06828184127807617
			 train-loss:  2.105148353110785 	 ± 0.25877278771626955
	data : 0.11782560348510743
	model : 0.06809158325195312
			 train-loss:  2.104344175202506 	 ± 0.2582503322023564
	data : 0.11797785758972168
	model : 0.06896414756774902
			 train-loss:  2.1022452373396265 	 ± 0.25900823303372505
	data : 0.11730442047119141
	model : 0.06927695274353027
			 train-loss:  2.1004520331398915 	 ± 0.25936883832301866
	data : 0.11691608428955078
	model : 0.06999096870422364
			 train-loss:  2.1011586905865185 	 ± 0.2588100619709854
	data : 0.11630029678344726
	model : 0.06905708312988282
			 train-loss:  2.1009027525033366 	 ± 0.2581087042407427
	data : 0.11689643859863282
	model : 0.06896586418151855
			 train-loss:  2.1015394058492447 	 ± 0.25753163910439275
	data : 0.11684832572937012
	model : 0.06878256797790527
			 train-loss:  2.10080391728417 	 ± 0.2570087392594937
	data : 0.11700024604797363
	model : 0.06819639205932618
			 train-loss:  2.099683403968811 	 ± 0.25674465008446445
	data : 0.11756925582885742
	model : 0.06749591827392579
			 train-loss:  2.0996866590989742 	 ± 0.25604220479093376
	data : 0.11813669204711914
	model : 0.06845364570617676
			 train-loss:  2.1003372099088584 	 ± 0.25549709952414756
	data : 0.1173891544342041
	model : 0.06835613250732422
			 train-loss:  2.1011032375129495 	 ± 0.25501741224076696
	data : 0.11755943298339844
	model : 0.06842799186706543
			 train-loss:  2.10097094761428 	 ± 0.25433732254844454
	data : 0.11741065979003906
	model : 0.06890339851379394
			 train-loss:  2.0986813510802977 	 ± 0.2555711451784452
	data : 0.11705822944641113
	model : 0.06973438262939453
			 train-loss:  2.099201905600568 	 ± 0.2549899099386313
	data : 0.11640191078186035
	model : 0.0696742057800293
			 train-loss:  2.1000472799180048 	 ± 0.25457845437615495
	data : 0.11651444435119629
	model : 0.0698592185974121
			 train-loss:  2.1001189175405 	 ± 0.25390953725831866
	data : 0.116241455078125
	model : 0.06995191574096679
			 train-loss:  2.1015318881778815 	 ± 0.2539918225255503
	data : 0.11621623039245606
	model : 0.0700838565826416
			 train-loss:  2.099203227708737 	 ± 0.2553655706869857
	data : 0.11616592407226563
	model : 0.06922965049743653
			 train-loss:  2.097422638087693 	 ± 0.25589534339280134
	data : 0.11676921844482421
	model : 0.0695338249206543
			 train-loss:  2.096685187718303 	 ± 0.2554404984843455
	data : 0.1166579246520996
	model : 0.06961870193481445
			 train-loss:  2.098457258786911 	 ± 0.2559774197437053
	data : 0.11659827232360839
	model : 0.06948795318603515
			 train-loss:  2.099551095646255 	 ± 0.25578007051735274
	data : 0.11671380996704102
	model : 0.06922855377197265
			 train-loss:  2.0994886837634943 	 ± 0.25513155280873917
	data : 0.11675100326538086
	model : 0.0709597110748291
			 train-loss:  2.0970162941951944 	 ± 0.2568415175104995
	data : 0.11518020629882812
	model : 0.07013254165649414
			 train-loss:  2.0993314070917255 	 ± 0.25825820386638193
	data : 0.1157078742980957
	model : 0.06954975128173828
			 train-loss:  2.0978784292936323 	 ± 0.2584258716089329
	data : 0.11625661849975585
	model : 0.0695986270904541
			 train-loss:  2.0974628931254298 	 ± 0.25784919393802314
	data : 0.11617603302001953
	model : 0.06973814964294434
			 train-loss:  2.096163372002026 	 ± 0.25786916636538304
	data : 0.1163975715637207
	model : 0.0689314842224121
			 train-loss:  2.0956491872007623 	 ± 0.257337024029102
	data : 0.11717791557312011
	model : 0.06857562065124512
			 train-loss:  2.094246250741622 	 ± 0.25748257307425
	data : 0.11750674247741699
	model : 0.06894035339355468
			 train-loss:  2.0955956575347154 	 ± 0.2575758867339178
	data : 0.1171837329864502
	model : 0.06893730163574219
			 train-loss:  2.094961120086966 	 ± 0.25711050819707115
	data : 0.11727447509765625
	model : 0.0688164234161377
			 train-loss:  2.0955805594218524 	 ± 0.2566427573849234
	data : 0.11724662780761719
	model : 0.06959915161132812
			 train-loss:  2.096119034748811 	 ± 0.256142274258062
	data : 0.11644740104675293
	model : 0.07052226066589355
			 train-loss:  2.098738652096981 	 ± 0.2583066488109208
	data : 0.11562581062316894
	model : 0.07060027122497559
			 train-loss:  2.0985039904004053 	 ± 0.2577132287660405
	data : 0.11564207077026367
	model : 0.07005000114440918
			 train-loss:  2.099010598603018 	 ± 0.2572066035082031
	data : 0.11621232032775879
	model : 0.07012968063354492
			 train-loss:  2.0977482790092252 	 ± 0.257253575278822
	data : 0.11619300842285156
	model : 0.06860918998718261
			 train-loss:  2.096274500161829 	 ± 0.2575445003940569
	data : 0.11768903732299804
	model : 0.06789517402648926
			 train-loss:  2.0955473084316076 	 ± 0.25716114861355777
	data : 0.11846261024475098
	model : 0.06804475784301758
			 train-loss:  2.0954158505728078 	 ± 0.2565696094567114
	data : 0.11826395988464355
	model : 0.06797056198120117
			 train-loss:  2.0943073519953974 	 ± 0.2564905276175768
	data : 0.11840057373046875
	model : 0.06724781990051269
			 train-loss:  2.0957266592210337 	 ± 0.2567476204103167
	data : 0.11911721229553222
	model : 0.06799120903015136
			 train-loss:  2.094573883288497 	 ± 0.2567203301847758
	data : 0.11836776733398438
	model : 0.06785659790039063
			 train-loss:  2.096240225992246 	 ± 0.257312476427212
	data : 0.11829538345336914
	model : 0.06683669090270997
			 train-loss:  2.095647653124549 	 ± 0.256876736053731
	data : 0.11910138130187989
	model : 0.06714024543762206
			 train-loss:  2.09439235521118 	 ± 0.2569703288651114
	data : 0.11850910186767578
	model : 0.06789665222167969
			 train-loss:  2.095228260164862 	 ± 0.2566918804526826
	data : 0.11783957481384277
	model : 0.0676302433013916
			 train-loss:  2.093418696536077 	 ± 0.25753095145990496
	data : 0.11793508529663085
	model : 0.06843523979187012
			 train-loss:  2.0929636699812755 	 ± 0.2570452911728299
	data : 0.11731061935424805
	model : 0.0687103271484375
			 train-loss:  2.093479799694485 	 ± 0.2565897473829401
	data : 0.11710000038146973
	model : 0.06825580596923828
			 train-loss:  2.093019692243728 	 ± 0.25611444864014643
	data : 0.1173851490020752
	model : 0.06758975982666016
			 train-loss:  2.0931136450578465 	 ± 0.2555536004260827
	data : 0.11778254508972168
	model : 0.06784591674804688
			 train-loss:  2.092197586047022 	 ± 0.25536580803752207
	data : 0.11770949363708497
	model : 0.06779541969299316
			 train-loss:  2.09163640628215 	 ± 0.2549484870825606
	data : 0.11768484115600586
	model : 0.06844239234924317
			 train-loss:  2.093094712236653 	 ± 0.2553490398845847
	data : 0.11728620529174805
	model : 0.06915535926818847
			 train-loss:  2.0918469263877704 	 ± 0.25549749611145967
	data : 0.11679439544677735
	model : 0.06920475959777832
			 train-loss:  2.090609378855804 	 ± 0.25563915518907326
	data : 0.11673574447631836
	model : 0.06914286613464356
			 train-loss:  2.092902335997815 	 ± 0.25746975554350643
	data : 0.11690235137939453
	model : 0.0688565731048584
			 train-loss:  2.0930039363029675 	 ± 0.25692369822102973
	data : 0.1169774055480957
	model : 0.06875925064086914
			 train-loss:  2.0931285066807526 	 ± 0.2563835508734284
	data : 0.11678647994995117
	model : 0.06854467391967774
			 train-loss:  2.092579223846985 	 ± 0.25597831889222
	data : 0.11684379577636719
	model : 0.06886725425720215
			 train-loss:  2.0923652472878307 	 ± 0.25545885942961666
	data : 0.11649508476257324
	model : 0.0681910514831543
			 train-loss:  2.092195383140019 	 ± 0.2549350285423574
	data : 0.11684861183166503
	model : 0.06792225837707519
			 train-loss:  2.0933010094335387 	 ± 0.25497229209026867
	data : 0.11731071472167968
	model : 0.06738481521606446
			 train-loss:  2.0921387751897176 	 ± 0.2550741647961229
	data : 0.11782841682434082
	model : 0.06728854179382324
			 train-loss:  2.0931686998897567 	 ± 0.2550439924256582
	data : 0.11783347129821778
	model : 0.06701021194458008
			 train-loss:  2.0925205369626196 	 ± 0.2547153210241033
	data : 0.11815991401672363
	model : 0.06741700172424317
			 train-loss:  2.093098431948281 	 ± 0.2543495985163024
	data : 0.11795949935913086
	model : 0.0677884578704834
			 train-loss:  2.093004017091188 	 ± 0.2538321221160655
	data : 0.11739907264709473
	model : 0.06840906143188477
			 train-loss:  2.093064168521336 	 ± 0.25331531027514076
	data : 0.11698970794677735
	model : 0.06796579360961914
			 train-loss:  2.0935437083244324 	 ± 0.2529113245332985
	data : 0.11762652397155762
	model : 0.0678187370300293
			 train-loss:  2.0932554402332073 	 ± 0.25243933169090826
	data : 0.11774516105651855
	model : 0.06812262535095215
			 train-loss:  2.0923840783296095 	 ± 0.25230179969995337
	data : 0.11747050285339355
	model : 0.0677882194519043
			 train-loss:  2.0913495299327804 	 ± 0.2523211896875773
	data : 0.11807403564453126
	model : 0.06735429763793946
			 train-loss:  2.0900496816635132 	 ± 0.25265001604897847
	data : 0.11845674514770507
	model : 0.06762995719909667
			 train-loss:  2.090890993635018 	 ± 0.2524968736221816
	data : 0.11830577850341797
	model : 0.06773495674133301
			 train-loss:  2.0916869072687057 	 ± 0.25231068064713463
	data : 0.11810464859008789
	model : 0.06733117103576661
			 train-loss:  2.089577152795 	 ± 0.2540289834682801
	data : 0.11828627586364746
	model : 0.06775627136230469
			 train-loss:  2.088572400292074 	 ± 0.2540316455729375
	data : 0.11769771575927734
	model : 0.06795296669006348
			 train-loss:  2.0899375424665565 	 ± 0.25446486540545715
	data : 0.11744627952575684
	model : 0.06825385093688965
			 train-loss:  2.0870575457811356 	 ± 0.25809784175954914
	data : 0.11598033905029297
	model : 0.05966286659240723
#epoch  59    val-loss:  2.4184371107502987  train-loss:  2.0870575457811356  lr:  3.90625e-05
			 train-loss:  2.2025649547576904 	 ± 0.0
	data : 5.77217960357666
	model : 0.07620882987976074
			 train-loss:  2.043098211288452 	 ± 0.15946674346923828
	data : 2.952797770500183
	model : 0.07278943061828613
			 train-loss:  1.9515592654546101 	 ± 0.1836078758038391
	data : 2.0071433385213218
	model : 0.07049028078715007
			 train-loss:  1.974776417016983 	 ± 0.16401523533778525
	data : 1.53529953956604
	model : 0.07022005319595337
			 train-loss:  2.0429006814956665 	 ± 0.2002110376112509
	data : 1.2515704154968261
	model : 0.06989245414733887
			 train-loss:  2.004085659980774 	 ± 0.20232831189386052
	data : 0.12063231468200683
	model : 0.06861667633056641
			 train-loss:  2.010458367211478 	 ± 0.1879689007164077
	data : 0.11726498603820801
	model : 0.06910829544067383
			 train-loss:  2.028931051492691 	 ± 0.1824950674233183
	data : 0.1169278621673584
	model : 0.06981434822082519
			 train-loss:  2.0179344018300376 	 ± 0.17484669187750293
	data : 0.11627230644226075
	model : 0.06985397338867187
			 train-loss:  2.0055270195007324 	 ± 0.16999916858868971
	data : 0.11624093055725097
	model : 0.07012815475463867
			 train-loss:  2.026148709383878 	 ± 0.1747140842291891
	data : 0.11603202819824218
	model : 0.07017154693603515
			 train-loss:  2.0571866432825723 	 ± 0.19641320485158553
	data : 0.11596722602844238
	model : 0.07009634971618653
			 train-loss:  2.0950518388014574 	 ± 0.22981704385871457
	data : 0.11622490882873535
	model : 0.07014460563659668
			 train-loss:  2.0802501440048218 	 ± 0.22779702245555905
	data : 0.11610493659973145
	model : 0.07028613090515137
			 train-loss:  2.085655434926351 	 ± 0.22100020721610122
	data : 0.11597533226013183
	model : 0.07042808532714843
			 train-loss:  2.1012588888406754 	 ± 0.22235228799375756
	data : 0.11588263511657715
	model : 0.07033352851867676
			 train-loss:  2.0893880198983585 	 ± 0.22087768875505348
	data : 0.11597242355346679
	model : 0.0698439598083496
			 train-loss:  2.105012151930067 	 ± 0.22411268732491457
	data : 0.11628761291503906
	model : 0.06971111297607421
			 train-loss:  2.1002806487836336 	 ± 0.21905699028867975
	data : 0.116459321975708
	model : 0.06950545310974121
			 train-loss:  2.091499853134155 	 ± 0.21691383500629474
	data : 0.11654772758483886
	model : 0.0688448429107666
			 train-loss:  2.102162236259097 	 ± 0.21699029275874468
	data : 0.11722254753112793
	model : 0.06799192428588867
			 train-loss:  2.099109584634954 	 ± 0.21246237755055628
	data : 0.11808462142944336
	model : 0.06808648109436036
			 train-loss:  2.113380836403888 	 ± 0.21830792836745194
	data : 0.11800642013549804
	model : 0.06812372207641601
			 train-loss:  2.1107498904069266 	 ± 0.214083605411251
	data : 0.11800251007080079
	model : 0.06826195716857911
			 train-loss:  2.117068634033203 	 ± 0.2120300804272726
	data : 0.11795687675476074
	model : 0.06876869201660156
			 train-loss:  2.139474318577693 	 ± 0.23617369993694576
	data : 0.11728868484497071
	model : 0.06941208839416504
			 train-loss:  2.1415266460842557 	 ± 0.23199499419263442
	data : 0.11649818420410156
	model : 0.06944088935852051
			 train-loss:  2.1361632304532185 	 ± 0.22951288134630443
	data : 0.11652913093566894
	model : 0.06962170600891113
			 train-loss:  2.1321829557418823 	 ± 0.2265023967424547
	data : 0.116302490234375
	model : 0.06960873603820801
			 train-loss:  2.123214320341746 	 ± 0.22787250985222104
	data : 0.11614537239074707
	model : 0.06936502456665039
			 train-loss:  2.1241320602355467 	 ± 0.22422337017036864
	data : 0.11632628440856933
	model : 0.06995892524719238
			 train-loss:  2.1170343682169914 	 ± 0.22420233089929348
	data : 0.11582007408142089
	model : 0.07014055252075195
			 train-loss:  2.117727113492561 	 ± 0.22081396914360227
	data : 0.11549725532531738
	model : 0.06918802261352539
			 train-loss:  2.141660080236547 	 ± 0.25734547588560075
	data : 0.11649327278137207
	model : 0.06817994117736817
			 train-loss:  2.1582094056265695 	 ± 0.2713787547178767
	data : 0.11741032600402831
	model : 0.06833453178405761
			 train-loss:  2.1513344016340046 	 ± 0.27065659641565804
	data : 0.11713776588439942
	model : 0.0679840087890625
			 train-loss:  2.1607865739513086 	 ± 0.2729313040258574
	data : 0.11739125251770019
	model : 0.06785202026367188
			 train-loss:  2.1600701714816846 	 ± 0.2693514131659754
	data : 0.11759514808654785
	model : 0.06896510124206542
			 train-loss:  2.169893830250471 	 ± 0.2726849555312793
	data : 0.11646156311035157
	model : 0.06904006004333496
			 train-loss:  2.1581180721521376 	 ± 0.27911687251328327
	data : 0.11653647422790528
	model : 0.06879158020019531
			 train-loss:  2.166982758335951 	 ± 0.28133500146966256
	data : 0.1170267105102539
	model : 0.0686793327331543
			 train-loss:  2.1653581034569513 	 ± 0.27816019408635917
	data : 0.11719508171081543
	model : 0.06868019104003906
			 train-loss:  2.1662212277567665 	 ± 0.27496364962370134
	data : 0.11708884239196778
	model : 0.06757073402404785
			 train-loss:  2.1597710360180247 	 ± 0.27509221512634535
	data : 0.1181882381439209
	model : 0.06855063438415528
			 train-loss:  2.165644894705878 	 ± 0.2747947261207214
	data : 0.11749944686889649
	model : 0.06933245658874512
			 train-loss:  2.1548045873641968 	 ± 0.28135142625726545
	data : 0.1168907642364502
	model : 0.06952719688415528
			 train-loss:  2.1535666090376835 	 ± 0.27846884591898813
	data : 0.11669774055480957
	model : 0.0696983814239502
			 train-loss:  2.1491853992144265 	 ± 0.27718503641659975
	data : 0.11668233871459961
	model : 0.07064533233642578
			 train-loss:  2.1524257562598406 	 ± 0.27525905901983433
	data : 0.11572389602661133
	model : 0.07082896232604981
			 train-loss:  2.1508545970916746 	 ± 0.2727144240712045
	data : 0.11549382209777832
	model : 0.07021417617797851
			 train-loss:  2.148769266465131 	 ± 0.2704298247043371
	data : 0.11605596542358398
	model : 0.07049007415771484
			 train-loss:  2.1417740950217614 	 ± 0.27243614572082053
	data : 0.11592750549316407
	model : 0.07114448547363281
			 train-loss:  2.143797123207236 	 ± 0.2702477865780352
	data : 0.11525521278381348
	model : 0.07193903923034668
			 train-loss:  2.1526952628736145 	 ± 0.2754591844397806
	data : 0.11442275047302246
	model : 0.07151527404785156
			 train-loss:  2.146198775551536 	 ± 0.27708699244206414
	data : 0.11468305587768554
	model : 0.07155251502990723
			 train-loss:  2.140705074582781 	 ± 0.2776078550685104
	data : 0.11468243598937988
	model : 0.07124905586242676
			 train-loss:  2.132172364937632 	 ± 0.2824735093411957
	data : 0.11491942405700684
	model : 0.07041015625
			 train-loss:  2.1316288072487404 	 ± 0.28005787371217833
	data : 0.11577882766723632
	model : 0.07076787948608398
			 train-loss:  2.138651724589073 	 ± 0.28277851588212183
	data : 0.11557612419128419
	model : 0.07149066925048828
			 train-loss:  2.136300732692083 	 ± 0.2809929954729067
	data : 0.1150327205657959
	model : 0.07075343132019044
			 train-loss:  2.1347665728115643 	 ± 0.2789335121618291
	data : 0.11560401916503907
	model : 0.07113971710205078
			 train-loss:  2.1356947248981846 	 ± 0.27676985379593994
	data : 0.11525454521179199
	model : 0.07024850845336914
			 train-loss:  2.1454689559482394 	 ± 0.2851470867154984
	data : 0.11594548225402831
	model : 0.06813812255859375
			 train-loss:  2.1468531992286444 	 ± 0.28312387088371016
	data : 0.11781349182128906
	model : 0.06711802482604981
			 train-loss:  2.141364181958712 	 ± 0.2843487028540309
	data : 0.11864280700683594
	model : 0.06746082305908203
			 train-loss:  2.1419456294088652 	 ± 0.28222525876046145
	data : 0.11821107864379883
	model : 0.06653447151184082
			 train-loss:  2.1424528271404664 	 ± 0.2801414871336199
	data : 0.11889939308166504
	model : 0.06652956008911133
			 train-loss:  2.1412560063249924 	 ± 0.27824650153906083
	data : 0.11892156600952149
	model : 0.06758651733398438
			 train-loss:  2.1435257697450942 	 ± 0.27685627071043883
	data : 0.11806249618530273
	model : 0.06791176795959472
			 train-loss:  2.1403791529791696 	 ± 0.2761115460783028
	data : 0.11781654357910157
	model : 0.06866459846496582
			 train-loss:  2.1404193119263986 	 ± 0.27416040916668494
	data : 0.11728239059448242
	model : 0.06860113143920898
			 train-loss:  2.1385559572113886 	 ± 0.2727022275839071
	data : 0.11753787994384765
	model : 0.0698822021484375
			 train-loss:  2.1346965979223382 	 ± 0.27280066125360053
	data : 0.11646695137023926
	model : 0.07003555297851563
			 train-loss:  2.139331672642682 	 ± 0.27382996253311803
	data : 0.11647968292236328
	model : 0.07065353393554688
			 train-loss:  2.134133148193359 	 ± 0.2756499621738752
	data : 0.1157639503479004
	model : 0.07050089836120606
			 train-loss:  2.1365061684658655 	 ± 0.2746005617543857
	data : 0.11589202880859376
	model : 0.0701690673828125
			 train-loss:  2.1346063397147437 	 ± 0.2733138987729494
	data : 0.11612958908081054
	model : 0.06974921226501465
			 train-loss:  2.1313846172430577 	 ± 0.27302382657111585
	data : 0.11621828079223633
	model : 0.06939997673034667
			 train-loss:  2.132161243052422 	 ± 0.2713770175361226
	data : 0.11634869575500488
	model : 0.06821212768554688
			 train-loss:  2.1339069694280624 	 ± 0.27012159180992146
	data : 0.11747794151306153
	model : 0.06811761856079102
			 train-loss:  2.1332025910601202 	 ± 0.2685229142515627
	data : 0.11748442649841309
	model : 0.06903786659240722
			 train-loss:  2.1323329849940973 	 ± 0.26699529026253016
	data : 0.11663012504577637
	model : 0.06911377906799317
			 train-loss:  2.131718724606985 	 ± 0.2654402976308431
	data : 0.11669201850891113
	model : 0.06906952857971191
			 train-loss:  2.1314876022792997 	 ± 0.263863966809305
	data : 0.11663937568664551
	model : 0.06982717514038086
			 train-loss:  2.1296775032492246 	 ± 0.2628313286910851
	data : 0.11600475311279297
	model : 0.06986541748046875
			 train-loss:  2.129289471825888 	 ± 0.26132326009484175
	data : 0.11588754653930664
	model : 0.06991000175476074
			 train-loss:  2.130680503516362 	 ± 0.2601371039529095
	data : 0.11595287322998046
	model : 0.0699005126953125
			 train-loss:  2.12941774725914 	 ± 0.25892285879053567
	data : 0.1161043643951416
	model : 0.06983470916748047
			 train-loss:  2.1295736987939042 	 ± 0.25746828319813864
	data : 0.11616554260253906
	model : 0.06976857185363769
			 train-loss:  2.1291609313752917 	 ± 0.2560635190126137
	data : 0.11619787216186524
	model : 0.07057528495788574
			 train-loss:  2.1327100428906114 	 ± 0.2568689351561394
	data : 0.11541571617126464
	model : 0.07044258117675781
			 train-loss:  2.130664667357569 	 ± 0.2562131184376567
	data : 0.1153602123260498
	model : 0.07046647071838379
			 train-loss:  2.126834338711154 	 ± 0.2574666396167813
	data : 0.115413236618042
	model : 0.07066965103149414
			 train-loss:  2.125253904373088 	 ± 0.25654660492500736
	data : 0.1153226375579834
	model : 0.07073807716369629
			 train-loss:  2.1243316788422435 	 ± 0.2553493798748786
	data : 0.11528382301330567
	model : 0.07002973556518555
			 train-loss:  2.1214797645807266 	 ± 0.255532344569342
	data : 0.11595420837402344
	model : 0.0700721263885498
			 train-loss:  2.1187636028860033 	 ± 0.255600977595028
	data : 0.1160212516784668
	model : 0.07003722190856934
			 train-loss:  2.118392028370682 	 ± 0.2543198786501744
	data : 0.11620087623596191
	model : 0.06991729736328126
			 train-loss:  2.1204546449160335 	 ± 0.2538547053043826
	data : 0.11612634658813477
	model : 0.06985907554626465
			 train-loss:  2.125193327665329 	 ± 0.2569452190580159
	data : 0.1160773754119873
	model : 0.06973118782043457
			 train-loss:  2.1273866183686962 	 ± 0.2566090923352828
	data : 0.11631722450256347
	model : 0.0698620319366455
			 train-loss:  2.124383809519749 	 ± 0.25712518037461213
	data : 0.116141939163208
	model : 0.07003726959228515
			 train-loss:  2.124274415877259 	 ± 0.2558763407344754
	data : 0.11584930419921875
	model : 0.07029929161071777
			 train-loss:  2.1240134491370273 	 ± 0.2546569677299689
	data : 0.11572012901306153
	model : 0.07027688026428222
			 train-loss:  2.1238513446989513 	 ± 0.2534468059208463
	data : 0.11581377983093262
	model : 0.070196533203125
			 train-loss:  2.1233373250601426 	 ± 0.2523034539823286
	data : 0.11585140228271484
	model : 0.07047057151794434
			 train-loss:  2.122735903641888 	 ± 0.25119802620430626
	data : 0.1156686782836914
	model : 0.06995592117309571
			 train-loss:  2.1210971618140184 	 ± 0.25060632625575424
	data : 0.11610164642333984
	model : 0.069757080078125
			 train-loss:  2.118541055863057 	 ± 0.2508644828636393
	data : 0.11633791923522949
	model : 0.0690638542175293
			 train-loss:  2.1171896024183794 	 ± 0.2501198735842753
	data : 0.11696052551269531
	model : 0.06824717521667481
			 train-loss:  2.117770212190645 	 ± 0.2490651116596531
	data : 0.1176614761352539
	model : 0.06732678413391113
			 train-loss:  2.1129046582749913 	 ± 0.25319425584178845
	data : 0.1184450626373291
	model : 0.06771111488342285
			 train-loss:  2.1116123009571988 	 ± 0.25244221253434884
	data : 0.1181380271911621
	model : 0.06777901649475097
			 train-loss:  2.1096185895434596 	 ± 0.25222454937048816
	data : 0.11811437606811523
	model : 0.06852083206176758
			 train-loss:  2.1089093343071315 	 ± 0.2512396803052886
	data : 0.11750154495239258
	model : 0.06953983306884766
			 train-loss:  2.1065343289539733 	 ± 0.25144761275003424
	data : 0.11645936965942383
	model : 0.06997919082641602
			 train-loss:  2.107597671003423 	 ± 0.2506325405961991
	data : 0.11601686477661133
	model : 0.06991400718688964
			 train-loss:  2.1087514707597634 	 ± 0.24988013615520704
	data : 0.11615519523620606
	model : 0.0698366641998291
			 train-loss:  2.107684825648781 	 ± 0.2490976278918081
	data : 0.11613445281982422
	model : 0.06982464790344238
			 train-loss:  2.1089617600043615 	 ± 0.24844835436487608
	data : 0.11599369049072265
	model : 0.06969566345214843
			 train-loss:  2.1098223885228813 	 ± 0.24759913064445432
	data : 0.11630749702453613
	model : 0.06982035636901855
			 train-loss:  2.1074980751412813 	 ± 0.2479042580338335
	data : 0.11615467071533203
	model : 0.07002911567687989
			 train-loss:  2.1066425389390653 	 ± 0.24707523466641326
	data : 0.11591897010803223
	model : 0.06918768882751465
			 train-loss:  2.109207808971405 	 ± 0.24771612547787755
	data : 0.11671977043151856
	model : 0.06917552947998047
			 train-loss:  2.1070728998184203 	 ± 0.24786598070437801
	data : 0.11668787002563477
	model : 0.0692896842956543
			 train-loss:  2.108781384097205 	 ± 0.24761827405957917
	data : 0.11655745506286622
	model : 0.06832418441772461
			 train-loss:  2.108608954534756 	 ± 0.2466490666872507
	data : 0.11745367050170899
	model : 0.067279052734375
			 train-loss:  2.1109925536438823 	 ± 0.2471478077686674
	data : 0.11840052604675293
	model : 0.06799840927124023
			 train-loss:  2.1088134133538534 	 ± 0.2474194054942345
	data : 0.11782622337341309
	model : 0.06785902976989747
			 train-loss:  2.1073731899261476 	 ± 0.24700818718305464
	data : 0.11816368103027344
	model : 0.0674046516418457
			 train-loss:  2.1059500088218512 	 ± 0.24659806139368137
	data : 0.11852221488952637
	model : 0.06768531799316406
			 train-loss:  2.1052912627205704 	 ± 0.2457778762864301
	data : 0.11839871406555176
	model : 0.06868119239807129
			 train-loss:  2.102478639523786 	 ± 0.2469753234099454
	data : 0.11742582321166992
	model : 0.06813650131225586
			 train-loss:  2.103330728723042 	 ± 0.2462481994746305
	data : 0.11800246238708496
	model : 0.06818094253540039
			 train-loss:  2.105058300053632 	 ± 0.24614818186291948
	data : 0.11781949996948242
	model : 0.06777138710021972
			 train-loss:  2.107978839208098 	 ± 0.2475780881331668
	data : 0.1182701587677002
	model : 0.06874613761901856
			 train-loss:  2.108508643442697 	 ± 0.24675022959902826
	data : 0.11721634864807129
	model : 0.06872806549072266
			 train-loss:  2.104896793330925 	 ± 0.24946282978901474
	data : 0.11717000007629394
	model : 0.06950345039367675
			 train-loss:  2.1073277536913646 	 ± 0.2501989483819277
	data : 0.1166189193725586
	model : 0.06887898445129395
			 train-loss:  2.107139063732965 	 ± 0.24931370465639596
	data : 0.11713948249816894
	model : 0.06953191757202148
			 train-loss:  2.1059849110055477 	 ± 0.24880309652754357
	data : 0.11645655632019043
	model : 0.0692030906677246
			 train-loss:  2.10210953967672 	 ± 0.2521599755530441
	data : 0.11692099571228028
	model : 0.06823539733886719
			 train-loss:  2.100471759175921 	 ± 0.25203352057119244
	data : 0.11782727241516114
	model : 0.06818509101867676
			 train-loss:  2.099560000002384 	 ± 0.25139342622024363
	data : 0.11772866249084472
	model : 0.06892805099487305
			 train-loss:  2.0996476346048816 	 ± 0.2505272596944054
	data : 0.11719765663146972
	model : 0.06907095909118652
			 train-loss:  2.098239238948038 	 ± 0.2502431556056925
	data : 0.11716175079345703
	model : 0.06893177032470703
			 train-loss:  2.101200123222507 	 ± 0.2519436393474698
	data : 0.11700954437255859
	model : 0.06921720504760742
			 train-loss:  2.10244681545206 	 ± 0.25154558580645636
	data : 0.11657567024230957
	model : 0.06932868957519531
			 train-loss:  2.100821905488136 	 ± 0.25147820041368907
	data : 0.11657223701477051
	model : 0.06922931671142578
			 train-loss:  2.0983550786972045 	 ± 0.25244083790623917
	data : 0.11662240028381347
	model : 0.0691260814666748
			 train-loss:  2.097997826456234 	 ± 0.25164159440157435
	data : 0.1166109561920166
	model : 0.07046828269958497
			 train-loss:  2.0973340680724695 	 ± 0.2509450477010687
	data : 0.11546816825866699
	model : 0.07105722427368164
			 train-loss:  2.096536302099041 	 ± 0.2503169260127696
	data : 0.1149993896484375
	model : 0.07108392715454101
			 train-loss:  2.0977195524550103 	 ± 0.24993179492574735
	data : 0.1150538444519043
	model : 0.07097172737121582
			 train-loss:  2.0959253503430273 	 ± 0.25011726674178547
	data : 0.11511750221252441
	model : 0.07103443145751953
			 train-loss:  2.0937555027313723 	 ± 0.2507736173236894
	data : 0.1151402473449707
	model : 0.06959543228149415
			 train-loss:  2.093186728513924 	 ± 0.2500746229696735
	data : 0.11675286293029785
	model : 0.06872520446777344
			 train-loss:  2.0899272395085684 	 ± 0.2526054659260686
	data : 0.11762065887451172
	model : 0.06867504119873047
			 train-loss:  2.0889998951797963 	 ± 0.25207950836317866
	data : 0.11763529777526856
	model : 0.06891379356384278
			 train-loss:  2.0868771843612195 	 ± 0.25271202254668723
	data : 0.1173011302947998
	model : 0.06887001991271972
			 train-loss:  2.0849469752045153 	 ± 0.2531063267683022
	data : 0.11743292808532715
	model : 0.06919794082641602
			 train-loss:  2.0841241236086243 	 ± 0.25253984603207796
	data : 0.1170508861541748
	model : 0.07009220123291016
			 train-loss:  2.0825569402952135 	 ± 0.25255294500340747
	data : 0.1163672924041748
	model : 0.0700592041015625
			 train-loss:  2.0830491003466816 	 ± 0.25186018237532254
	data : 0.11616683006286621
	model : 0.0700444221496582
			 train-loss:  2.083371801809831 	 ± 0.25112981511978544
	data : 0.11625833511352539
	model : 0.07006673812866211
			 train-loss:  2.08436175785869 	 ± 0.25069497284995684
	data : 0.11601996421813965
	model : 0.06999616622924805
			 train-loss:  2.084238765482417 	 ± 0.2499482853886913
	data : 0.11597561836242676
	model : 0.06981892585754394
			 train-loss:  2.086955800652504 	 ± 0.25166469008409603
	data : 0.11598739624023438
	model : 0.0695610523223877
			 train-loss:  2.085965612936302 	 ± 0.25124703278162336
	data : 0.11628575325012207
	model : 0.06922364234924316
			 train-loss:  2.0854542080093834 	 ± 0.2505951854017876
	data : 0.11674766540527344
	model : 0.06930327415466309
			 train-loss:  2.0855811597311007 	 ± 0.2498668592200024
	data : 0.11678123474121094
	model : 0.06853814125061035
			 train-loss:  2.0860395466172417 	 ± 0.24921154156143843
	data : 0.11754980087280273
	model : 0.06858291625976562
			 train-loss:  2.0850439140562376 	 ± 0.2488330702320263
	data : 0.11747050285339355
	model : 0.06883630752563477
			 train-loss:  2.0867173507295806 	 ± 0.2490913775727641
	data : 0.11744604110717774
	model : 0.06924214363098144
			 train-loss:  2.0857963500704084 	 ± 0.248675605992261
	data : 0.11719679832458496
	model : 0.0692378044128418
			 train-loss:  2.0857593234289777 	 ± 0.24796861861199704
	data : 0.1171335220336914
	model : 0.0700800895690918
			 train-loss:  2.087270372331479 	 ± 0.2480784126639933
	data : 0.11631698608398437
	model : 0.07094173431396485
			 train-loss:  2.0891194028800792 	 ± 0.2486006837880297
	data : 0.11558194160461426
	model : 0.0709451675415039
			 train-loss:  2.08872727242262 	 ± 0.24796049309276477
	data : 0.11533260345458984
	model : 0.07090439796447753
			 train-loss:  2.0878214690420362 	 ± 0.24756755033430852
	data : 0.1153043270111084
	model : 0.07104434967041015
			 train-loss:  2.087198082913351 	 ± 0.24702434038684262
	data : 0.11521944999694825
	model : 0.07105917930603027
			 train-loss:  2.0889598032930397 	 ± 0.24748233595922486
	data : 0.11519942283630372
	model : 0.07008194923400879
			 train-loss:  2.088065888712315 	 ± 0.2470996842164791
	data : 0.11624383926391602
	model : 0.06986055374145508
			 train-loss:  2.0883426782877548 	 ± 0.24645574792464084
	data : 0.11651263236999512
	model : 0.07017645835876465
			 train-loss:  2.0875985119793867 	 ± 0.2459959454314342
	data : 0.11605992317199706
	model : 0.07004189491271973
			 train-loss:  2.087521210793526 	 ± 0.24533602776082836
	data : 0.11625590324401855
	model : 0.06983952522277832
			 train-loss:  2.087641617831062 	 ± 0.2446846801686809
	data : 0.11639628410339356
	model : 0.07004914283752442
			 train-loss:  2.0865609461956836 	 ± 0.24448010201455858
	data : 0.1162628173828125
	model : 0.07026448249816894
			 train-loss:  2.086091226370877 	 ± 0.2439175144268924
	data : 0.11603851318359375
	model : 0.06980328559875489
			 train-loss:  2.0838412912268387 	 ± 0.2452333085827544
	data : 0.11660256385803222
	model : 0.06981124877929687
			 train-loss:  2.0845993676110712 	 ± 0.24481360043965764
	data : 0.11656103134155274
	model : 0.07048959732055664
			 train-loss:  2.0848632343113422 	 ± 0.24420246267088827
	data : 0.11605143547058105
	model : 0.07034549713134766
			 train-loss:  2.0854266072801977 	 ± 0.24369405544545342
	data : 0.11620392799377441
	model : 0.07030229568481446
			 train-loss:  2.0846116499802503 	 ± 0.24332870189079375
	data : 0.11614427566528321
	model : 0.07038202285766601
			 train-loss:  2.0838700049962755 	 ± 0.24292371072349728
	data : 0.11607184410095214
	model : 0.07041468620300292
			 train-loss:  2.0826670855891947 	 ± 0.2428847790714143
	data : 0.11602497100830078
	model : 0.07002029418945313
			 train-loss:  2.083008738338645 	 ± 0.24231474859133217
	data : 0.1163334846496582
	model : 0.07011442184448242
			 train-loss:  2.0827774471706815 	 ± 0.24172386798862652
	data : 0.11609225273132324
	model : 0.07008490562438965
			 train-loss:  2.08304011162801 	 ± 0.2411440827608787
	data : 0.11604413986206055
	model : 0.07019391059875488
			 train-loss:  2.0849380838871 	 ± 0.24202597742368795
	data : 0.11589345932006836
	model : 0.06994638442993165
			 train-loss:  2.0864568776752224 	 ± 0.24237676233917313
	data : 0.11602745056152344
	model : 0.06982369422912597
			 train-loss:  2.0858700151490694 	 ± 0.24191919440647527
	data : 0.11626086235046387
	model : 0.06982827186584473
			 train-loss:  2.0858223408901044 	 ± 0.2413235499315815
	data : 0.11619148254394532
	model : 0.06888227462768555
			 train-loss:  2.0867004517246697 	 ± 0.24105623607689924
	data : 0.11699504852294922
	model : 0.06884937286376953
			 train-loss:  2.0865374315075758 	 ± 0.24047884768136082
	data : 0.11701512336730957
	model : 0.0689894676208496
			 train-loss:  2.08743375771254 	 ± 0.24023747583062205
	data : 0.1170166015625
	model : 0.06801137924194336
			 train-loss:  2.085343659212048 	 ± 0.2415266991487168
	data : 0.11765761375427246
	model : 0.06789836883544922
			 train-loss:  2.0851137328606386 	 ± 0.24096811460306386
	data : 0.11795535087585449
	model : 0.06879720687866211
			 train-loss:  2.086165928954713 	 ± 0.24086943874377906
	data : 0.11722068786621094
	model : 0.06875400543212891
			 train-loss:  2.08436434893381 	 ± 0.24170262565340922
	data : 0.11733121871948242
	model : 0.06883106231689454
			 train-loss:  2.0845681677497394 	 ± 0.24114727926756982
	data : 0.11717066764831544
	model : 0.0690537929534912
			 train-loss:  2.085299355241488 	 ± 0.2408122014745712
	data : 0.11713294982910157
	model : 0.06943798065185547
			 train-loss:  2.084356253135932 	 ± 0.2406383636265564
	data : 0.11668848991394043
	model : 0.06894826889038086
			 train-loss:  2.083632117119905 	 ± 0.240307970872832
	data : 0.11732993125915528
	model : 0.06874537467956543
			 train-loss:  2.0864400176114812 	 ± 0.24324178750937864
	data : 0.11742467880249023
	model : 0.06899585723876953
			 train-loss:  2.0868629813194275 	 ± 0.2427573089163937
	data : 0.11733489036560059
	model : 0.06941652297973633
			 train-loss:  2.087389423001197 	 ± 0.2423208647938885
	data : 0.11682829856872559
	model : 0.06918559074401856
			 train-loss:  2.08651774848273 	 ± 0.24210519745514064
	data : 0.1169933795928955
	model : 0.06913318634033203
			 train-loss:  2.0865665951820267 	 ± 0.24155289010064465
	data : 0.1168905258178711
	model : 0.06879372596740722
			 train-loss:  2.088258823481473 	 ± 0.24230088292927063
	data : 0.11723637580871582
	model : 0.06834521293640136
			 train-loss:  2.086735386114854 	 ± 0.24280579183067943
	data : 0.11752543449401856
	model : 0.06827282905578613
			 train-loss:  2.0875305297138453 	 ± 0.24254652963374404
	data : 0.11769161224365235
	model : 0.06775002479553223
			 train-loss:  2.0860183623874136 	 ± 0.2430486555797233
	data : 0.11809911727905273
	model : 0.06814398765563964
			 train-loss:  2.084951572652374 	 ± 0.24302821809764735
	data : 0.11772165298461915
	model : 0.06790885925292969
			 train-loss:  2.0839905230204265 	 ± 0.2429137787561313
	data : 0.11773118972778321
	model : 0.06730022430419921
			 train-loss:  2.082886341398796 	 ± 0.24294101005490956
	data : 0.1181570053100586
	model : 0.06700167655944825
			 train-loss:  2.08181891829957 	 ± 0.24293586705540954
	data : 0.1183708667755127
	model : 0.06729774475097657
			 train-loss:  2.082613389220154 	 ± 0.2426978865259495
	data : 0.11795282363891602
	model : 0.06707897186279296
			 train-loss:  2.0819884647969076 	 ± 0.24235117073971502
	data : 0.11824593544006348
	model : 0.06758708953857422
			 train-loss:  2.082227951547374 	 ± 0.24185090116501345
	data : 0.11787147521972656
	model : 0.06816773414611817
			 train-loss:  2.0810910509778306 	 ± 0.241942000806567
	data : 0.11727442741394042
	model : 0.06852450370788574
			 train-loss:  2.081741395695456 	 ± 0.24162227287601723
	data : 0.11693639755249023
	model : 0.06787877082824707
			 train-loss:  2.0809659497420676 	 ± 0.24139234566116244
	data : 0.11758661270141602
	model : 0.0676569938659668
			 train-loss:  2.0819922165992932 	 ± 0.24138485336191642
	data : 0.11773366928100586
	model : 0.06732301712036133
			 train-loss:  2.0818714933192477 	 ± 0.24087780012655913
	data : 0.11831536293029785
	model : 0.06688861846923828
			 train-loss:  2.0832441560292647 	 ± 0.2412862322246672
	data : 0.11883516311645508
	model : 0.06641569137573242
			 train-loss:  2.0851439997113705 	 ± 0.24253910138062004
	data : 0.11925773620605469
	model : 0.06684408187866211
			 train-loss:  2.086427899969726 	 ± 0.24283476212575733
	data : 0.11880793571472167
	model : 0.066871976852417
			 train-loss:  2.0856126362309797 	 ± 0.24265238207137627
	data : 0.11862506866455078
	model : 0.0664907455444336
			 train-loss:  2.0857832739750544 	 ± 0.24216069761217454
	data : 0.11862339973449706
	model : 0.06621017456054687
			 train-loss:  2.085163875734163 	 ± 0.2418482033964016
	data : 0.11892070770263671
	model : 0.06677470207214356
			 train-loss:  2.087225657356672 	 ± 0.24346115574698446
	data : 0.1184006690979004
	model : 0.06616692543029785
			 train-loss:  2.0876359110506475 	 ± 0.24304349760816096
	data : 0.118780517578125
	model : 0.06573696136474609
			 train-loss:  2.088023197944047 	 ± 0.24262007095724758
	data : 0.11904587745666503
	model : 0.06606035232543946
			 train-loss:  2.088765345300947 	 ± 0.2424017874303038
	data : 0.11873826980590821
	model : 0.066302490234375
			 train-loss:  2.088802315839907 	 ± 0.24190929129532168
	data : 0.11849174499511719
	model : 0.0659451961517334
			 train-loss:  2.088239903392097 	 ± 0.24158020080591708
	data : 0.11876125335693359
	model : 0.06627821922302246
			 train-loss:  2.086366862539322 	 ± 0.2428831251816039
	data : 0.11880106925964355
	model : 0.06700873374938965
			 train-loss:  2.086339238657051 	 ± 0.24239530776293236
	data : 0.11861867904663086
	model : 0.06686220169067383
			 train-loss:  2.0872171006202698 	 ± 0.24230632069670557
	data : 0.11880154609680176
	model : 0.06725249290466309
			 train-loss:  2.086666405438427 	 ± 0.24197986608028224
	data : 0.11843891143798828
	model : 0.06698741912841796
			 train-loss:  2.0869149669768317 	 ± 0.24153137461532218
	data : 0.11878256797790528
	model : 0.0673905849456787
			 train-loss:  2.0875975482548648 	 ± 0.24129698180891798
	data : 0.11822752952575684
	model : 0.0675088882446289
			 train-loss:  2.0862741263832634 	 ± 0.24173977716447412
	data : 0.11814332008361816
	model : 0.06826725006103515
			 train-loss:  2.08813525835673 	 ± 0.24308179502089694
	data : 0.11751823425292969
	model : 0.06757516860961914
			 train-loss:  2.0874192393384874 	 ± 0.24287584859400008
	data : 0.11687626838684081
	model : 0.059346771240234374
#epoch  60    val-loss:  2.4945637803328666  train-loss:  2.0874192393384874  lr:  3.90625e-05
			 train-loss:  1.9273966550827026 	 ± 0.0
	data : 5.541266441345215
	model : 0.07237553596496582
			 train-loss:  2.082900583744049 	 ± 0.15550392866134644
	data : 2.839142918586731
	model : 0.07102775573730469
			 train-loss:  1.982682466506958 	 ± 0.1902848477007652
	data : 1.9313866297403972
	model : 0.07057309150695801
			 train-loss:  1.9559333324432373 	 ± 0.17118057999625502
	data : 1.4776235818862915
	model : 0.06992357969284058
			 train-loss:  2.086025094985962 	 ± 0.30189021100422253
	data : 1.2056127071380616
	model : 0.06960558891296387
			 train-loss:  2.065426786740621 	 ± 0.2794092597421604
	data : 0.12059850692749023
	model : 0.06888446807861329
			 train-loss:  2.109276839665004 	 ± 0.28009588830555954
	data : 0.11654896736145019
	model : 0.06886096000671386
			 train-loss:  2.0900674760341644 	 ± 0.26688947313950623
	data : 0.11654849052429199
	model : 0.06807460784912109
			 train-loss:  2.0833831363254123 	 ± 0.2523350760816282
	data : 0.11709728240966796
	model : 0.06856155395507812
			 train-loss:  2.0639573454856874 	 ± 0.24637764449468158
	data : 0.11671981811523438
	model : 0.06883554458618164
			 train-loss:  2.0648582957007666 	 ± 0.23492914314927593
	data : 0.11671719551086426
	model : 0.06899876594543457
			 train-loss:  2.1176675061384835 	 ± 0.28507776755367575
	data : 0.11650781631469727
	model : 0.0688161849975586
			 train-loss:  2.1002619174810557 	 ± 0.2804519576596181
	data : 0.11683316230773926
	model : 0.06878228187561035
			 train-loss:  2.07361330304827 	 ± 0.28682248887378575
	data : 0.1170802116394043
	model : 0.06922097206115722
			 train-loss:  2.058590881029765 	 ± 0.282740319731254
	data : 0.11668930053710938
	model : 0.06933021545410156
			 train-loss:  2.040356181561947 	 ± 0.28272472804275206
	data : 0.11669373512268066
	model : 0.06948909759521485
			 train-loss:  2.040053767316482 	 ± 0.2742859419463568
	data : 0.11660351753234863
	model : 0.06984367370605468
			 train-loss:  2.060459699895647 	 ± 0.2795210450210609
	data : 0.11631054878234863
	model : 0.07050099372863769
			 train-loss:  2.075092547818234 	 ± 0.2790590754737795
	data : 0.11569128036499024
	model : 0.06955180168151856
			 train-loss:  2.074473124742508 	 ± 0.27200654263832447
	data : 0.11655206680297851
	model : 0.06932559013366699
			 train-loss:  2.0718926872525896 	 ± 0.2657019293950801
	data : 0.11657676696777344
	model : 0.06925721168518066
			 train-loss:  2.0649013844403354 	 ± 0.2615625774686943
	data : 0.11676454544067383
	model : 0.06912989616394043
			 train-loss:  2.0680420709692915 	 ± 0.2562370456461114
	data : 0.11676011085510254
	model : 0.06945867538452148
			 train-loss:  2.0606139401594796 	 ± 0.25335897755606934
	data : 0.1164177417755127
	model : 0.06985559463500976
			 train-loss:  2.0655417442321777 	 ± 0.24941118403034807
	data : 0.11610264778137207
	model : 0.06994619369506835
			 train-loss:  2.06950397674854 	 ± 0.24536887491625745
	data : 0.1161740779876709
	model : 0.06894712448120117
			 train-loss:  2.0651318629582724 	 ± 0.2418119869799326
	data : 0.11692843437194825
	model : 0.06869897842407227
			 train-loss:  2.0749578859124864 	 ± 0.24288183120009219
	data : 0.11731095314025879
	model : 0.06763057708740235
			 train-loss:  2.063466285837108 	 ± 0.246282325395242
	data : 0.1181783676147461
	model : 0.0669870376586914
			 train-loss:  2.066293891270955 	 ± 0.2426211372302116
	data : 0.11870818138122559
	model : 0.06751737594604493
			 train-loss:  2.0727810167497203 	 ± 0.24130608912516005
	data : 0.11819744110107422
	model : 0.06856760978698731
			 train-loss:  2.0806974172592163 	 ± 0.24156103847978327
	data : 0.11722407341003419
	model : 0.06812009811401368
			 train-loss:  2.0705111532500298 	 ± 0.24475256912018292
	data : 0.1174778938293457
	model : 0.06828899383544922
			 train-loss:  2.0710050218245564 	 ± 0.24114309430917952
	data : 0.11760177612304687
	model : 0.06883869171142579
			 train-loss:  2.0688339267458233 	 ± 0.23801014226846162
	data : 0.11709775924682617
	model : 0.06819019317626954
			 train-loss:  2.0717481010489993 	 ± 0.23531358585396056
	data : 0.1176833152770996
	model : 0.06716451644897461
			 train-loss:  2.0739094018936157 	 ± 0.23247385668618475
	data : 0.11872482299804688
	model : 0.06790719032287598
			 train-loss:  2.083905147878747 	 ± 0.2373156837210768
	data : 0.1180962085723877
	model : 0.06875486373901367
			 train-loss:  2.0826386396701517 	 ± 0.2343834833765327
	data : 0.11732892990112305
	model : 0.06858525276184083
			 train-loss:  2.0923233419656753 	 ± 0.23920738226753827
	data : 0.11743865013122559
	model : 0.06880021095275879
			 train-loss:  2.092243790626526 	 ± 0.2362727468906328
	data : 0.11710848808288574
	model : 0.06895647048950196
			 train-loss:  2.1000345122246515 	 ± 0.2387135476302202
	data : 0.11702709197998047
	model : 0.06887588500976563
			 train-loss:  2.1033699152081513 	 ± 0.23690966885759265
	data : 0.11694364547729492
	model : 0.06871027946472168
			 train-loss:  2.1099584075537594 	 ± 0.2381536266643851
	data : 0.11680464744567871
	model : 0.06903619766235351
			 train-loss:  2.1022431400087145 	 ± 0.24098938634198946
	data : 0.11668496131896973
	model : 0.06909151077270508
			 train-loss:  2.0995830219724905 	 ± 0.23902258337536547
	data : 0.11692214012145996
	model : 0.06989531517028809
			 train-loss:  2.100469911352117 	 ± 0.23654261264477455
	data : 0.11602468490600586
	model : 0.06923389434814453
			 train-loss:  2.09784634411335 	 ± 0.23475569934763346
	data : 0.11683878898620606
	model : 0.06990456581115723
			 train-loss:  2.0921608355580545 	 ± 0.23566319185519272
	data : 0.11638751029968261
	model : 0.07009477615356445
			 train-loss:  2.0949555945396425 	 ± 0.2341134776256228
	data : 0.11634683609008789
	model : 0.06986498832702637
			 train-loss:  2.0918950006073596 	 ± 0.2328149340817041
	data : 0.11639790534973145
	model : 0.06976866722106934
			 train-loss:  2.0891304405835958 	 ± 0.23140919280740807
	data : 0.11665034294128418
	model : 0.06956977844238281
			 train-loss:  2.0897690147723793 	 ± 0.2292619408308677
	data : 0.11667370796203613
	model : 0.06812047958374023
			 train-loss:  2.0824723133334406 	 ± 0.23325844704341725
	data : 0.1180037498474121
	model : 0.06773371696472168
			 train-loss:  2.0766444726423785 	 ± 0.23506229108168344
	data : 0.11822190284729003
	model : 0.0678858757019043
			 train-loss:  2.0790361762046814 	 ± 0.2336283600742682
	data : 0.11820340156555176
	model : 0.06733403205871583
			 train-loss:  2.079593148147851 	 ± 0.2316074268716133
	data : 0.11853094100952148
	model : 0.06821703910827637
			 train-loss:  2.080597228017347 	 ± 0.22972723861982788
	data : 0.1177762508392334
	model : 0.06919455528259277
			 train-loss:  2.078820988283319 	 ± 0.22817342296580692
	data : 0.11677160263061523
	model : 0.06929621696472169
			 train-loss:  2.077559705575307 	 ± 0.2264713042357384
	data : 0.11661148071289062
	model : 0.06928009986877441
			 train-loss:  2.074019977303802 	 ± 0.22627466674913232
	data : 0.11663479804992676
	model : 0.07010102272033691
			 train-loss:  2.073400295549823 	 ± 0.2244946303786799
	data : 0.11597743034362792
	model : 0.07039551734924317
			 train-loss:  2.073206460665143 	 ± 0.22271102988184166
	data : 0.1157153606414795
	model : 0.07052035331726074
			 train-loss:  2.06816565990448 	 ± 0.224557362420482
	data : 0.1156855583190918
	model : 0.07039380073547363
			 train-loss:  2.071834292778602 	 ± 0.2247478403938466
	data : 0.1158203125
	model : 0.07030129432678223
			 train-loss:  2.070409641121373 	 ± 0.2233342578203854
	data : 0.11591181755065919
	model : 0.06994047164916992
			 train-loss:  2.071734944386269 	 ± 0.22192265410628637
	data : 0.11619677543640136
	model : 0.0696115493774414
			 train-loss:  2.0660300938522114 	 ± 0.2251797971603485
	data : 0.11628122329711914
	model : 0.06852550506591797
			 train-loss:  2.0651082906170166 	 ± 0.22367130656438328
	data : 0.11729164123535156
	model : 0.06875319480895996
			 train-loss:  2.066639367171696 	 ± 0.22243179844282737
	data : 0.11702122688293456
	model : 0.0689784049987793
			 train-loss:  2.073455872670026 	 ± 0.22810434790094125
	data : 0.11666440963745117
	model : 0.06919565200805664
			 train-loss:  2.0728395564688578 	 ± 0.22657427380859638
	data : 0.11668558120727539
	model : 0.06877384185791016
			 train-loss:  2.0782649566049445 	 ± 0.22967800857205198
	data : 0.11729135513305664
	model : 0.06959881782531738
			 train-loss:  2.0820780847523666 	 ± 0.23043554196704616
	data : 0.11654038429260254
	model : 0.06944079399108886
			 train-loss:  2.0827647733688353 	 ± 0.2289703602388327
	data : 0.11665406227111816
	model : 0.06930270195007324
			 train-loss:  2.0838031125696084 	 ± 0.22763666760694984
	data : 0.11679463386535645
	model : 0.06942257881164551
			 train-loss:  2.0835679986260156 	 ± 0.22616296490684337
	data : 0.11664786338806152
	model : 0.0698272705078125
			 train-loss:  2.0853131871957045 	 ± 0.22522974733254122
	data : 0.11632161140441895
	model : 0.06962165832519532
			 train-loss:  2.0826686892328383 	 ± 0.22501508785474855
	data : 0.11639761924743652
	model : 0.0697129249572754
			 train-loss:  2.083044110238552 	 ± 0.2236292170621779
	data : 0.11622757911682129
	model : 0.06952705383300781
			 train-loss:  2.08464973208345 	 ± 0.22270801671070664
	data : 0.11637144088745117
	model : 0.06966280937194824
			 train-loss:  2.0845740641035686 	 ± 0.22134692301757405
	data : 0.11616549491882325
	model : 0.06919474601745605
			 train-loss:  2.0878808282944092 	 ± 0.22203785748947963
	data : 0.11656427383422852
	model : 0.0693924903869629
			 train-loss:  2.090222533260073 	 ± 0.22174091428963938
	data : 0.11635041236877441
	model : 0.06868667602539062
			 train-loss:  2.0878606291378246 	 ± 0.22149305972956168
	data : 0.11705002784729004
	model : 0.06874003410339355
			 train-loss:  2.0858694467433665 	 ± 0.2209654466247522
	data : 0.11697802543640137
	model : 0.06839241981506347
			 train-loss:  2.0845442928116897 	 ± 0.22003529754076925
	data : 0.11723766326904297
	model : 0.06893892288208008
			 train-loss:  2.083629094741561 	 ± 0.21894799787883695
	data : 0.11677098274230957
	model : 0.06883382797241211
			 train-loss:  2.085396149185266 	 ± 0.2183446184101779
	data : 0.11712217330932617
	model : 0.06959714889526367
			 train-loss:  2.079497015476227 	 ± 0.22414691036895273
	data : 0.11646428108215331
	model : 0.07008728981018067
			 train-loss:  2.0753010786496677 	 ± 0.2264382013951408
	data : 0.1161435604095459
	model : 0.07000141143798828
			 train-loss:  2.0737929421922434 	 ± 0.22566326113390858
	data : 0.11623892784118653
	model : 0.06966004371643067
			 train-loss:  2.0717158497020765 	 ± 0.2253292154695225
	data : 0.11650490760803223
	model : 0.06983075141906739
			 train-loss:  2.072491706685817 	 ± 0.22425230446378663
	data : 0.11636123657226563
	model : 0.06984915733337402
			 train-loss:  2.0706479260795994 	 ± 0.2237840313389129
	data : 0.11628484725952148
	model : 0.0693291187286377
			 train-loss:  2.0706528139611087 	 ± 0.22261544342925368
	data : 0.11668524742126465
	model : 0.06852178573608399
			 train-loss:  2.073209699896193 	 ± 0.22287742841890504
	data : 0.11749582290649414
	model : 0.06791863441467286
			 train-loss:  2.070741820092104 	 ± 0.2230655481604852
	data : 0.11799483299255371
	model : 0.06790847778320312
			 train-loss:  2.06764325108191 	 ± 0.2240458458327741
	data : 0.11788220405578613
	model : 0.06695771217346191
			 train-loss:  2.0662828135490416 	 ± 0.2233333917316815
	data : 0.11888904571533203
	model : 0.06713666915893554
			 train-loss:  2.0681440358114713 	 ± 0.22300309202159366
	data : 0.11853814125061035
	model : 0.06807012557983398
			 train-loss:  2.0663014159483066 	 ± 0.222678571845987
	data : 0.11762166023254395
	model : 0.06908740997314453
			 train-loss:  2.065757052412311 	 ± 0.22166316152085688
	data : 0.11676235198974609
	model : 0.06902174949645996
			 train-loss:  2.0679207100318027 	 ± 0.22168512620966463
	data : 0.11681981086730957
	model : 0.06990690231323242
			 train-loss:  2.067871411641439 	 ± 0.22062753008305297
	data : 0.1160323143005371
	model : 0.06942987442016602
			 train-loss:  2.0671062368266986 	 ± 0.21972430774101528
	data : 0.11656503677368164
	model : 0.06937484741210938
			 train-loss:  2.0707654741322883 	 ± 0.22191644781975767
	data : 0.11645016670227051
	model : 0.06844425201416016
			 train-loss:  2.074288954337438 	 ± 0.22387343650412075
	data : 0.11747479438781738
	model : 0.06867222785949707
			 train-loss:  2.0713169563800915 	 ± 0.22497432008625423
	data : 0.11724700927734374
	model : 0.06877822875976562
			 train-loss:  2.0710279150442643 	 ± 0.2239697052441885
	data : 0.1172037124633789
	model : 0.0693436622619629
			 train-loss:  2.0738134652644664 	 ± 0.2248644858505007
	data : 0.11681933403015136
	model : 0.0693507194519043
			 train-loss:  2.071732790342399 	 ± 0.2249291321357543
	data : 0.11688008308410644
	model : 0.06928162574768067
			 train-loss:  2.0729552992677265 	 ± 0.22430509345670352
	data : 0.11670312881469727
	model : 0.06909093856811524
			 train-loss:  2.0739588141441345 	 ± 0.22357376987827782
	data : 0.11684346199035645
	model : 0.06804060935974121
			 train-loss:  2.0722231356993968 	 ± 0.22336967224825263
	data : 0.11758904457092285
	model : 0.06792969703674316
			 train-loss:  2.0733119896773635 	 ± 0.22271109995598046
	data : 0.11777963638305664
	model : 0.06797223091125489
			 train-loss:  2.072575671041114 	 ± 0.22189905717432132
	data : 0.11781101226806641
	model : 0.06878542900085449
			 train-loss:  2.071419763363014 	 ± 0.22131027230988173
	data : 0.11710405349731445
	model : 0.06879205703735351
			 train-loss:  2.072895467782221 	 ± 0.22096068389270804
	data : 0.11715898513793946
	model : 0.06984758377075195
			 train-loss:  2.0744454274574915 	 ± 0.22068675293013848
	data : 0.11630716323852539
	model : 0.0698965072631836
			 train-loss:  2.0735483642452017 	 ± 0.2199925190239925
	data : 0.11614012718200684
	model : 0.06976943016052246
			 train-loss:  2.074401509566385 	 ± 0.2192899559596441
	data : 0.116371488571167
	model : 0.06945223808288574
			 train-loss:  2.075545826578528 	 ± 0.2187621517530947
	data : 0.11664915084838867
	model : 0.06944503784179687
			 train-loss:  2.075254472994035 	 ± 0.21790222017414343
	data : 0.1166083812713623
	model : 0.06934504508972168
			 train-loss:  2.0743708906173706 	 ± 0.21725177915403598
	data : 0.11662063598632813
	model : 0.06916608810424804
			 train-loss:  2.0757071016326782 	 ± 0.21690303839648062
	data : 0.11678290367126465
	model : 0.06936554908752442
			 train-loss:  2.075339213130981 	 ± 0.21608686424398807
	data : 0.11657257080078125
	model : 0.0698817253112793
			 train-loss:  2.0736890425905585 	 ± 0.21604297871074768
	data : 0.11595296859741211
	model : 0.06938028335571289
			 train-loss:  2.0743840249009833 	 ± 0.21534756636956423
	data : 0.11638398170471191
	model : 0.06880922317504883
			 train-loss:  2.0726982896144572 	 ± 0.21537044038636252
	data : 0.11698770523071289
	model : 0.06807708740234375
			 train-loss:  2.0701808210547643 	 ± 0.21645840531909188
	data : 0.11768798828125
	model : 0.06868667602539062
			 train-loss:  2.070852267922777 	 ± 0.21577382838925288
	data : 0.11700677871704102
	model : 0.06841254234313965
			 train-loss:  2.073463772472582 	 ± 0.21704496415964739
	data : 0.11741418838500976
	model : 0.06834325790405274
			 train-loss:  2.0727030821700594 	 ± 0.21641146177815931
	data : 0.1174853801727295
	model : 0.06885428428649902
			 train-loss:  2.072510984208849 	 ± 0.2156199148389768
	data : 0.11695055961608887
	model : 0.06982169151306153
			 train-loss:  2.073154437191346 	 ± 0.21495578476450683
	data : 0.1160355567932129
	model : 0.06824932098388672
			 train-loss:  2.0710310265965703 	 ± 0.21559667454629738
	data : 0.11765913963317871
	model : 0.06827831268310547
			 train-loss:  2.073138930659363 	 ± 0.21622633299588742
	data : 0.11771059036254883
	model : 0.06827373504638672
			 train-loss:  2.0739606378747406 	 ± 0.2156632713093207
	data : 0.11769142150878906
	model : 0.06814160346984863
			 train-loss:  2.0741778552532195 	 ± 0.21490692445510595
	data : 0.11785321235656739
	model : 0.06716351509094239
			 train-loss:  2.070979727920911 	 ± 0.21746115898246887
	data : 0.11863961219787597
	model : 0.06814932823181152
			 train-loss:  2.0694460356739204 	 ± 0.21745802793560737
	data : 0.11780762672424316
	model : 0.06832013130187989
			 train-loss:  2.0676301375969306 	 ± 0.2177740839237172
	data : 0.11751270294189453
	model : 0.06894807815551758
			 train-loss:  2.0693157464265823 	 ± 0.2179507057785878
	data : 0.11689844131469726
	model : 0.06913328170776367
			 train-loss:  2.069160344682891 	 ± 0.2172058567078536
	data : 0.11675143241882324
	model : 0.07020130157470703
			 train-loss:  2.072038149180478 	 ± 0.21921701481815517
	data : 0.11584687232971191
	model : 0.0693939208984375
			 train-loss:  2.0696295718757476 	 ± 0.22040002055376698
	data : 0.11648759841918946
	model : 0.06924715042114257
			 train-loss:  2.070504718535655 	 ± 0.21991029150089783
	data : 0.11682343482971191
	model : 0.06868324279785157
			 train-loss:  2.072048952115462 	 ± 0.21997476727545467
	data : 0.11741161346435547
	model : 0.06862702369689941
			 train-loss:  2.072625176111857 	 ± 0.21935309130026034
	data : 0.11751208305358887
	model : 0.06833562850952149
			 train-loss:  2.0730755613339658 	 ± 0.21869512603030275
	data : 0.11779026985168457
	model : 0.06827397346496582
			 train-loss:  2.0732538417765967 	 ± 0.21798555578967813
	data : 0.11785974502563476
	model : 0.06834607124328614
			 train-loss:  2.072667995309518 	 ± 0.21739203793694847
	data : 0.11764979362487793
	model : 0.06883049011230469
			 train-loss:  2.073188273163585 	 ± 0.21678061496098033
	data : 0.11705422401428223
	model : 0.06836395263671875
			 train-loss:  2.074709740761788 	 ± 0.21690352231561827
	data : 0.11740260124206543
	model : 0.0681107521057129
			 train-loss:  2.0728069039491506 	 ± 0.21750120897370934
	data : 0.11761445999145508
	model : 0.06897807121276855
			 train-loss:  2.0729510465245338 	 ± 0.21681489821006777
	data : 0.11696219444274902
	model : 0.06910533905029297
			 train-loss:  2.074051349977904 	 ± 0.2165669676908932
	data : 0.11690878868103027
	model : 0.06905403137207031
			 train-loss:  2.071560354352747 	 ± 0.21814370131785013
	data : 0.11706304550170898
	model : 0.06870360374450683
			 train-loss:  2.0732065200805665 	 ± 0.21844936645466698
	data : 0.11743292808532715
	model : 0.06864924430847168
			 train-loss:  2.074159062427023 	 ± 0.21810296035503832
	data : 0.1174776554107666
	model : 0.06861605644226074
			 train-loss:  2.077162034717607 	 ± 0.22074224511267718
	data : 0.11742577552795411
	model : 0.06762185096740722
			 train-loss:  2.0790089112849324 	 ± 0.2213160040995298
	data : 0.11820516586303711
	model : 0.0677785873413086
			 train-loss:  2.081023764319536 	 ± 0.22213471262933152
	data : 0.1181410789489746
	model : 0.06912188529968262
			 train-loss:  2.080477819298253 	 ± 0.22157088756167484
	data : 0.117012357711792
	model : 0.06873536109924316
			 train-loss:  2.0806399404284464 	 ± 0.2209123129748639
	data : 0.11744565963745117
	model : 0.06865277290344238
			 train-loss:  2.080014502930784 	 ± 0.220397267551989
	data : 0.11753296852111816
	model : 0.06962103843688965
			 train-loss:  2.081144724573408 	 ± 0.22022521396243652
	data : 0.11677193641662598
	model : 0.06955432891845703
			 train-loss:  2.07936570489195 	 ± 0.22078014568522783
	data : 0.1167724609375
	model : 0.06923971176147461
			 train-loss:  2.078678658429314 	 ± 0.2203109566407161
	data : 0.11704854965209961
	model : 0.06947121620178223
			 train-loss:  2.0770083233626964 	 ± 0.2207427898310019
	data : 0.11671004295349122
	model : 0.06940622329711914
			 train-loss:  2.0796310076879903 	 ± 0.2227561434410428
	data : 0.11681523323059081
	model : 0.06932806968688965
			 train-loss:  2.0791443028201946 	 ± 0.22220310650546804
	data : 0.11673674583435059
	model : 0.06880521774291992
			 train-loss:  2.0779870433368903 	 ± 0.2220859086435682
	data : 0.11727495193481445
	model : 0.06864008903503419
			 train-loss:  2.079074903215681 	 ± 0.22191491243804562
	data : 0.11724214553833008
	model : 0.06930603981018066
			 train-loss:  2.0806100693616 	 ± 0.2222135209396943
	data : 0.11675691604614258
	model : 0.06940217018127441
			 train-loss:  2.079506488169654 	 ± 0.22206805541485247
	data : 0.11654963493347167
	model : 0.06937718391418457
			 train-loss:  2.0788032117854343 	 ± 0.22164096831828445
	data : 0.1167147159576416
	model : 0.06982288360595704
			 train-loss:  2.079830595234919 	 ± 0.2214456164590579
	data : 0.11630105972290039
	model : 0.0696995735168457
			 train-loss:  2.0813175645139483 	 ± 0.22172394935564735
	data : 0.11641192436218262
	model : 0.0695793628692627
			 train-loss:  2.08051975129059 	 ± 0.2213695332154449
	data : 0.11640830039978027
	model : 0.06967301368713379
			 train-loss:  2.0813335324381734 	 ± 0.2210318535888484
	data : 0.11640844345092774
	model : 0.06968164443969727
			 train-loss:  2.0797730723365406 	 ± 0.22143009992239934
	data : 0.11626386642456055
	model : 0.06980571746826172
			 train-loss:  2.0796245563289393 	 ± 0.2208367070618164
	data : 0.1162388801574707
	model : 0.06966853141784668
			 train-loss:  2.078406792718011 	 ± 0.22085764248745718
	data : 0.11637535095214843
	model : 0.0689931869506836
			 train-loss:  2.077899803397476 	 ± 0.22037105632716422
	data : 0.11708917617797851
	model : 0.06903095245361328
			 train-loss:  2.076838911535906 	 ± 0.22025677413908934
	data : 0.11706881523132324
	model : 0.06884303092956542
			 train-loss:  2.078002889105614 	 ± 0.22024612192174828
	data : 0.11731009483337403
	model : 0.06879472732543945
			 train-loss:  2.0792205787840343 	 ± 0.22029629292268257
	data : 0.11727104187011719
	model : 0.06893925666809082
			 train-loss:  2.0782659361236973 	 ± 0.22010742143866574
	data : 0.117396879196167
	model : 0.06956439018249512
			 train-loss:  2.0763627066038044 	 ± 0.2210924241185713
	data : 0.1168027400970459
	model : 0.06951022148132324
			 train-loss:  2.0785683436940112 	 ± 0.22261278144292487
	data : 0.11686511039733886
	model : 0.0697129249572754
			 train-loss:  2.076873138160903 	 ± 0.22327435113303706
	data : 0.1165651798248291
	model : 0.06973557472229004
			 train-loss:  2.0759583023405566 	 ± 0.22306052127880202
	data : 0.1165700912475586
	model : 0.0697256088256836
			 train-loss:  2.0746549698022694 	 ± 0.22322719432313498
	data : 0.1164015769958496
	model : 0.06984481811523438
			 train-loss:  2.075074770620891 	 ± 0.22273416673940277
	data : 0.11642656326293946
	model : 0.0698582649230957
			 train-loss:  2.0752593460421878 	 ± 0.22218315953935167
	data : 0.11628904342651367
	model : 0.06986837387084961
			 train-loss:  2.075677170295908 	 ± 0.221698958254737
	data : 0.11647276878356934
	model : 0.06983480453491211
			 train-loss:  2.0764748445108308 	 ± 0.22142589116389375
	data : 0.11645832061767578
	model : 0.06997780799865723
			 train-loss:  2.0782474249601366 	 ± 0.22228257781422886
	data : 0.11624226570129395
	model : 0.06998939514160156
			 train-loss:  2.0800067055877762 	 ± 0.22312045947479126
	data : 0.11627826690673829
	model : 0.06993002891540527
			 train-loss:  2.0804837487711767 	 ± 0.22267023115153192
	data : 0.11629858016967773
	model : 0.06987137794494629
			 train-loss:  2.0814524548394338 	 ± 0.22254738922538375
	data : 0.11635780334472656
	model : 0.0700303077697754
			 train-loss:  2.0813655181258333 	 ± 0.2220047153616404
	data : 0.11621718406677246
	model : 0.06915678977966308
			 train-loss:  2.0817962838382256 	 ± 0.22154802578674487
	data : 0.11691594123840332
	model : 0.0690610408782959
			 train-loss:  2.081292270456703 	 ± 0.22112741626291232
	data : 0.11676139831542968
	model : 0.06894726753234863
			 train-loss:  2.0801882156427354 	 ± 0.22116106403669913
	data : 0.11686849594116211
	model : 0.06810836791992188
			 train-loss:  2.0807881091649714 	 ± 0.22079754281539885
	data : 0.11749720573425293
	model : 0.06788549423217774
			 train-loss:  2.0794991049469944 	 ± 0.2210517869523086
	data : 0.11750750541687012
	model : 0.06813325881958007
			 train-loss:  2.0799851582163855 	 ± 0.2206367673315595
	data : 0.11735806465148926
	model : 0.0675236701965332
			 train-loss:  2.0802129100284303 	 ± 0.2201380528691111
	data : 0.11794915199279785
	model : 0.0676508903503418
			 train-loss:  2.0796739599614775 	 ± 0.21975773568487758
	data : 0.11787738800048828
	model : 0.06869521141052246
			 train-loss:  2.080418458566979 	 ± 0.2195090875015318
	data : 0.11704535484313965
	model : 0.06798887252807617
			 train-loss:  2.080913164348246 	 ± 0.21911459969533165
	data : 0.11781816482543946
	model : 0.06844415664672851
			 train-loss:  2.0804289679194605 	 ± 0.2187191608873443
	data : 0.11744365692138672
	model : 0.0690000057220459
			 train-loss:  2.079986909473384 	 ± 0.21830852744627996
	data : 0.11721258163452149
	model : 0.06890387535095215
			 train-loss:  2.078718936388394 	 ± 0.21860069407736327
	data : 0.11726384162902832
	model : 0.06886053085327148
			 train-loss:  2.0774417519569397 	 ± 0.21890872715212634
	data : 0.11750373840332032
	model : 0.0697206974029541
			 train-loss:  2.0780083833764134 	 ± 0.21856854038288887
	data : 0.11676397323608398
	model : 0.06989917755126954
			 train-loss:  2.077086398276416 	 ± 0.21849764948220213
	data : 0.11667370796203613
	model : 0.06990766525268555
			 train-loss:  2.0788581991627204 	 ± 0.2195810566517341
	data : 0.1164217472076416
	model : 0.06936759948730468
			 train-loss:  2.080532114248018 	 ± 0.22049465331015644
	data : 0.11681203842163086
	model : 0.06926827430725098
			 train-loss:  2.0804597128667104 	 ± 0.22000235998723783
	data : 0.11682791709899902
	model : 0.06922326087951661
			 train-loss:  2.0783590567963466 	 ± 0.2217408552954488
	data : 0.11677689552307129
	model : 0.06884284019470215
			 train-loss:  2.0790095053778757 	 ± 0.22146161893110405
	data : 0.11705737113952637
	model : 0.06869759559631347
			 train-loss:  2.0806133103581654 	 ± 0.2222768029469482
	data : 0.11702151298522949
	model : 0.06901545524597168
			 train-loss:  2.0805502641568623 	 ± 0.2217886912283956
	data : 0.11652684211730957
	model : 0.06865415573120118
			 train-loss:  2.0805572490943107 	 ± 0.2213018030596225
	data : 0.11663942337036133
	model : 0.06841878890991211
			 train-loss:  2.082383776335737 	 ± 0.2225337719232315
	data : 0.11706681251525879
	model : 0.06796541213989257
			 train-loss:  2.0839749958204186 	 ± 0.2233512772037925
	data : 0.11735730171203614
	model : 0.06782240867614746
			 train-loss:  2.0831213410282547 	 ± 0.2232430162637821
	data : 0.1174036979675293
	model : 0.06779212951660156
			 train-loss:  2.0823413657731025 	 ± 0.2230765770387724
	data : 0.11762981414794922
	model : 0.06790647506713868
			 train-loss:  2.081269379337458 	 ± 0.22319540177544173
	data : 0.11746349334716796
	model : 0.06793856620788574
			 train-loss:  2.0825231610200343 	 ± 0.2235387356731088
	data : 0.11726698875427247
	model : 0.06876440048217773
			 train-loss:  2.08131811060804 	 ± 0.2238229929514151
	data : 0.11677255630493164
	model : 0.06853117942810058
			 train-loss:  2.0815151071144364 	 ± 0.22336870343982299
	data : 0.11710357666015625
	model : 0.06862897872924804
			 train-loss:  2.0815386882814173 	 ± 0.22289725768714977
	data : 0.11699161529541016
	model : 0.0685551643371582
			 train-loss:  2.0813415521333196 	 ± 0.22244919655168835
	data : 0.11727123260498047
	model : 0.06862082481384277
			 train-loss:  2.0820653029565532 	 ± 0.22226396139957413
	data : 0.11717777252197266
	model : 0.06853904724121093
			 train-loss:  2.0827902873357136 	 ± 0.22208342774683457
	data : 0.11708593368530273
	model : 0.06920170783996582
			 train-loss:  2.0835647226863876 	 ± 0.22194669912312348
	data : 0.1166618824005127
	model : 0.06930742263793946
			 train-loss:  2.0842349046517996 	 ± 0.22173187820187268
	data : 0.1164435863494873
	model : 0.06941871643066407
			 train-loss:  2.0827575912200866 	 ± 0.22246540114343033
	data : 0.11603431701660157
	model : 0.06900854110717773
			 train-loss:  2.0830339804047444 	 ± 0.22205086431410492
	data : 0.11648855209350586
	model : 0.06823973655700684
			 train-loss:  2.0861409600900145 	 ± 0.22684961036862283
	data : 0.11699047088623046
	model : 0.06781563758850098
			 train-loss:  2.084438046304191 	 ± 0.22795182597382976
	data : 0.1170339584350586
	model : 0.06768674850463867
			 train-loss:  2.084710763533588 	 ± 0.22753012663527292
	data : 0.11740679740905761
	model : 0.06765227317810059
			 train-loss:  2.084843904260666 	 ± 0.22708057408734156
	data : 0.11763253211975097
	model : 0.06789617538452149
			 train-loss:  2.0858923522344077 	 ± 0.22722479776332336
	data : 0.11723647117614747
	model : 0.06832637786865234
			 train-loss:  2.08587481546402 	 ± 0.22677006165001026
	data : 0.11683502197265624
	model : 0.06819272041320801
			 train-loss:  2.08656017666319 	 ± 0.22657716506114675
	data : 0.11721196174621581
	model : 0.06748499870300292
			 train-loss:  2.0852131077221463 	 ± 0.22713202308504143
	data : 0.11779484748840333
	model : 0.06705303192138672
			 train-loss:  2.0859896495879404 	 ± 0.22701763664070712
	data : 0.1181459903717041
	model : 0.06681766510009765
			 train-loss:  2.0854512171482478 	 ± 0.22673211692151746
	data : 0.11872076988220215
	model : 0.06673622131347656
			 train-loss:  2.085405777014938 	 ± 0.2262882662668908
	data : 0.11888971328735351
	model : 0.06724162101745605
			 train-loss:  2.083478500135243 	 ± 0.22793316027720778
	data : 0.11739568710327149
	model : 0.05934529304504395
#epoch  61    val-loss:  2.4524096752467908  train-loss:  2.083478500135243  lr:  3.90625e-05
			 train-loss:  2.518920421600342 	 ± 0.0
	data : 5.633991479873657
	model : 0.0849604606628418
			 train-loss:  2.540596842765808 	 ± 0.02167642116546631
	data : 2.883345365524292
	model : 0.07796120643615723
			 train-loss:  2.385509411493937 	 ± 0.22003969528256265
	data : 1.9610248406728108
	model : 0.07385714848836263
			 train-loss:  2.200885772705078 	 ± 0.37225094308000023
	data : 1.5006917715072632
	model : 0.07213330268859863
			 train-loss:  2.234106206893921 	 ± 0.33951583275296693
	data : 1.2243220329284668
	model : 0.07071781158447266
			 train-loss:  2.2454784711201987 	 ± 0.31097557654867136
	data : 0.12159557342529297
	model : 0.06676034927368164
			 train-loss:  2.2470971175602505 	 ± 0.28793473862347513
	data : 0.11917314529418946
	model : 0.06643953323364257
			 train-loss:  2.283811926841736 	 ± 0.28631966844233525
	data : 0.11910471916198731
	model : 0.0670701503753662
			 train-loss:  2.263033707936605 	 ± 0.2762680855252411
	data : 0.11847038269042968
	model : 0.06685414314270019
			 train-loss:  2.2410486698150636 	 ± 0.27026232938138894
	data : 0.11864671707153321
	model : 0.06812944412231445
			 train-loss:  2.2257297039031982 	 ± 0.26219892615644497
	data : 0.11766915321350098
	model : 0.06816587448120118
			 train-loss:  2.190976003805796 	 ± 0.276234085839387
	data : 0.11757025718688965
	model : 0.06826448440551758
			 train-loss:  2.19699580852802 	 ± 0.2662151209052385
	data : 0.11746330261230468
	model : 0.06838092803955079
			 train-loss:  2.2142911979130337 	 ± 0.26400191120600175
	data : 0.11744322776794433
	model : 0.06825699806213378
			 train-loss:  2.199895668029785 	 ± 0.2606756217493302
	data : 0.11756558418273926
	model : 0.06781449317932128
			 train-loss:  2.179533764719963 	 ± 0.2644312767705024
	data : 0.11807169914245605
	model : 0.06911959648132324
			 train-loss:  2.183181734646068 	 ± 0.256950681049186
	data : 0.1171234130859375
	model : 0.06859817504882812
			 train-loss:  2.196038670010037 	 ± 0.25527590010783663
	data : 0.1177222728729248
	model : 0.06779398918151855
			 train-loss:  2.186811371853477 	 ± 0.25153246623761366
	data : 0.11837582588195801
	model : 0.06881542205810547
			 train-loss:  2.1917925119400024 	 ± 0.24612309088679113
	data : 0.11747493743896484
	model : 0.06898174285888672
			 train-loss:  2.1681510720934187 	 ± 0.262431573133301
	data : 0.11702260971069336
	model : 0.06788907051086426
			 train-loss:  2.1637190905484287 	 ± 0.25720099664318
	data : 0.11793928146362305
	model : 0.06847748756408692
			 train-loss:  2.1554273833399233 	 ± 0.2545362772043182
	data : 0.11743741035461426
	model : 0.06960048675537109
			 train-loss:  2.1437824219465256 	 ± 0.2553587759547851
	data : 0.11658000946044922
	model : 0.06914243698120118
			 train-loss:  2.123753981590271 	 ± 0.2687510045252635
	data : 0.11693520545959472
	model : 0.06823010444641113
			 train-loss:  2.1305185969059286 	 ± 0.26569369105109286
	data : 0.11783008575439453
	model : 0.0690004825592041
			 train-loss:  2.1136452930944936 	 ± 0.27455599563087857
	data : 0.11712536811828614
	model : 0.06891765594482421
			 train-loss:  2.100162459271295 	 ± 0.2785624888006488
	data : 0.11704072952270508
	model : 0.06876754760742188
			 train-loss:  2.098949724230273 	 ± 0.2737927679965936
	data : 0.1170844554901123
	model : 0.06893038749694824
			 train-loss:  2.0987017432848614 	 ± 0.2691941935151239
	data : 0.11702427864074708
	model : 0.06971259117126465
			 train-loss:  2.11610439900429 	 ± 0.28144891078995904
	data : 0.11627216339111328
	model : 0.06869635581970215
			 train-loss:  2.109557181596756 	 ± 0.2794045732818357
	data : 0.11719937324523926
	model : 0.06877236366271973
			 train-loss:  2.114834416996349 	 ± 0.276753367266607
	data : 0.11718554496765136
	model : 0.0685966968536377
			 train-loss:  2.1222310557084927 	 ± 0.2759441004857044
	data : 0.1172945499420166
	model : 0.067985200881958
			 train-loss:  2.116724954332624 	 ± 0.27386192534963866
	data : 0.11766996383666992
	model : 0.06816606521606446
			 train-loss:  2.1312374340163336 	 ± 0.2833521365988936
	data : 0.11750035285949707
	model : 0.06960792541503906
			 train-loss:  2.1296906406814986 	 ± 0.2796508683418245
	data : 0.1161656379699707
	model : 0.06868419647216797
			 train-loss:  2.1203136036270545 	 ± 0.281779982690516
	data : 0.1170423984527588
	model : 0.06791257858276367
			 train-loss:  2.117474063848838 	 ± 0.27869419754155583
	data : 0.11769723892211914
	model : 0.06783452033996581
			 train-loss:  2.115297457575798 	 ± 0.27552397596142897
	data : 0.11797041893005371
	model : 0.06766252517700196
			 train-loss:  2.118096773217364 	 ± 0.2727184632816993
	data : 0.11823296546936035
	model : 0.06712822914123535
			 train-loss:  2.1117646126520064 	 ± 0.27248571496374113
	data : 0.11873097419738769
	model : 0.0680234432220459
			 train-loss:  2.1105376953302426 	 ± 0.2694159981719864
	data : 0.11809983253479003
	model : 0.06797566413879394
			 train-loss:  2.109886884689331 	 ± 0.2663710462421062
	data : 0.11803932189941406
	model : 0.06883878707885742
			 train-loss:  2.112217590543959 	 ± 0.2638480726106095
	data : 0.1170569896697998
	model : 0.06980819702148437
			 train-loss:  2.1196209762407388 	 ± 0.2656480260341983
	data : 0.11603388786315919
	model : 0.06982560157775879
			 train-loss:  2.117467773721573 	 ± 0.26321222856718396
	data : 0.11589641571044922
	model : 0.06961922645568848
			 train-loss:  2.1189437260230384 	 ± 0.2606524818190996
	data : 0.11614751815795898
	model : 0.07027316093444824
			 train-loss:  2.119079979098573 	 ± 0.25798077946171727
	data : 0.11580166816711426
	model : 0.07014803886413574
			 train-loss:  2.124725131988525 	 ± 0.2584270123780361
	data : 0.11602597236633301
	model : 0.0692777156829834
			 train-loss:  2.114256517559874 	 ± 0.26637307432062474
	data : 0.11682038307189942
	model : 0.0692110538482666
			 train-loss:  2.1138920738146854 	 ± 0.2638121996474732
	data : 0.1168428897857666
	model : 0.0692293643951416
			 train-loss:  2.1196787672222785 	 ± 0.264622354945764
	data : 0.11670255661010742
	model : 0.06945343017578125
			 train-loss:  2.1125000185436673 	 ± 0.26731920353132893
	data : 0.11644492149353028
	model : 0.06962118148803711
			 train-loss:  2.115944836356423 	 ± 0.2660847561939632
	data : 0.1163243293762207
	model : 0.0695216178894043
			 train-loss:  2.112225430352347 	 ± 0.26513706101364437
	data : 0.11653528213500977
	model : 0.06877150535583496
			 train-loss:  2.1054396608419585 	 ± 0.26766206720797225
	data : 0.1173487663269043
	model : 0.0682060718536377
			 train-loss:  2.113272516891874 	 ± 0.2718545852525874
	data : 0.11777830123901367
	model : 0.06801066398620606
			 train-loss:  2.1111218565601413 	 ± 0.2700380705531214
	data : 0.11791415214538574
	model : 0.06807470321655273
			 train-loss:  2.111230425039927 	 ± 0.26777959655039396
	data : 0.11795358657836914
	model : 0.06823396682739258
			 train-loss:  2.1150374607961684 	 ± 0.26720781583659386
	data : 0.1176335334777832
	model : 0.06921272277832032
			 train-loss:  2.1106932720830365 	 ± 0.2672070249438641
	data : 0.11663837432861328
	model : 0.0699009895324707
			 train-loss:  2.1032633705744668 	 ± 0.2714569585516125
	data : 0.11612496376037598
	model : 0.0692328929901123
			 train-loss:  2.1103312484920025 	 ± 0.27510843349647984
	data : 0.1168527603149414
	model : 0.06899456977844239
			 train-loss:  2.1082443952560426 	 ± 0.2734940367882907
	data : 0.11677060127258301
	model : 0.06910476684570313
			 train-loss:  2.1049853960673013 	 ± 0.27268303958560297
	data : 0.11667323112487793
	model : 0.06938037872314454
			 train-loss:  2.1048446982654174 	 ± 0.27064285499936935
	data : 0.1163184642791748
	model : 0.06997981071472167
			 train-loss:  2.107660539009992 	 ± 0.26963238918185406
	data : 0.11581158638000488
	model : 0.07093453407287598
			 train-loss:  2.1053189056507056 	 ± 0.2683669862970661
	data : 0.11491856575012208
	model : 0.0708836555480957
			 train-loss:  2.101825819696699 	 ± 0.2680184396922865
	data : 0.11502470970153808
	model : 0.0706820011138916
			 train-loss:  2.1062254267679132 	 ± 0.26865795565454703
	data : 0.11536369323730469
	model : 0.07005643844604492
			 train-loss:  2.1040696452061334 	 ± 0.2674034450752631
	data : 0.11598057746887207
	model : 0.06950864791870118
			 train-loss:  2.100260228326876 	 ± 0.2675255625867279
	data : 0.11634573936462403
	model : 0.069171142578125
			 train-loss:  2.10396007911579 	 ± 0.2675856028719915
	data : 0.11676034927368165
	model : 0.06847496032714843
			 train-loss:  2.103007672627767 	 ± 0.265921952046597
	data : 0.11748728752136231
	model : 0.06850066184997558
			 train-loss:  2.1013589360212026 	 ± 0.2645522741645445
	data : 0.11742033958435058
	model : 0.06883244514465332
			 train-loss:  2.105292241294663 	 ± 0.26505614422790064
	data : 0.11724996566772461
	model : 0.0678934097290039
			 train-loss:  2.109145123224992 	 ± 0.26551289871287903
	data : 0.11801743507385254
	model : 0.06788039207458496
			 train-loss:  2.108062308045882 	 ± 0.26400035067370836
	data : 0.11783719062805176
	model : 0.0687744140625
			 train-loss:  2.1030175134539606 	 ± 0.26614944878186947
	data : 0.11718430519104003
	model : 0.06867032051086426
			 train-loss:  2.1106618760544578 	 ± 0.2731957517011328
	data : 0.117252779006958
	model : 0.06823005676269531
			 train-loss:  2.1096058720495643 	 ± 0.2716910953962345
	data : 0.11759247779846191
	model : 0.06917328834533691
			 train-loss:  2.106575165886477 	 ± 0.2714403913369988
	data : 0.11685333251953126
	model : 0.06949615478515625
			 train-loss:  2.10358390212059 	 ± 0.2711925519602582
	data : 0.11661729812622071
	model : 0.06861538887023926
			 train-loss:  2.104048724735484 	 ± 0.2696262399716088
	data : 0.11741461753845214
	model : 0.0679478645324707
			 train-loss:  2.1032645355823427 	 ± 0.26815154498326277
	data : 0.11813874244689941
	model : 0.06827259063720703
			 train-loss:  2.100682658710699 	 ± 0.2676789822186345
	data : 0.11793313026428223
	model : 0.06813130378723145
			 train-loss:  2.103396402163939 	 ± 0.2673546575797871
	data : 0.11812520027160645
	model : 0.0679098129272461
			 train-loss:  2.101213490025381 	 ± 0.26663591814767945
	data : 0.11823444366455078
	model : 0.06878938674926757
			 train-loss:  2.104105504353841 	 ± 0.26655045500725033
	data : 0.11719803810119629
	model : 0.06959152221679688
			 train-loss:  2.103970378309816 	 ± 0.2650849459003612
	data : 0.11650571823120118
	model : 0.06907806396484376
			 train-loss:  2.1103248648021533 	 ± 0.2705194230657376
	data : 0.11680564880371094
	model : 0.06899609565734863
			 train-loss:  2.1093358403892926 	 ± 0.26922826738050853
	data : 0.116676664352417
	model : 0.06911616325378418
			 train-loss:  2.107580376432297 	 ± 0.26832694396585477
	data : 0.11676740646362305
	model : 0.06903014183044434
			 train-loss:  2.107339001956739 	 ± 0.26692121978077915
	data : 0.11698698997497559
	model : 0.06908864974975586
			 train-loss:  2.105161535243193 	 ± 0.2663741941179666
	data : 0.1169461727142334
	model : 0.06963539123535156
			 train-loss:  2.1036413023152303 	 ± 0.26541586360139174
	data : 0.11662459373474121
	model : 0.06986680030822753
			 train-loss:  2.1043631020857365 	 ± 0.26415390343849077
	data : 0.11627182960510254
	model : 0.06992869377136231
			 train-loss:  2.1061273885495737 	 ± 0.26339610646969064
	data : 0.11603689193725586
	model : 0.07006030082702637
			 train-loss:  2.1061446249485014 	 ± 0.26207587303414637
	data : 0.11591277122497559
	model : 0.0700185775756836
			 train-loss:  2.1053904129727052 	 ± 0.26088428380679446
	data : 0.11576037406921387
	model : 0.0697166919708252
			 train-loss:  2.106517122072332 	 ± 0.2598491205920851
	data : 0.1158717155456543
	model : 0.06962409019470214
			 train-loss:  2.1075216369721494 	 ± 0.2587835763952167
	data : 0.11628012657165528
	model : 0.06961860656738281
			 train-loss:  2.108625291631772 	 ± 0.25777988056433515
	data : 0.116325044631958
	model : 0.06955771446228028
			 train-loss:  2.1055500246229624 	 ± 0.25845920571961484
	data : 0.1163015365600586
	model : 0.06935653686523438
			 train-loss:  2.1020874673465513 	 ± 0.2596725595680907
	data : 0.11654100418090821
	model : 0.06957149505615234
			 train-loss:  2.1021975902753454 	 ± 0.2584587747732225
	data : 0.11639575958251953
	model : 0.06977372169494629
			 train-loss:  2.100614528965067 	 ± 0.2577800650635814
	data : 0.11613869667053223
	model : 0.0696904182434082
			 train-loss:  2.0969023901388186 	 ± 0.25947863308000313
	data : 0.11635799407958984
	model : 0.06964726448059082
			 train-loss:  2.0975497180765323 	 ± 0.25838489195393344
	data : 0.1165158748626709
	model : 0.06966619491577149
			 train-loss:  2.099441165322656 	 ± 0.2579822070369063
	data : 0.11645393371582032
	model : 0.06966719627380372
			 train-loss:  2.0985899780477797 	 ± 0.25698443802338516
	data : 0.11644463539123535
	model : 0.06884207725524902
			 train-loss:  2.0980807848736247 	 ± 0.25590155694472266
	data : 0.11716980934143066
	model : 0.06892008781433105
			 train-loss:  2.10056355752443 	 ± 0.25614004330244045
	data : 0.1172088623046875
	model : 0.06941227912902832
			 train-loss:  2.1055783126665197 	 ± 0.26058408534642247
	data : 0.11674289703369141
	model : 0.06934156417846679
			 train-loss:  2.1050305304856134 	 ± 0.25952493747626704
	data : 0.11677422523498535
	model : 0.06951556205749512
			 train-loss:  2.1039005528148422 	 ± 0.25869990220442224
	data : 0.1167219638824463
	model : 0.07011260986328124
			 train-loss:  2.103897068460109 	 ± 0.257601386613979
	data : 0.11618986129760742
	model : 0.06954369544982911
			 train-loss:  2.104727352366728 	 ± 0.25667525390666557
	data : 0.11655468940734863
	model : 0.0685183048248291
			 train-loss:  2.1024632513523103 	 ± 0.2567940419999378
	data : 0.11757583618164062
	model : 0.06879744529724122
			 train-loss:  2.1014147425486036 	 ± 0.25598851445583853
	data : 0.11729402542114258
	model : 0.06873946189880371
			 train-loss:  2.10117445128863 	 ± 0.25495092455731333
	data : 0.11718235015869141
	model : 0.06938552856445312
			 train-loss:  2.1026994610220435 	 ± 0.2544705262536719
	data : 0.11665921211242676
	model : 0.06989374160766601
			 train-loss:  2.1023243809899976 	 ± 0.25347649445802173
	data : 0.11620559692382812
	model : 0.07048473358154297
			 train-loss:  2.1004410133361815 	 ± 0.2533301555638339
	data : 0.11553816795349121
	model : 0.06955890655517578
			 train-loss:  2.1059667988428994 	 ± 0.25977609827859305
	data : 0.11667890548706054
	model : 0.06941957473754883
			 train-loss:  2.107006889628613 	 ± 0.2590145932851168
	data : 0.11680421829223633
	model : 0.06895265579223633
			 train-loss:  2.1091395262628794 	 ± 0.2591178184109391
	data : 0.11714739799499511
	model : 0.06888246536254883
			 train-loss:  2.106728380964708 	 ± 0.25954904436208787
	data : 0.11714215278625488
	model : 0.06815214157104492
			 train-loss:  2.1095957508453957 	 ± 0.26059186898620695
	data : 0.11778383255004883
	model : 0.06852669715881347
			 train-loss:  2.1073265658080125 	 ± 0.26088145902891446
	data : 0.11749353408813476
	model : 0.06866660118103027
			 train-loss:  2.1042163959055236 	 ± 0.26231797440543136
	data : 0.11733665466308593
	model : 0.06844058036804199
			 train-loss:  2.1061312139482427 	 ± 0.2622543185114668
	data : 0.11763134002685546
	model : 0.06785836219787597
			 train-loss:  2.103775616902024 	 ± 0.26268243054940926
	data : 0.1181833267211914
	model : 0.0677973747253418
			 train-loss:  2.103026639090644 	 ± 0.2618512987061867
	data : 0.11827888488769531
	model : 0.06828265190124512
			 train-loss:  2.1057325461331535 	 ± 0.2627744299952205
	data : 0.11773910522460937
	model : 0.06899580955505372
			 train-loss:  2.1071113026055106 	 ± 0.2623069106468636
	data : 0.11712164878845215
	model : 0.06910820007324218
			 train-loss:  2.1077515197836836 	 ± 0.26146220055802427
	data : 0.11681437492370605
	model : 0.06983475685119629
			 train-loss:  2.105557417698044 	 ± 0.26179192021117903
	data : 0.11617770195007324
	model : 0.07042937278747559
			 train-loss:  2.1102592638560704 	 ± 0.2666803261765507
	data : 0.11572132110595704
	model : 0.06947636604309082
			 train-loss:  2.111902377284165 	 ± 0.26644321275468397
	data : 0.11634483337402343
	model : 0.06843900680541992
			 train-loss:  2.1120275342968147 	 ± 0.2655075342160447
	data : 0.11761131286621093
	model : 0.06754746437072753
			 train-loss:  2.112409438286628 	 ± 0.26461669397554993
	data : 0.11849637031555176
	model : 0.06752486228942871
			 train-loss:  2.114849684966935 	 ± 0.2653059890585616
	data : 0.11836590766906738
	model : 0.06775021553039551
			 train-loss:  2.1137829435282742 	 ± 0.26469926589960663
	data : 0.11808381080627442
	model : 0.06782727241516114
			 train-loss:  2.1149528173551166 	 ± 0.2641670815892544
	data : 0.11809229850769043
	model : 0.06717023849487305
			 train-loss:  2.115260766477001 	 ± 0.26329331513601223
	data : 0.11844549179077149
	model : 0.06808643341064453
			 train-loss:  2.113327821364274 	 ± 0.263446771357976
	data : 0.11770563125610352
	model : 0.06723346710205078
			 train-loss:  2.114022623772589 	 ± 0.2626972561470188
	data : 0.1186838150024414
	model : 0.06649022102355957
			 train-loss:  2.115285917123159 	 ± 0.2622738517935163
	data : 0.11930222511291504
	model : 0.06745223999023438
			 train-loss:  2.115304507167134 	 ± 0.2614040518618487
	data : 0.11853570938110351
	model : 0.06836996078491211
			 train-loss:  2.114554721273874 	 ± 0.2607056082914808
	data : 0.11767301559448243
	model : 0.06851563453674317
			 train-loss:  2.1149517219830183 	 ± 0.2598983250867383
	data : 0.11785411834716797
	model : 0.06955537796020508
			 train-loss:  2.113613624851425 	 ± 0.25958133349015833
	data : 0.1167747974395752
	model : 0.06947517395019531
			 train-loss:  2.113982138326091 	 ± 0.25878303002304975
	data : 0.1169355869293213
	model : 0.0692789077758789
			 train-loss:  2.11251232562921 	 ± 0.25860051144358154
	data : 0.1170259952545166
	model : 0.06928706169128418
			 train-loss:  2.1150938340812733 	 ± 0.2597843086923823
	data : 0.11699600219726562
	model : 0.06823902130126953
			 train-loss:  2.1146092565753793 	 ± 0.25903207239372705
	data : 0.11781716346740723
	model : 0.06806941032409668
			 train-loss:  2.1162724359980167 	 ± 0.2590611339974707
	data : 0.11806120872497558
	model : 0.06834826469421387
			 train-loss:  2.113927833735943 	 ± 0.2599370411395507
	data : 0.11761627197265626
	model : 0.06838798522949219
			 train-loss:  2.1126888716442984 	 ± 0.2596019977409262
	data : 0.11752257347106934
	model : 0.06746325492858887
			 train-loss:  2.1147301270637984 	 ± 0.26009235351019866
	data : 0.11854939460754395
	model : 0.06823348999023438
			 train-loss:  2.1130200346554715 	 ± 0.2602052442791367
	data : 0.11779084205627441
	model : 0.06884665489196777
			 train-loss:  2.1115339085823153 	 ± 0.2601036716810735
	data : 0.11742186546325684
	model : 0.06850028038024902
			 train-loss:  2.1100134379935986 	 ± 0.26004429722125355
	data : 0.11790051460266113
	model : 0.06854324340820313
			 train-loss:  2.1112700449415 	 ± 0.2597618407233484
	data : 0.11800308227539062
	model : 0.06906390190124512
			 train-loss:  2.114000698049625 	 ± 0.2613616998201229
	data : 0.1173637866973877
	model : 0.06925468444824219
			 train-loss:  2.114487092409815 	 ± 0.26065847411960147
	data : 0.11712722778320313
	model : 0.06871414184570312
			 train-loss:  2.1153481013676116 	 ± 0.2601256542711668
	data : 0.11744751930236816
	model : 0.06942973136901856
			 train-loss:  2.11480753772399 	 ± 0.25945463540662667
	data : 0.11675066947937011
	model : 0.06891846656799316
			 train-loss:  2.113998240197611 	 ± 0.2589099969073164
	data : 0.11701450347900391
	model : 0.06908831596374512
			 train-loss:  2.114943651265876 	 ± 0.258452107459719
	data : 0.11686310768127442
	model : 0.06913666725158692
			 train-loss:  2.1161603913830884 	 ± 0.2581976330273425
	data : 0.11666221618652343
	model : 0.06821656227111816
			 train-loss:  2.11476368602665 	 ± 0.2581092125838907
	data : 0.11746416091918946
	model : 0.06828370094299316
			 train-loss:  2.114678863797869 	 ± 0.2573731332407397
	data : 0.11751379966735839
	model : 0.06860852241516113
			 train-loss:  2.1147545738653704 	 ± 0.2566428722911106
	data : 0.11725916862487792
	model : 0.06868281364440917
			 train-loss:  2.113573959318258 	 ± 0.2563957093881608
	data : 0.11720170974731445
	model : 0.06816973686218261
			 train-loss:  2.1132065968567066 	 ± 0.255721191894517
	data : 0.11790742874145507
	model : 0.0681605339050293
			 train-loss:  2.1143385111952626 	 ± 0.25545265917919696
	data : 0.11796584129333496
	model : 0.06825151443481445
			 train-loss:  2.1133773704369863 	 ± 0.2550664346749473
	data : 0.11794900894165039
	model : 0.0681602954864502
			 train-loss:  2.114412054172537 	 ± 0.25473937154669124
	data : 0.11801509857177735
	model : 0.06827821731567382
			 train-loss:  2.113595887199863 	 ± 0.25427576844621685
	data : 0.11792559623718261
	model : 0.06857838630676269
			 train-loss:  2.1117544200251013 	 ± 0.2547940664161026
	data : 0.117578125
	model : 0.06957101821899414
			 train-loss:  2.1116204805996106 	 ± 0.2541072078538291
	data : 0.11668453216552735
	model : 0.06944928169250489
			 train-loss:  2.1116464821068015 	 ± 0.25341974646316817
	data : 0.11666231155395508
	model : 0.06968941688537597
			 train-loss:  2.112909254207406 	 ± 0.2533205290252606
	data : 0.11653833389282227
	model : 0.06965680122375488
			 train-loss:  2.1133331168781626 	 ± 0.25270841917829545
	data : 0.11666975021362305
	model : 0.0697174072265625
			 train-loss:  2.112125334587503 	 ± 0.2525760085030085
	data : 0.11652641296386719
	model : 0.06965537071228027
			 train-loss:  2.1107487646990983 	 ± 0.2526130477406051
	data : 0.11658644676208496
	model : 0.06983566284179688
			 train-loss:  2.1129275654491626 	 ± 0.25372171326852755
	data : 0.11656746864318848
	model : 0.06986708641052246
			 train-loss:  2.1121200429207367 	 ± 0.2533013324668293
	data : 0.11647205352783203
	model : 0.06999011039733886
			 train-loss:  2.1115872437755265 	 ± 0.25274811635910255
	data : 0.11627302169799805
	model : 0.07003297805786132
			 train-loss:  2.110431497578794 	 ± 0.2526006361503379
	data : 0.11619987487792968
	model : 0.06990933418273926
			 train-loss:  2.1099069517912326 	 ± 0.252054126173058
	data : 0.11636505126953126
	model : 0.06964669227600098
			 train-loss:  2.108026075363159 	 ± 0.25276826169982075
	data : 0.11636691093444824
	model : 0.06987166404724121
			 train-loss:  2.1096609015854035 	 ± 0.25315407097814857
	data : 0.11636943817138672
	model : 0.07100610733032227
			 train-loss:  2.107925804738466 	 ± 0.25367644546617973
	data : 0.11526350975036621
	model : 0.07097039222717286
			 train-loss:  2.1072473026285268 	 ± 0.2532141823218147
	data : 0.11539945602416993
	model : 0.07102432250976562
			 train-loss:  2.1056788686531873 	 ± 0.2535395454056129
	data : 0.11535992622375488
	model : 0.07030606269836426
			 train-loss:  2.1054594826698305 	 ± 0.2529238373320937
	data : 0.1161679744720459
	model : 0.07012424468994141
			 train-loss:  2.105034746340851 	 ± 0.2523653832210944
	data : 0.11620874404907226
	model : 0.06893906593322754
			 train-loss:  2.104041631268983 	 ± 0.25213337717824325
	data : 0.11755862236022949
	model : 0.06906571388244628
			 train-loss:  2.102800934772773 	 ± 0.25212898541195167
	data : 0.11750926971435546
	model : 0.06931066513061523
			 train-loss:  2.1015760957026015 	 ± 0.25211497229959207
	data : 0.1173431396484375
	model : 0.06943511962890625
			 train-loss:  2.102603956548179 	 ± 0.25192742307066895
	data : 0.11719822883605957
	model : 0.06947546005249024
			 train-loss:  2.1025594461311417 	 ± 0.25131601288613703
	data : 0.1171494960784912
	model : 0.06860003471374512
			 train-loss:  2.1046237911003223 	 ± 0.2524529500231621
	data : 0.11789360046386718
	model : 0.06855154037475586
			 train-loss:  2.1038738087965894 	 ± 0.2520764125604595
	data : 0.1176682949066162
	model : 0.0674868106842041
			 train-loss:  2.1050206029244016 	 ± 0.252015942020225
	data : 0.11854805946350097
	model : 0.0685307502746582
			 train-loss:  2.104528425988697 	 ± 0.2515158534580175
	data : 0.11753425598144532
	model : 0.0684199333190918
			 train-loss:  2.1033447754891563 	 ± 0.25150472964949155
	data : 0.11755046844482422
	model : 0.06910805702209473
			 train-loss:  2.102792318137187 	 ± 0.2510391548542109
	data : 0.11686205863952637
	model : 0.0690305233001709
			 train-loss:  2.1028570266956454 	 ± 0.2504509399571487
	data : 0.1170985221862793
	model : 0.07073845863342285
			 train-loss:  2.1034197027438153 	 ± 0.2499999988515951
	data : 0.11548213958740235
	model : 0.07046399116516114
			 train-loss:  2.103193256466888 	 ± 0.2494399230839868
	data : 0.11566791534423829
	model : 0.07051563262939453
			 train-loss:  2.101820769133391 	 ± 0.24967422477443704
	data : 0.11562023162841797
	model : 0.07077164649963379
			 train-loss:  2.104042652015862 	 ± 0.2512295586789072
	data : 0.11540169715881347
	model : 0.07072591781616211
			 train-loss:  2.1031897849992873 	 ± 0.25096734610055804
	data : 0.11537060737609864
	model : 0.07025618553161621
			 train-loss:  2.103644922443721 	 ± 0.2504838649007907
	data : 0.11595988273620605
	model : 0.07061419486999512
			 train-loss:  2.1030605299906298 	 ± 0.2500635251160713
	data : 0.11567625999450684
	model : 0.07105598449707032
			 train-loss:  2.1025855875662547 	 ± 0.24959656025433005
	data : 0.11528081893920898
	model : 0.07090172767639161
			 train-loss:  2.10198530313131 	 ± 0.2491936089152274
	data : 0.11541099548339843
	model : 0.07049770355224609
			 train-loss:  2.1018911541310126 	 ± 0.24863820828116967
	data : 0.11574053764343262
	model : 0.07013416290283203
			 train-loss:  2.1029860164437975 	 ± 0.24862077051564724
	data : 0.1159317970275879
	model : 0.06967477798461914
			 train-loss:  2.1035767078399656 	 ± 0.248225146813913
	data : 0.11628403663635253
	model : 0.06853399276733399
			 train-loss:  2.102488169627907 	 ± 0.2482130003441619
	data : 0.11737465858459473
	model : 0.0679403305053711
			 train-loss:  2.102019030617197 	 ± 0.24776607083544597
	data : 0.11790804862976074
	model : 0.06790242195129395
			 train-loss:  2.100908745799148 	 ± 0.2477874298252704
	data : 0.11804585456848145
	model : 0.0674807071685791
			 train-loss:  2.101779683708624 	 ± 0.24759531430319123
	data : 0.11825971603393555
	model : 0.06741418838500976
			 train-loss:  2.1009306752163432 	 ± 0.2473903185289854
	data : 0.11826534271240234
	model : 0.06712970733642579
			 train-loss:  2.1012979647813936 	 ± 0.24691709840297965
	data : 0.11850614547729492
	model : 0.06690325736999511
			 train-loss:  2.10455276020642 	 ± 0.25130141920120774
	data : 0.11837353706359863
	model : 0.0664867877960205
			 train-loss:  2.1049792551687347 	 ± 0.2508456961164185
	data : 0.11833081245422364
	model : 0.06636857986450195
			 train-loss:  2.1035034172555322 	 ± 0.2513208225990499
	data : 0.11840896606445313
	model : 0.06613001823425294
			 train-loss:  2.103253501019579 	 ± 0.2508146645269008
	data : 0.11870112419128417
	model : 0.06658177375793457
			 train-loss:  2.102859266228595 	 ± 0.2503556681772018
	data : 0.11814646720886231
	model : 0.06710319519042969
			 train-loss:  2.1020066124477466 	 ± 0.25017008809736696
	data : 0.11775565147399902
	model : 0.0671156406402588
			 train-loss:  2.1004871881308675 	 ± 0.2507374335784486
	data : 0.11819820404052735
	model : 0.06722869873046874
			 train-loss:  2.100163931627154 	 ± 0.25026202079526116
	data : 0.11808638572692871
	model : 0.06752357482910157
			 train-loss:  2.098866037031015 	 ± 0.2505448442586764
	data : 0.11788249015808105
	model : 0.06798686981201171
			 train-loss:  2.0973681864402107 	 ± 0.25109899403451363
	data : 0.11772637367248535
	model : 0.06836075782775879
			 train-loss:  2.097995853128512 	 ± 0.25076903792394406
	data : 0.1174778938293457
	model : 0.06846556663513184
			 train-loss:  2.0963474149076045 	 ± 0.25156295886381597
	data : 0.11745600700378418
	model : 0.0688166618347168
			 train-loss:  2.0965121657144827 	 ± 0.25106006779738893
	data : 0.11732215881347656
	model : 0.06891756057739258
			 train-loss:  2.0951321660255897 	 ± 0.2514727838190924
	data : 0.11715240478515625
	model : 0.0687413215637207
			 train-loss:  2.094688407773894 	 ± 0.2510572433134245
	data : 0.11720223426818847
	model : 0.06865901947021484
			 train-loss:  2.093986929669554 	 ± 0.25078996767309913
	data : 0.11725149154663086
	model : 0.06905760765075683
			 train-loss:  2.0917620134930455 	 ± 0.25271467865064706
	data : 0.11663222312927246
	model : 0.06818885803222656
			 train-loss:  2.0920314324428757 	 ± 0.25224239433662093
	data : 0.11741514205932617
	model : 0.06796274185180665
			 train-loss:  2.09202987241745 	 ± 0.2517374052552676
	data : 0.11753268241882324
	model : 0.06800799369812012
			 train-loss:  2.090542832218793 	 ± 0.25233324467031504
	data : 0.11725258827209473
	model : 0.06716303825378418
			 train-loss:  2.090479348387037 	 ± 0.2518340942272241
	data : 0.11803240776062011
	model : 0.06701807975769043
			 train-loss:  2.090664468735103 	 ± 0.2513530850791867
	data : 0.11816143989562988
	model : 0.06762747764587403
			 train-loss:  2.09279859629203 	 ± 0.2531440859154357
	data : 0.11738467216491699
	model : 0.06700563430786133
			 train-loss:  2.0907642528122548 	 ± 0.25471909845281276
	data : 0.11778378486633301
	model : 0.06621351242065429
			 train-loss:  2.090139606501907 	 ± 0.2544167273962392
	data : 0.11770105361938477
	model : 0.05777616500854492
#epoch  62    val-loss:  2.4300120065086768  train-loss:  2.090139606501907  lr:  3.90625e-05
			 train-loss:  1.810938835144043 	 ± 0.0
	data : 5.24761438369751
	model : 0.09135746955871582
			 train-loss:  1.8038069009780884 	 ± 0.00713193416595459
	data : 2.7236188650131226
	model : 0.08152318000793457
			 train-loss:  1.9040306409200032 	 ± 0.14185734296299518
	data : 1.8540225823720295
	model : 0.07754254341125488
			 train-loss:  1.9327290654182434 	 ± 0.13252708410145528
	data : 1.4192535877227783
	model : 0.07490003108978271
			 train-loss:  1.91874520778656 	 ± 0.12179053950697533
	data : 1.1590906620025634
	model : 0.07308239936828613
			 train-loss:  1.900774876276652 	 ± 0.11821778067355208
	data : 0.13339271545410156
	model : 0.06885080337524414
			 train-loss:  1.8961692707879203 	 ± 0.11002827512237122
	data : 0.11670379638671875
	model : 0.06774377822875977
			 train-loss:  1.918639823794365 	 ± 0.11885884034228066
	data : 0.1177520751953125
	model : 0.0677602767944336
			 train-loss:  1.9387990236282349 	 ± 0.12573327666661457
	data : 0.11807308197021485
	model : 0.06821813583374023
			 train-loss:  1.986974847316742 	 ± 0.1873930656875707
	data : 0.11767811775207519
	model : 0.0689997673034668
			 train-loss:  1.992318402637135 	 ± 0.17946954790024275
	data : 0.11705641746520996
	model : 0.06900672912597657
			 train-loss:  1.9936680694421132 	 ± 0.17188730710425787
	data : 0.11710200309753419
	model : 0.06987185478210449
			 train-loss:  1.983169702383188 	 ± 0.16910092731030227
	data : 0.11628160476684571
	model : 0.0697706699371338
			 train-loss:  1.9850867135184151 	 ± 0.16309625654627222
	data : 0.11624026298522949
	model : 0.07013134956359864
			 train-loss:  1.9746553500493367 	 ± 0.16232811356318808
	data : 0.11595087051391602
	model : 0.06978144645690917
			 train-loss:  1.9832312539219856 	 ± 0.16064466170142577
	data : 0.11637039184570312
	model : 0.06877269744873046
			 train-loss:  1.9861359245636885 	 ± 0.15628070741432093
	data : 0.11711816787719727
	model : 0.0681373119354248
			 train-loss:  2.0087534255451627 	 ± 0.1782222278453112
	data : 0.11769452095031738
	model : 0.06827611923217773
			 train-loss:  2.023946391908746 	 ± 0.18505752940141704
	data : 0.11763195991516114
	model : 0.06791014671325683
			 train-loss:  2.0251664936542513 	 ± 0.18045015668008385
	data : 0.11798682212829589
	model : 0.06821956634521484
			 train-loss:  2.019279814901806 	 ± 0.17805823490214565
	data : 0.117706298828125
	model : 0.06890134811401367
			 train-loss:  2.054693866859783 	 ± 0.23790936781737948
	data : 0.11727361679077149
	model : 0.06911754608154297
			 train-loss:  2.042604949163354 	 ± 0.23948921166750012
	data : 0.11712255477905273
	model : 0.06915845870971679
			 train-loss:  2.0567636638879776 	 ± 0.24408211540742994
	data : 0.11708850860595703
	model : 0.06935391426086426
			 train-loss:  2.067600712776184 	 ± 0.24497272294242142
	data : 0.11681981086730957
	model : 0.06946043968200684
			 train-loss:  2.047677856225234 	 ± 0.26005095752067675
	data : 0.11668162345886231
	model : 0.06979093551635743
			 train-loss:  2.0409685947276928 	 ± 0.2574726863947792
	data : 0.11622605323791504
	model : 0.07021980285644532
			 train-loss:  2.053547978401184 	 ± 0.26114576458010685
	data : 0.1157536506652832
	model : 0.07036175727844238
			 train-loss:  2.0474576210153512 	 ± 0.25861955562072525
	data : 0.11577205657958985
	model : 0.06995081901550293
			 train-loss:  2.044201139609019 	 ± 0.25487671698520553
	data : 0.11617388725280761
	model : 0.06979365348815918
			 train-loss:  2.0498975823002477 	 ± 0.252665929064813
	data : 0.11631512641906738
	model : 0.06881070137023926
			 train-loss:  2.045389126986265 	 ± 0.24995035650725783
	data : 0.11722416877746582
	model : 0.06868510246276856
			 train-loss:  2.045042316118876 	 ± 0.24614191498504456
	data : 0.1172722339630127
	model : 0.06888666152954101
			 train-loss:  2.051600803347195 	 ± 0.24540448178424637
	data : 0.11715245246887207
	model : 0.06972594261169433
			 train-loss:  2.05621451990945 	 ± 0.24336480681878372
	data : 0.11633076667785644
	model : 0.06889224052429199
			 train-loss:  2.071703073051241 	 ± 0.25686102342322154
	data : 0.11702136993408203
	model : 0.06967692375183106
			 train-loss:  2.071365578754528 	 ± 0.25337424466171543
	data : 0.11619114875793457
	model : 0.06949596405029297
			 train-loss:  2.0662869309124194 	 ± 0.2519194355444526
	data : 0.11636452674865723
	model : 0.06895985603332519
			 train-loss:  2.072197275284009 	 ± 0.2513236116432507
	data : 0.11678624153137207
	model : 0.06849808692932129
			 train-loss:  2.0699155420064925 	 ± 0.24857094518147457
	data : 0.11735763549804687
	model : 0.06954369544982911
			 train-loss:  2.0856660371873437 	 ± 0.2649596689503465
	data : 0.11633963584899902
	model : 0.06976490020751953
			 train-loss:  2.0843582919665744 	 ± 0.2619202729105201
	data : 0.11622734069824218
	model : 0.07005844116210938
			 train-loss:  2.081086982128232 	 ± 0.25972348822767183
	data : 0.11591086387634278
	model : 0.07020139694213867
			 train-loss:  2.0834376134655694 	 ± 0.2572173943183123
	data : 0.11567854881286621
	model : 0.06926941871643066
			 train-loss:  2.0807124773661294 	 ± 0.25498491787822475
	data : 0.11661386489868164
	model : 0.06831965446472169
			 train-loss:  2.0875222371972124 	 ± 0.25630190382007123
	data : 0.11755671501159667
	model : 0.06790022850036621
			 train-loss:  2.0923430919647217 	 ± 0.25566005090923927
	data : 0.11797399520874023
	model : 0.0669325351715088
			 train-loss:  2.098488653699557 	 ± 0.25646723899618773
	data : 0.11899008750915527
	model : 0.06692357063293457
			 train-loss:  2.0901935611452376 	 ± 0.26026121410565795
	data : 0.11897282600402832
	model : 0.06717147827148437
			 train-loss:  2.098229329586029 	 ± 0.2637144037091322
	data : 0.11870808601379394
	model : 0.06784934997558593
			 train-loss:  2.096292306395138 	 ± 0.2614751548288147
	data : 0.11801495552062988
	model : 0.06721467971801758
			 train-loss:  2.0954553599541006 	 ± 0.2590177361559273
	data : 0.11853928565979004
	model : 0.0679664134979248
			 train-loss:  2.09091147611726 	 ± 0.2586464243384736
	data : 0.11761031150817872
	model : 0.06807827949523926
			 train-loss:  2.08966315013391 	 ± 0.2564014670703343
	data : 0.11754159927368164
	model : 0.06867074966430664
			 train-loss:  2.0895579533143476 	 ± 0.25406102817936826
	data : 0.11699938774108887
	model : 0.06935725212097169
			 train-loss:  2.0939491199595586 	 ± 0.25387971537627674
	data : 0.11648483276367187
	model : 0.07052454948425294
			 train-loss:  2.0891483520206653 	 ± 0.2541943643432912
	data : 0.11530375480651855
	model : 0.07060222625732422
			 train-loss:  2.0885874464594085 	 ± 0.2520290857801575
	data : 0.1153946876525879
	model : 0.0695876121520996
			 train-loss:  2.089512479507317 	 ± 0.2499834042817977
	data : 0.11614756584167481
	model : 0.06961798667907715
			 train-loss:  2.0886798640092215 	 ± 0.24797394159105487
	data : 0.11609811782836914
	model : 0.06946010589599609
			 train-loss:  2.08570378921071 	 ± 0.24701102517729423
	data : 0.11634926795959473
	model : 0.06944732666015625
			 train-loss:  2.089216261140762 	 ± 0.24654193879541256
	data : 0.11658964157104493
	model : 0.06884737014770508
			 train-loss:  2.0853806677318754 	 ± 0.24643508145580656
	data : 0.11709256172180176
	model : 0.06985940933227539
			 train-loss:  2.0802851002663374 	 ± 0.24782478107894906
	data : 0.11637120246887207
	model : 0.06998586654663086
			 train-loss:  2.0781121474045974 	 ± 0.24652471206747145
	data : 0.11627111434936524
	model : 0.06977291107177734
			 train-loss:  2.0773701776157725 	 ± 0.24472309382936375
	data : 0.11639623641967774
	model : 0.06962647438049316
			 train-loss:  2.079216551424852 	 ± 0.2433526689257865
	data : 0.11657199859619141
	model : 0.07024035453796387
			 train-loss:  2.076697323252173 	 ± 0.24243524426308263
	data : 0.11626687049865722
	model : 0.06938815116882324
			 train-loss:  2.0812395223672837 	 ± 0.24356926165877343
	data : 0.11691493988037109
	model : 0.06933693885803223
			 train-loss:  2.0778832384518213 	 ± 0.243425001651678
	data : 0.11681299209594727
	model : 0.0682671070098877
			 train-loss:  2.07469504651889 	 ± 0.2431720859979549
	data : 0.11773271560668945
	model : 0.06746096611022949
			 train-loss:  2.074371960428026 	 ± 0.2414928316223496
	data : 0.11833596229553223
	model : 0.06653246879577637
			 train-loss:  2.071300589874999 	 ± 0.24124489435266108
	data : 0.11895165443420411
	model : 0.06673541069030761
			 train-loss:  2.0660190985009477 	 ± 0.2438214464982904
	data : 0.11874079704284668
	model : 0.06655535697937012
			 train-loss:  2.064312384923299 	 ± 0.24263511343927952
	data : 0.11896882057189942
	model : 0.06705107688903808
			 train-loss:  2.0602058015371623 	 ± 0.2436431229936222
	data : 0.11871805191040039
	model : 0.06813898086547851
			 train-loss:  2.0602418757104255 	 ± 0.24205605877793207
	data : 0.11766047477722168
	model : 0.06908559799194336
			 train-loss:  2.061756286865626 	 ± 0.24086627442298789
	data : 0.1168123722076416
	model : 0.06968283653259277
			 train-loss:  2.0585511500322364 	 ± 0.24100510711832296
	data : 0.11629700660705566
	model : 0.06988000869750977
			 train-loss:  2.0561200097203254 	 ± 0.2404669278725874
	data : 0.1161458969116211
	model : 0.07047362327575683
			 train-loss:  2.0547967545780135 	 ± 0.23927085687874858
	data : 0.11550655364990234
	model : 0.07019681930541992
			 train-loss:  2.0537982074225822 	 ± 0.23797716362830876
	data : 0.11573605537414551
	model : 0.07030067443847657
			 train-loss:  2.056426476283246 	 ± 0.23773355444048072
	data : 0.11554598808288574
	model : 0.06997404098510743
			 train-loss:  2.0533884323778606 	 ± 0.23792957903670078
	data : 0.11588377952575683
	model : 0.07023773193359376
			 train-loss:  2.0562179551405064 	 ± 0.23794326971157945
	data : 0.11568193435668946
	model : 0.06926841735839843
			 train-loss:  2.0560095656749815 	 ± 0.2365636354572872
	data : 0.11666936874389648
	model : 0.06903433799743652
			 train-loss:  2.057140524359955 	 ± 0.23543387178973102
	data : 0.11693110466003417
	model : 0.06798753738403321
			 train-loss:  2.0594380403106864 	 ± 0.2350711981062485
	data : 0.11789464950561523
	model : 0.06827797889709472
			 train-loss:  2.0613930774538702 	 ± 0.23446521595861836
	data : 0.11765403747558593
	model : 0.06709728240966797
			 train-loss:  2.058413349257575 	 ± 0.23484745559154477
	data : 0.11866154670715331
	model : 0.06790738105773926
			 train-loss:  2.0593068573501085 	 ± 0.23370729338425753
	data : 0.11772069931030274
	model : 0.06828703880310058
			 train-loss:  2.0601458134858506 	 ± 0.232571415345259
	data : 0.11752924919128419
	model : 0.06924643516540527
			 train-loss:  2.0597629675301175 	 ± 0.23134679733790547
	data : 0.11651902198791504
	model : 0.06922616958618164
			 train-loss:  2.06318047706117 	 ± 0.23246106386658907
	data : 0.11635780334472656
	model : 0.06999921798706055
			 train-loss:  2.062577729476126 	 ± 0.23130818026926822
	data : 0.1156529426574707
	model : 0.06911692619323731
			 train-loss:  2.065105438232422 	 ± 0.23141549429649927
	data : 0.11655001640319824
	model : 0.0689964771270752
			 train-loss:  2.0608343058025715 	 ± 0.23399214474179714
	data : 0.1166849136352539
	model : 0.06888608932495117
			 train-loss:  2.06481433036376 	 ± 0.23607237107181403
	data : 0.11722116470336914
	model : 0.06893210411071778
			 train-loss:  2.066533742528973 	 ± 0.2354930115589428
	data : 0.11731414794921875
	model : 0.0690155029296875
			 train-loss:  2.0626560759544375 	 ± 0.2374678555435102
	data : 0.11721324920654297
	model : 0.06997566223144532
			 train-loss:  2.067679537404882 	 ± 0.24157022240389056
	data : 0.11624999046325683
	model : 0.06999144554138184
			 train-loss:  2.0708449704974305 	 ± 0.24247900867597158
	data : 0.11622228622436523
	model : 0.0693699836730957
			 train-loss:  2.0696925052161355 	 ± 0.2415796100579164
	data : 0.11663784980773925
	model : 0.06927075386047363
			 train-loss:  2.074362773161668 	 ± 0.24504310196021803
	data : 0.11656341552734376
	model : 0.06920881271362304
			 train-loss:  2.075967082523164 	 ± 0.24442162358983907
	data : 0.11649503707885742
	model : 0.06888408660888672
			 train-loss:  2.07562348077882 	 ± 0.24329143735654168
	data : 0.11684784889221192
	model : 0.06864385604858399
			 train-loss:  2.075383424758911 	 ± 0.24216450526438832
	data : 0.11690216064453125
	model : 0.06939883232116699
			 train-loss:  2.0749902261628046 	 ± 0.2410750788081728
	data : 0.11615581512451172
	model : 0.0695460319519043
			 train-loss:  2.0741320312569993 	 ± 0.2401323597464156
	data : 0.11634302139282227
	model : 0.06960625648498535
			 train-loss:  2.0745827306400644 	 ± 0.23908466547955998
	data : 0.11628537178039551
	model : 0.06986584663391113
			 train-loss:  2.074636998477283 	 ± 0.2380059515087792
	data : 0.11602210998535156
	model : 0.07005543708801269
			 train-loss:  2.0747541189193726 	 ± 0.23694425561372834
	data : 0.11588969230651855
	model : 0.06990909576416016
			 train-loss:  2.077122057433677 	 ± 0.2372208720776095
	data : 0.11599898338317871
	model : 0.06979589462280274
			 train-loss:  2.0764700739007247 	 ± 0.23627980691912925
	data : 0.11607284545898437
	model : 0.06886663436889648
			 train-loss:  2.0766079985577126 	 ± 0.23525486956617467
	data : 0.11708168983459473
	model : 0.06882085800170898
			 train-loss:  2.0765255442981063 	 ± 0.23424031399034895
	data : 0.1170119285583496
	model : 0.06878061294555664
			 train-loss:  2.0795170567993426 	 ± 0.23545204183859886
	data : 0.11695117950439453
	model : 0.06886076927185059
			 train-loss:  2.0779698592121316 	 ± 0.23504878276177732
	data : 0.11697149276733398
	model : 0.06899175643920899
			 train-loss:  2.079516950775595 	 ± 0.2346616586667033
	data : 0.11680893898010254
	model : 0.07013258934020997
			 train-loss:  2.081258824467659 	 ± 0.2344531310993165
	data : 0.11597228050231934
	model : 0.07014608383178711
			 train-loss:  2.0833799612423607 	 ± 0.23463566225007892
	data : 0.1160205364227295
	model : 0.07002615928649902
			 train-loss:  2.0819036735862984 	 ± 0.23423565703744575
	data : 0.11633110046386719
	model : 0.06990752220153809
			 train-loss:  2.0805722358750134 	 ± 0.2337446209359944
	data : 0.1164372444152832
	model : 0.06893982887268066
			 train-loss:  2.0795973741239115 	 ± 0.23305111931454744
	data : 0.11735372543334961
	model : 0.06782355308532714
			 train-loss:  2.077523451805115 	 ± 0.23326308042491303
	data : 0.11826257705688477
	model : 0.06795539855957031
			 train-loss:  2.073825848481012 	 ± 0.2359848721284401
	data : 0.11830968856811523
	model : 0.06776609420776367
			 train-loss:  2.0724382503764835 	 ± 0.23556945664666187
	data : 0.11837410926818848
	model : 0.06778335571289062
			 train-loss:  2.073182483203709 	 ± 0.23479730229045856
	data : 0.11844344139099121
	model : 0.06789546012878418
			 train-loss:  2.0760175008182378 	 ± 0.23607453942384182
	data : 0.11814923286437988
	model : 0.06884398460388183
			 train-loss:  2.0747072925934424 	 ± 0.23563517139461956
	data : 0.11736059188842773
	model : 0.06842803955078125
			 train-loss:  2.0738257242523077 	 ± 0.23494918244158683
	data : 0.1176985263824463
	model : 0.06817102432250977
			 train-loss:  2.0744340320428214 	 ± 0.2341610622323862
	data : 0.11790628433227539
	model : 0.06871838569641113
			 train-loss:  2.0755361569555184 	 ± 0.23362250436975351
	data : 0.1172868251800537
	model : 0.06993861198425293
			 train-loss:  2.0751391978406195 	 ± 0.23279416383603166
	data : 0.11635632514953613
	model : 0.0700913906097412
			 train-loss:  2.077423326174418 	 ± 0.23343265118304327
	data : 0.1160315990447998
	model : 0.06951828002929687
			 train-loss:  2.0768100400181377 	 ± 0.23268199577893095
	data : 0.11652903556823731
	model : 0.06986846923828124
			 train-loss:  2.073701797610652 	 ± 0.23464790914171066
	data : 0.11615552902221679
	model : 0.06948108673095703
			 train-loss:  2.0725947920826897 	 ± 0.23415496267770355
	data : 0.11649489402770996
	model : 0.0690587043762207
			 train-loss:  2.0724928507701956 	 ± 0.2333142316099599
	data : 0.1169515609741211
	model : 0.06892294883728027
			 train-loss:  2.0712602998529164 	 ± 0.23293319153314007
	data : 0.11704645156860352
	model : 0.06998672485351562
			 train-loss:  2.071715959420441 	 ± 0.23216832623505237
	data : 0.11611709594726563
	model : 0.07017426490783692
			 train-loss:  2.0714944223283043 	 ± 0.2313643432430924
	data : 0.11611404418945312
	model : 0.07010025978088379
			 train-loss:  2.070367656387649 	 ± 0.23094460559145688
	data : 0.11603398323059082
	model : 0.0695115566253662
			 train-loss:  2.0697766269246736 	 ± 0.23024981697570296
	data : 0.11646728515625
	model : 0.06939411163330078
			 train-loss:  2.0718574696573717 	 ± 0.23080915105510608
	data : 0.11666450500488282
	model : 0.06913833618164063
			 train-loss:  2.0717505959615314 	 ± 0.23002095071382775
	data : 0.11673135757446289
	model : 0.06919126510620117
			 train-loss:  2.0740704885145433 	 ± 0.23094472471019992
	data : 0.11671237945556641
	model : 0.06889448165893555
			 train-loss:  2.0746046248319985 	 ± 0.23025427344154772
	data : 0.1170121669769287
	model : 0.0690415382385254
			 train-loss:  2.0738239856374343 	 ± 0.22967673430187813
	data : 0.11671695709228516
	model : 0.06890988349914551
			 train-loss:  2.0746643964449563 	 ± 0.22913961578667041
	data : 0.11681747436523438
	model : 0.06881241798400879
			 train-loss:  2.076495357696584 	 ± 0.22947791048026686
	data : 0.11704559326171875
	model : 0.06876955032348633
			 train-loss:  2.0780369725666548 	 ± 0.22950495928113504
	data : 0.11693310737609863
	model : 0.06921792030334473
			 train-loss:  2.078275855849771 	 ± 0.22877267181750544
	data : 0.11678128242492676
	model : 0.0696624755859375
			 train-loss:  2.079251836646687 	 ± 0.22834803159602998
	data : 0.11640334129333496
	model : 0.06998796463012695
			 train-loss:  2.081732315401877 	 ± 0.22968227086199455
	data : 0.11602182388305664
	model : 0.07049102783203125
			 train-loss:  2.0815922365738797 	 ± 0.2289515683562867
	data : 0.11563835144042969
	model : 0.06981401443481446
			 train-loss:  2.082881957102733 	 ± 0.22878905119102125
	data : 0.11627812385559082
	model : 0.06986923217773437
			 train-loss:  2.0805144551434096 	 ± 0.22998506771588248
	data : 0.11623673439025879
	model : 0.06912093162536621
			 train-loss:  2.0791031242166675 	 ± 0.22994604422836842
	data : 0.11705822944641113
	model : 0.06894435882568359
			 train-loss:  2.0799464382231236 	 ± 0.22947285404719914
	data : 0.11734046936035156
	model : 0.06810936927795411
			 train-loss:  2.0802858534807003 	 ± 0.2287993797107875
	data : 0.11788201332092285
	model : 0.06884264945983887
			 train-loss:  2.081347950446753 	 ± 0.22848988875050744
	data : 0.11725196838378907
	model : 0.0679046630859375
			 train-loss:  2.0791056521831113 	 ± 0.2295688504581352
	data : 0.11788058280944824
	model : 0.0687939167022705
			 train-loss:  2.0790886820816414 	 ± 0.22886797778395843
	data : 0.11714296340942383
	model : 0.06874194145202636
			 train-loss:  2.079606121236628 	 ± 0.2282695846281313
	data : 0.11703462600708008
	model : 0.06913518905639648
			 train-loss:  2.0794157550995607 	 ± 0.2275941236833127
	data : 0.11670126914978027
	model : 0.06898436546325684
			 train-loss:  2.0790051071943636 	 ± 0.22697335490146867
	data : 0.11670598983764649
	model : 0.06882848739624023
			 train-loss:  2.079450637102127 	 ± 0.22637006113927358
	data : 0.11699118614196777
	model : 0.06865477561950684
			 train-loss:  2.0789911083921173 	 ± 0.2257779111499077
	data : 0.1170647144317627
	model : 0.0678412914276123
			 train-loss:  2.082286261109745 	 ± 0.22915238515320938
	data : 0.11805200576782227
	model : 0.06773161888122559
			 train-loss:  2.081995510915567 	 ± 0.2285128134112345
	data : 0.11825509071350097
	model : 0.06795358657836914
			 train-loss:  2.0827460524647736 	 ± 0.22805884922915373
	data : 0.11813144683837891
	model : 0.06892256736755371
			 train-loss:  2.0834035721817457 	 ± 0.22756220942894403
	data : 0.11720447540283203
	model : 0.06886143684387207
			 train-loss:  2.0854038465982194 	 ± 0.2284275304345857
	data : 0.11732902526855468
	model : 0.06888995170593262
			 train-loss:  2.088497472490583 	 ± 0.23140060684322333
	data : 0.11720232963562012
	model : 0.06939487457275391
			 train-loss:  2.087388824332844 	 ± 0.2312079009715874
	data : 0.11683945655822754
	model : 0.06926560401916504
			 train-loss:  2.0863920079786227 	 ± 0.23093279787634727
	data : 0.11684098243713378
	model : 0.06888041496276856
			 train-loss:  2.087123077906919 	 ± 0.23048850432880608
	data : 0.1172780990600586
	model : 0.06803355216979981
			 train-loss:  2.0855643556104693 	 ± 0.23078265684232385
	data : 0.11801333427429199
	model : 0.06901984214782715
			 train-loss:  2.086096876859665 	 ± 0.23025095635822557
	data : 0.1170999526977539
	model : 0.06795930862426758
			 train-loss:  2.0876221057459796 	 ± 0.23052405024407654
	data : 0.11808114051818848
	model : 0.0679718017578125
			 train-loss:  2.0852854435260477 	 ± 0.23202933137530393
	data : 0.11828746795654296
	model : 0.0679025650024414
			 train-loss:  2.084308601467987 	 ± 0.23176946334427054
	data : 0.11821632385253907
	model : 0.06883153915405274
			 train-loss:  2.084511596871459 	 ± 0.23115510889814717
	data : 0.11741113662719727
	model : 0.06852083206176758
			 train-loss:  2.084783540545283 	 ± 0.2305590303314008
	data : 0.1176417350769043
	model : 0.06943440437316895
			 train-loss:  2.083943444554524 	 ± 0.23022215243655156
	data : 0.11667461395263672
	model : 0.06937837600708008
			 train-loss:  2.0813355484110785 	 ± 0.23234417096993437
	data : 0.11676273345947266
	model : 0.06950454711914063
			 train-loss:  2.0812812587048146 	 ± 0.23172659967438064
	data : 0.11659474372863769
	model : 0.06947550773620606
			 train-loss:  2.0815365238795205 	 ± 0.23113925429398538
	data : 0.1165764331817627
	model : 0.06906170845031738
			 train-loss:  2.0808716410084775 	 ± 0.23071133453338208
	data : 0.1169044017791748
	model : 0.06879429817199707
			 train-loss:  2.0801091687217435 	 ± 0.23034647789409438
	data : 0.11710638999938965
	model : 0.06902527809143066
			 train-loss:  2.0805959459394217 	 ± 0.22984430852153395
	data : 0.11689743995666504
	model : 0.06881132125854492
			 train-loss:  2.0811523149668245 	 ± 0.22937767279098234
	data : 0.1172070026397705
	model : 0.06878833770751953
			 train-loss:  2.0811377794472214 	 ± 0.22878581853953195
	data : 0.1171839714050293
	model : 0.06930727958679199
			 train-loss:  2.0796134679745406 	 ± 0.22918396391680843
	data : 0.11688876152038574
	model : 0.06950316429138184
			 train-loss:  2.079905618210228 	 ± 0.228634963948823
	data : 0.11669821739196777
	model : 0.06938323974609376
			 train-loss:  2.0779540895810586 	 ± 0.22968468755787821
	data : 0.11683306694030762
	model : 0.07001423835754395
			 train-loss:  2.0776490015212934 	 ± 0.22914395591414688
	data : 0.11647815704345703
	model : 0.07005743980407715
			 train-loss:  2.0769442397745412 	 ± 0.22878252325048393
	data : 0.11627197265625
	model : 0.0702817440032959
			 train-loss:  2.0759808254241943 	 ± 0.22861417498706035
	data : 0.1160776138305664
	model : 0.0702737808227539
			 train-loss:  2.0770810955199437 	 ± 0.2285750156232784
	data : 0.116019868850708
	model : 0.06928548812866211
			 train-loss:  2.0781265213938043 	 ± 0.22848975361581744
	data : 0.11666479110717773
	model : 0.06904935836791992
			 train-loss:  2.078988162167554 	 ± 0.22825502663940606
	data : 0.11675686836242676
	model : 0.06810188293457031
			 train-loss:  2.080260411197064 	 ± 0.22841528549941195
	data : 0.11775646209716797
	model : 0.0682755947113037
			 train-loss:  2.079676623460723 	 ± 0.22801000508834296
	data : 0.11754236221313477
	model : 0.0683661937713623
			 train-loss:  2.0817502841208744 	 ± 0.22938549389076562
	data : 0.11762661933898926
	model : 0.06931600570678711
			 train-loss:  2.0807224581207056 	 ± 0.2293057714317078
	data : 0.11685347557067871
	model : 0.06939654350280762
			 train-loss:  2.0800309742872534 	 ± 0.2289701288651841
	data : 0.11693954467773438
	model : 0.0703780174255371
			 train-loss:  2.081949670919391 	 ± 0.23009172522989538
	data : 0.11605372428894042
	model : 0.07009177207946778
			 train-loss:  2.080289164043608 	 ± 0.23079507893374474
	data : 0.11625227928161622
	model : 0.06980996131896973
			 train-loss:  2.0793016538800786 	 ± 0.2306918035908355
	data : 0.11643362045288086
	model : 0.06998233795166016
			 train-loss:  2.0805732579726093 	 ± 0.23088711254985625
	data : 0.11637134552001953
	model : 0.07013502120971679
			 train-loss:  2.080928359792826 	 ± 0.23040250657852357
	data : 0.11609892845153809
	model : 0.07008552551269531
			 train-loss:  2.079528188036981 	 ± 0.23077009177823224
	data : 0.11609277725219727
	model : 0.06959524154663085
			 train-loss:  2.0799770887507947 	 ± 0.23032642463954367
	data : 0.11655354499816895
	model : 0.06952567100524902
			 train-loss:  2.0778674102491803 	 ± 0.23186541063660232
	data : 0.11671719551086426
	model : 0.06977386474609375
			 train-loss:  2.077015168106501 	 ± 0.23166938487004238
	data : 0.11667337417602539
	model : 0.06891522407531739
			 train-loss:  2.078747763546235 	 ± 0.23254229178048016
	data : 0.11754369735717773
	model : 0.06789703369140625
			 train-loss:  2.079111428021296 	 ± 0.23207289021414332
	data : 0.118328857421875
	model : 0.0681952953338623
			 train-loss:  2.0786423683166504 	 ± 0.23164887609285245
	data : 0.11803646087646484
	model : 0.06832075119018555
			 train-loss:  2.080797881562246 	 ± 0.2333250161141256
	data : 0.11773152351379394
	model : 0.06718721389770507
			 train-loss:  2.080214876849372 	 ± 0.23296019416670868
	data : 0.11844143867492676
	model : 0.06731748580932617
			 train-loss:  2.0802965009159036 	 ± 0.23244045663928012
	data : 0.11826796531677246
	model : 0.0682377815246582
			 train-loss:  2.0791397893003056 	 ± 0.2325634039583481
	data : 0.11749386787414551
	model : 0.0676997184753418
			 train-loss:  2.079245204925537 	 ± 0.2320513843907966
	data : 0.11789917945861816
	model : 0.0668344497680664
			 train-loss:  2.079694485242388 	 ± 0.23163548320045907
	data : 0.1185903549194336
	model : 0.0667635440826416
			 train-loss:  2.079398429341253 	 ± 0.23116755859646243
	data : 0.11865315437316895
	model : 0.06697573661804199
			 train-loss:  2.0790063409428847 	 ± 0.23073568966644462
	data : 0.11829018592834473
	model : 0.06674065589904785
			 train-loss:  2.078635496343588 	 ± 0.2302994351152731
	data : 0.11871538162231446
	model : 0.06674885749816895
			 train-loss:  2.0793740298437036 	 ± 0.23006984683070247
	data : 0.11882600784301758
	model : 0.06754684448242188
			 train-loss:  2.0799560913275847 	 ± 0.22974097145042793
	data : 0.11819205284118653
	model : 0.06791200637817382
			 train-loss:  2.078404892107536 	 ± 0.23045443501303087
	data : 0.11794080734252929
	model : 0.06770944595336914
			 train-loss:  2.0798942080894767 	 ± 0.23107552951846183
	data : 0.11808900833129883
	model : 0.06796355247497558
			 train-loss:  2.0803809879172563 	 ± 0.2307009387841935
	data : 0.11770291328430176
	model : 0.06867880821228027
			 train-loss:  2.0802941210726473 	 ± 0.23021339743905977
	data : 0.11709623336791992
	model : 0.06867241859436035
			 train-loss:  2.082408979787665 	 ± 0.23200152395252752
	data : 0.11709084510803222
	model : 0.06881775856018066
			 train-loss:  2.0817778608467004 	 ± 0.2317144796321845
	data : 0.1166238784790039
	model : 0.0689241886138916
			 train-loss:  2.082521527254281 	 ± 0.2315104220719611
	data : 0.11640901565551758
	model : 0.06838040351867676
			 train-loss:  2.0816451730089707 	 ± 0.23142083573320413
	data : 0.11676435470581055
	model : 0.06781578063964844
			 train-loss:  2.0811023066441217 	 ± 0.2310906512485314
	data : 0.11718163490295411
	model : 0.06772561073303222
			 train-loss:  2.080972907948791 	 ± 0.23061942439527042
	data : 0.11725945472717285
	model : 0.06771554946899414
			 train-loss:  2.081283037327538 	 ± 0.23019279810440693
	data : 0.11763291358947754
	model : 0.06792640686035156
			 train-loss:  2.0799947329509405 	 ± 0.23059123556993336
	data : 0.11732258796691894
	model : 0.06828241348266602
			 train-loss:  2.0795386956363426 	 ± 0.23022800720975328
	data : 0.11700806617736817
	model : 0.06825647354125977
			 train-loss:  2.080248286286179 	 ± 0.23002488395455528
	data : 0.11678194999694824
	model : 0.06792745590209961
			 train-loss:  2.079069557713299 	 ± 0.23029711963309304
	data : 0.11717705726623535
	model : 0.06776719093322754
			 train-loss:  2.0792979059914347 	 ± 0.2298583623353392
	data : 0.11723732948303223
	model : 0.06769523620605469
			 train-loss:  2.078901259168502 	 ± 0.22947915615025152
	data : 0.11763410568237305
	model : 0.0678788661956787
			 train-loss:  2.0807830435205177 	 ± 0.2309272402347349
	data : 0.11772294044494629
	model : 0.06835236549377441
			 train-loss:  2.0819799938201906 	 ± 0.23123758460997007
	data : 0.11739149093627929
	model : 0.06809720993041993
			 train-loss:  2.0811608198629434 	 ± 0.23113967827224324
	data : 0.11754450798034669
	model : 0.0679563045501709
			 train-loss:  2.0812792721248807 	 ± 0.23068824522084097
	data : 0.11788263320922851
	model : 0.06778545379638672
			 train-loss:  2.0802694087914326 	 ± 0.23078933680466987
	data : 0.1178769588470459
	model : 0.06710796356201172
			 train-loss:  2.081147450631059 	 ± 0.23075760127478345
	data : 0.11859750747680664
	model : 0.0664205551147461
			 train-loss:  2.0817147614909155 	 ± 0.2304821000229129
	data : 0.11909527778625488
	model : 0.06685304641723633
			 train-loss:  2.079586452804506 	 ± 0.23252862959232742
	data : 0.11758370399475097
	model : 0.05818042755126953
#epoch  63    val-loss:  2.4230590933247615  train-loss:  2.079586452804506  lr:  3.90625e-05
			 train-loss:  1.8808003664016724 	 ± 0.0
	data : 5.506483793258667
	model : 0.0920114517211914
			 train-loss:  1.8980372548103333 	 ± 0.01723688840866089
	data : 2.8471213579177856
	model : 0.08051431179046631
			 train-loss:  2.0834779342015586 	 ± 0.2626300909914299
	data : 1.9370230038960774
	model : 0.07690111796061198
			 train-loss:  2.049762487411499 	 ± 0.23482145893717857
	data : 1.4815268516540527
	model : 0.07413476705551147
			 train-loss:  2.0489612102508543 	 ± 0.21003681163610516
	data : 1.2091856002807617
	model : 0.07262320518493652
			 train-loss:  2.0888235569000244 	 ± 0.21144247114469902
	data : 0.13166847229003906
	model : 0.06732864379882812
			 train-loss:  2.095039095197405 	 ± 0.19634884921208182
	data : 0.11800265312194824
	model : 0.06673908233642578
			 train-loss:  2.0732899755239487 	 ± 0.19247059829890267
	data : 0.11833376884460449
	model : 0.0668790340423584
			 train-loss:  2.0626693699094982 	 ± 0.1839326122544686
	data : 0.11824350357055664
	model : 0.06772003173828126
			 train-loss:  2.0369149684906005 	 ± 0.19083418980279443
	data : 0.11750755310058594
	model : 0.06833758354187011
			 train-loss:  2.0002232898365366 	 ± 0.21580032880730043
	data : 0.11711397171020507
	model : 0.06854424476623536
			 train-loss:  1.99958931406339 	 ± 0.20662378355838995
	data : 0.1171539306640625
	model : 0.06936712265014648
			 train-loss:  2.023342875333933 	 ± 0.21489546585348576
	data : 0.11651577949523925
	model : 0.06930017471313477
			 train-loss:  2.0374499644551958 	 ± 0.2132337097327591
	data : 0.11673707962036133
	model : 0.06974620819091797
			 train-loss:  2.073669505119324 	 ± 0.24658334439195342
	data : 0.11633825302124023
	model : 0.06903386116027832
			 train-loss:  2.0809010192751884 	 ± 0.2403904294644397
	data : 0.11687245368957519
	model : 0.07005658149719238
			 train-loss:  2.081604095066295 	 ± 0.2332299283443992
	data : 0.11611542701721192
	model : 0.07004566192626953
			 train-loss:  2.0654856165250144 	 ± 0.23620093004011827
	data : 0.11638226509094238
	model : 0.06968488693237304
			 train-loss:  2.0719269137633476 	 ± 0.2315196381793446
	data : 0.11692094802856445
	model : 0.06904182434082032
			 train-loss:  2.0694423615932465 	 ± 0.2259171589538808
	data : 0.11752433776855468
	model : 0.06980171203613281
			 train-loss:  2.05255765574319 	 ± 0.23304510463548922
	data : 0.11685867309570312
	model : 0.0694582462310791
			 train-loss:  2.0626754706556145 	 ± 0.2323599733955181
	data : 0.11710305213928222
	model : 0.06942024230957031
			 train-loss:  2.06635640496793 	 ± 0.22790743673791822
	data : 0.1169546127319336
	model : 0.06973581314086914
			 train-loss:  2.058885484933853 	 ± 0.2259674643107004
	data : 0.11644811630249023
	model : 0.06975259780883789
			 train-loss:  2.055208830833435 	 ± 0.22213345083284114
	data : 0.11614913940429687
	model : 0.06886515617370606
			 train-loss:  2.0681953751123867 	 ± 0.22729212527169493
	data : 0.11698651313781738
	model : 0.068870210647583
			 train-loss:  2.0687622979835227 	 ± 0.22306203167761934
	data : 0.11676068305969238
	model : 0.06895627975463867
			 train-loss:  2.0753314111913954 	 ± 0.22168622815490718
	data : 0.1167485237121582
	model : 0.06893863677978515
			 train-loss:  2.077831872578325 	 ± 0.21823198811824232
	data : 0.11673870086669921
	model : 0.06903014183044434
			 train-loss:  2.0722437659899393 	 ± 0.2166639680984301
	data : 0.11685447692871094
	model : 0.06970939636230469
			 train-loss:  2.064792279274233 	 ± 0.2170131689656459
	data : 0.11610999107360839
	model : 0.06970291137695313
			 train-loss:  2.064187116920948 	 ± 0.21362199912622487
	data : 0.11617507934570312
	model : 0.06949381828308106
			 train-loss:  2.05351253711816 	 ± 0.2188556421223607
	data : 0.1164736270904541
	model : 0.0695263385772705
			 train-loss:  2.055114234195036 	 ± 0.21580938998177165
	data : 0.116585111618042
	model : 0.0696859359741211
			 train-loss:  2.062431158338274 	 ± 0.21694074479047787
	data : 0.1165247917175293
	model : 0.06903748512268067
			 train-loss:  2.061772406101227 	 ± 0.21394195857112877
	data : 0.11708869934082031
	model : 0.06810951232910156
			 train-loss:  2.0727857641271643 	 ± 0.22113500219298507
	data : 0.11778073310852051
	model : 0.06803202629089355
			 train-loss:  2.0665543204859684 	 ± 0.22147363698078495
	data : 0.11780695915222168
	model : 0.06785526275634765
			 train-loss:  2.0690213472415238 	 ± 0.219144110814684
	data : 0.11799893379211426
	model : 0.06757292747497559
			 train-loss:  2.0779218316078185 	 ± 0.22341231843470744
	data : 0.11815476417541504
	model : 0.06768717765808105
			 train-loss:  2.071430651153006 	 ± 0.2244573214904754
	data : 0.11793413162231445
	model : 0.06864991188049316
			 train-loss:  2.078971905367715 	 ± 0.2269652628683187
	data : 0.11724648475646973
	model : 0.06883859634399414
			 train-loss:  2.083488189896872 	 ± 0.22621210209209777
	data : 0.11696004867553711
	model : 0.06833267211914062
			 train-loss:  2.0960033563050358 	 ± 0.23820995118037397
	data : 0.1174323558807373
	model : 0.06853046417236328
			 train-loss:  2.092162839571635 	 ± 0.23692189539091862
	data : 0.11709704399108886
	model : 0.06899213790893555
			 train-loss:  2.0871853595194607 	 ± 0.2366994145067063
	data : 0.11694087982177734
	model : 0.0689307689666748
			 train-loss:  2.0889501698473665 	 ± 0.2344735097797543
	data : 0.11696333885192871
	model : 0.06803030967712402
			 train-loss:  2.0927862798174224 	 ± 0.23350395106511945
	data : 0.11783838272094727
	model : 0.06850700378417969
			 train-loss:  2.091892677910474 	 ± 0.23119188501483035
	data : 0.11733036041259766
	model : 0.06845312118530274
			 train-loss:  2.0955289959907533 	 ± 0.23027942068852716
	data : 0.1175161361694336
	model : 0.0687490463256836
			 train-loss:  2.0910590676700367 	 ± 0.23019089437761397
	data : 0.11711044311523437
	model : 0.06838760375976563
			 train-loss:  2.0963898713772116 	 ± 0.23112365014560743
	data : 0.11750884056091308
	model : 0.06875138282775879
			 train-loss:  2.097513603714277 	 ± 0.22907622412290812
	data : 0.11721086502075195
	model : 0.06900897026062011
			 train-loss:  2.0869134355474404 	 ± 0.23970692351589362
	data : 0.11696271896362305
	model : 0.06903643608093261
			 train-loss:  2.091156075217507 	 ± 0.23955519653070353
	data : 0.1168900489807129
	model : 0.06898374557495117
			 train-loss:  2.088563037770135 	 ± 0.2381842589270144
	data : 0.11708483695983887
	model : 0.06929187774658203
			 train-loss:  2.093226516455935 	 ± 0.23865107870843577
	data : 0.11661195755004883
	model : 0.07014298439025879
			 train-loss:  2.095378304349965 	 ± 0.23714191306006607
	data : 0.1158034324645996
	model : 0.07020387649536133
			 train-loss:  2.1060762486215365 	 ± 0.24883930195847664
	data : 0.11581249237060547
	model : 0.06927547454833985
			 train-loss:  2.107386124134064 	 ± 0.24696196512857943
	data : 0.11666851043701172
	model : 0.06900587081909179
			 train-loss:  2.106830804074397 	 ± 0.24496709064218572
	data : 0.11678133010864258
	model : 0.06905884742736816
			 train-loss:  2.1015283830704226 	 ± 0.24648741588107664
	data : 0.11685752868652344
	model : 0.0685697078704834
			 train-loss:  2.1004864147731235 	 ± 0.24466094452125176
	data : 0.11721034049987793
	model : 0.0682558536529541
			 train-loss:  2.0963392090052366 	 ± 0.24496374861237546
	data : 0.1174710750579834
	model : 0.0688323974609375
			 train-loss:  2.113100713949937 	 ± 0.2776053402367414
	data : 0.11699810028076171
	model : 0.0682042121887207
			 train-loss:  2.1114953088037893 	 ± 0.2757981213677436
	data : 0.11740193367004395
	model : 0.06792378425598145
			 train-loss:  2.1063204719059505 	 ± 0.2769417285158796
	data : 0.11762118339538574
	model : 0.06763081550598145
			 train-loss:  2.107840746641159 	 ± 0.2751793611459676
	data : 0.11791706085205078
	model : 0.06794958114624024
			 train-loss:  2.1034955097281416 	 ± 0.27551796647745924
	data : 0.11748557090759278
	model : 0.06846785545349121
			 train-loss:  2.1059249350002833 	 ± 0.2742862820726847
	data : 0.11716504096984863
	model : 0.06936860084533691
			 train-loss:  2.1035156333950202 	 ± 0.2730927987154489
	data : 0.11632990837097168
	model : 0.06959190368652343
			 train-loss:  2.099013571937879 	 ± 0.2738300849794893
	data : 0.11624689102172851
	model : 0.069205904006958
			 train-loss:  2.0949966172649437 	 ± 0.274075790254208
	data : 0.11669983863830566
	model : 0.06882314682006836
			 train-loss:  2.0948019978162407 	 ± 0.2722227064905175
	data : 0.11703391075134277
	model : 0.06804060935974121
			 train-loss:  2.0914828824996947 	 ± 0.2719050487391212
	data : 0.11771121025085449
	model : 0.06819672584533691
			 train-loss:  2.0955704717259658 	 ± 0.2724200622280234
	data : 0.11784229278564454
	model : 0.06833896636962891
			 train-loss:  2.0897129619276367 	 ± 0.2754205532097927
	data : 0.11751303672790528
	model : 0.06833434104919434
			 train-loss:  2.0864912225649905 	 ± 0.275105782726646
	data : 0.1175316333770752
	model : 0.06852374076843262
			 train-loss:  2.0829052155530907 	 ± 0.27518759676906684
	data : 0.11741676330566406
	model : 0.0692216396331787
			 train-loss:  2.0825905546545984 	 ± 0.27347656692090183
	data : 0.11676030158996582
	model : 0.06809759140014648
			 train-loss:  2.0796732549314147 	 ± 0.273032886919802
	data : 0.11760354042053223
	model : 0.06710228919982911
			 train-loss:  2.081161510653612 	 ± 0.27169331132101426
	data : 0.11853909492492676
	model : 0.06711792945861816
			 train-loss:  2.0831940920956162 	 ± 0.2706781569569171
	data : 0.11832737922668457
	model : 0.06713566780090333
			 train-loss:  2.086423337459564 	 ± 0.270665787608712
	data : 0.11843414306640625
	model : 0.06719040870666504
			 train-loss:  2.0804593563079834 	 ± 0.27456490776744785
	data : 0.11830329895019531
	model : 0.06830558776855469
			 train-loss:  2.0804285393204798 	 ± 0.27296408042264764
	data : 0.11774287223815919
	model : 0.06831989288330079
			 train-loss:  2.080810089220946 	 ± 0.2714138525354477
	data : 0.1176149845123291
	model : 0.0691253662109375
			 train-loss:  2.08154142444784 	 ± 0.2699535213400539
	data : 0.11696667671203613
	model : 0.06829519271850586
			 train-loss:  2.0809771264537 	 ± 0.26848483491118086
	data : 0.11755967140197754
	model : 0.06731295585632324
			 train-loss:  2.078910375965966 	 ± 0.26770007804303936
	data : 0.11849331855773926
	model : 0.06699147224426269
			 train-loss:  2.0791089495459754 	 ± 0.2662318002714253
	data : 0.11862211227416992
	model : 0.06788330078125
			 train-loss:  2.0763439536094666 	 ± 0.2660914447235602
	data : 0.11791515350341797
	model : 0.06710467338562012
			 train-loss:  2.075871236862675 	 ± 0.26469581594916325
	data : 0.11854863166809082
	model : 0.0680150032043457
			 train-loss:  2.077383505537155 	 ± 0.2636876970346355
	data : 0.11783909797668457
	model : 0.06875925064086914
			 train-loss:  2.0741045424812716 	 ± 0.2642157157747489
	data : 0.11705026626586915
	model : 0.06909265518188476
			 train-loss:  2.0718083617587886 	 ± 0.2637871109481259
	data : 0.11657323837280273
	model : 0.06830544471740722
			 train-loss:  2.070751385590465 	 ± 0.2626281284583245
	data : 0.11749606132507324
	model : 0.06888403892517089
			 train-loss:  2.0731916610075505 	 ± 0.26238778909533306
	data : 0.11698722839355469
	model : 0.06857709884643555
			 train-loss:  2.0768642774736037 	 ± 0.26357875296951877
	data : 0.11721382141113282
	model : 0.06865053176879883
			 train-loss:  2.0786851012706755 	 ± 0.26288257066130355
	data : 0.11721076965332031
	model : 0.06858706474304199
			 train-loss:  2.0808388041977834 	 ± 0.26246306298091465
	data : 0.11731672286987305
	model : 0.06847386360168457
			 train-loss:  2.076476932740679 	 ± 0.26482657768020196
	data : 0.11732020378112792
	model : 0.06887397766113282
			 train-loss:  2.0794027696535426 	 ± 0.2651893387976512
	data : 0.11710958480834961
	model : 0.06914281845092773
			 train-loss:  2.079932296505341 	 ± 0.263966022146246
	data : 0.11689696311950684
	model : 0.06922726631164551
			 train-loss:  2.0799144506454468 	 ± 0.2627060969783168
	data : 0.11686968803405762
	model : 0.06887030601501465
			 train-loss:  2.086697093720706 	 ± 0.27054363641995355
	data : 0.11704750061035156
	model : 0.06948399543762207
			 train-loss:  2.0843733170322167 	 ± 0.27033719294016745
	data : 0.1164865493774414
	model : 0.06936087608337402
			 train-loss:  2.084984259472953 	 ± 0.26915692205579583
	data : 0.11630616188049317
	model : 0.06947340965270996
			 train-loss:  2.0876267164125357 	 ± 0.2693230960088179
	data : 0.11614627838134765
	model : 0.06955142021179199
			 train-loss:  2.086318628354506 	 ± 0.26844372012564477
	data : 0.11616802215576172
	model : 0.06964282989501953
			 train-loss:  2.0863329258051007 	 ± 0.26723182047693417
	data : 0.11605029106140137
	model : 0.06985936164855958
			 train-loss:  2.092519586639745 	 ± 0.27390460182973314
	data : 0.11586790084838867
	model : 0.06906156539916992
			 train-loss:  2.0920327754147285 	 ± 0.2727386045653522
	data : 0.11675558090209961
	model : 0.06895790100097657
			 train-loss:  2.0935473578018056 	 ± 0.27201663932151304
	data : 0.116856050491333
	model : 0.06791601181030274
			 train-loss:  2.0944932471150937 	 ± 0.271019613266833
	data : 0.11763215065002441
	model : 0.06811742782592774
			 train-loss:  2.096652073079142 	 ± 0.2708401520439209
	data : 0.11756939888000488
	model : 0.06829366683959961
			 train-loss:  2.103121656637925 	 ± 0.27853665235210917
	data : 0.11724309921264649
	model : 0.06918368339538575
			 train-loss:  2.1012392326936884 	 ± 0.2781003025999821
	data : 0.11648588180541992
	model : 0.06925077438354492
			 train-loss:  2.1009646163267246 	 ± 0.2769454153301164
	data : 0.11637940406799316
	model : 0.07011442184448242
			 train-loss:  2.1011107405026754 	 ± 0.2757936685896454
	data : 0.11565985679626464
	model : 0.07007894515991211
			 train-loss:  2.0993187200924583 	 ± 0.2753523112338928
	data : 0.11571006774902344
	model : 0.07003440856933593
			 train-loss:  2.0971100691889153 	 ± 0.27529562972080324
	data : 0.11577963829040527
	model : 0.07010612487792969
			 train-loss:  2.097841591369815 	 ± 0.27429328997713337
	data : 0.11588468551635742
	model : 0.07009553909301758
			 train-loss:  2.1001539259187636 	 ± 0.2743860930669509
	data : 0.1160494327545166
	model : 0.07010345458984375
			 train-loss:  2.097837640762329 	 ± 0.27450083457580066
	data : 0.11596980094909667
	model : 0.0699953556060791
			 train-loss:  2.101044085290697 	 ± 0.27574961016016714
	data : 0.11617054939270019
	model : 0.06917839050292969
			 train-loss:  2.1024365406336747 	 ± 0.2751062153277365
	data : 0.11700491905212403
	model : 0.06905512809753418
			 train-loss:  2.0992396380752325 	 ± 0.2763876200832527
	data : 0.1171198844909668
	model : 0.06911487579345703
			 train-loss:  2.0980143537817075 	 ± 0.27566304477399434
	data : 0.11722111701965332
	model : 0.0691004753112793
			 train-loss:  2.09620855313081 	 ± 0.2753656352496878
	data : 0.1173180103302002
	model : 0.0691767692565918
			 train-loss:  2.097993216441788 	 ± 0.27506628269770733
	data : 0.11726365089416504
	model : 0.07013382911682128
			 train-loss:  2.0959685706731044 	 ± 0.27500047446011944
	data : 0.11645565032958985
	model : 0.06966438293457031
			 train-loss:  2.098589523394305 	 ± 0.2756146038705007
	data : 0.11690707206726074
	model : 0.069537353515625
			 train-loss:  2.0986049824686193 	 ± 0.27458432317417725
	data : 0.11668605804443359
	model : 0.0695727825164795
			 train-loss:  2.0974268939759995 	 ± 0.27390515723628334
	data : 0.1166001319885254
	model : 0.06888046264648437
			 train-loss:  2.0971578333307717 	 ± 0.272914200772761
	data : 0.11716275215148926
	model : 0.06871428489685058
			 train-loss:  2.0981382375215962 	 ± 0.2721566051708488
	data : 0.11728453636169434
	model : 0.06818766593933105
			 train-loss:  2.0979093961093738 	 ± 0.27118196611490913
	data : 0.11773209571838379
	model : 0.06814231872558593
			 train-loss:  2.0980234274761282 	 ± 0.27020805111593316
	data : 0.11765503883361816
	model : 0.06817522048950195
			 train-loss:  2.0972899547645025 	 ± 0.26938012795768645
	data : 0.1176374912261963
	model : 0.06897592544555664
			 train-loss:  2.0967434120516404 	 ± 0.268501066228795
	data : 0.11687374114990234
	model : 0.06901803016662597
			 train-loss:  2.0952961864605757 	 ± 0.26810528857512017
	data : 0.11675438880920411
	model : 0.07002468109130859
			 train-loss:  2.0952427720690108 	 ± 0.26716697109089066
	data : 0.11583085060119629
	model : 0.0692530632019043
			 train-loss:  2.0964587893750934 	 ± 0.26663451087677154
	data : 0.11677889823913574
	model : 0.06906914710998535
			 train-loss:  2.0969275326564394 	 ± 0.2657730214291576
	data : 0.11706767082214356
	model : 0.06825199127197265
			 train-loss:  2.096374129595822 	 ± 0.26494509331126265
	data : 0.11775059700012207
	model : 0.06801462173461914
			 train-loss:  2.094956886200678 	 ± 0.26459711229425537
	data : 0.11800236701965332
	model : 0.06804695129394531
			 train-loss:  2.0948695363225163 	 ± 0.2637038146826631
	data : 0.1181182861328125
	model : 0.06799898147583008
			 train-loss:  2.0929256989651877 	 ± 0.26387915986873284
	data : 0.11798810958862305
	model : 0.06820025444030761
			 train-loss:  2.092627116839091 	 ± 0.2630233446189883
	data : 0.11772441864013672
	model : 0.06899781227111816
			 train-loss:  2.094815104212982 	 ± 0.2635170183419805
	data : 0.11699256896972657
	model : 0.06912593841552735
			 train-loss:  2.0937457147397494 	 ± 0.26297728324287395
	data : 0.11682233810424805
	model : 0.06901988983154297
			 train-loss:  2.09560525027755 	 ± 0.26311716256551265
	data : 0.11682953834533691
	model : 0.06949424743652344
			 train-loss:  2.0944275229008165 	 ± 0.26266577433977834
	data : 0.11647143363952636
	model : 0.06946625709533691
			 train-loss:  2.09297827136132 	 ± 0.26243407116965994
	data : 0.11639065742492676
	model : 0.06966462135314941
			 train-loss:  2.09313417550845 	 ± 0.26159878492435834
	data : 0.11624746322631836
	model : 0.06978435516357422
			 train-loss:  2.0921799696175154 	 ± 0.2610365468503443
	data : 0.116176176071167
	model : 0.06972699165344239
			 train-loss:  2.0925791218310974 	 ± 0.26025723064106165
	data : 0.1162644386291504
	model : 0.06992068290710449
			 train-loss:  2.0931744020689957 	 ± 0.25954540228522144
	data : 0.11617670059204102
	model : 0.06976637840270997
			 train-loss:  2.0945928171277046 	 ± 0.2593505047260867
	data : 0.11644296646118164
	model : 0.06958370208740235
			 train-loss:  2.093219015909278 	 ± 0.2591271433483562
	data : 0.11667938232421875
	model : 0.0695563793182373
			 train-loss:  2.0919934996852168 	 ± 0.25879372832602443
	data : 0.116691255569458
	model : 0.06978979110717773
			 train-loss:  2.0924855258567203 	 ± 0.25807465535436463
	data : 0.11637678146362304
	model : 0.07008981704711914
			 train-loss:  2.094605155107452 	 ± 0.25870590590947695
	data : 0.11601076126098633
	model : 0.07020721435546876
			 train-loss:  2.0941787647478507 	 ± 0.25797855264239283
	data : 0.11585068702697754
	model : 0.07017168998718262
			 train-loss:  2.093541905822524 	 ± 0.2573303996218719
	data : 0.11593413352966309
	model : 0.07009129524230957
			 train-loss:  2.092899942826368 	 ± 0.25669208273795185
	data : 0.11602268218994141
	model : 0.0698051929473877
			 train-loss:  2.0928489842585156 	 ± 0.2559278251933276
	data : 0.11631932258605956
	model : 0.06954374313354492
			 train-loss:  2.093734708058058 	 ± 0.25542764234109117
	data : 0.11661977767944336
	model : 0.06953930854797363
			 train-loss:  2.0961772462900945 	 ± 0.25664712990540267
	data : 0.11662144660949707
	model : 0.06947660446166992
			 train-loss:  2.095421798745094 	 ± 0.25608509706975263
	data : 0.11642284393310547
	model : 0.06956892013549805
			 train-loss:  2.0938167669052303 	 ± 0.2562007378668029
	data : 0.11627087593078614
	model : 0.06888051033020019
			 train-loss:  2.0937404577442678 	 ± 0.25546116049452156
	data : 0.11666197776794433
	model : 0.06926584243774414
			 train-loss:  2.092922969111081 	 ± 0.2549528554286482
	data : 0.11649799346923828
	model : 0.06919660568237304
			 train-loss:  2.0916050461360385 	 ± 0.25481708869531694
	data : 0.11654090881347656
	model : 0.06913981437683106
			 train-loss:  2.09158737144687 	 ± 0.25409225287270737
	data : 0.116782808303833
	model : 0.06905770301818848
			 train-loss:  2.089331729937408 	 ± 0.25513444426954157
	data : 0.11691689491271973
	model : 0.06932945251464843
			 train-loss:  2.0884125420216764 	 ± 0.2547104999681983
	data : 0.11666851043701172
	model : 0.06926164627075196
			 train-loss:  2.0872082690286904 	 ± 0.2545056849551993
	data : 0.11682653427124023
	model : 0.06924858093261718
			 train-loss:  2.0864771829711066 	 ± 0.2539861532008125
	data : 0.11684322357177734
	model : 0.06949439048767089
			 train-loss:  2.0847446898729105 	 ± 0.25434786948376503
	data : 0.11647586822509766
	model : 0.06939811706542968
			 train-loss:  2.084912679352603 	 ± 0.2536582177998478
	data : 0.11660690307617187
	model : 0.07004547119140625
			 train-loss:  2.0885102507846605 	 ± 0.2575780111372347
	data : 0.11614451408386231
	model : 0.06983466148376465
			 train-loss:  2.0873418469791827 	 ± 0.25736293251250686
	data : 0.11609282493591308
	model : 0.06907777786254883
			 train-loss:  2.087300096331416 	 ± 0.25666703930856083
	data : 0.11693768501281739
	model : 0.06883006095886231
			 train-loss:  2.087350389649791 	 ± 0.2559770583086189
	data : 0.11726799011230468
	model : 0.06885919570922852
			 train-loss:  2.0877622531697075 	 ± 0.2553534976617069
	data : 0.11724772453308105
	model : 0.06884584426879883
			 train-loss:  2.08715421595472 	 ± 0.2548091582303031
	data : 0.11732888221740723
	model : 0.06909699440002441
			 train-loss:  2.0848908941581765 	 ± 0.2560219285147781
	data : 0.11710596084594727
	model : 0.0699531078338623
			 train-loss:  2.0829958125164634 	 ± 0.25667295275810526
	data : 0.11637368202209472
	model : 0.0700498104095459
			 train-loss:  2.084733441238004 	 ± 0.25711817347388966
	data : 0.11628413200378418
	model : 0.06994194984436035
			 train-loss:  2.084679465740919 	 ± 0.25644880569016654
	data : 0.1163858413696289
	model : 0.06978049278259277
			 train-loss:  2.086757467200719 	 ± 0.2573991196725512
	data : 0.11634111404418945
	model : 0.06961889266967773
			 train-loss:  2.0861966413320956 	 ± 0.25685305763062327
	data : 0.11641345024108887
	model : 0.06876602172851562
			 train-loss:  2.0844180039870435 	 ± 0.25738860981827677
	data : 0.11721663475036621
	model : 0.06885991096496583
			 train-loss:  2.0849864817395503 	 ± 0.25685386788333914
	data : 0.11724734306335449
	model : 0.0690079689025879
			 train-loss:  2.0851267539910254 	 ± 0.256208651417096
	data : 0.11702446937561035
	model : 0.06901655197143555
			 train-loss:  2.0882999192584646 	 ± 0.25941266744030805
	data : 0.1171499252319336
	model : 0.06897692680358887
			 train-loss:  2.0886819560324126 	 ± 0.25881589026961227
	data : 0.11723480224609376
	model : 0.06988639831542968
			 train-loss:  2.090966973900795 	 ± 0.26017259032395806
	data : 0.11610746383666992
	model : 0.06965017318725586
			 train-loss:  2.092041810946678 	 ± 0.25996935710277796
	data : 0.11622061729431152
	model : 0.06962423324584961
			 train-loss:  2.093814711169441 	 ± 0.2605403428652019
	data : 0.11620512008666992
	model : 0.0694624900817871
			 train-loss:  2.093637361315084 	 ± 0.25991004841393417
	data : 0.1163419246673584
	model : 0.0695220947265625
			 train-loss:  2.094303741758945 	 ± 0.25944601506688725
	data : 0.11632008552551269
	model : 0.06955232620239257
			 train-loss:  2.0950742901825326 	 ± 0.25904634010856425
	data : 0.11641464233398438
	model : 0.06946487426757812
			 train-loss:  2.0953985134374746 	 ± 0.25845851429806777
	data : 0.11660041809082031
	model : 0.06958684921264649
			 train-loss:  2.097413217963804 	 ± 0.2594499094018308
	data : 0.11660137176513671
	model : 0.06915359497070313
			 train-loss:  2.097584108320566 	 ± 0.2588371580506027
	data : 0.11686358451843262
	model : 0.06871423721313477
			 train-loss:  2.0960148048172726 	 ± 0.2592071768721446
	data : 0.11719398498535157
	model : 0.06870946884155274
			 train-loss:  2.096067616485414 	 ± 0.2585904075640213
	data : 0.11739883422851563
	model : 0.06863713264465332
			 train-loss:  2.095609090339516 	 ± 0.25806246512236364
	data : 0.11729617118835449
	model : 0.06790604591369628
			 train-loss:  2.0968190628402636 	 ± 0.2580523469672215
	data : 0.11818900108337402
	model : 0.06770567893981934
			 train-loss:  2.0968926585336245 	 ± 0.2574481077206256
	data : 0.11827549934387208
	model : 0.0671811580657959
			 train-loss:  2.0960434357696602 	 ± 0.257144749001578
	data : 0.11866774559020996
	model : 0.06713142395019531
			 train-loss:  2.0972089584483657 	 ± 0.2571119949864012
	data : 0.11853504180908203
	model : 0.06745634078979493
			 train-loss:  2.09760314170961 	 ± 0.2565812459810024
	data : 0.1184417724609375
	model : 0.06792397499084472
			 train-loss:  2.0967495584268177 	 ± 0.25629657069779566
	data : 0.117730712890625
	model : 0.06865158081054687
			 train-loss:  2.097327818564319 	 ± 0.2558499029909173
	data : 0.11747469902038574
	model : 0.06950969696044922
			 train-loss:  2.0985898895350767 	 ± 0.25594434545667477
	data : 0.11682252883911133
	model : 0.06945652961730957
			 train-loss:  2.098881005157124 	 ± 0.2553983289887759
	data : 0.11679592132568359
	model : 0.06944456100463867
			 train-loss:  2.0975663624198186 	 ± 0.2555648226149758
	data : 0.11664748191833496
	model : 0.06991443634033204
			 train-loss:  2.0968911062489757 	 ± 0.25518609624816535
	data : 0.11633963584899902
	model : 0.06926641464233399
			 train-loss:  2.096739071367033 	 ± 0.2546233639631776
	data : 0.1168027400970459
	model : 0.06937680244445801
			 train-loss:  2.095274272773947 	 ± 0.2549943151727188
	data : 0.11655974388122559
	model : 0.06861968040466308
			 train-loss:  2.0961171499888103 	 ± 0.25473957812581827
	data : 0.1172271728515625
	model : 0.0684023380279541
			 train-loss:  2.0970543084946356 	 ± 0.2545638007789208
	data : 0.11740303039550781
	model : 0.06776251792907714
			 train-loss:  2.096582720458245 	 ± 0.2541013880791987
	data : 0.11775660514831543
	model : 0.06812739372253418
			 train-loss:  2.0952459223437727 	 ± 0.2543422501174238
	data : 0.11714234352111816
	model : 0.06762938499450684
			 train-loss:  2.0945543399544264 	 ± 0.2540010638800482
	data : 0.11761927604675293
	model : 0.06741843223571778
			 train-loss:  2.0956306426421456 	 ± 0.2539710882098366
	data : 0.11773543357849121
	model : 0.06695623397827148
			 train-loss:  2.095726355845794 	 ± 0.2534249281631103
	data : 0.11811361312866211
	model : 0.06663093566894532
			 train-loss:  2.097475272828135 	 ± 0.25427136673701234
	data : 0.11836075782775879
	model : 0.06633567810058594
			 train-loss:  2.098455420891103 	 ± 0.2541639695261834
	data : 0.11863093376159668
	model : 0.06645016670227051
			 train-loss:  2.097399484398019 	 ± 0.25413196052072856
	data : 0.11840615272521973
	model : 0.06740689277648926
			 train-loss:  2.0988625587301053 	 ± 0.2545763718331708
	data : 0.11756219863891601
	model : 0.06741385459899903
			 train-loss:  2.098448731131473 	 ± 0.25411564033540307
	data : 0.11758561134338379
	model : 0.0678131103515625
			 train-loss:  2.0983128185513653 	 ± 0.25358756036739394
	data : 0.11729516983032226
	model : 0.06825222969055175
			 train-loss:  2.0974696463897446 	 ± 0.25338695186245136
	data : 0.11709671020507813
	model : 0.06785416603088379
			 train-loss:  2.0969309477626528 	 ± 0.2529928341379451
	data : 0.11784048080444336
	model : 0.06721205711364746
			 train-loss:  2.0965367515881854 	 ± 0.252538756283907
	data : 0.11849074363708496
	model : 0.06752386093139648
			 train-loss:  2.0964585854304776 	 ± 0.25201718163703357
	data : 0.11827268600463867
	model : 0.06783814430236816
			 train-loss:  2.0954518342806288 	 ± 0.25198110221046366
	data : 0.11801133155822754
	model : 0.0679399013519287
			 train-loss:  2.094544673652806 	 ± 0.2518577640449925
	data : 0.11775412559509277
	model : 0.06845941543579101
			 train-loss:  2.0934795019079426 	 ± 0.251889003940617
	data : 0.11718144416809081
	model : 0.0689122200012207
			 train-loss:  2.091996616246749 	 ± 0.25243938354068124
	data : 0.11675734519958496
	model : 0.06901726722717286
			 train-loss:  2.093444217511309 	 ± 0.25294269043383294
	data : 0.11639976501464844
	model : 0.06799125671386719
			 train-loss:  2.0937192169760888 	 ± 0.25246698786694544
	data : 0.11715469360351563
	model : 0.06744065284729003
			 train-loss:  2.091938177904775 	 ± 0.2535075428269166
	data : 0.11773891448974609
	model : 0.06712255477905274
			 train-loss:  2.091131601467669 	 ± 0.25331663554226186
	data : 0.11795253753662109
	model : 0.06723370552062988
			 train-loss:  2.0926426043510435 	 ± 0.2539313684241464
	data : 0.11784877777099609
	model : 0.06720695495605469
			 train-loss:  2.090883499122711 	 ± 0.25494677032520713
	data : 0.11808409690856933
	model : 0.06798720359802246
			 train-loss:  2.0913508638503058 	 ± 0.25454813607132537
	data : 0.1178515911102295
	model : 0.06849665641784668
			 train-loss:  2.0907498819554746 	 ± 0.25422365153536636
	data : 0.11733684539794922
	model : 0.0684091567993164
			 train-loss:  2.089310285613293 	 ± 0.2547538895090056
	data : 0.11756329536437989
	model : 0.06803712844848633
			 train-loss:  2.090236765730615 	 ± 0.2546822746423417
	data : 0.11799044609069824
	model : 0.06745095252990722
			 train-loss:  2.091521678492427 	 ± 0.25501116620707764
	data : 0.1174816608428955
	model : 0.05858688354492188
#epoch  64    val-loss:  2.463566780090332  train-loss:  2.091521678492427  lr:  3.90625e-05
			 train-loss:  1.9589531421661377 	 ± 0.0
	data : 5.651594161987305
	model : 0.0770881175994873
			 train-loss:  2.0291343927383423 	 ± 0.07018125057220459
	data : 2.8921233415603638
	model : 0.0738527774810791
			 train-loss:  2.018381436665853 	 ± 0.059286232942503764
	data : 1.96664563814799
	model : 0.07167196273803711
			 train-loss:  2.0097968578338623 	 ± 0.053453045222443554
	data : 1.504419982433319
	model : 0.07145112752914429
			 train-loss:  1.9805679082870484 	 ± 0.07551892743731774
	data : 1.226473903656006
	model : 0.07112760543823242
			 train-loss:  2.004083812236786 	 ± 0.08670397353794573
	data : 0.11942405700683593
	model : 0.06947808265686035
			 train-loss:  1.9859483752931868 	 ± 0.09174422927379118
	data : 0.11632976531982422
	model : 0.06940817832946777
			 train-loss:  1.9836968779563904 	 ± 0.08602536158723731
	data : 0.11613831520080567
	model : 0.06994462013244629
			 train-loss:  2.052131493886312 	 ± 0.20986775254589402
	data : 0.1157911777496338
	model : 0.06878881454467774
			 train-loss:  2.0342727184295653 	 ± 0.2061806221499211
	data : 0.1169051170349121
	model : 0.06878609657287597
			 train-loss:  2.060952229933305 	 ± 0.21392481394807117
	data : 0.11688857078552246
	model : 0.06899657249450683
			 train-loss:  2.0690598289171853 	 ± 0.20657501937135261
	data : 0.1167266845703125
	model : 0.06892352104187012
			 train-loss:  2.064256191253662 	 ± 0.19916721875937354
	data : 0.11704888343811035
	model : 0.06862435340881348
			 train-loss:  2.0921684333256314 	 ± 0.21670808644523878
	data : 0.11731896400451661
	model : 0.06961030960083008
			 train-loss:  2.1077932675679523 	 ± 0.21736941975994112
	data : 0.11643686294555664
	model : 0.06888303756713868
			 train-loss:  2.130896955728531 	 ± 0.22869866463464297
	data : 0.1169661045074463
	model : 0.06890640258789063
			 train-loss:  2.111162907936994 	 ± 0.23549384230486178
	data : 0.11684269905090332
	model : 0.06903376579284667
			 train-loss:  2.094643235206604 	 ± 0.2387795612458965
	data : 0.1168673038482666
	model : 0.06935057640075684
			 train-loss:  2.1172402908927515 	 ± 0.2514083099809035
	data : 0.11654815673828126
	model : 0.06942782402038575
			 train-loss:  2.1259299755096435 	 ± 0.24795268332317216
	data : 0.11652927398681641
	model : 0.06968202590942382
			 train-loss:  2.1207528909047446 	 ± 0.24308215243155035
	data : 0.11617488861083984
	model : 0.06877832412719727
			 train-loss:  2.1353024786168877 	 ± 0.24667503481706402
	data : 0.11707124710083008
	model : 0.06859302520751953
			 train-loss:  2.1333726074384605 	 ± 0.2414226988934777
	data : 0.11699147224426269
	model : 0.06769795417785644
			 train-loss:  2.1303067902723947 	 ± 0.23679646033968527
	data : 0.11792268753051757
	model : 0.06740479469299317
			 train-loss:  2.140022859573364 	 ± 0.236844483043405
	data : 0.11807775497436523
	model : 0.0678105354309082
			 train-loss:  2.1336233570025516 	 ± 0.23443898585054535
	data : 0.11779017448425293
	model : 0.06779346466064454
			 train-loss:  2.1246541429449013 	 ± 0.23455839077362442
	data : 0.1177070140838623
	model : 0.06753172874450683
			 train-loss:  2.1241375037602017 	 ± 0.2303474109868838
	data : 0.11796131134033203
	model : 0.06921367645263672
			 train-loss:  2.1198126735358405 	 ± 0.22749504034058474
	data : 0.11619071960449219
	model : 0.06955718994140625
			 train-loss:  2.1179677844047546 	 ± 0.22389186034375183
	data : 0.11604986190795899
	model : 0.06969265937805176
			 train-loss:  2.1155134670196043 	 ± 0.22066095511679484
	data : 0.11607198715209961
	model : 0.07014427185058594
			 train-loss:  2.1278386153280735 	 ± 0.2277692758777441
	data : 0.11585860252380371
	model : 0.07031168937683105
			 train-loss:  2.136262362653559 	 ± 0.22929775727086005
	data : 0.11574029922485352
	model : 0.06978302001953125
			 train-loss:  2.137644259368672 	 ± 0.22604000450101083
	data : 0.11630620956420898
	model : 0.07000665664672852
			 train-loss:  2.134093887465341 	 ± 0.2237472401080939
	data : 0.11615619659423829
	model : 0.0704340934753418
			 train-loss:  2.1300871007972293 	 ± 0.22188757342203905
	data : 0.11575984954833984
	model : 0.07092452049255371
			 train-loss:  2.133509999996907 	 ± 0.219829999358512
	data : 0.11522073745727539
	model : 0.07111339569091797
			 train-loss:  2.12981821361341 	 ± 0.21807749925963543
	data : 0.11499109268188476
	model : 0.06997981071472167
			 train-loss:  2.1358781105432754 	 ± 0.21848069561224823
	data : 0.11613755226135254
	model : 0.0687260627746582
			 train-loss:  2.13609761595726 	 ± 0.2157367565316016
	data : 0.11726617813110352
	model : 0.06852502822875976
			 train-loss:  2.126658625719024 	 ± 0.2212938100468857
	data : 0.11746983528137207
	model : 0.06755938529968261
			 train-loss:  2.129015491122291 	 ± 0.21916368940148115
	data : 0.11831874847412109
	model : 0.0670544147491455
			 train-loss:  2.124416797660118 	 ± 0.21864102522981999
	data : 0.11867051124572754
	model : 0.06790595054626465
			 train-loss:  2.122951380231164 	 ± 0.21635569315967965
	data : 0.11781878471374511
	model : 0.06875886917114257
			 train-loss:  2.1243220620685155 	 ± 0.2141313482877661
	data : 0.11711149215698242
	model : 0.06832795143127442
			 train-loss:  2.1254149960434954 	 ± 0.21191790704716987
	data : 0.1175661563873291
	model : 0.06929960250854492
			 train-loss:  2.1147304037784007 	 ± 0.22182215397482383
	data : 0.11657600402832032
	model : 0.0699570655822754
			 train-loss:  2.1232266823450723 	 ± 0.2270963223755466
	data : 0.11622743606567383
	model : 0.06981921195983887
			 train-loss:  2.1205186746558367 	 ± 0.2255487384511728
	data : 0.1162074089050293
	model : 0.0698305606842041
			 train-loss:  2.1228582429885865 	 ± 0.22388165214633818
	data : 0.11623978614807129
	model : 0.06987557411193848
			 train-loss:  2.1144262271768905 	 ± 0.22955420905092785
	data : 0.11614751815795898
	model : 0.0693666934967041
			 train-loss:  2.1070126799436717 	 ± 0.23341971151945004
	data : 0.11674952507019043
	model : 0.07002854347229004
			 train-loss:  2.104380949488226 	 ± 0.23198469695978688
	data : 0.11594429016113281
	model : 0.06946654319763183
			 train-loss:  2.113006384284408 	 ± 0.23825068944006358
	data : 0.1165989875793457
	model : 0.06963720321655273
			 train-loss:  2.116348322955045 	 ± 0.23734875454488671
	data : 0.11627392768859864
	model : 0.06979680061340332
			 train-loss:  2.1125061341694424 	 ± 0.23693963810077448
	data : 0.11602272987365722
	model : 0.07031879425048829
			 train-loss:  2.1231131344510796 	 ± 0.2479030988555797
	data : 0.11548390388488769
	model : 0.06939964294433594
			 train-loss:  2.119169093411544 	 ± 0.2475540796334929
	data : 0.11639790534973145
	model : 0.06996264457702636
			 train-loss:  2.1171613325506953 	 ± 0.2459230185695234
	data : 0.11592688560485839
	model : 0.06983447074890137
			 train-loss:  2.1139896770318347 	 ± 0.24507889900274116
	data : 0.11599340438842773
	model : 0.06900897026062011
			 train-loss:  2.111564456439409 	 ± 0.24378662423796807
	data : 0.11676750183105469
	model : 0.0690720558166504
			 train-loss:  2.1114914686449113 	 ± 0.2418132829116975
	data : 0.1165999412536621
	model : 0.06917824745178222
			 train-loss:  2.116169645672753 	 ± 0.24269816953952802
	data : 0.11649255752563477
	model : 0.06903676986694336
			 train-loss:  2.124756094068289 	 ± 0.25025358843687656
	data : 0.11654658317565918
	model : 0.0688706398010254
			 train-loss:  2.1192717460485606 	 ± 0.25216733547243464
	data : 0.11656012535095214
	model : 0.06877617835998535
			 train-loss:  2.1201532410852835 	 ± 0.25035057854894943
	data : 0.11685733795166016
	model : 0.06854081153869629
			 train-loss:  2.1171797549546656 	 ± 0.24964676078694686
	data : 0.11705570220947266
	model : 0.06849040985107421
			 train-loss:  2.1135335704859566 	 ± 0.24959512282012153
	data : 0.11703462600708008
	model : 0.0687300682067871
			 train-loss:  2.1099464115889175 	 ± 0.24953930343694344
	data : 0.1167992115020752
	model : 0.06892304420471192
			 train-loss:  2.1129848054477147 	 ± 0.24903271119332118
	data : 0.11681685447692872
	model : 0.06908707618713379
			 train-loss:  2.1076963501916803 	 ± 0.2512002217329428
	data : 0.11660957336425781
	model : 0.06933765411376953
			 train-loss:  2.107634350657463 	 ± 0.24945022325603586
	data : 0.11661639213562011
	model : 0.06918559074401856
			 train-loss:  2.104227275064547 	 ± 0.24941691756472734
	data : 0.11679544448852539
	model : 0.06833119392395019
			 train-loss:  2.1029890002431095 	 ± 0.24795175340300504
	data : 0.11767010688781739
	model : 0.06818866729736328
			 train-loss:  2.106449508666992 	 ± 0.24808566362229273
	data : 0.11775341033935546
	model : 0.06967000961303711
			 train-loss:  2.103989913275367 	 ± 0.24736692358599513
	data : 0.11617059707641601
	model : 0.06875376701354981
			 train-loss:  2.103290838080567 	 ± 0.2458309502424018
	data : 0.11714520454406738
	model : 0.06888113021850586
			 train-loss:  2.11198591421812 	 ± 0.2558898410774121
	data : 0.11707358360290528
	model : 0.06972346305847169
			 train-loss:  2.1089227471170546 	 ± 0.25570027055887173
	data : 0.11621675491333008
	model : 0.06984381675720215
			 train-loss:  2.10951914191246 	 ± 0.2541524045975931
	data : 0.11603264808654785
	model : 0.06899256706237793
			 train-loss:  2.106220987108019 	 ± 0.25429553837321084
	data : 0.11698970794677735
	model : 0.0696019172668457
			 train-loss:  2.10658178678373 	 ± 0.25276105846467084
	data : 0.11626853942871093
	model : 0.06973333358764648
			 train-loss:  2.1067426377032175 	 ± 0.2512380096331967
	data : 0.11619729995727539
	model : 0.06966876983642578
			 train-loss:  2.1026935435476757 	 ± 0.2524478185353839
	data : 0.11624369621276856
	model : 0.06958580017089844
			 train-loss:  2.102415048374849 	 ± 0.250971417783512
	data : 0.11646175384521484
	model : 0.06948318481445312
			 train-loss:  2.103331524272298 	 ± 0.24965104344605935
	data : 0.1165705680847168
	model : 0.06960387229919433
			 train-loss:  2.104165734915898 	 ± 0.24833264953102932
	data : 0.11630420684814453
	model : 0.06929435729980468
			 train-loss:  2.1036314964294434 	 ± 0.24696791345674024
	data : 0.11653261184692383
	model : 0.0687485694885254
			 train-loss:  2.101261500562175 	 ± 0.24658085785008388
	data : 0.11698179244995117
	model : 0.06885337829589844
			 train-loss:  2.102240554491679 	 ± 0.24538103227453323
	data : 0.11686263084411622
	model : 0.06847128868103028
			 train-loss:  2.1011612834511224 	 ± 0.24424376480458027
	data : 0.1170417308807373
	model : 0.06901960372924805
			 train-loss:  2.103811616482942 	 ± 0.24422489775466646
	data : 0.11681180000305176
	model : 0.0690549373626709
			 train-loss:  2.1002212173195294 	 ± 0.2453373531971852
	data : 0.1166306972503662
	model : 0.07058019638061523
			 train-loss:  2.1011501436537885 	 ± 0.24419325005933964
	data : 0.11517453193664551
	model : 0.07056674957275391
			 train-loss:  2.1009300871899255 	 ± 0.24291399195022315
	data : 0.11514415740966796
	model : 0.07027535438537598
			 train-loss:  2.106530041744312 	 ± 0.24773312398618033
	data : 0.1155210018157959
	model : 0.06976432800292968
			 train-loss:  2.1047816964768873 	 ± 0.24704745909865522
	data : 0.11587185859680176
	model : 0.06923456192016601
			 train-loss:  2.1092523312082094 	 ± 0.24969654531166516
	data : 0.11663618087768554
	model : 0.06742362976074219
			 train-loss:  2.1064633094903193 	 ± 0.24996177749519014
	data : 0.11840343475341797
	model : 0.06743764877319336
			 train-loss:  2.106414988040924 	 ± 0.24870929308683684
	data : 0.11840291023254394
	model : 0.06832923889160156
			 train-loss:  2.1049609904242033 	 ± 0.24790176408689094
	data : 0.11760473251342773
	model : 0.06831464767456055
			 train-loss:  2.1047074035102247 	 ± 0.24669673039067
	data : 0.11770687103271485
	model : 0.06906800270080567
			 train-loss:  2.1042406894628285 	 ± 0.24554149927201194
	data : 0.11700983047485351
	model : 0.07011408805847168
			 train-loss:  2.102773178082246 	 ± 0.2448116215037496
	data : 0.11602072715759278
	model : 0.0700906753540039
			 train-loss:  2.103778752826509 	 ± 0.2438587806440575
	data : 0.11593403816223144
	model : 0.06987528800964356
			 train-loss:  2.1060024679831737 	 ± 0.24377307055730338
	data : 0.1160508155822754
	model : 0.0700878620147705
			 train-loss:  2.1106780422068088 	 ± 0.2473604628848847
	data : 0.11579675674438476
	model : 0.07033262252807618
			 train-loss:  2.105284982257419 	 ± 0.2524534723360861
	data : 0.11559586524963379
	model : 0.07026658058166504
			 train-loss:  2.103280662396632 	 ± 0.2521545552020275
	data : 0.11588788032531738
	model : 0.07022385597229004
			 train-loss:  2.103312613747337 	 ± 0.25100600298421477
	data : 0.11605377197265625
	model : 0.07050075531005859
			 train-loss:  2.1021824173025183 	 ± 0.25015378799651605
	data : 0.11571388244628907
	model : 0.07041668891906738
			 train-loss:  2.101870175983225 	 ± 0.2490562528329706
	data : 0.11566181182861328
	model : 0.06922469139099122
			 train-loss:  2.1063729313622535 	 ± 0.2524893383747728
	data : 0.11667742729187011
	model : 0.06933007240295411
			 train-loss:  2.103332794549172 	 ± 0.2534483037393618
	data : 0.11648349761962891
	model : 0.06960935592651367
			 train-loss:  2.1036338049432506 	 ± 0.25236441431315987
	data : 0.11629514694213867
	model : 0.06943507194519043
			 train-loss:  2.100547225310885 	 ± 0.2534449977089518
	data : 0.11659865379333496
	model : 0.06966896057128906
			 train-loss:  2.1026376496013413 	 ± 0.2533619161040865
	data : 0.1164738655090332
	model : 0.07058110237121581
			 train-loss:  2.1024974241095076 	 ± 0.25229062391832546
	data : 0.11572823524475098
	model : 0.07052145004272461
			 train-loss:  2.10113683868857 	 ± 0.25166271278812946
	data : 0.11565575599670411
	model : 0.07038435935974122
			 train-loss:  2.1021133601665496 	 ± 0.2508382235138536
	data : 0.11568889617919922
	model : 0.07033634185791016
			 train-loss:  2.0986377316072953 	 ± 0.25268441814939957
	data : 0.11573801040649415
	model : 0.0699549674987793
			 train-loss:  2.096447683748652 	 ± 0.25279717715553585
	data : 0.11603808403015137
	model : 0.06976833343505859
			 train-loss:  2.0996302153036845 	 ± 0.25420961097553274
	data : 0.11625838279724121
	model : 0.06961245536804199
			 train-loss:  2.0973343781886564 	 ± 0.25445961109341564
	data : 0.11634993553161621
	model : 0.0693099021911621
			 train-loss:  2.0977225522994996 	 ± 0.2534765873641798
	data : 0.11648793220520019
	model : 0.06882038116455078
			 train-loss:  2.0973866354851496 	 ± 0.2524966568126294
	data : 0.11691436767578126
	model : 0.06888289451599121
			 train-loss:  2.0937013410207794 	 ± 0.25487999742062284
	data : 0.11688413619995117
	model : 0.06916918754577636
			 train-loss:  2.092203475534916 	 ± 0.2544429614356706
	data : 0.11660943031311036
	model : 0.06840224266052246
			 train-loss:  2.0957822540933773 	 ± 0.2566685229260873
	data : 0.11760907173156739
	model : 0.06871285438537597
			 train-loss:  2.095263800254235 	 ± 0.25574723003364924
	data : 0.11738181114196777
	model : 0.06918201446533204
			 train-loss:  2.09477489231197 	 ± 0.2548302029297935
	data : 0.11695876121520996
	model : 0.06910285949707032
			 train-loss:  2.093735669598435 	 ± 0.2541415990913821
	data : 0.11698498725891113
	model : 0.06813416481018067
			 train-loss:  2.0927739672194745 	 ± 0.25342535729485727
	data : 0.11763906478881836
	model : 0.0682894229888916
			 train-loss:  2.092067867962282 	 ± 0.25260925496666403
	data : 0.11757159233093262
	model : 0.06726298332214356
			 train-loss:  2.0879958691420377 	 ± 0.2560481122014131
	data : 0.11848030090332032
	model : 0.06740379333496094
			 train-loss:  2.0863484144210815 	 ± 0.2558221583765454
	data : 0.11833415031433106
	model : 0.06745357513427734
			 train-loss:  2.083131350740029 	 ± 0.2576330879049917
	data : 0.11815376281738281
	model : 0.06834745407104492
			 train-loss:  2.0808759564938755 	 ± 0.2580517838597143
	data : 0.1174532413482666
	model : 0.0682291030883789
			 train-loss:  2.079457050604786 	 ± 0.2576615760327627
	data : 0.1175532341003418
	model : 0.06822624206542968
			 train-loss:  2.075930849143437 	 ± 0.26008386452161997
	data : 0.11757869720458984
	model : 0.0679957389831543
			 train-loss:  2.0754929890869356 	 ± 0.2592117196867828
	data : 0.11782665252685547
	model : 0.06802382469177246
			 train-loss:  2.0744844213337967 	 ± 0.25857487895382086
	data : 0.11808781623840332
	model : 0.06713576316833496
			 train-loss:  2.075613561210099 	 ± 0.25802025634274245
	data : 0.11873745918273926
	model : 0.06700830459594727
			 train-loss:  2.075143914255831 	 ± 0.25718411956517323
	data : 0.11867952346801758
	model : 0.0671837329864502
			 train-loss:  2.0759594415796214 	 ± 0.2564825146462444
	data : 0.11874299049377442
	model : 0.06661920547485352
			 train-loss:  2.075398957076138 	 ± 0.2556917296736812
	data : 0.11920900344848633
	model : 0.06667504310607911
			 train-loss:  2.073760451102743 	 ± 0.25558849155953434
	data : 0.11898317337036132
	model : 0.0666224479675293
			 train-loss:  2.074839846507923 	 ± 0.25505951764683643
	data : 0.11926851272583008
	model : 0.06706619262695312
			 train-loss:  2.076522815947565 	 ± 0.2550253664879537
	data : 0.11876635551452637
	model : 0.06760716438293457
			 train-loss:  2.076028005282084 	 ± 0.25424561373516885
	data : 0.11822652816772461
	model : 0.0684359073638916
			 train-loss:  2.0785558539510562 	 ± 0.25528660355909377
	data : 0.11740341186523437
	model : 0.06829319000244141
			 train-loss:  2.0786741485721185 	 ± 0.25444961145492356
	data : 0.11747622489929199
	model : 0.06912465095520019
			 train-loss:  2.0790618681440165 	 ± 0.25366175715118017
	data : 0.1165679931640625
	model : 0.06973490715026856
			 train-loss:  2.0780727987165575 	 ± 0.2531326539981567
	data : 0.11609086990356446
	model : 0.0699660301208496
			 train-loss:  2.0798691795718285 	 ± 0.2532976542103543
	data : 0.11588907241821289
	model : 0.07010235786437988
			 train-loss:  2.0799401922103686 	 ± 0.2524860454025592
	data : 0.1158843994140625
	model : 0.06989188194274902
			 train-loss:  2.0794249621166547 	 ± 0.25176292285198754
	data : 0.11620597839355469
	model : 0.07017979621887208
			 train-loss:  2.0814276594149916 	 ± 0.25221636784376256
	data : 0.11605973243713379
	model : 0.07004752159118652
			 train-loss:  2.079753094499216 	 ± 0.2523015503956313
	data : 0.11615500450134278
	model : 0.06930484771728515
			 train-loss:  2.079792334139347 	 ± 0.25151235894109036
	data : 0.1168022632598877
	model : 0.06925716400146484
			 train-loss:  2.0846704592615923 	 ± 0.25821102556316183
	data : 0.11685194969177246
	model : 0.06942086219787598
			 train-loss:  2.0852153374824995 	 ± 0.2575056738393944
	data : 0.11660423278808593
	model : 0.06903433799743652
			 train-loss:  2.086959376656936 	 ± 0.25767250200771974
	data : 0.11689453125
	model : 0.06903257369995117
			 train-loss:  2.0856581613784884 	 ± 0.25742232864380227
	data : 0.11694602966308594
	model : 0.06978287696838378
			 train-loss:  2.084914436484828 	 ± 0.25681774548825326
	data : 0.11620254516601562
	model : 0.06977953910827636
			 train-loss:  2.085320471999157 	 ± 0.25609614539085307
	data : 0.11634087562561035
	model : 0.06995854377746583
			 train-loss:  2.085612961632049 	 ± 0.25535604761321196
	data : 0.11633448600769043
	model : 0.0700754165649414
			 train-loss:  2.085015873823847 	 ± 0.25471182477629756
	data : 0.1162567138671875
	model : 0.07004227638244628
			 train-loss:  2.0845385084490804 	 ± 0.25403248408589707
	data : 0.11620335578918457
	model : 0.06914353370666504
			 train-loss:  2.083588114205529 	 ± 0.2535853882424444
	data : 0.11703810691833497
	model : 0.06879191398620606
			 train-loss:  2.083423295913384 	 ± 0.25285195539882166
	data : 0.11731610298156739
	model : 0.06848621368408203
			 train-loss:  2.083606798288434 	 ± 0.2521272684639766
	data : 0.11748456954956055
	model : 0.0688591480255127
			 train-loss:  2.083290924915689 	 ± 0.2514316506812269
	data : 0.1170191764831543
	model : 0.0696375846862793
			 train-loss:  2.0852779499415695 	 ± 0.25206666635287883
	data : 0.11631188392639161
	model : 0.07054190635681153
			 train-loss:  2.085382125718253 	 ± 0.2513492005414908
	data : 0.11534643173217773
	model : 0.07088117599487305
			 train-loss:  2.0837417034940287 	 ± 0.2515718308266029
	data : 0.11510243415832519
	model : 0.07104434967041015
			 train-loss:  2.082588987835383 	 ± 0.2513258545371708
	data : 0.11488580703735352
	model : 0.07004327774047851
			 train-loss:  2.0823934761325966 	 ± 0.25063238651705994
	data : 0.11594099998474121
	model : 0.06900291442871094
			 train-loss:  2.0813002106863694 	 ± 0.25035657232220243
	data : 0.11695628166198731
	model : 0.06901140213012695
			 train-loss:  2.0817619469430713 	 ± 0.24973658675499955
	data : 0.11701841354370117
	model : 0.06964344978332519
			 train-loss:  2.0809549909928884 	 ± 0.24928096236411068
	data : 0.11622190475463867
	model : 0.0697556495666504
			 train-loss:  2.079320667209206 	 ± 0.2495656581278011
	data : 0.11629772186279297
	model : 0.0695572853088379
			 train-loss:  2.081611829377263 	 ± 0.2507948734330021
	data : 0.11631975173950196
	model : 0.06981282234191895
			 train-loss:  2.080883835968764 	 ± 0.2503062454813463
	data : 0.11607322692871094
	model : 0.06974806785583496
			 train-loss:  2.083217636314598 	 ± 0.2516281568458962
	data : 0.11608562469482422
	model : 0.06839418411254883
			 train-loss:  2.084126203290878 	 ± 0.25125491671860084
	data : 0.1173326015472412
	model : 0.06842889785766601
			 train-loss:  2.083152493691062 	 ± 0.25093384246477585
	data : 0.11728310585021973
	model : 0.06944942474365234
			 train-loss:  2.082732782084891 	 ± 0.25033137985093623
	data : 0.11642723083496094
	model : 0.06993665695190429
			 train-loss:  2.0824404324173296 	 ± 0.24970042592230735
	data : 0.11605782508850097
	model : 0.07080883979797363
			 train-loss:  2.081415670169027 	 ± 0.24944061264486322
	data : 0.115272855758667
	model : 0.07153658866882324
			 train-loss:  2.0841654688900055 	 ± 0.25165754807552276
	data : 0.1146324634552002
	model : 0.07151284217834472
			 train-loss:  2.0866118017584085 	 ± 0.25326807502913967
	data : 0.11464972496032715
	model : 0.07124848365783691
			 train-loss:  2.0878342538299957 	 ± 0.25317836495041496
	data : 0.11471080780029297
	model : 0.07081332206726074
			 train-loss:  2.0868467129382893 	 ± 0.2528974031886314
	data : 0.11508979797363281
	model : 0.0698812484741211
			 train-loss:  2.088023794614352 	 ± 0.2527803444809064
	data : 0.11568350791931152
	model : 0.06978192329406738
			 train-loss:  2.089980880824887 	 ± 0.2536114729342584
	data : 0.11570286750793457
	model : 0.06943974494934083
			 train-loss:  2.0896291938530007 	 ± 0.25301488088494695
	data : 0.11583423614501953
	model : 0.06905245780944824
			 train-loss:  2.0926952651052764 	 ± 0.2560179102083167
	data : 0.11655955314636231
	model : 0.06902737617492676
			 train-loss:  2.090702912915292 	 ± 0.25690806166294033
	data : 0.11648054122924804
	model : 0.06833724975585938
			 train-loss:  2.089516122341156 	 ± 0.2568112717311623
	data : 0.11752748489379883
	model : 0.06863512992858886
			 train-loss:  2.088301656258047 	 ± 0.25674675326649016
	data : 0.11725845336914062
	model : 0.06901240348815918
			 train-loss:  2.0879094034138292 	 ± 0.25617082282926346
	data : 0.11708502769470215
	model : 0.06963300704956055
			 train-loss:  2.087861858565232 	 ± 0.2555399746940134
	data : 0.11656813621520996
	model : 0.06979961395263672
			 train-loss:  2.08614718095929 	 ± 0.2560808886210463
	data : 0.11638154983520507
	model : 0.07074227333068847
			 train-loss:  2.0854233416115364 	 ± 0.2556646555739826
	data : 0.11540536880493164
	model : 0.07061562538146973
			 train-loss:  2.083607369256251 	 ± 0.2563652726403228
	data : 0.11567115783691406
	model : 0.06965036392211914
			 train-loss:  2.084289348067869 	 ± 0.25593252910049696
	data : 0.11649212837219239
	model : 0.06937484741210938
			 train-loss:  2.0835892271537046 	 ± 0.25551519256783717
	data : 0.1166926383972168
	model : 0.06899347305297851
			 train-loss:  2.0862535939832623 	 ± 0.25778322621524213
	data : 0.1170614242553711
	model : 0.0687448501586914
			 train-loss:  2.0874672356105988 	 ± 0.25776654989287334
	data : 0.1172882080078125
	model : 0.06857123374938964
			 train-loss:  2.087416967509482 	 ± 0.25715603500204914
	data : 0.11731438636779785
	model : 0.06942815780639648
			 train-loss:  2.0879160132048264 	 ± 0.2566512124634033
	data : 0.11653151512145996
	model : 0.06962642669677735
			 train-loss:  2.089672349428347 	 ± 0.25732189325857285
	data : 0.1160557746887207
	model : 0.06973514556884766
			 train-loss:  2.0884749855950613 	 ± 0.25731404315617756
	data : 0.11620044708251953
	model : 0.06980299949645996
			 train-loss:  2.087334485941155 	 ± 0.2572565240335435
	data : 0.11609630584716797
	model : 0.06902213096618652
			 train-loss:  2.0886193541464984 	 ± 0.2573508627713707
	data : 0.11686649322509765
	model : 0.06876120567321778
			 train-loss:  2.087526626850603 	 ± 0.25725896892010136
	data : 0.11701269149780273
	model : 0.06869983673095703
			 train-loss:  2.08810688596253 	 ± 0.2568105394249916
	data : 0.11716585159301758
	model : 0.0688028335571289
			 train-loss:  2.0884720745696326 	 ± 0.25628027066330755
	data : 0.11680040359497071
	model : 0.06854705810546875
			 train-loss:  2.0902924505147067 	 ± 0.25711232743330503
	data : 0.11724252700805664
	model : 0.06901874542236328
			 train-loss:  2.090904090199535 	 ± 0.2566903310223194
	data : 0.11668877601623535
	model : 0.06849613189697265
			 train-loss:  2.0934451098914617 	 ± 0.25888235395487935
	data : 0.11712937355041504
	model : 0.06839656829833984
			 train-loss:  2.0928141546890875 	 ± 0.2584722691821857
	data : 0.11727876663208008
	model : 0.06844267845153809
			 train-loss:  2.0941343754529953 	 ± 0.25864715250587594
	data : 0.1172332763671875
	model : 0.06791520118713379
			 train-loss:  2.094092212253147 	 ± 0.25807251251219515
	data : 0.11743354797363281
	model : 0.06750521659851075
			 train-loss:  2.0928874390315166 	 ± 0.2581342825432129
	data : 0.11805534362792969
	model : 0.06761274337768555
			 train-loss:  2.092173249711024 	 ± 0.25778875853348665
	data : 0.11801223754882813
	model : 0.06720776557922363
			 train-loss:  2.093513604841734 	 ± 0.25801432481713243
	data : 0.1182673454284668
	model : 0.06636371612548828
			 train-loss:  2.09267727620737 	 ± 0.25775988950263323
	data : 0.11901540756225586
	model : 0.06651020050048828
			 train-loss:  2.090926735297493 	 ± 0.2585595414581348
	data : 0.11896219253540039
	model : 0.06697378158569336
			 train-loss:  2.092915404926647 	 ± 0.2597561067238211
	data : 0.1182969093322754
	model : 0.06682982444763183
			 train-loss:  2.0920814126729965 	 ± 0.2595054383287614
	data : 0.11828365325927734
	model : 0.06717143058776856
			 train-loss:  2.0941120601007355 	 ± 0.2607886216502882
	data : 0.11810169219970704
	model : 0.06767926216125489
			 train-loss:  2.0942928449720397 	 ± 0.2602454155629934
	data : 0.11770396232604981
	model : 0.06750850677490235
			 train-loss:  2.0931583343668185 	 ± 0.2602703559626358
	data : 0.1175797462463379
	model : 0.0674318790435791
			 train-loss:  2.0920462436595204 	 ± 0.26027726899376913
	data : 0.11758332252502442
	model : 0.06779270172119141
			 train-loss:  2.091336595358225 	 ± 0.2599562766639331
	data : 0.1173250675201416
	model : 0.06702971458435059
			 train-loss:  2.093185824506423 	 ± 0.2609670177749648
	data : 0.11794815063476563
	model : 0.0664738655090332
			 train-loss:  2.0939215197223997 	 ± 0.2606676968613281
	data : 0.11834254264831542
	model : 0.0666274070739746
			 train-loss:  2.093971326947212 	 ± 0.2601252119417973
	data : 0.11842408180236816
	model : 0.06645731925964356
			 train-loss:  2.092404445177292 	 ± 0.26071744507411143
	data : 0.11885986328125
	model : 0.06637392044067383
			 train-loss:  2.0912234965434746 	 ± 0.26082333434790117
	data : 0.11915025711059571
	model : 0.06735296249389648
			 train-loss:  2.0905683761761513 	 ± 0.26048554661426565
	data : 0.11816191673278809
	model : 0.06808834075927735
			 train-loss:  2.0906587765842186 	 ± 0.2599550363615389
	data : 0.11758089065551758
	model : 0.06861572265625
			 train-loss:  2.090558945889376 	 ± 0.2594286601978684
	data : 0.11716957092285156
	model : 0.06912088394165039
			 train-loss:  2.0906488163684442 	 ± 0.2589046507105146
	data : 0.11660327911376953
	model : 0.06907434463500976
			 train-loss:  2.0918817418789573 	 ± 0.2591026456781606
	data : 0.1165393352508545
	model : 0.06858382225036622
			 train-loss:  2.0912339129755573 	 ± 0.25878010004605606
	data : 0.11694984436035157
	model : 0.06857304573059082
			 train-loss:  2.0903750016507376 	 ± 0.2586139063361119
	data : 0.11681084632873535
	model : 0.06841082572937011
			 train-loss:  2.089317232608795 	 ± 0.25863531835655856
	data : 0.11692876815795898
	model : 0.06753311157226563
			 train-loss:  2.0900865042827044 	 ± 0.25840601720637735
	data : 0.11760339736938477
	model : 0.06727027893066406
			 train-loss:  2.0921861363781824 	 ± 0.26002926188722364
	data : 0.11767539978027344
	model : 0.06699528694152831
			 train-loss:  2.0916703435272095 	 ± 0.259643998324899
	data : 0.11783056259155274
	model : 0.06644501686096191
			 train-loss:  2.0922903676671307 	 ± 0.2593199823201863
	data : 0.11841726303100586
	model : 0.06605262756347656
			 train-loss:  2.0922726603115307 	 ± 0.2588111661471167
	data : 0.11874251365661621
	model : 0.06633172035217286
			 train-loss:  2.091417687945068 	 ± 0.2586657415391155
	data : 0.11771111488342285
	model : 0.05763053894042969
#epoch  65    val-loss:  2.4501701028723466  train-loss:  2.091417687945068  lr:  1.953125e-05
			 train-loss:  2.4316790103912354 	 ± 0.0
	data : 5.574543476104736
	model : 0.07427358627319336
			 train-loss:  2.199298143386841 	 ± 0.23238086700439453
	data : 2.8511438369750977
	model : 0.07208669185638428
			 train-loss:  2.043727397918701 	 ± 0.2905255443574078
	data : 1.9391534328460693
	model : 0.07111668586730957
			 train-loss:  2.0798409581184387 	 ± 0.2592612324163146
	data : 1.4835148453712463
	model : 0.07024842500686646
			 train-loss:  2.1248589515686036 	 ± 0.24875608182381664
	data : 1.2103272914886474
	model : 0.06978626251220703
			 train-loss:  2.0942561825116477 	 ± 0.23716865498356798
	data : 0.11898684501647949
	model : 0.06891427040100098
			 train-loss:  2.110317485673087 	 ± 0.2230721775340638
	data : 0.11642065048217773
	model : 0.06886115074157714
			 train-loss:  2.077222302556038 	 ± 0.22629203445972715
	data : 0.11656103134155274
	model : 0.06874713897705079
			 train-loss:  2.056936754120721 	 ± 0.22093058967883347
	data : 0.11666302680969239
	model : 0.06933732032775879
			 train-loss:  2.053659164905548 	 ± 0.20982367890641926
	data : 0.1161849021911621
	model : 0.06897640228271484
			 train-loss:  2.068095001307401 	 ± 0.20520123898406686
	data : 0.11655778884887695
	model : 0.06802406311035156
			 train-loss:  2.0586944222450256 	 ± 0.1989237689694239
	data : 0.11762189865112305
	model : 0.06801748275756836
			 train-loss:  2.0717608745281515 	 ± 0.19640660526134046
	data : 0.1176443099975586
	model : 0.06840686798095703
			 train-loss:  2.0812742710113525 	 ± 0.19234531212961953
	data : 0.11735177040100098
	model : 0.06768584251403809
			 train-loss:  2.080020062128703 	 ± 0.18588247286190404
	data : 0.1180922508239746
	model : 0.06848173141479492
			 train-loss:  2.0613903850317 	 ± 0.19390396692952608
	data : 0.11733651161193848
	model : 0.06854233741760254
			 train-loss:  2.0463096744873943 	 ± 0.19754974714618326
	data : 0.11738533973693847
	model : 0.06859755516052246
			 train-loss:  2.032259159617954 	 ± 0.2005340004969157
	data : 0.11749224662780762
	model : 0.06762509346008301
			 train-loss:  2.0328240394592285 	 ± 0.19520017592120828
	data : 0.11829304695129395
	model : 0.06732392311096191
			 train-loss:  2.0515723824501038 	 ± 0.2070663183109548
	data : 0.1185793399810791
	model : 0.06718244552612304
			 train-loss:  2.037691042536781 	 ± 0.2113966791026879
	data : 0.11866612434387207
	model : 0.06817450523376464
			 train-loss:  2.036880606954748 	 ± 0.20656972397003184
	data : 0.11781601905822754
	model : 0.06795358657836914
			 train-loss:  2.046028432638749 	 ± 0.20653524518717564
	data : 0.11806769371032715
	model : 0.06882119178771973
			 train-loss:  2.048689678311348 	 ± 0.20258907122882966
	data : 0.1172170639038086
	model : 0.06956300735473633
			 train-loss:  2.040285882949829 	 ± 0.20272051979408567
	data : 0.11648674011230468
	model : 0.06917829513549804
			 train-loss:  2.027561549956982 	 ± 0.20871685376761623
	data : 0.11680383682250976
	model : 0.06922373771667481
			 train-loss:  2.03707429656276 	 ± 0.2104806210397875
	data : 0.11670780181884766
	model : 0.0696631908416748
			 train-loss:  2.046215853520802 	 ± 0.21207595805626414
	data : 0.11629829406738282
	model : 0.06957712173461914
			 train-loss:  2.044659807764251 	 ± 0.2085500034020988
	data : 0.11638345718383789
	model : 0.06977806091308594
			 train-loss:  2.050695558389028 	 ± 0.20760494230849144
	data : 0.11634535789489746
	model : 0.0696709156036377
			 train-loss:  2.0659341312223867 	 ± 0.22062619656881816
	data : 0.11645598411560058
	model : 0.06951670646667481
			 train-loss:  2.062589794397354 	 ± 0.21794843061734093
	data : 0.11649527549743652
	model : 0.06946978569030762
			 train-loss:  2.058442729892153 	 ± 0.21589909463709409
	data : 0.11647439002990723
	model : 0.0694697380065918
			 train-loss:  2.06790487205281 	 ± 0.21953593326392304
	data : 0.1166421890258789
	model : 0.06920199394226074
			 train-loss:  2.0657473938805717 	 ± 0.21674237446819408
	data : 0.11678085327148438
	model : 0.06959342956542969
			 train-loss:  2.0609836710823908 	 ± 0.215561105046886
	data : 0.11646890640258789
	model : 0.06960062980651856
			 train-loss:  2.0716872730770626 	 ± 0.22211517658363442
	data : 0.11648616790771485
	model : 0.06944708824157715
			 train-loss:  2.06234757210079 	 ± 0.22641636145358454
	data : 0.11657834053039551
	model : 0.06865959167480469
			 train-loss:  2.0655304071230765 	 ± 0.22435430553994154
	data : 0.1172515869140625
	model : 0.06887125968933105
			 train-loss:  2.069557526707649 	 ± 0.22295509199778593
	data : 0.11701607704162598
	model : 0.06909103393554687
			 train-loss:  2.0746259602104744 	 ± 0.2225401532260403
	data : 0.11672720909118653
	model : 0.06910443305969238
			 train-loss:  2.0917080612409684 	 ± 0.24557829563838127
	data : 0.11662993431091309
	model : 0.06821694374084472
			 train-loss:  2.0870941239734027 	 ± 0.24454096776777018
	data : 0.11745061874389648
	model : 0.06926426887512208
			 train-loss:  2.084645363417539 	 ± 0.24227883512082327
	data : 0.11647849082946778
	model : 0.06836824417114258
			 train-loss:  2.0749842961629232 	 ± 0.2479947674303087
	data : 0.11723713874816895
	model : 0.06836004257202148
			 train-loss:  2.074106143868488 	 ± 0.24535508845356818
	data : 0.11731553077697754
	model : 0.06828808784484863
			 train-loss:  2.0596316469476577 	 ± 0.2618316174812938
	data : 0.1173926830291748
	model : 0.06835012435913086
			 train-loss:  2.066996231675148 	 ± 0.26396343354805735
	data : 0.11735496520996094
	model : 0.06727437973022461
			 train-loss:  2.0720103808811734 	 ± 0.26355553897978073
	data : 0.11837749481201172
	model : 0.06732568740844727
			 train-loss:  2.0729306650161745 	 ± 0.2609861891095999
	data : 0.11822113990783692
	model : 0.06697697639465332
			 train-loss:  2.077799212698843 	 ± 0.2606978408144284
	data : 0.11837143898010254
	model : 0.0669630527496338
			 train-loss:  2.0768557649392347 	 ± 0.25826686045553104
	data : 0.11837210655212402
	model : 0.06778273582458497
			 train-loss:  2.0811050657956107 	 ± 0.25764741067669694
	data : 0.1174917221069336
	model : 0.06862268447875977
			 train-loss:  2.0851750947810985 	 ± 0.25696466809925983
	data : 0.11669106483459472
	model : 0.06949815750122071
			 train-loss:  2.0819153048775414 	 ± 0.25574224591153316
	data : 0.11605772972106934
	model : 0.06989932060241699
			 train-loss:  2.0838970073631833 	 ± 0.253874297660886
	data : 0.11576943397521973
	model : 0.07000350952148438
			 train-loss:  2.081090000637791 	 ± 0.2525126916401685
	data : 0.11576461791992188
	model : 0.06984119415283203
			 train-loss:  2.079574169783757 	 ± 0.2505878580523821
	data : 0.11615047454833985
	model : 0.06992006301879883
			 train-loss:  2.0833269256656455 	 ± 0.25009356071951755
	data : 0.1160398006439209
	model : 0.0690584659576416
			 train-loss:  2.0805453697840375 	 ± 0.24891932179397447
	data : 0.11682052612304687
	model : 0.06871619224548339
			 train-loss:  2.080686592664875 	 ± 0.2468729918506885
	data : 0.11719050407409667
	model : 0.06775555610656739
			 train-loss:  2.0814496317217426 	 ± 0.24494649544134606
	data : 0.11788229942321778
	model : 0.06792902946472168
			 train-loss:  2.0849018059079607 	 ± 0.2445103461252511
	data : 0.11761279106140136
	model : 0.06711225509643555
			 train-loss:  2.086395088583231 	 ± 0.2428819613965697
	data : 0.11831011772155761
	model : 0.06718573570251465
			 train-loss:  2.083849279697125 	 ± 0.24186540991595712
	data : 0.11824197769165039
	model : 0.06761260032653808
			 train-loss:  2.0862035859714854 	 ± 0.2407754330743596
	data : 0.1179494857788086
	model : 0.06850275993347169
			 train-loss:  2.0855695098193725 	 ± 0.23902736000928163
	data : 0.11726765632629395
	model : 0.06770153045654297
			 train-loss:  2.0856609169174645 	 ± 0.23726447604646345
	data : 0.11783337593078613
	model : 0.06827692985534668
			 train-loss:  2.0914394509965093 	 ± 0.24031061000534704
	data : 0.11729655265808106
	model : 0.06901345252990723
			 train-loss:  2.0896549565451488 	 ± 0.23904795726732048
	data : 0.11658253669738769
	model : 0.06944146156311035
			 train-loss:  2.085744469938144 	 ± 0.23960282809795516
	data : 0.11637506484985352
	model : 0.06949653625488281
			 train-loss:  2.084198382165697 	 ± 0.23828948436663414
	data : 0.1161677360534668
	model : 0.07039556503295899
			 train-loss:  2.084515182939294 	 ± 0.23666700345877162
	data : 0.11537284851074218
	model : 0.07064452171325683
			 train-loss:  2.087660747605401 	 ± 0.2365938832493396
	data : 0.11520524024963379
	model : 0.07054014205932617
			 train-loss:  2.085253597895304 	 ± 0.23592179511349387
	data : 0.1152653694152832
	model : 0.06984119415283203
			 train-loss:  2.0857665821125635 	 ± 0.23440664107033934
	data : 0.11579451560974122
	model : 0.06986274719238281
			 train-loss:  2.0928412598448913 	 ± 0.24090820607810065
	data : 0.1159395694732666
	model : 0.06958837509155273
			 train-loss:  2.089850881160834 	 ± 0.2407929915132078
	data : 0.11625547409057617
	model : 0.06921887397766113
			 train-loss:  2.089530283891702 	 ± 0.23928088462119745
	data : 0.11657447814941406
	model : 0.06938409805297852
			 train-loss:  2.0891773611307145 	 ± 0.23780136622154147
	data : 0.11652865409851074
	model : 0.06924214363098144
			 train-loss:  2.084809512267878 	 ± 0.23953621067141703
	data : 0.11661715507507324
	model : 0.06901583671569825
			 train-loss:  2.0843661500186457 	 ± 0.23810458125879047
	data : 0.11677002906799316
	model : 0.06926064491271973
			 train-loss:  2.085223206554551 	 ± 0.2367930882656335
	data : 0.116489839553833
	model : 0.06965417861938476
			 train-loss:  2.0836953790414903 	 ± 0.23579058180117313
	data : 0.11609230041503907
	model : 0.06971073150634766
			 train-loss:  2.0843249671599446 	 ± 0.2344704881195208
	data : 0.11599140167236328
	model : 0.06992912292480469
			 train-loss:  2.0857936019121213 	 ± 0.2334962198900633
	data : 0.11563239097595215
	model : 0.07005276679992675
			 train-loss:  2.084011128579063 	 ± 0.23273816323655955
	data : 0.11566319465637206
	model : 0.06999697685241699
			 train-loss:  2.089514947750352 	 ± 0.23703781165396087
	data : 0.11582565307617188
	model : 0.06991596221923828
			 train-loss:  2.0890528044004117 	 ± 0.23574224298805954
	data : 0.11580266952514648
	model : 0.06982393264770508
			 train-loss:  2.0890294671058656 	 ± 0.23442900889337015
	data : 0.1158064365386963
	model : 0.06985836029052735
			 train-loss:  2.0832336289542064 	 ± 0.2395334927734953
	data : 0.11576027870178222
	model : 0.06999506950378417
			 train-loss:  2.082726004331008 	 ± 0.2382773340457456
	data : 0.11563096046447754
	model : 0.06999883651733399
			 train-loss:  2.081233670634608 	 ± 0.23742468670752415
	data : 0.11577353477478028
	model : 0.07011938095092773
			 train-loss:  2.085562173356401 	 ± 0.23981917753073967
	data : 0.1159437656402588
	model : 0.07013158798217774
			 train-loss:  2.0873703881313928 	 ± 0.2391969515491437
	data : 0.11597347259521484
	model : 0.07012343406677246
			 train-loss:  2.086914631227652 	 ± 0.2379893338341886
	data : 0.11597328186035157
	model : 0.06957831382751464
			 train-loss:  2.087239058976321 	 ± 0.23678074440063832
	data : 0.11649260520935059
	model : 0.06886563301086426
			 train-loss:  2.08656044882171 	 ± 0.2356643746672107
	data : 0.11715693473815918
	model : 0.06868314743041992
			 train-loss:  2.082984259634307 	 ± 0.23712875257164248
	data : 0.11721224784851074
	model : 0.0686614990234375
			 train-loss:  2.0821762132644652 	 ± 0.23607707596856373
	data : 0.11735243797302246
	model : 0.0686640739440918
			 train-loss:  2.0824241189673396 	 ± 0.23491855125051297
	data : 0.11747288703918457
	model : 0.06915946006774902
			 train-loss:  2.084611301328622 	 ± 0.2347953147442841
	data : 0.11711783409118652
	model : 0.06895112991333008
			 train-loss:  2.084233027060055 	 ± 0.23368398254120426
	data : 0.11732115745544433
	model : 0.06903181076049805
			 train-loss:  2.08300932095601 	 ± 0.2328891642979926
	data : 0.1171755313873291
	model : 0.06949329376220703
			 train-loss:  2.0884872754414876 	 ± 0.23841486323310687
	data : 0.11675362586975098
	model : 0.06969375610351562
			 train-loss:  2.087352061046744 	 ± 0.23757255689230103
	data : 0.11663341522216797
	model : 0.0700448989868164
			 train-loss:  2.086865009548508 	 ± 0.23651296297878205
	data : 0.11619820594787597
	model : 0.07143697738647461
			 train-loss:  2.084478179613749 	 ± 0.2367065864998384
	data : 0.11485576629638672
	model : 0.07167863845825195
			 train-loss:  2.083908680382125 	 ± 0.23569259405716628
	data : 0.11476755142211914
	model : 0.07114186286926269
			 train-loss:  2.082503020763397 	 ± 0.23507735029047944
	data : 0.11531953811645508
	model : 0.0709693431854248
			 train-loss:  2.0806467651246905 	 ± 0.23482447886348248
	data : 0.11535830497741699
	model : 0.07048549652099609
			 train-loss:  2.0798690308417593 	 ± 0.23391736241984035
	data : 0.11573867797851563
	model : 0.0694058895111084
			 train-loss:  2.0811330776298997 	 ± 0.23326393518975277
	data : 0.11656842231750489
	model : 0.06921286582946777
			 train-loss:  2.0819659180808485 	 ± 0.2324072806118989
	data : 0.11674003601074219
	model : 0.06929640769958496
			 train-loss:  2.0827337938806285 	 ± 0.231539808148555
	data : 0.11633925437927246
	model : 0.06926999092102051
			 train-loss:  2.0805548429489136 	 ± 0.23172078418419884
	data : 0.11632280349731446
	model : 0.06950383186340332
			 train-loss:  2.079041522792262 	 ± 0.2313033728288222
	data : 0.11614327430725098
	model : 0.07020812034606934
			 train-loss:  2.0779808507127275 	 ± 0.2306067601162769
	data : 0.11562371253967285
	model : 0.07017002105712891
			 train-loss:  2.0765486975677874 	 ± 0.23016215334752582
	data : 0.11585273742675781
	model : 0.06996541023254395
			 train-loss:  2.074777416388194 	 ± 0.2300141662992413
	data : 0.11626029014587402
	model : 0.07000942230224609
			 train-loss:  2.0744162906299937 	 ± 0.2290958798762379
	data : 0.11615867614746093
	model : 0.06986536979675292
			 train-loss:  2.076324679812447 	 ± 0.22911873288321602
	data : 0.11626482009887695
	model : 0.06956019401550292
			 train-loss:  2.0739021252810472 	 ± 0.22974897591647203
	data : 0.11643567085266113
	model : 0.06959147453308105
			 train-loss:  2.0757180192778186 	 ± 0.2297052430302148
	data : 0.1163261890411377
	model : 0.06959943771362305
			 train-loss:  2.0784063062667846 	 ± 0.23073473224920185
	data : 0.11617493629455566
	model : 0.06858572959899903
			 train-loss:  2.0825138612399026 	 ± 0.23436081101201747
	data : 0.11712532043457032
	model : 0.06831741333007812
			 train-loss:  2.0812183083511715 	 ± 0.2338888520692605
	data : 0.11731104850769043
	model : 0.06850051879882812
			 train-loss:  2.0800396911799908 	 ± 0.23335175324342697
	data : 0.11710877418518066
	model : 0.06850194931030273
			 train-loss:  2.0798223148020663 	 ± 0.23245853923993134
	data : 0.1171116828918457
	model : 0.0681788444519043
			 train-loss:  2.078329831820268 	 ± 0.23218236722406463
	data : 0.11764812469482422
	model : 0.06924772262573242
			 train-loss:  2.079936428834464 	 ± 0.23201871743578362
	data : 0.11671147346496583
	model : 0.0695807933807373
			 train-loss:  2.081691069133354 	 ± 0.23200900793997992
	data : 0.11662917137145996
	model : 0.06978006362915039
			 train-loss:  2.08245553378772 	 ± 0.23130196330465932
	data : 0.11643667221069336
	model : 0.07001514434814453
			 train-loss:  2.0808322162770514 	 ± 0.23119648846453808
	data : 0.11634235382080078
	model : 0.07104768753051757
			 train-loss:  2.0833624963407162 	 ± 0.23219342670147144
	data : 0.11545310020446778
	model : 0.07001814842224122
			 train-loss:  2.086246611440883 	 ± 0.23375266561898306
	data : 0.11628618240356445
	model : 0.06919388771057129
			 train-loss:  2.0858939651155124 	 ± 0.23293429802545712
	data : 0.11678490638732911
	model : 0.06914587020874023
			 train-loss:  2.0862075470495913 	 ± 0.23211781961973885
	data : 0.11695432662963867
	model : 0.06871047019958496
			 train-loss:  2.0840397610081185 	 ± 0.23267911092061064
	data : 0.11709017753601074
	model : 0.06822175979614258
			 train-loss:  2.0816344831671034 	 ± 0.23357444787809006
	data : 0.11753711700439454
	model : 0.06860580444335937
			 train-loss:  2.0833566417085363 	 ± 0.23363499238853924
	data : 0.11725907325744629
	model : 0.06939387321472168
			 train-loss:  2.0826387145149874 	 ± 0.23296690799846903
	data : 0.11658477783203125
	model : 0.06925387382507324
			 train-loss:  2.0839132353976058 	 ± 0.2326471792017457
	data : 0.11660494804382324
	model : 0.06951098442077637
			 train-loss:  2.0826708558532925 	 ± 0.23231350679679064
	data : 0.11625189781188965
	model : 0.06958246231079102
			 train-loss:  2.085322006817522 	 ± 0.23368671367301533
	data : 0.11613636016845703
	model : 0.07001032829284667
			 train-loss:  2.084241591904261 	 ± 0.233248152378178
	data : 0.11590604782104492
	model : 0.07002649307250977
			 train-loss:  2.0825081333822135 	 ± 0.23339518597690345
	data : 0.11603965759277343
	model : 0.07000722885131835
			 train-loss:  2.0823785864018105 	 ± 0.2326106552339704
	data : 0.1161407470703125
	model : 0.06934857368469238
			 train-loss:  2.0806024794610556 	 ± 0.2328335288497106
	data : 0.11689186096191406
	model : 0.06942658424377442
			 train-loss:  2.078205993970235 	 ± 0.23389264789431513
	data : 0.1167597770690918
	model : 0.06948342323303222
			 train-loss:  2.0787142744127487 	 ± 0.23319998545841839
	data : 0.11676611900329589
	model : 0.06951251029968261
			 train-loss:  2.0832703929198417 	 ± 0.23907937167631385
	data : 0.1166231632232666
	model : 0.06942100524902343
			 train-loss:  2.0806256930033364 	 ± 0.24051717735245645
	data : 0.11658101081848145
	model : 0.06996402740478516
			 train-loss:  2.0785681257000217 	 ± 0.2410821667816772
	data : 0.11603374481201172
	model : 0.06983780860900879
			 train-loss:  2.07587571144104 	 ± 0.2426149212534883
	data : 0.11624660491943359
	model : 0.0700289249420166
			 train-loss:  2.076709657143324 	 ± 0.2420588288913382
	data : 0.11610941886901856
	model : 0.07006230354309081
			 train-loss:  2.0779262940595102 	 ± 0.24176473728866127
	data : 0.116035795211792
	model : 0.07007708549499511
			 train-loss:  2.0776316531096835 	 ± 0.24102672047451829
	data : 0.11604490280151367
	model : 0.07003197669982911
			 train-loss:  2.0768806836889975 	 ± 0.24045293628802325
	data : 0.11596946716308594
	model : 0.06991896629333497
			 train-loss:  2.077706678956747 	 ± 0.2399265202503996
	data : 0.11592698097229004
	model : 0.06961894035339355
			 train-loss:  2.0779379917227705 	 ± 0.23919814187437
	data : 0.11611762046813964
	model : 0.06933913230895997
			 train-loss:  2.0784029452889055 	 ± 0.2385317016673833
	data : 0.11649255752563477
	model : 0.06941328048706055
			 train-loss:  2.081765103925225 	 ± 0.24161865614404757
	data : 0.11654062271118164
	model : 0.0694694995880127
			 train-loss:  2.079492012902004 	 ± 0.2426227805770871
	data : 0.11669549942016602
	model : 0.06966676712036132
			 train-loss:  2.0790934497659856 	 ± 0.24194028802577308
	data : 0.11655926704406738
	model : 0.0689725399017334
			 train-loss:  2.0804550568741487 	 ± 0.24184372550072541
	data : 0.11727051734924317
	model : 0.06909632682800293
			 train-loss:  2.0800194404796213 	 ± 0.24118386789166724
	data : 0.11702194213867187
	model : 0.06911849975585938
			 train-loss:  2.080036596882911 	 ± 0.24046508961024188
	data : 0.1169619083404541
	model : 0.06831741333007812
			 train-loss:  2.078121916076841 	 ± 0.24103360161887127
	data : 0.11760067939758301
	model : 0.06829319000244141
			 train-loss:  2.08126782950233 	 ± 0.24377859470738672
	data : 0.11763381958007812
	model : 0.06928291320800781
			 train-loss:  2.0809391702127735 	 ± 0.24310251733105695
	data : 0.11664433479309082
	model : 0.06930794715881347
			 train-loss:  2.0795825005963793 	 ± 0.2430431454075825
	data : 0.11664361953735351
	model : 0.06903486251831055
			 train-loss:  2.081167188682997 	 ± 0.2432292298683212
	data : 0.11692109107971191
	model : 0.07014570236206055
			 train-loss:  2.081466321972595 	 ± 0.24256120010022342
	data : 0.11597576141357421
	model : 0.06999635696411133
			 train-loss:  2.079118530409677 	 ± 0.24384183446540156
	data : 0.11603522300720215
	model : 0.06992425918579101
			 train-loss:  2.0789454613219607 	 ± 0.24315889388428538
	data : 0.11619987487792968
	model : 0.0697403907775879
			 train-loss:  2.0782082431060447 	 ± 0.24266820030820221
	data : 0.11630406379699706
	model : 0.07006978988647461
			 train-loss:  2.0785469499866616 	 ± 0.2420275410948117
	data : 0.1159541130065918
	model : 0.06952600479125977
			 train-loss:  2.0787673942203626 	 ± 0.24136845903857815
	data : 0.1163480281829834
	model : 0.06961312294006347
			 train-loss:  2.0779760930273268 	 ± 0.2409297731648297
	data : 0.1163447380065918
	model : 0.06984171867370606
			 train-loss:  2.077812200093138 	 ± 0.24027336107242986
	data : 0.11611104011535645
	model : 0.07006711959838867
			 train-loss:  2.0766541748256473 	 ± 0.24011832132896954
	data : 0.11613054275512695
	model : 0.0704582691192627
			 train-loss:  2.0778047113470692 	 ± 0.2399638792656288
	data : 0.11579928398132325
	model : 0.07061052322387695
			 train-loss:  2.077235744051311 	 ± 0.23943465819982457
	data : 0.11570215225219727
	model : 0.07039546966552734
			 train-loss:  2.076465506167025 	 ± 0.2390151257104225
	data : 0.11593589782714844
	model : 0.07011394500732422
			 train-loss:  2.07682882073105 	 ± 0.23842296203921312
	data : 0.1162449836730957
	model : 0.07012653350830078
			 train-loss:  2.076640460580428 	 ± 0.2377984889458921
	data : 0.11616621017456055
	model : 0.0696878433227539
			 train-loss:  2.076658801829561 	 ± 0.23716533552043528
	data : 0.11656208038330078
	model : 0.06976752281188965
			 train-loss:  2.0760616051456915 	 ± 0.23667876996047335
	data : 0.11649231910705567
	model : 0.07049570083618165
			 train-loss:  2.0757892363949826 	 ± 0.23608480592935566
	data : 0.115962553024292
	model : 0.07041659355163574
			 train-loss:  2.077062737255196 	 ± 0.23611939083824
	data : 0.11577963829040527
	model : 0.07033090591430664
			 train-loss:  2.076309809461236 	 ± 0.2357334676493512
	data : 0.1157371997833252
	model : 0.07000761032104492
			 train-loss:  2.076806150569817 	 ± 0.23522253077603655
	data : 0.11614913940429687
	model : 0.06910958290100097
			 train-loss:  2.076892370415717 	 ± 0.23461856148939447
	data : 0.11693053245544434
	model : 0.06857905387878419
			 train-loss:  2.0781265460527862 	 ± 0.23464671668114415
	data : 0.11727533340454102
	model : 0.06954364776611328
			 train-loss:  2.081214579392453 	 ± 0.23798671853530246
	data : 0.11649436950683593
	model : 0.06934356689453125
			 train-loss:  2.0801413761177643 	 ± 0.23785693869913047
	data : 0.11671781539916992
	model : 0.06883449554443359
			 train-loss:  2.07980949469287 	 ± 0.23730125351801187
	data : 0.1170166015625
	model : 0.0697598934173584
			 train-loss:  2.0789359766035225 	 ± 0.23702318732122357
	data : 0.11623506546020508
	model : 0.06972103118896485
			 train-loss:  2.0786368012428285 	 ± 0.23646755177965773
	data : 0.11628293991088867
	model : 0.06797394752502442
			 train-loss:  2.0772418252271208 	 ± 0.23670213584600197
	data : 0.11803240776062011
	model : 0.06787524223327637
			 train-loss:  2.0766106826244015 	 ± 0.23628500122720078
	data : 0.11826319694519043
	model : 0.06852083206176758
			 train-loss:  2.075642719644631 	 ± 0.23610344889590593
	data : 0.11780076026916504
	model : 0.06856956481933593
			 train-loss:  2.0741039859313593 	 ± 0.23654222236269992
	data : 0.11790938377380371
	model : 0.06867713928222656
			 train-loss:  2.0740477835259785 	 ± 0.2359659502221106
	data : 0.11789298057556152
	model : 0.0697331428527832
			 train-loss:  2.073361873048023 	 ± 0.23559729571070237
	data : 0.11685171127319335
	model : 0.07001266479492188
			 train-loss:  2.0731592587226833 	 ± 0.2350455216182233
	data : 0.11649246215820312
	model : 0.07017669677734376
			 train-loss:  2.0737439904075403 	 ± 0.2346306995268032
	data : 0.11631965637207031
	model : 0.07001242637634278
			 train-loss:  2.074320977384394 	 ± 0.23421658064387307
	data : 0.11622605323791504
	model : 0.06970882415771484
			 train-loss:  2.0749727334295 	 ± 0.2338481583517606
	data : 0.11656990051269531
	model : 0.0695298194885254
			 train-loss:  2.0761949049918007 	 ± 0.23396467346859676
	data : 0.1166036605834961
	model : 0.06919846534729004
			 train-loss:  2.076856200987438 	 ± 0.23360979497144044
	data : 0.11691193580627442
	model : 0.06920166015625
			 train-loss:  2.077880966271593 	 ± 0.2335379049004564
	data : 0.11682438850402832
	model : 0.06929078102111816
			 train-loss:  2.076102637241934 	 ± 0.23443271137589683
	data : 0.11680150032043457
	model : 0.06943192481994628
			 train-loss:  2.0767661333084106 	 ± 0.23408819471495124
	data : 0.11649713516235352
	model : 0.06959247589111328
			 train-loss:  2.0770029696049512 	 ± 0.23357151247120736
	data : 0.1165034294128418
	model : 0.06997871398925781
			 train-loss:  2.0769820130915138 	 ± 0.2330329113408941
	data : 0.11605839729309082
	model : 0.06976251602172852
			 train-loss:  2.0751497729108968 	 ± 0.234059237985668
	data : 0.11638646125793457
	model : 0.06951541900634765
			 train-loss:  2.0754282599714795 	 ± 0.23356044160289371
	data : 0.11677021980285644
	model : 0.0695767879486084
			 train-loss:  2.0772637145085766 	 ± 0.23460671516465853
	data : 0.11658344268798829
	model : 0.06941041946411133
			 train-loss:  2.0787501103198367 	 ± 0.23511129613857326
	data : 0.11663231849670411
	model : 0.06920318603515625
			 train-loss:  2.081662005669362 	 ± 0.23854184823279473
	data : 0.11689982414245606
	model : 0.06932640075683594
			 train-loss:  2.0803396311576057 	 ± 0.23882054389818605
	data : 0.11658692359924316
	model : 0.06874284744262696
			 train-loss:  2.081193284796817 	 ± 0.23862760967193708
	data : 0.11700854301452637
	model : 0.0680872917175293
			 train-loss:  2.0806153927909 	 ± 0.23825377719368065
	data : 0.11765503883361816
	model : 0.06768059730529785
			 train-loss:  2.079755506684295 	 ± 0.23807573651244499
	data : 0.11812820434570312
	model : 0.06773843765258789
			 train-loss:  2.0790815164339174 	 ± 0.23776675092139726
	data : 0.11823158264160157
	model : 0.06715555191040039
			 train-loss:  2.0794298042330825 	 ± 0.23730278562203594
	data : 0.1187899112701416
	model : 0.06772956848144532
			 train-loss:  2.079245210214473 	 ± 0.23680049524878014
	data : 0.11820659637451172
	model : 0.06793055534362794
			 train-loss:  2.0786297689313473 	 ± 0.23646862445706784
	data : 0.11806001663208007
	model : 0.06756062507629394
			 train-loss:  2.07854560907785 	 ± 0.23595968448815774
	data : 0.11824774742126465
	model : 0.06728925704956054
			 train-loss:  2.0778230104980797 	 ± 0.2357066017401102
	data : 0.11847863197326661
	model : 0.06770615577697754
			 train-loss:  2.078216400780903 	 ± 0.23527656243629555
	data : 0.11798348426818847
	model : 0.06770997047424317
			 train-loss:  2.0784515636598964 	 ± 0.23480073681927632
	data : 0.11798667907714844
	model : 0.06799001693725586
			 train-loss:  2.0780587982624135 	 ± 0.23437764888768972
	data : 0.11759662628173828
	model : 0.06784791946411133
			 train-loss:  2.079999533245119 	 ± 0.2357652055157486
	data : 0.11760692596435547
	model : 0.06741251945495605
			 train-loss:  2.0815965259125466 	 ± 0.23654299035118675
	data : 0.11760005950927735
	model : 0.06691226959228516
			 train-loss:  2.0811776814340543 	 ± 0.23613358181155683
	data : 0.11826562881469727
	model : 0.06636061668395996
			 train-loss:  2.0802272169161045 	 ± 0.2360948369275725
	data : 0.11869535446166993
	model : 0.06565055847167969
			 train-loss:  2.0795802156130474 	 ± 0.23581468733113845
	data : 0.1194157600402832
	model : 0.06571545600891113
			 train-loss:  2.079119115449581 	 ± 0.23543333032250324
	data : 0.11923360824584961
	model : 0.06560478210449219
			 train-loss:  2.078091206136814 	 ± 0.23548768161062658
	data : 0.11955089569091797
	model : 0.06559791564941406
			 train-loss:  2.0782179955101796 	 ± 0.23501091648381844
	data : 0.11947898864746094
	model : 0.06579532623291015
			 train-loss:  2.0789751669422527 	 ± 0.2348256626852215
	data : 0.11932816505432128
	model : 0.06614899635314941
			 train-loss:  2.080192665664517 	 ± 0.23511635412512627
	data : 0.11915888786315917
	model : 0.06650886535644532
			 train-loss:  2.0827797132778945 	 ± 0.23810654235287895
	data : 0.11905422210693359
	model : 0.06735939979553222
			 train-loss:  2.083590450074509 	 ± 0.23796404470278898
	data : 0.11817245483398438
	model : 0.06817426681518554
			 train-loss:  2.0844728009354685 	 ± 0.23788831934880478
	data : 0.11754078865051269
	model : 0.06823453903198243
			 train-loss:  2.0843912439652716 	 ± 0.23741362548185174
	data : 0.11745538711547851
	model : 0.06832985877990723
			 train-loss:  2.0850755095481874 	 ± 0.23718422222774857
	data : 0.11727848052978515
	model : 0.06863808631896973
			 train-loss:  2.0849896108961676 	 ± 0.23671516853153599
	data : 0.11690583229064941
	model : 0.06841673851013183
			 train-loss:  2.0863637957308026 	 ± 0.2372460704883311
	data : 0.11709303855895996
	model : 0.06741108894348144
			 train-loss:  2.084725297958012 	 ± 0.23820109767891934
	data : 0.11759181022644043
	model : 0.06764435768127441
			 train-loss:  2.0844719100186206 	 ± 0.23776589751328134
	data : 0.11738805770874024
	model : 0.06764035224914551
			 train-loss:  2.0843951739516915 	 ± 0.2373023833067036
	data : 0.11726164817810059
	model : 0.06684384346008301
			 train-loss:  2.082412503659725 	 ± 0.238945288731802
	data : 0.1169936180114746
	model : 0.057926177978515625
#epoch  66    val-loss:  2.4828935924329256  train-loss:  2.082412503659725  lr:  1.953125e-05
			 train-loss:  2.087710380554199 	 ± 0.0
	data : 5.782312631607056
	model : 0.07222318649291992
			 train-loss:  2.2633696794509888 	 ± 0.17565929889678955
	data : 2.959418296813965
	model : 0.07076406478881836
			 train-loss:  2.2126288414001465 	 ± 0.16037474287267944
	data : 2.0131935278574624
	model : 0.06974077224731445
			 train-loss:  2.115041732788086 	 ± 0.21876877042986467
	data : 1.5392974615097046
	model : 0.06910461187362671
			 train-loss:  2.133643054962158 	 ± 0.19917795229093604
	data : 1.255147933959961
	model : 0.06846294403076172
			 train-loss:  2.156784693400065 	 ± 0.1890437984003426
	data : 0.12254352569580078
	model : 0.06721715927124024
			 train-loss:  2.1143538100378856 	 ± 0.2035545909660046
	data : 0.11912388801574707
	model : 0.0672640323638916
			 train-loss:  2.07802776992321 	 ± 0.21328907824431897
	data : 0.11828446388244629
	model : 0.06698637008666992
			 train-loss:  2.1007523669136896 	 ± 0.21111323275274402
	data : 0.11878156661987305
	model : 0.06692180633544922
			 train-loss:  2.0678530812263487 	 ± 0.22327826658145244
	data : 0.11886539459228515
	model : 0.06776061058044433
			 train-loss:  2.069323702292009 	 ± 0.21293826317658054
	data : 0.1182187557220459
	model : 0.06787896156311035
			 train-loss:  2.073014050722122 	 ± 0.20423993402653284
	data : 0.11818962097167969
	model : 0.06800880432128906
			 train-loss:  2.08038866519928 	 ± 0.19788331026313996
	data : 0.1181119441986084
	model : 0.06877222061157226
			 train-loss:  2.0742732201303755 	 ± 0.19195572701913
	data : 0.11737756729125977
	model : 0.06937026977539062
			 train-loss:  2.0489601135253905 	 ± 0.2082332382618454
	data : 0.11679930686950683
	model : 0.06913132667541504
			 train-loss:  2.0631434470415115 	 ± 0.20897013702116352
	data : 0.11688795089721679
	model : 0.06977014541625977
			 train-loss:  2.0575465875513412 	 ± 0.20396318094779614
	data : 0.11619129180908203
	model : 0.0696798324584961
			 train-loss:  2.0329512092802258 	 ± 0.22265145052592977
	data : 0.11625800132751465
	model : 0.06974620819091797
			 train-loss:  2.0278110566892122 	 ± 0.21780750449407044
	data : 0.11615681648254395
	model : 0.06887845993041992
			 train-loss:  2.0282073318958282 	 ± 0.2122995223969804
	data : 0.11704893112182617
	model : 0.06900582313537598
			 train-loss:  2.065412958463033 	 ± 0.26572545592595626
	data : 0.11707768440246583
	model : 0.06903371810913086
			 train-loss:  2.0784926143559543 	 ± 0.26644529836622644
	data : 0.11709461212158204
	model : 0.06903162002563476
			 train-loss:  2.0666701016218765 	 ± 0.26642338559306905
	data : 0.11701045036315919
	model : 0.06974754333496094
			 train-loss:  2.0558229287465415 	 ± 0.2659512509348856
	data : 0.11623539924621581
	model : 0.07078418731689454
			 train-loss:  2.0695750236511232 	 ± 0.26914633176253433
	data : 0.11535019874572754
	model : 0.07087597846984864
			 train-loss:  2.0569218809788046 	 ± 0.2713966732263538
	data : 0.11530847549438476
	model : 0.07068510055541992
			 train-loss:  2.0679301994818227 	 ± 0.2721744082544551
	data : 0.11545701026916504
	model : 0.07074980735778809
			 train-loss:  2.0668076319353923 	 ± 0.26733360706798925
	data : 0.11536693572998047
	model : 0.06989355087280273
			 train-loss:  2.0804777597558908 	 ± 0.27246155018467594
	data : 0.11615238189697266
	model : 0.06968631744384765
			 train-loss:  2.068520983060201 	 ± 0.27551180956715693
	data : 0.1161283016204834
	model : 0.0698279857635498
			 train-loss:  2.076053680912141 	 ± 0.274153968141091
	data : 0.11599221229553222
	model : 0.07003130912780761
			 train-loss:  2.066184412688017 	 ± 0.27537449394586744
	data : 0.11593160629272461
	model : 0.07000346183776855
			 train-loss:  2.065467368472706 	 ± 0.2712003914279182
	data : 0.11602210998535156
	model : 0.06966023445129395
			 train-loss:  2.0622060649535237 	 ± 0.26783841876018327
	data : 0.11633143424987794
	model : 0.06968550682067871
			 train-loss:  2.0553334202085223 	 ± 0.26700881706499946
	data : 0.11624298095703126
	model : 0.06932682991027832
			 train-loss:  2.0626163714461856 	 ± 0.2667766403892648
	data : 0.11646151542663574
	model : 0.06910181045532227
			 train-loss:  2.0504820604582092 	 ± 0.2730328963976385
	data : 0.11656060218811035
	model : 0.06821393966674805
			 train-loss:  2.050744891166687 	 ± 0.2694211504376328
	data : 0.11748185157775878
	model : 0.06852488517761231
			 train-loss:  2.0495071655664687 	 ± 0.2660540291739234
	data : 0.11718573570251464
	model : 0.06845307350158691
			 train-loss:  2.0463842332363127 	 ± 0.26343022477669015
	data : 0.11729397773742675
	model : 0.0684995174407959
			 train-loss:  2.0529562031350483 	 ± 0.2634967591752253
	data : 0.11732134819030762
	model : 0.06853761672973632
			 train-loss:  2.0509609636806307 	 ± 0.26065428089791
	data : 0.11727437973022461
	model : 0.06934127807617188
			 train-loss:  2.056243577668833 	 ± 0.2598705281108984
	data : 0.11654987335205078
	model : 0.06951346397399902
			 train-loss:  2.0544335679574446 	 ± 0.25717451525627794
	data : 0.11666345596313477
	model : 0.06917853355407715
			 train-loss:  2.068715376324124 	 ± 0.27137366107032046
	data : 0.11690912246704102
	model : 0.06902775764465333
			 train-loss:  2.0800069518711255 	 ± 0.2788910185574507
	data : 0.11695518493652343
	model : 0.06918120384216309
			 train-loss:  2.0725601810090084 	 ± 0.2804927888258229
	data : 0.1168020248413086
	model : 0.06941475868225097
			 train-loss:  2.0726488356788955 	 ± 0.2775562760799613
	data : 0.11655340194702149
	model : 0.06936640739440918
			 train-loss:  2.066414455978238 	 ± 0.27808439784289635
	data : 0.11643199920654297
	model : 0.06995458602905273
			 train-loss:  2.0650995707511903 	 ± 0.2754433352107624
	data : 0.1161271095275879
	model : 0.07033452987670899
			 train-loss:  2.059020294862635 	 ± 0.2760965096146423
	data : 0.11556444168090821
	model : 0.07045178413391114
			 train-loss:  2.0643287346913266 	 ± 0.27604436433810053
	data : 0.11556434631347656
	model : 0.07018837928771973
			 train-loss:  2.06617234787851 	 ± 0.2737507794809386
	data : 0.11562952995300294
	model : 0.06996541023254395
			 train-loss:  2.0559193867224232 	 ± 0.28128855505954864
	data : 0.11576600074768066
	model : 0.06965856552124024
			 train-loss:  2.0530012889341873 	 ± 0.27954332858069075
	data : 0.1161728858947754
	model : 0.06989374160766601
			 train-loss:  2.0561677451644624 	 ± 0.2780296567315856
	data : 0.11622395515441894
	model : 0.06976919174194336
			 train-loss:  2.058312727693926 	 ± 0.27604708706562964
	data : 0.1161576271057129
	model : 0.06896834373474121
			 train-loss:  2.0558905848141373 	 ± 0.27426733861197156
	data : 0.11698474884033203
	model : 0.06827516555786133
			 train-loss:  2.049271854303651 	 ± 0.2765654616749466
	data : 0.11758127212524414
	model : 0.06862392425537109
			 train-loss:  2.052638618151347 	 ± 0.27546763484031067
	data : 0.11714000701904297
	model : 0.06742691993713379
			 train-loss:  2.047369177224206 	 ± 0.2762326270420774
	data : 0.11814565658569336
	model : 0.06726932525634766
			 train-loss:  2.047585258560796 	 ± 0.274001086197026
	data : 0.11838369369506836
	model : 0.06813211441040039
			 train-loss:  2.056076180367243 	 ± 0.279919345424529
	data : 0.11767616271972656
	model : 0.06935839653015137
			 train-loss:  2.0563396718353033 	 ± 0.2777317402049588
	data : 0.11655240058898926
	model : 0.06901931762695312
			 train-loss:  2.0562248615118173 	 ± 0.2755885920317502
	data : 0.11677212715148926
	model : 0.06993918418884278
			 train-loss:  2.0558116417942625 	 ± 0.2735131209408151
	data : 0.11613492965698242
	model : 0.06923766136169433
			 train-loss:  2.0579389653988738 	 ± 0.27201388215935574
	data : 0.11668319702148437
	model : 0.06934876441955566
			 train-loss:  2.0558135439367855 	 ± 0.2705662727050913
	data : 0.11642208099365234
	model : 0.06818037033081055
			 train-loss:  2.0571870976600093 	 ± 0.26883720357890994
	data : 0.11743144989013672
	model : 0.0678530216217041
			 train-loss:  2.0621620791299002 	 ± 0.2700902528564231
	data : 0.1176365852355957
	model : 0.06795806884765625
			 train-loss:  2.064009894787426 	 ± 0.26862670598631466
	data : 0.1174769401550293
	model : 0.06805219650268554
			 train-loss:  2.0630999952554703 	 ± 0.26686487741708304
	data : 0.11757416725158691
	model : 0.0679089069366455
			 train-loss:  2.059940354464805 	 ± 0.2663833498747838
	data : 0.11768512725830078
	model : 0.06868839263916016
			 train-loss:  2.056222086017196 	 ± 0.2664778263679853
	data : 0.1170689582824707
	model : 0.0684241771697998
			 train-loss:  2.054189632733663 	 ± 0.26527214383800246
	data : 0.11743206977844238
	model : 0.06831746101379395
			 train-loss:  2.052134830700724 	 ± 0.2641213057922786
	data : 0.11752028465270996
	model : 0.06912660598754883
			 train-loss:  2.0574162161195435 	 ± 0.2664093837341778
	data : 0.11675925254821777
	model : 0.06915402412414551
			 train-loss:  2.0600457527698617 	 ± 0.26569992629181804
	data : 0.11669821739196777
	model : 0.06959199905395508
			 train-loss:  2.056520167785355 	 ± 0.26584270787677766
	data : 0.11638455390930176
	model : 0.07016186714172364
			 train-loss:  2.0650102600455282 	 ± 0.2747424114965354
	data : 0.11581473350524903
	model : 0.06990580558776856
			 train-loss:  2.068342177956193 	 ± 0.2746627624838802
	data : 0.1158860206604004
	model : 0.06982927322387696
			 train-loss:  2.074367864829738 	 ± 0.2783175500320877
	data : 0.11607365608215332
	model : 0.0697402000427246
			 train-loss:  2.074182247541037 	 ± 0.276640963918917
	data : 0.11613650321960449
	model : 0.06843657493591308
			 train-loss:  2.0752792826720645 	 ± 0.27517092539031696
	data : 0.11718902587890626
	model : 0.0680356502532959
			 train-loss:  2.0759524022831637 	 ± 0.27361704173386237
	data : 0.11759042739868164
	model : 0.06823668479919434
			 train-loss:  2.079596501450206 	 ± 0.27408849176173183
	data : 0.11742901802062988
	model : 0.06736602783203124
			 train-loss:  2.0772706969030974 	 ± 0.2733609460862107
	data : 0.11793832778930664
	model : 0.06753339767456054
			 train-loss:  2.0736945163119924 	 ± 0.27384246253399613
	data : 0.11799921989440917
	model : 0.06842546463012696
			 train-loss:  2.076838455843122 	 ± 0.27389219915915153
	data : 0.11716289520263672
	model : 0.0689499855041504
			 train-loss:  2.0787544276979233 	 ± 0.2729654366872939
	data : 0.11671733856201172
	model : 0.06900224685668946
			 train-loss:  2.07810610467261 	 ± 0.27153115114052234
	data : 0.11680421829223633
	model : 0.06974349021911622
			 train-loss:  2.0786557301231054 	 ± 0.2701022992892981
	data : 0.1163109302520752
	model : 0.06881380081176758
			 train-loss:  2.084637503470144 	 ± 0.2747047421209786
	data : 0.11722831726074219
	model : 0.07002725601196289
			 train-loss:  2.0872455277341477 	 ± 0.2743947288755173
	data : 0.11604037284851074
	model : 0.07041583061218262
			 train-loss:  2.0880411148071287 	 ± 0.27305569591992257
	data : 0.11549811363220215
	model : 0.07063894271850586
			 train-loss:  2.086176988979181 	 ± 0.2722367980383733
	data : 0.11533427238464355
	model : 0.07080512046813965
			 train-loss:  2.08605463480212 	 ± 0.27083253336503643
	data : 0.11506214141845703
	model : 0.07177891731262206
			 train-loss:  2.084265785557883 	 ± 0.2700225681190233
	data : 0.11409139633178711
	model : 0.07054743766784669
			 train-loss:  2.0817758230247883 	 ± 0.26978378763009914
	data : 0.11521806716918945
	model : 0.07009053230285645
			 train-loss:  2.0793764817714693 	 ± 0.26949097601571015
	data : 0.11579775810241699
	model : 0.06894903182983399
			 train-loss:  2.0788367963073275 	 ± 0.2682078465755133
	data : 0.11681771278381348
	model : 0.06808114051818848
			 train-loss:  2.07775990635741 	 ± 0.26710920679266753
	data : 0.11760015487670898
	model : 0.06715302467346192
			 train-loss:  2.07826214160734 	 ± 0.2658577897114213
	data : 0.11843514442443848
	model : 0.06690521240234375
			 train-loss:  2.077118444901246 	 ± 0.26483102877925535
	data : 0.11873760223388671
	model : 0.06623516082763672
			 train-loss:  2.0776030267987933 	 ± 0.2636132359175264
	data : 0.11914181709289551
	model : 0.06725583076477051
			 train-loss:  2.074937288491231 	 ± 0.2637849516268764
	data : 0.11820511817932129
	model : 0.0672429084777832
			 train-loss:  2.070047937821005 	 ± 0.267331641332244
	data : 0.11840157508850098
	model : 0.0681447982788086
			 train-loss:  2.0686134409021446 	 ± 0.2665045307952949
	data : 0.11756043434143067
	model : 0.06834135055541993
			 train-loss:  2.0685076976041183 	 ± 0.2652814922332458
	data : 0.1175717830657959
	model : 0.0689840316772461
			 train-loss:  2.068107266859575 	 ± 0.26410600449560434
	data : 0.1168856143951416
	model : 0.06894884109497071
			 train-loss:  2.065918954643043 	 ± 0.2639135152264432
	data : 0.11674990653991699
	model : 0.07074546813964844
			 train-loss:  2.0678316227027347 	 ± 0.26350433896403136
	data : 0.11493620872497559
	model : 0.07070865631103515
			 train-loss:  2.067854879176722 	 ± 0.26233591500871695
	data : 0.11492500305175782
	model : 0.07000889778137206
			 train-loss:  2.06913669694934 	 ± 0.2615379749692694
	data : 0.11550436019897461
	model : 0.06921200752258301
			 train-loss:  2.0686766665914784 	 ± 0.2604446907850077
	data : 0.11648712158203126
	model : 0.06910529136657714
			 train-loss:  2.0694558024406433 	 ± 0.25945422380149324
	data : 0.1166989803314209
	model : 0.06859345436096191
			 train-loss:  2.069132264862713 	 ± 0.25836656532570534
	data : 0.11726565361022949
	model : 0.06895213127136231
			 train-loss:  2.0692497996960655 	 ± 0.2572726036464303
	data : 0.11697268486022949
	model : 0.06979613304138184
			 train-loss:  2.0673868656158447 	 ± 0.25698735891926017
	data : 0.11615328788757324
	model : 0.07089414596557617
			 train-loss:  2.0707163910071054 	 ± 0.25847891874578316
	data : 0.11525673866271972
	model : 0.07096576690673828
			 train-loss:  2.0720824367743877 	 ± 0.2578432105589816
	data : 0.11500797271728516
	model : 0.07003517150878906
			 train-loss:  2.069737648377653 	 ± 0.25807642194201647
	data : 0.11571393013000489
	model : 0.06976938247680664
			 train-loss:  2.068576567541293 	 ± 0.25734493864516217
	data : 0.11614603996276855
	model : 0.06962504386901855
			 train-loss:  2.0683943158195865 	 ± 0.2563131267892784
	data : 0.11616458892822265
	model : 0.06919679641723633
			 train-loss:  2.0659410648345946 	 ± 0.2567433236168095
	data : 0.11670441627502441
	model : 0.0690239429473877
			 train-loss:  2.065757157310607 	 ± 0.25573073751521336
	data : 0.1170313835144043
	model : 0.06973466873168946
			 train-loss:  2.068553382017481 	 ± 0.2566484785125428
	data : 0.11646647453308105
	model : 0.06954808235168457
			 train-loss:  2.066586170345545 	 ± 0.2566034379878919
	data : 0.1162592887878418
	model : 0.06867098808288574
			 train-loss:  2.064565716787826 	 ± 0.2566270080994714
	data : 0.11701803207397461
	model : 0.06878123283386231
			 train-loss:  2.064670822253594 	 ± 0.25564086297232336
	data : 0.11681303977966309
	model : 0.06889162063598633
			 train-loss:  2.065420350045648 	 ± 0.25480661613466526
	data : 0.11684694290161132
	model : 0.0686790943145752
			 train-loss:  2.0643862187862396 	 ± 0.2541154065945966
	data : 0.11688523292541504
	model : 0.06881442070007324
			 train-loss:  2.061927354425416 	 ± 0.25472964119126795
	data : 0.11685466766357422
	model : 0.06987586021423339
			 train-loss:  2.0609711550954564 	 ± 0.25401685282984077
	data : 0.11611504554748535
	model : 0.06917915344238282
			 train-loss:  2.060515562693278 	 ± 0.2531292465147991
	data : 0.11655073165893555
	model : 0.0691490650177002
			 train-loss:  2.062184931600795 	 ± 0.25294168701038855
	data : 0.11665849685668946
	model : 0.0689570426940918
			 train-loss:  2.0644827097871876 	 ± 0.2534374560014512
	data : 0.11698217391967773
	model : 0.06867589950561523
			 train-loss:  2.065158678137738 	 ± 0.252641455879656
	data : 0.11731090545654296
	model : 0.0683929443359375
			 train-loss:  2.067919427542378 	 ± 0.2538115695728288
	data : 0.1174530029296875
	model : 0.06925725936889648
			 train-loss:  2.067696169444493 	 ± 0.252917172406351
	data : 0.11676630973815919
	model : 0.06929407119750977
			 train-loss:  2.070647046921101 	 ± 0.2544258313404604
	data : 0.11663026809692383
	model : 0.06953964233398438
			 train-loss:  2.069167713044395 	 ± 0.25413620246011254
	data : 0.1163564682006836
	model : 0.06978311538696289
			 train-loss:  2.0682471165290246 	 ± 0.25348354835753395
	data : 0.11616430282592774
	model : 0.07015666961669922
			 train-loss:  2.068377915355894 	 ± 0.25260670631670235
	data : 0.11602067947387695
	model : 0.07000589370727539
			 train-loss:  2.0673542984600726 	 ± 0.25203364869111383
	data : 0.11609549522399902
	model : 0.07002005577087403
			 train-loss:  2.0671819325995773 	 ± 0.2511776123624397
	data : 0.1160954475402832
	model : 0.06998295783996582
			 train-loss:  2.067301170355609 	 ± 0.2503259549903425
	data : 0.11620378494262695
	model : 0.06996831893920899
			 train-loss:  2.069219909004263 	 ± 0.2505611174017741
	data : 0.11624212265014648
	model : 0.06971793174743653
			 train-loss:  2.0663792942994395 	 ± 0.2520986929961299
	data : 0.11633405685424805
	model : 0.06988601684570313
			 train-loss:  2.0661804819107057 	 ± 0.25126867844097833
	data : 0.11622939109802247
	model : 0.06911940574645996
			 train-loss:  2.071234410961732 	 ± 0.2579712381786017
	data : 0.11692981719970703
	model : 0.06902961730957032
			 train-loss:  2.0739448792056034 	 ± 0.2592695097025444
	data : 0.11695032119750977
	model : 0.06824150085449218
			 train-loss:  2.077452880884308 	 ± 0.26201498304237875
	data : 0.11764760017395019
	model : 0.06831259727478027
			 train-loss:  2.0766582744462148 	 ± 0.26134778400271946
	data : 0.11759529113769532
	model : 0.06738343238830566
			 train-loss:  2.076077297426039 	 ± 0.2606031122901686
	data : 0.11839523315429687
	model : 0.06818490028381348
			 train-loss:  2.077761331429848 	 ± 0.2606112267871838
	data : 0.11768665313720703
	model : 0.06833367347717285
			 train-loss:  2.0808366886369742 	 ± 0.2626043253233254
	data : 0.11740603446960449
	model : 0.0691314697265625
			 train-loss:  2.081577531144589 	 ± 0.2619365162195154
	data : 0.11657018661499023
	model : 0.06892518997192383
			 train-loss:  2.0845074526169016 	 ± 0.26369597255964217
	data : 0.11678099632263184
	model : 0.0695878505706787
			 train-loss:  2.086065239459276 	 ± 0.2636035165406226
	data : 0.11605615615844726
	model : 0.06965970993041992
			 train-loss:  2.088372268291734 	 ± 0.2643989408836489
	data : 0.11588859558105469
	model : 0.06948165893554688
			 train-loss:  2.0854383714405107 	 ± 0.2661975288241527
	data : 0.11626238822937011
	model : 0.0692784309387207
			 train-loss:  2.086797896338387 	 ± 0.265943263527029
	data : 0.11643538475036622
	model : 0.06943178176879883
			 train-loss:  2.0868129141447023 	 ± 0.2651312904602094
	data : 0.11622896194458007
	model : 0.06961793899536133
			 train-loss:  2.0857292970021564 	 ± 0.26469066167406685
	data : 0.11620860099792481
	model : 0.06929168701171876
			 train-loss:  2.084998728281044 	 ± 0.26405900256775977
	data : 0.11644878387451171
	model : 0.06960430145263671
			 train-loss:  2.0833612452010195 	 ± 0.2641112136786909
	data : 0.11626338958740234
	model : 0.06905112266540528
			 train-loss:  2.083873389022691 	 ± 0.2634071545948559
	data : 0.11687726974487304
	model : 0.06890215873718261
			 train-loss:  2.0844756717512594 	 ± 0.2627426838603008
	data : 0.11708016395568847
	model : 0.06894516944885254
			 train-loss:  2.084069878213546 	 ± 0.2620218811230309
	data : 0.11707353591918945
	model : 0.06906485557556152
			 train-loss:  2.081344756466603 	 ± 0.2636597073578029
	data : 0.11695270538330078
	model : 0.06895761489868164
			 train-loss:  2.080988527730454 	 ± 0.26293340533627596
	data : 0.11707401275634766
	model : 0.06975760459899902
			 train-loss:  2.078317600178581 	 ± 0.26450213642094617
	data : 0.11625657081604004
	model : 0.07000608444213867
			 train-loss:  2.0792935723545907 	 ± 0.26405319450026116
	data : 0.11618518829345703
	model : 0.07015199661254883
			 train-loss:  2.0789404603413173 	 ± 0.2633388727199531
	data : 0.11606159210205078
	model : 0.0703582763671875
			 train-loss:  2.0800103213299406 	 ± 0.26297081250096
	data : 0.11589879989624023
	model : 0.07037153244018554
			 train-loss:  2.0785484893173822 	 ± 0.26294306103880466
	data : 0.11585230827331543
	model : 0.07030720710754394
			 train-loss:  2.078554331586602 	 ± 0.26220342816564185
	data : 0.11590213775634765
	model : 0.06934885978698731
			 train-loss:  2.079059768655447 	 ± 0.26155693266851304
	data : 0.11657028198242188
	model : 0.0689774513244629
			 train-loss:  2.0779912134011584 	 ± 0.2612208763639805
	data : 0.11695823669433594
	model : 0.06889591217041016
			 train-loss:  2.077373756229548 	 ± 0.2606299588781625
	data : 0.11701288223266601
	model : 0.0686990737915039
			 train-loss:  2.077379517502837 	 ± 0.25991296781463474
	data : 0.11734929084777831
	model : 0.06853961944580078
			 train-loss:  2.0783421615433824 	 ± 0.2595269843391392
	data : 0.11745328903198242
	model : 0.06936979293823242
			 train-loss:  2.0780903165755062 	 ± 0.25884320891359286
	data : 0.11670942306518554
	model : 0.06864342689514161
			 train-loss:  2.0765704051868337 	 ± 0.2589646881693664
	data : 0.11738314628601074
	model : 0.06928668022155762
			 train-loss:  2.0762904779885405 	 ± 0.2582956716141259
	data : 0.11677231788635253
	model : 0.06944236755371094
			 train-loss:  2.077361045674207 	 ± 0.25801755336695675
	data : 0.11647624969482422
	model : 0.0692641258239746
			 train-loss:  2.0750978703194476 	 ± 0.25918478515917703
	data : 0.11662206649780274
	model : 0.06937551498413086
			 train-loss:  2.075810421080816 	 ± 0.25868276571441634
	data : 0.11639065742492676
	model : 0.07051453590393067
			 train-loss:  2.0743269888978255 	 ± 0.25880588799512017
	data : 0.11530365943908691
	model : 0.06901788711547852
			 train-loss:  2.0739590291577485 	 ± 0.2581773216412165
	data : 0.11676492691040039
	model : 0.06955876350402831
			 train-loss:  2.0745687652379274 	 ± 0.2576419507938548
	data : 0.11625151634216309
	model : 0.06996731758117676
			 train-loss:  2.0744422859478493 	 ± 0.2569795938405196
	data : 0.11603736877441406
	model : 0.06906099319458008
			 train-loss:  2.0755073084044704 	 ± 0.2567431043046708
	data : 0.11691803932189941
	model : 0.068878173828125
			 train-loss:  2.0761231306271677 	 ± 0.2562275503916132
	data : 0.11701164245605469
	model : 0.06974024772644043
			 train-loss:  2.0756943031233184 	 ± 0.25564321757420067
	data : 0.11621370315551757
	model : 0.06921844482421875
			 train-loss:  2.0763713938330635 	 ± 0.2551696843776449
	data : 0.11663947105407715
	model : 0.06830806732177734
			 train-loss:  2.0765881309605607 	 ± 0.2545426793187704
	data : 0.11771035194396973
	model : 0.06920347213745118
			 train-loss:  2.075057315466991 	 ± 0.2548144034403062
	data : 0.1169344425201416
	model : 0.06914472579956055
			 train-loss:  2.0796107226610183 	 ± 0.26216730955152534
	data : 0.11696763038635254
	model : 0.06817064285278321
			 train-loss:  2.081548199131714 	 ± 0.2629458356109374
	data : 0.1178853988647461
	model : 0.06795578002929688
			 train-loss:  2.0804991126060486 	 ± 0.2627155301664905
	data : 0.11800608634948731
	model : 0.06831121444702148
			 train-loss:  2.080788347521439 	 ± 0.2620998877946722
	data : 0.11759204864501953
	model : 0.0684809684753418
			 train-loss:  2.080830857449887 	 ± 0.2614573984467558
	data : 0.11738224029541015
	model : 0.06888875961303711
			 train-loss:  2.0815898912708932 	 ± 0.2610441321425152
	data : 0.11710352897644043
	model : 0.06978235244750977
			 train-loss:  2.081971355433603 	 ± 0.2604670289625853
	data : 0.11629996299743653
	model : 0.07024664878845215
			 train-loss:  2.0820516912257614 	 ± 0.25983967817227377
	data : 0.11587920188903808
	model : 0.07115740776062011
			 train-loss:  2.083557401139003 	 ± 0.2601179763850126
	data : 0.11501736640930176
	model : 0.07058572769165039
			 train-loss:  2.085005983211207 	 ± 0.2603345697181649
	data : 0.11554174423217774
	model : 0.06936473846435547
			 train-loss:  2.084512195700691 	 ± 0.25981207457143507
	data : 0.1166499137878418
	model : 0.06907753944396973
			 train-loss:  2.0835982770151436 	 ± 0.25953381255430524
	data : 0.11691479682922364
	model : 0.0687901496887207
			 train-loss:  2.0825165498931453 	 ± 0.2593973256810195
	data : 0.11725730895996093
	model : 0.06875433921813964
			 train-loss:  2.0821357668845306 	 ± 0.2588470791396284
	data : 0.1173187255859375
	model : 0.06914997100830078
			 train-loss:  2.082651793399704 	 ± 0.25835138119209095
	data : 0.11699051856994629
	model : 0.0696861743927002
			 train-loss:  2.0817171157792558 	 ± 0.2581122768043366
	data : 0.11645894050598145
	model : 0.07013282775878907
			 train-loss:  2.081211988572721 	 ± 0.2576205940849023
	data : 0.11592631340026856
	model : 0.07022981643676758
			 train-loss:  2.0830155023232035 	 ± 0.2583894356016405
	data : 0.11581082344055176
	model : 0.06981406211853028
			 train-loss:  2.0846683191596913 	 ± 0.25894331399046444
	data : 0.1161269187927246
	model : 0.069586181640625
			 train-loss:  2.086555053109992 	 ± 0.2598489883734108
	data : 0.11617259979248047
	model : 0.06983184814453125
			 train-loss:  2.0870204492048785 	 ± 0.25934921408466227
	data : 0.11574268341064453
	model : 0.06961274147033691
			 train-loss:  2.0866243947145624 	 ± 0.2588284583522905
	data : 0.11604571342468262
	model : 0.06950807571411133
			 train-loss:  2.085504309014157 	 ± 0.2587811220788744
	data : 0.11610980033874511
	model : 0.06857943534851074
			 train-loss:  2.086939386722753 	 ± 0.25908408511122116
	data : 0.11683039665222168
	model : 0.06851415634155274
			 train-loss:  2.0914374542023455 	 ± 0.26708944605487667
	data : 0.11719369888305664
	model : 0.06840677261352539
			 train-loss:  2.0898555427127414 	 ± 0.26754488842115803
	data : 0.11728968620300292
	model : 0.06794824600219726
			 train-loss:  2.0910739181316003 	 ± 0.2675771648936581
	data : 0.11759843826293945
	model : 0.0669642448425293
			 train-loss:  2.090671324519859 	 ± 0.267055728184661
	data : 0.11835002899169922
	model : 0.06724519729614258
			 train-loss:  2.0904511249901954 	 ± 0.26649008819611336
	data : 0.11820683479309083
	model : 0.066868257522583
			 train-loss:  2.092402743980874 	 ± 0.2675355295107735
	data : 0.11840319633483887
	model : 0.06668233871459961
			 train-loss:  2.0912795647330906 	 ± 0.2674938382683652
	data : 0.1188471794128418
	model : 0.06707477569580078
			 train-loss:  2.091517841661131 	 ± 0.26693868007795013
	data : 0.11853866577148438
	model : 0.06753826141357422
			 train-loss:  2.091209150593856 	 ± 0.26640407636729246
	data : 0.11820855140686035
	model : 0.06749753952026367
			 train-loss:  2.0901818428940016 	 ± 0.26629190477750575
	data : 0.11821503639221191
	model : 0.06768980026245117
			 train-loss:  2.0902702604603562 	 ± 0.2657257232315793
	data : 0.11792383193969727
	model : 0.06757993698120117
			 train-loss:  2.089364128417157 	 ± 0.2655217935245555
	data : 0.1179011344909668
	model : 0.06675953865051269
			 train-loss:  2.0909809990454526 	 ± 0.2661154634679946
	data : 0.1187023639678955
	model : 0.06643910408020019
			 train-loss:  2.090778538446386 	 ± 0.26557165858376747
	data : 0.11884541511535644
	model : 0.06642794609069824
			 train-loss:  2.0903133534583724 	 ± 0.2651098914219222
	data : 0.11847705841064453
	model : 0.06621184349060058
			 train-loss:  2.091073163882459 	 ± 0.26481424161594735
	data : 0.11863036155700683
	model : 0.06565947532653808
			 train-loss:  2.0924437354008356 	 ± 0.2651100562242176
	data : 0.11917877197265625
	model : 0.06629142761230469
			 train-loss:  2.0913785419028823 	 ± 0.2650736175333837
	data : 0.11825790405273437
	model : 0.06692390441894532
			 train-loss:  2.090919430590858 	 ± 0.2646213790708908
	data : 0.11782703399658204
	model : 0.06692872047424317
			 train-loss:  2.0909655260941618 	 ± 0.2640773028889959
	data : 0.11805386543273926
	model : 0.0673530101776123
			 train-loss:  2.0915201091375506 	 ± 0.26367736517481016
	data : 0.11802468299865723
	model : 0.06822800636291504
			 train-loss:  2.091474722842781 	 ± 0.26313965292537356
	data : 0.11710882186889648
	model : 0.06784319877624512
			 train-loss:  2.091545071059126 	 ± 0.26260658013787713
	data : 0.11767902374267578
	model : 0.06763091087341308
			 train-loss:  2.091285035677767 	 ± 0.2621061823063669
	data : 0.11797537803649902
	model : 0.06773180961608886
			 train-loss:  2.09168335699266 	 ± 0.2616521069475699
	data : 0.11786279678344727
	model : 0.06771903038024903
			 train-loss:  2.0912754751113525 	 ± 0.2612051629533463
	data : 0.11761894226074218
	model : 0.06788163185119629
			 train-loss:  2.0925048060417177 	 ± 0.26140299727513067
	data : 0.11779093742370605
	model : 0.06842389106750488
			 train-loss:  2.0925999632869585 	 ± 0.26088609305682037
	data : 0.11728534698486329
	model : 0.06876597404479981
			 train-loss:  2.0912135058925267 	 ± 0.2612928554538759
	data : 0.11689486503601074
	model : 0.06881499290466309
			 train-loss:  2.0904962088279575 	 ± 0.26102443674162124
	data : 0.11687278747558594
	model : 0.06881790161132813
			 train-loss:  2.091655737302435 	 ± 0.26116215945229104
	data : 0.11691403388977051
	model : 0.06857738494873047
			 train-loss:  2.0922065646040675 	 ± 0.260797366977077
	data : 0.11678972244262695
	model : 0.06861753463745117
			 train-loss:  2.0946729886345565 	 ± 0.2632504702730231
	data : 0.11590566635131835
	model : 0.05945887565612793
#epoch  67    val-loss:  2.455706420697664  train-loss:  2.0946729886345565  lr:  1.953125e-05
			 train-loss:  2.3090157508850098 	 ± 0.0
	data : 5.657106399536133
	model : 0.07672286033630371
			 train-loss:  2.135046660900116 	 ± 0.1739690899848938
	data : 2.896675229072571
	model : 0.07117283344268799
			 train-loss:  1.9740238189697266 	 ± 0.2683906495392982
	data : 1.9708669185638428
	model : 0.06942534446716309
			 train-loss:  1.971077710390091 	 ± 0.23248912713798855
	data : 1.5081567764282227
	model : 0.068581223487854
			 train-loss:  1.9736306190490722 	 ± 0.2080072709315505
	data : 1.2304507732391357
	model : 0.06882429122924805
			 train-loss:  1.9633692502975464 	 ± 0.1912650841378709
	data : 0.12229781150817871
	model : 0.06743373870849609
			 train-loss:  2.0481554440089633 	 ± 0.27292577157077985
	data : 0.11830058097839355
	model : 0.06818690299987792
			 train-loss:  2.065493017435074 	 ± 0.25938688740028415
	data : 0.11753206253051758
	model : 0.06894497871398926
			 train-loss:  2.0870833132002087 	 ± 0.2520614126160382
	data : 0.11686062812805176
	model : 0.06972823143005372
			 train-loss:  2.0907260894775392 	 ± 0.23937604000411888
	data : 0.11622099876403809
	model : 0.06964335441589356
			 train-loss:  2.0833153507926245 	 ± 0.2294360636914909
	data : 0.11638407707214356
	model : 0.06877069473266602
			 train-loss:  2.1048986117045083 	 ± 0.23103760237867554
	data : 0.11723613739013672
	model : 0.06878175735473632
			 train-loss:  2.0929846121714664 	 ± 0.2257779120822533
	data : 0.11734871864318848
	model : 0.06885232925415039
			 train-loss:  2.113825159413474 	 ± 0.23017561976765183
	data : 0.11727318763732911
	model : 0.06882472038269043
			 train-loss:  2.1089123487472534 	 ± 0.22312924833091155
	data : 0.11719059944152832
	model : 0.06893792152404785
			 train-loss:  2.112680695950985 	 ± 0.21653637507158194
	data : 0.11697535514831543
	model : 0.06985054016113282
			 train-loss:  2.1552313145469215 	 ± 0.2703678351185988
	data : 0.11617646217346192
	model : 0.06984047889709473
			 train-loss:  2.14891556236479 	 ± 0.26403755799504797
	data : 0.11623806953430176
	model : 0.06931195259094239
			 train-loss:  2.130626345935621 	 ± 0.268453898446622
	data : 0.11651296615600586
	model : 0.06929574012756348
			 train-loss:  2.1301868498325347 	 ± 0.26166350680081035
	data : 0.11661911010742188
	model : 0.06930460929870605
			 train-loss:  2.139764655204046 	 ± 0.25892490593978595
	data : 0.11668081283569336
	model : 0.06939730644226075
			 train-loss:  2.144072874025865 	 ± 0.2537410375904904
	data : 0.1166964054107666
	model : 0.06937942504882813
			 train-loss:  2.140215567920519 	 ± 0.24882226846403074
	data : 0.116890287399292
	model : 0.06962022781372071
			 train-loss:  2.138790632287661 	 ± 0.24367916007036633
	data : 0.11667456626892089
	model : 0.06964774131774902
			 train-loss:  2.1222477960586548 	 ± 0.2521355232062486
	data : 0.11669001579284669
	model : 0.06978406906127929
			 train-loss:  2.1383412205255947 	 ± 0.2600042125220679
	data : 0.11660532951354981
	model : 0.0700608253479004
			 train-loss:  2.135908122415896 	 ± 0.255445346055672
	data : 0.11627054214477539
	model : 0.07030940055847168
			 train-loss:  2.1368106986795152 	 ± 0.25088618940651164
	data : 0.1160252571105957
	model : 0.07019386291503907
			 train-loss:  2.1339201886078407 	 ± 0.24699664526124826
	data : 0.1161801815032959
	model : 0.07008781433105468
			 train-loss:  2.125919751326243 	 ± 0.24663731992983898
	data : 0.11615915298461914
	model : 0.0693589210510254
			 train-loss:  2.1351474292816652 	 ± 0.24783504953267524
	data : 0.11667871475219727
	model : 0.0680929183959961
			 train-loss:  2.1436498798429966 	 ± 0.2484830161013442
	data : 0.11766486167907715
	model : 0.06788139343261719
			 train-loss:  2.140724185741309 	 ± 0.24524823081545238
	data : 0.11780929565429688
	model : 0.0681635856628418
			 train-loss:  2.1295498223865734 	 ± 0.24999653070769717
	data : 0.11764659881591796
	model : 0.06743326187133789
			 train-loss:  2.1274060828345163 	 ± 0.2467161358651372
	data : 0.11836094856262207
	model : 0.06798672676086426
			 train-loss:  2.1166665256023407 	 ± 0.25142570153870447
	data : 0.11790971755981446
	model : 0.06859393119812011
			 train-loss:  2.1196165503682316 	 ± 0.24863561348391106
	data : 0.1173543930053711
	model : 0.06862988471984863
			 train-loss:  2.121907180861423 	 ± 0.245737611029501
	data : 0.11730198860168457
	model : 0.06794171333312989
			 train-loss:  2.124117530309237 	 ± 0.2429490564648035
	data : 0.11791338920593261
	model : 0.06791553497314454
			 train-loss:  2.1266891688108442 	 ± 0.2404299432161994
	data : 0.11795210838317871
	model : 0.06791243553161622
			 train-loss:  2.124559515860023 	 ± 0.23786142693526735
	data : 0.11781859397888184
	model : 0.06830406188964844
			 train-loss:  2.1296619943210056 	 ± 0.2372728567257176
	data : 0.11754069328308106
	model : 0.06830153465270997
			 train-loss:  2.1337734405384507 	 ± 0.2360065905941678
	data : 0.11741957664489747
	model : 0.06863059997558593
			 train-loss:  2.146987955678593 	 ± 0.24888155530031916
	data : 0.11710553169250489
	model : 0.0694882869720459
			 train-loss:  2.1528335279888577 	 ± 0.24913661018179614
	data : 0.11639213562011719
	model : 0.06864962577819825
			 train-loss:  2.1474707256192747 	 ± 0.24902591745411404
	data : 0.11711373329162597
	model : 0.06849417686462403
			 train-loss:  2.1417734242500144 	 ± 0.24937439302205622
	data : 0.11724586486816406
	model : 0.06863059997558593
			 train-loss:  2.1428591584165892 	 ± 0.24687530784848855
	data : 0.11732544898986816
	model : 0.06908221244812011
			 train-loss:  2.138621517590114 	 ± 0.24610070614476576
	data : 0.11681294441223145
	model : 0.06850724220275879
			 train-loss:  2.132626640796661 	 ± 0.24721495758614304
	data : 0.11723484992980956
	model : 0.06941089630126954
			 train-loss:  2.1298508994719563 	 ± 0.2455649288950668
	data : 0.11664152145385742
	model : 0.0694694995880127
			 train-loss:  2.129300261919315 	 ± 0.2432240554751196
	data : 0.11651315689086914
	model : 0.06846466064453124
			 train-loss:  2.1227037096923254 	 ± 0.24556976128874497
	data : 0.11706409454345704
	model : 0.06838054656982422
			 train-loss:  2.121872376512598 	 ± 0.2433606102164505
	data : 0.11714015007019044
	model : 0.06907076835632324
			 train-loss:  2.125107023932717 	 ± 0.24230678601330333
	data : 0.11649622917175292
	model : 0.06906337738037109
			 train-loss:  2.1287560888699124 	 ± 0.24165368146549082
	data : 0.1164708137512207
	model : 0.06845302581787109
			 train-loss:  2.1218446974168743 	 ± 0.2450448373596239
	data : 0.1171658992767334
	model : 0.06907734870910645
			 train-loss:  2.122301303107163 	 ± 0.24294765598698662
	data : 0.1168628215789795
	model : 0.06920943260192872
			 train-loss:  2.129185535139957 	 ± 0.24651965014353922
	data : 0.11680474281311035
	model : 0.06905279159545899
			 train-loss:  2.1349496642748513 	 ± 0.2484338104434504
	data : 0.11680521965026855
	model : 0.06899628639221192
			 train-loss:  2.133372803203395 	 ± 0.24669161895950914
	data : 0.11672053337097169
	model : 0.06958355903625488
			 train-loss:  2.1347837755756993 	 ± 0.24494210688994836
	data : 0.11605911254882813
	model : 0.06979269981384277
			 train-loss:  2.1419418312254406 	 ± 0.24944147477697232
	data : 0.11578211784362794
	model : 0.06983933448791504
			 train-loss:  2.147857215255499 	 ± 0.2518994379530128
	data : 0.11593012809753418
	model : 0.06996283531188965
			 train-loss:  2.1470908091618464 	 ± 0.25002942631607344
	data : 0.11582465171813965
	model : 0.07004213333129883
			 train-loss:  2.1503233042630283 	 ± 0.24949290167230015
	data : 0.11584968566894531
	model : 0.07002768516540528
			 train-loss:  2.1469652350269146 	 ± 0.24912227952442448
	data : 0.11591863632202148
	model : 0.06952614784240722
			 train-loss:  2.1564062115024116 	 ± 0.259077365804947
	data : 0.11651148796081542
	model : 0.06936707496643066
			 train-loss:  2.152271789053212 	 ± 0.2594429931737275
	data : 0.11654481887817383
	model : 0.06931958198547364
			 train-loss:  2.1578530652182444 	 ± 0.261722143053432
	data : 0.11674509048461915
	model : 0.06925597190856933
			 train-loss:  2.155180382056975 	 ± 0.2608327803552653
	data : 0.11682872772216797
	model : 0.06905827522277833
			 train-loss:  2.1551033639245563 	 ± 0.2590159211693343
	data : 0.11708416938781738
	model : 0.0697136402130127
			 train-loss:  2.145185269721567 	 ± 0.2706524622116514
	data : 0.11649703979492188
	model : 0.06909561157226562
			 train-loss:  2.1445165566495947 	 ± 0.26887821989920613
	data : 0.1172457218170166
	model : 0.06895942687988281
			 train-loss:  2.1392273171742757 	 ± 0.27092763828588706
	data : 0.11725692749023438
	model : 0.06896409988403321
			 train-loss:  2.1376979225560238 	 ± 0.269465027075045
	data : 0.1171541690826416
	model : 0.0691573143005371
			 train-loss:  2.135355529847083 	 ± 0.2684872300221469
	data : 0.11696562767028809
	model : 0.06855497360229493
			 train-loss:  2.1297003535123973 	 ± 0.271336985086549
	data : 0.11750230789184571
	model : 0.06845226287841796
			 train-loss:  2.1265069472638864 	 ± 0.27108530934723285
	data : 0.1175724983215332
	model : 0.06869316101074219
			 train-loss:  2.1262123346328736 	 ± 0.2693984248537326
	data : 0.11745529174804688
	model : 0.06854372024536133
			 train-loss:  2.1227774458166992 	 ± 0.26948728029574054
	data : 0.11754798889160156
	model : 0.06850557327270508
			 train-loss:  2.122638206656386 	 ± 0.2678419561662383
	data : 0.1175004005432129
	model : 0.06827034950256347
			 train-loss:  2.121657867029489 	 ± 0.26637152963322214
	data : 0.11760182380676269
	model : 0.0689624309539795
			 train-loss:  2.1186387822741555 	 ± 0.26620600714597764
	data : 0.1168093204498291
	model : 0.06930203437805176
			 train-loss:  2.1184080236098346 	 ± 0.2646439075676516
	data : 0.11663904190063476
	model : 0.0694958209991455
			 train-loss:  2.1170635306557943 	 ± 0.263392620065024
	data : 0.11669225692749023
	model : 0.06919832229614258
			 train-loss:  2.1119657343831557 	 ± 0.2661074559120518
	data : 0.11688380241394043
	model : 0.06994681358337403
			 train-loss:  2.114786593751474 	 ± 0.26589615158082114
	data : 0.11631455421447753
	model : 0.06978464126586914
			 train-loss:  2.11173458045788 	 ± 0.2659437420168042
	data : 0.11635346412658691
	model : 0.0692516803741455
			 train-loss:  2.109325298998091 	 ± 0.2654370760815399
	data : 0.11658673286437989
	model : 0.0692483901977539
			 train-loss:  2.1111403572690355 	 ± 0.26453561129307046
	data : 0.1165593147277832
	model : 0.06948347091674804
			 train-loss:  2.1077908096106155 	 ± 0.2650272044327475
	data : 0.11626310348510742
	model : 0.06952447891235351
			 train-loss:  2.1081009449497348 	 ± 0.26361526025632825
	data : 0.11619338989257813
	model : 0.06897249221801757
			 train-loss:  2.1106342736710895 	 ± 0.2633449625648223
	data : 0.11680216789245605
	model : 0.06916694641113282
			 train-loss:  2.110487471128765 	 ± 0.26195913633289014
	data : 0.11667203903198242
	model : 0.06919035911560059
			 train-loss:  2.111439829071363 	 ± 0.2607564651328093
	data : 0.1166761875152588
	model : 0.06919560432434083
			 train-loss:  2.1162390094442465 	 ± 0.26363620846125757
	data : 0.116829252243042
	model : 0.06915493011474609
			 train-loss:  2.1186348710741316 	 ± 0.2633469579162292
	data : 0.11675333976745605
	model : 0.06951541900634765
			 train-loss:  2.119551227550314 	 ± 0.26217053672731816
	data : 0.11633057594299316
	model : 0.06936359405517578
			 train-loss:  2.121275043487549 	 ± 0.26141966074068
	data : 0.11639747619628907
	model : 0.06838436126708984
			 train-loss:  2.1191927076566337 	 ± 0.26095443135368473
	data : 0.117100191116333
	model : 0.06857113838195801
			 train-loss:  2.1211016937798144 	 ± 0.2603798427599717
	data : 0.11704654693603515
	model : 0.06862974166870117
			 train-loss:  2.1211852969475165 	 ± 0.2591141557508769
	data : 0.11706643104553223
	model : 0.06910204887390137
			 train-loss:  2.121641504076811 	 ± 0.25790696821851616
	data : 0.11670122146606446
	model : 0.07047572135925292
			 train-loss:  2.120579711596171 	 ± 0.25690420090207355
	data : 0.11549434661865235
	model : 0.07136621475219726
			 train-loss:  2.1193611869272195 	 ± 0.25599420572533504
	data : 0.11477508544921874
	model : 0.07101483345031738
			 train-loss:  2.118422488185847 	 ± 0.2549783864837681
	data : 0.11516170501708985
	model : 0.07099995613098145
			 train-loss:  2.116455838636116 	 ± 0.25460919331072807
	data : 0.11519575119018555
	model : 0.070672607421875
			 train-loss:  2.1141704266224433 	 ± 0.2545490218032322
	data : 0.11538925170898437
	model : 0.06871619224548339
			 train-loss:  2.11612041646784 	 ± 0.2542058720223168
	data : 0.11719903945922852
	model : 0.06857352256774903
			 train-loss:  2.1172560335279584 	 ± 0.25333834381374304
	data : 0.11725120544433594
	model : 0.06887116432189941
			 train-loss:  2.1165210838828767 	 ± 0.2523236704422728
	data : 0.11690020561218262
	model : 0.06897306442260742
			 train-loss:  2.1135598959121027 	 ± 0.25315192047507523
	data : 0.11678252220153809
	model : 0.06905508041381836
			 train-loss:  2.110729719463148 	 ± 0.2538283995629275
	data : 0.1167567253112793
	model : 0.06874094009399415
			 train-loss:  2.111844834037449 	 ± 0.253002692234333
	data : 0.11710910797119141
	model : 0.06885743141174316
			 train-loss:  2.1123700347439995 	 ± 0.25197275631475896
	data : 0.11707234382629395
	model : 0.0688436508178711
			 train-loss:  2.110631405797779 	 ± 0.2515914672563898
	data : 0.11711745262145996
	model : 0.06874256134033203
			 train-loss:  2.111582338809967 	 ± 0.2507342027332403
	data : 0.11727771759033204
	model : 0.06940507888793945
			 train-loss:  2.110692051278443 	 ± 0.24986570217582993
	data : 0.11673130989074706
	model : 0.07039427757263184
			 train-loss:  2.114457741379738 	 ± 0.25219053397222363
	data : 0.11585545539855957
	model : 0.0705568790435791
			 train-loss:  2.113002202727578 	 ± 0.25165189485066913
	data : 0.11576848030090332
	model : 0.07053866386413574
			 train-loss:  2.1122996914582175 	 ± 0.25073752181186854
	data : 0.11583361625671387
	model : 0.07034835815429688
			 train-loss:  2.11244538644465 	 ± 0.2497213686935777
	data : 0.11602888107299805
	model : 0.07073960304260254
			 train-loss:  2.1137853062921956 	 ± 0.2491559451241425
	data : 0.11555113792419433
	model : 0.06991000175476074
			 train-loss:  2.115365918159485 	 ± 0.248780724557352
	data : 0.11622519493103027
	model : 0.06907358169555664
			 train-loss:  2.1166686964413475 	 ± 0.24821925321709673
	data : 0.11698031425476074
	model : 0.06922903060913085
			 train-loss:  2.1190877270510815 	 ± 0.24872670317837284
	data : 0.11666975021362305
	model : 0.0692824363708496
			 train-loss:  2.121533918194473 	 ± 0.24928217317805754
	data : 0.11656885147094727
	model : 0.06830682754516601
			 train-loss:  2.1188997704853385 	 ± 0.25009606552706526
	data : 0.1174931526184082
	model : 0.06843333244323731
			 train-loss:  2.1177260050406823 	 ± 0.24948873676726013
	data : 0.1174241542816162
	model : 0.0690216064453125
			 train-loss:  2.1197048052576664 	 ± 0.24955663579027085
	data : 0.11690645217895508
	model : 0.06894350051879883
			 train-loss:  2.1174178114443114 	 ± 0.24998376381513707
	data : 0.11689963340759277
	model : 0.06915144920349121
			 train-loss:  2.1178844987897945 	 ± 0.24909991491329808
	data : 0.11687922477722168
	model : 0.06922855377197265
			 train-loss:  2.117633715494355 	 ± 0.24818554923814048
	data : 0.11682300567626953
	model : 0.06960258483886719
			 train-loss:  2.1165077642158225 	 ± 0.2476079166151987
	data : 0.1164623737335205
	model : 0.06986966133117675
			 train-loss:  2.118011408868958 	 ± 0.24731377190726442
	data : 0.11617460250854492
	model : 0.06974658966064454
			 train-loss:  2.1185323748275313 	 ± 0.2464844003774876
	data : 0.11622090339660644
	model : 0.06875019073486328
			 train-loss:  2.1154281004615454 	 ± 0.24826299028819945
	data : 0.11711668968200684
	model : 0.06880927085876465
			 train-loss:  2.1128476154889992 	 ± 0.24921883388029817
	data : 0.11714677810668946
	model : 0.0690464973449707
			 train-loss:  2.1142310764108387 	 ± 0.2488622613465163
	data : 0.11696257591247558
	model : 0.06872010231018066
			 train-loss:  2.1151773667504603 	 ± 0.24823084663646447
	data : 0.11721568107604981
	model : 0.06841435432434081
			 train-loss:  2.1152352808227 	 ± 0.24735620604316605
	data : 0.11768379211425781
	model : 0.06933889389038086
			 train-loss:  2.11190862505586 	 ± 0.24965714002828543
	data : 0.11680240631103515
	model : 0.0694089412689209
			 train-loss:  2.111359156668186 	 ± 0.2488755178332987
	data : 0.11664543151855469
	model : 0.06870622634887695
			 train-loss:  2.1101102607003575 	 ± 0.2484682281948138
	data : 0.1172365665435791
	model : 0.06910490989685059
			 train-loss:  2.1094042740456045 	 ± 0.24776173724234396
	data : 0.11692018508911133
	model : 0.06957416534423828
			 train-loss:  2.106655022724956 	 ± 0.2491421508373402
	data : 0.11634621620178223
	model : 0.06900086402893066
			 train-loss:  2.1082803824463405 	 ± 0.2490798083831818
	data : 0.11693754196166992
	model : 0.06800613403320313
			 train-loss:  2.1063446846584344 	 ± 0.24935700331541896
	data : 0.11771979331970214
	model : 0.0686790943145752
			 train-loss:  2.1120701384544374 	 ± 0.2581641558313566
	data : 0.11693325042724609
	model : 0.06844325065612793
			 train-loss:  2.112760396193195 	 ± 0.2574467272876905
	data : 0.11710028648376465
	model : 0.06851673126220703
			 train-loss:  2.11275278031826 	 ± 0.25659848264968205
	data : 0.11710910797119141
	model : 0.06909408569335937
			 train-loss:  2.114762520478442 	 ± 0.2569559743614738
	data : 0.11645174026489258
	model : 0.07009830474853515
			 train-loss:  2.1142366466584144 	 ± 0.25620292972721176
	data : 0.11577115058898926
	model : 0.07021107673645019
			 train-loss:  2.113986769799263 	 ± 0.2553939569169137
	data : 0.11573338508605957
	model : 0.07015080451965332
			 train-loss:  2.1138611214283185 	 ± 0.25457887665746287
	data : 0.11593589782714844
	model : 0.06992483139038086
			 train-loss:  2.1153283141980506 	 ± 0.2544276192507947
	data : 0.11604189872741699
	model : 0.06980996131896973
			 train-loss:  2.113626526880868 	 ± 0.2545159972589876
	data : 0.11626276969909669
	model : 0.06972475051879883
			 train-loss:  2.110702986237388 	 ± 0.25636189613151894
	data : 0.11627693176269531
	model : 0.06974477767944336
			 train-loss:  2.1087849348783494 	 ± 0.2567014050282964
	data : 0.11642484664916992
	model : 0.06889281272888184
			 train-loss:  2.107388001791439 	 ± 0.25651227916016045
	data : 0.11694474220275879
	model : 0.06874985694885254
			 train-loss:  2.111331615918948 	 ± 0.26056913161243767
	data : 0.11731328964233398
	model : 0.06863627433776856
			 train-loss:  2.110419944751482 	 ± 0.2600276450313588
	data : 0.1172220230102539
	model : 0.06871800422668457
			 train-loss:  2.107910452092566 	 ± 0.26120603860555275
	data : 0.11752281188964844
	model : 0.06880717277526856
			 train-loss:  2.1047396421432496 	 ± 0.26356014461699295
	data : 0.11740140914916992
	model : 0.06992549896240234
			 train-loss:  2.106445754148874 	 ± 0.26367741143379547
	data : 0.11663594245910644
	model : 0.06963028907775878
			 train-loss:  2.1083715411717305 	 ± 0.26405509296317875
	data : 0.11674990653991699
	model : 0.06926932334899902
			 train-loss:  2.107869429957299 	 ± 0.26334799244862866
	data : 0.11704068183898926
	model : 0.0695577621459961
			 train-loss:  2.106512384301812 	 ± 0.263156192279029
	data : 0.1166231632232666
	model : 0.06914191246032715
			 train-loss:  2.105986269782571 	 ± 0.2624701892592362
	data : 0.11694483757019043
	model : 0.06902937889099121
			 train-loss:  2.1053754781421863 	 ± 0.26182275022469786
	data : 0.11688504219055176
	model : 0.06957130432128907
			 train-loss:  2.1054398888765378 	 ± 0.2610618868349267
	data : 0.11650333404541016
	model : 0.06928052902221679
			 train-loss:  2.1040614567740117 	 ± 0.2609332709846439
	data : 0.11677489280700684
	model : 0.06890735626220704
			 train-loss:  2.105206628640493 	 ± 0.26061801067204676
	data : 0.11711783409118652
	model : 0.06917438507080079
			 train-loss:  2.104783789089748 	 ± 0.2599321704733949
	data : 0.11682233810424805
	model : 0.0689249038696289
			 train-loss:  2.104717224159024 	 ± 0.25919417069631
	data : 0.11707167625427246
	model : 0.0687281608581543
			 train-loss:  2.1020109033853993 	 ± 0.2609427432469579
	data : 0.11720213890075684
	model : 0.06963324546813965
			 train-loss:  2.1006475050797624 	 ± 0.2608401775460206
	data : 0.11651773452758789
	model : 0.06986460685729981
			 train-loss:  2.099188268517649 	 ± 0.2608381260617743
	data : 0.11630887985229492
	model : 0.06984286308288574
			 train-loss:  2.099065602487988 	 ± 0.26011774391836634
	data : 0.11638860702514649
	model : 0.06916475296020508
			 train-loss:  2.0968802001594837 	 ± 0.2610499927073732
	data : 0.11703572273254395
	model : 0.06931171417236329
			 train-loss:  2.097773778569567 	 ± 0.2606092657489139
	data : 0.11703195571899414
	model : 0.06920375823974609
			 train-loss:  2.096464141470487 	 ± 0.2604960928497786
	data : 0.11697111129760743
	model : 0.06881179809570312
			 train-loss:  2.0955147186051244 	 ± 0.26010454943698613
	data : 0.1173738956451416
	model : 0.0686028003692627
			 train-loss:  2.0958536934208225 	 ± 0.2594413606712793
	data : 0.11771912574768066
	model : 0.06944966316223145
			 train-loss:  2.0950585558850277 	 ± 0.25896892470127714
	data : 0.11696906089782715
	model : 0.06943387985229492
			 train-loss:  2.0952145740947623 	 ± 0.2582843310456255
	data : 0.11668839454650878
	model : 0.06891112327575684
			 train-loss:  2.0970278680324554 	 ± 0.2587871973901289
	data : 0.11717476844787597
	model : 0.06828141212463379
			 train-loss:  2.0963937353204796 	 ± 0.2582480783510578
	data : 0.11760611534118652
	model : 0.0675889015197754
			 train-loss:  2.093324860146171 	 ± 0.26100011513662535
	data : 0.11818718910217285
	model : 0.0671389102935791
			 train-loss:  2.092413764349453 	 ± 0.2606187320417601
	data : 0.11851000785827637
	model : 0.06690707206726074
			 train-loss:  2.092691281810403 	 ± 0.25996744503103025
	data : 0.11885766983032227
	model : 0.06742138862609863
			 train-loss:  2.0929772736495025 	 ± 0.25932336001936407
	data : 0.11839709281921387
	model : 0.06753807067871094
			 train-loss:  2.0921128773197686 	 ± 0.25893274936461597
	data : 0.11849184036254883
	model : 0.0684903621673584
			 train-loss:  2.091846943512941 	 ± 0.2582945257270922
	data : 0.1176518440246582
	model : 0.06900200843811036
			 train-loss:  2.09160277369071 	 ± 0.25765732986848466
	data : 0.11715846061706543
	model : 0.07009010314941407
			 train-loss:  2.0920820496409074 	 ± 0.2570901214200838
	data : 0.1160761833190918
	model : 0.07019619941711426
			 train-loss:  2.0907328423827587 	 ± 0.2571383415508852
	data : 0.11600489616394043
	model : 0.07096905708312988
			 train-loss:  2.0907264288945413 	 ± 0.2564914674928712
	data : 0.11513409614562989
	model : 0.07080059051513672
			 train-loss:  2.089871833324432 	 ± 0.2561333049727522
	data : 0.11540007591247559
	model : 0.07081789970397949
			 train-loss:  2.088592762970806 	 ± 0.2561348955289024
	data : 0.11553378105163574
	model : 0.07033648490905761
			 train-loss:  2.0863697286879663 	 ± 0.25743664313087417
	data : 0.11622557640075684
	model : 0.07030057907104492
			 train-loss:  2.0853584341227713 	 ± 0.25720369931083353
	data : 0.11628179550170899
	model : 0.07015700340270996
			 train-loss:  2.087479563320384 	 ± 0.25834627008603456
	data : 0.11655607223510742
	model : 0.07023072242736816
			 train-loss:  2.0886320067615043 	 ± 0.2582405048338507
	data : 0.11622099876403809
	model : 0.0701479434967041
			 train-loss:  2.0878209789979807 	 ± 0.2578745265622104
	data : 0.11621980667114258
	model : 0.06979403495788575
			 train-loss:  2.086385372756184 	 ± 0.25807475230752325
	data : 0.11643061637878419
	model : 0.06982989311218261
			 train-loss:  2.0864524571941447 	 ± 0.2574554420561874
	data : 0.11641745567321778
	model : 0.06934661865234375
			 train-loss:  2.087027351822009 	 ± 0.2569725752747618
	data : 0.11664109230041504
	model : 0.06938819885253907
			 train-loss:  2.0862023569288706 	 ± 0.2566372953826598
	data : 0.11675529479980469
	model : 0.06861667633056641
			 train-loss:  2.086217919797129 	 ± 0.25602852719912295
	data : 0.11744070053100586
	model : 0.06852025985717773
			 train-loss:  2.087103750345842 	 ± 0.25574787696116363
	data : 0.11736278533935547
	model : 0.06844043731689453
			 train-loss:  2.085638357999739 	 ± 0.25603738985763497
	data : 0.11737279891967774
	model : 0.06856422424316407
			 train-loss:  2.08447282615109 	 ± 0.2560042294342641
	data : 0.11723527908325196
	model : 0.06861963272094726
			 train-loss:  2.0849599987961525 	 ± 0.25550758691155656
	data : 0.11734175682067871
	model : 0.0694580078125
			 train-loss:  2.08563351907112 	 ± 0.25510667571895296
	data : 0.11649045944213868
	model : 0.06953277587890624
			 train-loss:  2.086387973227259 	 ± 0.2547596088187004
	data : 0.11652412414550781
	model : 0.06879806518554688
			 train-loss:  2.0858021881602227 	 ± 0.254321062569174
	data : 0.11734871864318848
	model : 0.06846280097961426
			 train-loss:  2.085126034201008 	 ± 0.2539360750089096
	data : 0.11771106719970703
	model : 0.06822443008422852
			 train-loss:  2.083861222592267 	 ± 0.2540487514953476
	data : 0.11794357299804688
	model : 0.0676424503326416
			 train-loss:  2.084418281171117 	 ± 0.2536079604404783
	data : 0.11849660873413086
	model : 0.06774859428405762
			 train-loss:  2.084112689838753 	 ± 0.25307690474077876
	data : 0.11849207878112793
	model : 0.06769337654113769
			 train-loss:  2.0827686230697973 	 ± 0.25330170893087406
	data : 0.1183553695678711
	model : 0.0678243637084961
			 train-loss:  2.080929751907076 	 ± 0.2542230950422358
	data : 0.11816639900207519
	model : 0.06717219352722167
			 train-loss:  2.0822624100579157 	 ± 0.2544404829624576
	data : 0.1186431884765625
	model : 0.06750264167785644
			 train-loss:  2.0830968829382837 	 ± 0.2541853201805157
	data : 0.11820931434631347
	model : 0.0671201229095459
			 train-loss:  2.0851783689423278 	 ± 0.2555478715339121
	data : 0.118190336227417
	model : 0.06769890785217285
			 train-loss:  2.0860878710161175 	 ± 0.2553547796723547
	data : 0.11757512092590332
	model : 0.0679926872253418
			 train-loss:  2.086416605778657 	 ± 0.25484497255820876
	data : 0.11714606285095215
	model : 0.06857366561889648
			 train-loss:  2.0857700954312866 	 ± 0.2544784916608818
	data : 0.11635675430297851
	model : 0.06870574951171875
			 train-loss:  2.0868879855969253 	 ± 0.25449240745717217
	data : 0.11636271476745605
	model : 0.06838755607604981
			 train-loss:  2.0884813784525313 	 ± 0.25509548424428746
	data : 0.11709613800048828
	model : 0.06817255020141602
			 train-loss:  2.087159054473746 	 ± 0.2553430657628297
	data : 0.11741018295288086
	model : 0.06829915046691895
			 train-loss:  2.0871568734829244 	 ± 0.25479687893632846
	data : 0.11743278503417968
	model : 0.06850056648254395
			 train-loss:  2.0878914518559233 	 ± 0.2545023687291249
	data : 0.1175227165222168
	model : 0.06857533454895019
			 train-loss:  2.0888336209927574 	 ± 0.25437296593463926
	data : 0.11751556396484375
	model : 0.06910333633422852
			 train-loss:  2.0893402280686777 	 ± 0.25395502739022235
	data : 0.11700434684753418
	model : 0.06944074630737304
			 train-loss:  2.08907422999374 	 ± 0.25345402979486586
	data : 0.11666035652160645
	model : 0.06937971115112304
			 train-loss:  2.0889573276791116 	 ± 0.2529296652543233
	data : 0.1165170669555664
	model : 0.06922492980957032
			 train-loss:  2.089571358760198 	 ± 0.25258062244312196
	data : 0.11647777557373047
	model : 0.06898345947265624
			 train-loss:  2.089944105425316 	 ± 0.2521221900800853
	data : 0.11662096977233886
	model : 0.06875076293945312
			 train-loss:  2.092385039841833 	 ± 0.2544382984663097
	data : 0.11648879051208497
	model : 0.06858868598937988
			 train-loss:  2.093172609070201 	 ± 0.2542096318302342
	data : 0.11659069061279297
	model : 0.06854252815246582
			 train-loss:  2.0931018981777254 	 ± 0.2536905703014249
	data : 0.11667938232421875
	model : 0.06839871406555176
			 train-loss:  2.0924151790385346 	 ± 0.2533994521398931
	data : 0.11650595664978028
	model : 0.06835246086120605
			 train-loss:  2.0925373032810244 	 ± 0.2528911127004062
	data : 0.11645593643188476
	model : 0.06836647987365722
			 train-loss:  2.0907390735410005 	 ± 0.25394972963012846
	data : 0.1164863109588623
	model : 0.06837239265441894
			 train-loss:  2.0908431343493925 	 ± 0.2534424937888619
	data : 0.1165001392364502
	model : 0.06821703910827637
			 train-loss:  2.0933883257180335 	 ± 0.25608919771798316
	data : 0.1167816162109375
	model : 0.0684992790222168
			 train-loss:  2.0915811042785646 	 ± 0.25716258818610566
	data : 0.1170015811920166
	model : 0.06876168251037598
			 train-loss:  2.0908267711738193 	 ± 0.2569267889685297
	data : 0.11689224243164062
	model : 0.06903858184814453
			 train-loss:  2.0906405964541057 	 ± 0.25643347072274886
	data : 0.11678390502929688
	model : 0.0691915512084961
			 train-loss:  2.089086107585741 	 ± 0.2571131135346552
	data : 0.11667418479919434
	model : 0.06963901519775391
			 train-loss:  2.0878756703354244 	 ± 0.25732775565074184
	data : 0.11637048721313477
	model : 0.06963696479797363
			 train-loss:  2.087442653319415 	 ± 0.25691540050592554
	data : 0.11645359992980957
	model : 0.06963181495666504
			 train-loss:  2.089425972662866 	 ± 0.2583616566471348
	data : 0.11535277366638183
	model : 0.06079106330871582
#epoch  68    val-loss:  2.4536272726560893  train-loss:  2.089425972662866  lr:  1.953125e-05
			 train-loss:  1.9225609302520752 	 ± 0.0
	data : 5.646622180938721
	model : 0.07246637344360352
			 train-loss:  1.9291266798973083 	 ± 0.006565749645233154
	data : 2.888582229614258
	model : 0.06979012489318848
			 train-loss:  1.9935750563939412 	 ± 0.09130129158472736
	data : 1.9655532836914062
	model : 0.06959064801534016
			 train-loss:  2.040980190038681 	 ± 0.11398984372042308
	data : 1.5034024715423584
	model : 0.06952577829360962
			 train-loss:  2.072478938102722 	 ± 0.119848371267752
	data : 1.2259990692138671
	model : 0.0687929630279541
			 train-loss:  2.064263125260671 	 ± 0.110937781284025
	data : 0.12044157981872558
	model : 0.0674710750579834
			 train-loss:  2.0717310394559587 	 ± 0.10432468347787839
	data : 0.11830472946166992
	model : 0.06783666610717773
			 train-loss:  2.0440139919519424 	 ± 0.12206894646589632
	data : 0.11761054992675782
	model : 0.0672231674194336
			 train-loss:  2.08540501859453 	 ± 0.16416734407992978
	data : 0.11811766624450684
	model : 0.06739468574523926
			 train-loss:  2.1265941977500917 	 ± 0.19880835367663213
	data : 0.11808190345764161
	model : 0.06816349029541016
			 train-loss:  2.1205380721525713 	 ± 0.19052130570763232
	data : 0.11744747161865235
	model : 0.06908111572265625
			 train-loss:  2.129928002754847 	 ± 0.18504967629900668
	data : 0.11667408943176269
	model : 0.06839823722839355
			 train-loss:  2.105990712459271 	 ± 0.1961764581627741
	data : 0.11748857498168945
	model : 0.06843352317810059
			 train-loss:  2.110621086188725 	 ± 0.18977614279986435
	data : 0.11755547523498536
	model : 0.06824913024902343
			 train-loss:  2.0860291878382364 	 ± 0.20513567685881837
	data : 0.11771221160888672
	model : 0.06759033203125
			 train-loss:  2.088480405509472 	 ± 0.19884851664365477
	data : 0.11845450401306153
	model : 0.06657724380493164
			 train-loss:  2.1233171084347893 	 ± 0.23797550516243382
	data : 0.11928696632385254
	model : 0.06740708351135254
			 train-loss:  2.1238751345210605 	 ± 0.23128206495615913
	data : 0.11844573020935059
	model : 0.0673898696899414
			 train-loss:  2.105405010675129 	 ± 0.2383624878511577
	data : 0.1183751106262207
	model : 0.06747984886169434
			 train-loss:  2.1093713104724885 	 ± 0.23296939923825133
	data : 0.1182701587677002
	model : 0.06807236671447754
			 train-loss:  2.0838910795393444 	 ± 0.2543129427973583
	data : 0.11757884025573731
	model : 0.06900978088378906
			 train-loss:  2.0779932032931936 	 ± 0.24993155195578418
	data : 0.11676101684570313
	model : 0.06821465492248535
			 train-loss:  2.0779244640599126 	 ± 0.2444380925873603
	data : 0.1174818992614746
	model : 0.06894278526306152
			 train-loss:  2.0894587387641272 	 ± 0.24560192771941608
	data : 0.11680827140808106
	model : 0.0689915657043457
			 train-loss:  2.078864164352417 	 ± 0.24617346483867802
	data : 0.11674304008483886
	model : 0.06921281814575195
			 train-loss:  2.097179256952726 	 ± 0.2581794300622985
	data : 0.11673488616943359
	model : 0.06929507255554199
			 train-loss:  2.0843092423898204 	 ± 0.2617143932224102
	data : 0.11652917861938476
	model : 0.07060918807983399
			 train-loss:  2.0773034393787384 	 ± 0.2595638431141646
	data : 0.11530675888061523
	model : 0.07116780281066895
			 train-loss:  2.079750763958898 	 ± 0.25537789937005395
	data : 0.11473283767700196
	model : 0.07114186286926269
			 train-loss:  2.077251613140106 	 ± 0.25144595706979067
	data : 0.11479239463806153
	model : 0.07107844352722167
			 train-loss:  2.0882571858744465 	 ± 0.25459620951163037
	data : 0.11492419242858887
	model : 0.0710411548614502
			 train-loss:  2.079440053552389 	 ± 0.25535000579070294
	data : 0.11504263877868652
	model : 0.07059698104858399
			 train-loss:  2.075526082154476 	 ± 0.252424189662024
	data : 0.11567068099975586
	model : 0.0705794334411621
			 train-loss:  2.0679415885139916 	 ± 0.252472223195502
	data : 0.11577916145324707
	model : 0.07089376449584961
			 train-loss:  2.068358203342983 	 ± 0.24885119732306027
	data : 0.11563959121704101
	model : 0.07084040641784668
			 train-loss:  2.0766246252589755 	 ± 0.25019673624240546
	data : 0.11562924385070801
	model : 0.07161388397216797
			 train-loss:  2.0889091942761397 	 ± 0.25756424613473805
	data : 0.11475062370300293
	model : 0.07157044410705567
			 train-loss:  2.0857798331662227 	 ± 0.25486448574435283
	data : 0.11462798118591308
	model : 0.07064280509948731
			 train-loss:  2.074739621235774 	 ± 0.26061858847725805
	data : 0.11537332534790039
	model : 0.070379638671875
			 train-loss:  2.0699751734733582 	 ± 0.25905461968498505
	data : 0.11555814743041992
	model : 0.07043824195861817
			 train-loss:  2.0635347279106697 	 ± 0.259097775839342
	data : 0.11548399925231934
	model : 0.06970925331115722
			 train-loss:  2.059470698946998 	 ± 0.2573139215757805
	data : 0.11629376411437989
	model : 0.07026443481445313
			 train-loss:  2.0572022615477095 	 ± 0.2547288758210399
	data : 0.11577491760253907
	model : 0.07071938514709472
			 train-loss:  2.0558685768734324 	 ± 0.2519694126473968
	data : 0.11544008255004883
	model : 0.0705639362335205
			 train-loss:  2.0514696492089164 	 ± 0.25085683628262717
	data : 0.11541128158569336
	model : 0.07035131454467773
			 train-loss:  2.0487350562344426 	 ± 0.24879235760496987
	data : 0.11562161445617676
	model : 0.06940159797668458
			 train-loss:  2.0462226766221066 	 ± 0.2467205312877019
	data : 0.11650185585021973
	model : 0.06863932609558106
			 train-loss:  2.0509627014398575 	 ± 0.24629019987622008
	data : 0.11723690032958985
	model : 0.06859145164489747
			 train-loss:  2.050274055831286 	 ± 0.24381076628754245
	data : 0.11731305122375488
	model : 0.0687943458557129
			 train-loss:  2.0483032131195067 	 ± 0.24175430252903138
	data : 0.11713323593139649
	model : 0.06880989074707031
			 train-loss:  2.052395189509672 	 ± 0.24111485466100124
	data : 0.11712284088134765
	model : 0.0695723056793213
			 train-loss:  2.0504832611634183 	 ± 0.23917523896198561
	data : 0.11628136634826661
	model : 0.06929445266723633
			 train-loss:  2.054695190123792 	 ± 0.2388471418053299
	data : 0.11655158996582031
	model : 0.06885790824890137
			 train-loss:  2.0543412742791354 	 ± 0.23663928686548663
	data : 0.1168281078338623
	model : 0.06855635643005371
			 train-loss:  2.0578881762244485 	 ± 0.23592234229199768
	data : 0.11716055870056152
	model : 0.06827011108398437
			 train-loss:  2.0563482365437915 	 ± 0.23408516053334505
	data : 0.11734590530395508
	model : 0.06829752922058105
			 train-loss:  2.050571644515322 	 ± 0.23601524596705628
	data : 0.11758770942687988
	model : 0.06882905960083008
			 train-loss:  2.052225776787462 	 ± 0.2343048380761438
	data : 0.11704111099243164
	model : 0.06934785842895508
			 train-loss:  2.0539773177292386 	 ± 0.2326933769696452
	data : 0.11656317710876465
	model : 0.06952629089355469
			 train-loss:  2.0540710270404814 	 ± 0.23074724049367148
	data : 0.11625618934631347
	model : 0.0693746566772461
			 train-loss:  2.0559147987209383 	 ± 0.2292932651417762
	data : 0.11633024215698243
	model : 0.06887660026550294
			 train-loss:  2.0569478715619733 	 ± 0.22757968429270753
	data : 0.11683688163757325
	model : 0.06792874336242676
			 train-loss:  2.0580295362169783 	 ± 0.2259268669922614
	data : 0.11780252456665039
	model : 0.067818021774292
			 train-loss:  2.059085277840495 	 ± 0.2243114405038255
	data : 0.11772732734680176
	model : 0.06771187782287598
			 train-loss:  2.0646229909016536 	 ± 0.2269453169025599
	data : 0.1179274082183838
	model : 0.06830797195434571
			 train-loss:  2.0620815627502673 	 ± 0.22614958822877593
	data : 0.11743121147155762
	model : 0.06838049888610839
			 train-loss:  2.0533698014358976 	 ± 0.23534943004940806
	data : 0.11728544235229492
	model : 0.06936845779418946
			 train-loss:  2.0533523366731754 	 ± 0.2336125538877372
	data : 0.11650896072387695
	model : 0.06936087608337402
			 train-loss:  2.0505112910616226 	 ± 0.2330938664309045
	data : 0.11683902740478516
	model : 0.06936960220336914
			 train-loss:  2.056163035120283 	 ± 0.2361367844474017
	data : 0.11691246032714844
	model : 0.06846332550048828
			 train-loss:  2.0573954347153784 	 ± 0.23469456169063188
	data : 0.11765065193176269
	model : 0.06894826889038086
			 train-loss:  2.0662586390972137 	 ± 0.24473254741013578
	data : 0.11713647842407227
	model : 0.06890039443969727
			 train-loss:  2.064160591935458 	 ± 0.24370162796750414
	data : 0.11701412200927734
	model : 0.06890754699707032
			 train-loss:  2.0640278088079915 	 ± 0.24205205320801193
	data : 0.11700196266174316
	model : 0.06862058639526367
			 train-loss:  2.0733126322428386 	 ± 0.25335231436885725
	data : 0.11715164184570312
	model : 0.06955223083496094
			 train-loss:  2.0695864871928564 	 ± 0.25374029237913187
	data : 0.1163301944732666
	model : 0.0694948673248291
			 train-loss:  2.0671389149380968 	 ± 0.25298866520493263
	data : 0.11646361351013183
	model : 0.06943316459655761
			 train-loss:  2.0672817459473243 	 ± 0.2513648362439193
	data : 0.11653614044189453
	model : 0.06939730644226075
			 train-loss:  2.0640350323689134 	 ± 0.25140940615374974
	data : 0.11675777435302734
	model : 0.06894049644470215
			 train-loss:  2.0652225434780123 	 ± 0.2500560143485919
	data : 0.11720867156982422
	model : 0.06893200874328613
			 train-loss:  2.063635152063252 	 ± 0.24891292551332908
	data : 0.1171865463256836
	model : 0.06805758476257324
			 train-loss:  2.0620777199908003 	 ± 0.24778728099874686
	data : 0.11789655685424805
	model : 0.06820621490478515
			 train-loss:  2.0633970944278213 	 ± 0.24657967602754643
	data : 0.1176915168762207
	model : 0.06739044189453125
			 train-loss:  2.0649372168949673 	 ± 0.2455088245289236
	data : 0.11811704635620117
	model : 0.06828250885009765
			 train-loss:  2.066182119706098 	 ± 0.24432693661176516
	data : 0.11725430488586426
	model : 0.06819229125976563
			 train-loss:  2.0646256956943247 	 ± 0.24332575942782406
	data : 0.11758856773376465
	model : 0.06997065544128418
			 train-loss:  2.065811165447893 	 ± 0.24217295306871642
	data : 0.11581096649169922
	model : 0.06987333297729492
			 train-loss:  2.0663372820073906 	 ± 0.24084303829963719
	data : 0.11591253280639649
	model : 0.07078261375427246
			 train-loss:  2.0622858679696416 	 ± 0.2424830952443261
	data : 0.11508240699768066
	model : 0.07088742256164551
			 train-loss:  2.0600604070557487 	 ± 0.24204447522102304
	data : 0.11505126953125
	model : 0.07112455368041992
			 train-loss:  2.0656550493869155 	 ± 0.24649286607816295
	data : 0.11473407745361328
	model : 0.07019057273864746
			 train-loss:  2.0693218306354852 	 ± 0.24763245531679057
	data : 0.11569800376892089
	model : 0.06977910995483398
			 train-loss:  2.0666405808541084 	 ± 0.24763653933642535
	data : 0.11593427658081054
	model : 0.06976003646850586
			 train-loss:  2.069106232612691 	 ± 0.2474608291449604
	data : 0.11601781845092773
	model : 0.06868505477905273
			 train-loss:  2.0686240911483766 	 ± 0.24619933933900734
	data : 0.11692771911621094
	model : 0.06846795082092286
			 train-loss:  2.0686431961754956 	 ± 0.24491376510545224
	data : 0.11698346138000489
	model : 0.06826796531677246
			 train-loss:  2.0684462195819187 	 ± 0.24365569606880294
	data : 0.1173105239868164
	model : 0.06871366500854492
			 train-loss:  2.0711310286911164 	 ± 0.24384728151348398
	data : 0.11704540252685547
	model : 0.06877007484436035
			 train-loss:  2.0738400630276614 	 ± 0.2440903203576449
	data : 0.11683483123779297
	model : 0.06875882148742676
			 train-loss:  2.0765414988994597 	 ± 0.24434967048995448
	data : 0.11683535575866699
	model : 0.06868185997009277
			 train-loss:  2.078745687361991 	 ± 0.244134082201238
	data : 0.11705136299133301
	model : 0.06874728202819824
			 train-loss:  2.0760172126339933 	 ± 0.24447704219135527
	data : 0.11684489250183105
	model : 0.06787009239196777
			 train-loss:  2.0776038644383257 	 ± 0.24381452724028013
	data : 0.11776156425476074
	model : 0.06767411231994629
			 train-loss:  2.0786791913784466 	 ± 0.24288481612963334
	data : 0.11817021369934082
	model : 0.0686039924621582
			 train-loss:  2.0745528675260996 	 ± 0.2453608700175169
	data : 0.11732449531555175
	model : 0.06876902580261231
			 train-loss:  2.072750118543517 	 ± 0.24489845540235214
	data : 0.11722946166992188
	model : 0.06913895606994629
			 train-loss:  2.0762065437352546 	 ± 0.24633535238012427
	data : 0.11707468032836914
	model : 0.07025599479675293
			 train-loss:  2.0757764909002514 	 ± 0.24523260978788364
	data : 0.11618285179138184
	model : 0.07037100791931153
			 train-loss:  2.0746987194096276 	 ± 0.24436192527937864
	data : 0.116115140914917
	model : 0.07021689414978027
			 train-loss:  2.073364553668282 	 ± 0.24364713654424258
	data : 0.11628842353820801
	model : 0.06939301490783692
			 train-loss:  2.0746244707623043 	 ± 0.2429068340145197
	data : 0.1169424057006836
	model : 0.06925578117370605
			 train-loss:  2.071776082473142 	 ± 0.24367496599341207
	data : 0.11697874069213868
	model : 0.06906023025512695
			 train-loss:  2.073197403840259 	 ± 0.24306024323819897
	data : 0.11673893928527831
	model : 0.06877713203430176
			 train-loss:  2.0716678395605923 	 ± 0.24253746588642
	data : 0.11700301170349121
	model : 0.0690704345703125
			 train-loss:  2.072978821008102 	 ± 0.24188599440851452
	data : 0.11675615310668945
	model : 0.06906814575195312
			 train-loss:  2.070445549899134 	 ± 0.2423684302406427
	data : 0.11662368774414063
	model : 0.06915884017944336
			 train-loss:  2.0693281022911396 	 ± 0.24163036159839255
	data : 0.1166757583618164
	model : 0.06912732124328613
			 train-loss:  2.0658220477023366 	 ± 0.24357474118778127
	data : 0.11672639846801758
	model : 0.06942548751831054
			 train-loss:  2.065132832326809 	 ± 0.2426646791909903
	data : 0.11630935668945312
	model : 0.06936254501342773
			 train-loss:  2.0646424531936645 	 ± 0.24171066333688082
	data : 0.11648402214050294
	model : 0.07024812698364258
			 train-loss:  2.0650199503938027 	 ± 0.24074530508791978
	data : 0.1156423568725586
	model : 0.07017059326171875
			 train-loss:  2.0658232462210733 	 ± 0.23991938908694924
	data : 0.1154179573059082
	model : 0.07019867897033691
			 train-loss:  2.0634268250891834 	 ± 0.24040374556299615
	data : 0.11554856300354004
	model : 0.07036819458007812
			 train-loss:  2.063546681596387 	 ± 0.23943610320429923
	data : 0.11556987762451172
	model : 0.0704796314239502
			 train-loss:  2.061691146850586 	 ± 0.23936988841003992
	data : 0.11529736518859864
	model : 0.0696321964263916
			 train-loss:  2.064461062824915 	 ± 0.24042098953166788
	data : 0.11636109352111816
	model : 0.06942882537841796
			 train-loss:  2.0672385335907224 	 ± 0.24149352721957715
	data : 0.11656861305236817
	model : 0.06916494369506836
			 train-loss:  2.066973904147744 	 ± 0.24056682893819395
	data : 0.11680040359497071
	model : 0.06893491744995117
			 train-loss:  2.068804925726366 	 ± 0.24052632674743846
	data : 0.11681628227233887
	model : 0.06860942840576172
			 train-loss:  2.0706034990457387 	 ± 0.24046868643630973
	data : 0.11716666221618652
	model : 0.06838321685791016
			 train-loss:  2.0715625777499365 	 ± 0.23979856891489518
	data : 0.11716399192810059
	model : 0.06764311790466308
			 train-loss:  2.073349076690096 	 ± 0.23976200484070997
	data : 0.11801586151123047
	model : 0.06796250343322754
			 train-loss:  2.0726258709914704 	 ± 0.2390034189257211
	data : 0.11780519485473633
	model : 0.06784582138061523
			 train-loss:  2.070500347151685 	 ± 0.23936837762068525
	data : 0.11769428253173828
	model : 0.06709890365600586
			 train-loss:  2.0696892853136415 	 ± 0.2386649207702671
	data : 0.11835303306579589
	model : 0.0677295207977295
			 train-loss:  2.068716007120469 	 ± 0.23805460600937448
	data : 0.11787939071655273
	model : 0.06844925880432129
			 train-loss:  2.0700402486063267 	 ± 0.23768642693738307
	data : 0.11722774505615234
	model : 0.06793575286865235
			 train-loss:  2.071222612823265 	 ± 0.23722769364862517
	data : 0.11779794692993165
	model : 0.06796374320983886
			 train-loss:  2.073115914845638 	 ± 0.23741689505082683
	data : 0.11795639991760254
	model : 0.06883096694946289
			 train-loss:  2.0727286543164936 	 ± 0.2366115130619663
	data : 0.11716456413269043
	model : 0.0692739486694336
			 train-loss:  2.074953532388024 	 ± 0.23723609097054996
	data : 0.11667723655700683
	model : 0.06924304962158204
			 train-loss:  2.0762935977586556 	 ± 0.23693421524334077
	data : 0.11668887138366699
	model : 0.06974430084228515
			 train-loss:  2.0717031605593808 	 ± 0.242358185979596
	data : 0.11601772308349609
	model : 0.0701179027557373
			 train-loss:  2.0714696645736694 	 ± 0.24153133851887934
	data : 0.11602206230163574
	model : 0.06994929313659667
			 train-loss:  2.07352369242701 	 ± 0.24195578326892603
	data : 0.11610898971557618
	model : 0.06973881721496582
			 train-loss:  2.0717414821663946 	 ± 0.24207887967065692
	data : 0.11608624458312988
	model : 0.06990175247192383
			 train-loss:  2.0713899841113967 	 ± 0.2412914588182993
	data : 0.11604385375976563
	model : 0.06960630416870117
			 train-loss:  2.074410458674302 	 ± 0.2432474035072968
	data : 0.11637687683105469
	model : 0.06938104629516602
			 train-loss:  2.073883377465626 	 ± 0.24251454901213418
	data : 0.11630620956420898
	model : 0.06953287124633789
			 train-loss:  2.072827180226644 	 ± 0.24204841480830316
	data : 0.11641554832458496
	model : 0.06977019309997559
			 train-loss:  2.0749788694823814 	 ± 0.242680662291387
	data : 0.11634702682495117
	model : 0.06884970664978027
			 train-loss:  2.075483518211465 	 ± 0.2419605322728061
	data : 0.11690115928649902
	model : 0.06904082298278809
			 train-loss:  2.075335396660699 	 ± 0.2411754292147999
	data : 0.11675844192504883
	model : 0.06898894309997558
			 train-loss:  2.075003447470727 	 ± 0.24042618024311868
	data : 0.11690397262573242
	model : 0.06821346282958984
			 train-loss:  2.0747405544404063 	 ± 0.2396715620387168
	data : 0.11749453544616699
	model : 0.0672518253326416
			 train-loss:  2.074379624464573 	 ± 0.23894440501309439
	data : 0.11872420310974122
	model : 0.06817741394042968
			 train-loss:  2.076207138170862 	 ± 0.23927344126688305
	data : 0.1178706169128418
	model : 0.06819047927856445
			 train-loss:  2.0769759310951716 	 ± 0.23870948942520362
	data : 0.11788125038146972
	model : 0.068367338180542
			 train-loss:  2.0766529422136224 	 ± 0.23799227811955553
	data : 0.11760201454162597
	model : 0.0693270206451416
			 train-loss:  2.0758186161518095 	 ± 0.23748053038758193
	data : 0.11677384376525879
	model : 0.07010293006896973
			 train-loss:  2.075850168370312 	 ± 0.23674220084330694
	data : 0.1160006046295166
	model : 0.06933345794677734
			 train-loss:  2.0767535501056247 	 ± 0.23628858033558972
	data : 0.11663455963134765
	model : 0.0691420555114746
			 train-loss:  2.0782661247838496 	 ± 0.23634804991867242
	data : 0.11688199043273925
	model : 0.069354248046875
			 train-loss:  2.077337054217734 	 ± 0.23592474553553902
	data : 0.11678271293640137
	model : 0.06874480247497558
			 train-loss:  2.078844235160134 	 ± 0.2359993448304164
	data : 0.11737403869628907
	model : 0.06922168731689453
			 train-loss:  2.077033728720194 	 ± 0.23643399415033697
	data : 0.11695938110351563
	model : 0.06976861953735351
			 train-loss:  2.0787346184610604 	 ± 0.2367415028315815
	data : 0.11661367416381836
	model : 0.07002072334289551
			 train-loss:  2.079281035633314 	 ± 0.23614146245219106
	data : 0.11629776954650879
	model : 0.06962103843688965
			 train-loss:  2.0772901702914717 	 ± 0.23685165922585757
	data : 0.11653070449829102
	model : 0.07015366554260254
			 train-loss:  2.077665389986599 	 ± 0.23620438089659576
	data : 0.11596541404724121
	model : 0.06914353370666504
			 train-loss:  2.076761754632693 	 ± 0.23580723533009904
	data : 0.11672372817993164
	model : 0.06938347816467286
			 train-loss:  2.077484525913416 	 ± 0.23531063970858224
	data : 0.11648850440979004
	model : 0.0685194492340088
			 train-loss:  2.0798916665115796 	 ± 0.23674386383269322
	data : 0.11708202362060546
	model : 0.06776990890502929
			 train-loss:  2.083254024900239 	 ± 0.2401694976914323
	data : 0.11780824661254882
	model : 0.06678800582885742
			 train-loss:  2.0838970606667653 	 ± 0.2396324849046423
	data : 0.11874151229858398
	model : 0.06726164817810058
			 train-loss:  2.0832914384928616 	 ± 0.23908501171933788
	data : 0.11837172508239746
	model : 0.06732296943664551
			 train-loss:  2.0830201994901323 	 ± 0.2384358280947132
	data : 0.11852793693542481
	model : 0.06810250282287597
			 train-loss:  2.0818657801392373 	 ± 0.23826065125150547
	data : 0.11809053421020507
	model : 0.06902995109558105
			 train-loss:  2.0816991136060747 	 ± 0.23760459157163782
	data : 0.1173861026763916
	model : 0.06981229782104492
			 train-loss:  2.0818784574667615 	 ± 0.23695580855827122
	data : 0.1166426658630371
	model : 0.06991810798645019
			 train-loss:  2.082927343595094 	 ± 0.2367189771428673
	data : 0.11659736633300781
	model : 0.07070550918579102
			 train-loss:  2.083605966070196 	 ± 0.2362442388147239
	data : 0.11567850112915039
	model : 0.07012033462524414
			 train-loss:  2.085378134185499 	 ± 0.2368078248284217
	data : 0.11609296798706055
	model : 0.07074775695800781
			 train-loss:  2.0853028226157893 	 ± 0.23616564610504492
	data : 0.11529583930969238
	model : 0.07005009651184083
			 train-loss:  2.0843646146155694 	 ± 0.23587007710460337
	data : 0.11589612960815429
	model : 0.07092471122741699
			 train-loss:  2.0853822827339172 	 ± 0.2356420526413053
	data : 0.11472144126892089
	model : 0.07023434638977051
			 train-loss:  2.0849810519957925 	 ± 0.2350748468324954
	data : 0.11535882949829102
	model : 0.0707942008972168
			 train-loss:  2.0852531860483454 	 ± 0.23447834679283355
	data : 0.11499309539794922
	model : 0.06995587348937989
			 train-loss:  2.0847836278733753 	 ± 0.23394581915035054
	data : 0.11589884757995605
	model : 0.07010049819946289
			 train-loss:  2.084894805832913 	 ± 0.23333436610708116
	data : 0.11575851440429688
	model : 0.06922569274902343
			 train-loss:  2.0871189458207935 	 ± 0.23473339384535608
	data : 0.1170133113861084
	model : 0.06831035614013672
			 train-loss:  2.0871674871693053 	 ± 0.23412227207895717
	data : 0.11782164573669433
	model : 0.06842455863952637
			 train-loss:  2.0854624077446102 	 ± 0.23470712104575067
	data : 0.11771817207336426
	model : 0.06841773986816406
			 train-loss:  2.0848902599098755 	 ± 0.23423632508222633
	data : 0.11771144866943359
	model : 0.06822476387023926
			 train-loss:  2.085920511147915 	 ± 0.23407520914961608
	data : 0.11798224449157715
	model : 0.06824936866760253
			 train-loss:  2.0851124160143795 	 ± 0.23374985570445228
	data : 0.11785626411437988
	model : 0.06922883987426758
			 train-loss:  2.085375087515352 	 ± 0.23318482586842182
	data : 0.11733775138854981
	model : 0.06963486671447754
			 train-loss:  2.085257508537986 	 ± 0.23260108440974941
	data : 0.1168278694152832
	model : 0.0698401927947998
			 train-loss:  2.084347316967183 	 ± 0.23236914943568832
	data : 0.11651439666748047
	model : 0.07081232070922852
			 train-loss:  2.0840245419740677 	 ± 0.23183221746978538
	data : 0.1155919075012207
	model : 0.070013427734375
			 train-loss:  2.0856843736041246 	 ± 0.2324430927114828
	data : 0.11613020896911622
	model : 0.06970105171203614
			 train-loss:  2.0847524805824356 	 ± 0.2322431279501225
	data : 0.11610331535339355
	model : 0.06834220886230469
			 train-loss:  2.0838696792207916 	 ± 0.23200990913322955
	data : 0.11756339073181152
	model : 0.06827950477600098
			 train-loss:  2.082857999147153 	 ± 0.23188898650829884
	data : 0.11757235527038574
	model : 0.06820287704467773
			 train-loss:  2.0826728937102525 	 ± 0.2313378201599877
	data : 0.11761212348937988
	model : 0.06905150413513184
			 train-loss:  2.083082906250815 	 ± 0.23085029234201987
	data : 0.11697745323181152
	model : 0.06910166740417481
			 train-loss:  2.081475760049866 	 ± 0.2314443565595743
	data : 0.11685523986816407
	model : 0.0696976661682129
			 train-loss:  2.080722709114735 	 ± 0.23114139768251776
	data : 0.1162642478942871
	model : 0.06968741416931153
			 train-loss:  2.0812082735545325 	 ± 0.2306940788616713
	data : 0.1164933204650879
	model : 0.0687652587890625
			 train-loss:  2.0825445924486434 	 ± 0.23095357103477657
	data : 0.11735358238220214
	model : 0.06862149238586426
			 train-loss:  2.0798674906599577 	 ± 0.2336488849230796
	data : 0.11747150421142578
	model : 0.0679436206817627
			 train-loss:  2.079758271856128 	 ± 0.23310257376131052
	data : 0.11817345619201661
	model : 0.06804265975952148
			 train-loss:  2.078799278523441 	 ± 0.23297355463925817
	data : 0.11805787086486816
	model : 0.06779656410217286
			 train-loss:  2.0779033436953465 	 ± 0.23279609696421275
	data : 0.11832022666931152
	model : 0.06875700950622558
			 train-loss:  2.0768876502680222 	 ± 0.2327288702142937
	data : 0.11751332283020019
	model : 0.06877069473266602
			 train-loss:  2.075301935275396 	 ± 0.23335078627138178
	data : 0.11734647750854492
	model : 0.06993145942687988
			 train-loss:  2.074318115612329 	 ± 0.23326106084273687
	data : 0.11636109352111816
	model : 0.07017369270324707
			 train-loss:  2.0726703361633723 	 ± 0.23398787461888304
	data : 0.11612300872802735
	model : 0.07038316726684571
			 train-loss:  2.072144244903843 	 ± 0.233582234515844
	data : 0.11598548889160157
	model : 0.07034773826599121
			 train-loss:  2.073602690479972 	 ± 0.23404803820223255
	data : 0.11596908569335937
	model : 0.07033896446228027
			 train-loss:  2.074845144651594 	 ± 0.2342439541655656
	data : 0.11598305702209473
	model : 0.06989831924438476
			 train-loss:  2.075184968140748 	 ± 0.23377037439648465
	data : 0.11638402938842773
	model : 0.06978530883789062
			 train-loss:  2.0746500967863963 	 ± 0.23338174394791136
	data : 0.11629962921142578
	model : 0.06921005249023438
			 train-loss:  2.07581662652748 	 ± 0.23351089593789184
	data : 0.11662840843200684
	model : 0.06941709518432618
			 train-loss:  2.0771255106396147 	 ± 0.23381348674019964
	data : 0.11647038459777832
	model : 0.06850056648254395
			 train-loss:  2.0761338762477437 	 ± 0.23376933223895752
	data : 0.11719284057617188
	model : 0.0682530403137207
			 train-loss:  2.076639724197892 	 ± 0.23337778285292998
	data : 0.11724095344543457
	model : 0.06803936958312988
			 train-loss:  2.0765278041362762 	 ± 0.23287153227450644
	data : 0.11758885383605958
	model : 0.06803545951843262
			 train-loss:  2.0762855855658584 	 ± 0.2323913052349262
	data : 0.11746087074279785
	model : 0.0671478271484375
			 train-loss:  2.075254493692647 	 ± 0.23240992390440543
	data : 0.11784205436706544
	model : 0.06778550148010254
			 train-loss:  2.0758469502131143 	 ± 0.23208032093106556
	data : 0.11724414825439453
	model : 0.06767592430114747
			 train-loss:  2.0774345906644034 	 ± 0.23283336058775933
	data : 0.11738452911376954
	model : 0.06736726760864258
			 train-loss:  2.0805630330875706 	 ± 0.23716940406614936
	data : 0.11743488311767578
	model : 0.06796679496765137
			 train-loss:  2.081263079093053 	 ± 0.23690320702043935
	data : 0.11717023849487304
	model : 0.06826953887939453
			 train-loss:  2.080006649646353 	 ± 0.23717863219161434
	data : 0.11722335815429688
	model : 0.06778216361999512
			 train-loss:  2.0790905639276667 	 ± 0.2370918713977931
	data : 0.11779594421386719
	model : 0.06806735992431641
			 train-loss:  2.0800599699784934 	 ± 0.23705938550090014
	data : 0.11749801635742188
	model : 0.06858291625976562
			 train-loss:  2.0798576809778937 	 ± 0.2365813348610282
	data : 0.11725368499755859
	model : 0.06848244667053223
			 train-loss:  2.0783143941328617 	 ± 0.2372833614441658
	data : 0.11724667549133301
	model : 0.06877875328063965
			 train-loss:  2.077875492970149 	 ± 0.2368857018183116
	data : 0.11703133583068848
	model : 0.06945562362670898
			 train-loss:  2.0790318384210105 	 ± 0.23707152177423685
	data : 0.11639761924743652
	model : 0.06938648223876953
			 train-loss:  2.0787736768564904 	 ± 0.23661514119585525
	data : 0.11634263992309571
	model : 0.06912684440612793
			 train-loss:  2.080668120717806 	 ± 0.23795975509161094
	data : 0.11632356643676758
	model : 0.06862845420837402
			 train-loss:  2.081539929890242 	 ± 0.23786018678175352
	data : 0.11668457984924316
	model : 0.06775083541870117
			 train-loss:  2.0811700733340515 	 ± 0.23744455714541027
	data : 0.11728620529174805
	model : 0.06753339767456054
			 train-loss:  2.0817114211679475 	 ± 0.2371129058961562
	data : 0.11750602722167969
	model : 0.06712827682495118
			 train-loss:  2.082940530197823 	 ± 0.23741639155457708
	data : 0.11803889274597168
	model : 0.0667614459991455
			 train-loss:  2.081784000319819 	 ± 0.23763340713566314
	data : 0.11833348274230956
	model : 0.06664762496948243
			 train-loss:  2.083553451132104 	 ± 0.23878720156517158
	data : 0.11834902763366699
	model : 0.06735014915466309
			 train-loss:  2.0838667764663694 	 ± 0.23836043157224865
	data : 0.11771488189697266
	model : 0.06666793823242187
			 train-loss:  2.083029281570617 	 ± 0.23825341007520154
	data : 0.11825871467590332
	model : 0.06710095405578613
			 train-loss:  2.0828098673669118 	 ± 0.23780562337163227
	data : 0.11784324645996094
	model : 0.06767220497131347
			 train-loss:  2.081574711403828 	 ± 0.23814374759419313
	data : 0.11768269538879395
	model : 0.06757168769836426
			 train-loss:  2.08340849744992 	 ± 0.23945761275658559
	data : 0.11808962821960449
	model : 0.06765174865722656
			 train-loss:  2.0837999362571566 	 ± 0.23906903761678164
	data : 0.11801056861877442
	model : 0.06783208847045899
			 train-loss:  2.0852647153660655 	 ± 0.23974542595143408
	data : 0.11702618598937989
	model : 0.05858139991760254
#epoch  69    val-loss:  2.4607277355696024  train-loss:  2.0852647153660655  lr:  1.953125e-05
			 train-loss:  1.6203891038894653 	 ± 0.0
	data : 5.466093301773071
	model : 0.08082962036132812
			 train-loss:  1.8098483085632324 	 ± 0.1894592046737671
	data : 2.904423952102661
	model : 0.07331669330596924
			 train-loss:  1.8156204620997112 	 ± 0.154908023258884
	data : 1.9759678840637207
	model : 0.07073815663655598
			 train-loss:  1.8172779381275177 	 ± 0.13418499704821502
	data : 1.5119144916534424
	model : 0.07040709257125854
			 train-loss:  1.8676769018173218 	 ± 0.15673133991097102
	data : 1.2327280998229981
	model : 0.07014613151550293
			 train-loss:  1.8651580413182576 	 ± 0.14318630305645744
	data : 0.16290955543518065
	model : 0.06797070503234863
			 train-loss:  1.9113966396876745 	 ± 0.17436015248531647
	data : 0.11756329536437989
	model : 0.06798362731933594
			 train-loss:  1.916146457195282 	 ± 0.16358241131721338
	data : 0.11765556335449219
	model : 0.06832060813903809
			 train-loss:  1.9581327438354492 	 ± 0.1946503182286443
	data : 0.11732144355773926
	model : 0.06748023033142089
			 train-loss:  2.0154197454452514 	 ± 0.2522619208854848
	data : 0.11827168464660645
	model : 0.06761140823364258
			 train-loss:  2.0013321963223545 	 ± 0.24461310113584567
	data : 0.11803793907165527
	model : 0.06724328994750976
			 train-loss:  2.0372143387794495 	 ± 0.26270150633455597
	data : 0.11834096908569336
	model : 0.06803698539733886
			 train-loss:  2.0369785748995266 	 ± 0.2523967646330553
	data : 0.1177328109741211
	model : 0.06829671859741211
			 train-loss:  2.02413409948349 	 ± 0.24758549868535146
	data : 0.11770811080932617
	model : 0.06912388801574706
			 train-loss:  2.033189288775126 	 ± 0.24157805686532383
	data : 0.1170311450958252
	model : 0.06839928627014161
			 train-loss:  2.018711008131504 	 ± 0.24053434134719984
	data : 0.11767292022705078
	model : 0.06854329109191895
			 train-loss:  2.017887501155629 	 ± 0.2333758355370397
	data : 0.11754770278930664
	model : 0.06894235610961914
			 train-loss:  2.0289511481920877 	 ± 0.23134251685098653
	data : 0.11718087196350098
	model : 0.06908254623413086
			 train-loss:  2.045002077755175 	 ± 0.23524441288494702
	data : 0.11692447662353515
	model : 0.06915149688720704
			 train-loss:  2.0899087607860567 	 ± 0.3014772471344881
	data : 0.11680097579956054
	model : 0.07025041580200195
			 train-loss:  2.0848919607344127 	 ± 0.2950658755789174
	data : 0.11581168174743653
	model : 0.07090034484863281
			 train-loss:  2.0658253756436435 	 ± 0.3012318878537909
	data : 0.1152259349822998
	model : 0.07064337730407715
			 train-loss:  2.0826071863589077 	 ± 0.30494466113691454
	data : 0.11525902748107911
	model : 0.07092204093933105
			 train-loss:  2.0722028811772666 	 ± 0.3026654111659788
	data : 0.114984130859375
	model : 0.07090349197387695
			 train-loss:  2.0891717720031737 	 ± 0.30798171246406447
	data : 0.1150996208190918
	model : 0.06973176002502442
			 train-loss:  2.089843071424044 	 ± 0.3020195675143571
	data : 0.11619906425476074
	model : 0.06847853660583496
			 train-loss:  2.0738789682035095 	 ± 0.3073493498796477
	data : 0.11737179756164551
	model : 0.06883387565612793
			 train-loss:  2.0809046115194048 	 ± 0.3040109093152151
	data : 0.11725859642028809
	model : 0.06921515464782715
			 train-loss:  2.07377787294059 	 ± 0.3010942990427015
	data : 0.11708765029907227
	model : 0.06951680183410644
			 train-loss:  2.08334907690684 	 ± 0.30048707365602045
	data : 0.1167259693145752
	model : 0.07022247314453126
			 train-loss:  2.0997645547313075 	 ± 0.3089722610673898
	data : 0.11597328186035157
	model : 0.07095460891723633
			 train-loss:  2.093620676547289 	 ± 0.3060241444012676
	data : 0.11526331901550294
	model : 0.07043523788452148
			 train-loss:  2.083458965474909 	 ± 0.3067852775572922
	data : 0.11576623916625976
	model : 0.06970744132995606
			 train-loss:  2.078430908567765 	 ± 0.30361709133284753
	data : 0.11639108657836914
	model : 0.06951451301574707
			 train-loss:  2.09231630393437 	 ± 0.31000785706262696
	data : 0.11652064323425293
	model : 0.06962614059448242
			 train-loss:  2.078316089179781 	 ± 0.3166946512643302
	data : 0.11633119583129883
	model : 0.06976971626281739
			 train-loss:  2.0790351693694658 	 ± 0.31241547279210585
	data : 0.11619210243225098
	model : 0.07059969902038574
			 train-loss:  2.074587931758479 	 ± 0.30946195164166834
	data : 0.11528329849243164
	model : 0.0700796127319336
			 train-loss:  2.070562631655962 	 ± 0.3064748907673531
	data : 0.11558642387390136
	model : 0.07018437385559081
			 train-loss:  2.062599802017212 	 ± 0.3066782463469883
	data : 0.11554670333862305
	model : 0.06955409049987793
			 train-loss:  2.0710460151114116 	 ± 0.3075892508563055
	data : 0.11651649475097656
	model : 0.06941852569580079
			 train-loss:  2.0790455227806452 	 ± 0.3081917875615536
	data : 0.11670427322387696
	model : 0.06853904724121093
			 train-loss:  2.084226602731749 	 ± 0.3064322450707182
	data : 0.11752548217773437
	model : 0.0691521167755127
			 train-loss:  2.083227612755515 	 ± 0.30300086915644386
	data : 0.11704540252685547
	model : 0.06889524459838867
			 train-loss:  2.0809130721622044 	 ± 0.3000083787560928
	data : 0.1172780990600586
	model : 0.06915383338928223
			 train-loss:  2.0788177847862244 	 ± 0.2970622099973961
	data : 0.11678462028503418
	model : 0.06837010383605957
			 train-loss:  2.0755450624100704 	 ± 0.2947220314198413
	data : 0.11744050979614258
	model : 0.06841878890991211
			 train-loss:  2.076845029989878 	 ± 0.29177199333832265
	data : 0.11732802391052247
	model : 0.06858940124511718
			 train-loss:  2.083550141782177 	 ± 0.2924919486570923
	data : 0.1170475959777832
	model : 0.0688241958618164
			 train-loss:  2.089519338607788 	 ± 0.2925516081908288
	data : 0.11683735847473145
	model : 0.0691251277923584
			 train-loss:  2.092153796962663 	 ± 0.29026762898793385
	data : 0.11668972969055176
	model : 0.06911473274230957
			 train-loss:  2.090455371599931 	 ± 0.28771881996204846
	data : 0.11659321784973145
	model : 0.0688976764678955
			 train-loss:  2.0876374919459506 	 ± 0.28571506043377226
	data : 0.11695184707641601
	model : 0.06939535140991211
			 train-loss:  2.083354793213032 	 ± 0.28476915512446405
	data : 0.11644692420959472
	model : 0.06935362815856934
			 train-loss:  2.0822127840735694 	 ± 0.2822932357233542
	data : 0.11639609336853027
	model : 0.069476318359375
			 train-loss:  2.082601319466318 	 ± 0.27977624533685824
	data : 0.1163405418395996
	model : 0.07038340568542481
			 train-loss:  2.0773933247516028 	 ± 0.28003643686579205
	data : 0.11552534103393555
	model : 0.06973805427551269
			 train-loss:  2.0751019325749627 	 ± 0.27815033212195595
	data : 0.11601128578186035
	model : 0.06921439170837403
			 train-loss:  2.07391910835848 	 ± 0.27593013268515176
	data : 0.11659913063049317
	model : 0.0691025733947754
			 train-loss:  2.0716749290625254 	 ± 0.2741635003489995
	data : 0.11662449836730956
	model : 0.06887102127075195
			 train-loss:  2.069579171352699 	 ± 0.27239114080107324
	data : 0.11679239273071289
	model : 0.06795315742492676
			 train-loss:  2.078755747887396 	 ± 0.2795299479883389
	data : 0.11782903671264648
	model : 0.06789584159851074
			 train-loss:  2.0713802386843967 	 ± 0.28331856018114304
	data : 0.11786475181579589
	model : 0.06767268180847168
			 train-loss:  2.071641558781266 	 ± 0.2811040718702604
	data : 0.11818537712097169
	model : 0.06765451431274414
			 train-loss:  2.068691530594459 	 ± 0.2799299649233961
	data : 0.11822962760925293
	model : 0.06788477897644044
			 train-loss:  2.068025045322649 	 ± 0.27785315151527756
	data : 0.11809024810791016
	model : 0.0688093662261963
			 train-loss:  2.0643097002114823 	 ± 0.27741872623260616
	data : 0.11717820167541504
	model : 0.0689702033996582
			 train-loss:  2.0631653946988724 	 ± 0.27553057943524684
	data : 0.11705818176269531
	model : 0.06920561790466309
			 train-loss:  2.0635471793188565 	 ± 0.27354481172307454
	data : 0.1167220115661621
	model : 0.06854548454284667
			 train-loss:  2.0711108582360405 	 ± 0.27875661018796427
	data : 0.11744871139526367
	model : 0.06874194145202636
			 train-loss:  2.0693839234365545 	 ± 0.27716343352040596
	data : 0.11730961799621582
	model : 0.06912689208984375
			 train-loss:  2.0663471851083965 	 ± 0.2764188430280672
	data : 0.11717185974121094
	model : 0.06939549446105957
			 train-loss:  2.0699834594987845 	 ± 0.2762475697097676
	data : 0.11686186790466309
	model : 0.06845040321350097
			 train-loss:  2.072474788975071 	 ± 0.2751991234497631
	data : 0.11777319908142089
	model : 0.06919422149658203
			 train-loss:  2.077280651728312 	 ± 0.2764667988932948
	data : 0.11700439453125
	model : 0.0680729866027832
			 train-loss:  2.07765720706237 	 ± 0.2746612756535114
	data : 0.11790218353271484
	model : 0.06767964363098145
			 train-loss:  2.0842256855655026 	 ± 0.27881553867089187
	data : 0.11805882453918456
	model : 0.06823520660400391
			 train-loss:  2.0869343892121925 	 ± 0.27804031550645286
	data : 0.11765103340148926
	model : 0.06834278106689454
			 train-loss:  2.0843052275573153 	 ± 0.2772490380585852
	data : 0.11753511428833008
	model : 0.06825079917907714
			 train-loss:  2.0858707085251806 	 ± 0.27586192010280236
	data : 0.11760749816894531
	model : 0.06898727416992187
			 train-loss:  2.089859236905604 	 ± 0.27646512250774335
	data : 0.11688399314880371
	model : 0.06896390914916992
			 train-loss:  2.0912891757197496 	 ± 0.27507540306720457
	data : 0.11692309379577637
	model : 0.06883678436279297
			 train-loss:  2.0932053643536856 	 ± 0.27396335327032134
	data : 0.11710629463195801
	model : 0.06941790580749511
			 train-loss:  2.0923509782268885 	 ± 0.2724389547585059
	data : 0.11646990776062012
	model : 0.06961245536804199
			 train-loss:  2.096606993675232 	 ± 0.2736262424591676
	data : 0.11636481285095215
	model : 0.06969799995422363
			 train-loss:  2.0963347831437753 	 ± 0.27204231691106545
	data : 0.11644010543823242
	model : 0.06871662139892579
			 train-loss:  2.0972020338321555 	 ± 0.27059388279472246
	data : 0.1171567440032959
	model : 0.06874055862426758
			 train-loss:  2.1001697358759968 	 ± 0.27047222269197985
	data : 0.1171572208404541
	model : 0.06870636940002442
			 train-loss:  2.0992521958404713 	 ± 0.26908611978784597
	data : 0.11719627380371093
	model : 0.0683633804321289
			 train-loss:  2.0994707544644675 	 ± 0.2675949648342172
	data : 0.11752157211303711
	model : 0.06786937713623047
			 train-loss:  2.0950709869573405 	 ± 0.2693740712257042
	data : 0.11793632507324218
	model : 0.06890149116516113
			 train-loss:  2.096705621999243 	 ± 0.26835950421423915
	data : 0.11724987030029296
	model : 0.0690047264099121
			 train-loss:  2.099593779092194 	 ± 0.26834653451099516
	data : 0.11720695495605468
	model : 0.06937460899353028
			 train-loss:  2.095534163586637 	 ± 0.2697711709487699
	data : 0.11693758964538574
	model : 0.06971187591552734
			 train-loss:  2.091973556970295 	 ± 0.2705589415378994
	data : 0.11631917953491211
	model : 0.07018775939941406
			 train-loss:  2.0940359346568584 	 ± 0.26989570524412093
	data : 0.11586699485778809
	model : 0.0700951099395752
			 train-loss:  2.100343366259152 	 ± 0.2755212521447352
	data : 0.11575794219970703
	model : 0.06913447380065918
			 train-loss:  2.104522017800078 	 ± 0.27718419512429393
	data : 0.11646156311035157
	model : 0.06896719932556153
			 train-loss:  2.101014585206003 	 ± 0.27795792973915595
	data : 0.11668376922607422
	model : 0.0681997299194336
			 train-loss:  2.099937980175018 	 ± 0.2767720241619629
	data : 0.11746325492858886
	model : 0.06754584312438965
			 train-loss:  2.0992651457833773 	 ± 0.27548063610353124
	data : 0.11809220314025878
	model : 0.0676811695098877
			 train-loss:  2.10289307902841 	 ± 0.2765409865317837
	data : 0.11800880432128906
	model : 0.06865077018737793
			 train-loss:  2.1036075550375632 	 ± 0.27528986696951047
	data : 0.11716880798339843
	model : 0.06860361099243165
			 train-loss:  2.1032654413810143 	 ± 0.2739851618919944
	data : 0.11741142272949219
	model : 0.0683556079864502
			 train-loss:  2.1008162884485153 	 ± 0.27381885483087565
	data : 0.11792292594909667
	model : 0.06907715797424316
			 train-loss:  2.0997785014926262 	 ± 0.27273159419499465
	data : 0.11712784767150879
	model : 0.06835956573486328
			 train-loss:  2.0996988399006495 	 ± 0.2714553948321167
	data : 0.11781888008117676
	model : 0.06841793060302734
			 train-loss:  2.09662323417487 	 ± 0.27206228203976507
	data : 0.11774754524230957
	model : 0.0685664176940918
			 train-loss:  2.099463166446861 	 ± 0.2724148773362804
	data : 0.11743149757385254
	model : 0.0685262680053711
			 train-loss:  2.0986761472441935 	 ± 0.27129825795562895
	data : 0.11731929779052734
	model : 0.06850366592407227
			 train-loss:  2.099240059251184 	 ± 0.2701381806071099
	data : 0.11735882759094238
	model : 0.06909418106079102
			 train-loss:  2.1022589557937215 	 ± 0.27080380739439935
	data : 0.11676511764526368
	model : 0.06898932456970215
			 train-loss:  2.102322617463306 	 ± 0.2696037394941377
	data : 0.11691203117370605
	model : 0.06887054443359375
			 train-loss:  2.0992607526611864 	 ± 0.27038482784095275
	data : 0.11691555976867676
	model : 0.06981935501098632
			 train-loss:  2.1005372109620466 	 ± 0.26955144072727955
	data : 0.1161794662475586
	model : 0.06895012855529785
			 train-loss:  2.103194158652733 	 ± 0.26989524815580374
	data : 0.11706633567810058
	model : 0.06885852813720703
			 train-loss:  2.1015289502266126 	 ± 0.2693371679626695
	data : 0.11710352897644043
	model : 0.06865463256835938
			 train-loss:  2.1015948643118647 	 ± 0.2681944282379074
	data : 0.11719379425048829
	model : 0.06868953704833984
			 train-loss:  2.0991247301342106 	 ± 0.26840975516539617
	data : 0.11723446846008301
	model : 0.06869492530822754
			 train-loss:  2.1004031896591187 	 ± 0.2676526336275762
	data : 0.11716299057006836
	model : 0.06977009773254395
			 train-loss:  2.1025309089786752 	 ± 0.267561480787021
	data : 0.11622457504272461
	model : 0.07005276679992675
			 train-loss:  2.101066948937588 	 ± 0.2669488235251787
	data : 0.1161759376525879
	model : 0.07003741264343262
			 train-loss:  2.100963809625889 	 ± 0.2658638918256349
	data : 0.11627063751220704
	model : 0.07000617980957032
			 train-loss:  2.1011954680565865 	 ± 0.26480215396467116
	data : 0.11636996269226074
	model : 0.07011418342590332
			 train-loss:  2.103825506210327 	 ± 0.26536190341484794
	data : 0.11620125770568848
	model : 0.06987547874450684
			 train-loss:  2.103421370188395 	 ± 0.2643454007979315
	data : 0.11647043228149415
	model : 0.06990838050842285
			 train-loss:  2.104889357183862 	 ± 0.2638177302574996
	data : 0.11626963615417481
	model : 0.06942696571350097
			 train-loss:  2.1055028401315212 	 ± 0.26287610064419215
	data : 0.11661863327026367
	model : 0.06860466003417968
			 train-loss:  2.1058657243270282 	 ± 0.26188740184802045
	data : 0.11746816635131836
	model : 0.06845908164978028
			 train-loss:  2.1018999952536364 	 ± 0.26473801795528723
	data : 0.11768755912780762
	model : 0.06864581108093262
			 train-loss:  2.1022776310680475 	 ± 0.2637607779733647
	data : 0.1174659252166748
	model : 0.0694516658782959
			 train-loss:  2.101169896848274 	 ± 0.2630654885955188
	data : 0.11695094108581543
	model : 0.06960496902465821
			 train-loss:  2.0982351580956826 	 ± 0.26423474387869117
	data : 0.11678152084350586
	model : 0.07061548233032226
			 train-loss:  2.098926313777468 	 ± 0.26336759232855206
	data : 0.11585350036621093
	model : 0.07047758102416993
			 train-loss:  2.103283169534471 	 ± 0.26719338729865716
	data : 0.11599521636962891
	model : 0.07047700881958008
			 train-loss:  2.102012475623804 	 ± 0.26661834575858123
	data : 0.11608538627624512
	model : 0.06861457824707032
			 train-loss:  2.0998900815518233 	 ± 0.26679409828660305
	data : 0.11779928207397461
	model : 0.06923494338989258
			 train-loss:  2.103091410104779 	 ± 0.2684536188692162
	data : 0.11731009483337403
	model : 0.06826529502868653
			 train-loss:  2.1088439020321523 	 ± 0.2758902968903816
	data : 0.11784729957580567
	model : 0.06847972869873047
			 train-loss:  2.106369880267552 	 ± 0.27644631128561636
	data : 0.11754422187805176
	model : 0.06817646026611328
			 train-loss:  2.1065926010727036 	 ± 0.27547686594038456
	data : 0.11749005317687988
	model : 0.06887240409851074
			 train-loss:  2.1052548767815176 	 ± 0.27496437044422717
	data : 0.11685781478881836
	model : 0.06885581016540528
			 train-loss:  2.105180960435134 	 ± 0.2740026856122927
	data : 0.11690773963928222
	model : 0.0695676326751709
			 train-loss:  2.10579485197862 	 ± 0.273148296307382
	data : 0.11634135246276855
	model : 0.06856746673583984
			 train-loss:  2.1079277416755415 	 ± 0.27340542709744603
	data : 0.11727361679077149
	model : 0.06857013702392578
			 train-loss:  2.1082742998044783 	 ± 0.2724994541633527
	data : 0.1174278736114502
	model : 0.06794610023498535
			 train-loss:  2.1093957991827104 	 ± 0.2719088874422179
	data : 0.11778378486633301
	model : 0.06840376853942871
			 train-loss:  2.1113173075624414 	 ± 0.27198830601138074
	data : 0.11745133399963378
	model : 0.06797490119934083
			 train-loss:  2.110804381786577 	 ± 0.27114586873144647
	data : 0.11793637275695801
	model : 0.06878066062927246
			 train-loss:  2.1084375826517743 	 ± 0.2717804393859025
	data : 0.11717019081115723
	model : 0.06889567375183106
			 train-loss:  2.1063045563287295 	 ± 0.2721358260918459
	data : 0.11710567474365234
	model : 0.0697558879852295
			 train-loss:  2.108753859212524 	 ± 0.27290391448618007
	data : 0.11649155616760254
	model : 0.06911406517028809
			 train-loss:  2.107552938211977 	 ± 0.2724132666332779
	data : 0.11709780693054199
	model : 0.06877303123474121
			 train-loss:  2.105238285931674 	 ± 0.2730326471610218
	data : 0.11743154525756835
	model : 0.06907382011413574
			 train-loss:  2.104721672304215 	 ± 0.2722259725910229
	data : 0.11710810661315918
	model : 0.06856508255004883
			 train-loss:  2.1037886838118234 	 ± 0.27160054837063924
	data : 0.1174283504486084
	model : 0.06840720176696777
			 train-loss:  2.1029716471957554 	 ± 0.2709264529260514
	data : 0.11743779182434082
	model : 0.06856131553649902
			 train-loss:  2.10150334729424 	 ± 0.2706936565970421
	data : 0.11717662811279297
	model : 0.06952705383300781
			 train-loss:  2.1020855026425056 	 ± 0.2699402772485883
	data : 0.11644387245178223
	model : 0.06945986747741699
			 train-loss:  2.10204054787755 	 ± 0.2690959887308803
	data : 0.11665573120117187
	model : 0.06996817588806152
			 train-loss:  2.1010154804087575 	 ± 0.2685721602511524
	data : 0.1160696029663086
	model : 0.07000155448913574
			 train-loss:  2.099315517478519 	 ± 0.2686094224916605
	data : 0.11613812446594238
	model : 0.06954612731933593
			 train-loss:  2.099726249103897 	 ± 0.2678352236381485
	data : 0.11658949851989746
	model : 0.06940989494323731
			 train-loss:  2.0974185081516823 	 ± 0.26863800570082863
	data : 0.11656360626220703
	model : 0.06906604766845703
			 train-loss:  2.0966602347113867 	 ± 0.2679986988969795
	data : 0.11708793640136719
	model : 0.06927337646484374
			 train-loss:  2.097341368715447 	 ± 0.2673334674063758
	data : 0.11704883575439454
	model : 0.06929311752319336
			 train-loss:  2.0987191978328954 	 ± 0.26712239173066527
	data : 0.11689958572387696
	model : 0.06982574462890626
			 train-loss:  2.0983153063626516 	 ± 0.2663773379767305
	data : 0.11641278266906738
	model : 0.07006850242614746
			 train-loss:  2.097586043487639 	 ± 0.265756221836774
	data : 0.11623001098632812
	model : 0.07048535346984863
			 train-loss:  2.0977134690565222 	 ± 0.2649786110472027
	data : 0.11580410003662109
	model : 0.06943893432617188
			 train-loss:  2.098571562627603 	 ± 0.2644394699482267
	data : 0.11672453880310059
	model : 0.06952428817749023
			 train-loss:  2.096307711545811 	 ± 0.2653263132163376
	data : 0.116717529296875
	model : 0.06968011856079101
			 train-loss:  2.0950389707708634 	 ± 0.26508111262138606
	data : 0.11666135787963867
	model : 0.06947894096374511
			 train-loss:  2.0940848081961447 	 ± 0.264616063279354
	data : 0.1167292594909668
	model : 0.0691725730895996
			 train-loss:  2.0938167871747697 	 ± 0.2638826188009476
	data : 0.11678643226623535
	model : 0.07011442184448242
			 train-loss:  2.0930234789848328 	 ± 0.26334107674325336
	data : 0.11609277725219727
	model : 0.0701456069946289
			 train-loss:  2.096154463493218 	 ± 0.26586098322161733
	data : 0.11594676971435547
	model : 0.06994776725769043
			 train-loss:  2.0948587086763273 	 ± 0.2656730165408558
	data : 0.11603097915649414
	model : 0.06920890808105469
			 train-loss:  2.096749532822124 	 ± 0.2661282143259673
	data : 0.11687264442443848
	model : 0.06920919418334961
			 train-loss:  2.0967019498348236 	 ± 0.2653887032575618
	data : 0.116827392578125
	model : 0.06917390823364258
			 train-loss:  2.0950159637967527 	 ± 0.26561946496897915
	data : 0.11685948371887207
	model : 0.06898264884948731
			 train-loss:  2.094064840903649 	 ± 0.265197627003565
	data : 0.11715464591979981
	model : 0.06831932067871094
			 train-loss:  2.092004600769835 	 ± 0.26592852591546806
	data : 0.1176692008972168
	model : 0.06911540031433105
			 train-loss:  2.091692579181298 	 ± 0.2652384973193069
	data : 0.1170154094696045
	model : 0.06922273635864258
			 train-loss:  2.0889039284474142 	 ± 0.26721166088332104
	data : 0.11705217361450196
	model : 0.06836280822753907
			 train-loss:  2.088849461206826 	 ± 0.26649341162944223
	data : 0.11781220436096192
	model : 0.06879558563232421
			 train-loss:  2.0902281011489623 	 ± 0.2664441398054998
	data : 0.11771550178527831
	model : 0.0692514419555664
			 train-loss:  2.0904191364633276 	 ± 0.26574740738911584
	data : 0.11737332344055176
	model : 0.06871004104614258
			 train-loss:  2.0897729056222096 	 ± 0.2651915087552949
	data : 0.11769580841064453
	model : 0.06861095428466797
			 train-loss:  2.0930458909586855 	 ± 0.26829283719567604
	data : 0.11773948669433594
	model : 0.06944904327392579
			 train-loss:  2.0927298118930837 	 ± 0.2676250446205277
	data : 0.11695375442504882
	model : 0.06891040802001953
			 train-loss:  2.0924780977269015 	 ± 0.26694986225336614
	data : 0.11736521720886231
	model : 0.0690535068511963
			 train-loss:  2.0955090819245177 	 ± 0.2695493882941555
	data : 0.11719884872436523
	model : 0.06953659057617187
			 train-loss:  2.095531544734522 	 ± 0.2688539569185299
	data : 0.11673650741577149
	model : 0.06976041793823243
			 train-loss:  2.0948437433976395 	 ± 0.2683347659751488
	data : 0.11658134460449218
	model : 0.06970090866088867
			 train-loss:  2.097840281165376 	 ± 0.27090059638700253
	data : 0.11654105186462402
	model : 0.06972179412841797
			 train-loss:  2.09698631315667 	 ± 0.2704765143796827
	data : 0.11626009941101074
	model : 0.06882762908935547
			 train-loss:  2.09654225965943 	 ± 0.2698646094479383
	data : 0.11715350151062012
	model : 0.06790552139282227
			 train-loss:  2.0970476524314687 	 ± 0.2692796252979954
	data : 0.11798810958862305
	model : 0.06751694679260253
			 train-loss:  2.0987915384769438 	 ± 0.2697297639990934
	data : 0.11834335327148438
	model : 0.0675461769104004
			 train-loss:  2.0987871903092112 	 ± 0.26905796484215194
	data : 0.1183931827545166
	model : 0.06758394241333007
			 train-loss:  2.0986687570515246 	 ± 0.26839640569169826
	data : 0.1184432029724121
	model : 0.06855220794677734
			 train-loss:  2.0981932332363034 	 ± 0.2678198035332447
	data : 0.1176900863647461
	model : 0.06962566375732422
			 train-loss:  2.0979904146755444 	 ± 0.26717820366049605
	data : 0.11654953956604004
	model : 0.07001695632934571
			 train-loss:  2.0991736144554323 	 ± 0.26706098391327376
	data : 0.11614656448364258
	model : 0.07022390365600586
			 train-loss:  2.099014071584905 	 ± 0.26642178209468537
	data : 0.11614031791687011
	model : 0.07033586502075195
			 train-loss:  2.0986944765284443 	 ± 0.26581705313171794
	data : 0.11597666740417481
	model : 0.07032465934753418
			 train-loss:  2.098841056227684 	 ± 0.2651856857760187
	data : 0.11589040756225585
	model : 0.07027630805969239
			 train-loss:  2.099791924919238 	 ± 0.26490571072911007
	data : 0.11618857383728028
	model : 0.07003617286682129
			 train-loss:  2.0988172803606306 	 ± 0.2646495881955604
	data : 0.11647076606750488
	model : 0.06999502182006836
			 train-loss:  2.100012599574446 	 ± 0.2645893227457467
	data : 0.11633663177490235
	model : 0.06999287605285645
			 train-loss:  2.0998512506484985 	 ± 0.26397495841851765
	data : 0.11654586791992187
	model : 0.0698082447052002
			 train-loss:  2.098787500265059 	 ± 0.2638096306631119
	data : 0.11668763160705567
	model : 0.0696535587310791
			 train-loss:  2.097605031227397 	 ± 0.2637577148614003
	data : 0.11671476364135742
	model : 0.06986675262451172
			 train-loss:  2.09585401900979 	 ± 0.26438739155165353
	data : 0.11648821830749512
	model : 0.06888136863708497
			 train-loss:  2.095551172340358 	 ± 0.263812049488894
	data : 0.1173924446105957
	model : 0.06914887428283692
			 train-loss:  2.0962130040067684 	 ± 0.26338315712288535
	data : 0.11712207794189453
	model : 0.0686957836151123
			 train-loss:  2.094898014440449 	 ± 0.26349138347163054
	data : 0.11746530532836914
	model : 0.0687286376953125
			 train-loss:  2.0947910100902054 	 ± 0.2628938640320738
	data : 0.11735324859619141
	model : 0.06874499320983887
			 train-loss:  2.096006422151219 	 ± 0.26291166864405474
	data : 0.11729021072387695
	model : 0.06885595321655273
			 train-loss:  2.0945450105278742 	 ± 0.26321024500442786
	data : 0.11707940101623535
	model : 0.06864404678344727
			 train-loss:  2.093583018930109 	 ± 0.26300585964458034
	data : 0.11727843284606934
	model : 0.06923823356628418
			 train-loss:  2.096440768562625 	 ± 0.2658475281350777
	data : 0.11679143905639648
	model : 0.06912603378295898
			 train-loss:  2.097029590180942 	 ± 0.2653991555425977
	data : 0.1169771671295166
	model : 0.06888160705566407
			 train-loss:  2.099105164210002 	 ± 0.2666245551363736
	data : 0.11710638999938965
	model : 0.06952552795410157
			 train-loss:  2.0980376738362607 	 ± 0.2665154730405001
	data : 0.11659555435180664
	model : 0.06917390823364258
			 train-loss:  2.0957730241809123 	 ± 0.2680982280654793
	data : 0.11691694259643555
	model : 0.06909542083740235
			 train-loss:  2.0975611246468726 	 ± 0.26886278884830417
	data : 0.1171792984008789
	model : 0.06911425590515137
			 train-loss:  2.0968001071022067 	 ± 0.26852109838556665
	data : 0.11699695587158203
	model : 0.06842284202575684
			 train-loss:  2.096390456220378 	 ± 0.2680084249074939
	data : 0.11768569946289062
	model : 0.06826090812683105
			 train-loss:  2.0958419424114805 	 ± 0.2675570394647201
	data : 0.11777024269104004
	model : 0.06838674545288086
			 train-loss:  2.0965455595789284 	 ± 0.26719387814917134
	data : 0.11727123260498047
	model : 0.06821184158325196
			 train-loss:  2.095940482974564 	 ± 0.2667791257813376
	data : 0.11718220710754394
	model : 0.06803922653198242
			 train-loss:  2.0945590128246536 	 ± 0.2670423619426456
	data : 0.1173464298248291
	model : 0.06828751564025878
			 train-loss:  2.093726022700046 	 ± 0.2667780641898199
	data : 0.1169097900390625
	model : 0.06755690574645996
			 train-loss:  2.094849990080979 	 ± 0.266769266586876
	data : 0.1176215648651123
	model : 0.06740922927856445
			 train-loss:  2.092190804360788 	 ± 0.269322080777933
	data : 0.1176943302154541
	model : 0.06739888191223145
			 train-loss:  2.091345234077518 	 ± 0.2690707514692894
	data : 0.11765899658203124
	model : 0.06753168106079102
			 train-loss:  2.0915693887606825 	 ± 0.2685295192464797
	data : 0.11764049530029297
	model : 0.06778907775878906
			 train-loss:  2.090097128848235 	 ± 0.26893437219794536
	data : 0.11775221824645996
	model : 0.06879463195800781
			 train-loss:  2.089857546125705 	 ± 0.26840150129282186
	data : 0.11679210662841796
	model : 0.06855711936950684
			 train-loss:  2.089281928440756 	 ± 0.2679954000993754
	data : 0.11714634895324708
	model : 0.06816802024841309
			 train-loss:  2.0893514440874013 	 ± 0.267445587084626
	data : 0.11760916709899902
	model : 0.06827721595764161
			 train-loss:  2.0887509022579818 	 ± 0.26706110928968935
	data : 0.11749491691589356
	model : 0.06848382949829102
			 train-loss:  2.089572732789176 	 ± 0.26682452342176644
	data : 0.1173365592956543
	model : 0.06816687583923339
			 train-loss:  2.089913951187599 	 ± 0.2663352018652545
	data : 0.1176602840423584
	model : 0.06858587265014648
			 train-loss:  2.0889829233107777 	 ± 0.2661963406232718
	data : 0.11718411445617676
	model : 0.06821560859680176
			 train-loss:  2.088361931423987 	 ± 0.2658383245695695
	data : 0.11735501289367675
	model : 0.06761417388916016
			 train-loss:  2.088565623425097 	 ± 0.2653233670757257
	data : 0.11776418685913086
	model : 0.06749095916748046
			 train-loss:  2.087312115192413 	 ± 0.2655299467092114
	data : 0.11755490303039551
	model : 0.06685271263122558
			 train-loss:  2.086776509702918 	 ± 0.26513575749626617
	data : 0.11803345680236817
	model : 0.06641321182250977
			 train-loss:  2.087867342763477 	 ± 0.26517292995630726
	data : 0.11856532096862793
	model : 0.06712942123413086
			 train-loss:  2.0874655369242188 	 ± 0.2647252086751402
	data : 0.11799349784851074
	model : 0.06723790168762207
			 train-loss:  2.087998278497711 	 ± 0.2643394364318658
	data : 0.11791043281555176
	model : 0.06647934913635253
			 train-loss:  2.0886698395598167 	 ± 0.2640376284987012
	data : 0.11835074424743652
	model : 0.06684408187866211
			 train-loss:  2.087690695654601 	 ± 0.2639848782783301
	data : 0.1168644905090332
	model : 0.05815706253051758
#epoch  70    val-loss:  2.440157313095896  train-loss:  2.087690695654601  lr:  1.953125e-05
			 train-loss:  2.0206706523895264 	 ± 0.0
	data : 5.6373889446258545
	model : 0.07307744026184082
			 train-loss:  1.9421631693840027 	 ± 0.07850748300552368
	data : 2.8866124153137207
	model : 0.07269155979156494
			 train-loss:  1.9328747590382893 	 ± 0.0654331648229125
	data : 1.963394562403361
	model : 0.07051459948221843
			 train-loss:  1.8705198764801025 	 ± 0.12196523451620404
	data : 1.502216875553131
	model : 0.06955456733703613
			 train-loss:  1.9753167152404785 	 ± 0.23628356777640522
	data : 1.225470781326294
	model : 0.06869196891784668
			 train-loss:  1.982807199160258 	 ± 0.21634572736182767
	data : 0.1220935344696045
	model : 0.06804442405700684
			 train-loss:  2.00397184916905 	 ± 0.20689763642442166
	data : 0.11821002960205078
	model : 0.06768536567687988
			 train-loss:  1.9783801883459091 	 ± 0.20503739826935824
	data : 0.11782035827636719
	model : 0.06839232444763184
			 train-loss:  2.037422219912211 	 ± 0.25545425876679323
	data : 0.11727280616760254
	model : 0.06886529922485352
			 train-loss:  1.991102921962738 	 ± 0.2793572743865302
	data : 0.11697626113891602
	model : 0.06976914405822754
			 train-loss:  2.028403574770147 	 ± 0.29130616891248906
	data : 0.11608343124389649
	model : 0.06982641220092774
			 train-loss:  2.0071656902631125 	 ± 0.28766161051533207
	data : 0.11607909202575684
	model : 0.06982479095458985
			 train-loss:  2.00076109629411 	 ± 0.27726540499630004
	data : 0.11638426780700684
	model : 0.07003321647644042
			 train-loss:  2.0352191669600352 	 ± 0.29465338400667235
	data : 0.11622743606567383
	model : 0.0701573371887207
			 train-loss:  2.014382537206014 	 ± 0.2951455362120482
	data : 0.11607756614685058
	model : 0.07019534111022949
			 train-loss:  2.0161211490631104 	 ± 0.2858527569258751
	data : 0.11624512672424317
	model : 0.06996588706970215
			 train-loss:  2.0232303423040054 	 ± 0.27877207949863353
	data : 0.11641602516174317
	model : 0.0698936939239502
			 train-loss:  2.0461788839764066 	 ± 0.28696557208257106
	data : 0.11628789901733398
	model : 0.06917085647583007
			 train-loss:  2.0422687781484505 	 ± 0.2798039845213582
	data : 0.116961669921875
	model : 0.06927428245544434
			 train-loss:  2.057899796962738 	 ± 0.28110140963824204
	data : 0.11690220832824708
	model : 0.06951942443847656
			 train-loss:  2.0652054037366594 	 ± 0.2762655937381122
	data : 0.11643481254577637
	model : 0.06986308097839355
			 train-loss:  2.0502760193564673 	 ± 0.2784494254108362
	data : 0.11615262031555176
	model : 0.06977314949035644
			 train-loss:  2.04083948550017 	 ± 0.2759023291180507
	data : 0.11625199317932129
	model : 0.07119555473327636
			 train-loss:  2.0304004003604255 	 ± 0.27469392915084206
	data : 0.1148484230041504
	model : 0.07109622955322266
			 train-loss:  2.032044711112976 	 ± 0.26926450696714116
	data : 0.1149472713470459
	model : 0.07033185958862305
			 train-loss:  2.0359919392145596 	 ± 0.26477216360357647
	data : 0.11570730209350585
	model : 0.07005233764648437
			 train-loss:  2.067347045297976 	 ± 0.3050730285193354
	data : 0.1158747673034668
	model : 0.07001047134399414
			 train-loss:  2.068230709859303 	 ± 0.2996109534629837
	data : 0.11574592590332031
	model : 0.06925997734069825
			 train-loss:  2.0636642883563865 	 ± 0.29538987952907825
	data : 0.11650185585021973
	model : 0.06933751106262206
			 train-loss:  2.05733052889506 	 ± 0.2924210212275572
	data : 0.11651978492736817
	model : 0.0699303150177002
			 train-loss:  2.0678034136372228 	 ± 0.293329344377341
	data : 0.11593866348266602
	model : 0.07011055946350098
			 train-loss:  2.0738615542650223 	 ± 0.29067339337849013
	data : 0.11568651199340821
	model : 0.06926007270812988
			 train-loss:  2.069777691003048 	 ± 0.28716612210060094
	data : 0.11664757728576661
	model : 0.06909594535827637
			 train-loss:  2.06404041893342 	 ± 0.2848248519495313
	data : 0.11687164306640625
	model : 0.06891651153564453
			 train-loss:  2.076878878048488 	 ± 0.2905364325523742
	data : 0.1167797565460205
	model : 0.06845512390136718
			 train-loss:  2.071741839249929 	 ± 0.2880803323345605
	data : 0.11713619232177734
	model : 0.06827902793884277
			 train-loss:  2.069438724904447 	 ± 0.28449649115513354
	data : 0.11739959716796874
	model : 0.06922340393066406
			 train-loss:  2.077200384516465 	 ± 0.2846705178942164
	data : 0.11657447814941406
	model : 0.06935563087463378
			 train-loss:  2.0680423302528186 	 ± 0.28661207681230516
	data : 0.11643428802490234
	model : 0.06956968307495118
			 train-loss:  2.0708693832159044 	 ± 0.28355690331939326
	data : 0.11633896827697754
	model : 0.07009525299072265
			 train-loss:  2.0789789484768377 	 ± 0.28473502344847507
	data : 0.11600570678710938
	model : 0.07021036148071289
			 train-loss:  2.090861561752501 	 ± 0.2914322492004048
	data : 0.11594433784484863
	model : 0.07030982971191406
			 train-loss:  2.081588817197223 	 ± 0.29422592467611725
	data : 0.11579046249389649
	model : 0.07027416229248047
			 train-loss:  2.081486328081651 	 ± 0.2908640086769224
	data : 0.11590390205383301
	model : 0.0694551944732666
			 train-loss:  2.085982280307346 	 ± 0.2891560612472838
	data : 0.1165201187133789
	model : 0.06920981407165527
			 train-loss:  2.084728857745295 	 ± 0.28611936396441195
	data : 0.11669697761535644
	model : 0.06911292076110839
			 train-loss:  2.0839993142067117 	 ± 0.28310241923289203
	data : 0.11672010421752929
	model : 0.06897549629211426
			 train-loss:  2.0825423250595727 	 ± 0.2803159350568788
	data : 0.11682376861572266
	model : 0.06894841194152831
			 train-loss:  2.077864510672433 	 ± 0.27932730657877597
	data : 0.11691112518310547
	model : 0.06915817260742188
			 train-loss:  2.080547595024109 	 ± 0.27715702671376496
	data : 0.11675643920898438
	model : 0.0683018684387207
			 train-loss:  2.0785646532096114 	 ± 0.274784322602431
	data : 0.1173781394958496
	model : 0.06821017265319824
			 train-loss:  2.0775619378456702 	 ± 0.2722235380654033
	data : 0.11745667457580566
	model : 0.06818451881408691
			 train-loss:  2.081256735999629 	 ± 0.2709562963173302
	data : 0.1173816204071045
	model : 0.06796040534973144
			 train-loss:  2.0835796064800687 	 ± 0.2689678571146539
	data : 0.11752643585205078
	model : 0.06848983764648438
			 train-loss:  2.0828557578000155 	 ± 0.26656455444356625
	data : 0.11726799011230468
	model : 0.06924214363098144
			 train-loss:  2.082287162542343 	 ± 0.26420744550109
	data : 0.11668190956115723
	model : 0.06944108009338379
			 train-loss:  2.082349371491817 	 ± 0.26187999499406034
	data : 0.11649060249328613
	model : 0.06950397491455078
			 train-loss:  2.078248747463884 	 ± 0.2614520226818347
	data : 0.11649622917175292
	model : 0.06987791061401367
			 train-loss:  2.0734638561636713 	 ± 0.2617756385632813
	data : 0.11630921363830567
	model : 0.07004003524780274
			 train-loss:  2.0821675221125284 	 ± 0.2680556866003811
	data : 0.1160205364227295
	model : 0.06951055526733399
			 train-loss:  2.080169093413431 	 ± 0.2662997224353983
	data : 0.11645326614379883
	model : 0.06865582466125489
			 train-loss:  2.085615509940732 	 ± 0.2675466571165591
	data : 0.11739907264709473
	model : 0.07000799179077148
			 train-loss:  2.086442990908547 	 ± 0.2654947399235288
	data : 0.11600193977355958
	model : 0.06931848526000976
			 train-loss:  2.0957618448883295 	 ± 0.27360021180703936
	data : 0.11651053428649902
	model : 0.06819500923156738
			 train-loss:  2.1068497712795553 	 ± 0.2856111407584941
	data : 0.11762075424194336
	model : 0.06925778388977051
			 train-loss:  2.1088346513834866 	 ± 0.28389054563534133
	data : 0.11667871475219727
	model : 0.07036895751953125
			 train-loss:  2.1108217684190667 	 ± 0.2822260770904992
	data : 0.11562027931213378
	model : 0.06900639533996582
			 train-loss:  2.113636029117248 	 ± 0.2810886984203064
	data : 0.116961669921875
	model : 0.0695505142211914
			 train-loss:  2.105049601499585 	 ± 0.28788746025560624
	data : 0.11638064384460449
	model : 0.0700526237487793
			 train-loss:  2.1055005192756653 	 ± 0.2858482655170696
	data : 0.11589512825012208
	model : 0.06880655288696289
			 train-loss:  2.0988555726870683 	 ± 0.28922183306706406
	data : 0.11695194244384766
	model : 0.06860938072204589
			 train-loss:  2.097079290284051 	 ± 0.2875960553366316
	data : 0.11703290939331054
	model : 0.06853432655334472
			 train-loss:  2.100501292372403 	 ± 0.28709159513708793
	data : 0.11713833808898926
	model : 0.06835823059082032
			 train-loss:  2.0963034436509416 	 ± 0.2873920318711616
	data : 0.11731986999511719
	model : 0.06791563034057617
			 train-loss:  2.0929537216822305 	 ± 0.28692028754993054
	data : 0.11782841682434082
	model : 0.06889238357543945
			 train-loss:  2.0893784337922146 	 ± 0.28670324726719504
	data : 0.11703019142150879
	model : 0.06793050765991211
			 train-loss:  2.0925920892071415 	 ± 0.2862099437466583
	data : 0.11773343086242676
	model : 0.06801681518554688
			 train-loss:  2.09293276988543 	 ± 0.2843850593055698
	data : 0.11778149604797364
	model : 0.06820015907287598
			 train-loss:  2.089591696292539 	 ± 0.2841158668591909
	data : 0.11756844520568847
	model : 0.06929225921630859
			 train-loss:  2.0876787304878235 	 ± 0.28284606872655316
	data : 0.11656041145324707
	model : 0.06875886917114257
			 train-loss:  2.0866511545063537 	 ± 0.2812449000879144
	data : 0.11709733009338379
	model : 0.06964821815490722
			 train-loss:  2.086136320742165 	 ± 0.2795631323444884
	data : 0.11648731231689453
	model : 0.06890192031860351
			 train-loss:  2.0892091055950486 	 ± 0.27926359671115797
	data : 0.11716489791870117
	model : 0.06887264251708984
			 train-loss:  2.0851107835769653 	 ± 0.2800960803248613
	data : 0.1170987606048584
	model : 0.06778407096862793
			 train-loss:  2.0852096978355856 	 ± 0.2784450574572104
	data : 0.11817789077758789
	model : 0.06828718185424805
			 train-loss:  2.085806408593821 	 ± 0.27687611783088173
	data : 0.11778321266174316
	model : 0.06825084686279297
			 train-loss:  2.087752712183985 	 ± 0.2758713598364865
	data : 0.11765251159667969
	model : 0.06801061630249024
			 train-loss:  2.086079765449871 	 ± 0.27474291413692575
	data : 0.11769399642944336
	model : 0.06813673973083496
			 train-loss:  2.081207635697354 	 ± 0.2769917883443482
	data : 0.11756553649902343
	model : 0.06907892227172852
			 train-loss:  2.0763926174905567 	 ± 0.2791690680248057
	data : 0.11664185523986817
	model : 0.06819319725036621
			 train-loss:  2.076376109332829 	 ± 0.27763097890407357
	data : 0.11763348579406738
	model : 0.06824951171875
			 train-loss:  2.0762914587622103 	 ± 0.2761191730760521
	data : 0.11743803024291992
	model : 0.06919264793395996
			 train-loss:  2.073161380265349 	 ± 0.2762668157269452
	data : 0.11658453941345215
	model : 0.06938738822937011
			 train-loss:  2.075621175005081 	 ± 0.2758153508927412
	data : 0.11650147438049316
	model : 0.06942410469055176
			 train-loss:  2.0712855426888717 	 ± 0.27756136658179514
	data : 0.11623172760009766
	model : 0.07119865417480468
			 train-loss:  2.071627650409937 	 ± 0.27613208351673324
	data : 0.11439423561096192
	model : 0.07115607261657715
			 train-loss:  2.0727780590352323 	 ± 0.27493618599553904
	data : 0.11474738121032715
	model : 0.071185302734375
			 train-loss:  2.076657829236011 	 ± 0.2761859602246725
	data : 0.11483216285705566
	model : 0.07087230682373047
			 train-loss:  2.078244169553121 	 ± 0.27523591239207623
	data : 0.11503987312316895
	model : 0.07007737159729004
			 train-loss:  2.0749064111709594 	 ± 0.2758626141149737
	data : 0.11583819389343261
	model : 0.06914319992065429
			 train-loss:  2.073391149539759 	 ± 0.2749114702959014
	data : 0.11670103073120117
	model : 0.06928648948669433
			 train-loss:  2.0690567435002793 	 ± 0.27700697981831157
	data : 0.11674761772155762
	model : 0.06928400993347168
			 train-loss:  2.0715880567587694 	 ± 0.2768419356834641
	data : 0.11656289100646973
	model : 0.06886625289916992
			 train-loss:  2.0727810779443154 	 ± 0.2757736755323077
	data : 0.11693973541259765
	model : 0.06859855651855469
			 train-loss:  2.0766291334515525 	 ± 0.277248637468177
	data : 0.11713075637817383
	model : 0.06893811225891114
			 train-loss:  2.07898504216716 	 ± 0.2769917517518771
	data : 0.11680378913879394
	model : 0.06889009475708008
			 train-loss:  2.0815422679776345 	 ± 0.2769486533520733
	data : 0.11671757698059082
	model : 0.06898655891418456
			 train-loss:  2.0803098689626762 	 ± 0.2759581102979749
	data : 0.11681208610534669
	model : 0.06937837600708008
			 train-loss:  2.081920242090838 	 ± 0.27519866451450975
	data : 0.11635298728942871
	model : 0.07008743286132812
			 train-loss:  2.082595104520971 	 ± 0.2740354979794733
	data : 0.1158106803894043
	model : 0.06950712203979492
			 train-loss:  2.080787777900696 	 ± 0.27345607650264114
	data : 0.11623163223266601
	model : 0.06912569999694824
			 train-loss:  2.080856417970998 	 ± 0.2722335137672259
	data : 0.11643986701965332
	model : 0.06903390884399414
			 train-loss:  2.0795027861552957 	 ± 0.27140459730384914
	data : 0.11661343574523926
	model : 0.06910648345947265
			 train-loss:  2.0802838540913764 	 ± 0.2703391364127058
	data : 0.11686606407165527
	model : 0.06954560279846192
			 train-loss:  2.0768539760423743 	 ± 0.2716410230382653
	data : 0.11642656326293946
	model : 0.06993942260742188
			 train-loss:  2.075642580616063 	 ± 0.27077942080624234
	data : 0.11630806922912598
	model : 0.07020764350891114
			 train-loss:  2.0749651754004326 	 ± 0.2697184552785438
	data : 0.11622676849365235
	model : 0.0700948715209961
			 train-loss:  2.0746680296073525 	 ± 0.2685923804740695
	data : 0.11639165878295898
	model : 0.07004351615905761
			 train-loss:  2.073480007027378 	 ± 0.2677726229402298
	data : 0.11633243560791015
	model : 0.06975626945495605
			 train-loss:  2.0731981734434766 	 ± 0.2666722925985516
	data : 0.11668286323547364
	model : 0.07033243179321289
			 train-loss:  2.0725752755630116 	 ± 0.26565570184206255
	data : 0.11589598655700684
	model : 0.07036423683166504
			 train-loss:  2.07200713626674 	 ± 0.2646385114748584
	data : 0.11576147079467773
	model : 0.07094659805297851
			 train-loss:  2.0702279079251173 	 ± 0.2642922115639255
	data : 0.11514072418212891
	model : 0.07109427452087402
			 train-loss:  2.070226346292803 	 ± 0.26322436044728087
	data : 0.11499066352844238
	model : 0.07206964492797852
			 train-loss:  2.0717558727264405 	 ± 0.26272201917464993
	data : 0.11384954452514648
	model : 0.07057533264160157
			 train-loss:  2.071216002343193 	 ± 0.26174699882126407
	data : 0.1153635025024414
	model : 0.0706052303314209
			 train-loss:  2.0737884589067592 	 ± 0.26230867305332956
	data : 0.11538777351379395
	model : 0.07010231018066407
			 train-loss:  2.0760523211210966 	 ± 0.2625246245483609
	data : 0.11585879325866699
	model : 0.07041354179382324
			 train-loss:  2.076232063677884 	 ± 0.26151301428825324
	data : 0.11553053855895996
	model : 0.06959714889526367
			 train-loss:  2.073132725862356 	 ± 0.2628728731099391
	data : 0.11645431518554687
	model : 0.06996669769287109
			 train-loss:  2.0729179327724543 	 ± 0.26187907094275004
	data : 0.1160315990447998
	model : 0.06991925239562988
			 train-loss:  2.075664865248131 	 ± 0.2627728601228245
	data : 0.11604247093200684
	model : 0.06989493370056152
			 train-loss:  2.072722979058 	 ± 0.26395610606233333
	data : 0.1160661220550537
	model : 0.06947917938232422
			 train-loss:  2.072919328710926 	 ± 0.26297910000456065
	data : 0.11642050743103027
	model : 0.06948699951171874
			 train-loss:  2.0710855916694357 	 ± 0.2628617750293633
	data : 0.11650295257568359
	model : 0.06992316246032715
			 train-loss:  2.0679535620352802 	 ± 0.2644098125036458
	data : 0.11625175476074219
	model : 0.06999897956848145
			 train-loss:  2.0663219786038365 	 ± 0.2641292849024893
	data : 0.11611838340759277
	model : 0.07018370628356933
			 train-loss:  2.068161852117898 	 ± 0.2640501928071276
	data : 0.11622781753540039
	model : 0.06991572380065918
			 train-loss:  2.0668813093103093 	 ± 0.2635283563009723
	data : 0.11649971008300782
	model : 0.06894211769104004
			 train-loss:  2.0675977408885955 	 ± 0.2627213129907009
	data : 0.11725716590881348
	model : 0.06878972053527832
			 train-loss:  2.0688205500866506 	 ± 0.26218753561682256
	data : 0.11732792854309082
	model : 0.06864757537841797
			 train-loss:  2.0705146814735844 	 ± 0.26203603672897535
	data : 0.11745305061340332
	model : 0.06853318214416504
			 train-loss:  2.070086760120792 	 ± 0.26116800547157615
	data : 0.11731410026550293
	model : 0.06880350112915039
			 train-loss:  2.0713594199882612 	 ± 0.26070417563768744
	data : 0.11699352264404297
	model : 0.06916980743408203
			 train-loss:  2.0698524401105685 	 ± 0.26043224432038786
	data : 0.11671123504638672
	model : 0.06903390884399414
			 train-loss:  2.0718122222652173 	 ± 0.26060949251072146
	data : 0.11688556671142578
	model : 0.06893901824951172
			 train-loss:  2.0718896721496063 	 ± 0.25972323897729377
	data : 0.11701431274414062
	model : 0.06884579658508301
			 train-loss:  2.071240614394884 	 ± 0.25896390397425245
	data : 0.11712346076965333
	model : 0.06890959739685058
			 train-loss:  2.0730122279800827 	 ± 0.25899176813606556
	data : 0.11722016334533691
	model : 0.06950507164001465
			 train-loss:  2.0746269615491233 	 ± 0.2588784561002446
	data : 0.11666688919067383
	model : 0.0697258472442627
			 train-loss:  2.075422841981547 	 ± 0.2582038742994148
	data : 0.11637744903564454
	model : 0.0701484203338623
			 train-loss:  2.078580469677323 	 ± 0.2602617716645758
	data : 0.11592464447021485
	model : 0.06952710151672363
			 train-loss:  2.0777865850847532 	 ± 0.25959442985687997
	data : 0.11649413108825683
	model : 0.06922364234924316
			 train-loss:  2.074749374544466 	 ± 0.26146327942330333
	data : 0.1167675495147705
	model : 0.0687143325805664
			 train-loss:  2.0754099022957586 	 ± 0.2607473572217857
	data : 0.11720876693725586
	model : 0.0687941551208496
			 train-loss:  2.0772978502970476 	 ± 0.2609709372330254
	data : 0.11718158721923828
	model : 0.06839680671691895
			 train-loss:  2.079174807876538 	 ± 0.26119268417404684
	data : 0.11749839782714844
	model : 0.0691378116607666
			 train-loss:  2.082317174989966 	 ± 0.2633251371039537
	data : 0.11685075759887695
	model : 0.06913175582885742
			 train-loss:  2.08360818871912 	 ± 0.2629968965536252
	data : 0.11681318283081055
	model : 0.06972198486328125
			 train-loss:  2.0856508888304233 	 ± 0.263435986009445
	data : 0.11621499061584473
	model : 0.06891984939575195
			 train-loss:  2.0840773064157236 	 ± 0.26336981201943205
	data : 0.11687331199645996
	model : 0.06892547607421876
			 train-loss:  2.083120315163224 	 ± 0.26283632958347364
	data : 0.11709551811218262
	model : 0.06852178573608399
			 train-loss:  2.083388925330039 	 ± 0.26205114574270677
	data : 0.11731886863708496
	model : 0.06870565414428711
			 train-loss:  2.0842684841737515 	 ± 0.26149221642226156
	data : 0.11739792823791503
	model : 0.06864433288574219
			 train-loss:  2.0829748074213663 	 ± 0.26122449419294336
	data : 0.11749000549316406
	model : 0.06941494941711426
			 train-loss:  2.085983906165663 	 ± 0.26328916436124333
	data : 0.11678876876831054
	model : 0.07003779411315918
			 train-loss:  2.0871848867325014 	 ± 0.26295535358094907
	data : 0.11598801612854004
	model : 0.07034077644348144
			 train-loss:  2.086922199243591 	 ± 0.2621935569392285
	data : 0.11564521789550782
	model : 0.06959257125854493
			 train-loss:  2.0856971007127028 	 ± 0.2618985089302182
	data : 0.11609587669372559
	model : 0.06936888694763184
			 train-loss:  2.0869746095993937 	 ± 0.2616546701451672
	data : 0.11636953353881836
	model : 0.06845355033874512
			 train-loss:  2.0871297685723555 	 ± 0.26089631972633165
	data : 0.11728377342224121
	model : 0.06785998344421387
			 train-loss:  2.0862258069737014 	 ± 0.26040523032386437
	data : 0.11799454689025879
	model : 0.06794042587280273
			 train-loss:  2.0855055640887663 	 ± 0.25982328325283355
	data : 0.11804051399230957
	model : 0.06800117492675781
			 train-loss:  2.0875751191172105 	 ± 0.26050168857306993
	data : 0.11796455383300782
	model : 0.068211030960083
			 train-loss:  2.088872071674892 	 ± 0.2603191019079366
	data : 0.11776328086853027
	model : 0.06920762062072754
			 train-loss:  2.0883779634128916 	 ± 0.2596607893964087
	data : 0.11698250770568848
	model : 0.069354248046875
			 train-loss:  2.0882916288860773 	 ± 0.25892877860398317
	data : 0.11689624786376954
	model : 0.06949758529663086
			 train-loss:  2.091757376542252 	 ± 0.2622851102884762
	data : 0.11673173904418946
	model : 0.07010173797607422
			 train-loss:  2.0909250734904625 	 ± 0.26178705777163236
	data : 0.11615586280822754
	model : 0.0700270652770996
			 train-loss:  2.088270992040634 	 ± 0.2634627682634151
	data : 0.11623282432556152
	model : 0.06934518814086914
			 train-loss:  2.087187811814619 	 ± 0.2631355646023074
	data : 0.11693620681762695
	model : 0.06985578536987305
			 train-loss:  2.0873124298158583 	 ± 0.26241702474576456
	data : 0.11634392738342285
	model : 0.07003693580627442
			 train-loss:  2.0855439367190085 	 ± 0.2627843475989947
	data : 0.11621665954589844
	model : 0.06920256614685058
			 train-loss:  2.0834250029014503 	 ± 0.2636322423990353
	data : 0.11711411476135254
	model : 0.06930570602416992
			 train-loss:  2.0837852729333415 	 ± 0.26296417085501333
	data : 0.11702876091003418
	model : 0.06997079849243164
			 train-loss:  2.0852184302063397 	 ± 0.26297976915164084
	data : 0.11635079383850097
	model : 0.06930384635925294
			 train-loss:  2.0841685093660405 	 ± 0.26266625643149505
	data : 0.11690902709960938
	model : 0.0690502643585205
			 train-loss:  2.0840129662067333 	 ± 0.2619753794758413
	data : 0.11712307929992676
	model : 0.07013168334960937
			 train-loss:  2.0833294126722546 	 ± 0.2614494484271696
	data : 0.11607489585876465
	model : 0.07004079818725586
			 train-loss:  2.0828441117939196 	 ± 0.26084585360412527
	data : 0.11612114906311036
	model : 0.06998043060302735
			 train-loss:  2.0819208721839946 	 ± 0.2604731782913005
	data : 0.11629533767700195
	model : 0.07016077041625976
			 train-loss:  2.084033622095982 	 ± 0.26142968387995064
	data : 0.11616783142089844
	model : 0.07008852958679199
			 train-loss:  2.0831384640283535 	 ± 0.2610463734416203
	data : 0.11606674194335938
	model : 0.07000079154968261
			 train-loss:  2.080803958411069 	 ± 0.26238479186633934
	data : 0.11626439094543457
	model : 0.07016549110412598
			 train-loss:  2.0830506385901035 	 ± 0.2635753263494461
	data : 0.11616559028625488
	model : 0.06989269256591797
			 train-loss:  2.082423344558599 	 ± 0.2630479727987598
	data : 0.11620216369628907
	model : 0.06985859870910645
			 train-loss:  2.083500684820456 	 ± 0.2628126442481877
	data : 0.11643586158752442
	model : 0.06985468864440918
			 train-loss:  2.0852096821322585 	 ± 0.2632432667427082
	data : 0.11649098396301269
	model : 0.0695622444152832
			 train-loss:  2.084590030075917 	 ± 0.2627257449579258
	data : 0.11647582054138184
	model : 0.06917352676391601
			 train-loss:  2.0836253118515016 	 ± 0.2624212235083699
	data : 0.116790771484375
	model : 0.06873106956481934
			 train-loss:  2.0868542621384805 	 ± 0.2657207389407646
	data : 0.11716117858886718
	model : 0.06870741844177246
			 train-loss:  2.0857122971279787 	 ± 0.2655561896664789
	data : 0.11705670356750489
	model : 0.0687636375427246
			 train-loss:  2.0843761472279216 	 ± 0.26558111787004457
	data : 0.11717414855957031
	model : 0.06891021728515626
			 train-loss:  2.0845559356259367 	 ± 0.26494176774499495
	data : 0.11712665557861328
	model : 0.0682060718536377
			 train-loss:  2.083415266944141 	 ± 0.2647964489260905
	data : 0.11764116287231445
	model : 0.06878252029418945
			 train-loss:  2.0839144981023177 	 ± 0.26424964959759983
	data : 0.11714010238647461
	model : 0.06876025199890137
			 train-loss:  2.0829950277356133 	 ± 0.26394071735409713
	data : 0.11708788871765137
	model : 0.06907553672790527
			 train-loss:  2.0820863596521892 	 ± 0.2636298372815685
	data : 0.11669425964355469
	model : 0.06920948028564453
			 train-loss:  2.084343054077842 	 ± 0.2650045796155542
	data : 0.1167233943939209
	model : 0.07015695571899414
			 train-loss:  2.0855016430219013 	 ± 0.2649029209360773
	data : 0.11596603393554687
	model : 0.07006731033325195
			 train-loss:  2.085444479192038 	 ± 0.2642757416718309
	data : 0.1158027172088623
	model : 0.07016191482543946
			 train-loss:  2.0837572976103367 	 ± 0.2647883202564564
	data : 0.11575098037719726
	model : 0.06999907493591309
			 train-loss:  2.0842416057004614 	 ± 0.264260121339161
	data : 0.11593108177185059
	model : 0.06909828186035157
			 train-loss:  2.085560033811587 	 ± 0.26434321477587536
	data : 0.11655120849609375
	model : 0.06902842521667481
			 train-loss:  2.0858108160107633 	 ± 0.2637532619000247
	data : 0.11675243377685547
	model : 0.06914305686950684
			 train-loss:  2.087005917120863 	 ± 0.2637248503356978
	data : 0.11679415702819824
	model : 0.06852178573608399
			 train-loss:  2.087840910331445 	 ± 0.26340251375360363
	data : 0.11727705001831054
	model : 0.06809659004211426
			 train-loss:  2.088767454711669 	 ± 0.2631518847524996
	data : 0.11762700080871583
	model : 0.0688514232635498
			 train-loss:  2.088571520700847 	 ± 0.26256633144743385
	data : 0.11691131591796874
	model : 0.0681694507598877
			 train-loss:  2.0886210815473034 	 ± 0.2619699368177775
	data : 0.11732559204101563
	model : 0.06734113693237305
			 train-loss:  2.0876905589082124 	 ± 0.2617407202985772
	data : 0.11825013160705566
	model : 0.06768665313720704
			 train-loss:  2.0878669931007936 	 ± 0.26116372003073773
	data : 0.11808009147644043
	model : 0.06780929565429687
			 train-loss:  2.0859520232196345 	 ± 0.2621349429360863
	data : 0.11780838966369629
	model : 0.06829323768615722
			 train-loss:  2.088965132832527 	 ± 0.2653913056515293
	data : 0.11744070053100586
	model : 0.06905665397644042
			 train-loss:  2.0899783092074924 	 ± 0.26523471395678505
	data : 0.11691141128540039
	model : 0.07012219429016113
			 train-loss:  2.0914349017945013 	 ± 0.2655476353916782
	data : 0.11564764976501465
	model : 0.07063608169555664
			 train-loss:  2.0942392559303586 	 ± 0.26829509940800406
	data : 0.11523294448852539
	model : 0.0704472541809082
			 train-loss:  2.0932177843754753 	 ± 0.26814809609108786
	data : 0.11553335189819336
	model : 0.06996674537658691
			 train-loss:  2.092590398663517 	 ± 0.2677296332315656
	data : 0.11587700843811036
	model : 0.06947312355041504
			 train-loss:  2.0904789629189864 	 ± 0.26905097444646653
	data : 0.11615080833435058
	model : 0.06883249282836915
			 train-loss:  2.0897012169742997 	 ± 0.26872696450285144
	data : 0.11683349609375
	model : 0.0690220832824707
			 train-loss:  2.090839922428131 	 ± 0.2687051164435963
	data : 0.11666064262390137
	model : 0.06925220489501953
			 train-loss:  2.091574482651739 	 ± 0.26836121208449876
	data : 0.11658411026000977
	model : 0.06962451934814454
			 train-loss:  2.0917742731224775 	 ± 0.2678045416217919
	data : 0.1164928913116455
	model : 0.07007393836975098
			 train-loss:  2.091029248846338 	 ± 0.2674770428164655
	data : 0.1161118984222412
	model : 0.07082428932189941
			 train-loss:  2.0904083822743367 	 ± 0.26707939404000663
	data : 0.11550025939941407
	model : 0.07069988250732422
			 train-loss:  2.090058922767639 	 ± 0.2665694039409603
	data : 0.11547808647155762
	model : 0.07087440490722656
			 train-loss:  2.0888477578884412 	 ± 0.2666614688676455
	data : 0.11529712677001953
	model : 0.07081027030944824
			 train-loss:  2.0891957746888803 	 ± 0.26615717176661535
	data : 0.1151167869567871
	model : 0.0707883358001709
			 train-loss:  2.0900178417563438 	 ± 0.26590597914440817
	data : 0.11502394676208497
	model : 0.07058596611022949
			 train-loss:  2.089636724024887 	 ± 0.26541941162853644
	data : 0.1150209903717041
	model : 0.06944446563720703
			 train-loss:  2.0889160559197104 	 ± 0.26510662989183503
	data : 0.1159278392791748
	model : 0.0686159610748291
			 train-loss:  2.089318032617922 	 ± 0.2646344733086839
	data : 0.1164924144744873
	model : 0.06743183135986328
			 train-loss:  2.08961251671197 	 ± 0.264131527245208
	data : 0.11748719215393066
	model : 0.06683902740478516
			 train-loss:  2.0902101073946273 	 ± 0.2637571660799183
	data : 0.11818051338195801
	model : 0.0664456844329834
			 train-loss:  2.0906548485523317 	 ± 0.2633125637925277
	data : 0.1187060832977295
	model : 0.06678333282470703
			 train-loss:  2.0899595857149196 	 ± 0.2630051669419364
	data : 0.11852397918701171
	model : 0.06739573478698731
			 train-loss:  2.0912890645765487 	 ± 0.2633047200295047
	data : 0.11801419258117676
	model : 0.06760425567626953
			 train-loss:  2.0931286898004005 	 ± 0.26436760611254295
	data : 0.11800336837768555
	model : 0.0678290843963623
			 train-loss:  2.092028336048126 	 ± 0.26440906575548
	data : 0.11779537200927734
	model : 0.06796588897705078
			 train-loss:  2.0920408128267267 	 ± 0.2638819025490101
	data : 0.1177495002746582
	model : 0.06827163696289062
			 train-loss:  2.09177460982686 	 ± 0.2633915741335862
	data : 0.11762590408325195
	model : 0.06818313598632812
			 train-loss:  2.0910483895554375 	 ± 0.2631231938109605
	data : 0.1177304744720459
	model : 0.06892905235290528
			 train-loss:  2.0926963297400887 	 ± 0.26390967181898334
	data : 0.11698679924011231
	model : 0.06836657524108887
			 train-loss:  2.0907241886737298 	 ± 0.26526039412427166
	data : 0.11739721298217773
	model : 0.06823034286499023
			 train-loss:  2.0889244228601456 	 ± 0.26629721356690045
	data : 0.11626672744750977
	model : 0.059433603286743165
#epoch  71    val-loss:  2.433909642068963  train-loss:  2.0889244228601456  lr:  9.765625e-06
			 train-loss:  2.1194183826446533 	 ± 0.0
	data : 5.589956760406494
	model : 0.07219362258911133
			 train-loss:  2.1857742071151733 	 ± 0.06635582447052002
	data : 2.8635278940200806
	model : 0.07332444190979004
			 train-loss:  2.123264789581299 	 ± 0.10368341970451925
	data : 1.9484307765960693
	model : 0.07216286659240723
			 train-loss:  2.1713618636131287 	 ± 0.12248540416101658
	data : 1.4902481436729431
	model : 0.07165145874023438
			 train-loss:  2.1884693622589113 	 ± 0.11477284271835328
	data : 1.215372085571289
	model : 0.07111215591430664
			 train-loss:  2.145729045073191 	 ± 0.1418132977558507
	data : 0.12067451477050781
	model : 0.07059655189514161
			 train-loss:  2.0841801166534424 	 ± 0.199919066941154
	data : 0.11641745567321778
	model : 0.06890277862548828
			 train-loss:  2.0706168562173843 	 ± 0.19041904683194977
	data : 0.11661353111267089
	model : 0.06820921897888184
			 train-loss:  2.065617971950107 	 ± 0.18008470350648753
	data : 0.11728324890136718
	model : 0.06814169883728027
			 train-loss:  2.0470993757247924 	 ± 0.17964936974207205
	data : 0.11734857559204101
	model : 0.06829838752746582
			 train-loss:  2.043282985687256 	 ± 0.17171358079087928
	data : 0.11713428497314453
	model : 0.06841297149658203
			 train-loss:  2.050403356552124 	 ± 0.16609070063113981
	data : 0.11700053215026855
	model : 0.06920552253723145
			 train-loss:  2.0596818923950195 	 ± 0.16277962486045283
	data : 0.11637582778930664
	model : 0.06903572082519531
			 train-loss:  2.0500223551477705 	 ± 0.1606783596406781
	data : 0.11658382415771484
	model : 0.06820411682128906
			 train-loss:  2.077720808982849 	 ± 0.1866473334062225
	data : 0.11752123832702636
	model : 0.06732568740844727
			 train-loss:  2.0821014270186424 	 ± 0.18151514424851167
	data : 0.11850924491882324
	model : 0.06755609512329101
			 train-loss:  2.0652041365118587 	 ± 0.18862115368002824
	data : 0.11827211380004883
	model : 0.06664929389953614
			 train-loss:  2.0843464732170105 	 ± 0.19957625445374708
	data : 0.11906943321228028
	model : 0.06753277778625488
			 train-loss:  2.089119942564713 	 ± 0.19530611323064448
	data : 0.1182446002960205
	model : 0.06849226951599122
			 train-loss:  2.0913302481174467 	 ± 0.1906045055450965
	data : 0.11721200942993164
	model : 0.06997947692871094
			 train-loss:  2.089176308541071 	 ± 0.186260202286921
	data : 0.1158017635345459
	model : 0.06876835823059083
			 train-loss:  2.1013035503300754 	 ± 0.19027449776933597
	data : 0.11696367263793946
	model : 0.06962757110595703
			 train-loss:  2.1101749306139737 	 ± 0.19068747039728187
	data : 0.11620068550109863
	model : 0.06936516761779785
			 train-loss:  2.1043406228224435 	 ± 0.1887578884442218
	data : 0.11633367538452148
	model : 0.06909360885620117
			 train-loss:  2.0953787851333616 	 ± 0.19008394899700284
	data : 0.11651153564453125
	model : 0.0685349941253662
			 train-loss:  2.0890797789280233 	 ± 0.18903480073592693
	data : 0.11709361076354981
	model : 0.06959729194641114
			 train-loss:  2.0862037031738847 	 ± 0.18607991761457096
	data : 0.11627936363220215
	model : 0.06903157234191895
			 train-loss:  2.08521129829543 	 ± 0.1827995999479613
	data : 0.11660380363464355
	model : 0.0692056655883789
			 train-loss:  2.1016334213059524 	 ± 0.19953606169450394
	data : 0.1165247917175293
	model : 0.06939873695373536
			 train-loss:  2.0968766450881957 	 ± 0.19784758257875282
	data : 0.11633810997009278
	model : 0.06924858093261718
			 train-loss:  2.0899304151535034 	 ± 0.19831406995063255
	data : 0.11655993461608886
	model : 0.06915531158447266
			 train-loss:  2.08415050804615 	 ± 0.19782588999801307
	data : 0.11658868789672852
	model : 0.06954636573791503
			 train-loss:  2.0708103360551777 	 ± 0.20891121845146138
	data : 0.11638026237487793
	model : 0.0696345329284668
			 train-loss:  2.066246790044448 	 ± 0.2074789376830053
	data : 0.11630463600158691
	model : 0.06874985694885254
			 train-loss:  2.0548597097396852 	 ± 0.21500281641027053
	data : 0.11716327667236329
	model : 0.06900734901428222
			 train-loss:  2.0573975212044187 	 ± 0.21252662584241774
	data : 0.11690301895141601
	model : 0.06901803016662597
			 train-loss:  2.0636470865558936 	 ± 0.21296214701490737
	data : 0.11693181991577148
	model : 0.0690093994140625
			 train-loss:  2.0612899912031075 	 ± 0.21062988289253987
	data : 0.1170236587524414
	model : 0.06884646415710449
			 train-loss:  2.060575720591423 	 ± 0.2079585818004242
	data : 0.11717896461486817
	model : 0.06968512535095214
			 train-loss:  2.0604021817445757 	 ± 0.20534550631644188
	data : 0.11629691123962402
	model : 0.06959695816040039
			 train-loss:  2.0628437850533463 	 ± 0.2034128217647833
	data : 0.11629066467285157
	model : 0.06952252388000488
			 train-loss:  2.057869633038839 	 ± 0.2034847523584762
	data : 0.1162447452545166
	model : 0.0694425106048584
			 train-loss:  2.063021715297255 	 ± 0.20385769336324533
	data : 0.11606521606445312
	model : 0.06954517364501953
			 train-loss:  2.0641051205721768 	 ± 0.2016529991160576
	data : 0.11596932411193847
	model : 0.06950626373291016
			 train-loss:  2.0616120629840426 	 ± 0.20008439196514516
	data : 0.11617231369018555
	model : 0.06925244331359863
			 train-loss:  2.0554388398709507 	 ± 0.20218396438602626
	data : 0.11636843681335449
	model : 0.06909914016723633
			 train-loss:  2.058034881632379 	 ± 0.20079496288795542
	data : 0.11657028198242188
	model : 0.06962165832519532
			 train-loss:  2.0675168931484222 	 ± 0.2090558476185223
	data : 0.11618075370788575
	model : 0.0693469524383545
			 train-loss:  2.0692399132008457 	 ± 0.20725569781004077
	data : 0.1164886474609375
	model : 0.06976613998413086
			 train-loss:  2.076561632156372 	 ± 0.2114771775959631
	data : 0.11607465744018555
	model : 0.07000942230224609
			 train-loss:  2.0839708365646064 	 ± 0.21584832286218725
	data : 0.11592011451721192
	model : 0.07027955055236816
			 train-loss:  2.0862661462563734 	 ± 0.21439034057856768
	data : 0.11571650505065918
	model : 0.0701219081878662
			 train-loss:  2.0864781388696634 	 ± 0.21236366103388818
	data : 0.11590256690979003
	model : 0.06991100311279297
			 train-loss:  2.0918295515908136 	 ± 0.2139648665779877
	data : 0.11602296829223632
	model : 0.06862845420837402
			 train-loss:  2.0845609686591406 	 ± 0.21863560333762994
	data : 0.11720356941223145
	model : 0.06882100105285645
			 train-loss:  2.080906108021736 	 ± 0.2183635012882917
	data : 0.11710758209228515
	model : 0.06865425109863281
			 train-loss:  2.0752154421388056 	 ± 0.22058913731981553
	data : 0.11720395088195801
	model : 0.06765646934509277
			 train-loss:  2.0744703946442438 	 ± 0.21875157119878463
	data : 0.1180203914642334
	model : 0.06804947853088379
			 train-loss:  2.0714653791007347 	 ± 0.21809388273636582
	data : 0.1176290512084961
	model : 0.06800546646118164
			 train-loss:  2.072897775967916 	 ± 0.21654848543739136
	data : 0.11752514839172364
	model : 0.06756362915039063
			 train-loss:  2.066329809485889 	 ± 0.2207097547031549
	data : 0.11778526306152344
	model : 0.06762280464172363
			 train-loss:  2.0620305384359052 	 ± 0.2214827604830277
	data : 0.11775898933410645
	model : 0.06856279373168946
			 train-loss:  2.058622751917158 	 ± 0.2213503446862237
	data : 0.11706399917602539
	model : 0.06867771148681641
			 train-loss:  2.0638803374022245 	 ± 0.22354389256978552
	data : 0.1169804573059082
	model : 0.06959056854248047
			 train-loss:  2.069520196547875 	 ± 0.22635986082262466
	data : 0.11614327430725098
	model : 0.06987228393554687
			 train-loss:  2.074258177569418 	 ± 0.2278630981766953
	data : 0.11583828926086426
	model : 0.06993932723999023
			 train-loss:  2.0731052729620862 	 ± 0.22635010240747688
	data : 0.11563587188720703
	model : 0.07008461952209473
			 train-loss:  2.076243097291273 	 ± 0.22614287594547586
	data : 0.11562328338623047
	model : 0.07010831832885742
			 train-loss:  2.076598663260971 	 ± 0.22451732509275826
	data : 0.11561093330383301
	model : 0.07016716003417969
			 train-loss:  2.073704367024558 	 ± 0.22420063372252055
	data : 0.11572103500366211
	model : 0.07015633583068848
			 train-loss:  2.069769646080447 	 ± 0.2250370978801106
	data : 0.11571574211120605
	model : 0.06988534927368165
			 train-loss:  2.0719110717376075 	 ± 0.22419617194422042
	data : 0.11613945960998535
	model : 0.06963257789611817
			 train-loss:  2.0820135988601267 	 ± 0.2385870334360053
	data : 0.11621279716491699
	model : 0.06957411766052246
			 train-loss:  2.083935671561473 	 ± 0.2375378304350891
	data : 0.11617469787597656
	model : 0.06951923370361328
			 train-loss:  2.0804272222518922 	 ± 0.2378713517175259
	data : 0.11594347953796387
	model : 0.06856317520141601
			 train-loss:  2.0749971129392324 	 ± 0.24093510794661024
	data : 0.11690545082092285
	model : 0.0678621768951416
			 train-loss:  2.073286157149773 	 ± 0.2398297600192309
	data : 0.11745882034301758
	model : 0.06705083847045898
			 train-loss:  2.073160558174818 	 ± 0.2382899792152512
	data : 0.11812224388122558
	model : 0.0672238826751709
			 train-loss:  2.0756147677385353 	 ± 0.2377670257472244
	data : 0.11818904876708984
	model : 0.06721277236938476
			 train-loss:  2.073698323965073 	 ± 0.2368895139790562
	data : 0.11836447715759277
	model : 0.06802797317504883
			 train-loss:  2.073856821766606 	 ± 0.23542696003252656
	data : 0.11755776405334473
	model : 0.06883912086486817
			 train-loss:  2.0729999164255655 	 ± 0.2341140869217269
	data : 0.1169240951538086
	model : 0.06974735260009765
			 train-loss:  2.083039743354522 	 ± 0.24982893675534856
	data : 0.11616668701171876
	model : 0.06965956687927247
			 train-loss:  2.08400726602191 	 ± 0.24849379110107228
	data : 0.11610093116760253
	model : 0.0697214126586914
			 train-loss:  2.084707851970897 	 ± 0.2471111741640686
	data : 0.11604847908020019
	model : 0.06890230178833008
			 train-loss:  2.083682507969612 	 ± 0.24585208916843396
	data : 0.1167412281036377
	model : 0.06904072761535644
			 train-loss:  2.0828229928838797 	 ± 0.24456498858806097
	data : 0.11658635139465331
	model : 0.06906571388244628
			 train-loss:  2.0850255394523796 	 ± 0.24403771600555219
	data : 0.116534423828125
	model : 0.06910915374755859
			 train-loss:  2.0851880338754545 	 ± 0.24266763233924274
	data : 0.11659932136535645
	model : 0.06922516822814942
			 train-loss:  2.0878692428270975 	 ± 0.24263776250090952
	data : 0.11651325225830078
	model : 0.06925783157348633
			 train-loss:  2.0922139280444974 	 ± 0.2447958187840488
	data : 0.11648855209350586
	model : 0.06926050186157226
			 train-loss:  2.0962813094906183 	 ± 0.2465341784258503
	data : 0.11637473106384277
	model : 0.06842684745788574
			 train-loss:  2.0924492843689455 	 ± 0.24794460819164116
	data : 0.11706852912902832
	model : 0.06842870712280273
			 train-loss:  2.092997176850096 	 ± 0.2466788203725627
	data : 0.11701135635375977
	model : 0.06824316978454589
			 train-loss:  2.09358503567545 	 ± 0.2454432598391507
	data : 0.11714072227478027
	model : 0.068310546875
			 train-loss:  2.0912513472139835 	 ± 0.24521877688992275
	data : 0.11721324920654297
	model : 0.06827788352966309
			 train-loss:  2.088781904928463 	 ± 0.2451484252258219
	data : 0.11732273101806641
	model : 0.06899113655090332
			 train-loss:  2.0901853308385734 	 ± 0.2442858155019162
	data : 0.11682782173156739
	model : 0.06890029907226562
			 train-loss:  2.0907311319100734 	 ± 0.24310896797098833
	data : 0.11680440902709961
	model : 0.06897053718566895
			 train-loss:  2.0898543906211855 	 ± 0.24204761816286552
	data : 0.11689987182617187
	model : 0.06977934837341308
			 train-loss:  2.0901686814751956 	 ± 0.24086688758688027
	data : 0.11616110801696777
	model : 0.06960644721984863
			 train-loss:  2.0928115938224043 	 ± 0.2411504683139339
	data : 0.1162872314453125
	model : 0.06965737342834473
			 train-loss:  2.0945187008496626 	 ± 0.2405955125934708
	data : 0.11609559059143067
	model : 0.06910691261291504
			 train-loss:  2.0948720964101644 	 ± 0.2394628699906044
	data : 0.1166757583618164
	model : 0.06923499107360839
			 train-loss:  2.097254249027797 	 ± 0.2395548185553361
	data : 0.11650433540344238
	model : 0.06926121711730956
			 train-loss:  2.096907464963085 	 ± 0.2384486446336991
	data : 0.11652550697326661
	model : 0.06942338943481445
			 train-loss:  2.1035543557639436 	 ± 0.2470011698454551
	data : 0.11633195877075195
	model : 0.06940283775329589
			 train-loss:  2.1076200052543923 	 ± 0.24942600410597887
	data : 0.11635470390319824
	model : 0.06991610527038575
			 train-loss:  2.106841629798259 	 ± 0.24841095144287717
	data : 0.11582555770874023
	model : 0.06906213760375976
			 train-loss:  2.1084766387939453 	 ± 0.24786771411918826
	data : 0.11653652191162109
	model : 0.06822199821472168
			 train-loss:  2.1107686210323022 	 ± 0.247916830114696
	data : 0.11704630851745605
	model : 0.06815614700317382
			 train-loss:  2.1158795803785324 	 ± 0.252613363784859
	data : 0.11732416152954102
	model : 0.06725144386291504
			 train-loss:  2.1151122941380054 	 ± 0.2516241796350814
	data : 0.11814589500427246
	model : 0.06754989624023437
			 train-loss:  2.115401399763007 	 ± 0.2505369836993302
	data : 0.11786651611328125
	model : 0.06818790435791015
			 train-loss:  2.118806565326193 	 ± 0.2520809633978679
	data : 0.1172450065612793
	model : 0.06892647743225097
			 train-loss:  2.1158905810323256 	 ± 0.25293250662866745
	data : 0.11664204597473145
	model : 0.06903014183044434
			 train-loss:  2.1150648553147273 	 ± 0.2520062514270065
	data : 0.11650319099426269
	model : 0.07015185356140137
			 train-loss:  2.1130853426658502 	 ± 0.2518480009721964
	data : 0.1157599925994873
	model : 0.06977705955505371
			 train-loss:  2.1090805600671207 	 ± 0.2545327671903002
	data : 0.116050386428833
	model : 0.0697709560394287
			 train-loss:  2.1074258287747702 	 ± 0.25411193721686554
	data : 0.11618180274963379
	model : 0.06895227432250976
			 train-loss:  2.116449706810565 	 ± 0.2716815442300194
	data : 0.11703038215637207
	model : 0.0689152717590332
			 train-loss:  2.11530160415368 	 ± 0.2708603871350259
	data : 0.117026948928833
	model : 0.06925287246704101
			 train-loss:  2.116154819000058 	 ± 0.2699216480468885
	data : 0.11674957275390625
	model : 0.06853728294372559
			 train-loss:  2.118681583673723 	 ± 0.270287684521937
	data : 0.11750411987304688
	model : 0.06863136291503906
			 train-loss:  2.116664988517761 	 ± 0.27013932385318223
	data : 0.11745877265930176
	model : 0.06969342231750489
			 train-loss:  2.115409195423126 	 ± 0.2694312762914972
	data : 0.11651611328125
	model : 0.06986765861511231
			 train-loss:  2.1130990231131004 	 ± 0.26961836260417926
	data : 0.11649003028869628
	model : 0.06890206336975098
			 train-loss:  2.1119222501292825 	 ± 0.2688903271214786
	data : 0.11730527877807617
	model : 0.0697594165802002
			 train-loss:  2.1104095305583273 	 ± 0.26839231054786006
	data : 0.11651930809020997
	model : 0.0697854995727539
			 train-loss:  2.108758344100072 	 ± 0.26801497924542617
	data : 0.1164163589477539
	model : 0.06966757774353027
			 train-loss:  2.110422022470081 	 ± 0.26766305375112115
	data : 0.11634688377380371
	model : 0.06961321830749512
			 train-loss:  2.108753637834029 	 ± 0.26733012537427897
	data : 0.11620759963989258
	model : 0.06917462348937989
			 train-loss:  2.1081612558293163 	 ± 0.2664101782939945
	data : 0.11647272109985352
	model : 0.06908464431762695
			 train-loss:  2.112297396161663 	 ± 0.2696665383101396
	data : 0.11658430099487305
	model : 0.06879658699035644
			 train-loss:  2.1095772257557623 	 ± 0.2705048700674921
	data : 0.11675314903259278
	model : 0.06880226135253906
			 train-loss:  2.111529877080637 	 ± 0.2704617961646874
	data : 0.11691603660583497
	model : 0.06865582466125489
			 train-loss:  2.1090697737505835 	 ± 0.27099581429035907
	data : 0.11697297096252442
	model : 0.06958727836608887
			 train-loss:  2.107157486072485 	 ± 0.2709382850268959
	data : 0.11617937088012695
	model : 0.06935863494873047
			 train-loss:  2.104433795531019 	 ± 0.2718514190413347
	data : 0.11642036437988282
	model : 0.06973733901977539
			 train-loss:  2.1010907164641788 	 ± 0.27373126012633103
	data : 0.11612391471862793
	model : 0.06998076438903808
			 train-loss:  2.0983644309618796 	 ± 0.2746597192731945
	data : 0.1159712791442871
	model : 0.06950898170471191
			 train-loss:  2.0983489466385103 	 ± 0.27369096066322335
	data : 0.11652512550354004
	model : 0.06953291893005371
			 train-loss:  2.0991212254637603 	 ± 0.27288753993065545
	data : 0.11640005111694336
	model : 0.0697136402130127
			 train-loss:  2.098099522292614 	 ± 0.2722126884195026
	data : 0.11637377738952637
	model : 0.06963958740234374
			 train-loss:  2.0999646211492604 	 ± 0.27219410811560846
	data : 0.11647076606750488
	model : 0.06935286521911621
			 train-loss:  2.09951445256194 	 ± 0.27131449235410476
	data : 0.11660776138305665
	model : 0.06993398666381836
			 train-loss:  2.097557167617642 	 ± 0.2714223934899335
	data : 0.11599926948547364
	model : 0.06906075477600097
			 train-loss:  2.0987804813964948 	 ± 0.2709101873984895
	data : 0.11691269874572754
	model : 0.0693115234375
			 train-loss:  2.0998765070166363 	 ± 0.270328599939504
	data : 0.11673316955566407
	model : 0.06924610137939453
			 train-loss:  2.1027788424491884 	 ± 0.27174524348939616
	data : 0.11669859886169434
	model : 0.06899280548095703
			 train-loss:  2.1044179737962634 	 ± 0.27158690542557085
	data : 0.11688642501831055
	model : 0.06868162155151367
			 train-loss:  2.104086832780587 	 ± 0.27072263571376154
	data : 0.11726140975952148
	model : 0.0694793701171875
			 train-loss:  2.1057460222368927 	 ± 0.2706107227635016
	data : 0.11646218299865722
	model : 0.0694272518157959
			 train-loss:  2.105280130714565 	 ± 0.2697922388341794
	data : 0.11644148826599121
	model : 0.06944704055786133
			 train-loss:  2.1049296540598714 	 ± 0.2689557017567955
	data : 0.11638932228088379
	model : 0.06937594413757324
			 train-loss:  2.1030972386017823 	 ± 0.26906118337037577
	data : 0.11650357246398926
	model : 0.0692835807800293
			 train-loss:  2.1072447072168825 	 ± 0.2731597444461852
	data : 0.11653118133544922
	model : 0.06924328804016114
			 train-loss:  2.1083618855174584 	 ± 0.2726535181860928
	data : 0.11649031639099121
	model : 0.069236421585083
			 train-loss:  2.107674587447688 	 ± 0.2719320319588013
	data : 0.11648902893066407
	model : 0.0689056396484375
			 train-loss:  2.107098985463381 	 ± 0.27117806064126865
	data : 0.11714253425598145
	model : 0.06925702095031738
			 train-loss:  2.105411990088706 	 ± 0.27117547493428557
	data : 0.11672501564025879
	model : 0.06966094970703125
			 train-loss:  2.108153145990254 	 ± 0.2725655060177616
	data : 0.11642441749572754
	model : 0.0696303367614746
			 train-loss:  2.1045710082434437 	 ± 0.2755266109849786
	data : 0.11641011238098145
	model : 0.06962676048278808
			 train-loss:  2.10246570008557 	 ± 0.27599725742359826
	data : 0.11629223823547363
	model : 0.0700674057006836
			 train-loss:  2.103546896125331 	 ± 0.27550777860692627
	data : 0.1159134864807129
	model : 0.0700228214263916
			 train-loss:  2.1024245533598473 	 ± 0.2750547629532225
	data : 0.11593942642211914
	model : 0.07000408172607422
			 train-loss:  2.1014413141204926 	 ± 0.2745224580932982
	data : 0.11585464477539062
	model : 0.0702125072479248
			 train-loss:  2.1016870701596853 	 ± 0.2737226321461751
	data : 0.11567726135253906
	model : 0.07005615234375
			 train-loss:  2.1044365407447136 	 ± 0.2752285458833376
	data : 0.11575183868408204
	model : 0.06977944374084473
			 train-loss:  2.1057334920939277 	 ± 0.2749353229416881
	data : 0.11580162048339844
	model : 0.06978435516357422
			 train-loss:  2.103250567676031 	 ± 0.27603518555298945
	data : 0.11584787368774414
	model : 0.06972541809082031
			 train-loss:  2.102899502876193 	 ± 0.2752698715160968
	data : 0.11595640182495118
	model : 0.06960554122924804
			 train-loss:  2.1031231480526786 	 ± 0.27448881214819004
	data : 0.11621036529541015
	model : 0.06880974769592285
			 train-loss:  2.101198185449359 	 ± 0.2748675028023434
	data : 0.11696329116821289
	model : 0.06912131309509277
			 train-loss:  2.1009998376028878 	 ± 0.2740935297378744
	data : 0.11681780815124512
	model : 0.06898036003112792
			 train-loss:  2.098269669847055 	 ± 0.2756897194141915
	data : 0.11692485809326172
	model : 0.06932272911071777
			 train-loss:  2.097075283190625 	 ± 0.2753661019251954
	data : 0.11639189720153809
	model : 0.0686424732208252
			 train-loss:  2.095405437973108 	 ± 0.2754887336572087
	data : 0.11711387634277344
	model : 0.0695878028869629
			 train-loss:  2.0957441329956055 	 ± 0.2747552955944117
	data : 0.1162841796875
	model : 0.06947560310363769
			 train-loss:  2.0943590362866717 	 ± 0.2746169899539834
	data : 0.11626849174499512
	model : 0.06952953338623047
			 train-loss:  2.094639256514238 	 ± 0.2738831333465046
	data : 0.11638107299804687
	model : 0.06936850547790527
			 train-loss:  2.0959770378175673 	 ± 0.27372202193461215
	data : 0.11663904190063476
	model : 0.07049174308776855
			 train-loss:  2.0962330565426517 	 ± 0.2729949728763044
	data : 0.11554374694824218
	model : 0.07044744491577148
			 train-loss:  2.0968140778334243 	 ± 0.2723655620860998
	data : 0.11552267074584961
	model : 0.07045540809631348
			 train-loss:  2.0957604356714197 	 ± 0.2720041911381081
	data : 0.1154557228088379
	model : 0.0707120418548584
			 train-loss:  2.095835378093104 	 ± 0.27127392673385425
	data : 0.11539039611816407
	model : 0.07064852714538575
			 train-loss:  2.0954067643313485 	 ± 0.2706107654375699
	data : 0.11538863182067871
	model : 0.06926774978637695
			 train-loss:  2.09636438273369 	 ± 0.27020760402812544
	data : 0.11649608612060547
	model : 0.06919455528259277
			 train-loss:  2.0955342470653475 	 ± 0.26973208401919346
	data : 0.11664447784423829
	model : 0.06839776039123535
			 train-loss:  2.0938383623173364 	 ± 0.27002970715513774
	data : 0.11748228073120118
	model : 0.06834945678710938
			 train-loss:  2.094898647662857 	 ± 0.2697181534728968
	data : 0.11749076843261719
	model : 0.06804933547973632
			 train-loss:  2.093576541170478 	 ± 0.2696346574861866
	data : 0.11791524887084961
	model : 0.06902141571044922
			 train-loss:  2.092602774269223 	 ± 0.2692734826166188
	data : 0.11694631576538086
	model : 0.06925439834594727
			 train-loss:  2.090785844424336 	 ± 0.2697621043855985
	data : 0.11687893867492676
	model : 0.06927881240844727
			 train-loss:  2.090051514674456 	 ± 0.26926384440089784
	data : 0.11672563552856445
	model : 0.06916718482971192
			 train-loss:  2.088480453710167 	 ± 0.2694706104435979
	data : 0.11656889915466309
	model : 0.06917953491210938
			 train-loss:  2.08799945763525 	 ± 0.2688701449460688
	data : 0.11685776710510254
	model : 0.0691995620727539
			 train-loss:  2.0881020131737293 	 ± 0.2681941833363583
	data : 0.11695823669433594
	model : 0.0685349464416504
			 train-loss:  2.087650669280009 	 ± 0.26759485603397254
	data : 0.11736521720886231
	model : 0.06926069259643555
			 train-loss:  2.089094280004501 	 ± 0.26770074769005264
	data : 0.1167640209197998
	model : 0.06927075386047363
			 train-loss:  2.090663626419371 	 ± 0.2679547054542625
	data : 0.11676192283630371
	model : 0.06891756057739258
			 train-loss:  2.0901160871628486 	 ± 0.26740332765827735
	data : 0.11691098213195801
	model : 0.06891446113586426
			 train-loss:  2.0938260314499804 	 ± 0.2719054343464485
	data : 0.11697597503662109
	model : 0.07061247825622559
			 train-loss:  2.0940046082524693 	 ± 0.2712501139089241
	data : 0.1152583122253418
	model : 0.07176756858825684
			 train-loss:  2.0927886951260453 	 ± 0.2711444571779274
	data : 0.11420984268188476
	model : 0.07157478332519532
			 train-loss:  2.092438564717191 	 ± 0.270531990693853
	data : 0.11444487571716308
	model : 0.0721280574798584
			 train-loss:  2.0910394773391134 	 ± 0.270623777000388
	data : 0.11383094787597656
	model : 0.07189879417419434
			 train-loss:  2.089046221513015 	 ± 0.2714913476635624
	data : 0.11417884826660156
	model : 0.07068791389465331
			 train-loss:  2.0882828435259 	 ± 0.27106474374547246
	data : 0.11544589996337891
	model : 0.06948022842407227
			 train-loss:  2.087392120701926 	 ± 0.27072500262788224
	data : 0.11646676063537598
	model : 0.0696756362915039
			 train-loss:  2.086547710319266 	 ± 0.27035977425877034
	data : 0.11629900932312012
	model : 0.06946501731872559
			 train-loss:  2.0877358126190475 	 ± 0.27027294993915163
	data : 0.11643495559692382
	model : 0.06951432228088379
			 train-loss:  2.0879215544937924 	 ± 0.2696513223062057
	data : 0.11625552177429199
	model : 0.06864089965820312
			 train-loss:  2.088976731924253 	 ± 0.2694609707600568
	data : 0.11710629463195801
	model : 0.06871471405029297
			 train-loss:  2.088067575942638 	 ± 0.2691623716442379
	data : 0.11693201065063477
	model : 0.06817207336425782
			 train-loss:  2.086700047607775 	 ± 0.26928618955922007
	data : 0.11742658615112304
	model : 0.06754250526428222
			 train-loss:  2.0850429891990627 	 ± 0.2697665345004728
	data : 0.11811933517456055
	model : 0.06814241409301758
			 train-loss:  2.087758259488902 	 ± 0.2721029802331301
	data : 0.11765079498291016
	model : 0.06925225257873535
			 train-loss:  2.0884356427954756 	 ± 0.27166519543350776
	data : 0.11686649322509765
	model : 0.06949801445007324
			 train-loss:  2.0878698321906004 	 ± 0.2711763741386556
	data : 0.11675844192504883
	model : 0.07034902572631836
			 train-loss:  2.087786564999576 	 ± 0.2705649761720398
	data : 0.11600141525268555
	model : 0.07129836082458496
			 train-loss:  2.0890342518016025 	 ± 0.27059136698917363
	data : 0.11505928039550781
	model : 0.07034296989440918
			 train-loss:  2.0891898196908927 	 ± 0.2699939280525229
	data : 0.11582088470458984
	model : 0.06972999572753906
			 train-loss:  2.090164021189724 	 ± 0.26978311971503677
	data : 0.1160825252532959
	model : 0.06856646537780761
			 train-loss:  2.0910183117124768 	 ± 0.2694864188440593
	data : 0.1171614646911621
	model : 0.06752576828002929
			 train-loss:  2.0907839784579996 	 ± 0.2689125225051999
	data : 0.11795544624328613
	model : 0.06704702377319335
			 train-loss:  2.091386831279368 	 ± 0.2684725621945037
	data : 0.11818304061889648
	model : 0.06719117164611817
			 train-loss:  2.0909105377239094 	 ± 0.267979259276451
	data : 0.11786470413208008
	model : 0.06659159660339356
			 train-loss:  2.0935348886589815 	 ± 0.27031384664938474
	data : 0.11840553283691406
	model : 0.06714739799499511
			 train-loss:  2.0946626315946166 	 ± 0.27026491638492367
	data : 0.1178779125213623
	model : 0.06746249198913574
			 train-loss:  2.097701186741585 	 ± 0.2735881375750172
	data : 0.11748619079589843
	model : 0.06671347618103027
			 train-loss:  2.097026872737654 	 ± 0.2731901777985594
	data : 0.11812634468078613
	model : 0.0666236400604248
			 train-loss:  2.0974760879262835 	 ± 0.2726891577587547
	data : 0.11851997375488281
	model : 0.0672041893005371
			 train-loss:  2.09679790541657 	 ± 0.27230270965997305
	data : 0.11797580718994141
	model : 0.0673790454864502
			 train-loss:  2.0966676529417647 	 ± 0.2717300296270987
	data : 0.11777195930480958
	model : 0.06772150993347167
			 train-loss:  2.0963875823101756 	 ± 0.271187707626595
	data : 0.11777987480163574
	model : 0.06879987716674804
			 train-loss:  2.097853575074723 	 ± 0.27155047692667056
	data : 0.11697568893432617
	model : 0.06918134689331054
			 train-loss:  2.0964393580661103 	 ± 0.271852594835259
	data : 0.11663970947265626
	model : 0.06954913139343262
			 train-loss:  2.0972661119125875 	 ± 0.2715829348032515
	data : 0.11648306846618653
	model : 0.0695723533630371
			 train-loss:  2.0970877890785533 	 ± 0.2710305672827618
	data : 0.1163820743560791
	model : 0.06905293464660645
			 train-loss:  2.0961948277050033 	 ± 0.27082122593464464
	data : 0.11652116775512696
	model : 0.0681009292602539
			 train-loss:  2.0960995545072003 	 ± 0.2702651457838911
	data : 0.11738739013671876
	model : 0.06753730773925781
			 train-loss:  2.0944230433844737 	 ± 0.2709665041905083
	data : 0.1175931453704834
	model : 0.06735119819641114
			 train-loss:  2.094438456609601 	 ± 0.2704107816216637
	data : 0.117454195022583
	model : 0.06650514602661133
			 train-loss:  2.0933321860371805 	 ± 0.2704110747533042
	data : 0.11811280250549316
	model : 0.0664790153503418
			 train-loss:  2.0928500813197313 	 ± 0.26996638486030267
	data : 0.11833744049072266
	model : 0.06708569526672363
			 train-loss:  2.092746199866538 	 ± 0.2694242665644732
	data : 0.11793460845947265
	model : 0.06742992401123046
			 train-loss:  2.093798319178243 	 ± 0.2693884815166299
	data : 0.11779003143310547
	model : 0.0671243667602539
			 train-loss:  2.094078699747721 	 ± 0.2688832528472945
	data : 0.1178931713104248
	model : 0.0677863597869873
			 train-loss:  2.0926500301361086 	 ± 0.2692902596920701
	data : 0.11750569343566894
	model : 0.0683281421661377
			 train-loss:  2.0913249168737953 	 ± 0.26956875177205236
	data : 0.11706986427307128
	model : 0.06810259819030762
			 train-loss:  2.0907553514790913 	 ± 0.26918464884249244
	data : 0.117271089553833
	model : 0.06809506416320801
			 train-loss:  2.089022161461148 	 ± 0.2700573371200214
	data : 0.11742935180664063
	model : 0.06852035522460938
			 train-loss:  2.089658869533088 	 ± 0.269715407478529
	data : 0.11757040023803711
	model : 0.0686100959777832
			 train-loss:  2.0888710381937963 	 ± 0.2694787065705452
	data : 0.11746282577514648
	model : 0.06867728233337403
			 train-loss:  2.0898430147208273 	 ± 0.26939935875178145
	data : 0.11629581451416016
	model : 0.060253047943115236
#epoch  72    val-loss:  2.434937000274658  train-loss:  2.0898430147208273  lr:  9.765625e-06
			 train-loss:  1.989307165145874 	 ± 0.0
	data : 5.3552069664001465
	model : 0.10028624534606934
			 train-loss:  1.9710429906845093 	 ± 0.018264174461364746
	data : 2.8126965761184692
	model : 0.08414566516876221
			 train-loss:  2.0629071394602456 	 ± 0.1307686138684843
	data : 1.9138719240824382
	model : 0.07919947306315105
			 train-loss:  1.9535484910011292 	 ± 0.22068816179901338
	data : 1.4644432067871094
	model : 0.07624697685241699
			 train-loss:  1.9857415676116943 	 ± 0.20762511537190503
	data : 1.1951931476593018
	model : 0.07494130134582519
			 train-loss:  1.9662229418754578 	 ± 0.1944951778257937
	data : 0.14746994972229005
	model : 0.06891260147094727
			 train-loss:  1.9830549614770072 	 ± 0.18472742634461434
	data : 0.11673946380615234
	model : 0.06926546096801758
			 train-loss:  1.9448835998773575 	 ± 0.20014510789178755
	data : 0.11667075157165527
	model : 0.06941189765930175
			 train-loss:  1.9502184920840793 	 ± 0.1893009687989093
	data : 0.11668391227722168
	model : 0.06964082717895508
			 train-loss:  1.9500746846199035 	 ± 0.1795871856081771
	data : 0.11649127006530761
	model : 0.06967782974243164
			 train-loss:  1.9564565203406594 	 ± 0.17241483624597684
	data : 0.11640925407409668
	model : 0.06967349052429199
			 train-loss:  1.9658694366614025 	 ± 0.16800080071691376
	data : 0.11628289222717285
	model : 0.06882257461547851
			 train-loss:  1.9879582019952626 	 ± 0.17862848056969413
	data : 0.11707334518432617
	model : 0.0678640365600586
			 train-loss:  1.9949540836470467 	 ± 0.1739690556338558
	data : 0.11775484085083007
	model : 0.06812286376953125
			 train-loss:  2.015954852104187 	 ± 0.18553167391218964
	data : 0.11752762794494628
	model : 0.0681577205657959
			 train-loss:  2.020606331527233 	 ± 0.18054132715382148
	data : 0.11745495796203613
	model : 0.06786832809448243
			 train-loss:  2.0072055774576523 	 ± 0.18316956945886667
	data : 0.11772527694702148
	model : 0.06876173019409179
			 train-loss:  1.9993545479244657 	 ± 0.18092815968336545
	data : 0.11705498695373535
	model : 0.06964893341064453
			 train-loss:  1.9918113131272166 	 ± 0.17898690266520206
	data : 0.11634354591369629
	model : 0.06971426010131836
			 train-loss:  2.00010951757431 	 ± 0.1781652028407005
	data : 0.11631660461425782
	model : 0.06977963447570801
			 train-loss:  2.0220490864345004 	 ± 0.19964510851212622
	data : 0.11631584167480469
	model : 0.07008605003356934
			 train-loss:  2.059476072138006 	 ± 0.2597360175603192
	data : 0.11613407135009765
	model : 0.06980795860290527
			 train-loss:  2.0611554954362954 	 ± 0.25414894024885637
	data : 0.1162912368774414
	model : 0.06989235877990722
			 train-loss:  2.0606172184149423 	 ± 0.24881122912669934
	data : 0.11625747680664063
	model : 0.06983156204223633
			 train-loss:  2.074034433364868 	 ± 0.2524901312888438
	data : 0.11632285118103028
	model : 0.06970815658569336
			 train-loss:  2.0646806955337524 	 ± 0.25196548354693415
	data : 0.11635894775390625
	model : 0.06970334053039551
			 train-loss:  2.069169106306853 	 ± 0.24831238546437648
	data : 0.11637711524963379
	model : 0.07000536918640136
			 train-loss:  2.0889176641191756 	 ± 0.2645507165722688
	data : 0.11618881225585938
	model : 0.07000374794006348
			 train-loss:  2.0941698715604584 	 ± 0.26143093374800863
	data : 0.11600532531738281
	model : 0.06993827819824219
			 train-loss:  2.0888708353042604 	 ± 0.25861601353184044
	data : 0.11608719825744629
	model : 0.06988773345947266
			 train-loss:  2.0787652577123334 	 ± 0.26036211651658286
	data : 0.11614995002746582
	model : 0.07023749351501465
			 train-loss:  2.07407296448946 	 ± 0.2575899635315994
	data : 0.11575379371643066
	model : 0.06939468383789063
			 train-loss:  2.107410358660149 	 ± 0.3160793001614285
	data : 0.11660594940185547
	model : 0.06916699409484864
			 train-loss:  2.110117190024432 	 ± 0.31178437656206576
	data : 0.11697044372558593
	model : 0.06924257278442383
			 train-loss:  2.105115716797965 	 ± 0.30867877190634807
	data : 0.11690354347229004
	model : 0.06929445266723633
			 train-loss:  2.099170449707243 	 ± 0.3063869494645627
	data : 0.1168562889099121
	model : 0.0686274528503418
			 train-loss:  2.096752369726026 	 ± 0.30256627652148965
	data : 0.11740050315856934
	model : 0.06953563690185546
			 train-loss:  2.089989633936631 	 ± 0.30137919171400407
	data : 0.11654982566833497
	model : 0.06975207328796387
			 train-loss:  2.0939667499982395 	 ± 0.2984987788337055
	data : 0.11634020805358887
	model : 0.0690232276916504
			 train-loss:  2.087133613228798 	 ± 0.2978169933153058
	data : 0.11687545776367188
	model : 0.06913866996765136
			 train-loss:  2.087588272443632 	 ± 0.294176712816212
	data : 0.11676464080810547
	model : 0.06945104598999023
			 train-loss:  2.0985024741717746 	 ± 0.2989370760114604
	data : 0.11650452613830567
	model : 0.06933121681213379
			 train-loss:  2.1021798904552016 	 ± 0.29640030327801475
	data : 0.11656150817871094
	model : 0.06839003562927246
			 train-loss:  2.1128226654096083 	 ± 0.3012092791414189
	data : 0.1174314022064209
	model : 0.0691215991973877
			 train-loss:  2.121723691622416 	 ± 0.30363945296751993
	data : 0.11667861938476562
	model : 0.06890316009521484
			 train-loss:  2.1174452097519585 	 ± 0.3016892107875559
	data : 0.11689276695251465
	model : 0.06970577239990235
			 train-loss:  2.1119644134602646 	 ± 0.3007684509249768
	data : 0.1159245491027832
	model : 0.06911344528198242
			 train-loss:  2.106331542134285 	 ± 0.3001138407062248
	data : 0.1163069725036621
	model : 0.06947016716003418
			 train-loss:  2.100758284938579 	 ± 0.2995348491481153
	data : 0.11606884002685547
	model : 0.06857838630676269
			 train-loss:  2.0969639587402344 	 ± 0.29771152478661816
	data : 0.11697187423706054
	model : 0.06782307624816894
			 train-loss:  2.0937859217325845 	 ± 0.29563366286456677
	data : 0.11756243705749511
	model : 0.06698994636535645
			 train-loss:  2.094209446356847 	 ± 0.2927928544156013
	data : 0.11846518516540527
	model : 0.06752843856811523
			 train-loss:  2.097100334347419 	 ± 0.2907657627653159
	data : 0.11790499687194825
	model : 0.06720137596130371
			 train-loss:  2.104003500055384 	 ± 0.292411921098629
	data : 0.1180650234222412
	model : 0.06813912391662598
			 train-loss:  2.097203982960094 	 ± 0.2940182060422739
	data : 0.1174102783203125
	model : 0.06918458938598633
			 train-loss:  2.097357209239687 	 ± 0.29138343401960226
	data : 0.11649432182312011
	model : 0.06928954124450684
			 train-loss:  2.0965824670958937 	 ± 0.2888743135369405
	data : 0.11630353927612305
	model : 0.06848530769348145
			 train-loss:  2.09314390503127 	 ± 0.2875474838273319
	data : 0.11722660064697266
	model : 0.06935572624206543
			 train-loss:  2.0892858080944774 	 ± 0.2866103001564038
	data : 0.11660823822021485
	model : 0.06844120025634766
			 train-loss:  2.0904193937778475 	 ± 0.2843451934886054
	data : 0.11743631362915039
	model : 0.06814570426940918
			 train-loss:  2.0889438468901838 	 ± 0.2822363861284327
	data : 0.11781582832336426
	model : 0.06727023124694824
			 train-loss:  2.0939746229879317 	 ± 0.28269491052711954
	data : 0.11875801086425782
	model : 0.06798982620239258
			 train-loss:  2.095229392959958 	 ± 0.2806163104866173
	data : 0.11800074577331543
	model : 0.0679769515991211
			 train-loss:  2.093669941648841 	 ± 0.27869037304518657
	data : 0.11784944534301758
	model : 0.06795725822448731
			 train-loss:  2.0918688260591947 	 ± 0.27691342301670907
	data : 0.11775145530700684
	model : 0.06820454597473144
			 train-loss:  2.090972745057308 	 ± 0.2749025324086299
	data : 0.11761717796325684
	model : 0.0688621997833252
			 train-loss:  2.0886859715874517 	 ± 0.27347505721410614
	data : 0.11693949699401855
	model : 0.06855640411376954
			 train-loss:  2.0901579085518334 	 ± 0.2717240080636875
	data : 0.11718530654907226
	model : 0.06851582527160645
			 train-loss:  2.0933768680130225 	 ± 0.2710506873400176
	data : 0.11727261543273926
	model : 0.0697704792022705
			 train-loss:  2.094347562108721 	 ± 0.2692284169771004
	data : 0.11612167358398437
	model : 0.06956996917724609
			 train-loss:  2.0928438750790876 	 ± 0.2676215894977017
	data : 0.11630806922912598
	model : 0.06880435943603516
			 train-loss:  2.091058956252204 	 ± 0.26618184776086407
	data : 0.11713232994079589
	model : 0.06927919387817383
			 train-loss:  2.095655281249791 	 ± 0.26721391572021996
	data : 0.1167327880859375
	model : 0.06854462623596191
			 train-loss:  2.1004261906082564 	 ± 0.2685143621205205
	data : 0.11733698844909668
	model : 0.06826996803283691
			 train-loss:  2.0937226406733194 	 ± 0.27288094634380616
	data : 0.11742615699768066
	model : 0.06860561370849609
			 train-loss:  2.0912593493336127 	 ± 0.2719178305619232
	data : 0.1169811725616455
	model : 0.06942343711853027
			 train-loss:  2.0971766307756496 	 ± 0.27502751959979427
	data : 0.11641058921813965
	model : 0.06938867568969727
			 train-loss:  2.0935039336864767 	 ± 0.2751527237698617
	data : 0.11642026901245117
	model : 0.07019109725952148
			 train-loss:  2.0931493028809753 	 ± 0.2734236435819
	data : 0.11575350761413575
	model : 0.07053971290588379
			 train-loss:  2.096479633450508 	 ± 0.273316995068398
	data : 0.1155200481414795
	model : 0.07009301185607911
			 train-loss:  2.0938663688706765 	 ± 0.2726284335888416
	data : 0.1158968448638916
	model : 0.07033042907714844
			 train-loss:  2.091239322976368 	 ± 0.2719905438478742
	data : 0.11573901176452636
	model : 0.07028765678405761
			 train-loss:  2.092410824385034 	 ± 0.27055513763449845
	data : 0.11575727462768555
	model : 0.07000956535339356
			 train-loss:  2.0919410997913 	 ± 0.2689739135417123
	data : 0.11597824096679688
	model : 0.06950678825378417
			 train-loss:  2.093569877568413 	 ± 0.26780341749030767
	data : 0.11651444435119629
	model : 0.06954374313354492
			 train-loss:  2.0947246010913405 	 ± 0.26645463072834485
	data : 0.11646208763122559
	model : 0.06942515373229981
			 train-loss:  2.096030910809835 	 ± 0.26519569140670207
	data : 0.11645307540893554
	model : 0.06874570846557618
			 train-loss:  2.09802685407075 	 ± 0.2643409806149386
	data : 0.11707267761230469
	model : 0.06814770698547364
			 train-loss:  2.0998088496454645 	 ± 0.26338275082159507
	data : 0.11753635406494141
	model : 0.06740794181823731
			 train-loss:  2.096530666616228 	 ± 0.2637349568522308
	data : 0.11827940940856933
	model : 0.06763501167297363
			 train-loss:  2.099596027489547 	 ± 0.26388909369715063
	data : 0.11818919181823731
	model : 0.06727900505065917
			 train-loss:  2.0956939640252488 	 ± 0.2650775357539353
	data : 0.1183990478515625
	model : 0.0677515983581543
			 train-loss:  2.0996991306222896 	 ± 0.26643264734388933
	data : 0.11829838752746583
	model : 0.06845026016235352
			 train-loss:  2.097286470392917 	 ± 0.2660310658697652
	data : 0.11769642829895019
	model : 0.0692258358001709
			 train-loss:  2.0975497321078653 	 ± 0.26463950737077463
	data : 0.11676492691040039
	model : 0.06914253234863281
			 train-loss:  2.098452319701513 	 ± 0.26340451861235376
	data : 0.11673002243041992
	model : 0.06979308128356934
			 train-loss:  2.098574304089104 	 ± 0.2620459715152663
	data : 0.11615467071533203
	model : 0.07004780769348144
			 train-loss:  2.097058894682904 	 ± 0.26113244445909056
	data : 0.11576972007751465
	model : 0.07022581100463868
			 train-loss:  2.098148979321875 	 ± 0.26003425928589136
	data : 0.11559386253356933
	model : 0.0702120304107666
			 train-loss:  2.098001000881195 	 ± 0.25873501059309745
	data : 0.1157008171081543
	model : 0.0703054428100586
			 train-loss:  2.1018854863572827 	 ± 0.2603649717041691
	data : 0.11562466621398926
	model : 0.07074055671691895
			 train-loss:  2.10341182643292 	 ± 0.2595392312602552
	data : 0.11499371528625488
	model : 0.07047762870788574
			 train-loss:  2.1011578770517145 	 ± 0.2592774872416702
	data : 0.11518702507019044
	model : 0.07026858329772949
			 train-loss:  2.0999293568042607 	 ± 0.2583290090572748
	data : 0.11545453071594239
	model : 0.0698026180267334
			 train-loss:  2.10116290251414 	 ± 0.25740350858107347
	data : 0.11576862335205078
	model : 0.07001671791076661
			 train-loss:  2.1014710316118204 	 ± 0.256205919830261
	data : 0.11560754776000977
	model : 0.0691603660583496
			 train-loss:  2.0988779691892248 	 ± 0.2563995763482742
	data : 0.11651406288146973
	model : 0.06953396797180175
			 train-loss:  2.101185158446983 	 ± 0.2563232456363462
	data : 0.11620349884033203
	model : 0.0697716236114502
			 train-loss:  2.1080912406291437 	 ± 0.265046765879592
	data : 0.11613578796386718
	model : 0.07036452293395996
			 train-loss:  2.106328599019484 	 ± 0.26448025703128847
	data : 0.11562800407409668
	model : 0.06916522979736328
			 train-loss:  2.1044529364989684 	 ± 0.26402011289229854
	data : 0.11668791770935058
	model : 0.06851797103881836
			 train-loss:  2.104928604194096 	 ± 0.262886580693615
	data : 0.11748709678649902
	model : 0.0685335636138916
			 train-loss:  2.107680059112279 	 ± 0.26333564811697785
	data : 0.1173884391784668
	model : 0.0686342716217041
			 train-loss:  2.1092838843663535 	 ± 0.26273186548280714
	data : 0.11727585792541503
	model : 0.06845769882202149
			 train-loss:  2.11367218805396 	 ± 0.26575009168139957
	data : 0.11755537986755371
	model : 0.06907839775085449
			 train-loss:  2.1148108860542036 	 ± 0.264883755853203
	data : 0.11694440841674805
	model : 0.06956634521484376
			 train-loss:  2.1144256489908595 	 ± 0.26378197849978113
	data : 0.11654515266418457
	model : 0.0697892189025879
			 train-loss:  2.1124493873725503 	 ± 0.26353030116591303
	data : 0.1163968563079834
	model : 0.06971511840820313
			 train-loss:  2.111120411327907 	 ± 0.2628174843363796
	data : 0.11654849052429199
	model : 0.07019376754760742
			 train-loss:  2.1095444480578105 	 ± 0.26228415326926785
	data : 0.11613116264343262
	model : 0.07027807235717773
			 train-loss:  2.1084154657095917 	 ± 0.26149071148479297
	data : 0.11613383293151855
	model : 0.0695108413696289
			 train-loss:  2.10629610057737 	 ± 0.2614582538402988
	data : 0.1166337013244629
	model : 0.0692063331604004
			 train-loss:  2.1074618333723487 	 ± 0.2607113970674448
	data : 0.11691813468933106
	model : 0.06927061080932617
			 train-loss:  2.105085993966749 	 ± 0.26099151575398455
	data : 0.11685562133789062
	model : 0.06861686706542969
			 train-loss:  2.1044953117370606 	 ± 0.26002865804899283
	data : 0.1174764633178711
	model : 0.06875462532043457
			 train-loss:  2.107643719703432 	 ± 0.26137585175903233
	data : 0.11727318763732911
	model : 0.06895265579223633
			 train-loss:  2.1070324135577585 	 ± 0.260435192898015
	data : 0.11704754829406738
	model : 0.06879429817199707
			 train-loss:  2.105970495380461 	 ± 0.25969175837208663
	data : 0.11710467338562011
	model : 0.06870121955871582
			 train-loss:  2.1059180812318194 	 ± 0.2586839225649575
	data : 0.11710448265075683
	model : 0.06905250549316407
			 train-loss:  2.1060729182683504 	 ± 0.25769306449982854
	data : 0.11678309440612793
	model : 0.06821255683898926
			 train-loss:  2.10722426876767 	 ± 0.25704305217024886
	data : 0.11745924949645996
	model : 0.06885595321655273
			 train-loss:  2.107948168660655 	 ± 0.25620156101887076
	data : 0.11677069664001465
	model : 0.06813993453979492
			 train-loss:  2.1075357790280105 	 ± 0.25528055224947505
	data : 0.11742210388183594
	model : 0.06811275482177734
			 train-loss:  2.108082437693183 	 ± 0.25440435520936194
	data : 0.1173335075378418
	model : 0.06802058219909668
			 train-loss:  2.10779162424582 	 ± 0.2534827204725802
	data : 0.11752095222473144
	model : 0.06803450584411622
			 train-loss:  2.109033662606688 	 ± 0.25296105718679507
	data : 0.11757340431213378
	model : 0.06794204711914062
			 train-loss:  2.1133778956684752 	 ± 0.25707754156714335
	data : 0.11775746345520019
	model : 0.06872882843017578
			 train-loss:  2.1130857700886936 	 ± 0.25616722818485577
	data : 0.11722865104675292
	model : 0.06875176429748535
			 train-loss:  2.112956102803457 	 ± 0.25524864520817026
	data : 0.11729044914245605
	model : 0.06895108222961426
			 train-loss:  2.111534506082535 	 ± 0.25488705469334927
	data : 0.117122220993042
	model : 0.06967401504516602
			 train-loss:  2.1084802032362484 	 ± 0.2565398138600433
	data : 0.11647710800170899
	model : 0.06903648376464844
			 train-loss:  2.1045579331021913 	 ± 0.2598429894552002
	data : 0.11707472801208496
	model : 0.06834731101989747
			 train-loss:  2.106497730408515 	 ± 0.259962578462855
	data : 0.11762409210205078
	model : 0.06745309829711914
			 train-loss:  2.107386637892988 	 ± 0.2592763492144945
	data : 0.11837248802185059
	model : 0.06730704307556153
			 train-loss:  2.10597489291224 	 ± 0.25893552354904525
	data : 0.1186396598815918
	model : 0.0665679931640625
			 train-loss:  2.1049256308438027 	 ± 0.25835636833789755
	data : 0.11937899589538574
	model : 0.06761331558227539
			 train-loss:  2.1041897470448294 	 ± 0.2576295937769267
	data : 0.11837201118469239
	model : 0.06923379898071289
			 train-loss:  2.1075981654025413 	 ± 0.2600620824538656
	data : 0.11674976348876953
	model : 0.07008481025695801
			 train-loss:  2.110220026649885 	 ± 0.26114316746347443
	data : 0.11602492332458496
	model : 0.07034902572631836
			 train-loss:  2.116934007803599 	 ± 0.27286932669387637
	data : 0.1156834602355957
	model : 0.07129902839660644
			 train-loss:  2.1157468675777613 	 ± 0.2723526529870084
	data : 0.11488380432128906
	model : 0.07117924690246583
			 train-loss:  2.117754545650984 	 ± 0.2725740528010551
	data : 0.11502175331115723
	model : 0.07039337158203125
			 train-loss:  2.1157764469096865 	 ± 0.272774215090404
	data : 0.11588220596313477
	model : 0.07042579650878907
			 train-loss:  2.1164388981732456 	 ± 0.2720105895518639
	data : 0.11578879356384278
	model : 0.06929192543029786
			 train-loss:  2.115980823578373 	 ± 0.2711913009522868
	data : 0.11677241325378418
	model : 0.06927876472473145
			 train-loss:  2.1155081376051292 	 ± 0.270384750222603
	data : 0.11685972213745117
	model : 0.06924610137939453
			 train-loss:  2.1155957628966897 	 ± 0.26952449879938273
	data : 0.11701278686523438
	model : 0.06920146942138672
			 train-loss:  2.1144059677667255 	 ± 0.26908351473568815
	data : 0.11687889099121093
	model : 0.06843748092651367
			 train-loss:  2.112454540324661 	 ± 0.2693552107478197
	data : 0.1175614356994629
	model : 0.06938319206237793
			 train-loss:  2.110169565677643 	 ± 0.2700535770456227
	data : 0.11662569046020507
	model : 0.06959247589111328
			 train-loss:  2.1083935476978373 	 ± 0.2701492885520488
	data : 0.1165088176727295
	model : 0.07031126022338867
			 train-loss:  2.1068592998716564 	 ± 0.2700168899869623
	data : 0.11587276458740234
	model : 0.07031459808349609
			 train-loss:  2.1063669985788733 	 ± 0.26926026084312077
	data : 0.11599311828613282
	model : 0.07114911079406738
			 train-loss:  2.1072770080915313 	 ± 0.2686893957454307
	data : 0.11525902748107911
	model : 0.07112207412719726
			 train-loss:  2.108783986351707 	 ± 0.26856822845318157
	data : 0.11526498794555665
	model : 0.07078118324279785
			 train-loss:  2.107077249561448 	 ± 0.2686540875486596
	data : 0.11565628051757812
	model : 0.06919684410095214
			 train-loss:  2.106387839345875 	 ± 0.2679957659469104
	data : 0.1169090747833252
	model : 0.06917877197265625
			 train-loss:  2.104971225772585 	 ± 0.2678233650471202
	data : 0.11692285537719727
	model : 0.06897878646850586
			 train-loss:  2.1064529955034423 	 ± 0.26771960815493945
	data : 0.11713848114013672
	model : 0.06869993209838868
			 train-loss:  2.1071574631859273 	 ± 0.2670880912666464
	data : 0.1174086570739746
	model : 0.0679213523864746
			 train-loss:  2.10927953357585 	 ± 0.26773946232854007
	data : 0.11781530380249024
	model : 0.06875228881835938
			 train-loss:  2.109685182571411 	 ± 0.26701271139305044
	data : 0.11715402603149414
	model : 0.06884160041809081
			 train-loss:  2.1090331146482786 	 ± 0.2663771889386172
	data : 0.11687645912170411
	model : 0.06893935203552246
			 train-loss:  2.1079090566470704 	 ± 0.2660217954457292
	data : 0.11691646575927735
	model : 0.0681920051574707
			 train-loss:  2.107375170162746 	 ± 0.26535411329522857
	data : 0.11754159927368164
	model : 0.06893405914306641
			 train-loss:  2.1072292307561096 	 ± 0.26460623554410334
	data : 0.11695184707641601
	model : 0.06891579627990722
			 train-loss:  2.105606595001652 	 ± 0.26473436744110135
	data : 0.11701798439025879
	model : 0.06884169578552246
			 train-loss:  2.1045360967014615 	 ± 0.26437357934298816
	data : 0.11714816093444824
	model : 0.06880178451538085
			 train-loss:  2.103809582454532 	 ± 0.26381219849928217
	data : 0.11702470779418946
	model : 0.06964702606201172
			 train-loss:  2.1041146960523394 	 ± 0.26311003524483834
	data : 0.11622710227966308
	model : 0.06963777542114258
			 train-loss:  2.1021414731747536 	 ± 0.2637143742230902
	data : 0.11617493629455566
	model : 0.069758939743042
			 train-loss:  2.100499395485763 	 ± 0.26391514954819245
	data : 0.11635584831237793
	model : 0.06946616172790528
			 train-loss:  2.1008015955732167 	 ± 0.2632246561544049
	data : 0.1166374683380127
	model : 0.06995630264282227
			 train-loss:  2.099090165418127 	 ± 0.2635273503502788
	data : 0.11632184982299805
	model : 0.07010889053344727
			 train-loss:  2.098472946398967 	 ± 0.2629474729333563
	data : 0.11623010635375977
	model : 0.06942863464355468
			 train-loss:  2.0974611780976735 	 ± 0.2626005057117587
	data : 0.11699342727661133
	model : 0.06943702697753906
			 train-loss:  2.097550827551653 	 ± 0.2619002780522878
	data : 0.11681933403015136
	model : 0.06993365287780762
			 train-loss:  2.098136284883986 	 ± 0.2613254714980063
	data : 0.11654958724975586
	model : 0.06893630027770996
			 train-loss:  2.096831582841419 	 ± 0.26124642871319514
	data : 0.11732587814331055
	model : 0.06935811042785645
			 train-loss:  2.095010575495268 	 ± 0.26175795000728275
	data : 0.11706280708312988
	model : 0.07034173011779785
			 train-loss:  2.0955375901067446 	 ± 0.26117286785696464
	data : 0.11630582809448242
	model : 0.06929602622985839
			 train-loss:  2.0939776406933865 	 ± 0.2613824541632439
	data : 0.11708564758300781
	model : 0.06900234222412109
			 train-loss:  2.095732400454388 	 ± 0.26183581981234966
	data : 0.11722617149353028
	model : 0.06956429481506347
			 train-loss:  2.0962340100524353 	 ± 0.2612530687944701
	data : 0.11678123474121094
	model : 0.06860413551330566
			 train-loss:  2.0963645085310323 	 ± 0.2605886672714867
	data : 0.11746387481689453
	model : 0.06844067573547363
			 train-loss:  2.0957453384691354 	 ± 0.26006681731453185
	data : 0.11753344535827637
	model : 0.06933846473693847
			 train-loss:  2.0950233512723506 	 ± 0.25960276174795227
	data : 0.11682982444763183
	model : 0.06915655136108398
			 train-loss:  2.0937048624260255 	 ± 0.25960679796536007
	data : 0.1169820785522461
	model : 0.06915225982666015
			 train-loss:  2.092309098746908 	 ± 0.2596974252474155
	data : 0.11694555282592774
	model : 0.06965184211730957
			 train-loss:  2.0928049993515017 	 ± 0.2591418076676433
	data : 0.11658835411071777
	model : 0.06942481994628906
			 train-loss:  2.0909437540158704 	 ± 0.25983306457819116
	data : 0.11654763221740723
	model : 0.06959819793701172
			 train-loss:  2.091092134466266 	 ± 0.2591976522808598
	data : 0.1165534496307373
	model : 0.07069029808044433
			 train-loss:  2.089048996347512 	 ± 0.2601839764987682
	data : 0.11548805236816406
	model : 0.07101569175720215
			 train-loss:  2.08863988460279 	 ± 0.25961093303877575
	data : 0.11543827056884766
	model : 0.0714838981628418
			 train-loss:  2.088640471202571 	 ± 0.25897696169554124
	data : 0.11496782302856445
	model : 0.07179155349731445
			 train-loss:  2.0863562631375583 	 ± 0.26040948144161946
	data : 0.11495575904846192
	model : 0.07169737815856933
			 train-loss:  2.084660109690422 	 ± 0.26091789405801824
	data : 0.1147064208984375
	model : 0.07098288536071777
			 train-loss:  2.0846999837802005 	 ± 0.26029056412423396
	data : 0.11544489860534668
	model : 0.07057623863220215
			 train-loss:  2.0847957750256554 	 ± 0.2596707878404031
	data : 0.1154860019683838
	model : 0.06988797187805176
			 train-loss:  2.0832296343076795 	 ± 0.2600393479689247
	data : 0.11619458198547364
	model : 0.0697629451751709
			 train-loss:  2.0825930777319233 	 ± 0.25958636186078343
	data : 0.11622929573059082
	model : 0.07061028480529785
			 train-loss:  2.0856727356055997 	 ± 0.26280869631584197
	data : 0.115472412109375
	model : 0.06985769271850586
			 train-loss:  2.0865091600328545 	 ± 0.26247373669357754
	data : 0.11611886024475097
	model : 0.07002458572387696
			 train-loss:  2.087827356619256 	 ± 0.2625655210732176
	data : 0.11607470512390136
	model : 0.06940975189208984
			 train-loss:  2.0879381551298986 	 ± 0.26195920634222714
	data : 0.11642746925354004
	model : 0.06922073364257812
			 train-loss:  2.0890045392292516 	 ± 0.2618194436649463
	data : 0.11651535034179687
	model : 0.06832804679870605
			 train-loss:  2.088818500118871 	 ± 0.26122978584048867
	data : 0.1173619270324707
	model : 0.06885194778442383
			 train-loss:  2.0898450618490165 	 ± 0.2610682856753385
	data : 0.11682915687561035
	model : 0.06869239807128906
			 train-loss:  2.0889686890388734 	 ± 0.26079275794094686
	data : 0.11686205863952637
	model : 0.07013463973999023
			 train-loss:  2.0909782241691244 	 ± 0.26189327197221607
	data : 0.11553306579589843
	model : 0.07035255432128906
			 train-loss:  2.090062328593224 	 ± 0.2616529816565047
	data : 0.11524834632873535
	model : 0.06984515190124511
			 train-loss:  2.090874018432858 	 ± 0.2613417262434342
	data : 0.11598773002624511
	model : 0.06964297294616699
			 train-loss:  2.0909272293338863 	 ± 0.26075630514230574
	data : 0.11611437797546387
	model : 0.06912832260131836
			 train-loss:  2.0924261739211425 	 ± 0.26113473700855766
	data : 0.1167940616607666
	model : 0.06838979721069335
			 train-loss:  2.0915926853815714 	 ± 0.26085224113777034
	data : 0.11750435829162598
	model : 0.06845440864562988
			 train-loss:  2.091532128574574 	 ± 0.26027607966023447
	data : 0.11762080192565919
	model : 0.06885762214660644
			 train-loss:  2.0908683900791116 	 ± 0.25989376973706974
	data : 0.1169468879699707
	model : 0.06888103485107422
			 train-loss:  2.090148754810032 	 ± 0.25954976431713656
	data : 0.1169240951538086
	model : 0.06893682479858398
			 train-loss:  2.091979194936794 	 ± 0.2604531083625373
	data : 0.1164881706237793
	model : 0.06795802116394042
			 train-loss:  2.0930500906446707 	 ± 0.2603910607816824
	data : 0.11725940704345703
	model : 0.06745443344116211
			 train-loss:  2.092347225585541 	 ± 0.2600453947051657
	data : 0.11734347343444824
	model : 0.06730809211730956
			 train-loss:  2.0927078055924384 	 ± 0.2595422131096265
	data : 0.11745038032531738
	model : 0.0670090675354004
			 train-loss:  2.093788833577234 	 ± 0.25950755786732216
	data : 0.11773562431335449
	model : 0.06699690818786622
			 train-loss:  2.094855085397378 	 ± 0.2594634329473892
	data : 0.11797709465026855
	model : 0.06770873069763184
			 train-loss:  2.0946526872350812 	 ± 0.25892930569622186
	data : 0.1175039291381836
	model : 0.0679408073425293
			 train-loss:  2.0931827158240948 	 ± 0.25936092750793366
	data : 0.11734819412231445
	model : 0.06741986274719239
			 train-loss:  2.0937328826526045 	 ± 0.25895113907187384
	data : 0.11796655654907226
	model : 0.06730704307556153
			 train-loss:  2.0948529048126283 	 ± 0.25898117766396894
	data : 0.1181154727935791
	model : 0.06724791526794434
			 train-loss:  2.0957506885089634 	 ± 0.258809677643605
	data : 0.1180685043334961
	model : 0.06716399192810059
			 train-loss:  2.0950106998284657 	 ± 0.2585231674329686
	data : 0.11832332611083984
	model : 0.06726484298706055
			 train-loss:  2.094051642041978 	 ± 0.25841373355939545
	data : 0.1184959888458252
	model : 0.06811227798461914
			 train-loss:  2.0938800691573087 	 ± 0.25789302304683814
	data : 0.11775398254394531
	model : 0.06798386573791504
			 train-loss:  2.0937316829775585 	 ± 0.25737218379434906
	data : 0.11799635887145996
	model : 0.06806097030639649
			 train-loss:  2.091836001052231 	 ± 0.25853860639112486
	data : 0.11815776824951171
	model : 0.0681978702545166
			 train-loss:  2.0931282841429417 	 ± 0.2587988882641913
	data : 0.11784243583679199
	model : 0.06790146827697754
			 train-loss:  2.093164872347824 	 ± 0.2582729735577865
	data : 0.11803660392761231
	model : 0.06747298240661621
			 train-loss:  2.093216777330468 	 ± 0.25775090918347354
	data : 0.11813092231750488
	model : 0.06730008125305176
			 train-loss:  2.092671353490122 	 ± 0.2573735132223674
	data : 0.1181870460510254
	model : 0.06724781990051269
			 train-loss:  2.0919271913398223 	 ± 0.2571233817928669
	data : 0.11792912483215331
	model : 0.0671773910522461
			 train-loss:  2.0922456560134886 	 ± 0.25665782132654175
	data : 0.11801958084106445
	model : 0.0672884464263916
			 train-loss:  2.090971250933005 	 ± 0.25693738760462737
	data : 0.11765646934509277
	model : 0.06743335723876953
			 train-loss:  2.0903706872274004 	 ± 0.2566035453856415
	data : 0.11757259368896485
	model : 0.06814823150634766
			 train-loss:  2.0900066087368447 	 ± 0.2561611299174469
	data : 0.1170234203338623
	model : 0.06840319633483886
			 train-loss:  2.0890267918429037 	 ± 0.25613097087793163
	data : 0.11698851585388184
	model : 0.06863651275634766
			 train-loss:  2.088823295107075 	 ± 0.25564883269656424
	data : 0.11671295166015624
	model : 0.0689589500427246
			 train-loss:  2.0918289348483086 	 ± 0.25962408290988975
	data : 0.11562628746032715
	model : 0.0600830078125
#epoch  73    val-loss:  2.438947840740806  train-loss:  2.0918289348483086  lr:  9.765625e-06
			 train-loss:  2.0910966396331787 	 ± 0.0
	data : 5.628005743026733
	model : 0.07333731651306152
			 train-loss:  2.076748490333557 	 ± 0.014348149299621582
	data : 2.880358099937439
	model : 0.06967282295227051
			 train-loss:  2.166799306869507 	 ± 0.1278888008250346
	data : 1.95993177096049
	model : 0.0695785681406657
			 train-loss:  2.1844369769096375 	 ± 0.11489091019607509
	data : 1.4989070296287537
	model : 0.06963300704956055
			 train-loss:  2.1492959022521974 	 ± 0.12449705823378
	data : 1.2221972942352295
	model : 0.06976251602172852
			 train-loss:  2.125562230745951 	 ± 0.12543006181355792
	data : 0.11973609924316406
	model : 0.06908640861511231
			 train-loss:  2.144157750265939 	 ± 0.12473945588447474
	data : 0.11643691062927246
	model : 0.06985907554626465
			 train-loss:  2.1423505544662476 	 ± 0.11678100046312938
	data : 0.11590576171875
	model : 0.06991519927978515
			 train-loss:  2.1148683494991727 	 ± 0.13477634602173016
	data : 0.11592826843261719
	model : 0.0694343090057373
			 train-loss:  2.072960543632507 	 ± 0.17931696750529547
	data : 0.11641030311584473
	model : 0.06930665969848633
			 train-loss:  2.0530134547840464 	 ± 0.18223692052856377
	data : 0.11650271415710449
	model : 0.06925444602966309
			 train-loss:  2.0441764195760093 	 ± 0.1769231350258409
	data : 0.11659455299377441
	model : 0.06853652000427246
			 train-loss:  2.034807617847736 	 ± 0.1730527625270039
	data : 0.11730766296386719
	model : 0.06858329772949219
			 train-loss:  2.045688365186964 	 ± 0.17131038302305873
	data : 0.11729063987731933
	model : 0.06904058456420899
			 train-loss:  2.034960166613261 	 ± 0.17029998121253118
	data : 0.11687440872192383
	model : 0.06829671859741211
			 train-loss:  2.0492091104388237 	 ± 0.1738819696750855
	data : 0.11752138137817383
	model : 0.06845235824584961
			 train-loss:  2.0476367824217854 	 ± 0.16880749088416705
	data : 0.11736545562744141
	model : 0.06904487609863282
			 train-loss:  2.040353364414639 	 ± 0.1667773324365265
	data : 0.11687827110290527
	model : 0.06902995109558105
			 train-loss:  2.033693470452961 	 ± 0.1647699108990948
	data : 0.11698727607727051
	model : 0.06902947425842285
			 train-loss:  2.041524809598923 	 ± 0.16418567216530552
	data : 0.11716547012329101
	model : 0.06974959373474121
			 train-loss:  2.0741413036982217 	 ± 0.21667945246881576
	data : 0.11668429374694825
	model : 0.06950397491455078
			 train-loss:  2.0772021521221506 	 ± 0.21216182395501504
	data : 0.11684064865112305
	model : 0.06971631050109864
			 train-loss:  2.0675731949184253 	 ± 0.21235662982277365
	data : 0.11656432151794434
	model : 0.06974592208862304
			 train-loss:  2.066330462694168 	 ± 0.20797087926249996
	data : 0.11645684242248536
	model : 0.06981315612792968
			 train-loss:  2.069643802642822 	 ± 0.2044145016263205
	data : 0.11623177528381348
	model : 0.06984868049621581
			 train-loss:  2.0656891694435706 	 ± 0.20141782465521665
	data : 0.11605992317199706
	model : 0.0690622329711914
			 train-loss:  2.0638118849860296 	 ± 0.1978843310247331
	data : 0.11675615310668945
	model : 0.06892423629760742
			 train-loss:  2.0550723246165683 	 ± 0.1995544004453752
	data : 0.1167335033416748
	model : 0.06803808212280274
			 train-loss:  2.0594365761197846 	 ± 0.19743883888260516
	data : 0.11761107444763183
	model : 0.06794724464416504
			 train-loss:  2.0698482195536294 	 ± 0.20205533692567126
	data : 0.11773438453674316
	model : 0.067864990234375
			 train-loss:  2.067336247813317 	 ± 0.1992452757594984
	data : 0.1177330493927002
	model : 0.06799888610839844
			 train-loss:  2.086731683462858 	 ± 0.22387444429928532
	data : 0.1175999641418457
	model : 0.06802549362182617
			 train-loss:  2.0849436955018477 	 ± 0.22068821180171597
	data : 0.11758670806884766
	model : 0.06886887550354004
			 train-loss:  2.094058930873871 	 ± 0.22363525044126517
	data : 0.11673688888549805
	model : 0.06899123191833496
			 train-loss:  2.0899480070386613 	 ± 0.221716891227427
	data : 0.11681299209594727
	model : 0.0692474365234375
			 train-loss:  2.08715921971533 	 ± 0.21923748771532786
	data : 0.11678929328918457
	model : 0.06984891891479492
			 train-loss:  2.079139097316845 	 ± 0.22154373155069837
	data : 0.11616911888122558
	model : 0.0699800968170166
			 train-loss:  2.0872404449864437 	 ± 0.22409458079016029
	data : 0.11611819267272949
	model : 0.06908769607543945
			 train-loss:  2.0949941048255334 	 ± 0.22630789092579232
	data : 0.11685390472412109
	model : 0.06898951530456543
			 train-loss:  2.091233792901039 	 ± 0.2246916505856605
	data : 0.11686844825744629
	model : 0.06899394989013671
			 train-loss:  2.0914485018427778 	 ± 0.22193874773541816
	data : 0.11683969497680664
	model : 0.06912846565246582
			 train-loss:  2.0980071936334883 	 ± 0.22326598614158435
	data : 0.11679224967956543
	model : 0.06855087280273438
			 train-loss:  2.1057584202566813 	 ± 0.2263004094389862
	data : 0.11712126731872559
	model : 0.06938309669494629
			 train-loss:  2.107665774497119 	 ± 0.2240633907261343
	data : 0.11644763946533203
	model : 0.06934628486633301
			 train-loss:  2.103028236495124 	 ± 0.22368515159919442
	data : 0.11630101203918457
	model : 0.0692488670349121
			 train-loss:  2.103437903134719 	 ± 0.22125749893712976
	data : 0.11641387939453125
	model : 0.06905875205993653
			 train-loss:  2.097769856452942 	 ± 0.22224112642180407
	data : 0.11661720275878906
	model : 0.06878175735473632
			 train-loss:  2.1003944451610246 	 ± 0.22064880362061787
	data : 0.11692667007446289
	model : 0.06889314651489258
			 train-loss:  2.103347985111937 	 ± 0.2193422621619802
	data : 0.11692910194396973
	model : 0.0691061019897461
			 train-loss:  2.107046916484833 	 ± 0.2186760846153422
	data : 0.11685552597045898
	model : 0.06810617446899414
			 train-loss:  2.1023554147458543 	 ± 0.21904818522551867
	data : 0.11759753227233886
	model : 0.06827073097229004
			 train-loss:  2.1055296751169057 	 ± 0.2181129225632204
	data : 0.11763443946838378
	model : 0.0687899112701416
			 train-loss:  2.109556216113972 	 ± 0.21798788094792865
	data : 0.11727442741394042
	model : 0.06844077110290528
			 train-loss:  2.112763351864285 	 ± 0.21721851476959333
	data : 0.11748924255371093
	model : 0.0683213710784912
			 train-loss:  2.112810043855147 	 ± 0.21523501597480094
	data : 0.11757783889770508
	model : 0.06926836967468261
			 train-loss:  2.113709854228156 	 ± 0.21340897668271322
	data : 0.11667752265930176
	model : 0.0693392276763916
			 train-loss:  2.1096554806357934 	 ± 0.21369349404234766
	data : 0.11644706726074219
	model : 0.06979899406433106
			 train-loss:  2.1086769679497026 	 ± 0.21197207395158768
	data : 0.11612906455993652
	model : 0.07021384239196778
			 train-loss:  2.1171474982116183 	 ± 0.21984560788969748
	data : 0.11566119194030762
	model : 0.0693166732788086
			 train-loss:  2.1120793879032136 	 ± 0.22145431862359805
	data : 0.11646571159362792
	model : 0.06943807601928711
			 train-loss:  2.106748776357682 	 ± 0.2234792449895537
	data : 0.1164283275604248
	model : 0.07036709785461426
			 train-loss:  2.1021001165913 	 ± 0.22462335901350236
	data : 0.1156008243560791
	model : 0.06926021575927735
			 train-loss:  2.0936899923142933 	 ± 0.2324651411050007
	data : 0.1164865493774414
	model : 0.06942024230957031
			 train-loss:  2.0942428838461637 	 ± 0.23068360276640631
	data : 0.1165541172027588
	model : 0.07011709213256836
			 train-loss:  2.095100767795856 	 ± 0.22900509861043727
	data : 0.11597256660461426
	model : 0.06995272636413574
			 train-loss:  2.091022455331051 	 ± 0.22962983324971803
	data : 0.11604137420654297
	model : 0.06905903816223144
			 train-loss:  2.089540803610389 	 ± 0.2282273794660278
	data : 0.11688084602355957
	model : 0.06994733810424805
			 train-loss:  2.0908496537629295 	 ± 0.22679620268821143
	data : 0.1160548210144043
	model : 0.06890487670898438
			 train-loss:  2.0913832723230557 	 ± 0.22518975041918468
	data : 0.11670432090759278
	model : 0.06832728385925294
			 train-loss:  2.0927314605031695 	 ± 0.22385576663136847
	data : 0.1172602653503418
	model : 0.06850223541259766
			 train-loss:  2.0964501626055005 	 ± 0.22444068704407205
	data : 0.11710920333862304
	model : 0.06841583251953125
			 train-loss:  2.097227922744221 	 ± 0.22297295134407008
	data : 0.1171480655670166
	model : 0.06835708618164063
			 train-loss:  2.094760288930919 	 ± 0.22242820571362817
	data : 0.11742219924926758
	model : 0.06909618377685547
			 train-loss:  2.098062004591968 	 ± 0.22271401349091363
	data : 0.11685519218444824
	model : 0.06957516670227051
			 train-loss:  2.0948511584599814 	 ± 0.22294188364053838
	data : 0.11629972457885743
	model : 0.06944661140441895
			 train-loss:  2.093022726084057 	 ± 0.2220356568485095
	data : 0.1165395736694336
	model : 0.06957020759582519
			 train-loss:  2.0889733364055685 	 ± 0.22339603769146904
	data : 0.11640992164611816
	model : 0.06880884170532227
			 train-loss:  2.084178293362642 	 ± 0.22591234926287956
	data : 0.11703648567199706
	model : 0.06891298294067383
			 train-loss:  2.08370191688779 	 ± 0.2245173939733476
	data : 0.11694602966308594
	model : 0.06830968856811523
			 train-loss:  2.084008051455021 	 ± 0.22312633908979257
	data : 0.11746902465820312
	model : 0.06835870742797852
			 train-loss:  2.0825472937689886 	 ± 0.2221293184689297
	data : 0.11742625236511231
	model : 0.06916947364807129
			 train-loss:  2.083181552770661 	 ± 0.2208445026711437
	data : 0.11655588150024414
	model : 0.06989336013793945
			 train-loss:  2.084271494164524 	 ± 0.21973185950056326
	data : 0.11582751274108886
	model : 0.06900358200073242
			 train-loss:  2.0847188404628207 	 ± 0.2184580352056052
	data : 0.11671619415283203
	model : 0.06988987922668458
			 train-loss:  2.0848437589757585 	 ± 0.21717220386943287
	data : 0.11600356101989746
	model : 0.06890978813171386
			 train-loss:  2.083872068760007 	 ± 0.2160916604305707
	data : 0.11680359840393066
	model : 0.06805009841918945
			 train-loss:  2.08550413997694 	 ± 0.2153786179333208
	data : 0.11759490966796875
	model : 0.06823959350585937
			 train-loss:  2.0893541249361904 	 ± 0.2171413380779526
	data : 0.11730642318725586
	model : 0.06918087005615234
			 train-loss:  2.0878233065765897 	 ± 0.21639501148445695
	data : 0.11652197837829589
	model : 0.06907258033752442
			 train-loss:  2.090109188026852 	 ± 0.21626731332528082
	data : 0.11661949157714843
	model : 0.06986427307128906
			 train-loss:  2.086782482954172 	 ± 0.21737894268933303
	data : 0.11585378646850586
	model : 0.06983132362365722
			 train-loss:  2.082532390304234 	 ± 0.21996303724018823
	data : 0.11602568626403809
	model : 0.06959366798400879
			 train-loss:  2.0851394437974498 	 ± 0.22020168363480147
	data : 0.11629552841186523
	model : 0.06859664916992188
			 train-loss:  2.0853027232149812 	 ± 0.21903292622443446
	data : 0.1172360897064209
	model : 0.0680924415588379
			 train-loss:  2.082373319174114 	 ± 0.21972043754617027
	data : 0.1176903247833252
	model : 0.06828980445861817
			 train-loss:  2.08298517887791 	 ± 0.21865440743519943
	data : 0.11759228706359863
	model : 0.06827926635742188
			 train-loss:  2.081954908125179 	 ± 0.2177585034446244
	data : 0.11750245094299316
	model : 0.06931266784667969
			 train-loss:  2.078199330641299 	 ± 0.2197794894936771
	data : 0.1164400577545166
	model : 0.07030940055847168
			 train-loss:  2.0764704316553444 	 ± 0.21933546374378898
	data : 0.11544580459594726
	model : 0.07008905410766601
			 train-loss:  2.079816335439682 	 ± 0.22076067888194315
	data : 0.11573820114135742
	model : 0.06993236541748046
			 train-loss:  2.0793919834760155 	 ± 0.21970607024136404
	data : 0.11575651168823242
	model : 0.06915674209594727
			 train-loss:  2.077468225768968 	 ± 0.2194796113481267
	data : 0.11643195152282715
	model : 0.06736440658569336
			 train-loss:  2.080171044590404 	 ± 0.22011076914256678
	data : 0.11821231842041016
	model : 0.06724295616149903
			 train-loss:  2.079656815299621 	 ± 0.219112148892135
	data : 0.11824283599853516
	model : 0.06787638664245606
			 train-loss:  2.0812599465960546 	 ± 0.21867825117705528
	data : 0.11773686408996582
	model : 0.06802005767822265
			 train-loss:  2.0809651916881777 	 ± 0.21766526189391935
	data : 0.11767621040344238
	model : 0.06891870498657227
			 train-loss:  2.082975885578405 	 ± 0.21763254804040474
	data : 0.11692514419555664
	model : 0.06987495422363281
			 train-loss:  2.084077286499518 	 ± 0.21692203918303085
	data : 0.11595730781555176
	model : 0.06895694732666016
			 train-loss:  2.0841853563938666 	 ± 0.21592761196537402
	data : 0.11680707931518555
	model : 0.06920080184936524
			 train-loss:  2.0833103505047883 	 ± 0.21513792469666215
	data : 0.11661605834960938
	model : 0.06911702156066894
			 train-loss:  2.0810144205351135 	 ± 0.21551610871094448
	data : 0.11687450408935547
	model : 0.06890254020690918
			 train-loss:  2.0822650343179703 	 ± 0.21495602782684006
	data : 0.11707777976989746
	model : 0.06880159378051758
			 train-loss:  2.082236785804276 	 ± 0.21400299013531435
	data : 0.11724996566772461
	model : 0.06914720535278321
			 train-loss:  2.086022264079044 	 ± 0.2168290152038626
	data : 0.11694502830505371
	model : 0.06885881423950195
			 train-loss:  2.085014338078706 	 ± 0.21615228795067923
	data : 0.11697878837585449
	model : 0.0689931869506836
			 train-loss:  2.083678840563215 	 ± 0.21569456649182625
	data : 0.11688966751098633
	model : 0.06918826103210449
			 train-loss:  2.0845261465789924 	 ± 0.214964609479466
	data : 0.11668663024902344
	model : 0.06936392784118653
			 train-loss:  2.082274624856852 	 ± 0.2154327945160679
	data : 0.11652312278747559
	model : 0.07008805274963378
			 train-loss:  2.0785614181967342 	 ± 0.21828479277804036
	data : 0.11585907936096192
	model : 0.07061667442321777
			 train-loss:  2.0796052634716036 	 ± 0.2176714173790859
	data : 0.11559300422668457
	model : 0.07040328979492187
			 train-loss:  2.0801339799707588 	 ± 0.21684744324292268
	data : 0.11566500663757324
	model : 0.0699831485748291
			 train-loss:  2.086593057288498 	 ± 0.22734436873975344
	data : 0.11591911315917969
	model : 0.0697709083557129
			 train-loss:  2.0881900864888014 	 ± 0.22710441709947765
	data : 0.11591439247131348
	model : 0.06967043876647949
			 train-loss:  2.087985807849515 	 ± 0.22619816574415802
	data : 0.11610574722290039
	model : 0.06938838958740234
			 train-loss:  2.0923233451843264 	 ± 0.23041104539661886
	data : 0.11621088981628418
	model : 0.0692629337310791
			 train-loss:  2.093088600370619 	 ± 0.22965432398623986
	data : 0.11631412506103515
	model : 0.06951360702514649
			 train-loss:  2.092796436444981 	 ± 0.22877189411325805
	data : 0.11611199378967285
	model : 0.06960053443908691
			 train-loss:  2.0940352249890566 	 ± 0.22830373153930703
	data : 0.11623291969299317
	model : 0.0694849967956543
			 train-loss:  2.096170517825341 	 ± 0.22869664532596398
	data : 0.11626267433166504
	model : 0.06886367797851563
			 train-loss:  2.0995285180898815 	 ± 0.23098583656756935
	data : 0.11679182052612305
	model : 0.06912550926208497
			 train-loss:  2.0958871450132994 	 ± 0.23381813122729125
	data : 0.11666908264160156
	model : 0.0692603588104248
			 train-loss:  2.0975080809809943 	 ± 0.2336684374120367
	data : 0.11676459312438965
	model : 0.06922717094421386
			 train-loss:  2.1029165409561386 	 ± 0.24093898658642118
	data : 0.11664586067199707
	model : 0.06938061714172364
			 train-loss:  2.1026983590268378 	 ± 0.24005146457224744
	data : 0.1164863109588623
	model : 0.06997361183166503
			 train-loss:  2.101582486541183 	 ± 0.23950930867091078
	data : 0.11601414680480956
	model : 0.07009644508361816
			 train-loss:  2.100578025859945 	 ± 0.23891236192314735
	data : 0.11597023010253907
	model : 0.0701108455657959
			 train-loss:  2.0987568867467616 	 ± 0.2389843745406486
	data : 0.1158839225769043
	model : 0.07014641761779786
			 train-loss:  2.0986842893172 	 ± 0.23811843093073942
	data : 0.11604743003845215
	model : 0.07020330429077148
			 train-loss:  2.0984070961423913 	 ± 0.2372826880966218
	data : 0.11631884574890136
	model : 0.07010273933410645
			 train-loss:  2.099517191307885 	 ± 0.23679569363271233
	data : 0.11634664535522461
	model : 0.06996526718139648
			 train-loss:  2.1008512897694365 	 ± 0.23648192311612112
	data : 0.11640853881835937
	model : 0.0698155403137207
			 train-loss:  2.1010042185514743 	 ± 0.23565476588577902
	data : 0.11650943756103516
	model : 0.06939740180969238
			 train-loss:  2.1002731948465736 	 ± 0.23499087010145225
	data : 0.11686744689941406
	model : 0.06930313110351563
			 train-loss:  2.097057325144609 	 ± 0.237310159602177
	data : 0.11673898696899414
	model : 0.06955199241638184
			 train-loss:  2.0961380029546803 	 ± 0.2367476017739839
	data : 0.11669106483459472
	model : 0.06974372863769532
			 train-loss:  2.0954130128638386 	 ± 0.23609688753303548
	data : 0.11653580665588378
	model : 0.06966681480407715
			 train-loss:  2.093692509495482 	 ± 0.23620906849492396
	data : 0.11661958694458008
	model : 0.07010827064514161
			 train-loss:  2.0923416461493516 	 ± 0.23597877644450185
	data : 0.11626276969909669
	model : 0.07002749443054199
			 train-loss:  2.0911097758568373 	 ± 0.23566255993893157
	data : 0.11641407012939453
	model : 0.0699495792388916
			 train-loss:  2.0960831014315287 	 ± 0.2425942243049095
	data : 0.11653308868408203
	model : 0.06954417228698731
			 train-loss:  2.0983254601623837 	 ± 0.2433442736202419
	data : 0.11657114028930664
	model : 0.06980929374694825
			 train-loss:  2.0989920489097895 	 ± 0.24268075555247842
	data : 0.11627235412597656
	model : 0.06959443092346192
			 train-loss:  2.0961646893445183 	 ± 0.2443851541817695
	data : 0.11643190383911133
	model : 0.06975831985473632
			 train-loss:  2.096351039874089 	 ± 0.2436013092894681
	data : 0.1161952018737793
	model : 0.06922769546508789
			 train-loss:  2.0984750393898257 	 ± 0.2442406609393949
	data : 0.11645455360412597
	model : 0.06941709518432618
			 train-loss:  2.0963603854179382 	 ± 0.24487594663961262
	data : 0.11653151512145996
	model : 0.06883463859558106
			 train-loss:  2.098432083798062 	 ± 0.2454624883939911
	data : 0.11701369285583496
	model : 0.0681879997253418
			 train-loss:  2.097938329358644 	 ± 0.24476267702236007
	data : 0.11740579605102539
	model : 0.06807785034179688
			 train-loss:  2.0962264005493068 	 ± 0.24493883725855772
	data : 0.11746220588684082
	model : 0.06859192848205567
			 train-loss:  2.0959240697324275 	 ± 0.24420196202901429
	data : 0.11693644523620605
	model : 0.06864266395568848
			 train-loss:  2.0947341741242025 	 ± 0.2439072226826313
	data : 0.11698617935180664
	model : 0.06912503242492676
			 train-loss:  2.0977325130392006 	 ± 0.24611156487303185
	data : 0.11671171188354493
	model : 0.0691835880279541
			 train-loss:  2.099198848923291 	 ± 0.24606426917798638
	data : 0.11663789749145508
	model : 0.06842570304870606
			 train-loss:  2.098318526657616 	 ± 0.24557025756773082
	data : 0.11726703643798828
	model : 0.06854186058044434
			 train-loss:  2.099113525766315 	 ± 0.24503656855179076
	data : 0.11721315383911132
	model : 0.06866283416748047
			 train-loss:  2.0997183890227813 	 ± 0.2444209123990044
	data : 0.11689887046813965
	model : 0.06865816116333008
			 train-loss:  2.098728207057108 	 ± 0.2440217301911471
	data : 0.1170727252960205
	model : 0.06947531700134277
			 train-loss:  2.097365516282263 	 ± 0.24393086601729638
	data : 0.11646299362182617
	model : 0.06937789916992188
			 train-loss:  2.0986527641849406 	 ± 0.24377973701040465
	data : 0.1167107105255127
	model : 0.06923556327819824
			 train-loss:  2.098835896744448 	 ± 0.24307333925056698
	data : 0.11680855751037597
	model : 0.06917734146118164
			 train-loss:  2.0996114322316575 	 ± 0.24257240443886072
	data : 0.11704401969909668
	model : 0.0693169116973877
			 train-loss:  2.0990916400454767 	 ± 0.24196171566704838
	data : 0.11695871353149415
	model : 0.06842231750488281
			 train-loss:  2.101161020339569 	 ± 0.24278307252018308
	data : 0.11777381896972657
	model : 0.06919050216674805
			 train-loss:  2.1016481450234337 	 ± 0.2421691869602558
	data : 0.11702346801757812
	model : 0.06908593177795411
			 train-loss:  2.1020943825585503 	 ± 0.24154801563918488
	data : 0.11727261543273926
	model : 0.06889572143554687
			 train-loss:  2.1016072407364845 	 ± 0.24094701576814193
	data : 0.11742987632751464
	model : 0.06783070564270019
			 train-loss:  2.100610564657524 	 ± 0.24062896616542392
	data : 0.11815166473388672
	model : 0.06854562759399414
			 train-loss:  2.1000248428141135 	 ± 0.24007858910157473
	data : 0.11751132011413574
	model : 0.06861753463745117
			 train-loss:  2.1004245953852902 	 ± 0.23946643864289094
	data : 0.1173105239868164
	model : 0.06775612831115722
			 train-loss:  2.1013602475325266 	 ± 0.23912821065145995
	data : 0.11796779632568359
	model : 0.06787729263305664
			 train-loss:  2.1018400080296216 	 ± 0.23855357351748102
	data : 0.11767802238464356
	model : 0.06894149780273437
			 train-loss:  2.1031170186105665 	 ± 0.238516863172456
	data : 0.11680598258972168
	model : 0.06980061531066895
			 train-loss:  2.1031221998193876 	 ± 0.23786429530261752
	data : 0.11586947441101074
	model : 0.06892147064208984
			 train-loss:  2.103844906324926 	 ± 0.23741842362090604
	data : 0.11660919189453126
	model : 0.06981329917907715
			 train-loss:  2.1037502527236938 	 ± 0.23677936387037365
	data : 0.11575789451599121
	model : 0.06959834098815917
			 train-loss:  2.1023531876584536 	 ± 0.23690531143431573
	data : 0.116015625
	model : 0.06931161880493164
			 train-loss:  2.102862789669139 	 ± 0.23637322329219856
	data : 0.11624412536621094
	model : 0.06772546768188477
			 train-loss:  2.1012026394935366 	 ± 0.23683432614384206
	data : 0.11797060966491699
	model : 0.06776461601257325
			 train-loss:  2.098874914583075 	 ± 0.23835344329535943
	data : 0.11800694465637207
	model : 0.06796236038208008
			 train-loss:  2.0976665264681764 	 ± 0.2383051187785532
	data : 0.11796875
	model : 0.06821198463439941
			 train-loss:  2.0956349753584536 	 ± 0.23932440595571597
	data : 0.11780405044555664
	model : 0.0684999942779541
			 train-loss:  2.0959384559343257 	 ± 0.23873719668662527
	data : 0.11752538681030274
	model : 0.06853041648864747
			 train-loss:  2.095890835159183 	 ± 0.23811881756772182
	data : 0.11754560470581055
	model : 0.06956353187561035
			 train-loss:  2.096103139144858 	 ± 0.23752262919448694
	data : 0.1168086051940918
	model : 0.07025346755981446
			 train-loss:  2.0980400091562514 	 ± 0.23844384187788717
	data : 0.11594653129577637
	model : 0.06994609832763672
			 train-loss:  2.096395736446186 	 ± 0.23894056835956448
	data : 0.1160933494567871
	model : 0.06982054710388183
			 train-loss:  2.09805395942049 	 ± 0.2394613266329328
	data : 0.11609797477722168
	model : 0.07064495086669922
			 train-loss:  2.098956125553208 	 ± 0.2391912651127462
	data : 0.11529831886291504
	model : 0.07043476104736328
			 train-loss:  2.0996111020370942 	 ± 0.23876746501811538
	data : 0.11543183326721192
	model : 0.06866922378540039
			 train-loss:  2.0963551700115204 	 ± 0.24255817944793687
	data : 0.11713976860046386
	model : 0.06902666091918945
			 train-loss:  2.0961955590034598 	 ± 0.24196457745676556
	data : 0.11688952445983887
	model : 0.06894164085388184
			 train-loss:  2.0968875365682167 	 ± 0.24156420728728045
	data : 0.11707210540771484
	model : 0.06922187805175781
			 train-loss:  2.0962773796372813 	 ± 0.24112447948696047
	data : 0.11680426597595214
	model : 0.06851401329040527
			 train-loss:  2.0959557464309766 	 ± 0.2405764110671024
	data : 0.11748147010803223
	model : 0.06855435371398926
			 train-loss:  2.09361221674012 	 ± 0.24231194253286184
	data : 0.11757488250732422
	model : 0.06797795295715332
			 train-loss:  2.0957986654587164 	 ± 0.24374180444852966
	data : 0.11835041046142578
	model : 0.06818652153015137
			 train-loss:  2.095924853702674 	 ± 0.24315908849047116
	data : 0.1181070327758789
	model : 0.06793875694274902
			 train-loss:  2.0956622631503987 	 ± 0.24260328623207142
	data : 0.11824069023132325
	model : 0.06784377098083497
			 train-loss:  2.0941962785127632 	 ± 0.2429439446007857
	data : 0.11843061447143555
	model : 0.06864113807678222
			 train-loss:  2.0932127311116173 	 ± 0.2427815553221114
	data : 0.11756525039672852
	model : 0.06912460327148437
			 train-loss:  2.0925859218525096 	 ± 0.24237582452318443
	data : 0.11690044403076172
	model : 0.06898078918457032
			 train-loss:  2.093053600698147 	 ± 0.24189891899016863
	data : 0.11705408096313477
	model : 0.06993193626403808
			 train-loss:  2.0917467572879342 	 ± 0.24207938843778296
	data : 0.11609711647033691
	model : 0.07085075378417968
			 train-loss:  2.0889342895178036 	 ± 0.24497634955276207
	data : 0.11526632308959961
	model : 0.0708810806274414
			 train-loss:  2.089583518338758 	 ± 0.24459043374049205
	data : 0.11541686058044434
	model : 0.0710385799407959
			 train-loss:  2.0921311571642205 	 ± 0.24686628923704568
	data : 0.11526412963867187
	model : 0.07105321884155273
			 train-loss:  2.094332784551629 	 ± 0.24841318170968807
	data : 0.11523723602294922
	model : 0.07003893852233886
			 train-loss:  2.093064906400278 	 ± 0.24854550950870244
	data : 0.11628174781799316
	model : 0.06993246078491211
			 train-loss:  2.092314727230159 	 ± 0.2482246499831317
	data : 0.11627759933471679
	model : 0.07040638923645019
			 train-loss:  2.0916804980147967 	 ± 0.24783764551953483
	data : 0.1158207893371582
	model : 0.07054939270019531
			 train-loss:  2.0912961878927585 	 ± 0.24734198368559668
	data : 0.1157569408416748
	model : 0.07085914611816406
			 train-loss:  2.090664109668216 	 ± 0.2469631037575879
	data : 0.11550841331481934
	model : 0.07120046615600586
			 train-loss:  2.091678818245105 	 ± 0.2468721369904405
	data : 0.11522583961486817
	model : 0.07156887054443359
			 train-loss:  2.0908880659512112 	 ± 0.24660334915636412
	data : 0.11485743522644043
	model : 0.07076244354248047
			 train-loss:  2.0906457169850667 	 ± 0.24608146429694103
	data : 0.11544528007507324
	model : 0.07034058570861816
			 train-loss:  2.089393930097597 	 ± 0.24625334134834256
	data : 0.11565103530883789
	model : 0.07040643692016602
			 train-loss:  2.0888365591150024 	 ± 0.24585316379298064
	data : 0.11562485694885254
	model : 0.06947150230407714
			 train-loss:  2.0891913159897455 	 ± 0.24537164131337577
	data : 0.11634988784790039
	model : 0.06909966468811035
			 train-loss:  2.0897093907193844 	 ± 0.24496025017078218
	data : 0.11674156188964843
	model : 0.06973166465759277
			 train-loss:  2.0894275121066883 	 ± 0.24446436530072707
	data : 0.11627659797668458
	model : 0.0700918197631836
			 train-loss:  2.0881766870424343 	 ± 0.24467113016937347
	data : 0.11605114936828613
	model : 0.07000541687011719
			 train-loss:  2.087366190963778 	 ± 0.24445382471759616
	data : 0.11605772972106934
	model : 0.07062492370605469
			 train-loss:  2.0875054209017447 	 ± 0.24393789995883997
	data : 0.11566557884216308
	model : 0.0704148292541504
			 train-loss:  2.087280709009904 	 ± 0.24344027319529393
	data : 0.11587510108947754
	model : 0.07010812759399414
			 train-loss:  2.0858878653100197 	 ± 0.24385435428058858
	data : 0.11593728065490723
	model : 0.0697927474975586
			 train-loss:  2.0867687029353643 	 ± 0.243711523317477
	data : 0.11609115600585937
	model : 0.06955504417419434
			 train-loss:  2.0868264803906533 	 ± 0.2431984401944676
	data : 0.11631526947021484
	model : 0.06975493431091309
			 train-loss:  2.087891531591656 	 ± 0.2432402270590464
	data : 0.11596732139587403
	model : 0.07000041007995605
			 train-loss:  2.0890309192146717 	 ± 0.24336644174265845
	data : 0.11560640335083008
	model : 0.07031216621398925
			 train-loss:  2.088547709584236 	 ± 0.2429737626620943
	data : 0.11563854217529297
	model : 0.07023811340332031
			 train-loss:  2.0885393847073757 	 ± 0.2424691780099425
	data : 0.11571063995361328
	model : 0.07032861709594726
			 train-loss:  2.087615913596035 	 ± 0.24239201214665942
	data : 0.11568703651428222
	model : 0.07049570083618165
			 train-loss:  2.0892709155141573 	 ± 0.24325901338589243
	data : 0.11548781394958496
	model : 0.07048664093017579
			 train-loss:  2.0899448277520354 	 ± 0.24298721702718287
	data : 0.115570068359375
	model : 0.07058916091918946
			 train-loss:  2.0895392719580204 	 ± 0.24257355315690154
	data : 0.11545019149780274
	model : 0.07118101119995117
			 train-loss:  2.088842799993065 	 ± 0.24232535317747592
	data : 0.11503129005432129
	model : 0.07116589546203614
			 train-loss:  2.087967403021901 	 ± 0.24222376493676842
	data : 0.11514239311218262
	model : 0.0710970401763916
			 train-loss:  2.0872947838037246 	 ± 0.24196594210289935
	data : 0.11529507637023925
	model : 0.07136445045471192
			 train-loss:  2.0880930016797232 	 ± 0.24180653462564822
	data : 0.11514167785644532
	model : 0.07128877639770508
			 train-loss:  2.0877302131652833 	 ± 0.24139032882027459
	data : 0.11509189605712891
	model : 0.0710782527923584
			 train-loss:  2.0868701203410844 	 ± 0.24129252411834995
	data : 0.11511530876159667
	model : 0.07095746994018555
			 train-loss:  2.0866264833344355 	 ± 0.24084422619908455
	data : 0.11499757766723633
	model : 0.07077140808105468
			 train-loss:  2.0866542418483687 	 ± 0.2403681821145406
	data : 0.1150846004486084
	model : 0.07079877853393554
			 train-loss:  2.0865846496867384 	 ± 0.23989710357783633
	data : 0.11478152275085449
	model : 0.07056951522827148
			 train-loss:  2.0869134828156115 	 ± 0.23948360471966407
	data : 0.1149707317352295
	model : 0.07040929794311523
			 train-loss:  2.086035250686109 	 ± 0.23942648873027278
	data : 0.11419897079467774
	model : 0.061708688735961914
#epoch  74    val-loss:  2.4525281818289506  train-loss:  2.086035250686109  lr:  9.765625e-06
			 train-loss:  2.1868784427642822 	 ± 0.0
	data : 5.710057020187378
	model : 0.07733345031738281
			 train-loss:  2.2886719703674316 	 ± 0.10179352760314941
	data : 2.916687488555908
	model : 0.07251131534576416
			 train-loss:  2.1948724587758384 	 ± 0.15653959538799878
	data : 1.9844410419464111
	model : 0.07160035769144694
			 train-loss:  2.225302994251251 	 ± 0.14545286577149275
	data : 1.5171900987625122
	model : 0.07122296094894409
			 train-loss:  2.158377480506897 	 ± 0.18665831481264075
	data : 1.2368294715881347
	model : 0.07088308334350586
			 train-loss:  2.149326225121816 	 ± 0.17159272962689553
	data : 0.11801037788391114
	model : 0.06936979293823242
			 train-loss:  2.096546632902963 	 ± 0.20482158579950477
	data : 0.11670956611633301
	model : 0.06937894821166993
			 train-loss:  2.177313342690468 	 ± 0.2870030080461515
	data : 0.1163750171661377
	model : 0.06935606002807618
			 train-loss:  2.1468387842178345 	 ± 0.2839859387042595
	data : 0.11635680198669433
	model : 0.06928677558898926
			 train-loss:  2.134275567531586 	 ± 0.27203625082903243
	data : 0.1164590835571289
	model : 0.06972303390502929
			 train-loss:  2.1408286419781772 	 ± 0.2602028795816563
	data : 0.11598234176635742
	model : 0.06888270378112793
			 train-loss:  2.1607791682084403 	 ± 0.25776281499900694
	data : 0.11665158271789551
	model : 0.06854009628295898
			 train-loss:  2.1639705346180844 	 ± 0.2478971339713933
	data : 0.1169191837310791
	model : 0.06838631629943848
			 train-loss:  2.1706863215991428 	 ± 0.2401037549618682
	data : 0.11732020378112792
	model : 0.0684473991394043
			 train-loss:  2.169481142361959 	 ± 0.23200609187745183
	data : 0.1172682762145996
	model : 0.06780099868774414
			 train-loss:  2.174357406795025 	 ± 0.2254314071370732
	data : 0.11802077293395996
	model : 0.06802506446838379
			 train-loss:  2.161017565166249 	 ± 0.2251159025785707
	data : 0.11775870323181152
	model : 0.06887359619140625
			 train-loss:  2.1407327122158475 	 ± 0.2342153937932558
	data : 0.11703691482543946
	model : 0.06916203498840331
			 train-loss:  2.1283641928120662 	 ± 0.2339301026422212
	data : 0.11671905517578125
	model : 0.06882071495056152
			 train-loss:  2.1180221915245054 	 ± 0.2324205479257296
	data : 0.11707916259765624
	model : 0.06892070770263672
			 train-loss:  2.1077933311462402 	 ± 0.2313861591438194
	data : 0.11695051193237305
	model : 0.06939535140991211
			 train-loss:  2.1204622875560415 	 ± 0.23340199251497376
	data : 0.11659154891967774
	model : 0.0689615249633789
			 train-loss:  2.1151932788931807 	 ± 0.22960557704913315
	data : 0.11692123413085938
	model : 0.06837835311889648
			 train-loss:  2.1042632311582565 	 ± 0.23080256570666666
	data : 0.11738443374633789
	model : 0.06877360343933106
			 train-loss:  2.1011524534225465 	 ± 0.22665232819940925
	data : 0.11714377403259277
	model : 0.06898999214172363
			 train-loss:  2.0997915955690236 	 ± 0.22235502659197706
	data : 0.11704487800598144
	model : 0.06920700073242188
			 train-loss:  2.099777579307556 	 ± 0.21819850306972952
	data : 0.11687798500061035
	model : 0.06968512535095214
			 train-loss:  2.114551616566522 	 ± 0.22760394860598976
	data : 0.11621575355529785
	model : 0.06926074028015136
			 train-loss:  2.130822416009574 	 ± 0.23964539941597926
	data : 0.11656579971313477
	model : 0.06821413040161133
			 train-loss:  2.1267534772555035 	 ± 0.2366341448188527
	data : 0.11731209754943847
	model : 0.06728601455688477
			 train-loss:  2.13944782364753 	 ± 0.24294813830108092
	data : 0.11817970275878906
	model : 0.06717386245727539
			 train-loss:  2.127830173820257 	 ± 0.2477163047363765
	data : 0.11828484535217285
	model : 0.0668342113494873
			 train-loss:  2.114243081121734 	 ± 0.255756451452029
	data : 0.11880950927734375
	model : 0.06777167320251465
			 train-loss:  2.119326016482185 	 ± 0.25365349299700235
	data : 0.1180802822113037
	model : 0.06872210502624512
			 train-loss:  2.108522435597011 	 ± 0.2578181458568533
	data : 0.11727509498596192
	model : 0.06913180351257324
			 train-loss:  2.117264343632592 	 ± 0.2594196127012567
	data : 0.11697936058044434
	model : 0.06831798553466797
			 train-loss:  2.114124182108286 	 ± 0.2565826148355103
	data : 0.11764912605285645
	model : 0.06838679313659668
			 train-loss:  2.11416588958941 	 ± 0.2531841466560554
	data : 0.11762466430664062
	model : 0.06813225746154786
			 train-loss:  2.1221453837859325 	 ± 0.2547118256057632
	data : 0.1176673412322998
	model : 0.06814160346984863
			 train-loss:  2.1192920804023743 	 ± 0.25213820321121827
	data : 0.11773490905761719
	model : 0.06827549934387207
			 train-loss:  2.1228587394807397 	 ± 0.25006386863147745
	data : 0.11731142997741699
	model : 0.06895318031311035
			 train-loss:  2.124355571610587 	 ± 0.24725481488192189
	data : 0.1168281078338623
	model : 0.06837449073791504
			 train-loss:  2.1188796370528467 	 ± 0.24692631836319062
	data : 0.11738414764404297
	model : 0.06865653991699219
			 train-loss:  2.1166031170975077 	 ± 0.2445602481856576
	data : 0.11723260879516602
	model : 0.06858100891113281
			 train-loss:  2.1134727901882595 	 ± 0.2427174568579731
	data : 0.11717181205749512
	model : 0.06876764297485352
			 train-loss:  2.1186584078747295 	 ± 0.2425719516563866
	data : 0.1171177864074707
	model : 0.06891140937805176
			 train-loss:  2.114867953543967 	 ± 0.24135061499282467
	data : 0.11678261756896972
	model : 0.0687180519104004
			 train-loss:  2.1190711930394173 	 ± 0.24055547104601957
	data : 0.11695356369018554
	model : 0.06843781471252441
			 train-loss:  2.111865936493387 	 ± 0.24326514639603883
	data : 0.11718106269836426
	model : 0.06838698387145996
			 train-loss:  2.118680374622345 	 ± 0.24549901141495634
	data : 0.11732249259948731
	model : 0.06853809356689453
			 train-loss:  2.1121054420284198 	 ± 0.24748634597921904
	data : 0.11714315414428711
	model : 0.06855039596557617
			 train-loss:  2.11260164471773 	 ± 0.24512073287095834
	data : 0.11704363822937011
	model : 0.06950287818908692
			 train-loss:  2.1158481386472596 	 ± 0.24392329863337842
	data : 0.11606712341308593
	model : 0.06972336769104004
			 train-loss:  2.1235737292854875 	 ± 0.24811296203426514
	data : 0.11596159934997559
	model : 0.06999530792236328
			 train-loss:  2.1232658798044377 	 ± 0.24585745075175572
	data : 0.11573219299316406
	model : 0.06943902969360352
			 train-loss:  2.113795987197331 	 ± 0.2535721453483624
	data : 0.11618094444274903
	model : 0.06923727989196778
			 train-loss:  2.108899323563827 	 ± 0.25399510393323227
	data : 0.11654143333435059
	model : 0.06921901702880859
			 train-loss:  2.116179673836149 	 ± 0.2577254554401733
	data : 0.11658797264099122
	model : 0.06924586296081543
			 train-loss:  2.1191553200705577 	 ± 0.2565349218055255
	data : 0.11654963493347167
	model : 0.06914992332458496
			 train-loss:  2.113012115160624 	 ± 0.25872751889075263
	data : 0.11673755645751953
	model : 0.06976966857910157
			 train-loss:  2.1060120336344985 	 ± 0.2622644116687376
	data : 0.11631207466125489
	model : 0.07016196250915527
			 train-loss:  2.1073573231697083 	 ± 0.2603528808076912
	data : 0.11582713127136231
	model : 0.07029237747192382
			 train-loss:  2.1138765528088523 	 ± 0.2633300477418562
	data : 0.11578049659729003
	model : 0.07018322944641113
			 train-loss:  2.1107910703867674 	 ± 0.2624099976158411
	data : 0.11581659317016602
	model : 0.0701491355895996
			 train-loss:  2.106153335938087 	 ± 0.26301366254410774
	data : 0.1158517837524414
	model : 0.06939697265625
			 train-loss:  2.1038074836586462 	 ± 0.2616978393754679
	data : 0.1165555477142334
	model : 0.06922259330749511
			 train-loss:  2.104430762689505 	 ± 0.2597868801341766
	data : 0.1169774055480957
	model : 0.06911616325378418
			 train-loss:  2.1027265383916744 	 ± 0.25824664180902446
	data : 0.1170170783996582
	model : 0.06912150382995605
			 train-loss:  2.105042334915935 	 ± 0.2570787139008023
	data : 0.11687979698181153
	model : 0.06911425590515137
			 train-loss:  2.1019140243530274 	 ± 0.2565552308686653
	data : 0.11676721572875977
	model : 0.06999740600585938
			 train-loss:  2.1037831407197762 	 ± 0.2552216442460499
	data : 0.11613502502441406
	model : 0.06972255706787109
			 train-loss:  2.1052619417508445 	 ± 0.2537492040500358
	data : 0.11616663932800293
	model : 0.06953992843627929
			 train-loss:  2.1055273813744115 	 ± 0.2520152676474219
	data : 0.11627659797668458
	model : 0.06868705749511719
			 train-loss:  2.1048506240586975 	 ± 0.25037344709639353
	data : 0.11725211143493652
	model : 0.06867632865905762
			 train-loss:  2.100021227200826 	 ± 0.2521446918885772
	data : 0.1171445369720459
	model : 0.06851458549499512
			 train-loss:  2.0991916013391396 	 ± 0.2505833749859378
	data : 0.11716351509094239
	model : 0.06795449256896972
			 train-loss:  2.102568624855636 	 ± 0.2506856055176009
	data : 0.11785774230957032
	model : 0.06856050491333007
			 train-loss:  2.101202678986085 	 ± 0.24936169925243684
	data : 0.11736373901367188
	model : 0.06856775283813477
			 train-loss:  2.0979520414448993 	 ± 0.249436065554835
	data : 0.11717386245727539
	model : 0.06772446632385254
			 train-loss:  2.0989591136574743 	 ± 0.24803375329851474
	data : 0.11802549362182617
	model : 0.06680669784545898
			 train-loss:  2.0994787260338112 	 ± 0.24654173519745967
	data : 0.11876378059387208
	model : 0.06717100143432617
			 train-loss:  2.0963656626096587 	 ± 0.2466304066381678
	data : 0.1183039665222168
	model : 0.06620006561279297
			 train-loss:  2.095999716276146 	 ± 0.24516257577241363
	data : 0.11917266845703126
	model : 0.06715073585510253
			 train-loss:  2.1006655309881483 	 ± 0.24737835836553002
	data : 0.11831526756286621
	model : 0.06795177459716797
			 train-loss:  2.105118248041938 	 ± 0.24928205014001623
	data : 0.1176445484161377
	model : 0.0688431739807129
			 train-loss:  2.1044260582258536 	 ± 0.2479106497156886
	data : 0.11689181327819824
	model : 0.06934127807617188
			 train-loss:  2.106263241548648 	 ± 0.247069884610753
	data : 0.11636738777160645
	model : 0.06991949081420898
			 train-loss:  2.106434641913934 	 ± 0.24566726953704132
	data : 0.11582064628601074
	model : 0.07008867263793946
			 train-loss:  2.106964710053433 	 ± 0.24433382070856774
	data : 0.11588921546936035
	model : 0.070172119140625
			 train-loss:  2.1041310310363768 	 ± 0.2444388269391182
	data : 0.1156832218170166
	model : 0.06927103996276855
			 train-loss:  2.0987924389786774 	 ± 0.24831188822832326
	data : 0.11636881828308106
	model : 0.06924257278442383
			 train-loss:  2.0930122668328495 	 ± 0.2530393942608866
	data : 0.11657695770263672
	model : 0.06922321319580078
			 train-loss:  2.0938190631969 	 ± 0.25179423454415784
	data : 0.1165853500366211
	model : 0.06812410354614258
			 train-loss:  2.0905122110184204 	 ± 0.25247345428376367
	data : 0.11752290725708008
	model : 0.06813635826110839
			 train-loss:  2.0868645028064123 	 ± 0.2536190266319535
	data : 0.11763806343078613
	model : 0.06872067451477051
			 train-loss:  2.0842323936522007 	 ± 0.2535956295803089
	data : 0.11704936027526855
	model : 0.0678666114807129
			 train-loss:  2.0846576604646505 	 ± 0.2523194557835754
	data : 0.11768193244934082
	model : 0.06792635917663574
			 train-loss:  2.0795742796391856 	 ± 0.25597268914853916
	data : 0.11770458221435547
	model : 0.06952900886535644
			 train-loss:  2.080355729719605 	 ± 0.2547940818048616
	data : 0.1160700798034668
	model : 0.06954631805419922
			 train-loss:  2.083126140832901 	 ± 0.2550111104981219
	data : 0.11621432304382324
	model : 0.06915206909179687
			 train-loss:  2.08038059791716 	 ± 0.2552265642184943
	data : 0.11671228408813476
	model : 0.07012357711791992
			 train-loss:  2.076760490735372 	 ± 0.25656497916747606
	data : 0.11595077514648437
	model : 0.06998739242553711
			 train-loss:  2.0746832183263835 	 ± 0.25617697269428263
	data : 0.11590433120727539
	model : 0.06905689239501953
			 train-loss:  2.07412049976679 	 ± 0.25500633544204687
	data : 0.11690521240234375
	model : 0.06909518241882324
			 train-loss:  2.076963057972136 	 ± 0.2554393252661927
	data : 0.11681427955627441
	model : 0.06962556838989258
			 train-loss:  2.076413720283868 	 ± 0.2542938772162754
	data : 0.11624031066894532
	model : 0.06872763633728027
			 train-loss:  2.0773164301275093 	 ± 0.25327337897600455
	data : 0.11682658195495606
	model : 0.06875119209289551
			 train-loss:  2.080546510440332 	 ± 0.25430262063806464
	data : 0.1169355869293213
	model : 0.06896076202392579
			 train-loss:  2.0775329093320654 	 ± 0.2550634337440254
	data : 0.11679444313049317
	model : 0.06891083717346191
			 train-loss:  2.0792047077959235 	 ± 0.2545006285502682
	data : 0.11678915023803711
	model : 0.06890082359313965
			 train-loss:  2.080862424395106 	 ± 0.2539475023700643
	data : 0.11697926521301269
	model : 0.06983580589294433
			 train-loss:  2.085198369409357 	 ± 0.2569053967180223
	data : 0.11610240936279297
	model : 0.06900758743286133
			 train-loss:  2.0879654135324257 	 ± 0.2574370610348609
	data : 0.11662135124206544
	model : 0.06911473274230957
			 train-loss:  2.0878587135097435 	 ± 0.256307973754702
	data : 0.11652770042419433
	model : 0.0688889503479004
			 train-loss:  2.089862944768823 	 ± 0.25608681861243116
	data : 0.1166691780090332
	model : 0.06834797859191895
			 train-loss:  2.090574253222038 	 ± 0.2550946789972688
	data : 0.11712193489074707
	model : 0.06819133758544922
			 train-loss:  2.09169973165561 	 ± 0.2542912712613768
	data : 0.11749415397644043
	model : 0.06882429122924805
			 train-loss:  2.0923498879044744 	 ± 0.2533091125066083
	data : 0.1171487808227539
	model : 0.0678931713104248
			 train-loss:  2.0910501039328695 	 ± 0.2526373968162342
	data : 0.11796817779541016
	model : 0.06768193244934081
			 train-loss:  2.0915950894355775 	 ± 0.25165277251088247
	data : 0.11807403564453126
	model : 0.06813554763793946
			 train-loss:  2.092058146295469 	 ± 0.25066205810074454
	data : 0.11773099899291992
	model : 0.06820850372314453
			 train-loss:  2.095275712794945 	 ± 0.2521292053278877
	data : 0.1177905559539795
	model : 0.06843366622924804
			 train-loss:  2.094945725386705 	 ± 0.2511286497396166
	data : 0.11752538681030274
	model : 0.06929612159729004
			 train-loss:  2.0951224911597466 	 ± 0.2501216673081643
	data : 0.11679677963256836
	model : 0.06978826522827149
			 train-loss:  2.0953139820098876 	 ± 0.24912829745700343
	data : 0.11621942520141601
	model : 0.07003302574157715
			 train-loss:  2.095235924872141 	 ± 0.24813925841910991
	data : 0.11592483520507812
	model : 0.06971211433410644
			 train-loss:  2.093377774155985 	 ± 0.24803892297519795
	data : 0.11623582839965821
	model : 0.06975531578063965
			 train-loss:  2.0946860034018755 	 ± 0.24750760063041796
	data : 0.11646060943603516
	model : 0.06999011039733886
			 train-loss:  2.093708319257396 	 ± 0.2467944072252469
	data : 0.11632905006408692
	model : 0.06992611885070801
			 train-loss:  2.0921757496320286 	 ± 0.2464588234386527
	data : 0.1167872428894043
	model : 0.07019705772399902
			 train-loss:  2.0913952561735196 	 ± 0.24567756253994963
	data : 0.11677365303039551
	model : 0.07019739151000977
			 train-loss:  2.0889080257126778 	 ± 0.24639524858995612
	data : 0.11642980575561523
	model : 0.06992197036743164
			 train-loss:  2.0903212880729733 	 ± 0.24600364366728417
	data : 0.1165046215057373
	model : 0.07020187377929688
			 train-loss:  2.0926949675403423 	 ± 0.2466080624987313
	data : 0.11611528396606445
	model : 0.07053618431091309
			 train-loss:  2.0918578880804555 	 ± 0.24588400741990382
	data : 0.11573057174682617
	model : 0.0701064109802246
			 train-loss:  2.0899578376727947 	 ± 0.2459710753267483
	data : 0.11591143608093261
	model : 0.07059822082519532
			 train-loss:  2.0883691893876906 	 ± 0.245771008049879
	data : 0.11540441513061524
	model : 0.07079553604125977
			 train-loss:  2.088432162568189 	 ± 0.24488002363395442
	data : 0.11523232460021973
	model : 0.06937713623046875
			 train-loss:  2.088228948682332 	 ± 0.2440092479993624
	data : 0.11647267341613769
	model : 0.06815271377563477
			 train-loss:  2.085902363061905 	 ± 0.24467862763244916
	data : 0.11746273040771485
	model : 0.06830348968505859
			 train-loss:  2.088402138534167 	 ± 0.24559698803396668
	data : 0.11736526489257812
	model : 0.06719574928283692
			 train-loss:  2.0875764035842788 	 ± 0.2449270216044983
	data : 0.11862802505493164
	model : 0.06721158027648926
			 train-loss:  2.0862772464752197 	 ± 0.24455962381683705
	data : 0.1185187816619873
	model : 0.06739740371704102
			 train-loss:  2.086723940240012 	 ± 0.24376751225274018
	data : 0.11834640502929687
	model : 0.06756525039672852
			 train-loss:  2.0855811990540603 	 ± 0.24331221190767047
	data : 0.11816906929016113
	model : 0.06730942726135254
			 train-loss:  2.083941019561193 	 ± 0.24328054727212284
	data : 0.11823883056640624
	model : 0.06833872795104981
			 train-loss:  2.0873014626859807 	 ± 0.2458282309493328
	data : 0.11751880645751953
	model : 0.0681009292602539
			 train-loss:  2.0879520459755048 	 ± 0.245123269138419
	data : 0.11764039993286132
	model : 0.06877250671386718
			 train-loss:  2.0875542043839523 	 ± 0.24434726185151281
	data : 0.11703319549560547
	model : 0.06926875114440918
			 train-loss:  2.088440864086151 	 ± 0.24377179039804317
	data : 0.1166491985321045
	model : 0.0686582088470459
			 train-loss:  2.0860197883568063 	 ± 0.24476598207559114
	data : 0.11717023849487304
	model : 0.06773366928100585
			 train-loss:  2.0866370146211826 	 ± 0.2440773748480424
	data : 0.11796159744262695
	model : 0.06782040596008301
			 train-loss:  2.0835419612772323 	 ± 0.24625282920384622
	data : 0.11809859275817872
	model : 0.06841597557067872
			 train-loss:  2.0840606882974697 	 ± 0.24553585397696026
	data : 0.11756854057312012
	model : 0.06851849555969239
			 train-loss:  2.0832770432195358 	 ± 0.24493565077553087
	data : 0.11733918190002442
	model : 0.06965050697326661
			 train-loss:  2.0830017580435825 	 ± 0.24417339264277527
	data : 0.11641345024108887
	model : 0.07013878822326661
			 train-loss:  2.081451144188073 	 ± 0.24416384510914693
	data : 0.11594276428222657
	model : 0.06939539909362794
			 train-loss:  2.0798649584190754 	 ± 0.24420007493829357
	data : 0.11654424667358398
	model : 0.06887507438659668
			 train-loss:  2.078719102361667 	 ± 0.2438566665780065
	data : 0.11702585220336914
	model : 0.06949005126953126
			 train-loss:  2.078989540785551 	 ± 0.24311733719895035
	data : 0.11641454696655273
	model : 0.069236421585083
			 train-loss:  2.0770511479111193 	 ± 0.24359823500538064
	data : 0.11655402183532715
	model : 0.0695652961730957
			 train-loss:  2.076957055080084 	 ± 0.24284815953749844
	data : 0.11624784469604492
	model : 0.07053632736206054
			 train-loss:  2.0763489132278536 	 ± 0.24222578493508184
	data : 0.11541523933410644
	model : 0.07032098770141601
			 train-loss:  2.074650163330683 	 ± 0.24245812859632027
	data : 0.11573677062988282
	model : 0.06992697715759277
			 train-loss:  2.0761343775373517 	 ± 0.24246843079100508
	data : 0.11637721061706544
	model : 0.06996898651123047
			 train-loss:  2.076653706740184 	 ± 0.24182902763515973
	data : 0.11645822525024414
	model : 0.06998882293701172
			 train-loss:  2.074677630812822 	 ± 0.2424444281431423
	data : 0.11639585494995117
	model : 0.06994819641113281
			 train-loss:  2.075500657870656 	 ± 0.24195566810551056
	data : 0.11633753776550293
	model : 0.07001934051513672
			 train-loss:  2.0741600849219326 	 ± 0.24186371897063733
	data : 0.11603960990905762
	model : 0.06900696754455567
			 train-loss:  2.0739826412761913 	 ± 0.24116233848453694
	data : 0.11699047088623046
	model : 0.06907052993774414
			 train-loss:  2.075155730833087 	 ± 0.24094211844596458
	data : 0.11698994636535645
	model : 0.06904573440551758
			 train-loss:  2.0739774232686954 	 ± 0.24073430225591058
	data : 0.11699981689453125
	model : 0.06903767585754395
			 train-loss:  2.0720248566886594 	 ± 0.24139960359016333
	data : 0.11721382141113282
	model : 0.06931586265563965
			 train-loss:  2.070562696319887 	 ± 0.24147198768518052
	data : 0.11705713272094727
	model : 0.07013916969299316
			 train-loss:  2.0706448657172065 	 ± 0.24078351887338936
	data : 0.11637582778930664
	model : 0.06990480422973633
			 train-loss:  2.0715179815888405 	 ± 0.24037615908815327
	data : 0.1165440559387207
	model : 0.06989574432373047
			 train-loss:  2.072067330112565 	 ± 0.2398069373187479
	data : 0.11652021408081055
	model : 0.06940689086914062
			 train-loss:  2.0730776230940657 	 ± 0.23950982142168065
	data : 0.11684536933898926
	model : 0.0692986011505127
			 train-loss:  2.073984624953243 	 ± 0.23914621484207801
	data : 0.11685872077941895
	model : 0.06931614875793457
			 train-loss:  2.07568133076032 	 ± 0.23955895282923514
	data : 0.11663651466369629
	model : 0.06959843635559082
			 train-loss:  2.074773669242859 	 ± 0.23920644073067268
	data : 0.1162381649017334
	model : 0.06872539520263672
			 train-loss:  2.0739837301956427 	 ± 0.23878499090903188
	data : 0.11717138290405274
	model : 0.06943745613098144
			 train-loss:  2.072965362032906 	 ± 0.23852765886441826
	data : 0.11670246124267578
	model : 0.06937108039855958
			 train-loss:  2.071454670118249 	 ± 0.238754833364069
	data : 0.11694140434265136
	model : 0.06867103576660157
			 train-loss:  2.071907145268208 	 ± 0.23818776718652934
	data : 0.11758732795715332
	model : 0.06853785514831542
			 train-loss:  2.071697573507986 	 ± 0.2375637164016498
	data : 0.1178088665008545
	model : 0.06864795684814454
			 train-loss:  2.072131591684678 	 ± 0.23700159689862002
	data : 0.11749639511108398
	model : 0.0683718204498291
			 train-loss:  2.0720156317061567 	 ± 0.2363757520679125
	data : 0.11769661903381348
	model : 0.06785292625427246
			 train-loss:  2.07169169975967 	 ± 0.23579142562898928
	data : 0.1180809497833252
	model : 0.06788778305053711
			 train-loss:  2.0714518547058107 	 ± 0.23519321805603371
	data : 0.11803126335144043
	model : 0.06771235466003418
			 train-loss:  2.0718915537389786 	 ± 0.2346550057764572
	data : 0.11835012435913086
	model : 0.06843385696411133
			 train-loss:  2.0718567868073783 	 ± 0.2340436204942899
	data : 0.11754798889160156
	model : 0.06849274635314942
			 train-loss:  2.0708688046648094 	 ± 0.23383757939797256
	data : 0.11753182411193848
	model : 0.06912965774536133
			 train-loss:  2.070383143793676 	 ± 0.2333316952961826
	data : 0.11690268516540528
	model : 0.06994256973266602
			 train-loss:  2.0710755232052924 	 ± 0.232932357531305
	data : 0.11608142852783203
	model : 0.07019896507263183
			 train-loss:  2.073342446769987 	 ± 0.23448401452696654
	data : 0.11569123268127442
	model : 0.07010483741760254
			 train-loss:  2.0718471544043062 	 ± 0.23482310161914563
	data : 0.11593928337097167
	model : 0.07010393142700196
			 train-loss:  2.071485318318762 	 ± 0.2342844146745058
	data : 0.11602368354797363
	model : 0.07002363204956055
			 train-loss:  2.072606477306117 	 ± 0.2342269143358932
	data : 0.116359281539917
	model : 0.06989970207214355
			 train-loss:  2.0741264462471007 	 ± 0.23462243493120385
	data : 0.11648850440979004
	model : 0.0692288875579834
			 train-loss:  2.0745928287506104 	 ± 0.2341309898572694
	data : 0.11715846061706543
	model : 0.06945328712463379
			 train-loss:  2.07512888813963 	 ± 0.23367436066372402
	data : 0.11698203086853028
	model : 0.06936616897583008
			 train-loss:  2.0745172641547445 	 ± 0.23326012947282687
	data : 0.11690616607666016
	model : 0.06910324096679688
			 train-loss:  2.0763730395073985 	 ± 0.2341851469286957
	data : 0.11691207885742187
	model : 0.06885828971862792
			 train-loss:  2.07669798106682 	 ± 0.2336593621286317
	data : 0.11708569526672363
	model : 0.06925969123840332
			 train-loss:  2.077013647672042 	 ± 0.23313535190170448
	data : 0.11671042442321777
	model : 0.06916084289550781
			 train-loss:  2.0762627890720458 	 ± 0.2328210949878873
	data : 0.11668734550476074
	model : 0.06856379508972169
			 train-loss:  2.0772983150986524 	 ± 0.23273810851521895
	data : 0.1173337459564209
	model : 0.06876678466796875
			 train-loss:  2.077969722770618 	 ± 0.23238248370950684
	data : 0.11711931228637695
	model : 0.06896791458129883
			 train-loss:  2.07782046738125 	 ± 0.23183857336169666
	data : 0.11698522567749023
	model : 0.06910209655761719
			 train-loss:  2.077826286944168 	 ± 0.2312885557592983
	data : 0.11682939529418945
	model : 0.06825361251831055
			 train-loss:  2.0777051938029953 	 ± 0.2307491234594496
	data : 0.11770505905151367
	model : 0.06874589920043946
			 train-loss:  2.078091173104837 	 ± 0.23027541005142652
	data : 0.11713404655456543
	model : 0.0687328815460205
			 train-loss:  2.077921443453459 	 ± 0.22975010770886312
	data : 0.11720895767211914
	model : 0.06819028854370117
			 train-loss:  2.0812950239625088 	 ± 0.23446778791543169
	data : 0.11763710975646972
	model : 0.06865577697753907
			 train-loss:  2.08077302078406 	 ± 0.2340495968589307
	data : 0.11724762916564942
	model : 0.06990656852722169
			 train-loss:  2.0805146875469367 	 ± 0.2335405532361906
	data : 0.11604447364807129
	model : 0.07058382034301758
			 train-loss:  2.080783046166831 	 ± 0.2330378265616767
	data : 0.11561784744262696
	model : 0.07040681838989257
			 train-loss:  2.0832263786498815 	 ± 0.23528724059167833
	data : 0.11576685905456544
	model : 0.07056798934936523
			 train-loss:  2.0828834918412293 	 ± 0.23480672256390578
	data : 0.11550464630126953
	model : 0.06956377029418945
			 train-loss:  2.0818700801193444 	 ± 0.2347566009440961
	data : 0.1165740966796875
	model : 0.06834816932678223
			 train-loss:  2.08272842243985 	 ± 0.23457458846424065
	data : 0.1177133560180664
	model : 0.06737589836120605
			 train-loss:  2.0834169847548276 	 ± 0.23427279359468955
	data : 0.11832180023193359
	model : 0.06746916770935059
			 train-loss:  2.0832375852125034 	 ± 0.23376462989462873
	data : 0.11839017868041993
	model : 0.06772408485412598
			 train-loss:  2.0829901599884035 	 ± 0.23327396902242314
	data : 0.11825251579284668
	model : 0.0673868179321289
			 train-loss:  2.0819977041894355 	 ± 0.2332328889394073
	data : 0.11822171211242676
	model : 0.06748580932617188
			 train-loss:  2.083652815104581 	 ± 0.23404496616795373
	data : 0.11809415817260742
	model : 0.06778893470764161
			 train-loss:  2.0826345624630913 	 ± 0.23403452376153577
	data : 0.11783943176269532
	model : 0.06757187843322754
			 train-loss:  2.083358188383444 	 ± 0.2337784576076713
	data : 0.11791877746582032
	model : 0.06774482727050782
			 train-loss:  2.0819226835084996 	 ± 0.2342789846651784
	data : 0.11806855201721192
	model : 0.0681422233581543
			 train-loss:  2.0816262986236835 	 ± 0.23381454672896976
	data : 0.11783685684204101
	model : 0.06820554733276367
			 train-loss:  2.0832969022208245 	 ± 0.23468766618627296
	data : 0.11781587600708007
	model : 0.06826181411743164
			 train-loss:  2.0841061700567156 	 ± 0.23450768271271438
	data : 0.11777667999267578
	model : 0.06826548576354981
			 train-loss:  2.083605019455282 	 ± 0.2341310641136715
	data : 0.11760034561157226
	model : 0.06816229820251465
			 train-loss:  2.084244057472716 	 ± 0.2338367984841645
	data : 0.11754746437072754
	model : 0.06850953102111816
			 train-loss:  2.083088572247554 	 ± 0.23401220958536548
	data : 0.11711902618408203
	model : 0.06798796653747559
			 train-loss:  2.0847976092044815 	 ± 0.23498928204749966
	data : 0.11737775802612305
	model : 0.06772799491882324
			 train-loss:  2.0853019797501444 	 ± 0.23462360574697705
	data : 0.11734309196472167
	model : 0.06786890029907226
			 train-loss:  2.0844886911464036 	 ± 0.23446818836012032
	data : 0.11725025177001953
	model : 0.06698102951049804
			 train-loss:  2.0833899155259132 	 ± 0.2345950000570768
	data : 0.11788806915283204
	model : 0.06702280044555664
			 train-loss:  2.082828913486845 	 ± 0.23426904920748706
	data : 0.11775074005126954
	model : 0.0679412841796875
			 train-loss:  2.081871098230693 	 ± 0.2342569063448412
	data : 0.1171231746673584
	model : 0.06725564002990722
			 train-loss:  2.0839451835969838 	 ± 0.23599049275031395
	data : 0.1179893970489502
	model : 0.06673655509948731
			 train-loss:  2.087116384603938 	 ± 0.2406387377584875
	data : 0.11841464042663574
	model : 0.06755695343017579
			 train-loss:  2.0884934391294205 	 ± 0.24110856292526434
	data : 0.11761012077331542
	model : 0.06759681701660156
			 train-loss:  2.0880887038339444 	 ± 0.2407013883233544
	data : 0.11781225204467774
	model : 0.06734848022460938
			 train-loss:  2.0861723688449936 	 ± 0.24208674175800643
	data : 0.1178678035736084
	model : 0.06813249588012696
			 train-loss:  2.0841498124984 	 ± 0.24368029751767953
	data : 0.11716198921203613
	model : 0.06880064010620117
			 train-loss:  2.084450843343773 	 ± 0.24323668885481073
	data : 0.11689667701721192
	model : 0.06890525817871093
			 train-loss:  2.083440508842468 	 ± 0.24327269397357146
	data : 0.11686372756958008
	model : 0.0689742088317871
			 train-loss:  2.083398614746641 	 ± 0.2427885069985156
	data : 0.11673197746276856
	model : 0.06958098411560058
			 train-loss:  2.0815200062971266 	 ± 0.2441273581559691
	data : 0.11631255149841309
	model : 0.06929206848144531
			 train-loss:  2.0832449857425313 	 ± 0.24517838557402008
	data : 0.11645345687866211
	model : 0.068902587890625
			 train-loss:  2.0845431141027317 	 ± 0.24556489431863213
	data : 0.1166083812713623
	model : 0.06889514923095703
			 train-loss:  2.0846476624993717 	 ± 0.24508858550234894
	data : 0.11678667068481445
	model : 0.06896657943725586
			 train-loss:  2.0835797684267163 	 ± 0.24520312788544404
	data : 0.11560425758361817
	model : 0.06003603935241699
#epoch  75    val-loss:  2.4732997982125533  train-loss:  2.0835797684267163  lr:  9.765625e-06
			 train-loss:  1.6940559148788452 	 ± 0.0
	data : 5.130017518997192
	model : 0.0995488166809082
			 train-loss:  2.014630138874054 	 ± 0.32057422399520874
	data : 2.7839901447296143
	model : 0.08224916458129883
			 train-loss:  1.9428226550420125 	 ± 0.28075704478308516
	data : 1.896004358927409
	model : 0.07718531290690105
			 train-loss:  2.0193881690502167 	 ± 0.2769570769135896
	data : 1.451627790927887
	model : 0.07450515031814575
			 train-loss:  2.0952728033065795 	 ± 0.2905134916435102
	data : 1.1850854873657226
	model : 0.07292633056640625
			 train-loss:  2.0759122570355735 	 ± 0.2687115450602103
	data : 0.1828329086303711
	model : 0.06709098815917969
			 train-loss:  2.0258037192480907 	 ± 0.2774093726344968
	data : 0.11845426559448242
	model : 0.06797604560852051
			 train-loss:  2.061445251107216 	 ± 0.27609544830572147
	data : 0.11763849258422851
	model : 0.06844916343688964
			 train-loss:  2.0855751699871488 	 ± 0.269103818778811
	data : 0.11728444099426269
	model : 0.06904926300048828
			 train-loss:  2.0789988160133364 	 ± 0.2560554913246446
	data : 0.11676397323608398
	model : 0.06973743438720703
			 train-loss:  2.0627268986268477 	 ± 0.24950303858099593
	data : 0.11621613502502441
	model : 0.0694190502166748
			 train-loss:  2.0369850397109985 	 ± 0.2536793160224126
	data : 0.11663532257080078
	model : 0.07005934715270996
			 train-loss:  2.034071234556345 	 ± 0.24393612423147512
	data : 0.11614208221435547
	model : 0.07060842514038086
			 train-loss:  2.0423180801527843 	 ± 0.23693590593585379
	data : 0.11572346687316895
	model : 0.07059431076049805
			 train-loss:  2.045768713951111 	 ± 0.22926566435195436
	data : 0.11577649116516113
	model : 0.07057309150695801
			 train-loss:  2.0666592493653297 	 ± 0.23627058713678808
	data : 0.11595339775085449
	model : 0.07031712532043458
			 train-loss:  2.0520695868660423 	 ± 0.23652858178698097
	data : 0.11595873832702637
	model : 0.06973028182983398
			 train-loss:  2.05283651749293 	 ± 0.22988621266330211
	data : 0.11643996238708496
	model : 0.06962556838989258
			 train-loss:  2.0837155706004094 	 ± 0.25928652296423277
	data : 0.11642789840698242
	model : 0.0698617935180664
			 train-loss:  2.093709236383438 	 ± 0.2564480838322102
	data : 0.11620626449584961
	model : 0.06980195045471191
			 train-loss:  2.0957322177432833 	 ± 0.2504311733722655
	data : 0.1161339282989502
	model : 0.07007775306701661
			 train-loss:  2.086055625568737 	 ± 0.24865924639707762
	data : 0.11596136093139649
	model : 0.06996841430664062
			 train-loss:  2.1025036832560664 	 ± 0.2551371149236318
	data : 0.1160271167755127
	model : 0.06934680938720703
			 train-loss:  2.1004701058069863 	 ± 0.24995554203222903
	data : 0.11653285026550293
	model : 0.06826491355895996
			 train-loss:  2.1042184352874758 	 ± 0.24559287743414646
	data : 0.11769661903381348
	model : 0.06754903793334961
			 train-loss:  2.09150525698295 	 ± 0.2490715623803513
	data : 0.11829648017883301
	model : 0.0669675350189209
			 train-loss:  2.0831726524564953 	 ± 0.24808109458055372
	data : 0.11893043518066407
	model : 0.06738834381103516
			 train-loss:  2.0806599812848225 	 ± 0.2439604194035033
	data : 0.11876654624938965
	model : 0.0674224853515625
			 train-loss:  2.085387554661981 	 ± 0.24101905590884232
	data : 0.11872220039367676
	model : 0.06818718910217285
			 train-loss:  2.082868675390879 	 ± 0.23735594290328155
	data : 0.11787490844726563
	model : 0.06853408813476562
			 train-loss:  2.081280158412072 	 ± 0.233658287725259
	data : 0.1174229621887207
	model : 0.06883382797241211
			 train-loss:  2.088061448186636 	 ± 0.23305713234515124
	data : 0.1170985221862793
	model : 0.0679661750793457
			 train-loss:  2.0921704371770224 	 ± 0.230672885586249
	data : 0.11776223182678222
	model : 0.06824545860290528
			 train-loss:  2.0893821190385258 	 ± 0.22781910899666646
	data : 0.1174863338470459
	model : 0.06840448379516602
			 train-loss:  2.0888575315475464 	 ± 0.22456179912631516
	data : 0.11722497940063477
	model : 0.06878619194030762
			 train-loss:  2.084313174088796 	 ± 0.22304711445977293
	data : 0.11707935333251954
	model : 0.06938157081604004
			 train-loss:  2.0709393604381665 	 ± 0.23418868537525228
	data : 0.11641845703125
	model : 0.0691788673400879
			 train-loss:  2.0691012081347013 	 ± 0.23135704870841386
	data : 0.11683492660522461
	model : 0.06824913024902343
			 train-loss:  2.071289380391439 	 ± 0.22876968333336303
	data : 0.11766247749328614
	model : 0.06854267120361328
			 train-loss:  2.060621103644371 	 ± 0.235511893188996
	data : 0.11735696792602539
	model : 0.06881718635559082
			 train-loss:  2.076844061293253 	 ± 0.25424476469356677
	data : 0.11712989807128907
	model : 0.0677879810333252
			 train-loss:  2.088041302703676 	 ± 0.2612314109092891
	data : 0.11798763275146484
	model : 0.06875109672546387
			 train-loss:  2.0802257864974267 	 ± 0.2630974849881697
	data : 0.11699538230895996
	model : 0.07003841400146485
			 train-loss:  2.080191972580823 	 ± 0.2600906523512557
	data : 0.11583476066589356
	model : 0.069740629196167
			 train-loss:  2.07881281375885 	 ± 0.25734717596246015
	data : 0.1160661220550537
	model : 0.06951899528503418
			 train-loss:  2.073090058305989 	 ± 0.2574132593016627
	data : 0.1163238525390625
	model : 0.06956028938293457
			 train-loss:  2.0776001874436725 	 ± 0.25649066896631556
	data : 0.11632914543151855
	model : 0.06939053535461426
			 train-loss:  2.0803911462426186 	 ± 0.25452503867638476
	data : 0.11641645431518555
	model : 0.06971559524536133
			 train-loss:  2.0733980329669253 	 ± 0.25653122181482724
	data : 0.11615900993347168
	model : 0.06979556083679199
			 train-loss:  2.086153585910797 	 ± 0.2691925051422941
	data : 0.11614384651184081
	model : 0.06964335441589356
			 train-loss:  2.0860901276270547 	 ± 0.26654067526001596
	data : 0.11599187850952149
	model : 0.07050285339355469
			 train-loss:  2.0868551570635576 	 ± 0.26402187577321645
	data : 0.11523909568786621
	model : 0.07049331665039063
			 train-loss:  2.089585675383514 	 ± 0.26225943653755296
	data : 0.1151970386505127
	model : 0.06927609443664551
			 train-loss:  2.089329916018027 	 ± 0.2598264324646326
	data : 0.11641669273376465
	model : 0.06876473426818848
			 train-loss:  2.094548691402782 	 ± 0.26029415637795755
	data : 0.11699609756469727
	model : 0.06813106536865235
			 train-loss:  2.0905751224075044 	 ± 0.25963740248846856
	data : 0.11760587692260742
	model : 0.06785249710083008
			 train-loss:  2.0964666479512264 	 ± 0.26109899570890394
	data : 0.117759370803833
	model : 0.06774435043334961
			 train-loss:  2.095874985744213 	 ± 0.25887689755437976
	data : 0.11779522895812988
	model : 0.06821956634521484
			 train-loss:  2.100110132815474 	 ± 0.25869224539606894
	data : 0.11725320816040039
	model : 0.06851139068603515
			 train-loss:  2.096117579936981 	 ± 0.2583540298924943
	data : 0.11704225540161133
	model : 0.06932463645935058
			 train-loss:  2.0976464005767324 	 ± 0.2565011352810693
	data : 0.11657986640930176
	model : 0.06948995590209961
			 train-loss:  2.0962262576626194 	 ± 0.2546658258267636
	data : 0.11642689704895019
	model : 0.06980042457580567
			 train-loss:  2.099014187616015 	 ± 0.25358852841987445
	data : 0.11602892875671386
	model : 0.07020430564880371
			 train-loss:  2.0961132496595383 	 ± 0.25265097553956817
	data : 0.11574897766113282
	model : 0.0703216552734375
			 train-loss:  2.1009328842163084 	 ± 0.25364763922949063
	data : 0.11561045646667481
	model : 0.07027864456176758
			 train-loss:  2.1025178251844463 	 ± 0.2520428583115964
	data : 0.1155698299407959
	model : 0.07040629386901856
			 train-loss:  2.1033340425633673 	 ± 0.25024274019345155
	data : 0.11562299728393555
	model : 0.06935000419616699
			 train-loss:  2.103051820222069 	 ± 0.2484066467805123
	data : 0.11671562194824218
	model : 0.06903223991394043
			 train-loss:  2.105161704878876 	 ± 0.247213033655793
	data : 0.11694202423095704
	model : 0.06904797554016114
			 train-loss:  2.1057310683386667 	 ± 0.24548643723609692
	data : 0.11689367294311523
	model : 0.06833028793334961
			 train-loss:  2.1034226266431135 	 ± 0.2445155028034827
	data : 0.11746530532836914
	model : 0.06751227378845215
			 train-loss:  2.103041210108333 	 ± 0.24283280986454764
	data : 0.11828069686889649
	model : 0.0675623893737793
			 train-loss:  2.101100898768804 	 ± 0.241725178683735
	data : 0.11839351654052735
	model : 0.06753625869750976
			 train-loss:  2.102894557488931 	 ± 0.24057495611704868
	data : 0.11836080551147461
	model : 0.06742429733276367
			 train-loss:  2.102379732131958 	 ± 0.239006775479014
	data : 0.11836161613464355
	model : 0.06723299026489257
			 train-loss:  2.096780521304984 	 ± 0.24233023159766817
	data : 0.11856532096862793
	model : 0.06809811592102051
			 train-loss:  2.0978218905337447 	 ± 0.24092262398959507
	data : 0.11765031814575196
	model : 0.06872019767761231
			 train-loss:  2.092890357359862 	 ± 0.24325337019142015
	data : 0.11705608367919922
	model : 0.06859230995178223
			 train-loss:  2.0926283250881146 	 ± 0.24171996699486875
	data : 0.11717700958251953
	model : 0.06854228973388672
			 train-loss:  2.0971110582351686 	 ± 0.24348651396822438
	data : 0.11715097427368164
	model : 0.06997218132019042
			 train-loss:  2.0947758268426964 	 ± 0.24287862180712608
	data : 0.1155017375946045
	model : 0.06973028182983398
			 train-loss:  2.0874456792342952 	 ± 0.25024558050808365
	data : 0.11567983627319336
	model : 0.06972970962524414
			 train-loss:  2.0835843373493974 	 ± 0.25117916952412545
	data : 0.11553106307983399
	model : 0.06985936164855958
			 train-loss:  2.0821744955721355 	 ± 0.25000973467680065
	data : 0.1156240463256836
	model : 0.07052488327026367
			 train-loss:  2.0813878858790678 	 ± 0.24863927979217765
	data : 0.11510787010192872
	model : 0.07019639015197754
			 train-loss:  2.079255536545155 	 ± 0.24797000739347527
	data : 0.11585025787353516
	model : 0.07018513679504394
			 train-loss:  2.0835726589992127 	 ± 0.24977025843831557
	data : 0.11583166122436524
	model : 0.0694589614868164
			 train-loss:  2.0835301063277503 	 ± 0.24834737166136592
	data : 0.11633954048156739
	model : 0.06939401626586914
			 train-loss:  2.085298045297687 	 ± 0.24750449911722933
	data : 0.11635513305664062
	model : 0.06895818710327148
			 train-loss:  2.08686527411143 	 ± 0.2465693202441508
	data : 0.11693453788757324
	model : 0.068792724609375
			 train-loss:  2.083116140994397 	 ± 0.24777686932679013
	data : 0.11670942306518554
	model : 0.06905336380004883
			 train-loss:  2.080385639615681 	 ± 0.24779935572224882
	data : 0.11656370162963867
	model : 0.07034382820129395
			 train-loss:  2.079536295706226 	 ± 0.24659810311517089
	data : 0.11562399864196778
	model : 0.07043166160583496
			 train-loss:  2.0777245316099613 	 ± 0.24590440096358526
	data : 0.11543664932250977
	model : 0.07048850059509278
			 train-loss:  2.0763798425072117 	 ± 0.24495393166619545
	data : 0.11536688804626465
	model : 0.0705254077911377
			 train-loss:  2.0762142377595105 	 ± 0.2436801360574778
	data : 0.11551132202148437
	model : 0.07037458419799805
			 train-loss:  2.074720702220484 	 ± 0.2428620717974753
	data : 0.11559863090515136
	model : 0.06993403434753417
			 train-loss:  2.0749903090146122 	 ± 0.24163439249716687
	data : 0.11591753959655762
	model : 0.06905784606933593
			 train-loss:  2.0759668374302414 	 ± 0.24060520298893637
	data : 0.11663269996643066
	model : 0.06896262168884278
			 train-loss:  2.0775429725646974 	 ± 0.23991225700772026
	data : 0.11657013893127441
	model : 0.06894135475158691
			 train-loss:  2.0767488821898357 	 ± 0.23885365583418047
	data : 0.11674957275390625
	model : 0.06905694007873535
			 train-loss:  2.0794651964131523 	 ± 0.23924246980696745
	data : 0.11671361923217774
	model : 0.06902565956115722
			 train-loss:  2.080527387776421 	 ± 0.23831983189610081
	data : 0.11672410964965821
	model : 0.06992902755737304
			 train-loss:  2.0799373468527427 	 ± 0.23724688169352426
	data : 0.115848970413208
	model : 0.06997923851013184
			 train-loss:  2.0780947719301497 	 ± 0.23686095816975156
	data : 0.11591229438781739
	model : 0.06923432350158691
			 train-loss:  2.076056050804426 	 ± 0.23666486674488438
	data : 0.11653833389282227
	model : 0.06935625076293946
			 train-loss:  2.078353583255661 	 ± 0.23674107428603877
	data : 0.11652803421020508
	model : 0.06968846321105956
			 train-loss:  2.0818982389238148 	 ± 0.2384780894829065
	data : 0.11631898880004883
	model : 0.06955070495605468
			 train-loss:  2.079339830153579 	 ± 0.23886596393189102
	data : 0.11648211479187012
	model : 0.06943879127502442
			 train-loss:  2.0799613454125145 	 ± 0.2378662520071092
	data : 0.1165283203125
	model : 0.07008247375488282
			 train-loss:  2.077128131110389 	 ± 0.23864953862807153
	data : 0.1158297061920166
	model : 0.07000012397766113
			 train-loss:  2.0776175068957463 	 ± 0.23763768897722357
	data : 0.11589040756225585
	model : 0.0701972484588623
			 train-loss:  2.0728667816229627 	 ± 0.2418670991861824
	data : 0.11592926979064941
	model : 0.0706510066986084
			 train-loss:  2.0723765021876286 	 ± 0.24086033445673552
	data : 0.11549854278564453
	model : 0.07073235511779785
			 train-loss:  2.075393937981647 	 ± 0.2419652753842408
	data : 0.11539912223815918
	model : 0.07067413330078125
			 train-loss:  2.0770722257679908 	 ± 0.24159137409200435
	data : 0.11550707817077636
	model : 0.07060461044311524
			 train-loss:  2.0755028531082673 	 ± 0.24114981635345684
	data : 0.11541185379028321
	model : 0.07021288871765137
			 train-loss:  2.0741272114091 	 ± 0.24058640766023753
	data : 0.11561493873596192
	model : 0.06992907524108886
			 train-loss:  2.0725668348184154 	 ± 0.2401722739373846
	data : 0.11604146957397461
	model : 0.07006206512451171
			 train-loss:  2.076527912418048 	 ± 0.24304147378476804
	data : 0.11596770286560058
	model : 0.07030224800109863
			 train-loss:  2.072972135110335 	 ± 0.2451493613933818
	data : 0.11578655242919922
	model : 0.07024245262145996
			 train-loss:  2.0730427024794404 	 ± 0.24414381759132242
	data : 0.11603474617004395
	model : 0.07027988433837891
			 train-loss:  2.071285416440266 	 ± 0.24392282181353
	data : 0.11599225997924804
	model : 0.07023539543151855
			 train-loss:  2.073670418031754 	 ± 0.24437301459742095
	data : 0.11625442504882813
	model : 0.07005209922790527
			 train-loss:  2.073752582550049 	 ± 0.24339527938381242
	data : 0.11625499725341797
	model : 0.07041211128234863
			 train-loss:  2.075008517219907 	 ± 0.24283382129957173
	data : 0.1158968448638916
	model : 0.07045021057128906
			 train-loss:  2.073521709817601 	 ± 0.24245099061302824
	data : 0.11566376686096191
	model : 0.0703801155090332
			 train-loss:  2.0744619257748127 	 ± 0.24173438619659773
	data : 0.11579084396362305
	model : 0.07028861045837402
			 train-loss:  2.071480312088663 	 ± 0.24314696702927416
	data : 0.11565661430358887
	model : 0.06950125694274903
			 train-loss:  2.0703042892309336 	 ± 0.24257799929334573
	data : 0.11651034355163574
	model : 0.06937184333801269
			 train-loss:  2.0699123280648966 	 ± 0.2416916767478632
	data : 0.11665306091308594
	model : 0.06992158889770508
			 train-loss:  2.0671177370981737 	 ± 0.24288969823127765
	data : 0.1161642074584961
	model : 0.0698979377746582
			 train-loss:  2.0706479414961394 	 ± 0.245350484648905
	data : 0.11620531082153321
	model : 0.06968879699707031
			 train-loss:  2.069313041309812 	 ± 0.24491759940450927
	data : 0.11642775535583497
	model : 0.07049612998962403
			 train-loss:  2.0684944823936178 	 ± 0.24419272120087723
	data : 0.11571202278137208
	model : 0.0692359447479248
			 train-loss:  2.0668677468510235 	 ± 0.24402638142307695
	data : 0.11666584014892578
	model : 0.0686455249786377
			 train-loss:  2.0688920081966984 	 ± 0.24427748511459582
	data : 0.11727137565612793
	model : 0.06870541572570801
			 train-loss:  2.071407731892406 	 ± 0.24516554047641217
	data : 0.11710543632507324
	model : 0.06881561279296874
			 train-loss:  2.074631355649276 	 ± 0.2471998896945218
	data : 0.11685857772827149
	model : 0.0688169002532959
			 train-loss:  2.0730680329459052 	 ± 0.24700407730986623
	data : 0.11703929901123047
	model : 0.06972675323486328
			 train-loss:  2.0739737825190767 	 ± 0.24635982931431208
	data : 0.11630325317382813
	model : 0.06984052658081055
			 train-loss:  2.0764135743530705 	 ± 0.24719438345676895
	data : 0.1161074161529541
	model : 0.06986703872680664
			 train-loss:  2.0768125540726667 	 ± 0.246374429115894
	data : 0.11602315902709961
	model : 0.07082509994506836
			 train-loss:  2.0754864058560796 	 ± 0.24602910138704956
	data : 0.11504940986633301
	model : 0.07000374794006348
			 train-loss:  2.073644888812098 	 ± 0.24617310758625793
	data : 0.11567606925964355
	model : 0.06984596252441407
			 train-loss:  2.0752802295227575 	 ± 0.24611765760237753
	data : 0.11607747077941895
	model : 0.06971926689147949
			 train-loss:  2.0742586080719825 	 ± 0.2455895269429813
	data : 0.11614222526550293
	model : 0.06919231414794921
			 train-loss:  2.0788586751834766 	 ± 0.25103246407898494
	data : 0.11666584014892578
	model : 0.06834802627563477
			 train-loss:  2.0800036711980834 	 ± 0.2505761231338471
	data : 0.11762199401855469
	model : 0.06951050758361817
			 train-loss:  2.0787724033991495 	 ± 0.2501913092683302
	data : 0.11659417152404786
	model : 0.06909480094909667
			 train-loss:  2.0768991116656372 	 ± 0.25041472304895007
	data : 0.11680622100830078
	model : 0.0692563533782959
			 train-loss:  2.077153786232597 	 ± 0.24960925000177048
	data : 0.11659507751464844
	model : 0.06875066757202149
			 train-loss:  2.079337441064174 	 ± 0.2502445721241946
	data : 0.11689677238464355
	model : 0.06796073913574219
			 train-loss:  2.081363722875521 	 ± 0.250686851308186
	data : 0.1176544189453125
	model : 0.0674631118774414
			 train-loss:  2.08173432811614 	 ± 0.24991919603937995
	data : 0.11796755790710449
	model : 0.0680262565612793
			 train-loss:  2.081083607215148 	 ± 0.24924858136132105
	data : 0.11768174171447754
	model : 0.06803035736083984
			 train-loss:  2.083205646010721 	 ± 0.24986322495450147
	data : 0.11801214218139648
	model : 0.06809282302856445
			 train-loss:  2.085245831857754 	 ± 0.2503796807451232
	data : 0.11796274185180664
	model : 0.06863236427307129
			 train-loss:  2.0869858647292516 	 ± 0.25054757386971543
	data : 0.1174666404724121
	model : 0.06880531311035157
			 train-loss:  2.0899703688919544 	 ± 0.252582667418271
	data : 0.11735515594482422
	model : 0.06887407302856445
			 train-loss:  2.0881229394711323 	 ± 0.25287906647417574
	data : 0.1171372413635254
	model : 0.06852660179138184
			 train-loss:  2.0865733638221835 	 ± 0.2528629536975461
	data : 0.11739745140075683
	model : 0.06927275657653809
			 train-loss:  2.0944232545747345 	 ± 0.2711641063266686
	data : 0.11667799949645996
	model : 0.06946210861206055
			 train-loss:  2.0987824681328564 	 ± 0.2760055545682143
	data : 0.11640243530273438
	model : 0.06854777336120606
			 train-loss:  2.0992089141498913 	 ± 0.2752220907924821
	data : 0.11732048988342285
	model : 0.06839928627014161
			 train-loss:  2.098475947437516 	 ± 0.27455333810417654
	data : 0.11742053031921387
	model : 0.06877403259277344
			 train-loss:  2.0973246097564697 	 ± 0.2741317329751364
	data : 0.11711201667785645
	model : 0.06903996467590331
			 train-loss:  2.0969500953242894 	 ± 0.27335749461988734
	data : 0.11710772514343262
	model : 0.06915221214294434
			 train-loss:  2.0955517905703664 	 ± 0.27314949600002947
	data : 0.11709527969360352
	model : 0.0699091911315918
			 train-loss:  2.096513075688306 	 ± 0.2726314887892683
	data : 0.1162482738494873
	model : 0.06927495002746582
			 train-loss:  2.0968105869683606 	 ± 0.27186082769197295
	data : 0.11660194396972656
	model : 0.06862735748291016
			 train-loss:  2.095922580985136 	 ± 0.27131799239976623
	data : 0.11715826988220215
	model : 0.06789669990539551
			 train-loss:  2.093873394707035 	 ± 0.27186430068699474
	data : 0.11770148277282715
	model : 0.06699466705322266
			 train-loss:  2.0939534285972856 	 ± 0.27108399966339725
	data : 0.118560791015625
	model : 0.06717729568481445
			 train-loss:  2.093241982460022 	 ± 0.27047122377910804
	data : 0.11847343444824218
	model : 0.06787676811218261
			 train-loss:  2.0940564647316933 	 ± 0.26991688169717337
	data : 0.11803927421569824
	model : 0.0682650089263916
			 train-loss:  2.0925906256767317 	 ± 0.2698549250400039
	data : 0.11769123077392578
	model : 0.06825528144836426
			 train-loss:  2.090276775735148 	 ± 0.27085090101029563
	data : 0.11784420013427735
	model : 0.06942462921142578
			 train-loss:  2.091714239653262 	 ± 0.27077329788453525
	data : 0.11663398742675782
	model : 0.06937360763549805
			 train-loss:  2.092422694630093 	 ± 0.27018641202834426
	data : 0.11679506301879883
	model : 0.06949982643127442
			 train-loss:  2.09262349592388 	 ± 0.26945247500100605
	data : 0.11663279533386231
	model : 0.06928229331970215
			 train-loss:  2.0939898752904202 	 ± 0.26933925701045935
	data : 0.11675028800964356
	model : 0.07010698318481445
			 train-loss:  2.0940133847825515 	 ± 0.2686025365659123
	data : 0.11611895561218262
	model : 0.06973152160644532
			 train-loss:  2.0947945675124293 	 ± 0.26808001181915275
	data : 0.11656160354614258
	model : 0.07003769874572754
			 train-loss:  2.095637241569725 	 ± 0.2675987319977162
	data : 0.11603322029113769
	model : 0.0698099136352539
			 train-loss:  2.0964799068307363 	 ± 0.2671244129295656
	data : 0.11631484031677246
	model : 0.07019972801208496
			 train-loss:  2.0963180039655716 	 ± 0.26641836940770086
	data : 0.11596269607543945
	model : 0.06991486549377442
			 train-loss:  2.097275525965589 	 ± 0.26603129830050426
	data : 0.11615576744079589
	model : 0.06995162963867188
			 train-loss:  2.0973395519155673 	 ± 0.2653280306970326
	data : 0.11571440696716309
	model : 0.06966981887817383
			 train-loss:  2.0951603418902347 	 ± 0.26631934919996986
	data : 0.11612391471862793
	model : 0.06984133720397949
			 train-loss:  2.095682944926916 	 ± 0.2657189250200622
	data : 0.11610975265502929
	model : 0.0696044921875
			 train-loss:  2.096124926581979 	 ± 0.26509642794191834
	data : 0.11629014015197754
	model : 0.0697260856628418
			 train-loss:  2.0954947076313237 	 ± 0.2645529227420978
	data : 0.11616568565368653
	model : 0.06972880363464355
			 train-loss:  2.0975647746902153 	 ± 0.26543271095307813
	data : 0.11654477119445801
	model : 0.06965670585632325
			 train-loss:  2.097497694308941 	 ± 0.26475288808479164
	data : 0.11659426689147949
	model : 0.06970939636230469
			 train-loss:  2.096873869093097 	 ± 0.2642202767060758
	data : 0.1165226936340332
	model : 0.070042085647583
			 train-loss:  2.0965119551886158 	 ± 0.263597514396773
	data : 0.11631264686584472
	model : 0.07012453079223632
			 train-loss:  2.093886596385879 	 ± 0.2655005580314971
	data : 0.11623196601867676
	model : 0.06970529556274414
			 train-loss:  2.092727778544977 	 ± 0.2653341450026902
	data : 0.11670026779174805
	model : 0.06894598007202149
			 train-loss:  2.0944243454933167 	 ± 0.265749857957472
	data : 0.1175072193145752
	model : 0.06919474601745605
			 train-loss:  2.095319657776486 	 ± 0.2653901763008473
	data : 0.11712841987609864
	model : 0.06818265914916992
			 train-loss:  2.09676453382662 	 ± 0.2655238100537206
	data : 0.1180051326751709
	model : 0.06812934875488282
			 train-loss:  2.0964854714905687 	 ± 0.264898697088904
	data : 0.11807408332824706
	model : 0.06908936500549316
			 train-loss:  2.0977870716768154 	 ± 0.2648985801397448
	data : 0.11714344024658203
	model : 0.06893973350524903
			 train-loss:  2.0969638876798675 	 ± 0.2645131298212785
	data : 0.11718134880065918
	model : 0.06844549179077149
			 train-loss:  2.0972598692745836 	 ± 0.26390435447803257
	data : 0.11773786544799805
	model : 0.06902079582214356
			 train-loss:  2.0973854427752285 	 ± 0.26327230187493367
	data : 0.11721515655517578
	model : 0.06900229454040527
			 train-loss:  2.098134656938223 	 ± 0.2628597845329415
	data : 0.11728420257568359
	model : 0.06852602958679199
			 train-loss:  2.0987336492994757 	 ± 0.2623724369457617
	data : 0.117645263671875
	model : 0.06959218978881836
			 train-loss:  2.099298059940338 	 ± 0.2618741461230059
	data : 0.11666326522827149
	model : 0.0696521282196045
			 train-loss:  2.098114676950102 	 ± 0.2618150814578275
	data : 0.11659154891967774
	model : 0.07049970626831055
			 train-loss:  2.0994002324230268 	 ± 0.2618635352313504
	data : 0.11572756767272949
	model : 0.0698014259338379
			 train-loss:  2.098692562098794 	 ± 0.2614512257326765
	data : 0.11626729965209961
	model : 0.07013330459594727
			 train-loss:  2.096641021911229 	 ± 0.2625524660966653
	data : 0.11611351966857911
	model : 0.07044658660888672
			 train-loss:  2.096757286093956 	 ± 0.2619466889555646
	data : 0.11594080924987793
	model : 0.0696037769317627
			 train-loss:  2.0966781306046025 	 ± 0.2613422047357554
	data : 0.11671853065490723
	model : 0.06910934448242187
			 train-loss:  2.0960444200972805 	 ± 0.2609056262699186
	data : 0.11728692054748535
	model : 0.06982879638671875
			 train-loss:  2.0946733339117207 	 ± 0.26108891873221596
	data : 0.11676325798034667
	model : 0.06955084800720215
			 train-loss:  2.09231784681207 	 ± 0.2628035204420945
	data : 0.11683635711669922
	model : 0.06845030784606934
			 train-loss:  2.090869135206396 	 ± 0.26308056751665626
	data : 0.11787524223327636
	model : 0.06935534477233887
			 train-loss:  2.092232817438393 	 ± 0.26326285295812496
	data : 0.11702871322631836
	model : 0.06932048797607422
			 train-loss:  2.090910133477804 	 ± 0.2634041998347397
	data : 0.11720266342163085
	model : 0.0684746265411377
			 train-loss:  2.0908759714776624 	 ± 0.2628134367747174
	data : 0.11780748367309571
	model : 0.06755628585815429
			 train-loss:  2.0897950311856612 	 ± 0.2627224973671479
	data : 0.11860203742980957
	model : 0.06813263893127441
			 train-loss:  2.091353775130378 	 ± 0.2631740681825776
	data : 0.11784214973449707
	model : 0.0679624080657959
			 train-loss:  2.090788305860705 	 ± 0.2627281339365756
	data : 0.11774101257324218
	model : 0.0671083927154541
			 train-loss:  2.089624151784418 	 ± 0.2627323358726149
	data : 0.11807327270507813
	model : 0.06731481552124023
			 train-loss:  2.0908520085769786 	 ± 0.262807452768109
	data : 0.1177678108215332
	model : 0.06751136779785157
			 train-loss:  2.091631330256899 	 ± 0.26249690539835396
	data : 0.1176513671875
	model : 0.06666455268859864
			 train-loss:  2.0916796715363213 	 ± 0.26192665989068364
	data : 0.11845803260803223
	model : 0.06597504615783692
			 train-loss:  2.090298007060955 	 ± 0.26219773182808875
	data : 0.11904630661010743
	model : 0.06596031188964843
			 train-loss:  2.092141089254412 	 ± 0.2631273843620632
	data : 0.11912689208984376
	model : 0.06586341857910157
			 train-loss:  2.0916564147359824 	 ± 0.26266588868232377
	data : 0.11915645599365235
	model : 0.0656764030456543
			 train-loss:  2.0903962249429817 	 ± 0.2628089569762244
	data : 0.11922655105590821
	model : 0.0660512924194336
			 train-loss:  2.08861088955656 	 ± 0.26366739655109755
	data : 0.11898961067199706
	model : 0.0664377212524414
			 train-loss:  2.088685884314068 	 ± 0.2631106978466362
	data : 0.11906609535217286
	model : 0.06667776107788086
			 train-loss:  2.0881828220584726 	 ± 0.2626687384721008
	data : 0.11878786087036133
	model : 0.06681590080261231
			 train-loss:  2.08669100639199 	 ± 0.2631205400358647
	data : 0.11866965293884277
	model : 0.06707134246826171
			 train-loss:  2.0867924196450782 	 ± 0.2625741627490812
	data : 0.11850938796997071
	model : 0.06757578849792481
			 train-loss:  2.0873712117473286 	 ± 0.2621792981636482
	data : 0.11797213554382324
	model : 0.06723923683166504
			 train-loss:  2.087376413008979 	 ± 0.2616348046917884
	data : 0.11813621520996094
	model : 0.06732439994812012
			 train-loss:  2.0868652773297525 	 ± 0.2612142261045924
	data : 0.11825370788574219
	model : 0.06769032478332519
			 train-loss:  2.0889059908596086 	 ± 0.26260215163220907
	data : 0.11787824630737305
	model : 0.06802549362182617
			 train-loss:  2.0889690049359055 	 ± 0.2620653209406585
	data : 0.11760625839233399
	model : 0.06714353561401368
			 train-loss:  2.089429957526071 	 ± 0.2616290457546763
	data : 0.11823825836181641
	model : 0.06698222160339355
			 train-loss:  2.0908316829340245 	 ± 0.26201696680682424
	data : 0.11817121505737305
	model : 0.06695523262023925
			 train-loss:  2.091764051421934 	 ± 0.26189462475062764
	data : 0.11795592308044434
	model : 0.06689662933349609
			 train-loss:  2.091902135841308 	 ± 0.261375087524461
	data : 0.11831426620483398
	model : 0.06683516502380371
			 train-loss:  2.092852008390618 	 ± 0.2612782637751252
	data : 0.11833462715148926
	model : 0.06750168800354003
			 train-loss:  2.0923761916160584 	 ± 0.2608632588081229
	data : 0.11770420074462891
	model : 0.06828999519348145
			 train-loss:  2.0905019168359824 	 ± 0.26202433509318923
	data : 0.1171147346496582
	model : 0.06820111274719239
			 train-loss:  2.091324115083331 	 ± 0.2618281559557373
	data : 0.11745696067810059
	model : 0.0678870677947998
			 train-loss:  2.0897477233833945 	 ± 0.2625056982429062
	data : 0.11760568618774414
	model : 0.06773905754089356
			 train-loss:  2.090312227958769 	 ± 0.26214226620927866
	data : 0.11807742118835449
	model : 0.06749000549316406
			 train-loss:  2.089527748145309 	 ± 0.26192631966400864
	data : 0.11824793815612793
	model : 0.0673290729522705
			 train-loss:  2.0929214181378484 	 ± 0.26697236015292025
	data : 0.11736397743225098
	model : 0.05905370712280274
#epoch  76    val-loss:  2.4388967313264547  train-loss:  2.0929214181378484  lr:  9.765625e-06
			 train-loss:  2.50639009475708 	 ± 0.0
	data : 5.72819185256958
	model : 0.07221126556396484
			 train-loss:  2.368809938430786 	 ± 0.13758015632629395
	data : 2.9285651445388794
	model : 0.07110047340393066
			 train-loss:  2.351434310277303 	 ± 0.11498996123126522
	data : 1.9908223152160645
	model : 0.06963364283243816
			 train-loss:  2.485370457172394 	 ± 0.2524553281736907
	data : 1.522683560848236
	model : 0.06899946928024292
			 train-loss:  2.4368147373199465 	 ± 0.24579988995153387
	data : 1.2418533325195313
	model : 0.06911396980285645
			 train-loss:  2.391239126523336 	 ± 0.2464420210669758
	data : 0.11935205459594726
	model : 0.06904544830322265
			 train-loss:  2.3546319689069475 	 ± 0.24514880204625583
	data : 0.1162801742553711
	model : 0.06891012191772461
			 train-loss:  2.3340079188346863 	 ± 0.23571837718198174
	data : 0.116412353515625
	model : 0.06940164566040039
			 train-loss:  2.320578310224745 	 ± 0.22546020654662546
	data : 0.1160064697265625
	model : 0.0694467544555664
			 train-loss:  2.308547616004944 	 ± 0.21691406753167378
	data : 0.11592154502868653
	model : 0.06950864791870118
			 train-loss:  2.286743857643821 	 ± 0.21800991503413136
	data : 0.1160628318786621
	model : 0.06823983192443847
			 train-loss:  2.271344264348348 	 ± 0.21488660189911601
	data : 0.11725831031799316
	model : 0.06749901771545411
			 train-loss:  2.299683295763456 	 ± 0.2286076130285994
	data : 0.11793751716613769
	model : 0.06767168045043945
			 train-loss:  2.2698438252721513 	 ± 0.24516036113486742
	data : 0.11797094345092773
	model : 0.06827225685119628
			 train-loss:  2.2749921719233197 	 ± 0.23762948565437264
	data : 0.11755223274230957
	model : 0.06739754676818847
			 train-loss:  2.2506131678819656 	 ± 0.24870378815935695
	data : 0.11834592819213867
	model : 0.06818337440490722
			 train-loss:  2.2380240945255054 	 ± 0.2464769461604696
	data : 0.11790924072265625
	model : 0.06912059783935547
			 train-loss:  2.2182551092571683 	 ± 0.2530210525978692
	data : 0.11713814735412598
	model : 0.06912055015563964
			 train-loss:  2.2019131936525045 	 ± 0.25584613605638384
	data : 0.11705875396728516
	model : 0.06907134056091309
			 train-loss:  2.2355995297431948 	 ± 0.28938728136066455
	data : 0.11711373329162597
	model : 0.0700075626373291
			 train-loss:  2.2264343216305686 	 ± 0.2853719718743358
	data : 0.11617722511291503
	model : 0.07013683319091797
			 train-loss:  2.217569047754461 	 ± 0.28175508484041434
	data : 0.11587119102478027
	model : 0.06999177932739258
			 train-loss:  2.219242697176726 	 ± 0.2756737020578223
	data : 0.11607794761657715
	model : 0.06986360549926758
			 train-loss:  2.206310381491979 	 ± 0.27690453331243886
	data : 0.11614737510681153
	model : 0.06983442306518554
			 train-loss:  2.203938465118408 	 ± 0.2715586482530385
	data : 0.11613879203796387
	model : 0.06976456642150879
			 train-loss:  2.1948809073521542 	 ± 0.27010881463084657
	data : 0.11635808944702149
	model : 0.07023429870605469
			 train-loss:  2.1957245491169117 	 ± 0.2650945114996363
	data : 0.11583929061889649
	model : 0.07064142227172851
			 train-loss:  2.1961310846464976 	 ± 0.26032621333041417
	data : 0.11528472900390625
	model : 0.07096667289733886
			 train-loss:  2.176117448971189 	 ± 0.2768539813957186
	data : 0.11501131057739258
	model : 0.07100973129272461
			 train-loss:  2.181275514761607 	 ± 0.2736142413796393
	data : 0.11495213508605957
	model : 0.07137227058410645
			 train-loss:  2.1693915397890153 	 ± 0.2769235036706105
	data : 0.11464285850524902
	model : 0.07089848518371582
			 train-loss:  2.170296937227249 	 ± 0.272608844088375
	data : 0.11518888473510742
	model : 0.07034468650817871
			 train-loss:  2.16863768750971 	 ± 0.2686106737522791
	data : 0.11581559181213379
	model : 0.06961646080017089
			 train-loss:  2.1589341584373924 	 ± 0.2704381909570243
	data : 0.11644620895385742
	model : 0.06962156295776367
			 train-loss:  2.157547242300851 	 ± 0.2666694434600758
	data : 0.11649279594421387
	model : 0.06851844787597657
			 train-loss:  2.1518037948343487 	 ± 0.2651259962799827
	data : 0.11738958358764648
	model : 0.0675769329071045
			 train-loss:  2.148731660198521 	 ± 0.2621674733711069
	data : 0.11815195083618164
	model : 0.06793079376220704
			 train-loss:  2.1443697810173035 	 ± 0.2600519458937274
	data : 0.11787428855895996
	model : 0.06848034858703614
			 train-loss:  2.1417208176392775 	 ± 0.2572151527687355
	data : 0.11736769676208496
	model : 0.06826195716857911
			 train-loss:  2.14510096013546 	 ± 0.2548553179982385
	data : 0.11755604743957519
	model : 0.06864066123962402
			 train-loss:  2.148164327551679 	 ± 0.2524726232518843
	data : 0.11742620468139649
	model : 0.0695082664489746
			 train-loss:  2.137984880379268 	 ± 0.2578240012652794
	data : 0.11653628349304199
	model : 0.06933460235595704
			 train-loss:  2.1369722970696383 	 ± 0.25489290013880184
	data : 0.11671466827392578
	model : 0.06937880516052246
			 train-loss:  2.129218182780526 	 ± 0.25705878930258963
	data : 0.11667046546936036
	model : 0.06866469383239746
			 train-loss:  2.131661817762587 	 ± 0.2547028337211806
	data : 0.11732964515686035
	model : 0.06813178062438965
			 train-loss:  2.137070199717646 	 ± 0.2545182038493266
	data : 0.11764507293701172
	model : 0.06902976036071777
			 train-loss:  2.1296366579989168 	 ± 0.2567938384552046
	data : 0.11683754920959473
	model : 0.06904425621032714
			 train-loss:  2.1350981493790946 	 ± 0.2568485392927055
	data : 0.11673536300659179
	model : 0.06806755065917969
			 train-loss:  2.1379820327369536 	 ± 0.25499809269809937
	data : 0.11769018173217774
	model : 0.0689770221710205
			 train-loss:  2.1344753456115724 	 ± 0.2536258910467255
	data : 0.11686201095581054
	model : 0.06994733810424805
			 train-loss:  2.1341357090893913 	 ± 0.25113853611919223
	data : 0.11594247817993164
	model : 0.0690241813659668
			 train-loss:  2.1263127349890194 	 ± 0.2549094230679779
	data : 0.11686906814575196
	model : 0.06821584701538086
			 train-loss:  2.1263624294748844 	 ± 0.25249341976995493
	data : 0.11763038635253906
	model : 0.06895313262939454
			 train-loss:  2.128220052630813 	 ± 0.2505098955095496
	data : 0.11700754165649414
	model : 0.06881546974182129
			 train-loss:  2.1257231213829733 	 ± 0.24889932887899274
	data : 0.11702017784118653
	model : 0.0684821605682373
			 train-loss:  2.126470670104027 	 ± 0.24672929675054328
	data : 0.11738123893737792
	model : 0.06767401695251465
			 train-loss:  2.138342261314392 	 ± 0.2601916588044125
	data : 0.11836962699890137
	model : 0.06845355033874512
			 train-loss:  2.130100258465471 	 ± 0.2653384780637373
	data : 0.11755313873291015
	model : 0.06874661445617676
			 train-loss:  2.131878444703959 	 ± 0.26342855593451137
	data : 0.11724691390991211
	model : 0.0688741683959961
			 train-loss:  2.126859680811564 	 ± 0.2640532499638945
	data : 0.11716980934143066
	model : 0.06910104751586914
			 train-loss:  2.1223215138325924 	 ± 0.2642286851327512
	data : 0.11687159538269043
	model : 0.06991896629333497
			 train-loss:  2.123389096029343 	 ± 0.26222174642024637
	data : 0.11604981422424317
	model : 0.06994609832763672
			 train-loss:  2.1329026581749084 	 ± 0.27070334216221476
	data : 0.11602535247802734
	model : 0.06958250999450684
			 train-loss:  2.1357343960553408 	 ± 0.26951897032016886
	data : 0.11623711585998535
	model : 0.06954069137573242
			 train-loss:  2.132330032495352 	 ± 0.26882088800453424
	data : 0.11621241569519043
	model : 0.06950559616088867
			 train-loss:  2.125837029832782 	 ± 0.2718641039567085
	data : 0.11620206832885742
	model : 0.06970229148864746
			 train-loss:  2.125469919460923 	 ± 0.2698441219574255
	data : 0.115971040725708
	model : 0.06885991096496583
			 train-loss:  2.118997433606316 	 ± 0.27304187358245485
	data : 0.11691427230834961
	model : 0.0683974266052246
			 train-loss:  2.123242737590403 	 ± 0.27330740956681204
	data : 0.11743125915527344
	model : 0.06829557418823243
			 train-loss:  2.126156987462725 	 ± 0.2724258574171247
	data : 0.11769256591796876
	model : 0.06761674880981446
			 train-loss:  2.1212200329337323 	 ± 0.27363607320693695
	data : 0.1182823657989502
	model : 0.06751418113708496
			 train-loss:  2.1222266074683933 	 ± 0.27186151444904433
	data : 0.11840753555297852
	model : 0.06768789291381835
			 train-loss:  2.1306383985362642 	 ± 0.27926837954103645
	data : 0.11830153465270996
	model : 0.06843686103820801
			 train-loss:  2.132357560299538 	 ± 0.277763659356659
	data : 0.11745448112487793
	model : 0.06815991401672364
			 train-loss:  2.134639801979065 	 ± 0.27660330232533065
	data : 0.11749000549316406
	model : 0.06901493072509765
			 train-loss:  2.131929620316154 	 ± 0.2757781096227151
	data : 0.11667757034301758
	model : 0.06894221305847167
			 train-loss:  2.1271739671756693 	 ± 0.2771005072121846
	data : 0.11661210060119628
	model : 0.06966395378112793
			 train-loss:  2.126175872790508 	 ± 0.27545776227136676
	data : 0.11610093116760253
	model : 0.06866235733032226
			 train-loss:  2.1237881907933875 	 ± 0.27451992766362193
	data : 0.11712450981140136
	model : 0.06903457641601562
			 train-loss:  2.125012990832329 	 ± 0.27301590883423
	data : 0.11681056022644043
	model : 0.06886215209960937
			 train-loss:  2.1240887524169167 	 ± 0.27145129481529406
	data : 0.11684651374816894
	model : 0.06905264854431152
			 train-loss:  2.121772354695855 	 ± 0.2705953060270121
	data : 0.11675519943237304
	model : 0.06902236938476562
			 train-loss:  2.1171833420374306 	 ± 0.27215155721477385
	data : 0.11663169860839843
	model : 0.07007970809936523
			 train-loss:  2.114368383373533 	 ± 0.27173961389907914
	data : 0.11592121124267578
	model : 0.07003154754638671
			 train-loss:  2.109522254326764 	 ± 0.2737634391977774
	data : 0.11597084999084473
	model : 0.06986432075500489
			 train-loss:  2.1071474385816 	 ± 0.27304638684467697
	data : 0.11639819145202637
	model : 0.06969890594482422
			 train-loss:  2.1105732917785645 	 ± 0.27332529975339415
	data : 0.11665425300598145
	model : 0.06978302001953125
			 train-loss:  2.1084591218016366 	 ± 0.2724823749699762
	data : 0.11660223007202149
	model : 0.07032542228698731
			 train-loss:  2.10727992620361 	 ± 0.2711729649235746
	data : 0.11606316566467285
	model : 0.07036867141723632
			 train-loss:  2.103649323516422 	 ± 0.2718287280280404
	data : 0.115974760055542
	model : 0.07051496505737305
			 train-loss:  2.105129518351712 	 ± 0.27069550761903893
	data : 0.11577081680297852
	model : 0.07058115005493164
			 train-loss:  2.1010553007540493 	 ± 0.2720112325208659
	data : 0.11564764976501465
	model : 0.07034196853637695
			 train-loss:  2.097344799708295 	 ± 0.2728757176577019
	data : 0.1157999038696289
	model : 0.06975364685058594
			 train-loss:  2.098525959126493 	 ± 0.2716592812885044
	data : 0.11634087562561035
	model : 0.07026591300964355
			 train-loss:  2.09677050113678 	 ± 0.2707611668659422
	data : 0.1159945011138916
	model : 0.07078013420104981
			 train-loss:  2.098499992241462 	 ± 0.26987423947931943
	data : 0.11559972763061524
	model : 0.07087407112121583
			 train-loss:  2.095347061599653 	 ± 0.2702509795847794
	data : 0.11533598899841309
	model : 0.07015376091003418
			 train-loss:  2.0937856277640985 	 ± 0.26930804696375144
	data : 0.11599750518798828
	model : 0.07011666297912597
			 train-loss:  2.092506691662952 	 ± 0.26824340882113296
	data : 0.11609630584716797
	model : 0.06876354217529297
			 train-loss:  2.097241085767746 	 ± 0.27102401254987124
	data : 0.11730546951293945
	model : 0.06801810264587402
			 train-loss:  2.096834646593226 	 ± 0.2697095978638781
	data : 0.11779575347900391
	model : 0.06778068542480468
			 train-loss:  2.0958200167207157 	 ± 0.2685778746386505
	data : 0.11807780265808106
	model : 0.06772241592407227
			 train-loss:  2.0969251023912894 	 ± 0.26750384629992335
	data : 0.11804871559143067
	model : 0.06788959503173828
			 train-loss:  2.0962708489252972 	 ± 0.26629745780994846
	data : 0.11781373023986816
	model : 0.06865077018737793
			 train-loss:  2.0954709382284253 	 ± 0.26515185563732235
	data : 0.11725740432739258
	model : 0.06857662200927735
			 train-loss:  2.0972163148646086 	 ± 0.2645035211682528
	data : 0.11744484901428223
	model : 0.06786742210388183
			 train-loss:  2.0937865872249426 	 ± 0.26562218007551447
	data : 0.11811709403991699
	model : 0.06893048286437989
			 train-loss:  2.090528293892189 	 ± 0.26652920489782106
	data : 0.11723766326904297
	model : 0.06795897483825683
			 train-loss:  2.0888604231930654 	 ± 0.265869380672899
	data : 0.11821775436401367
	model : 0.06724390983581544
			 train-loss:  2.0855876608328385 	 ± 0.26685467747645913
	data : 0.11888270378112793
	model : 0.06673641204833984
			 train-loss:  2.0855774267299756 	 ± 0.2656499314846248
	data : 0.11925883293151855
	model : 0.06667537689208984
			 train-loss:  2.087236867419311 	 ± 0.2650386072756448
	data : 0.11937179565429687
	model : 0.06609711647033692
			 train-loss:  2.0858881663432163 	 ± 0.2642490292743517
	data : 0.11980371475219727
	model : 0.06683535575866699
			 train-loss:  2.082669312493843 	 ± 0.2653032649259213
	data : 0.11894669532775878
	model : 0.06757187843322754
			 train-loss:  2.0851811616317084 	 ± 0.2655052575098625
	data : 0.11825032234191894
	model : 0.06752753257751465
			 train-loss:  2.0855789513423524 	 ± 0.26439277664994515
	data : 0.11839456558227539
	model : 0.06848597526550293
			 train-loss:  2.085542055276724 	 ± 0.2632607682526133
	data : 0.11739215850830079
	model : 0.06907873153686524
			 train-loss:  2.083977533599078 	 ± 0.2626885519703659
	data : 0.11692271232604981
	model : 0.06884846687316895
			 train-loss:  2.0834157366712556 	 ± 0.2616536675377483
	data : 0.11712117195129394
	model : 0.06897473335266113
			 train-loss:  2.084182796875636 	 ± 0.2606954873084465
	data : 0.11697611808776856
	model : 0.06991043090820312
			 train-loss:  2.0801662364281897 	 ± 0.2633180538584027
	data : 0.11605896949768066
	model : 0.06909050941467285
			 train-loss:  2.080552559407031 	 ± 0.2622710907811788
	data : 0.11695003509521484
	model : 0.0689164161682129
			 train-loss:  2.0824565257483383 	 ± 0.26204798985217315
	data : 0.11701350212097168
	model : 0.06840314865112304
			 train-loss:  2.0812309740051145 	 ± 0.2613428948838437
	data : 0.117344331741333
	model : 0.06816391944885254
			 train-loss:  2.0813818712234498 	 ± 0.2603008476992724
	data : 0.11755290031433105
	model : 0.06728882789611816
			 train-loss:  2.0870442891877796 	 ± 0.26688322018214233
	data : 0.1184499740600586
	model : 0.0683321475982666
			 train-loss:  2.085888467435762 	 ± 0.2661468388689299
	data : 0.11744499206542969
	model : 0.06838860511779785
			 train-loss:  2.087678057141602 	 ± 0.26587117613510014
	data : 0.11752028465270996
	model : 0.06919469833374023
			 train-loss:  2.0853378671084264 	 ± 0.26615880215106885
	data : 0.1168971061706543
	model : 0.069431734085083
			 train-loss:  2.085486750419323 	 ± 0.26513853062721565
	data : 0.11675724983215333
	model : 0.0703730583190918
			 train-loss:  2.0870539368563934 	 ± 0.2647283525421134
	data : 0.11582117080688477
	model : 0.07020444869995117
			 train-loss:  2.084293578610276 	 ± 0.26560938967272685
	data : 0.11611695289611816
	model : 0.0700258731842041
			 train-loss:  2.084429283787433 	 ± 0.2646135674423943
	data : 0.11623115539550781
	model : 0.06985206604003906
			 train-loss:  2.08630264695011 	 ± 0.2645081517307376
	data : 0.11632304191589356
	model : 0.06972131729125977
			 train-loss:  2.088493733935886 	 ± 0.26474444630011534
	data : 0.11634864807128906
	model : 0.06979146003723144
			 train-loss:  2.086781240561429 	 ± 0.26451873754141647
	data : 0.11631007194519043
	model : 0.06968488693237304
			 train-loss:  2.0917580005896355 	 ± 0.2698664456518167
	data : 0.11628570556640624
	model : 0.06980676651000976
			 train-loss:  2.0909330516621685 	 ± 0.26906020548254656
	data : 0.11625761985778808
	model : 0.07138504981994628
			 train-loss:  2.092114752145122 	 ± 0.2684497789181776
	data : 0.11463580131530762
	model : 0.0714491844177246
			 train-loss:  2.0946807776178633 	 ± 0.26919467942194897
	data : 0.11438064575195313
	model : 0.0703885555267334
			 train-loss:  2.09509502404125 	 ± 0.2682831671448593
	data : 0.11530051231384278
	model : 0.07009663581848144
			 train-loss:  2.093937274435876 	 ± 0.267690081438236
	data : 0.11547994613647461
	model : 0.07018370628356933
			 train-loss:  2.0928363124807396 	 ± 0.26707488761290366
	data : 0.11531877517700195
	model : 0.0688326358795166
			 train-loss:  2.095582169791063 	 ± 0.2681638250677855
	data : 0.1169853687286377
	model : 0.06927371025085449
			 train-loss:  2.096810090130773 	 ± 0.26764344732303386
	data : 0.11671791076660157
	model : 0.07027864456176758
			 train-loss:  2.1004402466016274 	 ± 0.2702835419093331
	data : 0.11587262153625488
	model : 0.07064156532287598
			 train-loss:  2.1000842358790286 	 ± 0.2693969879210306
	data : 0.1156071662902832
	model : 0.06988344192504883
			 train-loss:  2.0984806652004653 	 ± 0.26918835185991263
	data : 0.11626477241516113
	model : 0.07000436782836914
			 train-loss:  2.098606158422944 	 ± 0.26828785835297647
	data : 0.11607670783996582
	model : 0.06875081062316894
			 train-loss:  2.098631447950999 	 ± 0.26739224820695545
	data : 0.11736083030700684
	model : 0.0678670883178711
			 train-loss:  2.099438112303121 	 ± 0.26668843197651854
	data : 0.1179591178894043
	model : 0.06766161918640137
			 train-loss:  2.0962008747615313 	 ± 0.26876986970671013
	data : 0.1181905746459961
	model : 0.06832623481750488
			 train-loss:  2.0944600861056957 	 ± 0.26874842600814797
	data : 0.11749835014343261
	model : 0.06820120811462402
			 train-loss:  2.093393184921958 	 ± 0.26819931930873825
	data : 0.11760272979736328
	model : 0.06911988258361816
			 train-loss:  2.0940648386555334 	 ± 0.26746266468947505
	data : 0.11682281494140626
	model : 0.06975831985473632
			 train-loss:  2.0939469826527133 	 ± 0.26660807204653164
	data : 0.116424560546875
	model : 0.069952392578125
			 train-loss:  2.090696354580533 	 ± 0.26884105530838154
	data : 0.11610770225524902
	model : 0.07002348899841308
			 train-loss:  2.0926087189324294 	 ± 0.26905806740844485
	data : 0.1162154197692871
	model : 0.06984610557556152
			 train-loss:  2.094760037068301 	 ± 0.26957039400461724
	data : 0.11631550788879394
	model : 0.06979537010192871
			 train-loss:  2.0952726185321806 	 ± 0.2688043837289708
	data : 0.11625394821166993
	model : 0.07000727653503418
			 train-loss:  2.0971363508923453 	 ± 0.2690032763242239
	data : 0.11611566543579102
	model : 0.06895279884338379
			 train-loss:  2.097172482514087 	 ± 0.2681721259518044
	data : 0.11709156036376953
	model : 0.06898341178894044
			 train-loss:  2.0969539405378095 	 ± 0.2673627164099867
	data : 0.11713361740112305
	model : 0.0692631721496582
			 train-loss:  2.097921057445247 	 ± 0.2668321712666342
	data : 0.11684837341308593
	model : 0.06926860809326171
			 train-loss:  2.0988804412610604 	 ± 0.2663059227649791
	data : 0.11687607765197754
	model : 0.06999406814575196
			 train-loss:  2.0982659832540764 	 ± 0.2656198782487423
	data : 0.11627435684204102
	model : 0.07176723480224609
			 train-loss:  2.096404978615081 	 ± 0.2659066665427738
	data : 0.11468076705932617
	model : 0.0715113639831543
			 train-loss:  2.097605903233801 	 ± 0.26556794776341786
	data : 0.11481318473815919
	model : 0.07046399116516114
			 train-loss:  2.096266267567697 	 ± 0.2653497994589004
	data : 0.11578235626220704
	model : 0.0695347785949707
			 train-loss:  2.0944782109821545 	 ± 0.26558737601821686
	data : 0.11653294563293456
	model : 0.06889386177062988
			 train-loss:  2.0944347569816992 	 ± 0.2648102721730801
	data : 0.11713166236877441
	model : 0.06805119514465333
			 train-loss:  2.0947943433772687 	 ± 0.2640812194914581
	data : 0.11785712242126464
	model : 0.06823673248291015
			 train-loss:  2.0954193441854048 	 ± 0.2634444214401311
	data : 0.11769042015075684
	model : 0.06817607879638672
			 train-loss:  2.0958702050406357 	 ± 0.26275323451246413
	data : 0.1177065372467041
	model : 0.06953153610229493
			 train-loss:  2.094127254486084 	 ± 0.2630082559756956
	data : 0.11646018028259278
	model : 0.06869363784790039
			 train-loss:  2.0930671271952717 	 ± 0.2626347074984365
	data : 0.1170621395111084
	model : 0.06842575073242188
			 train-loss:  2.093007466213851 	 ± 0.2618929467107271
	data : 0.11768431663513183
	model : 0.06843490600585937
			 train-loss:  2.093208287539107 	 ± 0.26116992253678684
	data : 0.11770009994506836
	model : 0.06937546730041504
			 train-loss:  2.091338637820835 	 ± 0.26163119755631625
	data : 0.11683239936828613
	model : 0.06802477836608886
			 train-loss:  2.0937120642926956 	 ± 0.26282871851140965
	data : 0.11814780235290527
	model : 0.0685837745666504
			 train-loss:  2.0910592698260566 	 ± 0.26450709068955064
	data : 0.1176344871520996
	model : 0.06911196708679199
			 train-loss:  2.0899212746829776 	 ± 0.2642233608146087
	data : 0.11716742515563965
	model : 0.06911401748657227
			 train-loss:  2.08765966644704 	 ± 0.2652609920202986
	data : 0.1170238971710205
	model : 0.06918087005615234
			 train-loss:  2.0868493415739224 	 ± 0.26476621153274554
	data : 0.11719889640808105
	model : 0.06922178268432617
			 train-loss:  2.0900721852843827 	 ± 0.2676441264493351
	data : 0.11715469360351563
	model : 0.06942481994628906
			 train-loss:  2.0876687925349 	 ± 0.2689179570490069
	data : 0.11704740524291993
	model : 0.06934022903442383
			 train-loss:  2.0891320788286585 	 ± 0.2689394185811916
	data : 0.11698617935180664
	model : 0.06912879943847657
			 train-loss:  2.0897420600373695 	 ± 0.2683528715890585
	data : 0.11729426383972168
	model : 0.06823840141296386
			 train-loss:  2.089630777242953 	 ± 0.2676463511737634
	data : 0.11779251098632812
	model : 0.068988037109375
			 train-loss:  2.0892104205332305 	 ± 0.26700363574862457
	data : 0.11694464683532715
	model : 0.06876425743103028
			 train-loss:  2.089399126187669 	 ± 0.26631645906123413
	data : 0.11706929206848145
	model : 0.06847262382507324
			 train-loss:  2.089181744183103 	 ± 0.26563901040590454
	data : 0.11722326278686523
	model : 0.0686737060546875
			 train-loss:  2.088092735394295 	 ± 0.2653792895874656
	data : 0.11706428527832032
	model : 0.06892361640930175
			 train-loss:  2.090017815840613 	 ± 0.2660420855103369
	data : 0.1170046329498291
	model : 0.06970977783203125
			 train-loss:  2.0898494200828748 	 ± 0.26536941503705264
	data : 0.11635146141052247
	model : 0.06909170150756835
			 train-loss:  2.0910743335071875 	 ± 0.26524369268849507
	data : 0.11696605682373047
	model : 0.06846990585327148
			 train-loss:  2.091237641228032 	 ± 0.26457950735672164
	data : 0.11750826835632325
	model : 0.06835074424743652
			 train-loss:  2.090933788304377 	 ± 0.2639449885944312
	data : 0.11766810417175293
	model : 0.06894440650939941
			 train-loss:  2.090465254520052 	 ± 0.26336350834281336
	data : 0.1169428825378418
	model : 0.06738886833190919
			 train-loss:  2.0914515602588652 	 ± 0.2630724669023248
	data : 0.11853365898132324
	model : 0.06826996803283691
			 train-loss:  2.0916724299910054 	 ± 0.26243583117745084
	data : 0.11779599189758301
	model : 0.06910123825073242
			 train-loss:  2.0895724278865475 	 ± 0.2634730042488222
	data : 0.11706151962280273
	model : 0.06928548812866211
			 train-loss:  2.092114096791873 	 ± 0.2652941755583926
	data : 0.11705269813537597
	model : 0.06958956718444824
			 train-loss:  2.093625703863069 	 ± 0.26551806104412284
	data : 0.11702322959899902
	model : 0.07047858238220214
			 train-loss:  2.09379915784045 	 ± 0.2648812501481825
	data : 0.11604704856872558
	model : 0.0704573631286621
			 train-loss:  2.094410795031242 	 ± 0.2643826291300525
	data : 0.11601967811584472
	model : 0.07059931755065918
			 train-loss:  2.0940306802878634 	 ± 0.2637996713638223
	data : 0.11611943244934082
	model : 0.06956787109375
			 train-loss:  2.0934342059951563 	 ± 0.2633046615797499
	data : 0.11678743362426758
	model : 0.06855778694152832
			 train-loss:  2.0930478532918904 	 ± 0.26273308372952436
	data : 0.11750254631042481
	model : 0.06791043281555176
			 train-loss:  2.0929947188922338 	 ± 0.2621079078934128
	data : 0.11809172630310058
	model : 0.06784038543701172
			 train-loss:  2.0919097017902897 	 ± 0.26195836572723435
	data : 0.11805958747863769
	model : 0.06772212982177735
			 train-loss:  2.0902278035316826 	 ± 0.26247927303039903
	data : 0.11811246871948242
	model : 0.06877031326293945
			 train-loss:  2.0902137045569265 	 ± 0.2618624800856646
	data : 0.1172027587890625
	model : 0.06853175163269043
			 train-loss:  2.090067369358562 	 ± 0.2612586648209725
	data : 0.11757020950317383
	model : 0.06908984184265136
			 train-loss:  2.0887960212175236 	 ± 0.2613130565867986
	data : 0.1171748161315918
	model : 0.06913285255432129
			 train-loss:  2.0903606194036977 	 ± 0.2617149108184166
	data : 0.11728396415710449
	model : 0.0691080093383789
			 train-loss:  2.0901734411441786 	 ± 0.26112567568588557
	data : 0.11715559959411621
	model : 0.06896820068359374
			 train-loss:  2.089669212288813 	 ± 0.2606319385474507
	data : 0.11742525100708008
	model : 0.06966891288757324
			 train-loss:  2.0889492682670348 	 ± 0.2602533821164695
	data : 0.11671357154846192
	model : 0.069724702835083
			 train-loss:  2.0903696423227136 	 ± 0.26051060488795147
	data : 0.11661334037780761
	model : 0.06969003677368164
			 train-loss:  2.0901386354843416 	 ± 0.2599431291998796
	data : 0.11651697158813476
	model : 0.06966104507446289
			 train-loss:  2.0898909627854287 	 ± 0.2593831444798871
	data : 0.11661124229431152
	model : 0.06964354515075684
			 train-loss:  2.088877155641804 	 ± 0.25924136633881745
	data : 0.1165846347808838
	model : 0.06956987380981446
			 train-loss:  2.0881838149258067 	 ± 0.25886919421241644
	data : 0.11669392585754394
	model : 0.06895480155944825
			 train-loss:  2.0888169786665176 	 ± 0.258467065171958
	data : 0.117161226272583
	model : 0.06830887794494629
			 train-loss:  2.0884806635105506 	 ± 0.2579439371592819
	data : 0.11768736839294433
	model : 0.06773600578308106
			 train-loss:  2.089629840220649 	 ± 0.2579543096525078
	data : 0.117927885055542
	model : 0.06728892326354981
			 train-loss:  2.089065353598511 	 ± 0.2575284728791503
	data : 0.1182016372680664
	model : 0.06691522598266601
			 train-loss:  2.0886938816595286 	 ± 0.25702677954511355
	data : 0.11829657554626465
	model : 0.06685972213745117
			 train-loss:  2.089055263996124 	 ± 0.25652571556430526
	data : 0.11825642585754395
	model : 0.06646924018859864
			 train-loss:  2.089709888288985 	 ± 0.2561623182303852
	data : 0.11847825050354004
	model : 0.06660490036010742
			 train-loss:  2.0891817869811224 	 ± 0.25573563695433926
	data : 0.11839547157287597
	model : 0.06686949729919434
			 train-loss:  2.0909923033652897 	 ± 0.2566719992171094
	data : 0.11810035705566406
	model : 0.06728172302246094
			 train-loss:  2.0899089080655675 	 ± 0.25665630167876
	data : 0.11781558990478516
	model : 0.06779069900512695
			 train-loss:  2.0903674090162236 	 ± 0.2562056616732632
	data : 0.1175950050354004
	model : 0.0685847282409668
			 train-loss:  2.0903411694502427 	 ± 0.25566259327223273
	data : 0.11700911521911621
	model : 0.06888756752014161
			 train-loss:  2.0894488815516863 	 ± 0.2554906353660892
	data : 0.11670289039611817
	model : 0.06862077713012696
			 train-loss:  2.088288033208927 	 ± 0.255578895694581
	data : 0.11708993911743164
	model : 0.06870136260986329
			 train-loss:  2.0880891812895133 	 ± 0.25506210049432076
	data : 0.11708626747131348
	model : 0.06831016540527343
			 train-loss:  2.088400832315286 	 ± 0.25457576245479796
	data : 0.11755566596984864
	model : 0.06845607757568359
			 train-loss:  2.0864602554883205 	 ± 0.255819670451402
	data : 0.11764469146728515
	model : 0.06849179267883301
			 train-loss:  2.084895259092662 	 ± 0.2564440207295676
	data : 0.11785302162170411
	model : 0.06900296211242676
			 train-loss:  2.085888226826986 	 ± 0.25638157555583363
	data : 0.11725296974182128
	model : 0.0687899112701416
			 train-loss:  2.085336104279659 	 ± 0.25600038435980627
	data : 0.11744246482849122
	model : 0.06913566589355469
			 train-loss:  2.08454293182918 	 ± 0.2557776534617827
	data : 0.11689844131469726
	model : 0.06901350021362304
			 train-loss:  2.084562016211874 	 ± 0.2552574255683103
	data : 0.11710033416748047
	model : 0.06875872611999512
			 train-loss:  2.086355096898098 	 ± 0.25628789756272097
	data : 0.11705732345581055
	model : 0.06827254295349121
			 train-loss:  2.0868401378393173 	 ± 0.2558842395571654
	data : 0.11726374626159668
	model : 0.06829018592834472
			 train-loss:  2.089594889357387 	 ± 0.2590285156348851
	data : 0.11717453002929687
	model : 0.06817026138305664
			 train-loss:  2.0889897503852843 	 ± 0.2586862403087524
	data : 0.1171541690826416
	model : 0.06815557479858399
			 train-loss:  2.088958281919776 	 ± 0.2581708942519796
	data : 0.1168372631072998
	model : 0.06825289726257325
			 train-loss:  2.089143415765157 	 ± 0.2576748351105
	data : 0.11704139709472657
	model : 0.06848835945129395
			 train-loss:  2.088977717128196 	 ± 0.25717854405232893
	data : 0.11708698272705079
	model : 0.06859922409057617
			 train-loss:  2.0893176620400795 	 ± 0.25672873617892594
	data : 0.1169515609741211
	model : 0.06811237335205078
			 train-loss:  2.090509140257742 	 ± 0.25692753522173045
	data : 0.11769428253173828
	model : 0.06823182106018066
			 train-loss:  2.0925977039150894 	 ± 0.25858506328187447
	data : 0.11664061546325684
	model : 0.05963263511657715
#epoch  77    val-loss:  2.474668967096429  train-loss:  2.0925977039150894  lr:  4.8828125e-06
			 train-loss:  2.65370774269104 	 ± 0.0
	data : 5.7162253856658936
	model : 0.0717625617980957
			 train-loss:  2.5058794021606445 	 ± 0.1478283405303955
	data : 2.9232583045959473
	model : 0.07065165042877197
			 train-loss:  2.34472926457723 	 ± 0.2578905695489608
	data : 1.9874437650044758
	model : 0.06963682174682617
			 train-loss:  2.306104004383087 	 ± 0.2331445722548805
	data : 1.5198214650154114
	model : 0.06960922479629517
			 train-loss:  2.2379082441329956 	 ± 0.2491741562078235
	data : 1.2391240119934082
	model : 0.06907067298889161
			 train-loss:  2.224486291408539 	 ± 0.22943527230191768
	data : 0.11950860023498536
	model : 0.06867666244506836
			 train-loss:  2.20604179586683 	 ± 0.21716736157056105
	data : 0.11663823127746582
	model : 0.06872057914733887
			 train-loss:  2.178453952074051 	 ± 0.21585661662553318
	data : 0.11680593490600585
	model : 0.06852183341979981
			 train-loss:  2.1296342743767633 	 ± 0.2459346386795137
	data : 0.11714091300964355
	model : 0.06868891716003418
			 train-loss:  2.1166378259658813 	 ± 0.23654942590048933
	data : 0.11691761016845703
	model : 0.06931581497192382
			 train-loss:  2.1183429848064077 	 ± 0.225605476495498
	data : 0.11645116806030273
	model : 0.06929216384887696
			 train-loss:  2.138908247152964 	 ± 0.2265139626496353
	data : 0.11642794609069824
	model : 0.0692075252532959
			 train-loss:  2.1285069355597863 	 ± 0.22059013846233042
	data : 0.11633777618408203
	model : 0.06980175971984863
			 train-loss:  2.126323870250157 	 ± 0.21271165767022046
	data : 0.11591496467590331
	model : 0.06931605339050292
			 train-loss:  2.1110968828201293 	 ± 0.21325076491057324
	data : 0.116522216796875
	model : 0.06866269111633301
			 train-loss:  2.1045657470822334 	 ± 0.20802279129800386
	data : 0.11712689399719238
	model : 0.06872797012329102
			 train-loss:  2.1051734545651604 	 ± 0.2018263900238888
	data : 0.11714801788330079
	model : 0.06888132095336914
			 train-loss:  2.08660712507036 	 ± 0.21054914811426637
	data : 0.11714334487915039
	model : 0.06877593994140625
			 train-loss:  2.0875789617237293 	 ± 0.20497496579184898
	data : 0.11729474067687988
	model : 0.06931133270263672
			 train-loss:  2.0720445036888124 	 ± 0.2109480223032074
	data : 0.11662635803222657
	model : 0.07005643844604492
			 train-loss:  2.053118348121643 	 ± 0.22258493246664712
	data : 0.11594529151916504
	model : 0.06920800209045411
			 train-loss:  2.036005036397414 	 ± 0.23117574265437663
	data : 0.11683025360107421
	model : 0.06861495971679688
			 train-loss:  2.039210822271264 	 ± 0.2265937872563031
	data : 0.11729164123535156
	model : 0.06787042617797852
			 train-loss:  2.0541244397560754 	 ± 0.233068547344564
	data : 0.11799068450927734
	model : 0.06769957542419433
			 train-loss:  2.0434503507614137 	 ± 0.2342703098380932
	data : 0.11828851699829102
	model : 0.06756420135498047
			 train-loss:  2.060942278458522 	 ± 0.24580662734818035
	data : 0.11846075057983399
	model : 0.06836495399475098
			 train-loss:  2.051648462260211 	 ± 0.24582277664445462
	data : 0.11755223274230957
	model : 0.06899347305297851
			 train-loss:  2.062309729201453 	 ± 0.24766823634473956
	data : 0.11711654663085938
	model : 0.07006731033325195
			 train-loss:  2.0661191159281236 	 ± 0.24419401748924927
	data : 0.11610755920410157
	model : 0.06996197700500488
			 train-loss:  2.074719806512197 	 ± 0.24451628176624093
	data : 0.11607685089111328
	model : 0.06919641494750976
			 train-loss:  2.083445998930162 	 ± 0.2452426412161525
	data : 0.11681971549987794
	model : 0.06911110877990723
			 train-loss:  2.0912469439208508 	 ± 0.24525690930613195
	data : 0.11686506271362304
	model : 0.06861486434936523
			 train-loss:  2.0839697808930366 	 ± 0.24499555651054788
	data : 0.11714591979980468
	model : 0.06831355094909668
			 train-loss:  2.0707171208718242 	 ± 0.2530876024095906
	data : 0.11760444641113281
	model : 0.06833539009094239
			 train-loss:  2.0705941234316145 	 ± 0.24944689532400308
	data : 0.11771931648254394
	model : 0.06917085647583007
			 train-loss:  2.0666360921329923 	 ± 0.24707008398181643
	data : 0.11669449806213379
	model : 0.06920089721679687
			 train-loss:  2.0650499962471627 	 ± 0.2438941652930628
	data : 0.11666746139526367
	model : 0.06971473693847656
			 train-loss:  2.0683294534683228 	 ± 0.24148895406456852
	data : 0.11634616851806641
	model : 0.07011394500732422
			 train-loss:  2.0664534721619043 	 ± 0.23865318570427155
	data : 0.11584272384643554
	model : 0.0697239875793457
			 train-loss:  2.071542939543724 	 ± 0.2377849110469822
	data : 0.11613054275512695
	model : 0.06952056884765626
			 train-loss:  2.0665458237252583 	 ± 0.23698406192832788
	data : 0.1165287971496582
	model : 0.06946463584899902
			 train-loss:  2.068290188198998 	 ± 0.2344120803964131
	data : 0.11686968803405762
	model : 0.0691723346710205
			 train-loss:  2.080202424248984 	 ± 0.2441945876010986
	data : 0.1169051170349121
	model : 0.0689152717590332
			 train-loss:  2.0800084200772373 	 ± 0.2414070527309317
	data : 0.1170969009399414
	model : 0.06941981315612793
			 train-loss:  2.0763414435916476 	 ± 0.23994576070081375
	data : 0.11671676635742187
	model : 0.06969456672668457
			 train-loss:  2.0676759222279424 	 ± 0.24433883604394047
	data : 0.11652436256408691
	model : 0.06933975219726562
			 train-loss:  2.066902419354053 	 ± 0.24178243305266478
	data : 0.11666765213012695
	model : 0.06959900856018067
			 train-loss:  2.071293443441391 	 ± 0.24113702694236716
	data : 0.11653261184692383
	model : 0.06974763870239258
			 train-loss:  2.071258004830808 	 ± 0.23866388757851062
	data : 0.11653070449829102
	model : 0.0699005126953125
			 train-loss:  2.075666832923889 	 ± 0.2382723078789663
	data : 0.11639599800109864
	model : 0.06881170272827149
			 train-loss:  2.0808862190620574 	 ± 0.2387940171447162
	data : 0.117134428024292
	model : 0.06838364601135254
			 train-loss:  2.0750236006883473 	 ± 0.240164276071001
	data : 0.11748790740966797
	model : 0.06739506721496583
			 train-loss:  2.080859850037773 	 ± 0.24158189644876596
	data : 0.11833515167236328
	model : 0.0664520263671875
			 train-loss:  2.0747836386715925 	 ± 0.24338820153142102
	data : 0.11889243125915527
	model : 0.06639094352722168
			 train-loss:  2.077956533432007 	 ± 0.2422899039078057
	data : 0.11907396316528321
	model : 0.06750993728637696
			 train-loss:  2.0828800669738223 	 ± 0.24287727304561277
	data : 0.11819510459899903
	model : 0.06841568946838379
			 train-loss:  2.083432988116616 	 ± 0.24077289913614738
	data : 0.1173482894897461
	model : 0.06930627822875976
			 train-loss:  2.0796927789161943 	 ± 0.24035278488329598
	data : 0.11658024787902832
	model : 0.06989445686340331
			 train-loss:  2.0770143512952126 	 ± 0.23917861173854055
	data : 0.11614770889282226
	model : 0.06969976425170898
			 train-loss:  2.084243232011795 	 ± 0.24359005407258053
	data : 0.11623029708862305
	model : 0.06917695999145508
			 train-loss:  2.0871098920947215 	 ± 0.24260349473442017
	data : 0.11664891242980957
	model : 0.06913628578186035
			 train-loss:  2.083039935558073 	 ± 0.24272946937750214
	data : 0.11665840148925781
	model : 0.06839046478271485
			 train-loss:  2.081885650044396 	 ± 0.240966808153609
	data : 0.11732797622680664
	model : 0.06873121261596679
			 train-loss:  2.084442863240838 	 ± 0.2399368986703316
	data : 0.1169884204864502
	model : 0.06875901222229004
			 train-loss:  2.0777224045533402 	 ± 0.24407900272919503
	data : 0.11693553924560547
	model : 0.06916799545288085
			 train-loss:  2.0798127633152586 	 ± 0.24280844037067253
	data : 0.11668915748596191
	model : 0.06921806335449218
			 train-loss:  2.08825983396217 	 ± 0.2505699453193602
	data : 0.11658797264099122
	model : 0.06971335411071777
			 train-loss:  2.082233537645901 	 ± 0.25356492830710214
	data : 0.11614489555358887
	model : 0.06881895065307617
			 train-loss:  2.079697786897853 	 ± 0.2525878083639072
	data : 0.11689248085021972
	model : 0.0680232048034668
			 train-loss:  2.0841854657445635 	 ± 0.25353258728738315
	data : 0.11742753982543945
	model : 0.06807327270507812
			 train-loss:  2.085243999118536 	 ± 0.2518965511703133
	data : 0.11756253242492676
	model : 0.06796936988830567
			 train-loss:  2.083974502152867 	 ± 0.25036976991127907
	data : 0.11773290634155273
	model : 0.068218994140625
			 train-loss:  2.0872274016680783 	 ± 0.25017629851100154
	data : 0.11756372451782227
	model : 0.06897282600402832
			 train-loss:  2.0852033318700016 	 ± 0.2490812413447999
	data : 0.11695823669433594
	model : 0.06991233825683593
			 train-loss:  2.0882405853271484 	 ± 0.24879085291262854
	data : 0.11642966270446778
	model : 0.06983966827392578
			 train-loss:  2.0868589046754336 	 ± 0.247438141474619
	data : 0.1162498950958252
	model : 0.06936230659484863
			 train-loss:  2.082436488820361 	 ± 0.2488310383895411
	data : 0.1167680263519287
	model : 0.06934823989868164
			 train-loss:  2.0794765521318483 	 ± 0.248591419361027
	data : 0.11683826446533203
	model : 0.06849350929260253
			 train-loss:  2.078043580055237 	 ± 0.24733703805014656
	data : 0.11762752532958984
	model : 0.06822819709777832
			 train-loss:  2.074201484024525 	 ± 0.2481473145378664
	data : 0.11772069931030274
	model : 0.06731834411621093
			 train-loss:  2.0780979659822254 	 ± 0.2490612062525726
	data : 0.11867280006408691
	model : 0.06802577972412109
			 train-loss:  2.081891113665046 	 ± 0.2498808285143549
	data : 0.11788129806518555
	model : 0.06796450614929199
			 train-loss:  2.082983202244862 	 ± 0.24856776149062978
	data : 0.11809830665588379
	model : 0.0682762622833252
			 train-loss:  2.0864433575244177 	 ± 0.2490865657370568
	data : 0.1177515983581543
	model : 0.06846222877502442
			 train-loss:  2.084128426103031 	 ± 0.24852431403895414
	data : 0.11770210266113282
	model : 0.06842999458312989
			 train-loss:  2.084782391093498 	 ± 0.2471487339604855
	data : 0.1177098274230957
	model : 0.06831693649291992
			 train-loss:  2.0901379489350593 	 ± 0.25069314114547053
	data : 0.1179351806640625
	model : 0.06819138526916504
			 train-loss:  2.08830083229325 	 ± 0.24985296630304907
	data : 0.11777658462524414
	model : 0.06901688575744629
			 train-loss:  2.0880232982421187 	 ± 0.24845897363886232
	data : 0.11712727546691895
	model : 0.06814451217651367
			 train-loss:  2.0872115267647637 	 ± 0.24719344772757626
	data : 0.11790990829467773
	model : 0.06908869743347168
			 train-loss:  2.0856293675663706 	 ± 0.24628928512542908
	data : 0.11705455780029297
	model : 0.06911988258361816
			 train-loss:  2.081945443930833 	 ± 0.24745518603942138
	data : 0.11688480377197266
	model : 0.06936421394348144
			 train-loss:  2.0792910604066748 	 ± 0.2474345315865926
	data : 0.11671571731567383
	model : 0.06912269592285156
			 train-loss:  2.080352487716269 	 ± 0.2463276401330888
	data : 0.1169830322265625
	model : 0.06916341781616211
			 train-loss:  2.081971099502162 	 ± 0.2455297717037563
	data : 0.11696004867553711
	model : 0.06925511360168457
			 train-loss:  2.0827418230473995 	 ± 0.24436311672190172
	data : 0.1169126033782959
	model : 0.069114351272583
			 train-loss:  2.085880160331726 	 ± 0.24503724388795162
	data : 0.11715507507324219
	model : 0.06838631629943848
			 train-loss:  2.0907688688258736 	 ± 0.24849308505562542
	data : 0.11776480674743653
	model : 0.0683659553527832
			 train-loss:  2.089501010047065 	 ± 0.2475532661669426
	data : 0.1176755428314209
	model : 0.06842923164367676
			 train-loss:  2.089508521556854 	 ± 0.24631240118978392
	data : 0.11733283996582031
	model : 0.06818709373474122
			 train-loss:  2.0849447155943013 	 ± 0.24930290890250745
	data : 0.1176569938659668
	model : 0.06826815605163575
			 train-loss:  2.0853237053927254 	 ± 0.24810706277402822
	data : 0.1176579475402832
	model : 0.06892685890197754
			 train-loss:  2.0838646298473322 	 ± 0.24733908032630417
	data : 0.11707124710083008
	model : 0.06894063949584961
			 train-loss:  2.0833289336699705 	 ± 0.24620711173767085
	data : 0.11702136993408203
	model : 0.06897697448730469
			 train-loss:  2.0848310368401664 	 ± 0.24551025393501513
	data : 0.11704139709472657
	model : 0.06930246353149414
			 train-loss:  2.088796666208303 	 ± 0.24770527393469433
	data : 0.11667451858520508
	model : 0.06919336318969727
			 train-loss:  2.0907984080715716 	 ± 0.24740493783144799
	data : 0.116768217086792
	model : 0.0690493106842041
			 train-loss:  2.089586536089579 	 ± 0.24657573925123533
	data : 0.11711835861206055
	model : 0.06804456710815429
			 train-loss:  2.087312376827275 	 ± 0.24657728032782877
	data : 0.11809029579162597
	model : 0.06801538467407227
			 train-loss:  2.085241390358318 	 ± 0.24640439126026098
	data : 0.11808772087097168
	model : 0.06786575317382812
			 train-loss:  2.0890352887076302 	 ± 0.24849837836148894
	data : 0.11816892623901368
	model : 0.06810231208801269
			 train-loss:  2.0899669602513313 	 ± 0.24758118100199006
	data : 0.11804780960083008
	model : 0.0679893970489502
			 train-loss:  2.089431643486023 	 ± 0.2465483522081477
	data : 0.11793293952941894
	model : 0.06814513206481934
			 train-loss:  2.0902602034702635 	 ± 0.2456225854377704
	data : 0.11788206100463867
	model : 0.0688979148864746
			 train-loss:  2.0887812096139657 	 ± 0.24506164129006752
	data : 0.11730213165283203
	model : 0.06947441101074218
			 train-loss:  2.088842940741572 	 ± 0.24400395274535247
	data : 0.11671466827392578
	model : 0.06880216598510742
			 train-loss:  2.089066026557205 	 ± 0.24297084346134096
	data : 0.11716790199279785
	model : 0.06908745765686035
			 train-loss:  2.0878523184081255 	 ± 0.24229504087042827
	data : 0.11691913604736329
	model : 0.0697265625
			 train-loss:  2.0918201017780462 	 ± 0.24509439328896732
	data : 0.11630163192749024
	model : 0.06970820426940919
			 train-loss:  2.091341249148051 	 ± 0.2441269228412728
	data : 0.11626148223876953
	model : 0.06902995109558105
			 train-loss:  2.0917330379328454 	 ± 0.24315392078275705
	data : 0.11692514419555664
	model : 0.06868658065795899
			 train-loss:  2.093493244687065 	 ± 0.24292818961002155
	data : 0.1172828197479248
	model : 0.06867527961730957
			 train-loss:  2.0940583391887384 	 ± 0.24201916104360532
	data : 0.11721563339233398
	model : 0.06883316040039063
			 train-loss:  2.0933107868317635 	 ± 0.24118384241814111
	data : 0.11706333160400391
	model : 0.068855619430542
			 train-loss:  2.093395771026611 	 ± 0.24021903389133942
	data : 0.11714892387390137
	model : 0.06899638175964355
			 train-loss:  2.0953011588444785 	 ± 0.2402103649575105
	data : 0.11697263717651367
	model : 0.0698476791381836
			 train-loss:  2.094881697902529 	 ± 0.23930910989771526
	data : 0.11609234809875488
	model : 0.06898694038391114
			 train-loss:  2.0930784940719604 	 ± 0.23923708592882895
	data : 0.1169318675994873
	model : 0.06894631385803222
			 train-loss:  2.0982555807098864 	 ± 0.24540047031162152
	data : 0.11695380210876465
	model : 0.06873750686645508
			 train-loss:  2.099190999911382 	 ± 0.24468556461894983
	data : 0.11711406707763672
	model : 0.06871633529663086
			 train-loss:  2.1017980557361633 	 ± 0.24555563730689245
	data : 0.117254638671875
	model : 0.06883330345153808
			 train-loss:  2.1074841889468106 	 ± 0.253132925079453
	data : 0.11735987663269043
	model : 0.0697286605834961
			 train-loss:  2.107986303200399 	 ± 0.25224547747441733
	data : 0.11645965576171875
	model : 0.06993579864501953
			 train-loss:  2.107305382614705 	 ± 0.25142516291867806
	data : 0.1163099765777588
	model : 0.07013344764709473
			 train-loss:  2.10944308174981 	 ± 0.2517115485566623
	data : 0.11606831550598144
	model : 0.06997709274291992
			 train-loss:  2.1076507314163098 	 ± 0.2516476127793983
	data : 0.11610689163208007
	model : 0.06988897323608398
			 train-loss:  2.1075051883711433 	 ± 0.25073325366948673
	data : 0.11625790596008301
	model : 0.0697361946105957
			 train-loss:  2.1060662865638733 	 ± 0.2503902062680909
	data : 0.11655325889587402
	model : 0.06907005310058593
			 train-loss:  2.105820752733903 	 ± 0.2495045690544456
	data : 0.1172189712524414
	model : 0.06953206062316894
			 train-loss:  2.105830135515758 	 ± 0.24861190898207114
	data : 0.11684947013854981
	model : 0.0691333293914795
			 train-loss:  2.105956518058236 	 ± 0.24773325105099986
	data : 0.11717476844787597
	model : 0.0701756477355957
			 train-loss:  2.1050354418620256 	 ± 0.24710157815987585
	data : 0.11612462997436523
	model : 0.07016792297363281
			 train-loss:  2.1034919532028944 	 ± 0.24692204704145548
	data : 0.11614089012145996
	model : 0.07071013450622558
			 train-loss:  2.1018158619602523 	 ± 0.24687814481817005
	data : 0.1156116008758545
	model : 0.0697587013244629
			 train-loss:  2.1031792139184886 	 ± 0.24656873001913562
	data : 0.11640357971191406
	model : 0.07025480270385742
			 train-loss:  2.104395576535839 	 ± 0.24615901309381225
	data : 0.11608610153198243
	model : 0.06854000091552734
			 train-loss:  2.103252178957673 	 ± 0.24570903147831327
	data : 0.11772942543029785
	model : 0.06884751319885254
			 train-loss:  2.104382995012644 	 ± 0.2452610414792461
	data : 0.11746430397033691
	model : 0.0687342643737793
			 train-loss:  2.104641778357077 	 ± 0.24445690537979628
	data : 0.11744160652160644
	model : 0.06928815841674804
			 train-loss:  2.106106171607971 	 ± 0.2442955315910301
	data : 0.11683149337768554
	model : 0.06926126480102539
			 train-loss:  2.1056158779472707 	 ± 0.2435592969213671
	data : 0.11683158874511719
	model : 0.06942172050476074
			 train-loss:  2.102336427883098 	 ± 0.24607891867978324
	data : 0.11658811569213867
	model : 0.06931514739990234
			 train-loss:  2.1024990073995653 	 ± 0.24528161094443854
	data : 0.11669387817382812
	model : 0.0690230369567871
			 train-loss:  2.0997235349246433 	 ± 0.2468825555839694
	data : 0.11705327033996582
	model : 0.06904006004333496
			 train-loss:  2.0974630163561914 	 ± 0.2476786131750796
	data : 0.1171966552734375
	model : 0.06915574073791504
			 train-loss:  2.0958722157356067 	 ± 0.24767662469940888
	data : 0.1169853687286377
	model : 0.06961450576782227
			 train-loss:  2.0956738557025885 	 ± 0.24689901627569502
	data : 0.11667723655700683
	model : 0.06933016777038574
			 train-loss:  2.0976181271709975 	 ± 0.24731922098398296
	data : 0.11689519882202148
	model : 0.06964712142944336
			 train-loss:  2.095687711763682 	 ± 0.24773148327795627
	data : 0.11654558181762695
	model : 0.06958975791931152
			 train-loss:  2.0945344157516956 	 ± 0.2473839209350778
	data : 0.11663641929626464
	model : 0.06953969001770019
			 train-loss:  2.094140730289199 	 ± 0.2466647231750587
	data : 0.11699175834655762
	model : 0.06880407333374024
			 train-loss:  2.094803716665433 	 ± 0.24604608569860575
	data : 0.11769909858703613
	model : 0.06807951927185059
			 train-loss:  2.093302690909684 	 ± 0.24603307105619354
	data : 0.11828718185424805
	model : 0.06717410087585449
			 train-loss:  2.0924797276171243 	 ± 0.24550675675465916
	data : 0.11895241737365722
	model : 0.06718454360961915
			 train-loss:  2.092961968797626 	 ± 0.24483956489554287
	data : 0.11905522346496582
	model : 0.06712613105773926
			 train-loss:  2.093148849096643 	 ± 0.24411278555568824
	data : 0.11876716613769531
	model : 0.06791019439697266
			 train-loss:  2.091846009928309 	 ± 0.24395898456047796
	data : 0.11802229881286622
	model : 0.06880393028259277
			 train-loss:  2.0907636150008155 	 ± 0.24363369631800727
	data : 0.11745405197143555
	model : 0.06996164321899415
			 train-loss:  2.092365255722633 	 ± 0.24379727930644787
	data : 0.11649079322814941
	model : 0.06992888450622559
			 train-loss:  2.092271121109233 	 ± 0.24308225126984956
	data : 0.116357421875
	model : 0.06994824409484864
			 train-loss:  2.091095895795097 	 ± 0.24285433403223902
	data : 0.11635088920593262
	model : 0.06917243003845215
			 train-loss:  2.0915113389492035 	 ± 0.24220826635735684
	data : 0.11706395149230957
	model : 0.06852693557739258
			 train-loss:  2.0902576963336483 	 ± 0.24206623009527167
	data : 0.11757493019104004
	model : 0.06812014579772949
			 train-loss:  2.0917303555313196 	 ± 0.24214559717843767
	data : 0.11799921989440917
	model : 0.06809053421020508
			 train-loss:  2.0896506111962454 	 ± 0.24300626443339593
	data : 0.11802220344543457
	model : 0.06815505027770996
			 train-loss:  2.0878354900262575 	 ± 0.24350172057761357
	data : 0.11792192459106446
	model : 0.06903996467590331
			 train-loss:  2.0879448221228216 	 ± 0.2428172204441804
	data : 0.11707758903503418
	model : 0.06970086097717285
			 train-loss:  2.087558047825031 	 ± 0.24218885947243302
	data : 0.11666927337646485
	model : 0.0699120044708252
			 train-loss:  2.090052097869319 	 ± 0.2437928843317081
	data : 0.11651568412780762
	model : 0.06979961395263672
			 train-loss:  2.0872975216971503 	 ± 0.24589219723385589
	data : 0.11673741340637207
	model : 0.07007741928100586
			 train-loss:  2.0859941389020635 	 ± 0.24583471743364216
	data : 0.11649565696716309
	model : 0.0700760841369629
			 train-loss:  2.0844802417597927 	 ± 0.24600300925755805
	data : 0.11644468307495118
	model : 0.0702589511871338
			 train-loss:  2.085091310120671 	 ± 0.24546841665528973
	data : 0.11606159210205078
	model : 0.07017297744750976
			 train-loss:  2.084982960768368 	 ± 0.24480486205611435
	data : 0.11599159240722656
	model : 0.0701113224029541
			 train-loss:  2.084792496062614 	 ± 0.24415600055053946
	data : 0.11592650413513184
	model : 0.06952576637268067
			 train-loss:  2.083363853475099 	 ± 0.24427289205855882
	data : 0.11662874221801758
	model : 0.06927237510681153
			 train-loss:  2.08603374970788 	 ± 0.24632505202474872
	data : 0.11673140525817871
	model : 0.06919822692871094
			 train-loss:  2.0851987790554127 	 ± 0.24593425608913072
	data : 0.11679267883300781
	model : 0.06852898597717286
			 train-loss:  2.0856810007145796 	 ± 0.24537187299057878
	data : 0.11757245063781738
	model : 0.06900029182434082
			 train-loss:  2.08583779335022 	 ± 0.2447347984992016
	data : 0.11723494529724121
	model : 0.06930704116821289
			 train-loss:  2.0841523876989076 	 ± 0.24519634396169113
	data : 0.1168985366821289
	model : 0.06983599662780762
			 train-loss:  2.0854963126281896 	 ± 0.24526126321145272
	data : 0.11649837493896484
	model : 0.06973190307617187
			 train-loss:  2.0862085473352145 	 ± 0.2448240401478648
	data : 0.11664071083068847
	model : 0.07103285789489747
			 train-loss:  2.0851016413305223 	 ± 0.24467594730016157
	data : 0.11540341377258301
	model : 0.07130823135375977
			 train-loss:  2.084072866806617 	 ± 0.24446807064369325
	data : 0.11513199806213378
	model : 0.07122302055358887
			 train-loss:  2.0868010222911835 	 ± 0.24680167797622046
	data : 0.11513872146606445
	model : 0.07063021659851074
			 train-loss:  2.086840240483357 	 ± 0.24617509312087982
	data : 0.11571969985961914
	model : 0.06980309486389161
			 train-loss:  2.0850753206195254 	 ± 0.24679900419875483
	data : 0.11648144721984863
	model : 0.06930227279663086
			 train-loss:  2.0852099603144967 	 ± 0.24618541518383394
	data : 0.11688070297241211
	model : 0.06887173652648926
			 train-loss:  2.084028903245926 	 ± 0.24613371695083852
	data : 0.11712470054626464
	model : 0.06882038116455078
			 train-loss:  2.087347278547524 	 ± 0.24996545266396672
	data : 0.11713614463806152
	model : 0.06897540092468261
			 train-loss:  2.0853671620387844 	 ± 0.25092130286452835
	data : 0.11691017150878906
	model : 0.06920108795166016
			 train-loss:  2.0859946963822313 	 ± 0.25046135934875263
	data : 0.116554594039917
	model : 0.06922454833984375
			 train-loss:  2.0861469127383887 	 ± 0.2498561418596051
	data : 0.11664972305297852
	model : 0.06919498443603515
			 train-loss:  2.083925732170663 	 ± 0.25125689551551367
	data : 0.11687517166137695
	model : 0.06854481697082519
			 train-loss:  2.0835582573436997 	 ± 0.2507015234060966
	data : 0.11745924949645996
	model : 0.06872520446777344
			 train-loss:  2.083794505123931 	 ± 0.25011821619657676
	data : 0.11722464561462402
	model : 0.06926383972167968
			 train-loss:  2.0836872722093878 	 ± 0.2495210158188434
	data : 0.1168025016784668
	model : 0.0690986156463623
			 train-loss:  2.089166079982046 	 ± 0.2611636358096802
	data : 0.11688098907470704
	model : 0.0688544750213623
			 train-loss:  2.0892148846671694 	 ± 0.26054203095124867
	data : 0.11696834564208984
	model : 0.069219970703125
			 train-loss:  2.089584921208603 	 ± 0.25997920728554547
	data : 0.11674609184265136
	model : 0.06912121772766114
			 train-loss:  2.0902700502917453 	 ± 0.25955618869106234
	data : 0.11697502136230468
	model : 0.06903858184814453
			 train-loss:  2.0889249278905804 	 ± 0.2596857906397595
	data : 0.11707277297973633
	model : 0.0689011573791504
			 train-loss:  2.088680888447806 	 ± 0.25910281803935925
	data : 0.11725025177001953
	model : 0.06815810203552246
			 train-loss:  2.0889551312424417 	 ± 0.25853068027893866
	data : 0.11779356002807617
	model : 0.06763720512390137
			 train-loss:  2.087891944028713 	 ± 0.25840221720189904
	data : 0.11840496063232422
	model : 0.06724519729614258
			 train-loss:  2.0857662022937826 	 ± 0.2596922374133341
	data : 0.11883335113525391
	model : 0.06744155883789063
			 train-loss:  2.085007015171401 	 ± 0.25933717691754954
	data : 0.11854863166809082
	model : 0.06773829460144043
			 train-loss:  2.0837399219269077 	 ± 0.2594198756189546
	data : 0.11818556785583496
	model : 0.06889238357543945
			 train-loss:  2.083505450595509 	 ± 0.25885287082156555
	data : 0.11722931861877442
	model : 0.06903777122497559
			 train-loss:  2.0837996707243076 	 ± 0.2583034338898781
	data : 0.11696782112121581
	model : 0.06864738464355469
			 train-loss:  2.0825574977977857 	 ± 0.2583817369881208
	data : 0.11729536056518555
	model : 0.06786489486694336
			 train-loss:  2.0832832319319516 	 ± 0.25802842860369885
	data : 0.11811695098876954
	model : 0.06775975227355957
			 train-loss:  2.083301857113838 	 ± 0.25745197828061345
	data : 0.11824502944946289
	model : 0.06697092056274415
			 train-loss:  2.0825556050406564 	 ± 0.2571219174348909
	data : 0.11904444694519042
	model : 0.06695842742919922
			 train-loss:  2.0837798756835735 	 ± 0.25720884423477747
	data : 0.1190831184387207
	model : 0.06727490425109864
			 train-loss:  2.0834527125967757 	 ± 0.25668880337491023
	data : 0.11878271102905273
	model : 0.06770248413085937
			 train-loss:  2.0840722327692465 	 ± 0.2562952947373691
	data : 0.11823410987854004
	model : 0.066845703125
			 train-loss:  2.0863010243036864 	 ± 0.2579399673641803
	data : 0.11878070831298829
	model : 0.06648645401000977
			 train-loss:  2.0871033559674803 	 ± 0.2576648371451463
	data : 0.11890969276428223
	model : 0.0664445400238037
			 train-loss:  2.0868784323399203 	 ± 0.25712914360851036
	data : 0.11879129409790039
	model : 0.06658673286437988
			 train-loss:  2.08616914368909 	 ± 0.2568007597646172
	data : 0.11838665008544921
	model : 0.06623210906982421
			 train-loss:  2.086052068313304 	 ± 0.25625529732923885
	data : 0.11861004829406738
	model : 0.06649670600891114
			 train-loss:  2.08588154346515 	 ± 0.2557204049309812
	data : 0.11852445602416992
	model : 0.0668100357055664
			 train-loss:  2.086018023085087 	 ± 0.2551842792467164
	data : 0.1184577465057373
	model : 0.06702475547790528
			 train-loss:  2.086286494792518 	 ± 0.2546763170138473
	data : 0.11817879676818847
	model : 0.06697101593017578
			 train-loss:  2.0858781639533706 	 ± 0.2542158623917452
	data : 0.11811237335205078
	model : 0.06729669570922851
			 train-loss:  2.0844615677825544 	 ± 0.2546169016656044
	data : 0.11791882514953614
	model : 0.06709518432617187
			 train-loss:  2.0833101970880104 	 ± 0.2547037850632114
	data : 0.11825447082519532
	model : 0.06715855598449708
			 train-loss:  2.082642470300198 	 ± 0.2543821338710053
	data : 0.11816391944885254
	model : 0.06755046844482422
			 train-loss:  2.0818040835906855 	 ± 0.25418587064224574
	data : 0.11811041831970215
	model : 0.0677708625793457
			 train-loss:  2.081679143196295 	 ± 0.2536675649545206
	data : 0.11824183464050293
	model : 0.06810064315795898
			 train-loss:  2.082487789201148 	 ± 0.25345744396895414
	data : 0.11795334815979004
	model : 0.06823258399963379
			 train-loss:  2.083352454373094 	 ± 0.2532964121114483
	data : 0.11782574653625488
	model : 0.06815567016601562
			 train-loss:  2.082933494509483 	 ± 0.252863653578973
	data : 0.1178293228149414
	model : 0.0683983325958252
			 train-loss:  2.0841395985789415 	 ± 0.2530543554507575
	data : 0.11756434440612792
	model : 0.06782541275024415
			 train-loss:  2.0856563489929383 	 ± 0.25365957606073775
	data : 0.11811776161193847
	model : 0.06754388809204101
			 train-loss:  2.0856188396292348 	 ± 0.2531483354398555
	data : 0.11832880973815918
	model : 0.06788382530212403
			 train-loss:  2.0850698488304413 	 ± 0.2527873788312879
	data : 0.11776738166809082
	model : 0.06787810325622559
			 train-loss:  2.0857703704833983 	 ± 0.252523355309537
	data : 0.117592191696167
	model : 0.0674400806427002
			 train-loss:  2.0849715153059636 	 ± 0.2523361476668225
	data : 0.11762480735778809
	model : 0.06767621040344238
			 train-loss:  2.084315305191373 	 ± 0.25204948366529567
	data : 0.11726169586181641
	model : 0.06699280738830567
			 train-loss:  2.084301573014542 	 ± 0.2515509634206451
	data : 0.11802077293395996
	model : 0.06637921333312988
			 train-loss:  2.083946407779934 	 ± 0.25111884770105536
	data : 0.11874208450317383
	model : 0.06603116989135742
			 train-loss:  2.0842498643725524 	 ± 0.25067263257660405
	data : 0.1191403865814209
	model : 0.06575198173522949
			 train-loss:  2.0864739934913814 	 ± 0.2526909850609099
	data : 0.11846528053283692
	model : 0.05721049308776856
#epoch  78    val-loss:  2.4488989428470007  train-loss:  2.0864739934913814  lr:  4.8828125e-06
			 train-loss:  2.269495964050293 	 ± 0.0
	data : 5.231334447860718
	model : 0.07175850868225098
			 train-loss:  1.8845431804656982 	 ± 0.3849527835845947
	data : 2.871357798576355
	model : 0.07048988342285156
			 train-loss:  1.9692349433898926 	 ± 0.3363596584125903
	data : 1.952895720799764
	model : 0.07037901878356934
			 train-loss:  1.9920584559440613 	 ± 0.29396615967064427
	data : 1.493850827217102
	model : 0.07025426626205444
			 train-loss:  2.007311773300171 	 ± 0.2646951778681851
	data : 1.2181994915008545
	model : 0.06956758499145507
			 train-loss:  1.9906604687372844 	 ± 0.24448437722505262
	data : 0.19555292129516602
	model : 0.06868867874145508
			 train-loss:  1.9845344168799264 	 ± 0.22684540416006746
	data : 0.11682229042053223
	model : 0.06880860328674317
			 train-loss:  1.9816243350505829 	 ± 0.212334082871193
	data : 0.11683816909790039
	model : 0.06867785453796386
			 train-loss:  1.9691137340333726 	 ± 0.20329376443201497
	data : 0.11679072380065918
	model : 0.06851344108581543
			 train-loss:  1.9886637091636659 	 ± 0.20158207494616515
	data : 0.11709127426147461
	model : 0.06911301612854004
			 train-loss:  1.9945051778446545 	 ± 0.19308661094644458
	data : 0.11661834716796875
	model : 0.06969780921936035
			 train-loss:  2.006405939658483 	 ± 0.1890330099007294
	data : 0.11632790565490722
	model : 0.06965017318725586
			 train-loss:  2.0328602332335253 	 ± 0.20342739607498928
	data : 0.116310453414917
	model : 0.06892905235290528
			 train-loss:  2.0508189456803456 	 ± 0.20644490191264778
	data : 0.11692194938659668
	model : 0.06949663162231445
			 train-loss:  2.0309942245483397 	 ± 0.21279208547814893
	data : 0.1164555549621582
	model : 0.07004613876342773
			 train-loss:  2.011317864060402 	 ± 0.2196766478150906
	data : 0.11593751907348633
	model : 0.06914019584655762
			 train-loss:  2.0212983944836784 	 ± 0.21682460546997182
	data : 0.11664652824401855
	model : 0.06845288276672364
			 train-loss:  2.03175061278873 	 ± 0.21507745576193243
	data : 0.1173398494720459
	model : 0.06848998069763183
			 train-loss:  2.0337816790530554 	 ± 0.20951830020620305
	data : 0.11737999916076661
	model : 0.06797633171081544
			 train-loss:  2.02420979142189 	 ± 0.20843181580286935
	data : 0.11776523590087891
	model : 0.0668825626373291
			 train-loss:  2.0442017770948864 	 ± 0.2221905899019304
	data : 0.11889834403991699
	model : 0.06770820617675781
			 train-loss:  2.0376253073865715 	 ± 0.21916404210370785
	data : 0.11814837455749512
	model : 0.0684422492980957
			 train-loss:  2.04077598841294 	 ± 0.214855488130746
	data : 0.11736555099487304
	model : 0.06831998825073242
			 train-loss:  2.034893681605657 	 ± 0.21221513301764586
	data : 0.11743216514587403
	model : 0.06839261054992676
			 train-loss:  2.040370626449585 	 ± 0.20965156419686343
	data : 0.11742076873779297
	model : 0.06903924942016601
			 train-loss:  2.047912524296687 	 ± 0.2090101768127084
	data : 0.11683421134948731
	model : 0.06809992790222168
			 train-loss:  2.038701715292754 	 ± 0.21041172901164956
	data : 0.11767315864562988
	model : 0.06813540458679199
			 train-loss:  2.02646227819579 	 ± 0.21618653257857712
	data : 0.117924165725708
	model : 0.06896986961364746
			 train-loss:  2.0239551889485328 	 ± 0.21284032346327905
	data : 0.11726889610290528
	model : 0.06953530311584473
			 train-loss:  2.0320884545644122 	 ± 0.21379738100983198
	data : 0.11685876846313477
	model : 0.06955599784851074
			 train-loss:  2.0398907969074864 	 ± 0.21461855071308433
	data : 0.1166107177734375
	model : 0.07058801651000976
			 train-loss:  2.0423575714230537 	 ± 0.2116845448062321
	data : 0.1154637336730957
	model : 0.06976132392883301
			 train-loss:  2.0323987368381387 	 ± 0.21593092780543463
	data : 0.11603989601135253
	model : 0.07000384330749512
			 train-loss:  2.0433861718458286 	 ± 0.2218979287808807
	data : 0.11569433212280274
	model : 0.06936020851135254
			 train-loss:  2.0445805617741177 	 ± 0.2188158457434191
	data : 0.11621160507202148
	model : 0.06925039291381836
			 train-loss:  2.0427940686543784 	 ± 0.2160140469232998
	data : 0.11634130477905273
	model : 0.069122314453125
			 train-loss:  2.045302932326858 	 ± 0.21360601529903192
	data : 0.1165095329284668
	model : 0.06991243362426758
			 train-loss:  2.0539040314523795 	 ± 0.2171727994764277
	data : 0.1158520221710205
	model : 0.06973142623901367
			 train-loss:  2.0529902287018604 	 ± 0.21444445017324873
	data : 0.11596784591674805
	model : 0.06917595863342285
			 train-loss:  2.057669794559479 	 ± 0.2137540567635383
	data : 0.11634478569030762
	model : 0.06833524703979492
			 train-loss:  2.0563526037262707 	 ± 0.2112954963383803
	data : 0.11741251945495605
	model : 0.06753125190734863
			 train-loss:  2.05494776510057 	 ± 0.2089586281632163
	data : 0.11826362609863281
	model : 0.0676682472229004
			 train-loss:  2.0621838098348575 	 ± 0.21177206527335507
	data : 0.11808018684387207
	model : 0.06760845184326172
			 train-loss:  2.0569190112027256 	 ± 0.21217923287920362
	data : 0.11810636520385742
	model : 0.06780714988708496
			 train-loss:  2.05961045689053 	 ± 0.21056664631165106
	data : 0.11783123016357422
	model : 0.06846847534179687
			 train-loss:  2.0523525295050247 	 ± 0.2138806305545009
	data : 0.11720213890075684
	model : 0.0685488224029541
			 train-loss:  2.0607600846189134 	 ± 0.21914202948331016
	data : 0.11717591285705567
	model : 0.06825838088989258
			 train-loss:  2.057156148056189 	 ± 0.2182503104417805
	data : 0.11743426322937012
	model : 0.06868500709533691
			 train-loss:  2.0561130363114026 	 ± 0.21613264415057798
	data : 0.11727051734924317
	model : 0.06954879760742187
			 train-loss:  2.055892140865326 	 ± 0.21396598892178337
	data : 0.11663427352905273
	model : 0.06977639198303223
			 train-loss:  2.059165456715752 	 ± 0.21311850897427376
	data : 0.11634268760681152
	model : 0.0704740047454834
			 train-loss:  2.0704881021609673 	 ± 0.22601850424224518
	data : 0.11563577651977539
	model : 0.07029848098754883
			 train-loss:  2.0724162403142676 	 ± 0.22430744437235414
	data : 0.11571135520935058
	model : 0.06987323760986328
			 train-loss:  2.068102507679551 	 ± 0.22442890042411714
	data : 0.11594114303588868
	model : 0.06948361396789551
			 train-loss:  2.072061289440502 	 ± 0.2242740042149261
	data : 0.11617984771728515
	model : 0.0693779468536377
			 train-loss:  2.0756428050143376 	 ± 0.22384399601397137
	data : 0.11617565155029297
	model : 0.06948614120483398
			 train-loss:  2.076024103582951 	 ± 0.22189011091277386
	data : 0.11627388000488281
	model : 0.06997108459472656
			 train-loss:  2.0730318447639204 	 ± 0.22112596891035846
	data : 0.11584572792053223
	model : 0.0699465274810791
			 train-loss:  2.071604255902565 	 ± 0.21951341912622574
	data : 0.11589293479919434
	model : 0.06908206939697266
			 train-loss:  2.069194330771764 	 ± 0.2184621144101936
	data : 0.1168304443359375
	model : 0.06827235221862793
			 train-loss:  2.07324161490456 	 ± 0.21892039081354006
	data : 0.11761832237243652
	model : 0.06802511215209961
			 train-loss:  2.0727276513653417 	 ± 0.21718482673908074
	data : 0.11774744987487792
	model : 0.06789741516113282
			 train-loss:  2.0681024562744867 	 ± 0.21851055206808445
	data : 0.11788387298583984
	model : 0.06746492385864258
			 train-loss:  2.0821662712842226 	 ± 0.24384758127124803
	data : 0.1182178020477295
	model : 0.06746559143066407
			 train-loss:  2.0791090690172638 	 ± 0.24319749889889905
	data : 0.11806516647338867
	model : 0.06851811408996582
			 train-loss:  2.078269732720924 	 ± 0.24144290899428006
	data : 0.11715888977050781
	model : 0.06792383193969727
			 train-loss:  2.074481932084952 	 ± 0.2416020262738138
	data : 0.11762642860412598
	model : 0.06770343780517578
			 train-loss:  2.072575569152832 	 ± 0.24032608436870506
	data : 0.11794815063476563
	model : 0.06755733489990234
			 train-loss:  2.0703030081762783 	 ± 0.23931310565018352
	data : 0.11804819107055664
	model : 0.06844172477722169
			 train-loss:  2.0689373459134783 	 ± 0.2378682326228726
	data : 0.117313814163208
	model : 0.06743359565734863
			 train-loss:  2.07154939544033 	 ± 0.23719606366679974
	data : 0.11806535720825195
	model : 0.06727728843688965
			 train-loss:  2.0757170816262565 	 ± 0.23814659038268599
	data : 0.11827650070190429
	model : 0.06756119728088379
			 train-loss:  2.0780950539732634 	 ± 0.23736899405823791
	data : 0.11798291206359864
	model : 0.06821484565734863
			 train-loss:  2.078314729639002 	 ± 0.23576716521443863
	data : 0.11759700775146484
	model : 0.06814641952514648
			 train-loss:  2.075942090352376 	 ± 0.2350778260824646
	data : 0.11756310462951661
	model : 0.06899528503417969
			 train-loss:  2.07478644816499 	 ± 0.2337405000660689
	data : 0.11708254814147949
	model : 0.07002196311950684
			 train-loss:  2.083887095575209 	 ± 0.24539669183490972
	data : 0.11614203453063965
	model : 0.06996693611145019
			 train-loss:  2.0807938545178146 	 ± 0.24532475990804187
	data : 0.11622796058654786
	model : 0.06971321105957032
			 train-loss:  2.080532037759129 	 ± 0.24377809330183606
	data : 0.11640934944152832
	model : 0.06988897323608398
			 train-loss:  2.076415541768074 	 ± 0.2449971620029721
	data : 0.11625776290893555
	model : 0.06901698112487793
			 train-loss:  2.079560109126715 	 ± 0.24509924823358326
	data : 0.11692719459533692
	model : 0.06972823143005372
			 train-loss:  2.0799750758380426 	 ± 0.2436287833729845
	data : 0.11610846519470215
	model : 0.06951794624328614
			 train-loss:  2.076602537947965 	 ± 0.24407485074629637
	data : 0.11613359451293945
	model : 0.06956663131713867
			 train-loss:  2.0753496133145832 	 ± 0.24288604533719046
	data : 0.11614360809326171
	model : 0.06945505142211914
			 train-loss:  2.0749914716271793 	 ± 0.24147538715363173
	data : 0.11629743576049804
	model : 0.07014851570129395
			 train-loss:  2.0783352006313414 	 ± 0.242038591219293
	data : 0.11551027297973633
	model : 0.06920061111450196
			 train-loss:  2.081356381547862 	 ± 0.24226903112210532
	data : 0.11649045944213868
	model : 0.06936306953430176
			 train-loss:  2.0853415686975825 	 ± 0.24373964038085713
	data : 0.11631488800048828
	model : 0.06955327987670898
			 train-loss:  2.083219284421942 	 ± 0.24318276104411055
	data : 0.11600127220153808
	model : 0.06955204010009766
			 train-loss:  2.0839864042070175 	 ± 0.24193623553391944
	data : 0.11616120338439942
	model : 0.06964654922485351
			 train-loss:  2.0820371973645555 	 ± 0.24131279987037782
	data : 0.11610374450683594
	model : 0.06970129013061524
			 train-loss:  2.0797806706117545 	 ± 0.24096115118994882
	data : 0.11608481407165527
	model : 0.06985325813293457
			 train-loss:  2.0807510947668426 	 ± 0.23984284302441428
	data : 0.11609120368957519
	model : 0.07023448944091797
			 train-loss:  2.0794026382425996 	 ± 0.23891783271312533
	data : 0.11590189933776855
	model : 0.07068672180175781
			 train-loss:  2.077874218790155 	 ± 0.23811858531071195
	data : 0.11561646461486816
	model : 0.07079138755798339
			 train-loss:  2.078731805086136 	 ± 0.23702257070995422
	data : 0.11554498672485351
	model : 0.07083444595336914
			 train-loss:  2.08572122976952 	 ± 0.2455408812843587
	data : 0.11542606353759766
	model : 0.07069196701049804
			 train-loss:  2.0878078791559957 	 ± 0.24514784363618072
	data : 0.11555213928222656
	model : 0.07116341590881348
			 train-loss:  2.0870631848922883 	 ± 0.24401796645244672
	data : 0.11492671966552734
	model : 0.07073421478271484
			 train-loss:  2.083534677028656 	 ± 0.24532000872064913
	data : 0.11522555351257324
	model : 0.07050719261169433
			 train-loss:  2.087110127552901 	 ± 0.24670717634812842
	data : 0.115496826171875
	model : 0.07057828903198242
			 train-loss:  2.0873986692989575 	 ± 0.2455119745459283
	data : 0.11547613143920898
	model : 0.07077937126159668
			 train-loss:  2.084781943015682 	 ± 0.24574243561154316
	data : 0.11527051925659179
	model : 0.07030439376831055
			 train-loss:  2.084055034013895 	 ± 0.24466937434932925
	data : 0.11579933166503906
	model : 0.07037715911865235
			 train-loss:  2.0857494967324395 	 ± 0.24411387335119095
	data : 0.11556272506713867
	model : 0.07061152458190918
			 train-loss:  2.0829689547700703 	 ± 0.24462460392237137
	data : 0.11525321006774902
	model : 0.06981134414672852
			 train-loss:  2.0806587043209612 	 ± 0.24463786006841914
	data : 0.11594624519348144
	model : 0.06883049011230469
			 train-loss:  2.081731770877485 	 ± 0.2437555022068385
	data : 0.11680717468261718
	model : 0.06842947006225586
			 train-loss:  2.078706545567294 	 ± 0.24466313949652502
	data : 0.11711153984069825
	model : 0.06824617385864258
			 train-loss:  2.0800348617813804 	 ± 0.24394300922774956
	data : 0.11740632057189941
	model : 0.06808571815490723
			 train-loss:  2.0789334945850544 	 ± 0.2431162540485934
	data : 0.11757426261901856
	model : 0.0680276870727539
			 train-loss:  2.07963875574725 	 ± 0.24214251143737364
	data : 0.11775555610656738
	model : 0.06831436157226563
			 train-loss:  2.0806984774834287 	 ± 0.24132943648358063
	data : 0.11742782592773438
	model : 0.06837587356567383
			 train-loss:  2.0822709715157224 	 ± 0.24084941345203345
	data : 0.11740503311157227
	model : 0.06861982345581055
			 train-loss:  2.083301015522169 	 ± 0.24005201869144555
	data : 0.11711587905883789
	model : 0.06879901885986328
			 train-loss:  2.0829620299668146 	 ± 0.2390427147381621
	data : 0.11717138290405274
	model : 0.06961216926574706
			 train-loss:  2.0789184621256642 	 ± 0.24197041639748415
	data : 0.11631293296813965
	model : 0.0699242115020752
			 train-loss:  2.0802214681091957 	 ± 0.2413548096469514
	data : 0.11621537208557128
	model : 0.06990189552307129
			 train-loss:  2.0797312049304737 	 ± 0.24039757187953553
	data : 0.11624646186828613
	model : 0.06980066299438477
			 train-loss:  2.080251583456993 	 ± 0.23946111463942518
	data : 0.11628232002258301
	model : 0.06975255012512208
			 train-loss:  2.078074690724207 	 ± 0.23965890580152197
	data : 0.11629252433776856
	model : 0.0696601390838623
			 train-loss:  2.077506603764706 	 ± 0.2387564669213666
	data : 0.11646761894226074
	model : 0.0689126968383789
			 train-loss:  2.0754970292734907 	 ± 0.2388176743653433
	data : 0.11702914237976074
	model : 0.0680504322052002
			 train-loss:  2.0765850476680265 	 ± 0.23815863697669137
	data : 0.1179384708404541
	model : 0.06794958114624024
			 train-loss:  2.077681668281555 	 ± 0.23751820897595352
	data : 0.11793031692504882
	model : 0.06795997619628906
			 train-loss:  2.0749495171365284 	 ± 0.2385377193059401
	data : 0.1180051326751709
	model : 0.06808958053588868
			 train-loss:  2.0758276539524707 	 ± 0.23780111811291157
	data : 0.11776914596557617
	model : 0.06901512145996094
			 train-loss:  2.0746220033615828 	 ± 0.23725974408622527
	data : 0.116981840133667
	model : 0.06996994018554688
			 train-loss:  2.0764848261840583 	 ± 0.23727618203857495
	data : 0.11594247817993164
	model : 0.06912312507629395
			 train-loss:  2.074254042368669 	 ± 0.23771593316958928
	data : 0.11679210662841796
	model : 0.06904015541076661
			 train-loss:  2.073989443196595 	 ± 0.23682609897506401
	data : 0.11682720184326172
	model : 0.0690232753753662
			 train-loss:  2.073146098490917 	 ± 0.23612469974670713
	data : 0.11695008277893067
	model : 0.06875066757202149
			 train-loss:  2.0727620062075163 	 ± 0.23527672600843047
	data : 0.1171884536743164
	model : 0.06821374893188477
			 train-loss:  2.072682756986191 	 ± 0.23439896556160592
	data : 0.11774506568908691
	model : 0.06887359619140625
			 train-loss:  2.072066142823961 	 ± 0.23363826600544832
	data : 0.11734366416931152
	model : 0.06896791458129883
			 train-loss:  2.069138828445883 	 ± 0.23524944931186292
	data : 0.11719117164611817
	model : 0.06838951110839844
			 train-loss:  2.0704537725796666 	 ± 0.23489039912423296
	data : 0.11784768104553223
	model : 0.06878190040588379
			 train-loss:  2.0697926913482556 	 ± 0.23416567750776351
	data : 0.11757335662841797
	model : 0.06935529708862305
			 train-loss:  2.0683848737812727 	 ± 0.23390722004382453
	data : 0.11715645790100097
	model : 0.06958146095275879
			 train-loss:  2.0674357959202356 	 ± 0.23333878261631882
	data : 0.11686911582946777
	model : 0.06951394081115722
			 train-loss:  2.064673630058343 	 ± 0.23479560698689717
	data : 0.1169011116027832
	model : 0.07014055252075195
			 train-loss:  2.0638240172829425 	 ± 0.23418480849733073
	data : 0.11634798049926758
	model : 0.06935286521911621
			 train-loss:  2.0696865045107327 	 ± 0.2435967442225057
	data : 0.11693892478942872
	model : 0.06912174224853515
			 train-loss:  2.0695331626468234 	 ± 0.24275637432229308
	data : 0.1168938159942627
	model : 0.06891627311706543
			 train-loss:  2.067356933396438 	 ± 0.24332328118803284
	data : 0.1170083999633789
	model : 0.06887779235839844
			 train-loss:  2.0669173214533556 	 ± 0.24254632469857476
	data : 0.11704235076904297
	model : 0.06845326423645019
			 train-loss:  2.0687605296673417 	 ± 0.24274378893212628
	data : 0.11718997955322266
	model : 0.06898975372314453
			 train-loss:  2.0698392568407833 	 ± 0.24227559745848845
	data : 0.11673736572265625
	model : 0.06876444816589355
			 train-loss:  2.0691554098321285 	 ± 0.241604499380992
	data : 0.11698770523071289
	model : 0.06899085044860839
			 train-loss:  2.070480613708496 	 ± 0.2413405295264537
	data : 0.11679816246032715
	model : 0.06910948753356934
			 train-loss:  2.072223385438224 	 ± 0.24148521496575098
	data : 0.11662073135375976
	model : 0.07005810737609863
			 train-loss:  2.071148264564966 	 ± 0.24105185258064327
	data : 0.11602325439453125
	model : 0.07019562721252441
			 train-loss:  2.07131985355826 	 ± 0.24027212319843919
	data : 0.11593747138977051
	model : 0.0701176643371582
			 train-loss:  2.0692299983718176 	 ± 0.24088180762430328
	data : 0.1162069320678711
	model : 0.07006440162658692
			 train-loss:  2.069537866500116 	 ± 0.24013390663160628
	data : 0.11630373001098633
	model : 0.07004694938659668
			 train-loss:  2.0723280960168595 	 ± 0.2418705939687075
	data : 0.11625180244445801
	model : 0.06906781196594239
			 train-loss:  2.0742156361318695 	 ± 0.24224896685859532
	data : 0.11704602241516113
	model : 0.06940755844116211
			 train-loss:  2.0762641497805148 	 ± 0.24284146422802977
	data : 0.11673216819763184
	model : 0.06976494789123536
			 train-loss:  2.075022648715373 	 ± 0.24257908637444212
	data : 0.11617307662963867
	model : 0.06988611221313476
			 train-loss:  2.075695104151964 	 ± 0.24196845564670796
	data : 0.11599760055541992
	model : 0.0697357177734375
			 train-loss:  2.0757765362721794 	 ± 0.24121802956764532
	data : 0.11616230010986328
	model : 0.07012362480163574
			 train-loss:  2.0772829872590526 	 ± 0.2412308782580642
	data : 0.11581153869628906
	model : 0.06974153518676758
			 train-loss:  2.077084666380853 	 ± 0.24050301468933996
	data : 0.1161562442779541
	model : 0.06942529678344726
			 train-loss:  2.0811309487354466 	 ± 0.24527068338286392
	data : 0.11650428771972657
	model : 0.06947121620178223
			 train-loss:  2.0824721748178656 	 ± 0.24512880999774844
	data : 0.11652536392211914
	model : 0.0696481704711914
			 train-loss:  2.0832785202796202 	 ± 0.24460874601850374
	data : 0.1164163589477539
	model : 0.06973447799682617
			 train-loss:  2.08435712959952 	 ± 0.24427091239827448
	data : 0.11639885902404785
	model : 0.06904501914978027
			 train-loss:  2.0833968144087565 	 ± 0.24385880873567248
	data : 0.1170501708984375
	model : 0.0684896469116211
			 train-loss:  2.083610648234215 	 ± 0.24315205929746525
	data : 0.11764211654663086
	model : 0.06823263168334961
			 train-loss:  2.083179515249589 	 ± 0.2425006289063767
	data : 0.11794219017028809
	model : 0.06767706871032715
			 train-loss:  2.0830180888984637 	 ± 0.24179968357460185
	data : 0.11841979026794433
	model : 0.06676445007324219
			 train-loss:  2.084245575721874 	 ± 0.24162949290116764
	data : 0.1191584587097168
	model : 0.06788849830627441
			 train-loss:  2.085822366565638 	 ± 0.241815974883693
	data : 0.11829252243041992
	model : 0.06822500228881836
			 train-loss:  2.087288532448911 	 ± 0.24189003870015677
	data : 0.11791300773620605
	model : 0.0683504581451416
			 train-loss:  2.0897148302623205 	 ± 0.2433120788494547
	data : 0.11769919395446778
	model : 0.06885428428649902
			 train-loss:  2.0908610732717947 	 ± 0.2430932485633909
	data : 0.11714224815368653
	model : 0.06970753669738769
			 train-loss:  2.092220899748937 	 ± 0.2430759300640309
	data : 0.11624698638916016
	model : 0.06944837570190429
			 train-loss:  2.0924392181835816 	 ± 0.2424095724482943
	data : 0.11642069816589355
	model : 0.07001042366027832
			 train-loss:  2.0934259525224483 	 ± 0.24208971063315993
	data : 0.11576409339904785
	model : 0.06989946365356445
			 train-loss:  2.092173382308748 	 ± 0.24199725210291198
	data : 0.1158132553100586
	model : 0.06983518600463867
			 train-loss:  2.0905580362562315 	 ± 0.24229899108070024
	data : 0.11615257263183594
	model : 0.06987004280090332
			 train-loss:  2.0903185592902886 	 ± 0.24165389601026235
	data : 0.11619563102722168
	model : 0.06983380317687989
			 train-loss:  2.0907975168176036 	 ± 0.24107934208524603
	data : 0.11618685722351074
	model : 0.06984000205993653
			 train-loss:  2.0911296437615934 	 ± 0.24046531998553733
	data : 0.11629242897033691
	model : 0.0690920352935791
			 train-loss:  2.090423910037891 	 ± 0.24000552771471145
	data : 0.11671228408813476
	model : 0.06911487579345703
			 train-loss:  2.09225526420019 	 ± 0.24065208370501398
	data : 0.11656346321105956
	model : 0.06911716461181641
			 train-loss:  2.091239188444168 	 ± 0.24040747945682872
	data : 0.11659584045410157
	model : 0.06889019012451172
			 train-loss:  2.0927139812327447 	 ± 0.240613922121392
	data : 0.11680636405944825
	model : 0.06928129196166992
			 train-loss:  2.092841390579466 	 ± 0.23998289164652947
	data : 0.1166884422302246
	model : 0.07019100189208985
			 train-loss:  2.092044937610626 	 ± 0.23960084174210122
	data : 0.11623668670654297
	model : 0.07033071517944336
			 train-loss:  2.093151939476972 	 ± 0.23945945599213164
	data : 0.11619281768798828
	model : 0.07010445594787598
			 train-loss:  2.0943188313394785 	 ± 0.23937889108366156
	data : 0.11644911766052246
	model : 0.0705233097076416
			 train-loss:  2.09363157329164 	 ± 0.23894776995078956
	data : 0.1162501335144043
	model : 0.07023239135742188
			 train-loss:  2.094006310418709 	 ± 0.2383879818364956
	data : 0.11639466285705566
	model : 0.0692373275756836
			 train-loss:  2.093271048252399 	 ± 0.23799638319764027
	data : 0.11712608337402344
	model : 0.06916346549987792
			 train-loss:  2.0930151033158206 	 ± 0.23741537697001308
	data : 0.11716127395629883
	model : 0.06838154792785645
			 train-loss:  2.0922271600229485 	 ± 0.23706882247916292
	data : 0.1177018165588379
	model : 0.06801800727844239
			 train-loss:  2.093467708789941 	 ± 0.237109585123918
	data : 0.11776294708251953
	model : 0.06792125701904297
			 train-loss:  2.0947256938895986 	 ± 0.23717457334883213
	data : 0.11795930862426758
	model : 0.06790761947631836
			 train-loss:  2.0924498200416566 	 ± 0.23874936671028535
	data : 0.11815023422241211
	model : 0.06788535118103027
			 train-loss:  2.092297814971772 	 ± 0.23816442398915766
	data : 0.1180305004119873
	model : 0.06881446838378906
			 train-loss:  2.093486844903172 	 ± 0.2381714963043377
	data : 0.11718640327453614
	model : 0.06880698204040528
			 train-loss:  2.095055442725496 	 ± 0.2386278393625485
	data : 0.1171647548675537
	model : 0.06788439750671386
			 train-loss:  2.0948556392800572 	 ± 0.2380592703698078
	data : 0.11785745620727539
	model : 0.06862916946411132
			 train-loss:  2.094850647158739 	 ± 0.2374779388864985
	data : 0.11722517013549805
	model : 0.06864991188049316
			 train-loss:  2.095821381772606 	 ± 0.23730820117198065
	data : 0.11741909980773926
	model : 0.06870021820068359
			 train-loss:  2.0942050581393032 	 ± 0.23786824872718557
	data : 0.11744236946105957
	model : 0.06925029754638672
			 train-loss:  2.095027271371621 	 ± 0.2375904404975155
	data : 0.11700572967529296
	model : 0.07020936012268067
			 train-loss:  2.0950875704368337 	 ± 0.23702295616609959
	data : 0.11609630584716797
	model : 0.0704495906829834
			 train-loss:  2.0950845775150118 	 ± 0.23645794631482353
	data : 0.11573944091796876
	model : 0.06979660987854004
			 train-loss:  2.094459483973788 	 ± 0.23607081287592885
	data : 0.1163564682006836
	model : 0.069537353515625
			 train-loss:  2.0950687999995248 	 ± 0.23567963670694209
	data : 0.11667108535766602
	model : 0.06888461112976074
			 train-loss:  2.0943737388216834 	 ± 0.23534344339730928
	data : 0.11722002029418946
	model : 0.06881003379821778
			 train-loss:  2.092447336032012 	 ± 0.2364702299687998
	data : 0.11732621192932129
	model : 0.06879935264587403
			 train-loss:  2.0903445266013922 	 ± 0.2379166912109472
	data : 0.11732773780822754
	model : 0.06944317817687988
			 train-loss:  2.0926055345270367 	 ± 0.23966937328816804
	data : 0.11658005714416504
	model : 0.06974830627441406
			 train-loss:  2.0909068600922684 	 ± 0.24041623993648614
	data : 0.11631865501403808
	model : 0.0694737434387207
			 train-loss:  2.0925395691066706 	 ± 0.24106699537439127
	data : 0.11657576560974121
	model : 0.0694765567779541
			 train-loss:  2.0911700703782032 	 ± 0.2413644621250999
	data : 0.11651206016540527
	model : 0.0689117431640625
			 train-loss:  2.092035180872137 	 ± 0.24115535047936776
	data : 0.11702890396118164
	model : 0.0688093662261963
			 train-loss:  2.0911067713439735 	 ± 0.24100286703194435
	data : 0.11716651916503906
	model : 0.06867527961730957
			 train-loss:  2.0909827910028063 	 ± 0.24046651863671353
	data : 0.11718211174011231
	model : 0.06892704963684082
			 train-loss:  2.0925663891394577 	 ± 0.2410841623136275
	data : 0.11708779335021972
	model : 0.06834969520568848
			 train-loss:  2.0931848495134284 	 ± 0.24072265784555155
	data : 0.11760272979736328
	model : 0.06805882453918458
			 train-loss:  2.092014173931546 	 ± 0.24082533468127917
	data : 0.11779508590698243
	model : 0.0675919532775879
			 train-loss:  2.0910653213484096 	 ± 0.2407130883561484
	data : 0.11821427345275878
	model : 0.06690216064453125
			 train-loss:  2.090132404529051 	 ± 0.240591420789318
	data : 0.11888422966003417
	model : 0.06657629013061524
			 train-loss:  2.0908588725223876 	 ± 0.24031261744268276
	data : 0.1191525936126709
	model : 0.06641287803649902
			 train-loss:  2.09047864305921 	 ± 0.23985606724969222
	data : 0.11950445175170898
	model : 0.06724305152893066
			 train-loss:  2.093438075936359 	 ± 0.24348806160004222
	data : 0.11873788833618164
	model : 0.06728940010070801
			 train-loss:  2.094060656312224 	 ± 0.24314385527430457
	data : 0.11853928565979004
	model : 0.06792664527893066
			 train-loss:  2.0932250418539704 	 ± 0.24295145084256584
	data : 0.11777119636535645
	model : 0.06828012466430664
			 train-loss:  2.0919105566622362 	 ± 0.24325489858077246
	data : 0.11720662117004395
	model : 0.06825642585754395
			 train-loss:  2.091455983300494 	 ± 0.24283372141156231
	data : 0.11703519821166992
	model : 0.06797027587890625
			 train-loss:  2.090482328800445 	 ± 0.24277380500536025
	data : 0.11717844009399414
	model : 0.06802468299865723
			 train-loss:  2.089488522986234 	 ± 0.24273746334331137
	data : 0.11729445457458496
	model : 0.06789255142211914
			 train-loss:  2.091045727206685 	 ± 0.24340323568794991
	data : 0.11745991706848144
	model : 0.0671654224395752
			 train-loss:  2.0908765717714775 	 ± 0.2429053054548717
	data : 0.11799993515014648
	model : 0.06710696220397949
			 train-loss:  2.0918119008571034 	 ± 0.24282570916689275
	data : 0.11788983345031738
	model : 0.06660041809082032
			 train-loss:  2.0906645566225053 	 ± 0.2429676103027243
	data : 0.1185835361480713
	model : 0.0667497158050537
			 train-loss:  2.091542873145139 	 ± 0.24284450570412236
	data : 0.11825394630432129
	model : 0.06694793701171875
			 train-loss:  2.0913042697039517 	 ± 0.2423705479391482
	data : 0.11817965507507325
	model : 0.0672839641571045
			 train-loss:  2.0900943818896884 	 ± 0.2426025273663529
	data : 0.11832427978515625
	model : 0.06816258430480956
			 train-loss:  2.0899018391233977 	 ± 0.2421234848092882
	data : 0.1177785873413086
	model : 0.06889896392822266
			 train-loss:  2.089985648953185 	 ± 0.24163239648508847
	data : 0.11708674430847169
	model : 0.06905512809753418
			 train-loss:  2.089812545272393 	 ± 0.241155995343795
	data : 0.11715183258056641
	model : 0.06855435371398926
			 train-loss:  2.089100723324517 	 ± 0.240926149867982
	data : 0.11766953468322754
	model : 0.06865434646606446
			 train-loss:  2.0901216388710084 	 ± 0.24097467906708697
	data : 0.11751809120178222
	model : 0.06853971481323243
			 train-loss:  2.0885754938585213 	 ± 0.2417197714435593
	data : 0.11766533851623535
	model : 0.06799521446228027
			 train-loss:  2.0886434836387635 	 ± 0.24123823317427065
	data : 0.11814341545104981
	model : 0.06755285263061524
			 train-loss:  2.088217162041075 	 ± 0.240851544639207
	data : 0.11829814910888672
	model : 0.06790742874145508
			 train-loss:  2.088838008188066 	 ± 0.24057435065637184
	data : 0.11791605949401855
	model : 0.06790776252746582
			 train-loss:  2.0877096968677202 	 ± 0.24076560595383756
	data : 0.11760563850402832
	model : 0.06769146919250488
			 train-loss:  2.0871520342789296 	 ± 0.240454852494414
	data : 0.1176365852355957
	model : 0.06805806159973145
			 train-loss:  2.0864201470917347 	 ± 0.24026621501995724
	data : 0.11733427047729492
	model : 0.0683210849761963
			 train-loss:  2.0861354544758797 	 ± 0.239839576323059
	data : 0.11628503799438476
	model : 0.05915889739990234
#epoch  79    val-loss:  2.4524500370025635  train-loss:  2.0861354544758797  lr:  4.8828125e-06
			 train-loss:  1.8721035718917847 	 ± 0.0
	data : 5.9377100467681885
	model : 0.0764470100402832
			 train-loss:  1.8529181480407715 	 ± 0.019185423851013184
	data : 3.032060980796814
	model : 0.07133626937866211
			 train-loss:  2.0064289569854736 	 ± 0.21766148928540935
	data : 2.0611887772878013
	model : 0.07073934872945149
			 train-loss:  2.0570637583732605 	 ± 0.20790392558390677
	data : 1.5746841430664062
	model : 0.0704450011253357
			 train-loss:  2.0766688346862794 	 ± 0.1900438630619681
	data : 1.2829492568969727
	model : 0.06947660446166992
			 train-loss:  2.115973432858785 	 ± 0.19447745908915537
	data : 0.11917400360107422
	model : 0.0673647403717041
			 train-loss:  2.1603762422289168 	 ± 0.21035225243147854
	data : 0.11783509254455567
	model : 0.0671626091003418
			 train-loss:  2.1396494805812836 	 ± 0.2042651509164167
	data : 0.11797080039978028
	model : 0.06722421646118164
			 train-loss:  2.169792996512519 	 ± 0.210611671836901
	data : 0.11818928718566894
	model : 0.06722517013549804
			 train-loss:  2.125017857551575 	 ± 0.24075893815866498
	data : 0.11801815032958984
	model : 0.06729245185852051
			 train-loss:  2.1077148047360508 	 ± 0.23598578213603696
	data : 0.11806979179382324
	model : 0.06733684539794922
			 train-loss:  2.133337120215098 	 ± 0.24139189881134537
	data : 0.11801204681396485
	model : 0.06732425689697266
			 train-loss:  2.122220231936528 	 ± 0.235097341168861
	data : 0.11790533065795898
	model : 0.06706247329711915
			 train-loss:  2.1594486151422774 	 ± 0.2633253332250878
	data : 0.118182373046875
	model : 0.06709804534912109
			 train-loss:  2.1592314799626666 	 ± 0.25439773802290466
	data : 0.11828632354736328
	model : 0.06713747978210449
			 train-loss:  2.152098350226879 	 ± 0.24786396296988386
	data : 0.11818671226501465
	model : 0.06696701049804688
			 train-loss:  2.1577686071395874 	 ± 0.24153065808155322
	data : 0.11831393241882324
	model : 0.06793522834777832
			 train-loss:  2.14837908744812 	 ± 0.23789679157817264
	data : 0.11759743690490723
	model : 0.06757078170776368
			 train-loss:  2.1211120768597254 	 ± 0.2588416947922587
	data : 0.11800942420959473
	model : 0.06823787689208985
			 train-loss:  2.1213593542575837 	 ± 0.2522899791725797
	data : 0.11767168045043945
	model : 0.06902103424072266
			 train-loss:  2.107607779048738 	 ± 0.25377428258407675
	data : 0.11715965270996094
	model : 0.0698631763458252
			 train-loss:  2.115939199924469 	 ± 0.250861943708077
	data : 0.11644215583801269
	model : 0.06958727836608887
			 train-loss:  2.131617343944052 	 ± 0.2561313055557266
	data : 0.11659049987792969
	model : 0.07027311325073242
			 train-loss:  2.1443788756926856 	 ± 0.2580997501831143
	data : 0.11585874557495117
	model : 0.06968178749084472
			 train-loss:  2.1405679845809935 	 ± 0.2535732856362568
	data : 0.11624021530151367
	model : 0.06879644393920899
			 train-loss:  2.1349493952897878 	 ± 0.2502310343512018
	data : 0.11692423820495605
	model : 0.0689021110534668
			 train-loss:  2.1308820291801736 	 ± 0.24642768815747115
	data : 0.11676530838012696
	model : 0.06981110572814941
			 train-loss:  2.1315829711300984 	 ± 0.24201459414950166
	data : 0.11595287322998046
	model : 0.06970844268798829
			 train-loss:  2.1482837734551263 	 ± 0.2536947817342903
	data : 0.11589369773864747
	model : 0.07060251235961915
			 train-loss:  2.1520553390185038 	 ± 0.2502562500726052
	data : 0.11507248878479004
	model : 0.07046327590942383
			 train-loss:  2.163009032126396 	 ± 0.25339184727476294
	data : 0.1151113510131836
	model : 0.07049064636230469
			 train-loss:  2.1510512121021748 	 ± 0.25813488602609924
	data : 0.11497211456298828
	model : 0.06989116668701172
			 train-loss:  2.146739479267236 	 ± 0.25536117966049315
	data : 0.11565709114074707
	model : 0.0697817325592041
			 train-loss:  2.1457673696910633 	 ± 0.2516398129228191
	data : 0.11581459045410156
	model : 0.06856846809387207
			 train-loss:  2.1500640562602453 	 ± 0.24928110630600345
	data : 0.11703076362609863
	model : 0.06960983276367187
			 train-loss:  2.1591546568605633 	 ± 0.2516094092109807
	data : 0.11634206771850586
	model : 0.06952443122863769
			 train-loss:  2.156369695792327 	 ± 0.24874787060253184
	data : 0.11638641357421875
	model : 0.06948328018188477
			 train-loss:  2.160805040284207 	 ± 0.24693131596366266
	data : 0.1163053035736084
	model : 0.06971549987792969
			 train-loss:  2.1628386699236355 	 ± 0.2440671342652238
	data : 0.1161536693572998
	model : 0.07002077102661133
			 train-loss:  2.154975193738937 	 ± 0.24594935033114676
	data : 0.11582589149475098
	model : 0.06985435485839844
			 train-loss:  2.150967432231438 	 ± 0.24425023388690692
	data : 0.11602449417114258
	model : 0.06989083290100098
			 train-loss:  2.155819066933223 	 ± 0.2433162894350035
	data : 0.11618051528930665
	model : 0.06993246078491211
			 train-loss:  2.1513999229253726 	 ± 0.2421698123871494
	data : 0.11609187126159667
	model : 0.07070455551147461
			 train-loss:  2.1575085791674526 	 ± 0.2427301458958224
	data : 0.11524662971496583
	model : 0.06976885795593261
			 train-loss:  2.150963396496243 	 ± 0.24391303951503052
	data : 0.1158991813659668
	model : 0.06994476318359374
			 train-loss:  2.1447125984274824 	 ± 0.2448642354268164
	data : 0.11582813262939454
	model : 0.0698540210723877
			 train-loss:  2.1417523698603853 	 ± 0.24307586676406187
	data : 0.11592655181884766
	model : 0.06888136863708497
			 train-loss:  2.132709721724192 	 ± 0.24839099629818873
	data : 0.11686739921569825
	model : 0.06711654663085938
			 train-loss:  2.134443253886943 	 ± 0.24613652477627618
	data : 0.11858148574829101
	model : 0.06796016693115234
			 train-loss:  2.1370442152023315 	 ± 0.24434199401823423
	data : 0.11773672103881835
	model : 0.06695041656494141
			 train-loss:  2.134991444793402 	 ± 0.24236966828110643
	data : 0.11841115951538086
	model : 0.0670473575592041
			 train-loss:  2.1337762016516466 	 ± 0.2401847196585272
	data : 0.11838731765747071
	model : 0.06802468299865723
			 train-loss:  2.130678948366417 	 ± 0.23895411354385285
	data : 0.11748027801513672
	model : 0.06900463104248047
			 train-loss:  2.1303335803526418 	 ± 0.23674458814256732
	data : 0.11650009155273437
	model : 0.06914381980895996
			 train-loss:  2.1346596002578737 	 ± 0.23672668981966344
	data : 0.11655178070068359
	model : 0.07046275138854981
			 train-loss:  2.1276174123798097 	 ± 0.24034642167803005
	data : 0.11546201705932617
	model : 0.06995220184326172
			 train-loss:  2.1207495258565534 	 ± 0.24370957554772374
	data : 0.11610956192016601
	model : 0.06992034912109375
			 train-loss:  2.1162838422018906 	 ± 0.24394062681467754
	data : 0.11620688438415527
	model : 0.06991677284240723
			 train-loss:  2.1118601055468544 	 ± 0.24419963873747277
	data : 0.1163902759552002
	model : 0.07001609802246093
			 train-loss:  2.114766267935435 	 ± 0.24318279854892338
	data : 0.11633477210998536
	model : 0.0687180519104004
			 train-loss:  2.111850492289809 	 ± 0.24223646511372493
	data : 0.1173238754272461
	model : 0.06924810409545898
			 train-loss:  2.1133237077343847 	 ± 0.24055034757574262
	data : 0.11672639846801758
	model : 0.06904315948486328
			 train-loss:  2.110355804836939 	 ± 0.23977512344740343
	data : 0.11664576530456543
	model : 0.06848587989807128
			 train-loss:  2.1134496740996838 	 ± 0.2391585944726978
	data : 0.11700787544250488
	model : 0.0682858943939209
			 train-loss:  2.115373967244075 	 ± 0.23781057144546133
	data : 0.11718554496765136
	model : 0.0690927505493164
			 train-loss:  2.115137721552993 	 ± 0.236009785574057
	data : 0.11652626991271972
	model : 0.06889214515686035
			 train-loss:  2.1223071475527178 	 ± 0.24137462694106854
	data : 0.11663217544555664
	model : 0.06907410621643066
			 train-loss:  2.1199430630487552 	 ± 0.24037341090789674
	data : 0.11675348281860351
	model : 0.07011704444885254
			 train-loss:  2.1152016218157783 	 ± 0.24180719678820145
	data : 0.11583285331726074
	model : 0.0705411434173584
			 train-loss:  2.1133031657763888 	 ± 0.2405911672331291
	data : 0.11548404693603516
	model : 0.07062592506408691
			 train-loss:  2.1128674812719854 	 ± 0.23891866412714388
	data : 0.11552066802978515
	model : 0.07085504531860351
			 train-loss:  2.1083791007598243 	 ± 0.24024915289585425
	data : 0.11525950431823731
	model : 0.0700183391571045
			 train-loss:  2.1020884448534822 	 ± 0.24449577620073534
	data : 0.11618075370788575
	model : 0.07022953033447266
			 train-loss:  2.1039086902463757 	 ± 0.24333565641940047
	data : 0.11585183143615722
	model : 0.06886210441589355
			 train-loss:  2.106393502553304 	 ± 0.24265127807639647
	data : 0.11696233749389648
	model : 0.06892662048339844
			 train-loss:  2.10278173653703 	 ± 0.2430705137246331
	data : 0.11685400009155274
	model : 0.06891274452209473
			 train-loss:  2.10740711936703 	 ± 0.244830382158526
	data : 0.11687774658203125
	model : 0.06985292434692383
			 train-loss:  2.11053513105099 	 ± 0.24479957745323552
	data : 0.11602969169616699
	model : 0.06895480155944825
			 train-loss:  2.1100227425370037 	 ± 0.2432873689458231
	data : 0.11707053184509278
	model : 0.0698936939239502
			 train-loss:  2.1053856417536734 	 ± 0.2452500727426595
	data : 0.11627073287963867
	model : 0.06905279159545899
			 train-loss:  2.1018382914272355 	 ± 0.2457879793376008
	data : 0.11710786819458008
	model : 0.06810827255249023
			 train-loss:  2.093130877832087 	 ± 0.2565469655550538
	data : 0.11786046028137206
	model : 0.06825060844421386
			 train-loss:  2.0943267532141814 	 ± 0.2552266584571932
	data : 0.11770014762878418
	model : 0.06843881607055664
			 train-loss:  2.092993913661866 	 ± 0.25399332585273315
	data : 0.11758666038513184
	model : 0.06753826141357422
			 train-loss:  2.090973273445578 	 ± 0.2531730803197225
	data : 0.11834139823913574
	model : 0.06750154495239258
			 train-loss:  2.09269707701927 	 ± 0.25219808998270554
	data : 0.1183426856994629
	model : 0.06835522651672363
			 train-loss:  2.0945797405023683 	 ± 0.25135158209894115
	data : 0.11755251884460449
	model : 0.06846704483032226
			 train-loss:  2.102382042191245 	 ± 0.260299624781562
	data : 0.11736893653869629
	model : 0.06854081153869629
			 train-loss:  2.1028176023719016 	 ± 0.25886538430091544
	data : 0.11721677780151367
	model : 0.06853260993957519
			 train-loss:  2.1002828571531507 	 ± 0.25853149981663054
	data : 0.11729493141174316
	model : 0.06943082809448242
			 train-loss:  2.0988341255502387 	 ± 0.2574741561392725
	data : 0.11639218330383301
	model : 0.06958770751953125
			 train-loss:  2.0950507871482684 	 ± 0.2586018324264557
	data : 0.11628975868225097
	model : 0.06918778419494628
			 train-loss:  2.0991641821399813 	 ± 0.26021618780129996
	data : 0.11663703918457032
	model : 0.06877079010009765
			 train-loss:  2.1005031390393034 	 ± 0.2591502458936245
	data : 0.11709637641906738
	model : 0.06957554817199707
			 train-loss:  2.0963030012030353 	 ± 0.26097927795407877
	data : 0.11641125679016114
	model : 0.06962776184082031
			 train-loss:  2.0949691720306873 	 ± 0.2599417568761887
	data : 0.11636724472045898
	model : 0.06968040466308593
			 train-loss:  2.094070339940258 	 ± 0.25874829519006226
	data : 0.11644058227539063
	model : 0.06949434280395508
			 train-loss:  2.0930702382204482 	 ± 0.257613139788399
	data : 0.11657161712646484
	model : 0.06979165077209473
			 train-loss:  2.089740762806902 	 ± 0.2584193314087498
	data : 0.11633820533752441
	model : 0.06988887786865235
			 train-loss:  2.093269290924072 	 ± 0.2595098184924421
	data : 0.11622962951660157
	model : 0.06903772354125977
			 train-loss:  2.094184514319543 	 ± 0.25838406235182376
	data : 0.11695685386657714
	model : 0.06854143142700195
			 train-loss:  2.094666983567032 	 ± 0.25716006985224277
	data : 0.11759634017944336
	model : 0.06906180381774903
			 train-loss:  2.0945622504336163 	 ± 0.2559108612315276
	data : 0.11703286170959473
	model : 0.06923098564147949
			 train-loss:  2.098660489687553 	 ± 0.2580515394333453
	data : 0.11690683364868164
	model : 0.06918191909790039
			 train-loss:  2.0945941334679015 	 ± 0.2601462418412142
	data : 0.11690483093261719
	model : 0.06969780921936035
			 train-loss:  2.094488694982709 	 ± 0.2589184832998035
	data : 0.11655287742614746
	model : 0.0698310375213623
			 train-loss:  2.0961563074700185 	 ± 0.2582770393589081
	data : 0.11623625755310059
	model : 0.06856775283813477
			 train-loss:  2.0939362104292267 	 ± 0.25810222061553134
	data : 0.11752429008483886
	model : 0.06749744415283203
			 train-loss:  2.0910091826675137 	 ± 0.25871003550091115
	data : 0.11838254928588868
	model : 0.0675722599029541
			 train-loss:  2.091698870875619 	 ± 0.25763203976183413
	data : 0.11844110488891602
	model : 0.06691389083862305
			 train-loss:  2.090689012596199 	 ± 0.2566875163426446
	data : 0.1189490795135498
	model : 0.06662731170654297
			 train-loss:  2.0895163023046086 	 ± 0.255837533843889
	data : 0.11907472610473632
	model : 0.0676661491394043
			 train-loss:  2.088764807819265 	 ± 0.254827130308951
	data : 0.11806712150573731
	model : 0.06848273277282715
			 train-loss:  2.0915511145926358 	 ± 0.25543007031497483
	data : 0.11717758178710938
	model : 0.06834731101989747
			 train-loss:  2.0921136410340018 	 ± 0.25438799276987084
	data : 0.11725234985351562
	model : 0.06852030754089355
			 train-loss:  2.0951941989619156 	 ± 0.2554343541599816
	data : 0.11715760231018066
	model : 0.06894063949584961
			 train-loss:  2.0929901426673956 	 ± 0.25544580118791305
	data : 0.11659464836120606
	model : 0.06805086135864258
			 train-loss:  2.089200212793835 	 ± 0.25764337188965225
	data : 0.11747159957885742
	model : 0.06746892929077149
			 train-loss:  2.091241531011437 	 ± 0.2575150378143558
	data : 0.1183089256286621
	model : 0.06670303344726562
			 train-loss:  2.0937948058048885 	 ± 0.25794798763773796
	data : 0.11887612342834472
	model : 0.06651225090026855
			 train-loss:  2.0907732662090583 	 ± 0.2590035416549477
	data : 0.11899456977844239
	model : 0.06663947105407715
			 train-loss:  2.0915354285083834 	 ± 0.25807608015025674
	data : 0.11929306983947754
	model : 0.06686487197875976
			 train-loss:  2.0946452123362844 	 ± 0.2593098636132322
	data : 0.11886215209960938
	model : 0.06750497817993165
			 train-loss:  2.0966473981257407 	 ± 0.25921498867186415
	data : 0.11799249649047852
	model : 0.06850500106811523
			 train-loss:  2.099051215171814 	 ± 0.2595599830132149
	data : 0.11722145080566407
	model : 0.06847982406616211
			 train-loss:  2.096567536157275 	 ± 0.26001494944082276
	data : 0.11721811294555665
	model : 0.06854414939880371
			 train-loss:  2.095469641873217 	 ± 0.259282290390817
	data : 0.11727705001831054
	model : 0.068275785446167
			 train-loss:  2.096200570464134 	 ± 0.2583988068280477
	data : 0.1176900863647461
	model : 0.06833453178405761
			 train-loss:  2.099968139515367 	 ± 0.2609008484253509
	data : 0.11771478652954101
	model : 0.06822633743286133
			 train-loss:  2.1006278111384464 	 ± 0.26000342222888645
	data : 0.11781125068664551
	model : 0.06922831535339355
			 train-loss:  2.0970130985929765 	 ± 0.26226767725760863
	data : 0.11698646545410156
	model : 0.06825780868530273
			 train-loss:  2.0975049156131167 	 ± 0.26133298278673367
	data : 0.11782274246215821
	model : 0.06910290718078613
			 train-loss:  2.0971116051638035 	 ± 0.2603878869767806
	data : 0.11687674522399902
	model : 0.06892738342285157
			 train-loss:  2.0958773796238117 	 ± 0.25980467336704033
	data : 0.11712946891784667
	model : 0.0683164119720459
			 train-loss:  2.0974326531092324 	 ± 0.25946600737204933
	data : 0.1176833152770996
	model : 0.06798276901245118
			 train-loss:  2.0942330719793545 	 ± 0.26116973009482736
	data : 0.11796126365661622
	model : 0.06800251007080078
			 train-loss:  2.0918702861688434 	 ± 0.2616696435297731
	data : 0.11794219017028809
	model : 0.06814045906066894
			 train-loss:  2.0934304422226506 	 ± 0.2613585766113575
	data : 0.11796307563781738
	model : 0.06833553314208984
			 train-loss:  2.0928535641526147 	 ± 0.2605049014817397
	data : 0.11770110130310059
	model : 0.06823468208312988
			 train-loss:  2.091528882299151 	 ± 0.26004227364736143
	data : 0.11777210235595703
	model : 0.06836566925048829
			 train-loss:  2.0915552352337126 	 ± 0.25911868471974575
	data : 0.11769895553588867
	model : 0.06922335624694824
			 train-loss:  2.0914129757545363 	 ± 0.25821020862629646
	data : 0.11686191558837891
	model : 0.06878795623779296
			 train-loss:  2.095152524801401 	 ± 0.261136038764855
	data : 0.11720733642578125
	model : 0.0684239387512207
			 train-loss:  2.0940667705403433 	 ± 0.2605514388443527
	data : 0.11745691299438477
	model : 0.06840949058532715
			 train-loss:  2.0946870754505027 	 ± 0.25975810627299645
	data : 0.11748757362365722
	model : 0.06860413551330566
			 train-loss:  2.091643362012628 	 ± 0.2614487119508509
	data : 0.11718950271606446
	model : 0.06859369277954101
			 train-loss:  2.0898443995689857 	 ± 0.2614630392236601
	data : 0.11708779335021972
	model : 0.06880154609680175
			 train-loss:  2.0889156452707343 	 ± 0.2608214124278215
	data : 0.11703176498413086
	model : 0.06857428550720215
			 train-loss:  2.088727049379541 	 ± 0.2599548245904005
	data : 0.11723074913024903
	model : 0.06843962669372558
			 train-loss:  2.0887020643552145 	 ± 0.25908703898324403
	data : 0.1175504207611084
	model : 0.06849708557128906
			 train-loss:  2.0859347818703053 	 ± 0.2604423697374389
	data : 0.11755743026733398
	model : 0.06854076385498047
			 train-loss:  2.0849410993488213 	 ± 0.25987126544258676
	data : 0.11763787269592285
	model : 0.06822094917297364
			 train-loss:  2.083727948805865 	 ± 0.2594520872198372
	data : 0.11781105995178223
	model : 0.06871581077575684
			 train-loss:  2.0839559102987315 	 ± 0.25862371031119075
	data : 0.11750993728637696
	model : 0.07037239074707032
			 train-loss:  2.0836585952389624 	 ± 0.25781449248250826
	data : 0.11575026512145996
	model : 0.07032036781311035
			 train-loss:  2.0842762910402737 	 ± 0.25710187390112826
	data : 0.11594166755676269
	model : 0.06942586898803711
			 train-loss:  2.083019695464213 	 ± 0.25676190287364753
	data : 0.11669931411743165
	model : 0.06999106407165527
			 train-loss:  2.0812472882150095 	 ± 0.25690975349879974
	data : 0.11611399650573731
	model : 0.06998996734619141
			 train-loss:  2.0846433707003325 	 ± 0.2596339424327531
	data : 0.11597084999084473
	model : 0.06924228668212891
			 train-loss:  2.085533782094717 	 ± 0.25906472795911073
	data : 0.11665053367614746
	model : 0.06909184455871582
			 train-loss:  2.0878379263492843 	 ± 0.2598982988339019
	data : 0.1166262149810791
	model : 0.06903862953186035
			 train-loss:  2.088199112886264 	 ± 0.25913543088607865
	data : 0.11665987968444824
	model : 0.06878256797790527
			 train-loss:  2.0892731594893097 	 ± 0.25870075471876347
	data : 0.11699113845825196
	model : 0.06873016357421875
			 train-loss:  2.092810545752688 	 ± 0.2618351202423806
	data : 0.11706733703613281
	model : 0.06801929473876953
			 train-loss:  2.09563605929866 	 ± 0.26353638799650875
	data : 0.11757326126098633
	model : 0.06800761222839355
			 train-loss:  2.0964872284107896 	 ± 0.26296879351846014
	data : 0.11754980087280273
	model : 0.0689117431640625
			 train-loss:  2.096493511856673 	 ± 0.2621802921178766
	data : 0.11653752326965332
	model : 0.06848983764648438
			 train-loss:  2.097128548082851 	 ± 0.26152761625144777
	data : 0.11709008216857911
	model : 0.06851687431335449
			 train-loss:  2.097543519629529 	 ± 0.26080818570114134
	data : 0.1170931339263916
	model : 0.06911649703979492
			 train-loss:  2.0959854448542874 	 ± 0.2608276262672401
	data : 0.11671743392944336
	model : 0.06907114982604981
			 train-loss:  2.096395004562467 	 ± 0.260118672684542
	data : 0.11690678596496581
	model : 0.06892638206481934
			 train-loss:  2.095286158628242 	 ± 0.25976642004864925
	data : 0.11713919639587403
	model : 0.06957297325134278
			 train-loss:  2.0951774864527533 	 ± 0.25901848307183634
	data : 0.11664395332336426
	model : 0.0697479248046875
			 train-loss:  2.0945329707244347 	 ± 0.2584121917571023
	data : 0.11641998291015625
	model : 0.06983766555786133
			 train-loss:  2.0931707845415386 	 ± 0.25829855728395495
	data : 0.11631178855895996
	model : 0.07077531814575196
			 train-loss:  2.093820123509927 	 ± 0.2577069101113984
	data : 0.11540784835815429
	model : 0.0706902027130127
			 train-loss:  2.0968181925304865 	 ± 0.26003768805835087
	data : 0.11554598808288574
	model : 0.07054181098937988
			 train-loss:  2.0946913883927163 	 ± 0.2608454277645386
	data : 0.11565985679626464
	model : 0.07085566520690918
			 train-loss:  2.093388386944819 	 ± 0.26069605720788425
	data : 0.1154317855834961
	model : 0.07086715698242188
			 train-loss:  2.0945608894030254 	 ± 0.2604437512500543
	data : 0.1154329776763916
	model : 0.06983671188354493
			 train-loss:  2.095298944915856 	 ± 0.2599119882152829
	data : 0.11633329391479492
	model : 0.06994318962097168
			 train-loss:  2.09447598195338 	 ± 0.2594333247311935
	data : 0.11632018089294434
	model : 0.0700878620147705
			 train-loss:  2.0930374000893264 	 ± 0.2594504033142172
	data : 0.11599702835083008
	model : 0.06961469650268555
			 train-loss:  2.0912472664014152 	 ± 0.2598751814420617
	data : 0.11645493507385254
	model : 0.06946868896484375
			 train-loss:  2.092860793423008 	 ± 0.26009439374959287
	data : 0.11703243255615234
	model : 0.06877894401550293
			 train-loss:  2.0926756442234082 	 ± 0.259406497021905
	data : 0.11773686408996582
	model : 0.06848478317260742
			 train-loss:  2.090651963483841 	 ± 0.26017994666145416
	data : 0.11804370880126953
	model : 0.06837272644042969
			 train-loss:  2.0897065914691764 	 ± 0.2598088902667656
	data : 0.11814494132995605
	model : 0.06846532821655274
			 train-loss:  2.0890799331917336 	 ± 0.25926307262598663
	data : 0.11803960800170898
	model : 0.06804156303405762
			 train-loss:  2.090263865495983 	 ± 0.25909165528230726
	data : 0.1181290626525879
	model : 0.06807570457458496
			 train-loss:  2.088011145591736 	 ± 0.2602714539084592
	data : 0.11801743507385254
	model : 0.06760635375976562
			 train-loss:  2.087201351299882 	 ± 0.2598339128610837
	data : 0.11838583946228028
	model : 0.06778812408447266
			 train-loss:  2.0867108637804814 	 ± 0.25924899523115824
	data : 0.11821832656860351
	model : 0.0678070068359375
			 train-loss:  2.088244687036141 	 ± 0.25945645583564775
	data : 0.1183751106262207
	model : 0.06830344200134278
			 train-loss:  2.089313649519896 	 ± 0.2592182744671769
	data : 0.1177764892578125
	model : 0.06884570121765136
			 train-loss:  2.0881632511713066 	 ± 0.2590547295393597
	data : 0.11736807823181153
	model : 0.06972703933715821
			 train-loss:  2.0888143874667016 	 ± 0.25855714274441804
	data : 0.11655516624450683
	model : 0.06954727172851563
			 train-loss:  2.088569477953092 	 ± 0.25792630133908745
	data : 0.11658754348754882
	model : 0.06936473846435547
			 train-loss:  2.0889790471474727 	 ± 0.257341969802904
	data : 0.11641407012939453
	model : 0.06939630508422852
			 train-loss:  2.088335871100426 	 ± 0.25685810554662303
	data : 0.11640663146972656
	model : 0.06896347999572754
			 train-loss:  2.089260626195082 	 ± 0.2565519080932208
	data : 0.11679763793945312
	model : 0.06909384727478027
			 train-loss:  2.0890998067242084 	 ± 0.25592624707590717
	data : 0.11679196357727051
	model : 0.06916437149047852
			 train-loss:  2.0901045264868903 	 ± 0.2556941606260769
	data : 0.11685695648193359
	model : 0.06915702819824218
			 train-loss:  2.088916922901191 	 ± 0.2556273218512419
	data : 0.1168816089630127
	model : 0.06822400093078614
			 train-loss:  2.0897970752018256 	 ± 0.25531275382872176
	data : 0.11776485443115234
	model : 0.06873488426208496
			 train-loss:  2.0882786529735453 	 ± 0.2556185075507913
	data : 0.11726570129394531
	model : 0.06813583374023438
			 train-loss:  2.0901345878407573 	 ± 0.25638785281699217
	data : 0.11751856803894042
	model : 0.06771321296691894
			 train-loss:  2.0905588103028445 	 ± 0.25584360731418176
	data : 0.11801528930664062
	model : 0.06767535209655762
			 train-loss:  2.0888081657829467 	 ± 0.2564765765719578
	data : 0.11822710037231446
	model : 0.06791567802429199
			 train-loss:  2.0874688926197233 	 ± 0.25659670401931
	data : 0.11796894073486328
	model : 0.06822900772094727
			 train-loss:  2.086883785600346 	 ± 0.25612831801742086
	data : 0.11770091056823731
	model : 0.06883578300476074
			 train-loss:  2.0865938348590203 	 ± 0.25555823662995436
	data : 0.11726188659667969
	model : 0.06926097869873046
			 train-loss:  2.0864760293647158 	 ± 0.254963398741511
	data : 0.116851806640625
	model : 0.06923379898071289
			 train-loss:  2.0886105198726477 	 ± 0.25626744890998965
	data : 0.11696362495422363
	model : 0.07021527290344239
			 train-loss:  2.0870248711386394 	 ± 0.25672087149302797
	data : 0.11605730056762695
	model : 0.070184326171875
			 train-loss:  2.0874236308866077 	 ± 0.25619265079595854
	data : 0.11595797538757324
	model : 0.06986064910888672
			 train-loss:  2.087088500849113 	 ± 0.2556491146230935
	data : 0.11623706817626953
	model : 0.06983189582824707
			 train-loss:  2.0874169479816334 	 ± 0.2551079750006772
	data : 0.1163261890411377
	model : 0.070033597946167
			 train-loss:  2.088359754923816 	 ± 0.254905250070785
	data : 0.11602535247802734
	model : 0.06894583702087402
			 train-loss:  2.087333016503941 	 ± 0.25477873918172816
	data : 0.11705303192138672
	model : 0.06892762184143067
			 train-loss:  2.0863921119077173 	 ± 0.25458446902980847
	data : 0.11723647117614747
	model : 0.06915645599365235
			 train-loss:  2.0869438803947724 	 ± 0.25414284053770825
	data : 0.11702556610107422
	model : 0.06826534271240234
			 train-loss:  2.085653784563723 	 ± 0.25429988907524487
	data : 0.1177323341369629
	model : 0.0681920051574707
			 train-loss:  2.0854413589196548 	 ± 0.2537514492911686
	data : 0.1178441047668457
	model : 0.0688551902770996
			 train-loss:  2.0842767832014295 	 ± 0.25378616616449107
	data : 0.11715397834777833
	model : 0.06847734451293945
			 train-loss:  2.0866882568967027 	 ± 0.25579454797125595
	data : 0.11742825508117676
	model : 0.06768593788146973
			 train-loss:  2.086302161216736 	 ± 0.2552964922007493
	data : 0.11799583435058594
	model : 0.06800026893615722
			 train-loss:  2.0867361818489276 	 ± 0.25481993416365417
	data : 0.11760635375976562
	model : 0.0679253101348877
			 train-loss:  2.0863249260265233 	 ± 0.2543387695744268
	data : 0.11766414642333985
	model : 0.06802401542663575
			 train-loss:  2.0858278233072034 	 ± 0.25389672126269747
	data : 0.11761775016784667
	model : 0.06841397285461426
			 train-loss:  2.0864342549146513 	 ± 0.2535134448900561
	data : 0.11713147163391113
	model : 0.06830034255981446
			 train-loss:  2.086940700637883 	 ± 0.2530835701110027
	data : 0.11744775772094726
	model : 0.06836204528808594
			 train-loss:  2.087337990175501 	 ± 0.25261237865063474
	data : 0.11726202964782714
	model : 0.06800537109375
			 train-loss:  2.088458041859488 	 ± 0.252651164326781
	data : 0.11766376495361328
	model : 0.06748614311218262
			 train-loss:  2.0893076369102963 	 ± 0.25244778995674766
	data : 0.11810998916625977
	model : 0.06719207763671875
			 train-loss:  2.0901787685135664 	 ± 0.2522660882380843
	data : 0.11834373474121093
	model : 0.06773109436035156
			 train-loss:  2.0907340532616723 	 ± 0.25187781216758093
	data : 0.11757702827453613
	model : 0.06742019653320312
			 train-loss:  2.091357400437363 	 ± 0.25153122379563625
	data : 0.11803579330444336
	model : 0.0676384449005127
			 train-loss:  2.0906012547066024 	 ± 0.2512753769787748
	data : 0.11757550239562989
	model : 0.0679750919342041
			 train-loss:  2.090314280986786 	 ± 0.2507905843173435
	data : 0.11722612380981445
	model : 0.06802082061767578
			 train-loss:  2.089634396723197 	 ± 0.2504912703062022
	data : 0.11738982200622558
	model : 0.06807103157043456
			 train-loss:  2.0928269683822127 	 ± 0.2548391489616877
	data : 0.11745762825012207
	model : 0.06783819198608398
			 train-loss:  2.0922103708173023 	 ± 0.2544950752605187
	data : 0.11739172935485839
	model : 0.06783256530761719
			 train-loss:  2.0933646926137266 	 ± 0.25460968020007047
	data : 0.11759886741638184
	model : 0.06801319122314453
			 train-loss:  2.0937829168475406 	 ± 0.2541735064425508
	data : 0.11754145622253417
	model : 0.0673762321472168
			 train-loss:  2.093344277482692 	 ± 0.25374926974299566
	data : 0.11797895431518554
	model : 0.06725354194641113
			 train-loss:  2.0934083316972862 	 ± 0.2532370791487647
	data : 0.11823740005493164
	model : 0.0683295726776123
			 train-loss:  2.0933726891394584 	 ± 0.2527266256087249
	data : 0.11771626472473144
	model : 0.06795072555541992
			 train-loss:  2.092971475727587 	 ± 0.25229775934868687
	data : 0.11800346374511719
	model : 0.06784586906433106
			 train-loss:  2.0936353154182434 	 ± 0.25201046175797204
	data : 0.11817970275878906
	model : 0.06859369277954101
			 train-loss:  2.0927184568458346 	 ± 0.2519253962079645
	data : 0.11754164695739747
	model : 0.0682596206665039
			 train-loss:  2.0908178710748277 	 ± 0.2532216879229519
	data : 0.11760363578796387
	model : 0.06764063835144044
			 train-loss:  2.092757701402596 	 ± 0.2545899467241105
	data : 0.11810660362243652
	model : 0.06723270416259766
			 train-loss:  2.093652531387299 	 ± 0.25448662449487913
	data : 0.11834254264831542
	model : 0.06688647270202637
			 train-loss:  2.0932106812795004 	 ± 0.254084742890581
	data : 0.11841259002685547
	model : 0.06666064262390137
			 train-loss:  2.0921559799462557 	 ± 0.25414667704919086
	data : 0.11763544082641601
	model : 0.058348560333251955
#epoch  80    val-loss:  2.451671995614704  train-loss:  2.0921559799462557  lr:  4.8828125e-06
			 train-loss:  2.035085439682007 	 ± 0.0
	data : 5.903631210327148
	model : 0.07259368896484375
			 train-loss:  2.246483325958252 	 ± 0.21139788627624512
	data : 3.0200058221817017
	model : 0.07026159763336182
			 train-loss:  2.3516398270924888 	 ± 0.22783434847047448
	data : 2.0530134042104087
	model : 0.0689542293548584
			 train-loss:  2.290334403514862 	 ± 0.22406791969695444
	data : 1.5693994760513306
	model : 0.06915295124053955
			 train-loss:  2.2308778524398805 	 ± 0.23303534490026248
	data : 1.2786618232727052
	model : 0.0693821907043457
			 train-loss:  2.255663534005483 	 ± 0.21983223155551485
	data : 0.1210301399230957
	model : 0.06789617538452149
			 train-loss:  2.196933014052255 	 ± 0.2492350488684124
	data : 0.11770691871643066
	model : 0.06828699111938477
			 train-loss:  2.125587373971939 	 ± 0.29997457958246165
	data : 0.11698775291442871
	model : 0.0689589500427246
			 train-loss:  2.148161437776354 	 ± 0.2899364583344835
	data : 0.11641168594360352
	model : 0.0688539981842041
			 train-loss:  2.192263460159302 	 ± 0.30522406571039545
	data : 0.11662859916687011
	model : 0.06864714622497559
			 train-loss:  2.179676792838357 	 ± 0.2937289975865407
	data : 0.11689791679382325
	model : 0.06864819526672364
			 train-loss:  2.1854863365491233 	 ± 0.28188340816845103
	data : 0.11685190200805665
	model : 0.06823039054870605
			 train-loss:  2.2149198972261868 	 ± 0.2893822665732966
	data : 0.11721563339233398
	model : 0.06802382469177246
			 train-loss:  2.2133490528379167 	 ± 0.27891324114102134
	data : 0.11744365692138672
	model : 0.06749820709228516
			 train-loss:  2.1952830870946247 	 ± 0.2778052027354168
	data : 0.11770420074462891
	model : 0.06765456199645996
			 train-loss:  2.178328536450863 	 ± 0.2768828259628344
	data : 0.11754722595214843
	model : 0.06862392425537109
			 train-loss:  2.155477390569799 	 ± 0.2837415096555902
	data : 0.11689648628234864
	model : 0.06920351982116699
			 train-loss:  2.1443682644102307 	 ± 0.2795255339599491
	data : 0.1164085865020752
	model : 0.06936736106872558
			 train-loss:  2.1433097061358 	 ± 0.27210724095694994
	data : 0.11605868339538575
	model : 0.07013473510742188
			 train-loss:  2.16437406539917 	 ± 0.2806611309827093
	data : 0.11566200256347656
	model : 0.06980843544006347
			 train-loss:  2.18987614767892 	 ± 0.2966931245406201
	data : 0.1158447265625
	model : 0.06896767616271973
			 train-loss:  2.177538042718714 	 ± 0.2953343817111364
	data : 0.11650524139404297
	model : 0.0686375617980957
			 train-loss:  2.1738936952922656 	 ± 0.2893480720326075
	data : 0.11707663536071777
	model : 0.06876211166381836
			 train-loss:  2.1676240315039954 	 ± 0.28484728728190245
	data : 0.11720280647277832
	model : 0.06865067481994629
			 train-loss:  2.174555125236511 	 ± 0.281150172393471
	data : 0.11700410842895508
	model : 0.06899123191833496
			 train-loss:  2.173422607091757 	 ± 0.27574857376005313
	data : 0.11685662269592285
	model : 0.07065377235412598
			 train-loss:  2.1647368448751942 	 ± 0.27419442850505166
	data : 0.11526689529418946
	model : 0.0708117961883545
			 train-loss:  2.1658961262021745 	 ± 0.2693209581735282
	data : 0.11503262519836426
	model : 0.07053899765014648
			 train-loss:  2.17681831326978 	 ± 0.27087423374097747
	data : 0.11513175964355468
	model : 0.0706700325012207
			 train-loss:  2.1632512331008913 	 ± 0.27616117230022036
	data : 0.11516222953796387
	model : 0.06995677947998047
			 train-loss:  2.1590275379919235 	 ± 0.2726536631518815
	data : 0.11589617729187011
	model : 0.06905612945556641
			 train-loss:  2.152203556150198 	 ± 0.27103590717404963
	data : 0.11688733100891113
	model : 0.06831021308898926
			 train-loss:  2.1562282352736504 	 ± 0.2678669925307511
	data : 0.11757979393005372
	model : 0.06854157447814942
			 train-loss:  2.147969589513891 	 ± 0.2681289316456827
	data : 0.11755361557006835
	model : 0.06843109130859375
			 train-loss:  2.13736526284899 	 ± 0.27140816867822753
	data : 0.11766304969787597
	model : 0.06928062438964844
			 train-loss:  2.1314327948623233 	 ± 0.2699037098446503
	data : 0.11675748825073243
	model : 0.06974186897277831
			 train-loss:  2.1235119974291004 	 ± 0.27043992407564416
	data : 0.11628541946411133
	model : 0.07073688507080078
			 train-loss:  2.126141303463986 	 ± 0.2673366138300255
	data : 0.11542034149169922
	model : 0.07092666625976562
			 train-loss:  2.117621220075167 	 ± 0.26906285378677514
	data : 0.1151585578918457
	model : 0.070582914352417
			 train-loss:  2.1158774316310884 	 ± 0.2659013725670796
	data : 0.1155360221862793
	model : 0.07050585746765137
			 train-loss:  2.1139037899854705 	 ± 0.2629351132943951
	data : 0.11572871208190919
	model : 0.07012948989868165
			 train-loss:  2.12171086810884 	 ± 0.2645520262652554
	data : 0.11607928276062011
	model : 0.06941275596618653
			 train-loss:  2.1149397478547205 	 ± 0.26511463719442774
	data : 0.11658544540405273
	model : 0.06923322677612305
			 train-loss:  2.116628286513415 	 ± 0.2623184457337167
	data : 0.11678352355957031
	model : 0.06911077499389648
			 train-loss:  2.1073199934429594 	 ± 0.26663492728398075
	data : 0.11689939498901367
	model : 0.06895675659179687
			 train-loss:  2.1032320675642593 	 ± 0.265142716114187
	data : 0.11685647964477539
	model : 0.06878042221069336
			 train-loss:  2.1017093100446336 	 ± 0.26251012465860707
	data : 0.11696000099182129
	model : 0.06931490898132324
			 train-loss:  2.1010595560073853 	 ± 0.2597994426614645
	data : 0.11659321784973145
	model : 0.06914896965026855
			 train-loss:  2.0998801357892094 	 ± 0.2572645635365137
	data : 0.11678581237792969
	model : 0.06903691291809082
			 train-loss:  2.097670533657074 	 ± 0.25514817121848005
	data : 0.1167722225189209
	model : 0.06913576126098633
			 train-loss:  2.09259990383597 	 ± 0.2551659684844338
	data : 0.11662721633911133
	model : 0.06937704086303711
			 train-loss:  2.0865777616317454 	 ± 0.25633403683268663
	data : 0.1163515567779541
	model : 0.0693234920501709
			 train-loss:  2.0930050341588147 	 ± 0.25809977214449414
	data : 0.11623849868774414
	model : 0.06942129135131836
			 train-loss:  2.0888650152418347 	 ± 0.2574689869737861
	data : 0.11620993614196777
	model : 0.0690847396850586
			 train-loss:  2.0861051992936566 	 ± 0.25592244330727276
	data : 0.11665816307067871
	model : 0.06880254745483398
			 train-loss:  2.0843919047287534 	 ± 0.2539452028707497
	data : 0.11681160926818848
	model : 0.06852927207946777
			 train-loss:  2.0837697397198593 	 ± 0.25175081276406214
	data : 0.11705083847045898
	model : 0.06849980354309082
			 train-loss:  2.09073164956323 	 ± 0.25504592960922995
	data : 0.11706480979919434
	model : 0.06908469200134278
			 train-loss:  2.09230885667316 	 ± 0.25316040425643027
	data : 0.11665019989013672
	model : 0.07032170295715331
			 train-loss:  2.0932488600413004 	 ± 0.2511456812665099
	data : 0.11557636260986329
	model : 0.07052178382873535
			 train-loss:  2.0848887064417854 	 ± 0.25735905035472223
	data : 0.11549229621887207
	model : 0.07003269195556641
			 train-loss:  2.08505658372756 	 ± 0.2552785043341233
	data : 0.11609654426574707
	model : 0.06972026824951172
			 train-loss:  2.077705470342485 	 ± 0.25977514496937143
	data : 0.11645636558532715
	model : 0.06924242973327636
			 train-loss:  2.0845846235752106 	 ± 0.26345784349785917
	data : 0.11685614585876465
	model : 0.0693540096282959
			 train-loss:  2.085304120870737 	 ± 0.26148674897696117
	data : 0.11660251617431641
	model : 0.06949505805969239
			 train-loss:  2.093076471126441 	 ± 0.26695681869462745
	data : 0.11650495529174805
	model : 0.07029609680175782
			 train-loss:  2.0971347502808073 	 ± 0.2670005015269749
	data : 0.11569671630859375
	model : 0.06984891891479492
			 train-loss:  2.0928242048796486 	 ± 0.26736830391998034
	data : 0.1160496711730957
	model : 0.0697096824645996
			 train-loss:  2.0859038069628286 	 ± 0.2714892878333541
	data : 0.11597666740417481
	model : 0.06921439170837403
			 train-loss:  2.0887814794267925 	 ± 0.27060094840456933
	data : 0.11654105186462402
	model : 0.0689551830291748
			 train-loss:  2.087887854643271 	 ± 0.2687925530997254
	data : 0.11668930053710938
	model : 0.06862006187438965
			 train-loss:  2.089669234222836 	 ± 0.2673411256097374
	data : 0.11705737113952637
	model : 0.0692408561706543
			 train-loss:  2.092330703996632 	 ± 0.26646242734178016
	data : 0.1165738582611084
	model : 0.06933135986328125
			 train-loss:  2.097955291335647 	 ± 0.2689835727232154
	data : 0.11651010513305664
	model : 0.06982913017272949
			 train-loss:  2.09511648495992 	 ± 0.2682980053925527
	data : 0.11611723899841309
	model : 0.06998248100280761
			 train-loss:  2.0972245429691516 	 ± 0.26715156244734267
	data : 0.11601190567016602
	model : 0.06998295783996582
			 train-loss:  2.097012832567289 	 ± 0.26541756003295597
	data : 0.11590819358825684
	model : 0.07102584838867188
			 train-loss:  2.0909257164368262 	 ± 0.26906579275950454
	data : 0.11490964889526367
	model : 0.07086095809936524
			 train-loss:  2.0862143326409255 	 ± 0.2705759919198446
	data : 0.11522397994995118
	model : 0.07056126594543458
			 train-loss:  2.08724415153265 	 ± 0.2690353265649247
	data : 0.11555089950561523
	model : 0.07044577598571777
			 train-loss:  2.0854081165643388 	 ± 0.2678733069189488
	data : 0.11564793586730956
	model : 0.07072691917419434
			 train-loss:  2.081498989244787 	 ± 0.26854946928545725
	data : 0.11552867889404297
	model : 0.06986083984375
			 train-loss:  2.077282262135701 	 ± 0.2696440999065679
	data : 0.11643095016479492
	model : 0.07002439498901367
			 train-loss:  2.0837502564702715 	 ± 0.27443518506547576
	data : 0.1163144588470459
	model : 0.06990880966186523
			 train-loss:  2.0799168208066154 	 ± 0.2750691096547408
	data : 0.1162043571472168
	model : 0.07002882957458496
			 train-loss:  2.0774226632229116 	 ± 0.27443028706704076
	data : 0.11635160446166992
	model : 0.06998367309570312
			 train-loss:  2.076457806017207 	 ± 0.27299521816261485
	data : 0.11642990112304688
	model : 0.07004613876342773
			 train-loss:  2.0738249786875467 	 ± 0.27254827680988014
	data : 0.11615681648254395
	model : 0.0698974609375
			 train-loss:  2.074906221936258 	 ± 0.2712025208276747
	data : 0.11626782417297363
	model : 0.06977782249450684
			 train-loss:  2.0715966992908053 	 ± 0.2714928871693595
	data : 0.11654715538024903
	model : 0.06974568367004394
			 train-loss:  2.0694171373660746 	 ± 0.2707876467211069
	data : 0.1163114070892334
	model : 0.0699202537536621
			 train-loss:  2.0649737249249998 	 ± 0.27262726314571895
	data : 0.11596412658691406
	model : 0.06963629722595215
			 train-loss:  2.064723294268372 	 ± 0.27116820273211534
	data : 0.11642584800720215
	model : 0.06973519325256347
			 train-loss:  2.066525266525593 	 ± 0.27028118221884345
	data : 0.11605901718139648
	model : 0.0698235034942627
			 train-loss:  2.0675360805109926 	 ± 0.2690334438898212
	data : 0.11587343215942383
	model : 0.06987199783325196
			 train-loss:  2.0683086117108664 	 ± 0.26773446266958484
	data : 0.11587281227111816
	model : 0.06976785659790039
			 train-loss:  2.0644150284147753 	 ± 0.2690689773825742
	data : 0.11602287292480469
	model : 0.07060432434082031
			 train-loss:  2.064206679256595 	 ± 0.2677005211564539
	data : 0.11504631042480469
	model : 0.07066874504089356
			 train-loss:  2.065552435740076 	 ± 0.26667804282144025
	data : 0.11521639823913574
	model : 0.07066073417663574
			 train-loss:  2.0669144809246065 	 ± 0.2656871624283505
	data : 0.11525483131408691
	model : 0.07035179138183593
			 train-loss:  2.0684305358641217 	 ± 0.26480295113624885
	data : 0.11571903228759765
	model : 0.06952457427978516
			 train-loss:  2.0734872595936644 	 ± 0.26835752641124166
	data : 0.11650447845458985
	model : 0.06798000335693359
			 train-loss:  2.072436502836283 	 ± 0.2672624122136223
	data : 0.11798162460327148
	model : 0.06799135208129883
			 train-loss:  2.069411584964165 	 ± 0.26774024908144956
	data : 0.11782379150390625
	model : 0.06712241172790527
			 train-loss:  2.0722976230439687 	 ± 0.26808276082205557
	data : 0.11865568161010742
	model : 0.06647777557373047
			 train-loss:  2.0720648225748315 	 ± 0.2668258866323667
	data : 0.11907567977905273
	model : 0.06734347343444824
			 train-loss:  2.0733472387367318 	 ± 0.265904111975525
	data : 0.11816196441650391
	model : 0.06749253273010254
			 train-loss:  2.0712165291662568 	 ± 0.2655863186608135
	data : 0.11806793212890625
	model : 0.06728873252868653
			 train-loss:  2.0755175658322256 	 ± 0.2681172421070209
	data : 0.11838445663452149
	model : 0.06845221519470215
			 train-loss:  2.0755554361776873 	 ± 0.26689603775727005
	data : 0.11722626686096191
	model : 0.06848125457763672
			 train-loss:  2.0800270851667935 	 ± 0.2697985779940156
	data : 0.11748414039611817
	model : 0.06843037605285644
			 train-loss:  2.079578681715897 	 ± 0.268632963162947
	data : 0.11743760108947754
	model : 0.06915020942687988
			 train-loss:  2.0794197489730024 	 ± 0.26744696938994295
	data : 0.1168830394744873
	model : 0.06929235458374024
			 train-loss:  2.0771897111022684 	 ± 0.26732452312809774
	data : 0.11671137809753418
	model : 0.06815214157104492
			 train-loss:  2.077891194302103 	 ± 0.26626506615672196
	data : 0.11782503128051758
	model : 0.0681182861328125
			 train-loss:  2.0775291077021896 	 ± 0.26514332153657416
	data : 0.11778616905212402
	model : 0.06805658340454102
			 train-loss:  2.079425216740013 	 ± 0.26479645860627105
	data : 0.1180044174194336
	model : 0.0680917739868164
			 train-loss:  2.0783469808303705 	 ± 0.2639298667737698
	data : 0.11790075302124023
	model : 0.06813969612121581
			 train-loss:  2.0821126198568263 	 ± 0.26598279914259004
	data : 0.11786293983459473
	model : 0.0691009521484375
			 train-loss:  2.0830399960279466 	 ± 0.26506534198576703
	data : 0.11700215339660644
	model : 0.07020978927612305
			 train-loss:  2.0813344263833415 	 ± 0.2646281419553354
	data : 0.11588835716247559
	model : 0.07017545700073242
			 train-loss:  2.084202540702507 	 ± 0.26542307496819456
	data : 0.11599149703979492
	model : 0.0704078197479248
			 train-loss:  2.083554117660212 	 ± 0.26443892395586505
	data : 0.11594147682189941
	model : 0.07043018341064453
			 train-loss:  2.085669074327715 	 ± 0.2644129225215509
	data : 0.11594834327697753
	model : 0.07037143707275391
			 train-loss:  2.085211922645569 	 ± 0.26340234341690516
	data : 0.11598091125488282
	model : 0.07026338577270508
			 train-loss:  2.0836283753788662 	 ± 0.2629517180670683
	data : 0.11594324111938477
	model : 0.07040457725524903
			 train-loss:  2.085800432783412 	 ± 0.2630467927880539
	data : 0.11569685935974121
	model : 0.07011384963989258
			 train-loss:  2.0870218677446246 	 ± 0.26237856582550695
	data : 0.115826416015625
	model : 0.06986613273620605
			 train-loss:  2.088603052057961 	 ± 0.2619711192727821
	data : 0.11584582328796386
	model : 0.06960396766662598
			 train-loss:  2.086773328597729 	 ± 0.26178775852455966
	data : 0.11605510711669922
	model : 0.06902580261230469
			 train-loss:  2.0867563176701087 	 ± 0.2607867265672556
	data : 0.11693263053894043
	model : 0.068878173828125
			 train-loss:  2.0857516445896844 	 ± 0.26005137729792976
	data : 0.11709437370300294
	model : 0.06909050941467285
			 train-loss:  2.086554786316434 	 ± 0.2592361705085813
	data : 0.1169363021850586
	model : 0.06926264762878417
			 train-loss:  2.0862510284381126 	 ± 0.2582908167266724
	data : 0.11689281463623047
	model : 0.06929011344909668
			 train-loss:  2.0849567740051835 	 ± 0.2577681706766988
	data : 0.11681723594665527
	model : 0.06899828910827636
			 train-loss:  2.086377262192614 	 ± 0.25734853610098535
	data : 0.11700353622436524
	model : 0.06906180381774903
			 train-loss:  2.0841499245079764 	 ± 0.25771990905075687
	data : 0.11689386367797852
	model : 0.06889653205871582
			 train-loss:  2.0832818945248923 	 ± 0.25698536270031264
	data : 0.11697902679443359
	model : 0.06900177001953126
			 train-loss:  2.08255808473491 	 ± 0.2562004223058252
	data : 0.11699213981628417
	model : 0.06934528350830078
			 train-loss:  2.083493033477238 	 ± 0.2555216483901143
	data : 0.11667990684509277
	model : 0.0703284740447998
			 train-loss:  2.0816459495125086 	 ± 0.2555501800511745
	data : 0.115608549118042
	model : 0.07026948928833007
			 train-loss:  2.0795596183185845 	 ± 0.255851002607922
	data : 0.11576714515686035
	model : 0.07023530006408692
			 train-loss:  2.0815657952448703 	 ± 0.2560732103504322
	data : 0.11583027839660645
	model : 0.06994686126708985
			 train-loss:  2.0847460660669537 	 ± 0.2580008471403409
	data : 0.1159862995147705
	model : 0.0695641040802002
			 train-loss:  2.0833165900460604 	 ± 0.25768124084902344
	data : 0.11624822616577149
	model : 0.06941108703613282
			 train-loss:  2.0835997380622446 	 ± 0.25681988826925944
	data : 0.11649842262268066
	model : 0.06941075325012207
			 train-loss:  2.082015812802477 	 ± 0.25665942268029235
	data : 0.11634354591369629
	model : 0.06950855255126953
			 train-loss:  2.0793189470832414 	 ± 0.2578722701409753
	data : 0.11611366271972656
	model : 0.06964106559753418
			 train-loss:  2.079358398514306 	 ± 0.25700591828255176
	data : 0.11623549461364746
	model : 0.06999640464782715
			 train-loss:  2.0793325805664065 	 ± 0.25614799316757214
	data : 0.11600151062011718
	model : 0.06949329376220703
			 train-loss:  2.0801916880323397 	 ± 0.25551514447239915
	data : 0.11647863388061523
	model : 0.06942791938781738
			 train-loss:  2.0817284333078483 	 ± 0.2553723992589671
	data : 0.11663255691528321
	model : 0.06921453475952148
			 train-loss:  2.0806793917238324 	 ± 0.25486485493147976
	data : 0.11689844131469726
	model : 0.06916546821594238
			 train-loss:  2.0810965764058103 	 ± 0.2540884295816268
	data : 0.11675443649291992
	model : 0.06839847564697266
			 train-loss:  2.0823110257425617 	 ± 0.253715471631938
	data : 0.1175194263458252
	model : 0.06893200874328613
			 train-loss:  2.082375182555272 	 ± 0.2529022349841398
	data : 0.11702404022216797
	model : 0.06905527114868164
			 train-loss:  2.081190375006123 	 ± 0.25252948898058813
	data : 0.11696105003356934
	model : 0.06983470916748047
			 train-loss:  2.0819317253330087 	 ± 0.2519004073540667
	data : 0.11640911102294922
	model : 0.07024407386779785
			 train-loss:  2.081537922973153 	 ± 0.2511558028663237
	data : 0.11599302291870117
	model : 0.07086987495422363
			 train-loss:  2.0826755583286287 	 ± 0.25078032645792053
	data : 0.11540546417236328
	model : 0.0707900047302246
			 train-loss:  2.0838711513495594 	 ± 0.2504572958648045
	data : 0.1155123233795166
	model : 0.07067399024963379
			 train-loss:  2.0824096622290433 	 ± 0.25037078485173603
	data : 0.1156959056854248
	model : 0.06913738250732422
			 train-loss:  2.081286957658873 	 ± 0.2500103028348372
	data : 0.11682391166687012
	model : 0.06801395416259766
			 train-loss:  2.079856960511789 	 ± 0.24991466527775283
	data : 0.11788506507873535
	model : 0.06803522109985352
			 train-loss:  2.0786996949802745 	 ± 0.24959657410258684
	data : 0.11772770881652832
	model : 0.0672497272491455
			 train-loss:  2.0784489627344063 	 ± 0.24886448307505632
	data : 0.11852402687072754
	model : 0.06740398406982422
			 train-loss:  2.0777282507833608 	 ± 0.24829195745825142
	data : 0.1182666301727295
	model : 0.06830177307128907
			 train-loss:  2.0782007582130886 	 ± 0.247627186289434
	data : 0.11753592491149903
	model : 0.06819453239440917
			 train-loss:  2.076580366439368 	 ± 0.24778518967912663
	data : 0.11767539978027344
	model : 0.06835999488830566
			 train-loss:  2.075383276799146 	 ± 0.2475449846953725
	data : 0.11758284568786621
	model : 0.06917433738708496
			 train-loss:  2.073955976475052 	 ± 0.2475206803269366
	data : 0.11679272651672364
	model : 0.06907100677490234
			 train-loss:  2.0742632649665653 	 ± 0.2468328048815709
	data : 0.11693086624145507
	model : 0.06833300590515137
			 train-loss:  2.0757811469149727 	 ± 0.24692213293655668
	data : 0.11757211685180664
	model : 0.06936187744140625
			 train-loss:  2.0739625699218665 	 ± 0.2473707415754417
	data : 0.11668128967285156
	model : 0.06916961669921876
			 train-loss:  2.0733539084025794 	 ± 0.24679358819376293
	data : 0.11693744659423828
	model : 0.06927862167358398
			 train-loss:  2.071673213758252 	 ± 0.247093789443882
	data : 0.11685795783996582
	model : 0.06915812492370606
			 train-loss:  2.075145528141388 	 ± 0.25066396146325265
	data : 0.1169497013092041
	model : 0.06994585990905762
			 train-loss:  2.0753795116135243 	 ± 0.24997824082696726
	data : 0.11634879112243653
	model : 0.06939420700073243
			 train-loss:  2.0756998082113 	 ± 0.24931562457458875
	data : 0.11687922477722168
	model : 0.06868720054626465
			 train-loss:  2.0752065506246353 	 ± 0.2487096864236835
	data : 0.11747684478759765
	model : 0.06856698989868164
			 train-loss:  2.0744160137123826 	 ± 0.2482483639564407
	data : 0.11753191947937011
	model : 0.06776990890502929
			 train-loss:  2.074133128255278 	 ± 0.24759467546594935
	data : 0.11834092140197754
	model : 0.06775541305541992
			 train-loss:  2.0749357026782844 	 ± 0.24715453556912143
	data : 0.11837096214294433
	model : 0.06822047233581544
			 train-loss:  2.0737107273029243 	 ± 0.24703842233683007
	data : 0.11776947975158691
	model : 0.0680269718170166
			 train-loss:  2.0743728128639427 	 ± 0.24653348427276173
	data : 0.1179051399230957
	model : 0.06814985275268555
			 train-loss:  2.0738826323581 	 ± 0.2459602459313521
	data : 0.1177342414855957
	model : 0.06816725730895996
			 train-loss:  2.073047103091357 	 ± 0.24556624461319762
	data : 0.11763858795166016
	model : 0.06726422309875488
			 train-loss:  2.072549333597751 	 ± 0.24500684653367966
	data : 0.11840133666992188
	model : 0.06712503433227539
			 train-loss:  2.076081462007351 	 ± 0.24911085795770188
	data : 0.11854701042175293
	model : 0.06777787208557129
			 train-loss:  2.077079885256918 	 ± 0.24883330229047831
	data : 0.11801300048828126
	model : 0.06763038635253907
			 train-loss:  2.0758911102854145 	 ± 0.24872140957507177
	data : 0.11816244125366211
	model : 0.06830868721008301
			 train-loss:  2.0762889832258224 	 ± 0.24813378598075722
	data : 0.11773133277893066
	model : 0.06956028938293457
			 train-loss:  2.075070643054389 	 ± 0.24806522123569807
	data : 0.11658649444580078
	model : 0.06967535018920898
			 train-loss:  2.0753580528436246 	 ± 0.24745726688144454
	data : 0.11655430793762207
	model : 0.06905622482299804
			 train-loss:  2.072579550743103 	 ± 0.24983748142552217
	data : 0.11703200340270996
	model : 0.06898841857910157
			 train-loss:  2.074863377274299 	 ± 0.2512317604011632
	data : 0.11720104217529297
	model : 0.06890749931335449
			 train-loss:  2.0753584821817235 	 ± 0.250689149769758
	data : 0.11716132164001465
	model : 0.06839866638183593
			 train-loss:  2.077365159386336 	 ± 0.2516364858109809
	data : 0.11762619018554688
	model : 0.06750669479370117
			 train-loss:  2.0791760503347194 	 ± 0.2522935432423165
	data : 0.11841764450073242
	model : 0.06879138946533203
			 train-loss:  2.0796876341104507 	 ± 0.25176547355805656
	data : 0.11726574897766114
	model : 0.06885185241699218
			 train-loss:  2.0793720910798257 	 ± 0.25117805368084484
	data : 0.1169137954711914
	model : 0.0692704200744629
			 train-loss:  2.079387959277276 	 ± 0.2505556554485144
	data : 0.11661109924316407
	model : 0.06938114166259765
			 train-loss:  2.0800203308096075 	 ± 0.25009930650023077
	data : 0.11632375717163086
	model : 0.07027020454406738
			 train-loss:  2.0829909882124733 	 ± 0.25305035588496194
	data : 0.11559839248657226
	model : 0.06977448463439942
			 train-loss:  2.0837756453490837 	 ± 0.25268106259745476
	data : 0.11614103317260742
	model : 0.06979880332946778
			 train-loss:  2.082728597145636 	 ± 0.25251242058111667
	data : 0.11618447303771973
	model : 0.06969952583312988
			 train-loss:  2.081258469733639 	 ± 0.25278392671803396
	data : 0.11637511253356933
	model : 0.06977663040161133
			 train-loss:  2.082017877927193 	 ± 0.2524121243362427
	data : 0.11650204658508301
	model : 0.06952881813049316
			 train-loss:  2.08232344052438 	 ± 0.25184610306008537
	data : 0.1166560173034668
	model : 0.0698239803314209
			 train-loss:  2.080857785542806 	 ± 0.2521376431081883
	data : 0.11653404235839844
	model : 0.0702282428741455
			 train-loss:  2.081103871783939 	 ± 0.2515647285528375
	data : 0.11633486747741699
	model : 0.07029247283935547
			 train-loss:  2.080093555292993 	 ± 0.2513994339702122
	data : 0.11622967720031738
	model : 0.0703007698059082
			 train-loss:  2.081024040638561 	 ± 0.25117425057356585
	data : 0.11614322662353516
	model : 0.07061371803283692
			 train-loss:  2.081640960457169 	 ± 0.25074840725388453
	data : 0.11589999198913574
	model : 0.07078232765197753
			 train-loss:  2.081670567601226 	 ± 0.25016496670703997
	data : 0.11546335220336915
	model : 0.07052931785583497
			 train-loss:  2.080349960812816 	 ± 0.2503352496445063
	data : 0.11553721427917481
	model : 0.06960568428039551
			 train-loss:  2.079599104169327 	 ± 0.250001447015495
	data : 0.11641082763671876
	model : 0.06906671524047851
			 train-loss:  2.0816414689798965 	 ± 0.25123531810934524
	data : 0.11690688133239746
	model : 0.0691138744354248
			 train-loss:  2.0815684387128646 	 ± 0.25066338443044006
	data : 0.11686058044433593
	model : 0.06835875511169434
			 train-loss:  2.0808780816468326 	 ± 0.2503016291061717
	data : 0.11760983467102051
	model : 0.06759190559387207
			 train-loss:  2.0815194834411415 	 ± 0.24991583473332715
	data : 0.11821084022521973
	model : 0.0684391975402832
			 train-loss:  2.0795855028135284 	 ± 0.25100435168429025
	data : 0.11728987693786622
	model : 0.06891841888427734
			 train-loss:  2.079361097695047 	 ± 0.25046324774910383
	data : 0.11710338592529297
	model : 0.06844696998596192
			 train-loss:  2.0801484510302544 	 ± 0.2501799931227675
	data : 0.11729974746704101
	model : 0.06789097785949708
			 train-loss:  2.080521504084269 	 ± 0.24968585225156942
	data : 0.11763963699340821
	model : 0.06781396865844727
			 train-loss:  2.079397908881702 	 ± 0.24970227394403052
	data : 0.11766119003295898
	model : 0.06762881278991699
			 train-loss:  2.0788527044430705 	 ± 0.2492864389231358
	data : 0.11766171455383301
	model : 0.0671191692352295
			 train-loss:  2.08163671796782 	 ± 0.252251034535329
	data : 0.11784849166870118
	model : 0.06653590202331543
			 train-loss:  2.08144419734655 	 ± 0.2517164520751406
	data : 0.11857872009277344
	model : 0.06659965515136719
			 train-loss:  2.0815921602041825 	 ± 0.25117862645210476
	data : 0.11867203712463378
	model : 0.06697158813476563
			 train-loss:  2.0815454711129653 	 ± 0.25063536030057765
	data : 0.11834602355957032
	model : 0.0668680191040039
			 train-loss:  2.0810941976719888 	 ± 0.2501886465314775
	data : 0.11864142417907715
	model : 0.06741137504577636
			 train-loss:  2.0823678852662506 	 ± 0.25040383886164763
	data : 0.11818432807922363
	model : 0.06761002540588379
			 train-loss:  2.0829349934545336 	 ± 0.2500181201923356
	data : 0.1180809497833252
	model : 0.06835751533508301
			 train-loss:  2.081074290072664 	 ± 0.2511040087659356
	data : 0.1175887107849121
	model : 0.06787786483764649
			 train-loss:  2.080561745469853 	 ± 0.2506946020691088
	data : 0.11813473701477051
	model : 0.06751360893249511
			 train-loss:  2.082662174470314 	 ± 0.25223756313078083
	data : 0.1182326316833496
	model : 0.0669590950012207
			 train-loss:  2.083819516065742 	 ± 0.2523368950242649
	data : 0.11872973442077636
	model : 0.06727838516235352
			 train-loss:  2.0843779876142365 	 ± 0.2519557907548176
	data : 0.1182631492614746
	model : 0.06688709259033203
			 train-loss:  2.0845936591426533 	 ± 0.25145244127251826
	data : 0.11828370094299316
	model : 0.0667811393737793
			 train-loss:  2.0859122627503646 	 ± 0.25176032991526975
	data : 0.11845545768737793
	model : 0.06682262420654297
			 train-loss:  2.0847168175642157 	 ± 0.25192411536351716
	data : 0.11839461326599121
	model : 0.06681246757507324
			 train-loss:  2.0841073793638882 	 ± 0.2515839151171265
	data : 0.11834359169006348
	model : 0.06630792617797851
			 train-loss:  2.084555250699403 	 ± 0.25116489774095874
	data : 0.11869125366210938
	model : 0.06650505065917969
			 train-loss:  2.085041868443392 	 ± 0.2507670220774228
	data : 0.11854681968688965
	model : 0.06684036254882812
			 train-loss:  2.0844962950644454 	 ± 0.25040247057048837
	data : 0.11805849075317383
	model : 0.06721811294555664
			 train-loss:  2.085495499464182 	 ± 0.25038601034679786
	data : 0.11774401664733887
	model : 0.0676567554473877
			 train-loss:  2.0856145730903073 	 ± 0.24988769737101782
	data : 0.1175422191619873
	model : 0.06863245964050294
			 train-loss:  2.085446900153256 	 ± 0.2493993886580104
	data : 0.11688590049743652
	model : 0.06849045753479004
			 train-loss:  2.085993940830231 	 ± 0.24904973191071977
	data : 0.11706147193908692
	model : 0.06897835731506348
			 train-loss:  2.0857194681091613 	 ± 0.24859100580283383
	data : 0.11680822372436524
	model : 0.06924386024475097
			 train-loss:  2.084598712977909 	 ± 0.24873186194483168
	data : 0.11674957275390625
	model : 0.06870956420898437
			 train-loss:  2.084558949168963 	 ± 0.2482406128560299
	data : 0.1172548770904541
	model : 0.06830220222473145
			 train-loss:  2.084895217981864 	 ± 0.24780919769355433
	data : 0.1177001953125
	model : 0.06820721626281738
			 train-loss:  2.0842006818920957 	 ± 0.2475703978823062
	data : 0.11778807640075684
	model : 0.06773185729980469
			 train-loss:  2.0935708028264344 	 ± 0.28886064279065404
	data : 0.1171496868133545
	model : 0.05827665328979492
#epoch  81    val-loss:  2.5122575257953845  train-loss:  2.0935708028264344  lr:  4.8828125e-06
			 train-loss:  2.421900749206543 	 ± 0.0
	data : 5.847551107406616
	model : 0.0740499496459961
			 train-loss:  2.3702685832977295 	 ± 0.05163216590881348
	data : 2.99117112159729
	model : 0.07143259048461914
			 train-loss:  2.318397601445516 	 ± 0.08460763098512741
	data : 2.0342896779378257
	model : 0.07077566782633464
			 train-loss:  2.195685714483261 	 ± 0.22481872702512629
	data : 1.554796576499939
	model : 0.07054418325424194
			 train-loss:  2.1356793642044067 	 ± 0.23417475592410225
	data : 1.2668883323669433
	model : 0.06956634521484376
			 train-loss:  2.074246366818746 	 ± 0.25410282583833477
	data : 0.12129068374633789
	model : 0.06793980598449707
			 train-loss:  2.08059447152274 	 ± 0.23576683723971964
	data : 0.11812119483947754
	model : 0.06809053421020508
			 train-loss:  2.0316234827041626 	 ± 0.25578282892006743
	data : 0.11714639663696289
	model : 0.06825952529907227
			 train-loss:  2.071053425470988 	 ± 0.26569379041747976
	data : 0.11706867218017578
	model : 0.06830263137817383
			 train-loss:  2.0434188842773438 	 ± 0.265342951748517
	data : 0.11740126609802246
	model : 0.06828818321228028
			 train-loss:  2.0589360107075083 	 ± 0.25770927132890503
	data : 0.11756396293640137
	model : 0.06908206939697266
			 train-loss:  2.0400431553522744 	 ± 0.254570034835892
	data : 0.11686744689941406
	model : 0.06828079223632813
			 train-loss:  2.0478184589972863 	 ± 0.2460615723026407
	data : 0.1178140640258789
	model : 0.06824560165405273
			 train-loss:  2.0385229757853915 	 ± 0.23946782653187101
	data : 0.11781225204467774
	model : 0.06828689575195312
			 train-loss:  2.0278276522954304 	 ± 0.23478353826040246
	data : 0.11760072708129883
	model : 0.06918568611145019
			 train-loss:  2.024906024336815 	 ± 0.22760962550411332
	data : 0.11676850318908691
	model : 0.06934819221496583
			 train-loss:  2.014915298013126 	 ± 0.22440088085010113
	data : 0.11667404174804688
	model : 0.06931843757629394
			 train-loss:  2.0250751972198486 	 ± 0.22206533665989364
	data : 0.11672368049621581
	model : 0.06908988952636719
			 train-loss:  2.0172604510658667 	 ± 0.21867065784849732
	data : 0.11677236557006836
	model : 0.06881465911865234
			 train-loss:  2.021238887310028 	 ± 0.2138381278390927
	data : 0.11701407432556152
	model : 0.06878981590270997
			 train-loss:  2.015499818892706 	 ± 0.2102570307980232
	data : 0.11695370674133301
	model : 0.06866955757141113
			 train-loss:  2.0242413824254815 	 ± 0.20929232471320716
	data : 0.11719274520874023
	model : 0.06960301399230957
			 train-loss:  2.0325616235318393 	 ± 0.20837890473128784
	data : 0.11617255210876465
	model : 0.0697594165802002
			 train-loss:  2.019026259581248 	 ± 0.21407070650517895
	data : 0.11612629890441895
	model : 0.06988587379455566
			 train-loss:  2.0076534223556517 	 ± 0.21701937914785197
	data : 0.11608867645263672
	model : 0.0698307991027832
			 train-loss:  2.010718827064221 	 ± 0.2133562506575413
	data : 0.11622276306152343
	model : 0.06915326118469238
			 train-loss:  2.0033907051439637 	 ± 0.21267620097203185
	data : 0.11686253547668457
	model : 0.06830248832702637
			 train-loss:  2.005425891705922 	 ± 0.20911145648909396
	data : 0.11782622337341309
	model : 0.06745676994323731
			 train-loss:  2.0159343809917054 	 ± 0.21286556000087573
	data : 0.11843843460083008
	model : 0.06761021614074707
			 train-loss:  2.0190582076708474 	 ± 0.20996272426438417
	data : 0.11815738677978516
	model : 0.06768918037414551
			 train-loss:  2.0220790409272715 	 ± 0.2072101178116734
	data : 0.1179476261138916
	model : 0.06847209930419922
			 train-loss:  2.018356569111347 	 ± 0.20499717701397313
	data : 0.11725597381591797
	model : 0.06946687698364258
			 train-loss:  2.0119476968591865 	 ± 0.20509692684645278
	data : 0.11628403663635253
	model : 0.06984143257141114
			 train-loss:  2.027345727471744 	 ± 0.22057159684019564
	data : 0.11618375778198242
	model : 0.06973147392272949
			 train-loss:  2.0416851248059955 	 ± 0.23292230422806454
	data : 0.11622862815856934
	model : 0.06916146278381348
			 train-loss:  2.0431619683901467 	 ± 0.22983062227230322
	data : 0.11672482490539551
	model : 0.06896052360534669
			 train-loss:  2.051146378388276 	 ± 0.23170998924254912
	data : 0.11671876907348633
	model : 0.06940112113952637
			 train-loss:  2.0624606734827946 	 ± 0.23877422003179194
	data : 0.11602363586425782
	model : 0.06922383308410644
			 train-loss:  2.0575149059295654 	 ± 0.2376568040372866
	data : 0.11610069274902343
	model : 0.06928625106811523
			 train-loss:  2.0535309851169585 	 ± 0.2359824802166266
	data : 0.11626524925231933
	model : 0.07010087966918946
			 train-loss:  2.0550231410235895 	 ± 0.23327784892369507
	data : 0.11567034721374511
	model : 0.06935782432556152
			 train-loss:  2.0592502242042903 	 ± 0.23206781862893353
	data : 0.11635866165161132
	model : 0.06816396713256836
			 train-loss:  2.0675105605014537 	 ± 0.23551817793293386
	data : 0.1176724910736084
	model : 0.0679203987121582
			 train-loss:  2.0674126798456367 	 ± 0.23282733791876062
	data : 0.11792116165161133
	model : 0.06731433868408203
			 train-loss:  2.0634732484817504 	 ± 0.23170406916174296
	data : 0.11831369400024414
	model : 0.06697940826416016
			 train-loss:  2.053387628949207 	 ± 0.2389499096218002
	data : 0.11862034797668457
	model : 0.067604398727417
			 train-loss:  2.054473027269891 	 ± 0.23650881669997734
	data : 0.11814260482788086
	model : 0.06816678047180176
			 train-loss:  2.053668938577175 	 ± 0.23409713056987705
	data : 0.11747822761535645
	model : 0.0690962791442871
			 train-loss:  2.057606733575159 	 ± 0.2332967386786991
	data : 0.11658592224121093
	model : 0.06943330764770508
			 train-loss:  2.057528569698334 	 ± 0.23095263644856978
	data : 0.11626267433166504
	model : 0.06952424049377441
			 train-loss:  2.0596666920418834 	 ± 0.22917642447853404
	data : 0.11612277030944824
	model : 0.06946606636047363
			 train-loss:  2.059211004238862 	 ± 0.22698543674982263
	data : 0.1162287712097168
	model : 0.0685802936553955
			 train-loss:  2.0500788216320975 	 ± 0.23427953268548524
	data : 0.11718602180480957
	model : 0.0684328556060791
			 train-loss:  2.045242768746835 	 ± 0.23475520861035634
	data : 0.1173792839050293
	model : 0.06796560287475586
			 train-loss:  2.0525428728623822 	 ± 0.23871688233149987
	data : 0.11774449348449707
	model : 0.06795916557312012
			 train-loss:  2.0527352137225017 	 ± 0.23658018082346105
	data : 0.11770658493041992
	model : 0.06831598281860352
			 train-loss:  2.0584902637883236 	 ± 0.23841770616339114
	data : 0.11726536750793456
	model : 0.07000017166137695
			 train-loss:  2.0514103564722785 	 ± 0.24232226648917982
	data : 0.11570954322814941
	model : 0.06927595138549805
			 train-loss:  2.056277050810345 	 ± 0.24310190996433048
	data : 0.11646838188171386
	model : 0.06905574798583984
			 train-loss:  2.068315416574478 	 ± 0.25819367875781213
	data : 0.11675286293029785
	model : 0.06808586120605468
			 train-loss:  2.0767533134241574 	 ± 0.2642782811427347
	data : 0.11758303642272949
	model : 0.06806378364562989
			 train-loss:  2.079727613156842 	 ± 0.26316562031776397
	data : 0.11767697334289551
	model : 0.06729207038879395
			 train-loss:  2.084844909017048 	 ± 0.26415983096943596
	data : 0.11845388412475585
	model : 0.06824278831481934
			 train-loss:  2.085390029475093 	 ± 0.2621236695548799
	data : 0.11747326850891113
	model : 0.06915140151977539
			 train-loss:  2.087248649963966 	 ± 0.26052417447920745
	data : 0.11659059524536133
	model : 0.06934061050415039
			 train-loss:  2.085075470534238 	 ± 0.25913595874652373
	data : 0.11648750305175781
	model : 0.06917357444763184
			 train-loss:  2.088226234734948 	 ± 0.25846544746604244
	data : 0.11651844978332519
	model : 0.06898250579833984
			 train-loss:  2.0890912676558777 	 ± 0.2566556154870305
	data : 0.11664886474609375
	model : 0.06882243156433106
			 train-loss:  2.0841608220252437 	 ± 0.2580125360774485
	data : 0.11678118705749511
	model : 0.06822648048400878
			 train-loss:  2.0869392020361763 	 ± 0.2572005061519771
	data : 0.11745252609252929
	model : 0.06820683479309082
			 train-loss:  2.0882420674176285 	 ± 0.25561534131490987
	data : 0.1174433708190918
	model : 0.06729135513305665
			 train-loss:  2.0891741812229156 	 ± 0.2539555101888905
	data : 0.1182173728942871
	model : 0.06750078201293945
			 train-loss:  2.0868522369698304 	 ± 0.25297848231586095
	data : 0.11811819076538085
	model : 0.0675417423248291
			 train-loss:  2.0831833530116723 	 ± 0.25321118646282775
	data : 0.11801724433898926
	model : 0.06728019714355468
			 train-loss:  2.08516702969869 	 ± 0.2520956450118585
	data : 0.11825823783874512
	model : 0.06795916557312012
			 train-loss:  2.079343540103812 	 ± 0.25545934748680704
	data : 0.1176884651184082
	model : 0.06879177093505859
			 train-loss:  2.07663331558178 	 ± 0.25489251973367905
	data : 0.11695356369018554
	model : 0.06866726875305176
			 train-loss:  2.0814380447069802 	 ± 0.25673882228626665
	data : 0.11696367263793946
	model : 0.06826105117797851
			 train-loss:  2.0785016440138033 	 ± 0.25642349550481225
	data : 0.11734476089477539
	model : 0.06910262107849122
			 train-loss:  2.085219383239746 	 ± 0.26171780611592727
	data : 0.11641349792480468
	model : 0.06841301918029785
			 train-loss:  2.086115183653655 	 ± 0.26022062655747685
	data : 0.11699013710021973
	model : 0.06868243217468262
			 train-loss:  2.0848755196827216 	 ± 0.2588695862028551
	data : 0.11685919761657715
	model : 0.06884360313415527
			 train-loss:  2.083392933190587 	 ± 0.2576554146265664
	data : 0.11667633056640625
	model : 0.06832308769226074
			 train-loss:  2.0821994017987024 	 ± 0.256347877631832
	data : 0.11712121963500977
	model : 0.06833934783935547
			 train-loss:  2.0862654110964605 	 ± 0.2575458211307941
	data : 0.11724352836608887
	model : 0.06904659271240235
			 train-loss:  2.082111488941104 	 ± 0.25889235899259094
	data : 0.11659846305847169
	model : 0.06880049705505371
			 train-loss:  2.079675690881137 	 ± 0.2583894269892621
	data : 0.11699481010437011
	model : 0.06868000030517578
			 train-loss:  2.0792202299291436 	 ± 0.2569522317365541
	data : 0.11713881492614746
	model : 0.06964898109436035
			 train-loss:  2.0761588905634505 	 ± 0.2571134378733293
	data : 0.11632275581359863
	model : 0.06950769424438477
			 train-loss:  2.076054996914334 	 ± 0.25568291850302677
	data : 0.1165283203125
	model : 0.06960701942443848
			 train-loss:  2.075045889550513 	 ± 0.25445433518537963
	data : 0.11653046607971192
	model : 0.0697371482849121
			 train-loss:  2.071987389222435 	 ± 0.25474396846318004
	data : 0.1163865566253662
	model : 0.06959075927734375
			 train-loss:  2.0713830007019864 	 ± 0.25343698531100034
	data : 0.1165048599243164
	model : 0.06860342025756835
			 train-loss:  2.0721014953674155 	 ± 0.2521805191073901
	data : 0.1174318790435791
	model : 0.06886234283447265
			 train-loss:  2.0738725900650024 	 ± 0.2514367713744368
	data : 0.1171231746673584
	model : 0.06878271102905273
			 train-loss:  2.072406876832247 	 ± 0.2505314223739341
	data : 0.11708989143371581
	model : 0.06862444877624511
			 train-loss:  2.071378186805961 	 ± 0.24944039168127669
	data : 0.11726570129394531
	model : 0.06830534934997559
			 train-loss:  2.0720352931898467 	 ± 0.2482488456420313
	data : 0.1174771785736084
	model : 0.06929545402526856
			 train-loss:  2.068229230967435 	 ± 0.24984921095503748
	data : 0.11659002304077148
	model : 0.06935706138610839
			 train-loss:  2.0673422348499297 	 ± 0.24875343491465746
	data : 0.11654796600341796
	model : 0.06982111930847168
			 train-loss:  2.0667710245245754 	 ± 0.2475848205224311
	data : 0.11634626388549804
	model : 0.06994647979736328
			 train-loss:  2.068305549668331 	 ± 0.24685038321237077
	data : 0.11632738113403321
	model : 0.07053170204162598
			 train-loss:  2.069905914149238 	 ± 0.24618031530988813
	data : 0.11585965156555175
	model : 0.07044296264648438
			 train-loss:  2.0681749891776304 	 ± 0.24562289819446
	data : 0.11596932411193847
	model : 0.07030830383300782
			 train-loss:  2.0686003378459383 	 ± 0.24448895012290756
	data : 0.11605010032653809
	model : 0.06968173980712891
			 train-loss:  2.072095033132805 	 ± 0.2459538311844918
	data : 0.11647448539733887
	model : 0.06919126510620117
			 train-loss:  2.072286638144021 	 ± 0.2448097644582813
	data : 0.11677522659301758
	model : 0.06909904479980469
			 train-loss:  2.0714414196985738 	 ± 0.2438305491305143
	data : 0.11704082489013672
	model : 0.06997203826904297
			 train-loss:  2.069805281971573 	 ± 0.24330434330872208
	data : 0.11610517501831055
	model : 0.06982970237731934
			 train-loss:  2.0733582529154693 	 ± 0.24502004320347542
	data : 0.11631608009338379
	model : 0.06999545097351074
			 train-loss:  2.06962036119925 	 ± 0.24704426549928518
	data : 0.11619348526000976
	model : 0.07140569686889649
			 train-loss:  2.0638214253953526 	 ± 0.2534139311652738
	data : 0.11482863426208496
	model : 0.07137398719787598
			 train-loss:  2.0629822733127967 	 ± 0.2524463940499709
	data : 0.11462893486022949
	model : 0.07058148384094239
			 train-loss:  2.063786304833596 	 ± 0.25148201663989955
	data : 0.11545662879943848
	model : 0.07074470520019531
			 train-loss:  2.063629472774008 	 ± 0.25039182846194225
	data : 0.11520218849182129
	model : 0.0709183692932129
			 train-loss:  2.0669271627376817 	 ± 0.2518058413020853
	data : 0.1150707721710205
	model : 0.06987233161926269
			 train-loss:  2.0677148531644773 	 ± 0.2508709258901705
	data : 0.11613740921020507
	model : 0.06985678672790527
			 train-loss:  2.065254015437627 	 ± 0.25121979054052807
	data : 0.11604905128479004
	model : 0.0695767879486084
			 train-loss:  2.0670762843444566 	 ± 0.2509439653586122
	data : 0.11632070541381836
	model : 0.06947731971740723
			 train-loss:  2.0720950782299044 	 ± 0.25582319394002645
	data : 0.11638112068176269
	model : 0.06929454803466797
			 train-loss:  2.073407328818455 	 ± 0.255169110190698
	data : 0.11635332107543946
	model : 0.06869111061096192
			 train-loss:  2.0725929043332085 	 ± 0.2542790466752646
	data : 0.11700096130371093
	model : 0.06836500167846679
			 train-loss:  2.072074567399374 	 ± 0.2533079909160947
	data : 0.11749539375305176
	model : 0.06792511940002441
			 train-loss:  2.071518068352053 	 ± 0.25236000296945976
	data : 0.11789870262145996
	model : 0.06753334999084473
			 train-loss:  2.071263991355896 	 ± 0.2513644592316759
	data : 0.11834630966186524
	model : 0.06806478500366211
			 train-loss:  2.0739094558216276 	 ± 0.2521060106427173
	data : 0.1180729866027832
	model : 0.06834897994995118
			 train-loss:  2.0722976190837348 	 ± 0.25176246451834694
	data : 0.11771516799926758
	model : 0.06880159378051758
			 train-loss:  2.0745672760531306 	 ± 0.2520781008454794
	data : 0.1173133373260498
	model : 0.0695375919342041
			 train-loss:  2.071025906607162 	 ± 0.25427558034028325
	data : 0.11658201217651368
	model : 0.07008957862854004
			 train-loss:  2.077112125433408 	 ± 0.26255883541688213
	data : 0.11619467735290527
	model : 0.06963262557983399
			 train-loss:  2.0783556117356277 	 ± 0.26193876745100203
	data : 0.11648850440979004
	model : 0.06994867324829102
			 train-loss:  2.0769102067658394 	 ± 0.26146857411737934
	data : 0.11623215675354004
	model : 0.0699625015258789
			 train-loss:  2.075232003864489 	 ± 0.26119637582092764
	data : 0.11624946594238281
	model : 0.07001996040344238
			 train-loss:  2.07485827937055 	 ± 0.26025562800024477
	data : 0.11632142066955567
	model : 0.0699162483215332
			 train-loss:  2.077720308303833 	 ± 0.26139794973992453
	data : 0.11629414558410645
	model : 0.06996397972106934
			 train-loss:  2.0790559123544132 	 ± 0.260897083133303
	data : 0.11632781028747559
	model : 0.07022747993469239
			 train-loss:  2.0793501478042047 	 ± 0.2599658066002089
	data : 0.11597170829772949
	model : 0.06928529739379882
			 train-loss:  2.0812568837317866 	 ± 0.2599818788012309
	data : 0.11685738563537598
	model : 0.06891975402832032
			 train-loss:  2.085620394713587 	 ± 0.26406791860262563
	data : 0.11697592735290527
	model : 0.06880326271057129
			 train-loss:  2.0841526338032312 	 ± 0.2636915464156784
	data : 0.11706681251525879
	model : 0.06857151985168457
			 train-loss:  2.0873376224057894 	 ± 0.2654435335433586
	data : 0.11743631362915039
	model : 0.06851081848144532
			 train-loss:  2.088767431151699 	 ± 0.26505155007852976
	data : 0.11746439933776856
	model : 0.06879305839538574
			 train-loss:  2.0896446271376177 	 ± 0.26432993463077087
	data : 0.11719093322753907
	model : 0.06914606094360351
			 train-loss:  2.0901657972070904 	 ± 0.2634842409597818
	data : 0.11699137687683106
	model : 0.06839723587036133
			 train-loss:  2.0885704048748672 	 ± 0.2632711135860645
	data : 0.11772394180297852
	model : 0.06908831596374512
			 train-loss:  2.0888226399682974 	 ± 0.2623855312100624
	data : 0.1170729637145996
	model : 0.0695103645324707
			 train-loss:  2.088470281386862 	 ± 0.26152619864090826
	data : 0.11665287017822265
	model : 0.0692967414855957
			 train-loss:  2.0893913322203868 	 ± 0.2608802847143508
	data : 0.11711115837097168
	model : 0.06900029182434082
			 train-loss:  2.089145041952197 	 ± 0.2600206373710723
	data : 0.11728382110595703
	model : 0.06965351104736328
			 train-loss:  2.0876118405659994 	 ± 0.2598273444300493
	data : 0.11674537658691406
	model : 0.06921114921569824
			 train-loss:  2.087117545652074 	 ± 0.2590363107591519
	data : 0.11709475517272949
	model : 0.06786098480224609
			 train-loss:  2.0892507151553503 	 ± 0.259510069945191
	data : 0.11843571662902833
	model : 0.06866679191589356
			 train-loss:  2.08811623754065 	 ± 0.25903849113244776
	data : 0.11741065979003906
	model : 0.0688591480255127
			 train-loss:  2.090209570798007 	 ± 0.25949117952374295
	data : 0.11730732917785644
	model : 0.06842575073242188
			 train-loss:  2.0931848556764665 	 ± 0.2612747707975539
	data : 0.11756634712219238
	model : 0.06810302734375
			 train-loss:  2.0929771814590845 	 ± 0.2604488391201809
	data : 0.11779227256774902
	model : 0.06882219314575196
			 train-loss:  2.090987010366598 	 ± 0.2608053268225969
	data : 0.11714615821838378
	model : 0.06883187294006347
			 train-loss:  2.0910695904417884 	 ± 0.25998074256936415
	data : 0.11720576286315917
	model : 0.06809725761413574
			 train-loss:  2.0931266111397893 	 ± 0.26044854203503276
	data : 0.11786246299743652
	model : 0.06880788803100586
			 train-loss:  2.093104624003172 	 ± 0.25963351265804885
	data : 0.11721687316894532
	model : 0.06922831535339355
			 train-loss:  2.094438193747716 	 ± 0.2593750436964063
	data : 0.11688418388366699
	model : 0.06946043968200684
			 train-loss:  2.0962207354145286 	 ± 0.25956059636502643
	data : 0.11654572486877442
	model : 0.06943221092224121
			 train-loss:  2.096388908251663 	 ± 0.2587720262193208
	data : 0.11653294563293456
	model : 0.0702436923980713
			 train-loss:  2.0963661837868575 	 ± 0.2579820439214406
	data : 0.11584877967834473
	model : 0.07105574607849122
			 train-loss:  2.096947202537999 	 ± 0.2573066974120414
	data : 0.1149895191192627
	model : 0.07084865570068359
			 train-loss:  2.097452866743846 	 ± 0.2566127250518603
	data : 0.11521148681640625
	model : 0.07073731422424316
			 train-loss:  2.098391753470826 	 ± 0.2561290868174155
	data : 0.1152733325958252
	model : 0.07055983543395997
			 train-loss:  2.0975214384851 	 ± 0.25561321212785304
	data : 0.11547136306762695
	model : 0.07055149078369141
			 train-loss:  2.0957752035919732 	 ± 0.25585891943141387
	data : 0.11554927825927734
	model : 0.0696946620941162
			 train-loss:  2.0953538151348337 	 ± 0.2551640934493529
	data : 0.11639266014099121
	model : 0.06981334686279297
			 train-loss:  2.0939599575355037 	 ± 0.25506517696498593
	data : 0.11614642143249512
	model : 0.06977896690368653
			 train-loss:  2.0932015908318897 	 ± 0.25451590210121094
	data : 0.11625509262084961
	model : 0.06940436363220215
			 train-loss:  2.096307450636274 	 ± 0.2570273875029004
	data : 0.11653995513916016
	model : 0.0685842514038086
			 train-loss:  2.095726348887915 	 ± 0.2564016840306939
	data : 0.11722478866577149
	model : 0.06854820251464844
			 train-loss:  2.095483296258109 	 ± 0.25568815965461905
	data : 0.1173210620880127
	model : 0.06881465911865234
			 train-loss:  2.095858710733327 	 ± 0.25500910135821964
	data : 0.11716985702514648
	model : 0.06812815666198731
			 train-loss:  2.0938880975637058 	 ± 0.25562806248916475
	data : 0.1177553653717041
	model : 0.06872844696044922
			 train-loss:  2.0943579385789595 	 ± 0.25498562405521197
	data : 0.11727800369262695
	model : 0.06949777603149414
			 train-loss:  2.0935313601733587 	 ± 0.25451140750039136
	data : 0.11684675216674804
	model : 0.0693333625793457
			 train-loss:  2.095076474216249 	 ± 0.25464392810864406
	data : 0.1169776439666748
	model : 0.06806325912475586
			 train-loss:  2.093887397597508 	 ± 0.2544401327462009
	data : 0.11808843612670898
	model : 0.06908812522888183
			 train-loss:  2.094665751352415 	 ± 0.2539561460674263
	data : 0.11729049682617188
	model : 0.06890578269958496
			 train-loss:  2.0955754121144614 	 ± 0.25355847699335216
	data : 0.11729059219360352
	model : 0.06803526878356933
			 train-loss:  2.0931139523568363 	 ± 0.2550514580348353
	data : 0.11806092262268067
	model : 0.06839661598205567
			 train-loss:  2.0919539812448864 	 ± 0.2548473973891694
	data : 0.1177208423614502
	model : 0.06946234703063965
			 train-loss:  2.0920859690635436 	 ± 0.25416774053416374
	data : 0.11676087379455566
	model : 0.06932058334350585
			 train-loss:  2.0928861712389453 	 ± 0.2537220513180089
	data : 0.11687288284301758
	model : 0.0688753604888916
			 train-loss:  2.0917680510815155 	 ± 0.253507881113205
	data : 0.11740655899047851
	model : 0.0694882869720459
			 train-loss:  2.0908011128662753 	 ± 0.25318370209847746
	data : 0.11645998954772949
	model : 0.06914691925048828
			 train-loss:  2.0892339116648624 	 ± 0.25343404415109444
	data : 0.11695032119750977
	model : 0.06898164749145508
			 train-loss:  2.0908136455176387 	 ± 0.2537059211695864
	data : 0.11704549789428711
	model : 0.0686917781829834
			 train-loss:  2.090650117645661 	 ± 0.25305445824370837
	data : 0.11709012985229492
	model : 0.06932530403137208
			 train-loss:  2.0909843222464923 	 ± 0.25244050427119613
	data : 0.11647629737854004
	model : 0.06966605186462402
			 train-loss:  2.0928982791212416 	 ± 0.2531891114457448
	data : 0.11638011932373046
	model : 0.06993699073791504
			 train-loss:  2.0917018780341516 	 ± 0.2530882668717538
	data : 0.11608209609985351
	model : 0.07073755264282226
			 train-loss:  2.092724036197273 	 ± 0.25284501935114784
	data : 0.11523852348327637
	model : 0.07069458961486816
			 train-loss:  2.09176532871227 	 ± 0.2525593603637551
	data : 0.11543879508972169
	model : 0.07068910598754882
			 train-loss:  2.091268968702567 	 ± 0.2520170894207134
	data : 0.11559224128723145
	model : 0.07063918113708496
			 train-loss:  2.091203744087986 	 ± 0.2513847585685151
	data : 0.11566038131713867
	model : 0.07067527770996093
			 train-loss:  2.094035604596138 	 ± 0.2539176888480525
	data : 0.11550869941711425
	model : 0.0689807415008545
			 train-loss:  2.0952097889202745 	 ± 0.25382901202864067
	data : 0.11708064079284668
	model : 0.06927180290222168
			 train-loss:  2.0962793927381536 	 ± 0.25365363311257755
	data : 0.1168062686920166
	model : 0.06846632957458496
			 train-loss:  2.095425532956429 	 ± 0.2533189539318072
	data : 0.11732478141784668
	model : 0.06859374046325684
			 train-loss:  2.0954652849365685 	 ± 0.25269794611035423
	data : 0.1171536922454834
	model : 0.06839051246643066
			 train-loss:  2.0941902381617847 	 ± 0.2527378283697145
	data : 0.11753759384155274
	model : 0.06928882598876954
			 train-loss:  2.093822211895174 	 ± 0.252178698768614
	data : 0.11699185371398926
	model : 0.06897110939025879
			 train-loss:  2.092690360718879 	 ± 0.25209280472989026
	data : 0.11717810630798339
	model : 0.0697786808013916
			 train-loss:  2.0935631073438206 	 ± 0.2517993620540706
	data : 0.11654868125915527
	model : 0.06972122192382812
			 train-loss:  2.0965256953353517 	 ± 0.2548041530777992
	data : 0.11656298637390136
	model : 0.06988162994384765
			 train-loss:  2.0964765491939725 	 ± 0.25419774550706586
	data : 0.11642923355102539
	model : 0.06984190940856934
			 train-loss:  2.0949590765469446 	 ± 0.25454631482877027
	data : 0.11639995574951172
	model : 0.07006373405456542
			 train-loss:  2.094992904730563 	 ± 0.25394573552896016
	data : 0.11632652282714843
	model : 0.06950597763061524
			 train-loss:  2.095050759718452 	 ± 0.25335031791286644
	data : 0.11676883697509766
	model : 0.06944432258605956
			 train-loss:  2.0962401143858367 	 ± 0.25335301351758094
	data : 0.1168832778930664
	model : 0.06844406127929688
			 train-loss:  2.095291530254275 	 ± 0.25314375663530575
	data : 0.11776089668273926
	model : 0.06837706565856934
			 train-loss:  2.0944464670287237 	 ± 0.25286088082269814
	data : 0.11762890815734864
	model : 0.06829190254211426
			 train-loss:  2.0933008457658477 	 ± 0.25283881385887275
	data : 0.11774659156799316
	model : 0.06884288787841797
			 train-loss:  2.094038201034616 	 ± 0.25249198378082105
	data : 0.11729617118835449
	model : 0.06825332641601563
			 train-loss:  2.092266026152868 	 ± 0.253270107908243
	data : 0.11773390769958496
	model : 0.06921348571777344
			 train-loss:  2.0917908029122785 	 ± 0.25279168164944477
	data : 0.11688957214355469
	model : 0.06928486824035644
			 train-loss:  2.0910598746252274 	 ± 0.2524520032604073
	data : 0.11685366630554199
	model : 0.06857447624206543
			 train-loss:  2.090153923442772 	 ± 0.252242577717792
	data : 0.11740264892578126
	model : 0.06844844818115234
			 train-loss:  2.0894044771322755 	 ± 0.2519239744277029
	data : 0.11754522323608399
	model : 0.06845932006835938
			 train-loss:  2.087351744728429 	 ± 0.25322325589279165
	data : 0.11765708923339843
	model : 0.06826314926147461
			 train-loss:  2.0877730994754367 	 ± 0.25273859915254904
	data : 0.11776280403137207
	model : 0.06809425354003906
			 train-loss:  2.0877312168610835 	 ± 0.25217960559955127
	data : 0.11787700653076172
	model : 0.06867709159851074
			 train-loss:  2.0874711816006295 	 ± 0.25165389527376614
	data : 0.11702876091003418
	model : 0.06778388023376465
			 train-loss:  2.088066744176965 	 ± 0.251261690184709
	data : 0.11761040687561035
	model : 0.06770820617675781
			 train-loss:  2.0878260593747466 	 ± 0.2507388228985197
	data : 0.11750526428222656
	model : 0.06761021614074707
			 train-loss:  2.08620352278585 	 ± 0.2513950691422008
	data : 0.11741065979003906
	model : 0.06702208518981934
			 train-loss:  2.085246328667645 	 ± 0.25127001551412503
	data : 0.11800360679626465
	model : 0.06701641082763672
			 train-loss:  2.0859041162605942 	 ± 0.2509271412453156
	data : 0.11840834617614746
	model : 0.06774601936340333
			 train-loss:  2.085356119876256 	 ± 0.25052717660190366
	data : 0.1176614761352539
	model : 0.0675480842590332
			 train-loss:  2.0859804683261447 	 ± 0.25017288127526516
	data : 0.11779446601867676
	model : 0.06680707931518555
			 train-loss:  2.0864187189873227 	 ± 0.24973003025529297
	data : 0.11843090057373047
	model : 0.0669133186340332
			 train-loss:  2.0878490431834074 	 ± 0.2501631439295
	data : 0.1184682846069336
	model : 0.06689648628234864
			 train-loss:  2.0874593348442754 	 ± 0.24970659409366258
	data : 0.11833267211914063
	model : 0.06687469482421875
			 train-loss:  2.085851724909133 	 ± 0.250407466399943
	data : 0.11876916885375977
	model : 0.06756296157836914
			 train-loss:  2.085920322390281 	 ± 0.24988529319283911
	data : 0.1182013988494873
	model : 0.0682215690612793
			 train-loss:  2.085936236878236 	 ± 0.24936427678568435
	data : 0.11774735450744629
	model : 0.06829118728637695
			 train-loss:  2.0853659879122532 	 ± 0.24900314821326233
	data : 0.11762123107910157
	model : 0.06812138557434082
			 train-loss:  2.084939228109092 	 ± 0.24857644846045607
	data : 0.11785669326782226
	model : 0.06819620132446289
			 train-loss:  2.086184496251644 	 ± 0.2488196887441875
	data : 0.11763215065002441
	model : 0.0679854393005371
			 train-loss:  2.08703840120894 	 ± 0.24866581443797156
	data : 0.1178400993347168
	model : 0.06811137199401855
			 train-loss:  2.0876163380486625 	 ± 0.24832196735927142
	data : 0.11751985549926758
	model : 0.06776442527770996
			 train-loss:  2.0903444731138587 	 ± 0.25146888525318695
	data : 0.11766462326049805
	model : 0.0678802490234375
			 train-loss:  2.0899077378786526 	 ± 0.25105278945221765
	data : 0.11743307113647461
	model : 0.06781268119812012
			 train-loss:  2.089053312136281 	 ± 0.2509057206230475
	data : 0.11729493141174316
	model : 0.06717500686645508
			 train-loss:  2.089270018669496 	 ± 0.25042464160229927
	data : 0.117789888381958
	model : 0.06682486534118652
			 train-loss:  2.089207238674164 	 ± 0.24992525384158193
	data : 0.11824989318847656
	model : 0.06734585762023926
			 train-loss:  2.088361318842823 	 ± 0.24978525274413263
	data : 0.1177757740020752
	model : 0.06715145111083984
			 train-loss:  2.087946566797438 	 ± 0.24937573930177107
	data : 0.11788663864135743
	model : 0.06691160202026367
			 train-loss:  2.088154455889826 	 ± 0.24890429257063532
	data : 0.11808552742004394
	model : 0.06710281372070312
			 train-loss:  2.0878718157452862 	 ± 0.24845451709489802
	data : 0.11819257736206054
	model : 0.06704292297363282
			 train-loss:  2.0863247787251193 	 ± 0.24918963501147995
	data : 0.11826181411743164
	model : 0.06703758239746094
			 train-loss:  2.087121027521789 	 ± 0.24902728133349666
	data : 0.11747918128967286
	model : 0.05844998359680176
#epoch  82    val-loss:  2.4202043759195426  train-loss:  2.087121027521789  lr:  4.8828125e-06
			 train-loss:  1.9416377544403076 	 ± 0.0
	data : 5.981995105743408
	model : 0.07555913925170898
			 train-loss:  2.2960078716278076 	 ± 0.3543701171875
	data : 3.0553611516952515
	model : 0.07119119167327881
			 train-loss:  2.216902812321981 	 ± 0.3102160657550558
	data : 2.0762268702189126
	model : 0.06972002983093262
			 train-loss:  2.0853559970855713 	 ± 0.35226296908124616
	data : 1.5868165493011475
	model : 0.06991922855377197
			 train-loss:  2.104019355773926 	 ± 0.31727691901376737
	data : 1.292436408996582
	model : 0.06970362663269043
			 train-loss:  2.1112108627955117 	 ± 0.2900789403741647
	data : 0.11934776306152343
	model : 0.06798782348632812
			 train-loss:  2.0702336515699113 	 ± 0.28670499700771157
	data : 0.11731929779052734
	model : 0.0679091453552246
			 train-loss:  2.051937520503998 	 ± 0.27252160424342603
	data : 0.11750807762145996
	model : 0.06816344261169434
			 train-loss:  2.0559376610649958 	 ± 0.2571848188019748
	data : 0.11737771034240722
	model : 0.06815900802612304
			 train-loss:  2.0482808351516724 	 ± 0.24506584985996438
	data : 0.1173208236694336
	model : 0.06843290328979493
			 train-loss:  2.09437988021157 	 ± 0.275406496717069
	data : 0.11700844764709473
	model : 0.06911220550537109
			 train-loss:  2.109862486521403 	 ± 0.2686351285865947
	data : 0.11644411087036133
	model : 0.0699380874633789
			 train-loss:  2.135013011785654 	 ± 0.2724046226049941
	data : 0.11589326858520507
	model : 0.07011251449584961
			 train-loss:  2.117738766329629 	 ± 0.2697835559191121
	data : 0.11578540802001953
	model : 0.07004342079162598
			 train-loss:  2.1268117666244506 	 ± 0.2628372625579707
	data : 0.11598548889160157
	model : 0.06987724304199219
			 train-loss:  2.1102976948022842 	 ± 0.26240508941210006
	data : 0.11631064414978028
	model : 0.06951360702514649
			 train-loss:  2.128054885303273 	 ± 0.26429365805869637
	data : 0.11655182838439941
	model : 0.06931171417236329
			 train-loss:  2.10838739739524 	 ± 0.2693441855691404
	data : 0.11668548583984376
	model : 0.0687685489654541
			 train-loss:  2.1141977749372782 	 ± 0.26331683057469746
	data : 0.11710906028747559
	model : 0.0687861442565918
			 train-loss:  2.1004219591617583 	 ± 0.26358045167661603
	data : 0.11712846755981446
	model : 0.06815738677978515
			 train-loss:  2.1046790906361172 	 ± 0.2579317758489791
	data : 0.11760182380676269
	model : 0.06849308013916015
			 train-loss:  2.1032905957915564 	 ± 0.2520818336581474
	data : 0.11736793518066406
	model : 0.06852006912231445
			 train-loss:  2.105271603750146 	 ± 0.2467159307832086
	data : 0.11729631423950196
	model : 0.06929574012756348
			 train-loss:  2.0969436864058175 	 ± 0.24480134667193612
	data : 0.1166541576385498
	model : 0.06931858062744141
			 train-loss:  2.08290762424469 	 ± 0.24951724658404414
	data : 0.11661992073059083
	model : 0.0700922966003418
			 train-loss:  2.061135654266064 	 ± 0.267796101285725
	data : 0.11595621109008789
	model : 0.06943120956420898
			 train-loss:  2.0732193920347424 	 ± 0.26991681056015837
	data : 0.11659908294677734
	model : 0.06960391998291016
			 train-loss:  2.0716273742062703 	 ± 0.2651821060624037
	data : 0.11640496253967285
	model : 0.06990132331848145
			 train-loss:  2.0856274119738876 	 ± 0.2708961018039734
	data : 0.11604824066162109
	model : 0.06984186172485352
			 train-loss:  2.0853619456291197 	 ± 0.26634673843043083
	data : 0.11616702079772949
	model : 0.06971955299377441
			 train-loss:  2.1146668580270584 	 ± 0.3072710787268663
	data : 0.11617488861083984
	model : 0.07009782791137695
			 train-loss:  2.1105126701295376 	 ± 0.303315028546659
	data : 0.11571230888366699
	model : 0.07030534744262695
			 train-loss:  2.110064806360187 	 ± 0.2986947374473145
	data : 0.11560444831848145
	model : 0.06998176574707031
			 train-loss:  2.1031276092809787 	 ± 0.2969555294998415
	data : 0.11612024307250976
	model : 0.07029471397399903
			 train-loss:  2.094956694330488 	 ± 0.29653508218665264
	data : 0.11584105491638183
	model : 0.07037925720214844
			 train-loss:  2.102816618151135 	 ± 0.2960620107233637
	data : 0.11575207710266114
	model : 0.07029156684875489
			 train-loss:  2.10818206619572 	 ± 0.2938028098843654
	data : 0.11584219932556153
	model : 0.06985249519348144
			 train-loss:  2.0969234830454777 	 ± 0.29789003079945864
	data : 0.11612563133239746
	model : 0.0689774990081787
			 train-loss:  2.1044752964606652 	 ± 0.2977083484152785
	data : 0.11655502319335938
	model : 0.06874117851257325
			 train-loss:  2.106483381986618 	 ± 0.2942308075133383
	data : 0.116463041305542
	model : 0.06873259544372559
			 train-loss:  2.103168298558491 	 ± 0.29137579288024174
	data : 0.11679129600524903
	model : 0.06914258003234863
			 train-loss:  2.1063415237835477 	 ± 0.28860227182811216
	data : 0.11641411781311035
	model : 0.06910147666931152
			 train-loss:  2.1012382756832033 	 ± 0.2871377316275903
	data : 0.11641969680786132
	model : 0.06996045112609864
			 train-loss:  2.090225330807946 	 ± 0.2928984637235855
	data : 0.1157710075378418
	model : 0.06893763542175294
			 train-loss:  2.0930649942821926 	 ± 0.2902376247067224
	data : 0.11694192886352539
	model : 0.06877055168151855
			 train-loss:  2.09318608045578 	 ± 0.28706668274894276
	data : 0.11702327728271485
	model : 0.06866583824157715
			 train-loss:  2.111924813148823 	 ± 0.3111372344954473
	data : 0.11731796264648438
	model : 0.06831822395324708
			 train-loss:  2.115049568315347 	 ± 0.30862354348206805
	data : 0.1176224708557129
	model : 0.06835789680480957
			 train-loss:  2.1259041829985015 	 ± 0.31457929445595123
	data : 0.11755633354187012
	model : 0.06969056129455567
			 train-loss:  2.1253703808784485 	 ± 0.31144002976634405
	data : 0.1164520263671875
	model : 0.0699389934539795
			 train-loss:  2.1194395154130223 	 ± 0.31121020190395016
	data : 0.116070556640625
	model : 0.0691007137298584
			 train-loss:  2.1211185478247128 	 ± 0.3084364310393242
	data : 0.11676497459411621
	model : 0.0694807529449463
			 train-loss:  2.127852257692589 	 ± 0.30934753474191934
	data : 0.116420316696167
	model : 0.0693284034729004
			 train-loss:  2.130656001744447 	 ± 0.3071487961608467
	data : 0.11645841598510742
	model : 0.06901497840881347
			 train-loss:  2.128218954259699 	 ± 0.304870169279275
	data : 0.11682815551757812
	model : 0.06926054954528808
			 train-loss:  2.1210985481739044 	 ± 0.30671580230572665
	data : 0.11680974960327148
	model : 0.07020077705383301
			 train-loss:  2.1233308524416206 	 ± 0.3044720189243197
	data : 0.11606836318969727
	model : 0.07038116455078125
			 train-loss:  2.1211581970083304 	 ± 0.3022812327926759
	data : 0.11606793403625489
	model : 0.07066011428833008
			 train-loss:  2.118814918954494 	 ± 0.3002394178096764
	data : 0.11597537994384766
	model : 0.07050943374633789
			 train-loss:  2.1155617157618205 	 ± 0.2987737093571067
	data : 0.11601138114929199
	model : 0.06998262405395508
			 train-loss:  2.1112384444377463 	 ± 0.29820093441559614
	data : 0.11636734008789062
	model : 0.06940054893493652
			 train-loss:  2.111584936418841 	 ± 0.29579869159559263
	data : 0.11694416999816895
	model : 0.06900687217712402
			 train-loss:  2.106493149484907 	 ± 0.2961679541186089
	data : 0.11715850830078126
	model : 0.0689023494720459
			 train-loss:  2.1090798545628786 	 ± 0.29456143459311107
	data : 0.11722164154052735
	model : 0.0690384864807129
			 train-loss:  2.106178817382226 	 ± 0.29320674537159436
	data : 0.11695904731750488
	model : 0.06907076835632324
			 train-loss:  2.112267452659029 	 ± 0.2950885680282039
	data : 0.11692900657653808
	model : 0.06960372924804688
			 train-loss:  2.1117466545816677 	 ± 0.29290869492300275
	data : 0.11633296012878418
	model : 0.06992716789245605
			 train-loss:  2.1057652445400463 	 ± 0.2948404344852868
	data : 0.11599678993225097
	model : 0.06992058753967285
			 train-loss:  2.111055823339932 	 ± 0.29592963553818175
	data : 0.11594672203063965
	model : 0.06982626914978027
			 train-loss:  2.1227653196879794 	 ± 0.30948996120148914
	data : 0.11607213020324707
	model : 0.06995210647583008
			 train-loss:  2.1245009932719485 	 ± 0.3076456477004637
	data : 0.11586489677429199
	model : 0.07004456520080567
			 train-loss:  2.122448499004046 	 ± 0.3059908870430703
	data : 0.11566720008850098
	model : 0.0692368507385254
			 train-loss:  2.1243873279388636 	 ± 0.30433282203825424
	data : 0.11667332649230958
	model : 0.06912236213684082
			 train-loss:  2.124469607262998 	 ± 0.3022703423624957
	data : 0.11682839393615722
	model : 0.06906118392944335
			 train-loss:  2.120626254081726 	 ± 0.3020632538376767
	data : 0.11689524650573731
	model : 0.06889762878417968
			 train-loss:  2.1162391132430027 	 ± 0.3024651703727851
	data : 0.11717286109924316
	model : 0.06872019767761231
			 train-loss:  2.1133528065371823 	 ± 0.3015463452332029
	data : 0.11729574203491211
	model : 0.06955504417419434
			 train-loss:  2.110472899216872 	 ± 0.30067100665925406
	data : 0.1164853572845459
	model : 0.06967110633850097
			 train-loss:  2.113675536988657 	 ± 0.30009789920136387
	data : 0.11646313667297363
	model : 0.0697333812713623
			 train-loss:  2.1105459541082383 	 ± 0.2995108738805419
	data : 0.11628646850585937
	model : 0.06937108039855958
			 train-loss:  2.108435304076583 	 ± 0.29825435478609713
	data : 0.11662487983703614
	model : 0.06884937286376953
			 train-loss:  2.1081781445479972 	 ± 0.2964391870822394
	data : 0.11717877388000489
	model : 0.06930780410766602
			 train-loss:  2.106572332152401 	 ± 0.2950065936280468
	data : 0.11665639877319336
	model : 0.06901135444641113
			 train-loss:  2.1087262743995305 	 ± 0.2939011872114265
	data : 0.11693215370178223
	model : 0.06900815963745117
			 train-loss:  2.1110886349397546 	 ± 0.29296839409535924
	data : 0.11703639030456543
	model : 0.06935677528381348
			 train-loss:  2.109331944654154 	 ± 0.29171005770606434
	data : 0.11668672561645507
	model : 0.06988797187805176
			 train-loss:  2.111421271302234 	 ± 0.29067519963927557
	data : 0.11606917381286622
	model : 0.06882853507995605
			 train-loss:  2.1093059967864645 	 ± 0.28969157066299833
	data : 0.11701745986938476
	model : 0.06909151077270508
			 train-loss:  2.108070902610093 	 ± 0.2882924062198019
	data : 0.11683382987976074
	model : 0.06905717849731445
			 train-loss:  2.108099636766646 	 ± 0.2866864360537258
	data : 0.11708536148071289
	model : 0.06878681182861328
			 train-loss:  2.1096003356870714 	 ± 0.28546212412146504
	data : 0.1172149658203125
	model : 0.06897544860839844
			 train-loss:  2.106200176736583 	 ± 0.2857532795129559
	data : 0.11713600158691406
	model : 0.06954855918884277
			 train-loss:  2.1074592041712936 	 ± 0.2844692610587083
	data : 0.11668310165405274
	model : 0.06952686309814453
			 train-loss:  2.1067361045391 	 ± 0.28303799612919994
	data : 0.11674189567565918
	model : 0.06902804374694824
			 train-loss:  2.106608940425672 	 ± 0.2815470809693091
	data : 0.1171480655670166
	model : 0.0687774658203125
			 train-loss:  2.103063481549422 	 ± 0.28220067157240514
	data : 0.1174532413482666
	model : 0.06862297058105468
			 train-loss:  2.108432681290145 	 ± 0.2856286647729337
	data : 0.1175835132598877
	model : 0.06799230575561524
			 train-loss:  2.1079474614591014 	 ± 0.2842078193338432
	data : 0.11791648864746093
	model : 0.06808419227600097
			 train-loss:  2.1138680920456396 	 ± 0.28877926457329395
	data : 0.1176231861114502
	model : 0.06783075332641601
			 train-loss:  2.1161790442466737 	 ± 0.28825030532245505
	data : 0.11767926216125488
	model : 0.0686535358428955
			 train-loss:  2.117406660967534 	 ± 0.2870823696657572
	data : 0.11713194847106934
	model : 0.06854228973388672
			 train-loss:  2.1176594332152723 	 ± 0.28568293164656944
	data : 0.11720786094665528
	model : 0.06908640861511231
			 train-loss:  2.1162101551167014 	 ± 0.2846692868177004
	data : 0.11691346168518066
	model : 0.06898655891418456
			 train-loss:  2.114110039976927 	 ± 0.28409801928864803
	data : 0.1169891357421875
	model : 0.06936430931091309
			 train-loss:  2.111339170592172 	 ± 0.2841504609127514
	data : 0.11655817031860352
	model : 0.06934962272644044
			 train-loss:  2.117658719701587 	 ± 0.2901260517757104
	data : 0.11646208763122559
	model : 0.06945586204528809
			 train-loss:  2.114589983057753 	 ± 0.29049041037046963
	data : 0.11645760536193847
	model : 0.06956543922424316
			 train-loss:  2.1117612189716763 	 ± 0.29061924036538644
	data : 0.11626009941101074
	model : 0.06974673271179199
			 train-loss:  2.1125559347485185 	 ± 0.2894009236008156
	data : 0.11613483428955078
	model : 0.070194673538208
			 train-loss:  2.1106151364066386 	 ± 0.28879417365052185
	data : 0.11582274436950683
	model : 0.07012686729431153
			 train-loss:  2.1097547449507155 	 ± 0.287631943240231
	data : 0.11575846672058106
	model : 0.0702136516571045
			 train-loss:  2.1127752172095433 	 ± 0.28810785487966983
	data : 0.11566734313964844
	model : 0.06941690444946289
			 train-loss:  2.113181487648888 	 ± 0.28686243166853875
	data : 0.11647605895996094
	model : 0.06866250038146973
			 train-loss:  2.1153507295407747 	 ± 0.2865308815664347
	data : 0.11714653968811035
	model : 0.06897878646850586
			 train-loss:  2.1186261425847595 	 ± 0.2874179242947873
	data : 0.11701374053955078
	model : 0.06805086135864258
			 train-loss:  2.1197066101534614 	 ± 0.28641083827000724
	data : 0.11788420677185059
	model : 0.0679666519165039
			 train-loss:  2.1164176555780263 	 ± 0.2873757904856399
	data : 0.11792178153991699
	model : 0.06884407997131348
			 train-loss:  2.11359846288875 	 ± 0.2877757313219213
	data : 0.11734418869018555
	model : 0.06941299438476563
			 train-loss:  2.1148030928203037 	 ± 0.28686265350116236
	data : 0.11678171157836914
	model : 0.06925458908081054
			 train-loss:  2.114646584788958 	 ± 0.28566999375885
	data : 0.11700868606567383
	model : 0.07054219245910645
			 train-loss:  2.1130340099334717 	 ± 0.28503500168119117
	data : 0.11577410697937011
	model : 0.07015953063964844
			 train-loss:  2.11319133883617 	 ± 0.2838696972041992
	data : 0.1162106990814209
	model : 0.07018909454345704
			 train-loss:  2.112151466734041 	 ± 0.2829466194982329
	data : 0.11611027717590332
	model : 0.07017302513122559
			 train-loss:  2.1110878827110415 	 ± 0.2820501604966764
	data : 0.11621632575988769
	model : 0.069110107421875
			 train-loss:  2.1139908895492554 	 ± 0.28277354602410787
	data : 0.11701326370239258
	model : 0.0680150032043457
			 train-loss:  2.111662893068223 	 ± 0.28284927812799215
	data : 0.11808538436889648
	model : 0.06815290451049805
			 train-loss:  2.1113640511129783 	 ± 0.2817534671539951
	data : 0.11790032386779785
	model : 0.06806759834289551
			 train-loss:  2.1100364234298468 	 ± 0.28104923129671733
	data : 0.11790494918823242
	model : 0.06806879043579102
			 train-loss:  2.1091424411581468 	 ± 0.28014041709227
	data : 0.11786379814147949
	model : 0.0690497875213623
			 train-loss:  2.111658243949597 	 ± 0.28051995714330585
	data : 0.11700387001037597
	model : 0.0699129581451416
			 train-loss:  2.1123436234379542 	 ± 0.279556461466188
	data : 0.11630001068115234
	model : 0.07012977600097656
			 train-loss:  2.112085953806386 	 ± 0.27851113713556713
	data : 0.11620469093322754
	model : 0.06917939186096192
			 train-loss:  2.11159174334734 	 ± 0.27752021954925876
	data : 0.11708292961120606
	model : 0.06924729347229004
			 train-loss:  2.1100062638966004 	 ± 0.2770867068262325
	data : 0.11695294380187989
	model : 0.06916022300720215
			 train-loss:  2.1102087948057386 	 ± 0.27606850739824906
	data : 0.11689190864562989
	model : 0.06948709487915039
			 train-loss:  2.111106417634908 	 ± 0.2752493381399022
	data : 0.1166031837463379
	model : 0.0685875415802002
			 train-loss:  2.1107740463131535 	 ± 0.2742703290832806
	data : 0.11722030639648437
	model : 0.0695047378540039
			 train-loss:  2.10733380611392 	 ± 0.2762255238564685
	data : 0.11641731262207031
	model : 0.06957573890686035
			 train-loss:  2.105222739761682 	 ± 0.2763451216127673
	data : 0.11644830703735351
	model : 0.06962614059448242
			 train-loss:  2.1032886411462512 	 ± 0.2762989555601
	data : 0.11649508476257324
	model : 0.06914963722229003
			 train-loss:  2.1012309028747236 	 ± 0.27639190773709954
	data : 0.1168182373046875
	model : 0.07023344039916993
			 train-loss:  2.0997278505647685 	 ± 0.27599466271128736
	data : 0.1158442497253418
	model : 0.07017807960510254
			 train-loss:  2.09915303016876 	 ± 0.27511323954355316
	data : 0.1158900260925293
	model : 0.06966485977172851
			 train-loss:  2.0980547848674984 	 ± 0.27447070293915
	data : 0.11629705429077149
	model : 0.06958036422729492
			 train-loss:  2.0979353214132375 	 ± 0.27352637147560005
	data : 0.11628584861755371
	model : 0.06962251663208008
			 train-loss:  2.0979262133167214 	 ± 0.27258804987722474
	data : 0.11628756523132325
	model : 0.06944255828857422
			 train-loss:  2.098725975776205 	 ± 0.27183112110399793
	data : 0.1164553165435791
	model : 0.06949100494384766
			 train-loss:  2.1000907533877604 	 ± 0.27141608628759417
	data : 0.11644740104675293
	model : 0.06918997764587402
			 train-loss:  2.100343827433234 	 ± 0.2705212809474418
	data : 0.11677088737487792
	model : 0.06841692924499512
			 train-loss:  2.1024900674819946 	 ± 0.2708878561843697
	data : 0.11744670867919922
	model : 0.06847057342529297
			 train-loss:  2.101893950771812 	 ± 0.27008808212460644
	data : 0.11734905242919921
	model : 0.06861896514892578
			 train-loss:  2.1023824889408913 	 ± 0.2692650979409514
	data : 0.11737332344055176
	model : 0.06841669082641602
			 train-loss:  2.1030839689416823 	 ± 0.26852301186314737
	data : 0.11750545501708984
	model : 0.06858859062194825
			 train-loss:  2.103401026168427 	 ± 0.2676784947078397
	data : 0.11731095314025879
	model : 0.06914787292480469
			 train-loss:  2.1002941031609814 	 ± 0.2695849856195392
	data : 0.11677355766296386
	model : 0.06895837783813477
			 train-loss:  2.097383229396282 	 ± 0.2711522378753469
	data : 0.11694622039794922
	model : 0.06896853446960449
			 train-loss:  2.0973780982813257 	 ± 0.2702873238505502
	data : 0.11682939529418945
	model : 0.06941032409667969
			 train-loss:  2.1000935993617094 	 ± 0.2715705655152628
	data : 0.11644110679626465
	model : 0.06987962722778321
			 train-loss:  2.1000589852063163 	 ± 0.27071557266723245
	data : 0.11610169410705566
	model : 0.06919374465942382
			 train-loss:  2.09823694601655 	 ± 0.270844476571255
	data : 0.11677656173706055
	model : 0.06927790641784667
			 train-loss:  2.0970714284766534 	 ± 0.2704042300048094
	data : 0.11669301986694336
	model : 0.06915178298950195
			 train-loss:  2.0988135852931458 	 ± 0.2704732000265168
	data : 0.11688547134399414
	model : 0.06802225112915039
			 train-loss:  2.097808298157768 	 ± 0.269945663419321
	data : 0.11788897514343262
	model : 0.06816587448120118
			 train-loss:  2.0968370895560193 	 ± 0.26940689861443057
	data : 0.11781916618347169
	model : 0.06923279762268067
			 train-loss:  2.0978669809572623 	 ± 0.2689129016601902
	data : 0.11685466766357422
	model : 0.06836996078491211
			 train-loss:  2.0961648656661254 	 ± 0.2689917446581473
	data : 0.11771621704101562
	model : 0.06842808723449707
			 train-loss:  2.0979609546547167 	 ± 0.2691817066711656
	data : 0.11765413284301758
	model : 0.06930603981018066
			 train-loss:  2.097118792789323 	 ± 0.2685999467446862
	data : 0.11687908172607422
	model : 0.06916189193725586
			 train-loss:  2.096942330958575 	 ± 0.26781386017827497
	data : 0.11699500083923339
	model : 0.06887445449829102
			 train-loss:  2.098594070182127 	 ± 0.2678869710273735
	data : 0.11742711067199707
	model : 0.06956019401550292
			 train-loss:  2.099177092139484 	 ± 0.2672106763432397
	data : 0.11676564216613769
	model : 0.06947493553161621
			 train-loss:  2.098685823900755 	 ± 0.26651020654650937
	data : 0.11668872833251953
	model : 0.06948575973510743
			 train-loss:  2.097573343729008 	 ± 0.2661390516197483
	data : 0.116508150100708
	model : 0.07002315521240235
			 train-loss:  2.0957593445120186 	 ± 0.26644361219213347
	data : 0.11604728698730468
	model : 0.07064800262451172
			 train-loss:  2.095607320921762 	 ± 0.2656888219634589
	data : 0.11533665657043457
	model : 0.07107300758361816
			 train-loss:  2.093994900584221 	 ± 0.2657902365222906
	data : 0.11517181396484374
	model : 0.07017650604248046
			 train-loss:  2.0960897483394643 	 ± 0.2664914347175119
	data : 0.11600322723388672
	model : 0.06983246803283691
			 train-loss:  2.094942734482583 	 ± 0.26617959621292103
	data : 0.11643366813659668
	model : 0.06934971809387207
			 train-loss:  2.0954700715049017 	 ± 0.26552826136466645
	data : 0.1167935848236084
	model : 0.06881480216979981
			 train-loss:  2.09368290371365 	 ± 0.2658670384002148
	data : 0.11732134819030762
	model : 0.06854786872863769
			 train-loss:  2.094015375685297 	 ± 0.265169101626423
	data : 0.11736750602722168
	model : 0.06938347816467286
			 train-loss:  2.0957044847718964 	 ± 0.2654142373983683
	data : 0.11686692237854004
	model : 0.06956734657287597
			 train-loss:  2.0941618700496485 	 ± 0.26550493734721575
	data : 0.11663284301757812
	model : 0.06871752738952637
			 train-loss:  2.0937703523946847 	 ± 0.2648354388556467
	data : 0.11747450828552246
	model : 0.06874723434448242
			 train-loss:  2.0930011665498887 	 ± 0.2643247044853622
	data : 0.11749200820922852
	model : 0.06893329620361328
			 train-loss:  2.0918540589271055 	 ± 0.2640745174310115
	data : 0.11734814643859863
	model : 0.06902666091918945
			 train-loss:  2.090845060858497 	 ± 0.26372674643136373
	data : 0.11697044372558593
	model : 0.06937274932861329
			 train-loss:  2.089166984913197 	 ± 0.26402352354975256
	data : 0.11676487922668458
	model : 0.07019462585449218
			 train-loss:  2.0883174415618653 	 ± 0.26358163338450274
	data : 0.11620073318481446
	model : 0.06932573318481446
			 train-loss:  2.087482000024695 	 ± 0.2631378593566987
	data : 0.11690354347229004
	model : 0.06911444664001465
			 train-loss:  2.086096290518476 	 ± 0.26314225842739697
	data : 0.1170236587524414
	model : 0.06912016868591309
			 train-loss:  2.085051133607825 	 ± 0.2628532718924819
	data : 0.11712064743041992
	model : 0.06895761489868164
			 train-loss:  2.0854619519080524 	 ± 0.26223321277988887
	data : 0.11724710464477539
	model : 0.06845827102661133
			 train-loss:  2.084502140885776 	 ± 0.26189614624377455
	data : 0.1176307201385498
	model : 0.06930756568908691
			 train-loss:  2.0835430573194453 	 ± 0.26156509540191025
	data : 0.11686973571777344
	model : 0.06918668746948242
			 train-loss:  2.0837903557991493 	 ± 0.260919838086061
	data : 0.1170581340789795
	model : 0.06912007331848144
			 train-loss:  2.084122216035872 	 ± 0.2602982291973337
	data : 0.11710591316223144
	model : 0.06847934722900391
			 train-loss:  2.083754127675837 	 ± 0.259691473956115
	data : 0.1175912857055664
	model : 0.06907320022583008
			 train-loss:  2.082780979985568 	 ± 0.2593998421857127
	data : 0.11698479652404785
	model : 0.0692178726196289
			 train-loss:  2.081110501885414 	 ± 0.2598213725906287
	data : 0.11681790351867676
	model : 0.06972012519836426
			 train-loss:  2.081776502120554 	 ± 0.25934533053725
	data : 0.11649546623229981
	model : 0.0696901798248291
			 train-loss:  2.0807708749676697 	 ± 0.25909515366406627
	data : 0.11655316352844239
	model : 0.07023611068725585
			 train-loss:  2.0806863167015788 	 ± 0.2584589945157919
	data : 0.11619510650634765
	model : 0.06987104415893555
			 train-loss:  2.0805771759912077 	 ± 0.25782942769131223
	data : 0.11653122901916504
	model : 0.0697321891784668
			 train-loss:  2.0791347172202133 	 ± 0.25802364379275133
	data : 0.11657028198242188
	model : 0.06949586868286133
			 train-loss:  2.081236403543972 	 ± 0.2591496050995207
	data : 0.11673798561096191
	model : 0.0696286678314209
			 train-loss:  2.0796701078829556 	 ± 0.25949847138508814
	data : 0.11657495498657226
	model : 0.06991105079650879
			 train-loss:  2.078610178942864 	 ± 0.2593227010513474
	data : 0.11646518707275391
	model : 0.06952142715454102
			 train-loss:  2.0791128851009897 	 ± 0.25880314074492133
	data : 0.11667308807373047
	model : 0.06941556930541992
			 train-loss:  2.077553889297304 	 ± 0.2591680642404641
	data : 0.11697807312011718
	model : 0.06932291984558106
			 train-loss:  2.0749839606443286 	 ± 0.26122156372008554
	data : 0.11691765785217285
	model : 0.06925296783447266
			 train-loss:  2.0744303549235723 	 ± 0.2607287886516372
	data : 0.11695575714111328
	model : 0.06907634735107422
			 train-loss:  2.0739241256400454 	 ± 0.26022044016248536
	data : 0.11681532859802246
	model : 0.06989998817443847
			 train-loss:  2.0749767834895123 	 ± 0.26006590820897074
	data : 0.11625127792358399
	model : 0.06995468139648438
			 train-loss:  2.07402489407118 	 ± 0.25983379800028666
	data : 0.11615958213806152
	model : 0.06975626945495605
			 train-loss:  2.0738446850467613 	 ± 0.2592450998499584
	data : 0.1163557529449463
	model : 0.06962785720825196
			 train-loss:  2.074598682091533 	 ± 0.25888434851002196
	data : 0.1166154384613037
	model : 0.0695760726928711
			 train-loss:  2.073949832435048 	 ± 0.2584666862732887
	data : 0.11678032875061035
	model : 0.06947264671325684
			 train-loss:  2.07380640343444 	 ± 0.2578845997592491
	data : 0.1168123722076416
	model : 0.06907091140747071
			 train-loss:  2.073990302736109 	 ± 0.2573122230148651
	data : 0.11713938713073731
	model : 0.06884446144104003
			 train-loss:  2.0743703701916862 	 ± 0.25679129372349474
	data : 0.11716585159301758
	model : 0.06867814064025879
			 train-loss:  2.074806482942255 	 ± 0.2562942966318114
	data : 0.11721248626708984
	model : 0.06796884536743164
			 train-loss:  2.0762152286922984 	 ± 0.2565789943343405
	data : 0.11774740219116211
	model : 0.06779189109802246
			 train-loss:  2.076473098780428 	 ± 0.25603459301412934
	data : 0.11773900985717774
	model : 0.06787543296813965
			 train-loss:  2.07793891482883 	 ± 0.25640525174655215
	data : 0.11786975860595703
	model : 0.06750755310058594
			 train-loss:  2.078286480059666 	 ± 0.25589046946081806
	data : 0.11811280250549316
	model : 0.06749515533447266
			 train-loss:  2.0785449118341117 	 ± 0.2553557682274078
	data : 0.11795272827148437
	model : 0.0675893783569336
			 train-loss:  2.0789045388238474 	 ± 0.25485276712817545
	data : 0.11771163940429688
	model : 0.06742396354675292
			 train-loss:  2.0801855809823917 	 ± 0.25503033640417183
	data : 0.11773419380187988
	model : 0.06732645034790039
			 train-loss:  2.0822934316552204 	 ± 0.2564666502969519
	data : 0.11740517616271973
	model : 0.06716232299804688
			 train-loss:  2.082052012026568 	 ± 0.25593711537703867
	data : 0.11782517433166503
	model : 0.06695313453674316
			 train-loss:  2.081527737194094 	 ± 0.2555092104774373
	data : 0.11811165809631348
	model : 0.06715011596679688
			 train-loss:  2.080503347093967 	 ± 0.2554373087663098
	data : 0.11794571876525879
	model : 0.06713666915893554
			 train-loss:  2.0840022920543313 	 ± 0.26042639969478676
	data : 0.11817498207092285
	model : 0.06707048416137695
			 train-loss:  2.089642901116229 	 ± 0.2738217666749091
	data : 0.11815357208251953
	model : 0.06770920753479004
			 train-loss:  2.0908922674292225 	 ± 0.27391142782543193
	data : 0.11742305755615234
	model : 0.06746773719787598
			 train-loss:  2.0913096651246277 	 ± 0.2734081469270439
	data : 0.11762266159057617
	model : 0.06780157089233399
			 train-loss:  2.090783573499247 	 ± 0.27295333991115084
	data : 0.11768321990966797
	model : 0.0681467056274414
			 train-loss:  2.092517991943838 	 ± 0.27369280020207704
	data : 0.11725101470947266
	model : 0.06853909492492676
			 train-loss:  2.0909889683127405 	 ± 0.27414301839061195
	data : 0.11716413497924805
	model : 0.06883354187011718
			 train-loss:  2.0914421729527075 	 ± 0.27366374483764827
	data : 0.11702108383178711
	model : 0.06964068412780762
			 train-loss:  2.0914089142783614 	 ± 0.2730982266205612
	data : 0.11644515991210938
	model : 0.06970157623291015
			 train-loss:  2.090517585169631 	 ± 0.27288821536947183
	data : 0.11628808975219726
	model : 0.06961989402770996
			 train-loss:  2.090455203271303 	 ± 0.27233018027906863
	data : 0.11638178825378417
	model : 0.06944427490234376
			 train-loss:  2.0885854219903752 	 ± 0.27333873128054764
	data : 0.1165916919708252
	model : 0.06874747276306152
			 train-loss:  2.0881108738542573 	 ± 0.272883710254945
	data : 0.11722064018249512
	model : 0.06845593452453613
			 train-loss:  2.0880795543493047 	 ± 0.2723311968750423
	data : 0.11731233596801757
	model : 0.06823930740356446
			 train-loss:  2.0902623615918623 	 ± 0.2739381302306028
	data : 0.1172245979309082
	model : 0.06802806854248047
			 train-loss:  2.0928807024017395 	 ± 0.2764795490336789
	data : 0.11737947463989258
	model : 0.06804451942443848
			 train-loss:  2.0939048199653625 	 ± 0.2763988651369197
	data : 0.11717715263366699
	model : 0.06847949028015136
			 train-loss:  2.092652350782873 	 ± 0.27655765140941097
	data : 0.11665668487548828
	model : 0.06822452545166016
			 train-loss:  2.0926752852069006 	 ± 0.27600861962188733
	data : 0.11673974990844727
	model : 0.06809773445129394
			 train-loss:  2.09126497351605 	 ± 0.2763708941155271
	data : 0.11696462631225586
	model : 0.06809697151184083
			 train-loss:  2.090616321469855 	 ± 0.2760192178832919
	data : 0.1168900489807129
	model : 0.06819734573364258
			 train-loss:  2.09029396795759 	 ± 0.2755253730974536
	data : 0.11698803901672364
	model : 0.0683816909790039
			 train-loss:  2.0933895870111883 	 ± 0.27939455949008907
	data : 0.1160168170928955
	model : 0.06003327369689941
#epoch  83    val-loss:  2.45662554314262  train-loss:  2.0933895870111883  lr:  2.44140625e-06
			 train-loss:  2.57767653465271 	 ± 0.0
	data : 5.214115381240845
	model : 0.09430551528930664
			 train-loss:  2.2245219945907593 	 ± 0.3531545400619507
	data : 2.8425296545028687
	model : 0.07990550994873047
			 train-loss:  2.1954503854115806 	 ± 0.2912657486982963
	data : 1.9350361029307048
	model : 0.07650295893351237
			 train-loss:  2.162166714668274 	 ± 0.25874738740329506
	data : 1.4801583886146545
	model : 0.07493436336517334
			 train-loss:  2.198388385772705 	 ± 0.24250403341143625
	data : 1.2071494102478026
	model : 0.07386136054992676
			 train-loss:  2.150555749734243 	 ± 0.2458589918414331
	data : 0.18754172325134277
	model : 0.06891064643859864
			 train-loss:  2.130770904677255 	 ± 0.23272311787699684
	data : 0.11669607162475586
	model : 0.06914706230163574
			 train-loss:  2.171380117535591 	 ± 0.24276284843012413
	data : 0.1164748191833496
	model : 0.0692284107208252
			 train-loss:  2.1777087450027466 	 ± 0.2295779011983509
	data : 0.11653714179992676
	model : 0.06925930976867675
			 train-loss:  2.144368362426758 	 ± 0.239665686821446
	data : 0.1165191650390625
	model : 0.06921825408935547
			 train-loss:  2.137085632844405 	 ± 0.22966984424576617
	data : 0.11646723747253418
	model : 0.0685093879699707
			 train-loss:  2.1581885615984597 	 ± 0.23076225976432455
	data : 0.11701674461364746
	model : 0.06903748512268067
			 train-loss:  2.132286557784447 	 ± 0.23917762942490225
	data : 0.11656994819641113
	model : 0.06942811012268066
			 train-loss:  2.1144990580422536 	 ± 0.23923406294037045
	data : 0.11641154289245606
	model : 0.06979179382324219
			 train-loss:  2.1107459545135496 	 ± 0.2315482860044251
	data : 0.11621570587158203
	model : 0.07033023834228516
			 train-loss:  2.097266562283039 	 ± 0.23019362703639762
	data : 0.11592698097229004
	model : 0.07139892578125
			 train-loss:  2.099489485516268 	 ± 0.22349756575926372
	data : 0.11498799324035644
	model : 0.07178697586059571
			 train-loss:  2.107446160581377 	 ± 0.21966416384530785
	data : 0.11475963592529297
	model : 0.07134137153625489
			 train-loss:  2.127119886247735 	 ± 0.22952068849612656
	data : 0.11505270004272461
	model : 0.07004404067993164
			 train-loss:  2.13102565407753 	 ± 0.224355977299055
	data : 0.11616706848144531
	model : 0.0695383071899414
			 train-loss:  2.1196634485608055 	 ± 0.22476802549274794
	data : 0.11651887893676757
	model : 0.06880354881286621
			 train-loss:  2.1185035651380364 	 ± 0.21966457003351716
	data : 0.1172569751739502
	model : 0.06790804862976074
			 train-loss:  2.1076776255731997 	 ± 0.22075554084845292
	data : 0.11791138648986817
	model : 0.06730060577392578
			 train-loss:  2.0926576058069863 	 ± 0.22779659707336158
	data : 0.1184880256652832
	model : 0.06722426414489746
			 train-loss:  2.0966696071624757 	 ± 0.22405790717961738
	data : 0.1185429573059082
	model : 0.06727137565612792
			 train-loss:  2.0926469518588138 	 ± 0.2206255778325491
	data : 0.11845054626464843
	model : 0.06771903038024903
			 train-loss:  2.0842748571325234 	 ± 0.22066996879511527
	data : 0.11798019409179687
	model : 0.06762948036193847
			 train-loss:  2.099695563316345 	 ± 0.23103391618193173
	data : 0.11799130439758301
	model : 0.06728825569152833
			 train-loss:  2.1006586962732774 	 ± 0.22707282771540058
	data : 0.11804590225219727
	model : 0.06820096969604492
			 train-loss:  2.106056292851766 	 ± 0.2251404469799794
	data : 0.11738371849060059
	model : 0.06804637908935547
			 train-loss:  2.0849172684454147 	 ± 0.24991772156651829
	data : 0.11758790016174317
	model : 0.06798491477966309
			 train-loss:  2.0894974172115326 	 ± 0.24730009823346485
	data : 0.11744394302368164
	model : 0.0686408519744873
			 train-loss:  2.1023133957024775 	 ± 0.25408672466624527
	data : 0.11688475608825684
	model : 0.06948351860046387
			 train-loss:  2.1130337364533367 	 ± 0.25778631954000514
	data : 0.1162179946899414
	model : 0.07042756080627441
			 train-loss:  2.115987695966448 	 ± 0.25466013967221074
	data : 0.11512718200683594
	model : 0.06987695693969727
			 train-loss:  2.1094312965869904 	 ± 0.25407650671830495
	data : 0.11579742431640624
	model : 0.0692105770111084
			 train-loss:  2.099470589611981 	 ± 0.257646869997777
	data : 0.11641550064086914
	model : 0.06845426559448242
			 train-loss:  2.1030080506676123 	 ± 0.25514313942938655
	data : 0.11724963188171386
	model : 0.06849884986877441
			 train-loss:  2.095484589919066 	 ± 0.25608540586889933
	data : 0.11726541519165039
	model : 0.06755213737487793
			 train-loss:  2.095359334349632 	 ± 0.252865287437624
	data : 0.1182340145111084
	model : 0.06821022033691407
			 train-loss:  2.095749491598548 	 ± 0.24977471746286684
	data : 0.11775827407836914
	model : 0.06886115074157714
			 train-loss:  2.094132125377655 	 ± 0.2470004975406553
	data : 0.11732316017150879
	model : 0.06966805458068848
			 train-loss:  2.1020871678064035 	 ± 0.24949609118725474
	data : 0.1165438175201416
	model : 0.06976776123046875
			 train-loss:  2.107065951282328 	 ± 0.24879602426179692
	data : 0.11645636558532715
	model : 0.06983699798583984
			 train-loss:  2.10512658490075 	 ± 0.2463522038499496
	data : 0.11637048721313477
	model : 0.07001585960388183
			 train-loss:  2.1014761095461636 	 ± 0.24488720269121128
	data : 0.11593079566955566
	model : 0.07002720832824708
			 train-loss:  2.1014269565014128 	 ± 0.24226824228650098
	data : 0.11586117744445801
	model : 0.0702141284942627
			 train-loss:  2.0990051875511804 	 ± 0.24030556581090454
	data : 0.11566052436828614
	model : 0.06988434791564942
			 train-loss:  2.09535128729684 	 ± 0.23918425310135755
	data : 0.11600151062011718
	model : 0.06980276107788086
			 train-loss:  2.099806938171387 	 ± 0.2388256962809859
	data : 0.11609992980957032
	model : 0.06953792572021485
			 train-loss:  2.0978449793422924 	 ± 0.23687927472245673
	data : 0.11644744873046875
	model : 0.0704761028289795
			 train-loss:  2.0959227360211887 	 ± 0.23499183790470884
	data : 0.11554903984069824
	model : 0.07043728828430176
			 train-loss:  2.099349300816374 	 ± 0.2340722220527608
	data : 0.11562776565551758
	model : 0.06984133720397949
			 train-loss:  2.0960476288089045 	 ± 0.23313715733886506
	data : 0.11616697311401367
	model : 0.06987390518188477
			 train-loss:  2.0996964302929966 	 ± 0.23255889918170475
	data : 0.11626625061035156
	model : 0.07001538276672363
			 train-loss:  2.099633270076343 	 ± 0.23047360300311462
	data : 0.11615233421325684
	model : 0.06838755607604981
			 train-loss:  2.100207146845366 	 ± 0.2284833215925582
	data : 0.11776986122131347
	model : 0.06766142845153808
			 train-loss:  2.107285094672236 	 ± 0.23272322002477133
	data : 0.11824846267700195
	model : 0.06798968315124512
			 train-loss:  2.1049408043845226 	 ± 0.23143223590040893
	data : 0.11779098510742188
	model : 0.06783347129821778
			 train-loss:  2.1017509837945303 	 ± 0.2307997418803051
	data : 0.11782498359680176
	model : 0.06788983345031738
			 train-loss:  2.096934271640465 	 ± 0.23192091188662942
	data : 0.11773276329040527
	model : 0.06869282722473144
			 train-loss:  2.0911340982683244 	 ± 0.23446094515372376
	data : 0.11711139678955078
	model : 0.06954960823059082
			 train-loss:  2.088635696305169 	 ± 0.2334231547546169
	data : 0.1163970947265625
	model : 0.0692148208618164
			 train-loss:  2.0879632998257875 	 ± 0.23165384311475307
	data : 0.11679196357727051
	model : 0.06936435699462891
			 train-loss:  2.08507780111753 	 ± 0.23102117057627464
	data : 0.11657490730285644
	model : 0.06838436126708984
			 train-loss:  2.087176982200507 	 ± 0.22988814569883922
	data : 0.11729989051818848
	model : 0.06740775108337402
			 train-loss:  2.087110410875349 	 ± 0.22816675383836962
	data : 0.11808032989501953
	model : 0.066566801071167
			 train-loss:  2.0826534891829773 	 ± 0.22940221542844194
	data : 0.11894845962524414
	model : 0.06750192642211914
			 train-loss:  2.0852348821750586 	 ± 0.22872650465865338
	data : 0.11809239387512208
	model : 0.06746501922607422
			 train-loss:  2.081473059313638 	 ± 0.2292267123144621
	data : 0.11812257766723633
	model : 0.06929278373718262
			 train-loss:  2.0795823940089053 	 ± 0.22815573606048953
	data : 0.1165761947631836
	model : 0.07018227577209472
			 train-loss:  2.0803243236409292 	 ± 0.226652014908678
	data : 0.1157827377319336
	model : 0.06996102333068847
			 train-loss:  2.0833804623721397 	 ± 0.22658309847883193
	data : 0.11599445343017578
	model : 0.06920595169067383
			 train-loss:  2.0930430357520646 	 ± 0.23971188279348282
	data : 0.11668896675109863
	model : 0.06917533874511719
			 train-loss:  2.0904985284805297 	 ± 0.23911240844540024
	data : 0.11671509742736816
	model : 0.0683366298675537
			 train-loss:  2.0947692911875877 	 ± 0.24039634079698158
	data : 0.11746211051940918
	model : 0.06824498176574707
			 train-loss:  2.095652817131637 	 ± 0.23895439514622963
	data : 0.11747217178344727
	model : 0.06912732124328613
			 train-loss:  2.1016888389220605 	 ± 0.24325407693049772
	data : 0.11678705215454102
	model : 0.06897921562194824
			 train-loss:  2.0988801929015146 	 ± 0.24297907067681074
	data : 0.11706175804138183
	model : 0.06900286674499512
			 train-loss:  2.0935638308525086 	 ± 0.2460359245244126
	data : 0.11715254783630372
	model : 0.06884474754333496
			 train-loss:  2.0900100499023626 	 ± 0.24656985935966633
	data : 0.11728096008300781
	model : 0.06893901824951172
			 train-loss:  2.0891854719417853 	 ± 0.24517411499199135
	data : 0.11732945442199708
	model : 0.0679293155670166
			 train-loss:  2.0865564604839646 	 ± 0.24485278254515144
	data : 0.11805744171142578
	model : 0.06888647079467773
			 train-loss:  2.0837721711113337 	 ± 0.24470921209472285
	data : 0.11723117828369141
	model : 0.07045960426330566
			 train-loss:  2.0828355200150432 	 ± 0.24341690945491817
	data : 0.11549844741821289
	model : 0.07060461044311524
			 train-loss:  2.0806450039841407 	 ± 0.24283879040665576
	data : 0.11541590690612794
	model : 0.07068486213684082
			 train-loss:  2.082745913801522 	 ± 0.24222395202969957
	data : 0.11536188125610351
	model : 0.07157855033874512
			 train-loss:  2.0867153839631514 	 ± 0.2436730246733497
	data : 0.11452503204345703
	model : 0.0705629825592041
			 train-loss:  2.0892764664767833 	 ± 0.24348838868805642
	data : 0.11527457237243652
	model : 0.06895699501037597
			 train-loss:  2.084545502397749 	 ± 0.24621099763445223
	data : 0.11707439422607421
	model : 0.06929731369018555
			 train-loss:  2.087080867735894 	 ± 0.24603298572609988
	data : 0.11673564910888672
	model : 0.06928744316101074
			 train-loss:  2.0904034000375997 	 ± 0.24673637782940544
	data : 0.11664109230041504
	model : 0.06940755844116211
			 train-loss:  2.091912373419731 	 ± 0.24583269315020165
	data : 0.11658358573913574
	model : 0.07044792175292969
			 train-loss:  2.092552642872993 	 ± 0.2445995217969482
	data : 0.11576147079467773
	model : 0.0710054874420166
			 train-loss:  2.094739516157853 	 ± 0.24423082295653767
	data : 0.1153036117553711
	model : 0.07111735343933105
			 train-loss:  2.0911001786589622 	 ± 0.24553127736862548
	data : 0.11529803276062012
	model : 0.07041149139404297
			 train-loss:  2.092783212661743 	 ± 0.24481837550066857
	data : 0.11602296829223632
	model : 0.07016873359680176
			 train-loss:  2.088194946853482 	 ± 0.2477226403299068
	data : 0.11629309654235839
	model : 0.069921875
			 train-loss:  2.0884397342951613 	 ± 0.2464802529150234
	data : 0.11646976470947265
	model : 0.06927013397216797
			 train-loss:  2.0909493803977965 	 ± 0.2465127257542098
	data : 0.11696276664733887
	model : 0.06851344108581543
			 train-loss:  2.0907348193744624 	 ± 0.24529871391427324
	data : 0.11758599281311036
	model : 0.06920270919799805
			 train-loss:  2.0888865064172184 	 ± 0.2447990709181705
	data : 0.1169395923614502
	model : 0.0689168930053711
			 train-loss:  2.0858992650670913 	 ± 0.2454689021577183
	data : 0.1171532154083252
	model : 0.069158935546875
			 train-loss:  2.084731330092137 	 ± 0.2445733152393948
	data : 0.11688451766967774
	model : 0.06929235458374024
			 train-loss:  2.083056754157657 	 ± 0.24400423479202743
	data : 0.11693797111511231
	model : 0.06994681358337403
			 train-loss:  2.082326783324188 	 ± 0.24296571106177023
	data : 0.1162869930267334
	model : 0.06994781494140626
			 train-loss:  2.082276188324545 	 ± 0.24182825313532408
	data : 0.11614794731140136
	model : 0.07028355598449706
			 train-loss:  2.0804302438541695 	 ± 0.24146224930241453
	data : 0.11568269729614258
	model : 0.07066407203674316
			 train-loss:  2.079250278822873 	 ± 0.2406646814122682
	data : 0.11543898582458496
	model : 0.07085700035095215
			 train-loss:  2.082331908832897 	 ± 0.24171896991095163
	data : 0.11509151458740234
	model : 0.06959090232849122
			 train-loss:  2.0823274217210375 	 ± 0.24062768690845737
	data : 0.11632695198059081
	model : 0.06950788497924805
			 train-loss:  2.079943396151066 	 ± 0.24086423880898497
	data : 0.11648068428039551
	model : 0.06892709732055664
			 train-loss:  2.082049463702514 	 ± 0.24082970666124606
	data : 0.11716284751892089
	model : 0.06831183433532714
			 train-loss:  2.082860857771154 	 ± 0.23992619621461986
	data : 0.11758065223693848
	model : 0.0679396152496338
			 train-loss:  2.0855432873186857 	 ± 0.2405915561520143
	data : 0.11796197891235352
	model : 0.06816291809082031
			 train-loss:  2.0851030031154894 	 ± 0.239598804468823
	data : 0.11764793395996094
	model : 0.06828627586364747
			 train-loss:  2.0838695448687954 	 ± 0.23894227042914146
	data : 0.11763195991516114
	model : 0.06877050399780274
			 train-loss:  2.0857692067906006 	 ± 0.23881328659832998
	data : 0.11707758903503418
	model : 0.06885747909545899
			 train-loss:  2.0827826712311817 	 ± 0.24001044918846828
	data : 0.11697778701782227
	model : 0.0691988468170166
			 train-loss:  2.085543535153071 	 ± 0.2408983933192877
	data : 0.11667413711547851
	model : 0.06911115646362305
			 train-loss:  2.08333376813526 	 ± 0.24111905925095198
	data : 0.11671767234802247
	model : 0.06909222602844238
			 train-loss:  2.0817751542466585 	 ± 0.24074010678209554
	data : 0.11672410964965821
	model : 0.06837501525878906
			 train-loss:  2.0815245572144425 	 ± 0.23977546813232078
	data : 0.11763586997985839
	model : 0.06753945350646973
			 train-loss:  2.0810845503883977 	 ± 0.238856528646269
	data : 0.11853613853454589
	model : 0.06668605804443359
			 train-loss:  2.0805754842758177 	 ± 0.23796671224521193
	data : 0.1190920352935791
	model : 0.06785664558410645
			 train-loss:  2.0809575007075356 	 ± 0.23705899771037758
	data : 0.11818351745605468
	model : 0.0679776668548584
			 train-loss:  2.080287834790748 	 ± 0.23624347099711537
	data : 0.11789169311523437
	model : 0.06870889663696289
			 train-loss:  2.0800246531143785 	 ± 0.23533752554552215
	data : 0.11718244552612304
	model : 0.06960029602050781
			 train-loss:  2.0785538909971253 	 ± 0.2350134072806104
	data : 0.1163142204284668
	model : 0.07033486366271972
			 train-loss:  2.0779253005981446 	 ± 0.23421660204871408
	data : 0.11579480171203613
	model : 0.06992788314819336
			 train-loss:  2.077978998650121 	 ± 0.23332173628239178
	data : 0.11612024307250976
	model : 0.06978845596313477
			 train-loss:  2.07931858662403 	 ± 0.23294139751479842
	data : 0.11644906997680664
	model : 0.07007412910461426
			 train-loss:  2.078980069411428 	 ± 0.2320966143756419
	data : 0.11600594520568848
	model : 0.07020707130432129
			 train-loss:  2.0787132782722586 	 ± 0.23124942974410198
	data : 0.11608562469482422
	model : 0.07013592720031739
			 train-loss:  2.078881194856432 	 ± 0.230399557913329
	data : 0.1161839485168457
	model : 0.07002463340759277
			 train-loss:  2.083261714262121 	 ± 0.23512579901885686
	data : 0.11618518829345703
	model : 0.0698939323425293
			 train-loss:  2.0807243428961204 	 ± 0.23612752900656994
	data : 0.11618494987487793
	model : 0.06984152793884277
			 train-loss:  2.0813473661740622 	 ± 0.23538342570048448
	data : 0.11631608009338379
	model : 0.06979799270629883
			 train-loss:  2.0799185809471625 	 ± 0.23513501304604537
	data : 0.1162236213684082
	model : 0.06893515586853027
			 train-loss:  2.0828936653477803 	 ± 0.2369047558950307
	data : 0.1170076847076416
	model : 0.06876296997070312
			 train-loss:  2.080155665147389 	 ± 0.23827578998957988
	data : 0.11710166931152344
	model : 0.06862692832946778
			 train-loss:  2.0788652964041265 	 ± 0.23792918680260525
	data : 0.11731572151184082
	model : 0.06892600059509277
			 train-loss:  2.0781798479440328 	 ± 0.23723646209049673
	data : 0.11729474067687988
	model : 0.06940793991088867
			 train-loss:  2.0835743281576367 	 ± 0.2450543966170787
	data : 0.11673698425292969
	model : 0.07040352821350097
			 train-loss:  2.0824825928129 	 ± 0.24455907166516228
	data : 0.11591939926147461
	model : 0.07043366432189942
			 train-loss:  2.0821439093106413 	 ± 0.24375422116902065
	data : 0.1158592700958252
	model : 0.07047357559204101
			 train-loss:  2.082387924194336 	 ± 0.2429416027759907
	data : 0.11568102836608887
	model : 0.06992936134338379
			 train-loss:  2.081263140246675 	 ± 0.2425032160467223
	data : 0.11592702865600586
	model : 0.06938114166259765
			 train-loss:  2.080552982803959 	 ± 0.24184244126627502
	data : 0.1165656566619873
	model : 0.06866755485534667
			 train-loss:  2.080176362991333 	 ± 0.24107878895702736
	data : 0.1174471378326416
	model : 0.0688089370727539
			 train-loss:  2.078399116629796 	 ± 0.2412630913664264
	data : 0.117486572265625
	model : 0.06887516975402833
			 train-loss:  2.077017038276321 	 ± 0.2410671356873038
	data : 0.11762261390686035
	model : 0.06902599334716797
			 train-loss:  2.0779868161756228 	 ± 0.2405753298138163
	data : 0.11744136810302734
	model : 0.06905360221862793
			 train-loss:  2.0819266299148658 	 ± 0.24469481849382702
	data : 0.11729822158813477
	model : 0.06948208808898926
			 train-loss:  2.08115025489561 	 ± 0.24409441832606052
	data : 0.11658124923706055
	model : 0.0693995475769043
			 train-loss:  2.0807239535527353 	 ± 0.2433686857520375
	data : 0.1166804313659668
	model : 0.0696645736694336
			 train-loss:  2.0792979374053373 	 ± 0.24324534161383346
	data : 0.11622824668884277
	model : 0.0695995807647705
			 train-loss:  2.0796771924706956 	 ± 0.24252091730591907
	data : 0.11643023490905761
	model : 0.06936092376708984
			 train-loss:  2.078739397180905 	 ± 0.2420442843277617
	data : 0.11653523445129395
	model : 0.06932315826416016
			 train-loss:  2.0805064290761948 	 ± 0.2423133064375672
	data : 0.1165696620941162
	model : 0.06858081817626953
			 train-loss:  2.081088448163145 	 ± 0.2416717688181899
	data : 0.1173548698425293
	model : 0.0684241771697998
			 train-loss:  2.081417348649767 	 ± 0.24096085555021712
	data : 0.11749868392944336
	model : 0.0686126708984375
			 train-loss:  2.0820461460417765 	 ± 0.24035385799892897
	data : 0.11716289520263672
	model : 0.06885018348693847
			 train-loss:  2.080727325706947 	 ± 0.24021079261896883
	data : 0.11706852912902832
	model : 0.06902680397033692
			 train-loss:  2.079997447042754 	 ± 0.2396641129729804
	data : 0.11683349609375
	model : 0.06989531517028809
			 train-loss:  2.080256352941674 	 ± 0.23896428599043226
	data : 0.1158869743347168
	model : 0.06995749473571777
			 train-loss:  2.0821425743445663 	 ± 0.2394840083480434
	data : 0.1159907341003418
	model : 0.06895585060119629
			 train-loss:  2.085236369144349 	 ± 0.24209431499850065
	data : 0.11692466735839843
	model : 0.06887669563293457
			 train-loss:  2.0856466589594733 	 ± 0.24143557191804899
	data : 0.11695051193237305
	model : 0.06907238960266113
			 train-loss:  2.0863103838527906 	 ± 0.24087900707565266
	data : 0.11693081855773926
	model : 0.06904473304748535
			 train-loss:  2.0873564516591747 	 ± 0.2405606074842878
	data : 0.11697750091552735
	model : 0.06852908134460449
			 train-loss:  2.0880378096602685 	 ± 0.240025711895934
	data : 0.11753463745117188
	model : 0.0693099021911621
			 train-loss:  2.088179971441368 	 ± 0.23933825261806854
	data : 0.11686849594116211
	model : 0.06928510665893554
			 train-loss:  2.086095983269571 	 ± 0.24021850019501434
	data : 0.11691532135009766
	model : 0.0691762924194336
			 train-loss:  2.0874100637435915 	 ± 0.24015755223354604
	data : 0.11700544357299805
	model : 0.06905531883239746
			 train-loss:  2.085024720565839 	 ± 0.2415443503827534
	data : 0.11719856262207032
	model : 0.06972336769104004
			 train-loss:  2.0838189946729586 	 ± 0.24139161608672735
	data : 0.11655359268188477
	model : 0.0689483642578125
			 train-loss:  2.084918407911665 	 ± 0.24115657742448773
	data : 0.117210054397583
	model : 0.06871795654296875
			 train-loss:  2.0820012978335334 	 ± 0.24361094961211283
	data : 0.11728248596191407
	model : 0.06856193542480468
			 train-loss:  2.0816208832793768 	 ± 0.24298661930239845
	data : 0.11742758750915527
	model : 0.06878428459167481
			 train-loss:  2.0833302881177618 	 ± 0.24339734427579482
	data : 0.11710295677185059
	model : 0.06853785514831542
			 train-loss:  2.0823150283687717 	 ± 0.24311175704735674
	data : 0.11735687255859376
	model : 0.06940393447875977
			 train-loss:  2.0822399300955685 	 ± 0.24244872417166435
	data : 0.11647400856018067
	model : 0.06961798667907715
			 train-loss:  2.081285214294558 	 ± 0.242133684281017
	data : 0.1164710521697998
	model : 0.06968994140625
			 train-loss:  2.0814408128326005 	 ± 0.24148760602622676
	data : 0.1163820743560791
	model : 0.06952576637268067
			 train-loss:  2.08088053810981 	 ± 0.24095810528068382
	data : 0.11666574478149414
	model : 0.0696253776550293
			 train-loss:  2.081080871469834 	 ± 0.24032849959203453
	data : 0.11648793220520019
	model : 0.06973147392272949
			 train-loss:  2.0815618279132435 	 ± 0.23977869403443267
	data : 0.11653532981872558
	model : 0.06906976699829101
			 train-loss:  2.0803978077318304 	 ± 0.23967551246761432
	data : 0.11714448928833007
	model : 0.06937265396118164
			 train-loss:  2.0799792164250426 	 ± 0.23911321359683235
	data : 0.11699838638305664
	model : 0.0693662166595459
			 train-loss:  2.081112442216324 	 ± 0.23899744940851353
	data : 0.11695570945739746
	model : 0.0685917854309082
			 train-loss:  2.0810881418486438 	 ± 0.23837448427279476
	data : 0.11758527755737305
	model : 0.06846017837524414
			 train-loss:  2.0828653036621567 	 ± 0.23902797448957863
	data : 0.11773509979248047
	model : 0.06918249130249024
			 train-loss:  2.0823894419620945 	 ± 0.2385027657583992
	data : 0.11691870689392089
	model : 0.068412446975708
			 train-loss:  2.082529373658009 	 ± 0.23789841813969548
	data : 0.11738023757934571
	model : 0.06780905723571777
			 train-loss:  2.084036496220803 	 ± 0.23822222942304447
	data : 0.11789026260375976
	model : 0.06850337982177734
			 train-loss:  2.0821983614548816 	 ± 0.2390062620750877
	data : 0.11723384857177735
	model : 0.06770968437194824
			 train-loss:  2.082171783904837 	 ± 0.23840223876574132
	data : 0.11804037094116211
	model : 0.0676335334777832
			 train-loss:  2.0815792209538984 	 ± 0.2379486188001474
	data : 0.1180509090423584
	model : 0.06780791282653809
			 train-loss:  2.08331767141819 	 ± 0.23861656953425478
	data : 0.11802754402160645
	model : 0.0678781509399414
			 train-loss:  2.084783712429787 	 ± 0.23892352235476572
	data : 0.11809325218200684
	model : 0.06790771484375
			 train-loss:  2.08507375138821 	 ± 0.23836686408069066
	data : 0.1182797908782959
	model : 0.06870808601379394
			 train-loss:  2.0845091636544963 	 ± 0.23791438773919463
	data : 0.11746444702148437
	model : 0.06872415542602539
			 train-loss:  2.084460410417295 	 ± 0.23733156439657213
	data : 0.11757898330688477
	model : 0.0689608097076416
			 train-loss:  2.0837899178993413 	 ± 0.23694560439754436
	data : 0.11729135513305664
	model : 0.0696993350982666
			 train-loss:  2.083649421779855 	 ± 0.23637835364267898
	data : 0.11654524803161621
	model : 0.0698781967163086
			 train-loss:  2.083550791233634 	 ± 0.2358109493275457
	data : 0.11636600494384766
	model : 0.06988916397094727
			 train-loss:  2.083150745011293 	 ± 0.23531381403272147
	data : 0.11635909080505372
	model : 0.06990141868591308
			 train-loss:  2.082230111058249 	 ± 0.23512538007960437
	data : 0.11630253791809082
	model : 0.06915802955627441
			 train-loss:  2.08377682084129 	 ± 0.23562826763920458
	data : 0.11705069541931153
	model : 0.06819233894348145
			 train-loss:  2.082789560629858 	 ± 0.2355042095073127
	data : 0.11786007881164551
	model : 0.06718196868896484
			 train-loss:  2.082871527604337 	 ± 0.23495113537055107
	data : 0.11878767013549804
	model : 0.06726837158203125
			 train-loss:  2.0837802422438427 	 ± 0.2347720876304406
	data : 0.11875925064086915
	model : 0.0673668384552002
			 train-loss:  2.0824369325816074 	 ± 0.2350419688394612
	data : 0.1185821533203125
	model : 0.06834673881530762
			 train-loss:  2.0831221857736275 	 ± 0.23470889052471283
	data : 0.11764802932739257
	model : 0.06929435729980468
			 train-loss:  2.0850670922685555 	 ± 0.23589509789157476
	data : 0.116721773147583
	model : 0.07004928588867188
			 train-loss:  2.0834651916257796 	 ± 0.23652555093020988
	data : 0.1160233497619629
	model : 0.06913800239562988
			 train-loss:  2.0852524151495837 	 ± 0.23744651036969638
	data : 0.11681985855102539
	model : 0.06843404769897461
			 train-loss:  2.0862222447242913 	 ± 0.23733613849281796
	data : 0.11744585037231445
	model : 0.06747832298278808
			 train-loss:  2.0861059416424146 	 ± 0.23680237866662343
	data : 0.11824836730957031
	model : 0.06732616424560547
			 train-loss:  2.086103915089396 	 ± 0.23626602116143702
	data : 0.11852130889892579
	model : 0.06719937324523925
			 train-loss:  2.086117384670017 	 ± 0.23573337492854704
	data : 0.11857824325561524
	model : 0.06810908317565918
			 train-loss:  2.085299069036817 	 ± 0.23552004248243633
	data : 0.11774673461914062
	model : 0.06865277290344238
			 train-loss:  2.086088768605675 	 ± 0.2352894522299508
	data : 0.1173492431640625
	model : 0.06882767677307129
			 train-loss:  2.0864865922927858 	 ± 0.23484149536651663
	data : 0.11717305183410645
	model : 0.06834597587585449
			 train-loss:  2.0888947116590177 	 ± 0.23708918745847538
	data : 0.11764779090881347
	model : 0.06844568252563477
			 train-loss:  2.0896504644780434 	 ± 0.2368390565305995
	data : 0.11735663414001465
	model : 0.06836123466491699
			 train-loss:  2.088590184324666 	 ± 0.23685841752694273
	data : 0.11746973991394043
	model : 0.06817026138305664
			 train-loss:  2.089671359312066 	 ± 0.2369038657759542
	data : 0.11770119667053222
	model : 0.06847934722900391
			 train-loss:  2.0888684298681177 	 ± 0.23670036326437371
	data : 0.11740574836730958
	model : 0.06901340484619141
			 train-loss:  2.0876933774906834 	 ± 0.236858803164065
	data : 0.11698651313781738
	model : 0.06900362968444824
			 train-loss:  2.08738407389871 	 ± 0.23639452773694947
	data : 0.1168914794921875
	model : 0.06821837425231933
			 train-loss:  2.0865355233777745 	 ± 0.23624051948645702
	data : 0.1175264835357666
	model : 0.06766772270202637
			 train-loss:  2.0869531081273007 	 ± 0.23582135272689744
	data : 0.11806263923645019
	model : 0.06715168952941894
			 train-loss:  2.0858944314591428 	 ± 0.23587566885350317
	data : 0.11829142570495606
	model : 0.06659059524536133
			 train-loss:  2.0858632588790633 	 ± 0.23537588684988622
	data : 0.11834421157836914
	model : 0.06566753387451171
			 train-loss:  2.084828952696756 	 ± 0.2354156228391362
	data : 0.11912384033203124
	model : 0.06574296951293945
			 train-loss:  2.0880756473340907 	 ± 0.2401788486652229
	data : 0.11907014846801758
	model : 0.06569151878356934
			 train-loss:  2.087072163446179 	 ± 0.24017530483713043
	data : 0.11886010169982911
	model : 0.06599998474121094
			 train-loss:  2.0859492674469946 	 ± 0.24030226865857157
	data : 0.11880073547363282
	model : 0.06600289344787598
			 train-loss:  2.0854994345502735 	 ± 0.23990443446817558
	data : 0.11906571388244629
	model : 0.06691541671752929
			 train-loss:  2.0852673866532068 	 ± 0.23943535161478013
	data : 0.11853246688842774
	model : 0.06707134246826171
			 train-loss:  2.0851503836274636 	 ± 0.2389491107285959
	data : 0.1185333251953125
	model : 0.06750731468200684
			 train-loss:  2.0856577749135066 	 ± 0.23859009639786322
	data : 0.11845293045043945
	model : 0.06758155822753906
			 train-loss:  2.0860836783233956 	 ± 0.23819560506275275
	data : 0.11848936080932618
	model : 0.06816773414611817
			 train-loss:  2.0860118958039013 	 ± 0.23771362999227888
	data : 0.11816740036010742
	model : 0.06823320388793945
			 train-loss:  2.0851139283855917 	 ± 0.237649646490297
	data : 0.11803202629089356
	model : 0.06804046630859376
			 train-loss:  2.0842593913116763 	 ± 0.23754997560671892
	data : 0.11798181533813476
	model : 0.06841912269592285
			 train-loss:  2.0852845773160698 	 ± 0.2376215773334684
	data : 0.11757054328918456
	model : 0.06878681182861328
			 train-loss:  2.0856379523277284 	 ± 0.23721140689099462
	data : 0.11707892417907714
	model : 0.06853752136230469
			 train-loss:  2.0861562737430708 	 ± 0.2368802134972638
	data : 0.11701774597167969
	model : 0.06840105056762695
			 train-loss:  2.0846968042472054 	 ± 0.23753780919820794
	data : 0.11708850860595703
	model : 0.06869668960571289
			 train-loss:  2.084851911887821 	 ± 0.2370806886155992
	data : 0.1168487548828125
	model : 0.06846189498901367
			 train-loss:  2.085354567043425 	 ± 0.23674857561372112
	data : 0.11690244674682618
	model : 0.06760129928588868
			 train-loss:  2.0842280313080432 	 ± 0.2369650429797228
	data : 0.1175316333770752
	model : 0.06703042984008789
			 train-loss:  2.0875989804044366 	 ± 0.24255046299716165
	data : 0.11708579063415528
	model : 0.057607507705688475
#epoch  84    val-loss:  2.41139681088297  train-loss:  2.0875989804044366  lr:  2.44140625e-06
			 train-loss:  2.4485480785369873 	 ± 0.0
	data : 5.844696044921875
	model : 0.07458162307739258
			 train-loss:  2.1065900921821594 	 ± 0.3419579863548279
	data : 2.989405393600464
	model : 0.07031691074371338
			 train-loss:  2.0764769315719604 	 ± 0.2824366262339873
	data : 2.0328078269958496
	model : 0.07008544603983562
			 train-loss:  2.196439415216446 	 ± 0.3209374211981023
	data : 1.553676187992096
	model : 0.06996172666549683
			 train-loss:  2.1655522108078005 	 ± 0.2936268724843765
	data : 1.2661008834838867
	model : 0.06993489265441895
			 train-loss:  2.171520253022512 	 ± 0.26837542953267257
	data : 0.12027068138122558
	model : 0.06898312568664551
			 train-loss:  2.136261531284877 	 ± 0.2630496097482919
	data : 0.11671247482299804
	model : 0.06986546516418457
			 train-loss:  2.0872934758663177 	 ± 0.27808416567734195
	data : 0.11597037315368652
	model : 0.06916413307189942
			 train-loss:  2.0718023247188992 	 ± 0.26581628589069284
	data : 0.11649174690246582
	model : 0.06913981437683106
			 train-loss:  2.0789734601974486 	 ± 0.2530914749981036
	data : 0.11650757789611817
	model : 0.06935281753540039
			 train-loss:  2.1080842234871606 	 ± 0.2582759212532238
	data : 0.11631622314453124
	model : 0.06909766197204589
			 train-loss:  2.1186116337776184 	 ± 0.24973319879942815
	data : 0.11637272834777831
	model : 0.06833739280700683
			 train-loss:  2.142943400603074 	 ± 0.2543101510087905
	data : 0.11694188117980957
	model : 0.06816678047180176
			 train-loss:  2.1554064580372403 	 ± 0.24914527424347824
	data : 0.11720747947692871
	model : 0.0682295799255371
			 train-loss:  2.1529256025950114 	 ± 0.24087612621045304
	data : 0.1172370433807373
	model : 0.06801447868347169
			 train-loss:  2.155246749520302 	 ± 0.23340049756320408
	data : 0.1175572395324707
	model : 0.06770792007446289
			 train-loss:  2.1433153993943157 	 ± 0.23140667178717203
	data : 0.11803293228149414
	model : 0.06754932403564454
			 train-loss:  2.187944319513109 	 ± 0.2905747568819593
	data : 0.11819267272949219
	model : 0.06836228370666504
			 train-loss:  2.181217908859253 	 ± 0.2842608199452905
	data : 0.11743512153625488
	model : 0.06835451126098632
			 train-loss:  2.1984243631362914 	 ± 0.2870351587787444
	data : 0.11753363609313965
	model : 0.0683600902557373
			 train-loss:  2.1925234340486073 	 ± 0.28135796995692236
	data : 0.11730370521545411
	model : 0.06873083114624023
			 train-loss:  2.186711300503124 	 ± 0.2761764280094943
	data : 0.11681914329528809
	model : 0.06970553398132324
			 train-loss:  2.1712059508199277 	 ± 0.2797254598281586
	data : 0.11586623191833496
	model : 0.06989774703979493
			 train-loss:  2.1533899704615274 	 ± 0.28685620020708824
	data : 0.1157440185546875
	model : 0.06939263343811035
			 train-loss:  2.134114508628845 	 ± 0.2964996841584755
	data : 0.11620841026306153
	model : 0.06951737403869629
			 train-loss:  2.126086629354037 	 ± 0.29349957524048526
	data : 0.116276216506958
	model : 0.06977086067199707
			 train-loss:  2.1241507353606046 	 ± 0.28818222710920316
	data : 0.1161348819732666
	model : 0.06966891288757324
			 train-loss:  2.1297684737614224 	 ± 0.28449086635145276
	data : 0.11633462905883789
	model : 0.06965122222900391
			 train-loss:  2.133475188551278 	 ± 0.2802300870829043
	data : 0.116481351852417
	model : 0.0701815128326416
			 train-loss:  2.1310598850250244 	 ± 0.27582684548862013
	data : 0.11602602005004883
	model : 0.06954221725463867
			 train-loss:  2.1285782552534536 	 ± 0.27168179044530827
	data : 0.11658978462219238
	model : 0.06940674781799316
			 train-loss:  2.126181922852993 	 ± 0.26773572106815574
	data : 0.11672191619873047
	model : 0.0687283992767334
			 train-loss:  2.1319730209581778 	 ± 0.2656753659146978
	data : 0.11716628074645996
	model : 0.06864285469055176
			 train-loss:  2.1240169055321636 	 ± 0.2656996616436685
	data : 0.11703906059265137
	model : 0.06795077323913574
			 train-loss:  2.118732758930751 	 ± 0.26368281654996284
	data : 0.11751952171325683
	model : 0.06757864952087403
			 train-loss:  2.111449566152361 	 ± 0.26354098043703644
	data : 0.11799688339233398
	model : 0.06767315864562988
			 train-loss:  2.1100063259537154 	 ± 0.26009941033083983
	data : 0.11783394813537598
	model : 0.06782059669494629
			 train-loss:  2.1084219468267342 	 ± 0.25683511206830123
	data : 0.1182124137878418
	model : 0.06695585250854492
			 train-loss:  2.1046196191738815 	 ± 0.2546021908033242
	data : 0.11902084350585937
	model : 0.06711444854736329
			 train-loss:  2.1015665531158447 	 ± 0.2521214904656893
	data : 0.11895480155944824
	model : 0.0679656982421875
			 train-loss:  2.097085688172317 	 ± 0.2506351933144542
	data : 0.11827635765075684
	model : 0.06792216300964356
			 train-loss:  2.0912933406375704 	 ± 0.250395559758075
	data : 0.11844840049743652
	model : 0.06828360557556153
			 train-loss:  2.086364130641139 	 ± 0.2495201865051225
	data : 0.11784071922302246
	model : 0.06828374862670898
			 train-loss:  2.086791282350367 	 ± 0.2466843364320796
	data : 0.11785969734191895
	model : 0.06864452362060547
			 train-loss:  2.089429336123996 	 ± 0.2445548604606623
	data : 0.11763887405395508
	model : 0.06869239807128906
			 train-loss:  2.0895989096683003 	 ± 0.2418847243010501
	data : 0.11743569374084473
	model : 0.06786890029907226
			 train-loss:  2.095777268105365 	 ± 0.2429388427272106
	data : 0.11810503005981446
	model : 0.06874270439147949
			 train-loss:  2.0994551877180734 	 ± 0.24171364525187025
	data : 0.11708364486694336
	model : 0.06967377662658691
			 train-loss:  2.0989719167047616 	 ± 0.23925789407986756
	data : 0.11613426208496094
	model : 0.06888847351074219
			 train-loss:  2.093390870094299 	 ± 0.24005355457497812
	data : 0.11657414436340333
	model : 0.06893572807312012
			 train-loss:  2.0875399556814456 	 ± 0.24126220834065037
	data : 0.11654829978942871
	model : 0.06988687515258789
			 train-loss:  2.091854501229066 	 ± 0.24090965016230856
	data : 0.1155705451965332
	model : 0.06908822059631348
			 train-loss:  2.0878006579740993 	 ± 0.24040998873164465
	data : 0.11639399528503418
	model : 0.06898150444030762
			 train-loss:  2.0817547374301486 	 ± 0.24220645281467867
	data : 0.11662936210632324
	model : 0.06940903663635253
			 train-loss:  2.0868515491485597 	 ± 0.24289942912398726
	data : 0.11631288528442382
	model : 0.06935701370239258
			 train-loss:  2.09132410798754 	 ± 0.24299540512600304
	data : 0.11638307571411133
	model : 0.06921358108520508
			 train-loss:  2.0982878793749893 	 ± 0.24642753293688255
	data : 0.11653103828430175
	model : 0.06888437271118164
			 train-loss:  2.095921921318975 	 ± 0.24494610029076364
	data : 0.11681838035583496
	model : 0.06881022453308105
			 train-loss:  2.0935234457759533 	 ± 0.2435473721571986
	data : 0.11689472198486328
	model : 0.07003765106201172
			 train-loss:  2.0950743158658347 	 ± 0.2418028958898231
	data : 0.11572494506835937
	model : 0.07033982276916503
			 train-loss:  2.0964681164163057 	 ± 0.2400556163819955
	data : 0.11551413536071778
	model : 0.07001228332519531
			 train-loss:  2.1004353107944613 	 ± 0.24011932986626064
	data : 0.11602683067321777
	model : 0.06952204704284667
			 train-loss:  2.1071349287789967 	 ± 0.24397737617610635
	data : 0.11656432151794434
	model : 0.06910905838012696
			 train-loss:  2.111426744610071 	 ± 0.2444490184506626
	data : 0.11694016456604003
	model : 0.0684882640838623
			 train-loss:  2.1212515941032994 	 ± 0.2549780004037819
	data : 0.11747746467590332
	model : 0.06742262840270996
			 train-loss:  2.1184783884973237 	 ± 0.2540248352295039
	data : 0.11826944351196289
	model : 0.06778411865234375
			 train-loss:  2.115839371040686 	 ± 0.2530319238655615
	data : 0.11789841651916504
	model : 0.06864700317382813
			 train-loss:  2.1133954262032226 	 ± 0.25195989790474005
	data : 0.11696653366088867
	model : 0.06831593513488769
			 train-loss:  2.1127885821936787 	 ± 0.2501774907515783
	data : 0.11727967262268066
	model : 0.06816649436950684
			 train-loss:  2.116148715359824 	 ± 0.2499473857484324
	data : 0.11754274368286133
	model : 0.06812906265258789
			 train-loss:  2.120295892299061 	 ± 0.2505947321789055
	data : 0.11760306358337402
	model : 0.06807732582092285
			 train-loss:  2.114215075969696 	 ± 0.25406859010222466
	data : 0.11762733459472656
	model : 0.06815595626831054
			 train-loss:  2.116238296848454 	 ± 0.2529057467009577
	data : 0.11769123077392578
	model : 0.06892070770263672
			 train-loss:  2.1153166326316626 	 ± 0.2513145151725
	data : 0.11695318222045899
	model : 0.06900415420532227
			 train-loss:  2.114420258204142 	 ± 0.24975252506370382
	data : 0.11690921783447265
	model : 0.06903386116027832
			 train-loss:  2.1141911305879293 	 ± 0.24811191050254436
	data : 0.11691880226135254
	model : 0.06946940422058105
			 train-loss:  2.1131234107079444 	 ± 0.24667121397151073
	data : 0.1166417121887207
	model : 0.06945867538452148
			 train-loss:  2.1062129048200755 	 ± 0.25247523465945765
	data : 0.11677494049072265
	model : 0.06968536376953124
			 train-loss:  2.1074517814418936 	 ± 0.25111068649420576
	data : 0.11668224334716797
	model : 0.0695523738861084
			 train-loss:  2.103183203935623 	 ± 0.25240405943829924
	data : 0.11691250801086425
	model : 0.06950798034667968
			 train-loss:  2.099447429916005 	 ± 0.2530568585511236
	data : 0.11701745986938476
	model : 0.06930937767028808
			 train-loss:  2.098774790763855 	 ± 0.2515819411608222
	data : 0.1172558307647705
	model : 0.06936373710632324
			 train-loss:  2.1017869438033507 	 ± 0.25154501100101634
	data : 0.11694750785827637
	model : 0.06830720901489258
			 train-loss:  2.101957068556831 	 ± 0.250048039886071
	data : 0.11764616966247558
	model : 0.06821713447570801
			 train-loss:  2.099853606785045 	 ± 0.24931928894137675
	data : 0.11774868965148926
	model : 0.06878018379211426
			 train-loss:  2.102401086064272 	 ± 0.24897577431200843
	data : 0.1170738697052002
	model : 0.06786723136901855
			 train-loss:  2.09738931984737 	 ± 0.2518661361497252
	data : 0.11761784553527832
	model : 0.06767435073852539
			 train-loss:  2.1041310375387017 	 ± 0.25820514429979896
	data : 0.11798138618469238
	model : 0.06861276626586914
			 train-loss:  2.1030119215504506 	 ± 0.25696499682660684
	data : 0.11722741127014161
	model : 0.06881241798400879
			 train-loss:  2.1011440714200336 	 ± 0.25614027527017763
	data : 0.11692695617675782
	model : 0.06890320777893066
			 train-loss:  2.10041122515123 	 ± 0.25482388244202414
	data : 0.1168405532836914
	model : 0.06968269348144532
			 train-loss:  2.10210058093071 	 ± 0.2539470427858797
	data : 0.11631345748901367
	model : 0.06970648765563965
			 train-loss:  2.106560141809525 	 ± 0.2561744253598036
	data : 0.11616687774658203
	model : 0.06961946487426758
			 train-loss:  2.1047770444383014 	 ± 0.25538770899114843
	data : 0.11601099967956544
	model : 0.06970329284667968
			 train-loss:  2.1067206181977927 	 ± 0.2547379207621746
	data : 0.11590042114257812
	model : 0.0703505039215088
			 train-loss:  2.1039604606727758 	 ± 0.2548317301930602
	data : 0.11533970832824707
	model : 0.07042927742004394
			 train-loss:  2.109044727590895 	 ± 0.2583627532040427
	data : 0.11505017280578614
	model : 0.07065916061401367
			 train-loss:  2.1102248199132023 	 ± 0.25730382884676706
	data : 0.11510510444641113
	model : 0.07047877311706544
			 train-loss:  2.1094165799593685 	 ± 0.25612602174404586
	data : 0.11541824340820313
	model : 0.0702596664428711
			 train-loss:  2.109360319375992 	 ± 0.2548427887627156
	data : 0.11555514335632325
	model : 0.06979179382324219
			 train-loss:  2.107075835218524 	 ± 0.25460501857574686
	data : 0.11617193222045899
	model : 0.06973209381103515
			 train-loss:  2.108765917665818 	 ± 0.2539225927484762
	data : 0.11634740829467774
	model : 0.06972317695617676
			 train-loss:  2.105741058738486 	 ± 0.25452695847307244
	data : 0.1161534309387207
	model : 0.07000164985656739
			 train-loss:  2.10470083126655 	 ± 0.2535202226945797
	data : 0.116156005859375
	model : 0.0699047565460205
			 train-loss:  2.1098457722436814 	 ± 0.2577077951331063
	data : 0.1161583423614502
	model : 0.0698267936706543
			 train-loss:  2.1098971771744064 	 ± 0.2564898526120932
	data : 0.11608643531799316
	model : 0.06915302276611328
			 train-loss:  2.1076325534660127 	 ± 0.25635100054174903
	data : 0.11662240028381347
	model : 0.06892991065979004
			 train-loss:  2.1069516705142126 	 ± 0.2552586157041182
	data : 0.11682777404785157
	model : 0.06895966529846191
			 train-loss:  2.1057485484201974 	 ± 0.25439245450145137
	data : 0.11676435470581055
	model : 0.0693021297454834
			 train-loss:  2.1024428855289115 	 ± 0.2555744223692469
	data : 0.11656303405761718
	model : 0.06930098533630372
			 train-loss:  2.102533912873483 	 ± 0.2544223728478867
	data : 0.11667160987854004
	model : 0.0699723243713379
			 train-loss:  2.1019047424197197 	 ± 0.25337073763934204
	data : 0.11620492935180664
	model : 0.06907167434692382
			 train-loss:  2.103109677281 	 ± 0.25256925238268013
	data : 0.11704134941101074
	model : 0.06895303726196289
			 train-loss:  2.1011991542682313 	 ± 0.2522778551020597
	data : 0.11714591979980468
	model : 0.0688094139099121
			 train-loss:  2.101341481830763 	 ± 0.2511831968725599
	data : 0.11727519035339355
	model : 0.06855192184448242
			 train-loss:  2.1038512566993974 	 ± 0.2515421928227284
	data : 0.11733689308166503
	model : 0.06800451278686523
			 train-loss:  2.1022348566951914 	 ± 0.2510692229671835
	data : 0.11783347129821778
	model : 0.06902537345886231
			 train-loss:  2.101368617203276 	 ± 0.2501786297837165
	data : 0.11688756942749023
	model : 0.06842756271362305
			 train-loss:  2.103407477130409 	 ± 0.2501077887225153
	data : 0.1175374984741211
	model : 0.0684178352355957
			 train-loss:  2.1025083025296527 	 ± 0.2492565680386187
	data : 0.11751036643981934
	model : 0.06779870986938477
			 train-loss:  2.1054192771596356 	 ± 0.2502643163648036
	data : 0.1180915355682373
	model : 0.06805987358093261
			 train-loss:  2.1034911167426187 	 ± 0.25013736874161824
	data : 0.11768527030944824
	model : 0.0677797794342041
			 train-loss:  2.101216061328485 	 ± 0.25038265037239515
	data : 0.11802105903625489
	model : 0.0676114559173584
			 train-loss:  2.0982097512291324 	 ± 0.2515900573838922
	data : 0.11810269355773925
	model : 0.0675086498260498
			 train-loss:  2.098445801734924 	 ± 0.25059546240695185
	data : 0.11847591400146484
	model : 0.06854081153869629
			 train-loss:  2.097069581349691 	 ± 0.25007286156511144
	data : 0.11753473281860352
	model : 0.0688786506652832
			 train-loss:  2.0959939815866666 	 ± 0.24937881722368013
	data : 0.11738166809082032
	model : 0.06816573143005371
			 train-loss:  2.093770948238671 	 ± 0.24966288295084602
	data : 0.11793527603149415
	model : 0.06898322105407714
			 train-loss:  2.090370074723118 	 ± 0.2516521511514247
	data : 0.1170626163482666
	model : 0.06917152404785157
			 train-loss:  2.092065871678866 	 ± 0.25142121896842023
	data : 0.11691622734069824
	model : 0.06861467361450195
			 train-loss:  2.0905454486381005 	 ± 0.25105897594234117
	data : 0.11742362976074219
	model : 0.06765117645263671
			 train-loss:  2.090775348923423 	 ± 0.25012002854037346
	data : 0.11823105812072754
	model : 0.06840519905090332
			 train-loss:  2.090537888663156 	 ± 0.24919288845209445
	data : 0.11766877174377441
	model : 0.06894407272338868
			 train-loss:  2.09186696116604 	 ± 0.2487340350212987
	data : 0.11731896400451661
	model : 0.06893596649169922
			 train-loss:  2.088618612289429 	 ± 0.25064770363884203
	data : 0.11719760894775391
	model : 0.06936821937561036
			 train-loss:  2.0849572043208515 	 ± 0.2533221778486441
	data : 0.11692166328430176
	model : 0.06937074661254883
			 train-loss:  2.086890966352755 	 ± 0.2534014193996184
	data : 0.11687393188476562
	model : 0.06940007209777832
			 train-loss:  2.086458416088768 	 ± 0.2525323853384803
	data : 0.11677389144897461
	model : 0.06805400848388672
			 train-loss:  2.0873927409700355 	 ± 0.2518616263197673
	data : 0.11779866218566895
	model : 0.06799192428588867
			 train-loss:  2.092901746715818 	 ± 0.2592290644962544
	data : 0.11756181716918945
	model : 0.06713590621948243
			 train-loss:  2.0931485333341233 	 ± 0.25832468081001214
	data : 0.11844911575317382
	model : 0.06803407669067382
			 train-loss:  2.090368237293942 	 ± 0.25952193756947134
	data : 0.11763529777526856
	model : 0.06724414825439454
			 train-loss:  2.094431792105828 	 ± 0.26310723565197314
	data : 0.11806550025939941
	model : 0.06749067306518555
			 train-loss:  2.090465765860346 	 ± 0.2664469636991582
	data : 0.1177666187286377
	model : 0.0674325942993164
			 train-loss:  2.09120250570363 	 ± 0.26567373205580713
	data : 0.11801352500915527
	model : 0.06843609809875488
			 train-loss:  2.0913972152422553 	 ± 0.2647727084062059
	data : 0.11682872772216797
	model : 0.06847500801086426
			 train-loss:  2.0936210577179786 	 ± 0.2652352249714956
	data : 0.11682062149047852
	model : 0.06931729316711426
			 train-loss:  2.093192583805806 	 ± 0.26438868414366073
	data : 0.11639113426208496
	model : 0.06987962722778321
			 train-loss:  2.091569800504902 	 ± 0.26423850314531
	data : 0.11602139472961426
	model : 0.07034926414489746
			 train-loss:  2.0888285692532857 	 ± 0.26547343483224156
	data : 0.1156808853149414
	model : 0.0706608772277832
			 train-loss:  2.088682372838456 	 ± 0.2645989818119807
	data : 0.1155250072479248
	model : 0.0704503059387207
			 train-loss:  2.09052590868975 	 ± 0.26469832662066645
	data : 0.11571521759033203
	model : 0.07058548927307129
			 train-loss:  2.090315306887907 	 ± 0.26384465747727226
	data : 0.11556420326232911
	model : 0.06986665725708008
			 train-loss:  2.088727595744195 	 ± 0.2637188858962775
	data : 0.11619243621826172
	model : 0.06949338912963868
			 train-loss:  2.0893360053339314 	 ± 0.26297521031548016
	data : 0.11648678779602051
	model : 0.06904315948486328
			 train-loss:  2.087773506457989 	 ± 0.26285180467500413
	data : 0.11697530746459961
	model : 0.06875448226928711
			 train-loss:  2.087615778491755 	 ± 0.262020765759021
	data : 0.11722278594970703
	model : 0.06880760192871094
			 train-loss:  2.0866723603840116 	 ± 0.2614576314515932
	data : 0.11726846694946289
	model : 0.06958994865417481
			 train-loss:  2.0856687842674977 	 ± 0.26093924086958153
	data : 0.11661314964294434
	model : 0.068790864944458
			 train-loss:  2.0847337186336516 	 ± 0.2603896132140831
	data : 0.1171351432800293
	model : 0.06899003982543946
			 train-loss:  2.084654112039886 	 ± 0.25958164327628286
	data : 0.11688389778137206
	model : 0.06925530433654785
			 train-loss:  2.0856335943127857 	 ± 0.25907749504277333
	data : 0.11674175262451172
	model : 0.06884469985961914
			 train-loss:  2.086738612754213 	 ± 0.25866421264519623
	data : 0.11693239212036133
	model : 0.06788578033447265
			 train-loss:  2.0879451792414594 	 ± 0.25833408633559773
	data : 0.11780104637145997
	model : 0.06869621276855468
			 train-loss:  2.08778476859584 	 ± 0.257558258465604
	data : 0.11741747856140136
	model : 0.06845426559448242
			 train-loss:  2.0863735352654054 	 ± 0.2574203782151542
	data : 0.1174478530883789
	model : 0.06788840293884277
			 train-loss:  2.0842397248673583 	 ± 0.25811678670863597
	data : 0.11793498992919922
	model : 0.06835627555847168
			 train-loss:  2.084566363976115 	 ± 0.2573820511628364
	data : 0.1176790714263916
	model : 0.06923365592956543
			 train-loss:  2.082067369003973 	 ± 0.25865554484699355
	data : 0.11680665016174316
	model : 0.0685844898223877
			 train-loss:  2.0811604275422937 	 ± 0.25816304029691045
	data : 0.11723523139953614
	model : 0.0686882495880127
			 train-loss:  2.079372901665537 	 ± 0.2584600401802266
	data : 0.11735515594482422
	model : 0.06947088241577148
			 train-loss:  2.0810813245385194 	 ± 0.2586741390415498
	data : 0.11679654121398926
	model : 0.06837081909179688
			 train-loss:  2.080229349219041 	 ± 0.2581673529008617
	data : 0.11751623153686523
	model : 0.06846628189086915
			 train-loss:  2.0787722242289575 	 ± 0.2581368819459572
	data : 0.11748347282409669
	model : 0.06842775344848633
			 train-loss:  2.079110152380807 	 ± 0.25743688624411437
	data : 0.1176680088043213
	model : 0.06855149269104004
			 train-loss:  2.0798981257460336 	 ± 0.2569160424211068
	data : 0.11747531890869141
	model : 0.0680048942565918
			 train-loss:  2.0800250250067416 	 ± 0.256194794312376
	data : 0.11778650283813477
	model : 0.06870198249816895
			 train-loss:  2.0803895103797485 	 ± 0.2555201493307212
	data : 0.11746425628662109
	model : 0.06894721984863281
			 train-loss:  2.0818420535359303 	 ± 0.2555412960459338
	data : 0.11722893714904785
	model : 0.06944551467895507
			 train-loss:  2.0861945033073424 	 ± 0.26139914362558103
	data : 0.11665506362915039
	model : 0.06930656433105468
			 train-loss:  2.0865308171477768 	 ± 0.26071509436632356
	data : 0.11680517196655274
	model : 0.06976857185363769
			 train-loss:  2.086976800646101 	 ± 0.26006708183544136
	data : 0.1163210391998291
	model : 0.06973471641540527
			 train-loss:  2.088925455437332 	 ± 0.26068447979850556
	data : 0.1163264274597168
	model : 0.06925578117370605
			 train-loss:  2.0878352205390516 	 ± 0.26039313696446065
	data : 0.11673164367675781
	model : 0.0694338321685791
			 train-loss:  2.089423239553297 	 ± 0.2605802880330799
	data : 0.11665215492248535
	model : 0.06938052177429199
			 train-loss:  2.0880732920862015 	 ± 0.26052669304791704
	data : 0.1167069911956787
	model : 0.07020702362060546
			 train-loss:  2.0864616830081224 	 ± 0.26075714596240845
	data : 0.11582317352294921
	model : 0.07066364288330078
			 train-loss:  2.085858079347205 	 ± 0.26019367499905843
	data : 0.11532402038574219
	model : 0.07100863456726074
			 train-loss:  2.0861504513119895 	 ± 0.25953538099402884
	data : 0.11522951126098632
	model : 0.07149715423583984
			 train-loss:  2.084207294489208 	 ± 0.2602263096066479
	data : 0.11451497077941894
	model : 0.07142405509948731
			 train-loss:  2.083763962640812 	 ± 0.25961612489441716
	data : 0.11473379135131836
	model : 0.07076764106750488
			 train-loss:  2.0824797420452037 	 ± 0.2595466997881063
	data : 0.11566991806030273
	model : 0.07044048309326172
			 train-loss:  2.0837633764187906 	 ± 0.2594837418206334
	data : 0.11604485511779786
	model : 0.06936888694763184
			 train-loss:  2.0830945329567823 	 ± 0.2589808485704505
	data : 0.1171236515045166
	model : 0.06855010986328125
			 train-loss:  2.0844394793877234 	 ± 0.2589943021868363
	data : 0.11794381141662598
	model : 0.06863679885864257
			 train-loss:  2.084074839037292 	 ± 0.25838293544213325
	data : 0.11771979331970214
	model : 0.0685760498046875
			 train-loss:  2.0832509728252586 	 ± 0.2579842734056874
	data : 0.11765942573547364
	model : 0.06888790130615234
			 train-loss:  2.0835317650226632 	 ± 0.25736215108002525
	data : 0.11733579635620117
	model : 0.06978282928466797
			 train-loss:  2.0841400850957363 	 ± 0.2568573666009957
	data : 0.11632637977600098
	model : 0.06937685012817382
			 train-loss:  2.0833016669750215 	 ± 0.25648725950307116
	data : 0.11679201126098633
	model : 0.06868743896484375
			 train-loss:  2.082588151319703 	 ± 0.25604734541601887
	data : 0.11733698844909668
	model : 0.06789717674255372
			 train-loss:  2.0822632861609507 	 ± 0.25545430207250686
	data : 0.11815047264099121
	model : 0.06771955490112305
			 train-loss:  2.081136103921336 	 ± 0.2553274112261249
	data : 0.11826119422912598
	model : 0.06786570549011231
			 train-loss:  2.082001062584858 	 ± 0.2549988098565908
	data : 0.1181140422821045
	model : 0.06766128540039062
			 train-loss:  2.082105007985743 	 ± 0.2543804336324291
	data : 0.11839761734008789
	model : 0.06813807487487793
			 train-loss:  2.0819654887162367 	 ± 0.2537701167260491
	data : 0.11801671981811523
	model : 0.06890635490417481
			 train-loss:  2.0826335615581937 	 ± 0.25333792992882687
	data : 0.11719837188720703
	model : 0.06800026893615722
			 train-loss:  2.0822547679910293 	 ± 0.2527869653293265
	data : 0.11807537078857422
	model : 0.06801447868347169
			 train-loss:  2.0815572162564293 	 ± 0.25238207255149187
	data : 0.11803312301635742
	model : 0.06929268836975097
			 train-loss:  2.081705123469943 	 ± 0.25178952534563254
	data : 0.11702914237976074
	model : 0.06942958831787109
			 train-loss:  2.0825520640865887 	 ± 0.2514918200459724
	data : 0.11689763069152832
	model : 0.06944465637207031
			 train-loss:  2.0818401822504007 	 ± 0.25111098138269816
	data : 0.11699967384338379
	model : 0.06964640617370606
			 train-loss:  2.081450686208519 	 ± 0.2505850073887197
	data : 0.11694045066833496
	model : 0.0695688247680664
			 train-loss:  2.0810278938195417 	 ± 0.25007498053466326
	data : 0.11701598167419433
	model : 0.06939663887023925
			 train-loss:  2.081705229226933 	 ± 0.24968941393636745
	data : 0.11707515716552734
	model : 0.06856288909912109
			 train-loss:  2.081976760868673 	 ± 0.24914257346074986
	data : 0.11800451278686523
	model : 0.06826696395874024
			 train-loss:  2.081652807749911 	 ± 0.24861344288136705
	data : 0.11821470260620118
	model : 0.06857895851135254
			 train-loss:  2.080872205419278 	 ± 0.24830897058140708
	data : 0.11783032417297364
	model : 0.06862463951110839
			 train-loss:  2.0817373201727323 	 ± 0.24807047549969918
	data : 0.11785621643066406
	model : 0.06828904151916504
			 train-loss:  2.081340554085645 	 ± 0.2475756732279876
	data : 0.11804022789001464
	model : 0.06848182678222656
			 train-loss:  2.081961798452144 	 ± 0.24718672022597674
	data : 0.11769709587097169
	model : 0.06786079406738281
			 train-loss:  2.0836252237225437 	 ± 0.24786598611231644
	data : 0.11812772750854492
	model : 0.06739764213562012
			 train-loss:  2.082825022962596 	 ± 0.24759683700508897
	data : 0.11844234466552735
	model : 0.06652193069458008
			 train-loss:  2.08159483649901 	 ± 0.24772564109662523
	data : 0.11919808387756348
	model : 0.06588482856750488
			 train-loss:  2.0834773593478735 	 ± 0.2487751564967286
	data : 0.11981654167175293
	model : 0.06598286628723145
			 train-loss:  2.0862608272417456 	 ± 0.25171107256351843
	data : 0.11968822479248047
	model : 0.06668367385864257
			 train-loss:  2.0845846263322536 	 ± 0.2524169817170594
	data : 0.11906552314758301
	model : 0.0675506591796875
			 train-loss:  2.0846485092974545 	 ± 0.25186466649024836
	data : 0.11831102371215821
	model : 0.06756067276000977
			 train-loss:  2.084695614581545 	 ± 0.2513151484998261
	data : 0.1180948257446289
	model : 0.0682793140411377
			 train-loss:  2.08531110442203 	 ± 0.25094112805561736
	data : 0.11742029190063477
	model : 0.06809759140014648
			 train-loss:  2.086940055801755 	 ± 0.2516130900185475
	data : 0.11776013374328613
	model : 0.06798596382141113
			 train-loss:  2.0868311149292977 	 ± 0.2510756944595356
	data : 0.11768221855163574
	model : 0.06773166656494141
			 train-loss:  2.0865848386748156 	 ± 0.2505644067781856
	data : 0.11776504516601563
	model : 0.06798310279846191
			 train-loss:  2.0880542285421972 	 ± 0.25103245288600756
	data : 0.11763758659362793
	model : 0.0670595645904541
			 train-loss:  2.0885822747616056 	 ± 0.25062797247792185
	data : 0.11827025413513184
	model : 0.06703324317932129
			 train-loss:  2.087477697659347 	 ± 0.25066898341013033
	data : 0.11798586845397949
	model : 0.06652178764343261
			 train-loss:  2.086229709633292 	 ± 0.2508732285610691
	data : 0.11873250007629395
	model : 0.06595206260681152
			 train-loss:  2.08719133128639 	 ± 0.25078295753346724
	data : 0.1193380355834961
	model : 0.06542391777038574
			 train-loss:  2.0865045646244513 	 ± 0.25048193007136293
	data : 0.11968202590942383
	model : 0.06566300392150878
			 train-loss:  2.087568134566148 	 ± 0.2504997562890656
	data : 0.11926970481872559
	model : 0.06577372550964355
			 train-loss:  2.0877928709093467 	 ± 0.250003750907737
	data : 0.11914324760437012
	model : 0.06620383262634277
			 train-loss:  2.0860413623250222 	 ± 0.25096401892425213
	data : 0.11873087882995606
	model : 0.06662850379943848
			 train-loss:  2.0865466481864208 	 ± 0.25057042061283574
	data : 0.11829590797424316
	model : 0.06767473220825196
			 train-loss:  2.0857574231311924 	 ± 0.25035889604830014
	data : 0.11749134063720704
	model : 0.06791863441467286
			 train-loss:  2.0858957694501292 	 ± 0.24985678279930504
	data : 0.11788883209228515
	model : 0.068585205078125
			 train-loss:  2.0866734075352427 	 ± 0.24964533747210055
	data : 0.11753301620483399
	model : 0.06873021125793458
			 train-loss:  2.086231875998771 	 ± 0.24923569816427338
	data : 0.11746158599853515
	model : 0.06824111938476562
			 train-loss:  2.0863599061004576 	 ± 0.2487408378969196
	data : 0.11791386604309081
	model : 0.06781115531921386
			 train-loss:  2.0851721059845154 	 ± 0.24894460836249516
	data : 0.11826376914978028
	model : 0.06835336685180664
			 train-loss:  2.0869952206611635 	 ± 0.2501062522100419
	data : 0.11759428977966309
	model : 0.06780266761779785
			 train-loss:  2.086332263699566 	 ± 0.24982753989693543
	data : 0.1179621696472168
	model : 0.06749868392944336
			 train-loss:  2.086019911936351 	 ± 0.24938046104752717
	data : 0.11815228462219238
	model : 0.06770162582397461
			 train-loss:  2.086705393941977 	 ± 0.24912489400599222
	data : 0.11790990829467773
	model : 0.06719636917114258
			 train-loss:  2.0857615902667908 	 ± 0.249086798318215
	data : 0.11839284896850585
	model : 0.06640934944152832
			 train-loss:  2.08552872059392 	 ± 0.24862561485628923
	data : 0.11878180503845215
	model : 0.06640701293945313
			 train-loss:  2.08540968503803 	 ± 0.24814682330872287
	data : 0.1179135799407959
	model : 0.057921981811523436
#epoch  85    val-loss:  2.501567062578703  train-loss:  2.08540968503803  lr:  2.44140625e-06
			 train-loss:  2.0620951652526855 	 ± 0.0
	data : 5.858411073684692
	model : 0.07238554954528809
			 train-loss:  2.028045356273651 	 ± 0.034049808979034424
	data : 2.997289299964905
	model : 0.07111525535583496
			 train-loss:  2.0687758525212607 	 ± 0.06395993258928237
	data : 2.036762078603109
	model : 0.06966845194498698
			 train-loss:  2.051190733909607 	 ± 0.06321284618158671
	data : 1.5570917129516602
	model : 0.06968510150909424
			 train-loss:  2.168833065032959 	 ± 0.2419825684350986
	data : 1.268923044204712
	model : 0.07010364532470703
			 train-loss:  2.2281453609466553 	 ± 0.25765489533122016
	data : 0.12021012306213379
	model : 0.06924910545349121
			 train-loss:  2.183538249560765 	 ± 0.2623758576516216
	data : 0.11634287834167481
	model : 0.06868886947631836
			 train-loss:  2.1510035544633865 	 ± 0.2600874834342795
	data : 0.11675300598144531
	model : 0.0685570240020752
			 train-loss:  2.119387639893426 	 ± 0.26100931192589377
	data : 0.1169133186340332
	model : 0.0685652732849121
			 train-loss:  2.0879199504852295 	 ± 0.26500040398279034
	data : 0.11692657470703124
	model : 0.06827659606933593
			 train-loss:  2.086178346113725 	 ± 0.25272798736028307
	data : 0.11706743240356446
	model : 0.06872448921203614
			 train-loss:  2.096559246381124 	 ± 0.2444058304464651
	data : 0.1169745922088623
	model : 0.06872978210449218
			 train-loss:  2.110051466868474 	 ± 0.2394237893213109
	data : 0.11707277297973633
	model : 0.06938710212707519
			 train-loss:  2.082703718117305 	 ± 0.2509021501713459
	data : 0.116489839553833
	model : 0.0685415267944336
			 train-loss:  2.071698546409607 	 ± 0.2458672194979515
	data : 0.11728448867797851
	model : 0.06814322471618653
			 train-loss:  2.0517416074872017 	 ± 0.2502932533838351
	data : 0.11778063774108886
	model : 0.06805691719055176
			 train-loss:  2.0540259515537933 	 ± 0.2429919825592169
	data : 0.11772971153259278
	model : 0.06766471862792969
			 train-loss:  2.057935270998213 	 ± 0.2366952202521285
	data : 0.11805810928344726
	model : 0.06780047416687011
			 train-loss:  2.0477138193030107 	 ± 0.2344281724571831
	data : 0.11802363395690918
	model : 0.06864681243896484
			 train-loss:  2.0361148059368133 	 ± 0.2340191119328703
	data : 0.1171882152557373
	model : 0.0683093547821045
			 train-loss:  2.0559369779768444 	 ± 0.24498053050322893
	data : 0.11750626564025879
	model : 0.06868567466735839
			 train-loss:  2.045208692550659 	 ± 0.2443450493220514
	data : 0.11729307174682617
	model : 0.07007012367248536
			 train-loss:  2.0540717166403066 	 ± 0.2425630340311586
	data : 0.11609897613525391
	model : 0.07041215896606445
			 train-loss:  2.048921396334966 	 ± 0.23873706135864725
	data : 0.11578044891357422
	model : 0.07051348686218262
			 train-loss:  2.051097078323364 	 ± 0.23415630527121697
	data : 0.11568951606750488
	model : 0.07132534980773926
			 train-loss:  2.0428870778817396 	 ± 0.23324978787772888
	data : 0.11496853828430176
	model : 0.07099595069885253
			 train-loss:  2.051431077497977 	 ± 0.23299880802736037
	data : 0.11523966789245606
	model : 0.07052192687988282
			 train-loss:  2.0424735248088837 	 ± 0.23348659656950382
	data : 0.11563582420349121
	model : 0.0700638771057129
			 train-loss:  2.0385311430898208 	 ± 0.2303721239386064
	data : 0.11605048179626465
	model : 0.06969141960144043
			 train-loss:  2.0404228568077087 	 ± 0.22672902434692407
	data : 0.11654787063598633
	model : 0.06953606605529786
			 train-loss:  2.034228140308011 	 ± 0.22560812521651674
	data : 0.11645927429199218
	model : 0.06954679489135743
			 train-loss:  2.0317091196775436 	 ± 0.2224975078717374
	data : 0.11650018692016602
	model : 0.06970949172973633
			 train-loss:  2.0360810034202808 	 ± 0.22049175263419388
	data : 0.11631274223327637
	model : 0.06979165077209473
			 train-loss:  2.0393951289794026 	 ± 0.218057711923372
	data : 0.11619501113891602
	model : 0.06951828002929687
			 train-loss:  2.0301886967250278 	 ± 0.22152291881889108
	data : 0.11625399589538574
	model : 0.06949539184570312
			 train-loss:  2.0264742142624326 	 ± 0.21952719544431323
	data : 0.11611967086791992
	model : 0.06922717094421386
			 train-loss:  2.03469946255555 	 ± 0.22209292530801106
	data : 0.11615090370178223
	model : 0.06910638809204102
			 train-loss:  2.042648180534965 	 ± 0.22442141712735217
	data : 0.11650710105895996
	model : 0.06905784606933593
			 train-loss:  2.048758742136833 	 ± 0.2247052369493896
	data : 0.11653017997741699
	model : 0.06969671249389649
			 train-loss:  2.059546563029289 	 ± 0.23188108704542978
	data : 0.11598100662231445
	model : 0.06952080726623536
			 train-loss:  2.0535632226525284 	 ± 0.232140944096157
	data : 0.11639938354492188
	model : 0.06975626945495605
			 train-loss:  2.065418385324024 	 ± 0.24159610242580623
	data : 0.11631731986999512
	model : 0.06972899436950683
			 train-loss:  2.0687931859216024 	 ± 0.2397699216795278
	data : 0.11627483367919922
	model : 0.06959371566772461
			 train-loss:  2.069147066636519 	 ± 0.2370409629841123
	data : 0.11648621559143066
	model : 0.06861352920532227
			 train-loss:  2.0625193225012883 	 ± 0.2384797146101798
	data : 0.11726503372192383
	model : 0.0686112403869629
			 train-loss:  2.055754972540814 	 ± 0.24019836247892234
	data : 0.11730036735534669
	model : 0.06805391311645508
			 train-loss:  2.0477870956380317 	 ± 0.2436967369979513
	data : 0.11773915290832519
	model : 0.06816835403442383
			 train-loss:  2.0530518367886543 	 ± 0.24383102759690767
	data : 0.11779847145080566
	model : 0.06755657196044922
			 train-loss:  2.050524731071628 	 ± 0.2419644033612237
	data : 0.11824073791503906
	model : 0.06827616691589355
			 train-loss:  2.0603741073608397 	 ± 0.24925757259331552
	data : 0.11770138740539551
	model : 0.06843605041503906
			 train-loss:  2.057331227788738 	 ± 0.24773790786999458
	data : 0.11741232872009277
	model : 0.06888670921325683
			 train-loss:  2.061933079591164 	 ± 0.24753551147323263
	data : 0.11696147918701172
	model : 0.06871953010559081
			 train-loss:  2.0569792288654254 	 ± 0.24777778870501257
	data : 0.11695733070373535
	model : 0.06866488456726075
			 train-loss:  2.059232508694684 	 ± 0.24602033464631903
	data : 0.11729803085327148
	model : 0.06853361129760742
			 train-loss:  2.0638463453813034 	 ± 0.24612000456006464
	data : 0.11729345321655274
	model : 0.06842780113220215
			 train-loss:  2.060790777206421 	 ± 0.24496299012807574
	data : 0.11750655174255371
	model : 0.0682973861694336
			 train-loss:  2.064434842059487 	 ± 0.24433123044884322
	data : 0.11755542755126953
	model : 0.06837401390075684
			 train-loss:  2.062307826403914 	 ± 0.24274751856010998
	data : 0.11746792793273926
	model : 0.06897673606872559
			 train-loss:  2.065683158777528 	 ± 0.2420503921112448
	data : 0.11694183349609374
	model : 0.06911587715148926
			 train-loss:  2.06012135942777 	 ± 0.24379705130346632
	data : 0.11684584617614746
	model : 0.06938714981079101
			 train-loss:  2.061005946065559 	 ± 0.24188752499830696
	data : 0.11661434173583984
	model : 0.06956944465637208
			 train-loss:  2.070250386191953 	 ± 0.2505572030486065
	data : 0.11655240058898926
	model : 0.06886806488037109
			 train-loss:  2.0726583287829445 	 ± 0.249282789135582
	data : 0.11720008850097656
	model : 0.06902399063110351
			 train-loss:  2.0753219928592443 	 ± 0.24822959801229114
	data : 0.11699213981628417
	model : 0.06896452903747559
			 train-loss:  2.0760820920650778 	 ± 0.24638778617384863
	data : 0.11699028015136718
	model : 0.06874017715454102
			 train-loss:  2.0721422722845366 	 ± 0.24656861262198265
	data : 0.1171541690826416
	model : 0.06865777969360351
			 train-loss:  2.0659505377954512 	 ± 0.24983786053203438
	data : 0.11723251342773437
	model : 0.06925349235534668
			 train-loss:  2.06512589840328 	 ± 0.248085857601853
	data : 0.11661753654479981
	model : 0.06932291984558106
			 train-loss:  2.0619613640550254 	 ± 0.24766021888203377
	data : 0.11646561622619629
	model : 0.06917672157287598
			 train-loss:  2.0633839232581 	 ± 0.24616863110729276
	data : 0.11679129600524903
	model : 0.06922593116760253
			 train-loss:  2.0666522207394453 	 ± 0.2459536780779963
	data : 0.1166682243347168
	model : 0.06933250427246093
			 train-loss:  2.066296478112539 	 ± 0.2442580879180977
	data : 0.11654696464538575
	model : 0.06952447891235351
			 train-loss:  2.062590969751959 	 ± 0.24460854992456943
	data : 0.11630754470825196
	model : 0.06949343681335449
			 train-loss:  2.064400703520388 	 ± 0.24344171573897538
	data : 0.11649079322814941
	model : 0.0698617935180664
			 train-loss:  2.065620967547099 	 ± 0.24204105709623938
	data : 0.11609735488891601
	model : 0.06990900039672851
			 train-loss:  2.0650997491259324 	 ± 0.24048577510953267
	data : 0.11610941886901856
	model : 0.06997833251953126
			 train-loss:  2.0636505352986325 	 ± 0.23925288207955514
	data : 0.11611981391906738
	model : 0.07001428604125977
			 train-loss:  2.0653428313059683 	 ± 0.23817764001311958
	data : 0.11624064445495605
	model : 0.06991801261901856
			 train-loss:  2.069533793232109 	 ± 0.23954229153289489
	data : 0.1162531852722168
	model : 0.06995396614074707
			 train-loss:  2.067394897341728 	 ± 0.23879838408096724
	data : 0.11613926887512208
	model : 0.06988606452941895
			 train-loss:  2.0691912998387845 	 ± 0.2378630382054406
	data : 0.11622891426086426
	model : 0.06983966827392578
			 train-loss:  2.0682836669247324 	 ± 0.23654929057663746
	data : 0.11626739501953125
	model : 0.0688694953918457
			 train-loss:  2.0671203265707176 	 ± 0.23535585609862877
	data : 0.11704325675964355
	model : 0.06917963027954102
			 train-loss:  2.064146260420481 	 ± 0.23551451480166263
	data : 0.11676688194274902
	model : 0.06914339065551758
			 train-loss:  2.064704190983492 	 ± 0.23418087195242618
	data : 0.1169355869293213
	model : 0.06915488243103027
			 train-loss:  2.0628466023955236 	 ± 0.23344443120244393
	data : 0.11698060035705567
	model : 0.06821422576904297
			 train-loss:  2.0642581040831818 	 ± 0.2324677378939199
	data : 0.11768412590026855
	model : 0.0689467430114746
			 train-loss:  2.064785420894623 	 ± 0.2311954487876346
	data : 0.11695795059204102
	model : 0.06883044242858886
			 train-loss:  2.0664321974422153 	 ± 0.23041137897268
	data : 0.1171494960784912
	model : 0.068572998046875
			 train-loss:  2.0671815792719523 	 ± 0.2292367800039797
	data : 0.11732654571533203
	model : 0.06854124069213867
			 train-loss:  2.0661040125312384 	 ± 0.22820284346723227
	data : 0.11734867095947266
	model : 0.06926054954528808
			 train-loss:  2.0706980863343114 	 ± 0.2311516606620341
	data : 0.11681156158447266
	model : 0.06841301918029785
			 train-loss:  2.0725672206571026 	 ± 0.2306035112084059
	data : 0.11741361618041993
	model : 0.06816940307617188
			 train-loss:  2.0723391809362046 	 ± 0.22938415898523776
	data : 0.11752228736877442
	model : 0.0684114933013916
			 train-loss:  2.0731882961172805 	 ± 0.2283221450986245
	data : 0.11733775138854981
	model : 0.0684743881225586
			 train-loss:  2.0705975579718747 	 ± 0.22852921796855513
	data : 0.11719532012939453
	model : 0.06851391792297364
			 train-loss:  2.0699241173636054 	 ± 0.22744391246733103
	data : 0.1172976016998291
	model : 0.06942887306213379
			 train-loss:  2.0725025242688706 	 ± 0.22770099332930685
	data : 0.11669273376464843
	model : 0.0696803092956543
			 train-loss:  2.072421260554381 	 ± 0.22654949782635792
	data : 0.11636476516723633
	model : 0.06979026794433593
			 train-loss:  2.070878310203552 	 ± 0.22593609073905913
	data : 0.11635432243347169
	model : 0.06992459297180176
			 train-loss:  2.0740798109828837 	 ± 0.2270829381998558
	data : 0.11626458168029785
	model : 0.07015113830566407
			 train-loss:  2.0734714877371694 	 ± 0.22604973146971138
	data : 0.11584644317626953
	model : 0.07015471458435059
			 train-loss:  2.072885832740265 	 ± 0.2250274749900351
	data : 0.11593356132507324
	model : 0.07014346122741699
			 train-loss:  2.070295606668179 	 ± 0.22548064592903608
	data : 0.11613411903381347
	model : 0.07003216743469239
			 train-loss:  2.074105962117513 	 ± 0.22774387548239547
	data : 0.11621766090393067
	model : 0.06990203857421876
			 train-loss:  2.073727803410224 	 ± 0.22670018603001701
	data : 0.11616911888122558
	model : 0.06985673904418946
			 train-loss:  2.0722448458181364 	 ± 0.2261543222578305
	data : 0.1161895751953125
	model : 0.06901211738586426
			 train-loss:  2.070421874523163 	 ± 0.22589331656084635
	data : 0.11689410209655762
	model : 0.06805391311645508
			 train-loss:  2.0688138336216637 	 ± 0.2254748578414162
	data : 0.11757326126098633
	model : 0.06829233169555664
			 train-loss:  2.0689214663072066 	 ± 0.2244504451730161
	data : 0.11736550331115722
	model : 0.0683516025543213
			 train-loss:  2.068081842886435 	 ± 0.22361058312432533
	data : 0.11747102737426758
	model : 0.06823587417602539
			 train-loss:  2.0681494580847875 	 ± 0.2226112231451868
	data : 0.11759400367736816
	model : 0.06825113296508789
			 train-loss:  2.070157190339755 	 ± 0.2226402499492089
	data : 0.11761932373046875
	model : 0.06958947181701661
			 train-loss:  2.071004058185377 	 ± 0.22184433693208355
	data : 0.11652445793151855
	model : 0.06916513442993164
			 train-loss:  2.0700217982997065 	 ± 0.2211265366758485
	data : 0.11687746047973632
	model : 0.06928772926330566
			 train-loss:  2.0704157013317634 	 ± 0.2202118597765676
	data : 0.1169558048248291
	model : 0.06850299835205079
			 train-loss:  2.070522859565213 	 ± 0.2192718013475991
	data : 0.11762452125549316
	model : 0.06920051574707031
			 train-loss:  2.072289629507873 	 ± 0.2191754477431417
	data : 0.11693296432495118
	model : 0.06867442131042481
			 train-loss:  2.076154541568596 	 ± 0.22225397092881638
	data : 0.11737136840820313
	model : 0.06802568435668946
			 train-loss:  2.0809439410765966 	 ± 0.22740899439930154
	data : 0.11802172660827637
	model : 0.06787939071655273
			 train-loss:  2.087374355182175 	 ± 0.23716973318996418
	data : 0.11811280250549316
	model : 0.06829133033752441
			 train-loss:  2.088953333799956 	 ± 0.23683347525477966
	data : 0.11762847900390624
	model : 0.06875038146972656
			 train-loss:  2.088120871443089 	 ± 0.23604792561034935
	data : 0.11707606315612792
	model : 0.06899099349975586
			 train-loss:  2.090455174446106 	 ± 0.23651533316532197
	data : 0.11676650047302246
	model : 0.06996536254882812
			 train-loss:  2.0890784587860107 	 ± 0.23606568938498879
	data : 0.11568942070007324
	model : 0.06906538009643555
			 train-loss:  2.090761858319479 	 ± 0.23587912391449609
	data : 0.11655240058898926
	model : 0.07012553215026855
			 train-loss:  2.0905357852695494 	 ± 0.234962334810895
	data : 0.11556997299194335
	model : 0.06985015869140625
			 train-loss:  2.089872732758522 	 ± 0.23416196502502767
	data : 0.1160057544708252
	model : 0.06966195106506348
			 train-loss:  2.0897672712340833 	 ± 0.23325564641342864
	data : 0.11598811149597169
	model : 0.06954822540283204
			 train-loss:  2.0878610510092517 	 ± 0.2333632707604557
	data : 0.11611695289611816
	model : 0.06952672004699707
			 train-loss:  2.086168670472298 	 ± 0.23327032126773858
	data : 0.11598219871520996
	model : 0.06949167251586914
			 train-loss:  2.0838578460794506 	 ± 0.23388530466836724
	data : 0.11644654273986817
	model : 0.06986794471740723
			 train-loss:  2.081459177167792 	 ± 0.23462846553365327
	data : 0.11612973213195801
	model : 0.07022514343261718
			 train-loss:  2.078812318061715 	 ± 0.2357360216048806
	data : 0.11616921424865723
	model : 0.07041220664978028
			 train-loss:  2.079972270683006 	 ± 0.23524482357927579
	data : 0.11617317199707031
	model : 0.0718104362487793
			 train-loss:  2.0765471624977447 	 ± 0.23773293575578083
	data : 0.11503229141235352
	model : 0.07047615051269532
			 train-loss:  2.0775188747113638 	 ± 0.23713462593985346
	data : 0.11601414680480956
	model : 0.06934704780578613
			 train-loss:  2.0804083943367004 	 ± 0.23868222363734096
	data : 0.1168665885925293
	model : 0.0682497501373291
			 train-loss:  2.0800021955435226 	 ± 0.23786997108138372
	data : 0.11770462989807129
	model : 0.06751399040222168
			 train-loss:  2.080329088653837 	 ± 0.23705024474884648
	data : 0.1183438777923584
	model : 0.06648392677307129
			 train-loss:  2.0801162254725787 	 ± 0.23622157270310368
	data : 0.1190873146057129
	model : 0.06734108924865723
			 train-loss:  2.0787133297450104 	 ± 0.23597706166672158
	data : 0.11854925155639648
	model : 0.06809673309326172
			 train-loss:  2.07860234900788 	 ± 0.2351542383182755
	data : 0.11784701347351074
	model : 0.06882195472717285
			 train-loss:  2.080102057920562 	 ± 0.2350215530718901
	data : 0.11711606979370118
	model : 0.069378662109375
			 train-loss:  2.082085680139476 	 ± 0.23541623456748667
	data : 0.11650047302246094
	model : 0.06977133750915528
			 train-loss:  2.080253879501395 	 ± 0.2356432802398365
	data : 0.11618566513061523
	model : 0.06893205642700195
			 train-loss:  2.084422388044344 	 ± 0.24018113863438875
	data : 0.11673135757446289
	model : 0.06810984611511231
			 train-loss:  2.0862343641551764 	 ± 0.2403743773987323
	data : 0.1175356388092041
	model : 0.06808958053588868
			 train-loss:  2.084176140343583 	 ± 0.24087139173890704
	data : 0.11759414672851562
	model : 0.06723003387451172
			 train-loss:  2.0834264659881594 	 ± 0.2402414901234885
	data : 0.11846690177917481
	model : 0.06775174140930176
			 train-loss:  2.082357857401008 	 ± 0.23980207929261108
	data : 0.11804327964782715
	model : 0.06906323432922364
			 train-loss:  2.084661964523165 	 ± 0.24068311111058643
	data : 0.11715764999389648
	model : 0.06960558891296387
			 train-loss:  2.0840222290138795 	 ± 0.24002489661144602
	data : 0.11660332679748535
	model : 0.0700836181640625
			 train-loss:  2.0843217465784645 	 ± 0.23927300968453435
	data : 0.11627283096313476
	model : 0.07129430770874023
			 train-loss:  2.084395767027332 	 ± 0.238501681221493
	data : 0.11532888412475586
	model : 0.07108101844787598
			 train-loss:  2.0842799238669567 	 ± 0.23774039842014674
	data : 0.11575870513916016
	model : 0.0705033779144287
			 train-loss:  2.081309869790533 	 ± 0.2398678894411206
	data : 0.11601376533508301
	model : 0.06997809410095215
			 train-loss:  2.078765604314925 	 ± 0.2412234538559851
	data : 0.11654744148254395
	model : 0.06943936347961426
			 train-loss:  2.0762917508119307 	 ± 0.24246595629480838
	data : 0.11712789535522461
	model : 0.06915097236633301
			 train-loss:  2.0772729314863683 	 ± 0.24202350268975975
	data : 0.11733670234680176
	model : 0.0682039737701416
			 train-loss:  2.077608889674548 	 ± 0.24130812773012494
	data : 0.11786079406738281
	model : 0.06829652786254883
			 train-loss:  2.0774666770004933 	 ± 0.24056896431519698
	data : 0.11779217720031739
	model : 0.06893811225891114
			 train-loss:  2.078033728833579 	 ± 0.23993846218435413
	data : 0.11750979423522949
	model : 0.06901416778564454
			 train-loss:  2.0776033147079187 	 ± 0.23926893444589964
	data : 0.11736946105957032
	model : 0.06895327568054199
			 train-loss:  2.0795015385656646 	 ± 0.23977820835998664
	data : 0.11731457710266113
	model : 0.06951961517333985
			 train-loss:  2.0775523810501557 	 ± 0.24036246121171076
	data : 0.11668815612792968
	model : 0.06927156448364258
			 train-loss:  2.0804392927421067 	 ± 0.24251112758423493
	data : 0.11689243316650391
	model : 0.06950244903564454
			 train-loss:  2.081590221751304 	 ± 0.24224531399254454
	data : 0.11657752990722656
	model : 0.06928629875183105
			 train-loss:  2.0843764755147447 	 ± 0.24421256310966105
	data : 0.11643919944763184
	model : 0.06937112808227539
			 train-loss:  2.083122284973369 	 ± 0.24403850052895065
	data : 0.11623783111572265
	model : 0.06909303665161133
			 train-loss:  2.0812277089782625 	 ± 0.2445745637028007
	data : 0.11683053970336914
	model : 0.0692368984222412
			 train-loss:  2.082866849594338 	 ± 0.2448027477659549
	data : 0.116886568069458
	model : 0.06909370422363281
			 train-loss:  2.083783326810495 	 ± 0.24438994763722893
	data : 0.11708507537841797
	model : 0.06876144409179688
			 train-loss:  2.0851185191636796 	 ± 0.24431865431787572
	data : 0.11763210296630859
	model : 0.0685211181640625
			 train-loss:  2.083092793055943 	 ± 0.24508066119690916
	data : 0.11781497001647949
	model : 0.06918339729309082
			 train-loss:  2.0849941846999256 	 ± 0.24567444013051987
	data : 0.11708393096923828
	model : 0.06934819221496583
			 train-loss:  2.084113683404222 	 ± 0.24525779559723226
	data : 0.11671051979064942
	model : 0.06875166893005372
			 train-loss:  2.085061402133342 	 ± 0.24489269771562366
	data : 0.11711673736572266
	model : 0.0692720890045166
			 train-loss:  2.0843324914324883 	 ± 0.24440123811314926
	data : 0.1166839599609375
	model : 0.06922793388366699
			 train-loss:  2.081963215933906 	 ± 0.24577414718326812
	data : 0.11676645278930664
	model : 0.06899085044860839
			 train-loss:  2.081368753264622 	 ± 0.2452240036711343
	data : 0.1167989730834961
	model : 0.06874232292175293
			 train-loss:  2.079647335377368 	 ± 0.2456435517551549
	data : 0.11714687347412109
	model : 0.06927957534790039
			 train-loss:  2.0804877776265798 	 ± 0.24523372133047353
	data : 0.11671013832092285
	model : 0.06894097328186036
			 train-loss:  2.0819420762684033 	 ± 0.24535642383135337
	data : 0.11715631484985352
	model : 0.06915888786315919
			 train-loss:  2.0813589682450164 	 ± 0.24482020603205323
	data : 0.1170769214630127
	model : 0.06931757926940918
			 train-loss:  2.080996589635008 	 ± 0.24421094484551215
	data : 0.11692347526550292
	model : 0.06939620971679687
			 train-loss:  2.0836339500498644 	 ± 0.2461987309835451
	data : 0.11665353775024415
	model : 0.06933889389038086
			 train-loss:  2.0810943968752595 	 ± 0.2479867456108946
	data : 0.1166572093963623
	model : 0.068963623046875
			 train-loss:  2.0818172598642017 	 ± 0.2475283389870488
	data : 0.11684455871582031
	model : 0.06898784637451172
			 train-loss:  2.0857230136269016 	 ± 0.2526479406310648
	data : 0.11690635681152343
	model : 0.06888694763183593
			 train-loss:  2.0857168954080314 	 ± 0.2519857047428932
	data : 0.11706676483154296
	model : 0.06896276473999023
			 train-loss:  2.0848365053534508 	 ± 0.2516229808037515
	data : 0.11710667610168457
	model : 0.06926722526550293
			 train-loss:  2.083987679506213 	 ± 0.25124571445382915
	data : 0.11687002182006836
	model : 0.07021384239196778
			 train-loss:  2.084619362329699 	 ± 0.250750946070554
	data : 0.11596059799194336
	model : 0.07017455101013184
			 train-loss:  2.0863613434326953 	 ± 0.25128129334230703
	data : 0.1159848690032959
	model : 0.07021374702453613
			 train-loss:  2.0864592814932066 	 ± 0.25064318116524353
	data : 0.11603984832763672
	model : 0.07010655403137207
			 train-loss:  2.085405663185313 	 ± 0.25044099645138085
	data : 0.116123628616333
	model : 0.06978888511657715
			 train-loss:  2.0839673094075137 	 ± 0.25062220025676474
	data : 0.11641373634338378
	model : 0.06943292617797851
			 train-loss:  2.0850937228706017 	 ± 0.25049366357364766
	data : 0.11678194999694824
	model : 0.069451904296875
			 train-loss:  2.0848082369565963 	 ± 0.2498990977420827
	data : 0.11668605804443359
	model : 0.06868009567260742
			 train-loss:  2.083965597461112 	 ± 0.24956136117703334
	data : 0.1175581932067871
	model : 0.06882572174072266
			 train-loss:  2.0837425365306363 	 ± 0.24896295473629193
	data : 0.11740260124206543
	model : 0.06881504058837891
			 train-loss:  2.0834258418952305 	 ± 0.24838977383914476
	data : 0.11729922294616699
	model : 0.06813058853149415
			 train-loss:  2.083400611199585 	 ± 0.24778048823305487
	data : 0.11803722381591797
	model : 0.06809635162353515
			 train-loss:  2.0845704770669706 	 ± 0.24773952709910418
	data : 0.11792473793029785
	model : 0.06894097328186036
			 train-loss:  2.0823996900354773 	 ± 0.2490842465072697
	data : 0.11712226867675782
	model : 0.06892485618591308
			 train-loss:  2.08179877979168 	 ± 0.248631499588488
	data : 0.11728053092956543
	model : 0.06896591186523438
			 train-loss:  2.082045603830081 	 ± 0.24805852808350293
	data : 0.1172821044921875
	model : 0.06965017318725586
			 train-loss:  2.083315361629833 	 ± 0.248141032842988
	data : 0.11657524108886719
	model : 0.06973896026611329
			 train-loss:  2.0822807249568758 	 ± 0.2480009912388668
	data : 0.1166539192199707
	model : 0.06981940269470215
			 train-loss:  2.083104690104299 	 ± 0.24770057341911117
	data : 0.11666359901428222
	model : 0.06971602439880371
			 train-loss:  2.0835300352213517 	 ± 0.24719291005970445
	data : 0.1166806697845459
	model : 0.0697969913482666
			 train-loss:  2.0834885813260864 	 ± 0.24661270096810786
	data : 0.11664438247680664
	model : 0.07008299827575684
			 train-loss:  2.0816199896491576 	 ± 0.24754261665895785
	data : 0.11639165878295898
	model : 0.07008199691772461
			 train-loss:  2.080541526439578 	 ± 0.24746966659613015
	data : 0.11649794578552246
	model : 0.07054324150085449
			 train-loss:  2.081059357634297 	 ± 0.24701288167103505
	data : 0.11603741645812989
	model : 0.07076215744018555
			 train-loss:  2.0803707101927373 	 ± 0.24665080933308417
	data : 0.11582574844360352
	model : 0.07091717720031739
			 train-loss:  2.080386825110934 	 ± 0.24608456072110643
	data : 0.11572203636169434
	model : 0.07083644866943359
			 train-loss:  2.080758723494125 	 ± 0.24558347559702057
	data : 0.11582374572753906
	model : 0.06994161605834961
			 train-loss:  2.081969738548452 	 ± 0.2456792167738277
	data : 0.11639537811279296
	model : 0.06849365234375
			 train-loss:  2.082130091762111 	 ± 0.24513428987029753
	data : 0.11755037307739258
	model : 0.0682858943939209
			 train-loss:  2.0829728310172624 	 ± 0.2449022192415976
	data : 0.11780872344970703
	model : 0.06808485984802246
			 train-loss:  2.0827301676497867 	 ± 0.2443792421233432
	data : 0.11771769523620605
	model : 0.06723766326904297
			 train-loss:  2.0814941711723804 	 ± 0.24453072544062263
	data : 0.1183441162109375
	model : 0.06721968650817871
			 train-loss:  2.0794027487436932 	 ± 0.24598639313048526
	data : 0.11845870018005371
	model : 0.06727266311645508
			 train-loss:  2.0805784411134973 	 ± 0.24607432238746507
	data : 0.11822743415832519
	model : 0.06718745231628417
			 train-loss:  2.079893631557011 	 ± 0.2457474447729092
	data : 0.11815037727355956
	model : 0.06685867309570312
			 train-loss:  2.0799645326639475 	 ± 0.24521025955111214
	data : 0.11875014305114746
	model : 0.0676508903503418
			 train-loss:  2.078516456758091 	 ± 0.2456493476987702
	data : 0.11815977096557617
	model : 0.06832032203674317
			 train-loss:  2.079769992310068 	 ± 0.24584767235791113
	data : 0.11729598045349121
	model : 0.06897978782653809
			 train-loss:  2.0785552836083747 	 ± 0.24600568545943732
	data : 0.11683616638183594
	model : 0.0690239429473877
			 train-loss:  2.0794526215257316 	 ± 0.24585350340519466
	data : 0.11683030128479004
	model : 0.06951694488525391
			 train-loss:  2.0802267876817435 	 ± 0.24560857960191623
	data : 0.11632518768310547
	model : 0.0692223072052002
			 train-loss:  2.0806991106424575 	 ± 0.24518923519059307
	data : 0.1164362907409668
	model : 0.06911630630493164
			 train-loss:  2.0819117556227016 	 ± 0.24536919046613076
	data : 0.11667380332946778
	model : 0.06836047172546386
			 train-loss:  2.081221681530193 	 ± 0.24507720583946546
	data : 0.11715779304504395
	model : 0.06815757751464843
			 train-loss:  2.0804544863318593 	 ± 0.24484344796281762
	data : 0.11725177764892578
	model : 0.0678792953491211
			 train-loss:  2.0824764205628084 	 ± 0.24630334551551242
	data : 0.11740732192993164
	model : 0.06791863441467286
			 train-loss:  2.082527840985414 	 ± 0.24578880656413388
	data : 0.11726140975952148
	model : 0.06794257164001465
			 train-loss:  2.0834427893161775 	 ± 0.24568372833784577
	data : 0.11718177795410156
	model : 0.06853299140930176
			 train-loss:  2.0818681078827725 	 ± 0.24638414128172215
	data : 0.11682372093200684
	model : 0.06852316856384277
			 train-loss:  2.0827917436922876 	 ± 0.24629229630133914
	data : 0.11677179336547852
	model : 0.06827793121337891
			 train-loss:  2.0816574847256697 	 ± 0.24641755116584688
	data : 0.11688485145568847
	model : 0.06845989227294921
			 train-loss:  2.083992128000885 	 ± 0.2485904993321782
	data : 0.11685900688171387
	model : 0.06857037544250488
			 train-loss:  2.0854236928784116 	 ± 0.24908844006112335
	data : 0.11689600944519044
	model : 0.06877255439758301
			 train-loss:  2.0860314722952804 	 ± 0.24876361730269203
	data : 0.11671328544616699
	model : 0.06901540756225585
			 train-loss:  2.0857036369532227 	 ± 0.24831277970585902
	data : 0.11677641868591308
	model : 0.06951837539672852
			 train-loss:  2.0861821765861204 	 ± 0.2479257422210954
	data : 0.11641073226928711
	model : 0.06949687004089355
			 train-loss:  2.085004916152801 	 ± 0.24812100109521218
	data : 0.11640534400939942
	model : 0.0695946216583252
			 train-loss:  2.085220426082611 	 ± 0.2476476120371125
	data : 0.11655879020690918
	model : 0.06932892799377441
			 train-loss:  2.0863823031049327 	 ± 0.24783560943181487
	data : 0.11663131713867188
	model : 0.06919260025024414
			 train-loss:  2.0856000927705614 	 ± 0.24765363791055264
	data : 0.116572904586792
	model : 0.06895747184753417
			 train-loss:  2.0855060259815263 	 ± 0.24716823006451372
	data : 0.11685385704040527
	model : 0.06885862350463867
			 train-loss:  2.087359165112803 	 ± 0.24843600237223548
	data : 0.11705513000488281
	model : 0.06851091384887695
			 train-loss:  2.0872204841351976 	 ± 0.2479582451503364
	data : 0.11688919067382812
	model : 0.06852159500122071
			 train-loss:  2.0877082222141325 	 ± 0.24759600922031055
	data : 0.11596717834472656
	model : 0.05929865837097168
#epoch  86    val-loss:  2.4556517601013184  train-loss:  2.0877082222141325  lr:  2.44140625e-06
			 train-loss:  2.019388437271118 	 ± 0.0
	data : 5.800735235214233
	model : 0.0793919563293457
			 train-loss:  2.0990335941314697 	 ± 0.07964515686035156
	data : 2.9667277336120605
	model : 0.07443034648895264
			 train-loss:  2.2021713256835938 	 ± 0.15969872890427986
	data : 2.0165132681528726
	model : 0.07175660133361816
			 train-loss:  2.0955967009067535 	 ± 0.23065605335478376
	data : 1.542031466960907
	model : 0.07066357135772705
			 train-loss:  2.0989245653152464 	 ± 0.2064123801651663
	data : 1.2573013305664062
	model : 0.06975827217102051
			 train-loss:  2.158321718374888 	 ± 0.23053235751731982
	data : 0.12107386589050292
	model : 0.06707072257995605
			 train-loss:  2.184491923877171 	 ± 0.22285035104568413
	data : 0.11847524642944336
	model : 0.06699371337890625
			 train-loss:  2.1956733018159866 	 ± 0.2105460913418033
	data : 0.11842412948608398
	model : 0.06752595901489258
			 train-loss:  2.12771299150255 	 ± 0.27632041076402347
	data : 0.11803579330444336
	model : 0.06715521812438965
			 train-loss:  2.1257526159286497 	 ± 0.26220652187891036
	data : 0.11836466789245606
	model : 0.06876163482666016
			 train-loss:  2.0891796458851206 	 ± 0.27545939088876303
	data : 0.11675496101379394
	model : 0.06972231864929199
			 train-loss:  2.119182765483856 	 ± 0.2818807881628164
	data : 0.11601572036743164
	model : 0.06984410285949708
			 train-loss:  2.1436033799098086 	 ± 0.283727185408569
	data : 0.11607122421264648
	model : 0.06983017921447754
			 train-loss:  2.1220014691352844 	 ± 0.2842839938909686
	data : 0.11601076126098633
	model : 0.07046232223510743
			 train-loss:  2.1213563044865924 	 ± 0.2746550392055978
	data : 0.115443754196167
	model : 0.06945395469665527
			 train-loss:  2.1203204318881035 	 ± 0.2659638586943134
	data : 0.11650123596191406
	model : 0.06915135383605957
			 train-loss:  2.1315918459611782 	 ± 0.2619322504413198
	data : 0.1166905403137207
	model : 0.06890721321105957
			 train-loss:  2.1314706868595548 	 ± 0.2545528821828648
	data : 0.11701397895812989
	model : 0.06910099983215331
			 train-loss:  2.185820635996367 	 ± 0.33846314119766274
	data : 0.11688117980957032
	model : 0.06975245475769043
			 train-loss:  2.1909143030643463 	 ± 0.33063937528931503
	data : 0.11622347831726074
	model : 0.06938238143920898
			 train-loss:  2.18412416889554 	 ± 0.32409672403149853
	data : 0.1165585994720459
	model : 0.06950182914733886
			 train-loss:  2.180450065569444 	 ± 0.31709254251234964
	data : 0.11654534339904785
	model : 0.06959357261657714
			 train-loss:  2.176572286564371 	 ± 0.31065553266257606
	data : 0.11626858711242676
	model : 0.0695610523223877
			 train-loss:  2.183712437748909 	 ± 0.30603647048001714
	data : 0.11611413955688477
	model : 0.06907987594604492
			 train-loss:  2.1887086629867554 	 ± 0.3008505988233475
	data : 0.11643919944763184
	model : 0.06942710876464844
			 train-loss:  2.1898829616033115 	 ± 0.29506670728939344
	data : 0.11617403030395508
	model : 0.06908726692199707
			 train-loss:  2.1878837082121105 	 ± 0.28973035416921605
	data : 0.11639251708984374
	model : 0.06876559257507324
			 train-loss:  2.2021373893533434 	 ± 0.2939918538438321
	data : 0.11684575080871581
	model : 0.06783518791198731
			 train-loss:  2.193316574754386 	 ± 0.2926250384634623
	data : 0.11772785186767579
	model : 0.06790618896484375
			 train-loss:  2.198393472035726 	 ± 0.28900271808186223
	data : 0.11777687072753906
	model : 0.0671377182006836
			 train-loss:  2.192155676503335 	 ± 0.28634873383002246
	data : 0.11844263076782227
	model : 0.06773548126220703
			 train-loss:  2.1814018078148365 	 ± 0.28812887983196245
	data : 0.11792955398559571
	model : 0.06733236312866211
			 train-loss:  2.1814397356726904 	 ± 0.2837297884224795
	data : 0.11829791069030762
	model : 0.0673858642578125
			 train-loss:  2.174908764222089 	 ± 0.2820326894642098
	data : 0.1183842658996582
	model : 0.06669960021972657
			 train-loss:  2.170989213671003 	 ± 0.2789124139305548
	data : 0.11910290718078613
	model : 0.06768279075622559
			 train-loss:  2.1709396839141846 	 ± 0.27501150499104887
	data : 0.11831355094909668
	model : 0.06748466491699219
			 train-loss:  2.176277205750749 	 ± 0.273153522678396
	data : 0.11854667663574218
	model : 0.06859345436096191
			 train-loss:  2.1640273677675346 	 ± 0.27964534076569353
	data : 0.11766839027404785
	model : 0.06953625679016114
			 train-loss:  2.155555147391099 	 ± 0.280934037110864
	data : 0.11685476303100586
	model : 0.07036724090576171
			 train-loss:  2.166897949576378 	 ± 0.28630148824688856
	data : 0.11603946685791015
	model : 0.07034487724304199
			 train-loss:  2.156359489371137 	 ± 0.29053687403902684
	data : 0.11605381965637207
	model : 0.07031445503234864
			 train-loss:  2.152235351857685 	 ± 0.2882693572093297
	data : 0.11612787246704101
	model : 0.07000813484191895
			 train-loss:  2.1542591688244843 	 ± 0.28519941677200084
	data : 0.11617555618286132
	model : 0.06998209953308106
			 train-loss:  2.158105706626719 	 ± 0.28306593229164057
	data : 0.11628050804138183
	model : 0.06983528137207032
			 train-loss:  2.1526186148325603 	 ± 0.28225962667373616
	data : 0.11628837585449218
	model : 0.06993398666381836
			 train-loss:  2.1482283913570903 	 ± 0.28072381859146983
	data : 0.1161872386932373
	model : 0.06993951797485351
			 train-loss:  2.144266425295079 	 ± 0.2790183003014422
	data : 0.11610054969787598
	model : 0.0701840877532959
			 train-loss:  2.1408304373423257 	 ± 0.27709961074686346
	data : 0.11581082344055176
	model : 0.06999764442443848
			 train-loss:  2.1426901574037513 	 ± 0.27455997654613107
	data : 0.11602287292480469
	model : 0.0701981544494629
			 train-loss:  2.14633563041687 	 ± 0.27299578990814366
	data : 0.11614766120910644
	model : 0.06968750953674316
			 train-loss:  2.14756693092047 	 ± 0.2704462950112345
	data : 0.11647930145263671
	model : 0.06965999603271485
			 train-loss:  2.148239337480985 	 ± 0.26787626926565317
	data : 0.11650662422180176
	model : 0.06859188079833985
			 train-loss:  2.1465916183759584 	 ± 0.2656030039722018
	data : 0.11734972000122071
	model : 0.06820144653320312
			 train-loss:  2.1446781997327453 	 ± 0.2635006825571964
	data : 0.11764225959777833
	model : 0.06831035614013672
			 train-loss:  2.144945660504428 	 ± 0.2611016306474998
	data : 0.11730213165283203
	model : 0.06846661567687988
			 train-loss:  2.1366425263030187 	 ± 0.265985863603898
	data : 0.11713666915893554
	model : 0.06891121864318847
			 train-loss:  2.137888839370326 	 ± 0.26380724559918095
	data : 0.11680183410644532
	model : 0.06911473274230957
			 train-loss:  2.1377540148537735 	 ± 0.2615251381290237
	data : 0.11689190864562989
	model : 0.06952152252197266
			 train-loss:  2.1365207272060847 	 ± 0.259469404265946
	data : 0.11640048027038574
	model : 0.06926469802856446
			 train-loss:  2.134753721952438 	 ± 0.25765580729733467
	data : 0.1168323040008545
	model : 0.06948037147521972
			 train-loss:  2.141858876728621 	 ± 0.26139473019267895
	data : 0.11657614707946777
	model : 0.0691330909729004
			 train-loss:  2.13948320381103 	 ± 0.25994119798502946
	data : 0.1167065143585205
	model : 0.06994037628173828
			 train-loss:  2.1372355355156794 	 ± 0.2584765378587482
	data : 0.11605396270751953
	model : 0.07002439498901367
			 train-loss:  2.130411410704255 	 ± 0.2621069233987209
	data : 0.11595115661621094
	model : 0.06978063583374024
			 train-loss:  2.128683048028212 	 ± 0.260450184900414
	data : 0.11593513488769532
	model : 0.06968865394592286
			 train-loss:  2.126976697733908 	 ± 0.25883539642912795
	data : 0.11624035835266114
	model : 0.06956844329833985
			 train-loss:  2.126051509558265 	 ± 0.2570064589077521
	data : 0.1161867618560791
	model : 0.06952276229858398
			 train-loss:  2.1237679877701927 	 ± 0.2557935336987772
	data : 0.11617236137390137
	model : 0.06959538459777832
			 train-loss:  2.11695014566615 	 ± 0.2600825027325778
	data : 0.11615157127380371
	model : 0.06985573768615723
			 train-loss:  2.1170473269053867 	 ± 0.25821934981054645
	data : 0.11614346504211426
	model : 0.06989760398864746
			 train-loss:  2.1119959505511003 	 ± 0.25985431472138454
	data : 0.11606330871582031
	model : 0.06987948417663574
			 train-loss:  2.114742091960377 	 ± 0.25907886761472754
	data : 0.11616706848144531
	model : 0.06970930099487305
			 train-loss:  2.113156073713956 	 ± 0.2576499462339689
	data : 0.1162477970123291
	model : 0.06897649765014649
			 train-loss:  2.1154430492504224 	 ± 0.25664806507941973
	data : 0.11693024635314941
	model : 0.06887254714965821
			 train-loss:  2.114179935455322 	 ± 0.2551627912324799
	data : 0.11686081886291504
	model : 0.06896576881408692
			 train-loss:  2.110458526172136 	 ± 0.25551914153290767
	data : 0.11674094200134277
	model : 0.06917076110839844
			 train-loss:  2.103119874929453 	 ± 0.26179218887684397
	data : 0.11663250923156739
	model : 0.06932630538940429
			 train-loss:  2.1016124930137243 	 ± 0.2604447231259564
	data : 0.1164780616760254
	model : 0.07009000778198242
			 train-loss:  2.1037919687319406 	 ± 0.2595059473891227
	data : 0.11590261459350586
	model : 0.07025909423828125
			 train-loss:  2.104798160493374 	 ± 0.2580339635555721
	data : 0.11570463180541993
	model : 0.06965909004211426
			 train-loss:  2.1072684320402733 	 ± 0.2573863079438865
	data : 0.11638021469116211
	model : 0.06951313018798828
			 train-loss:  2.1076587046064983 	 ± 0.2558361779006208
	data : 0.11653485298156738
	model : 0.06871895790100098
			 train-loss:  2.1076681226132865 	 ± 0.2542903401164743
	data : 0.11721296310424804
	model : 0.06863198280334473
			 train-loss:  2.101607674644107 	 ± 0.2587320661287996
	data : 0.1173438549041748
	model : 0.0683011531829834
			 train-loss:  2.100418368507834 	 ± 0.25743647664453084
	data : 0.11774506568908691
	model : 0.0686464786529541
			 train-loss:  2.1020310507264246 	 ± 0.2563668859066725
	data : 0.11719107627868652
	model : 0.0676908016204834
			 train-loss:  2.101987085123172 	 ± 0.2548895807543278
	data : 0.11800775527954102
	model : 0.06848535537719727
			 train-loss:  2.104791045188904 	 ± 0.2547831007388634
	data : 0.11724600791931153
	model : 0.06834969520568848
			 train-loss:  2.1051109640785817 	 ± 0.25336546618335143
	data : 0.11727771759033204
	model : 0.06863994598388672
			 train-loss:  2.100519197516971 	 ± 0.25565073170431146
	data : 0.11702699661254883
	model : 0.06890678405761719
			 train-loss:  2.107566821706164 	 ± 0.26288646822204226
	data : 0.116756010055542
	model : 0.06969251632690429
			 train-loss:  2.106526427942774 	 ± 0.26164213599662867
	data : 0.11589570045471191
	model : 0.06950349807739258
			 train-loss:  2.105112173224008 	 ± 0.26058496788309476
	data : 0.11616411209106445
	model : 0.068809175491333
			 train-loss:  2.1071871163997242 	 ± 0.2599664172765297
	data : 0.11681714057922363
	model : 0.0687211036682129
			 train-loss:  2.108807400653237 	 ± 0.2590712704130331
	data : 0.11694021224975586
	model : 0.0684725284576416
			 train-loss:  2.109555679063002 	 ± 0.25782158671363076
	data : 0.11729202270507813
	model : 0.06861343383789062
			 train-loss:  2.109232895153085 	 ± 0.2565086640170346
	data : 0.11728363037109375
	model : 0.06905856132507324
			 train-loss:  2.107444311891283 	 ± 0.2558038424065433
	data : 0.11706709861755371
	model : 0.0699355125427246
			 train-loss:  2.1087541640406906 	 ± 0.2548387331528034
	data : 0.11629137992858887
	model : 0.06963000297546387
			 train-loss:  2.111832128763199 	 ± 0.25540412082079633
	data : 0.11661925315856933
	model : 0.06983122825622559
			 train-loss:  2.107848650158042 	 ± 0.25723961787364386
	data : 0.1164461612701416
	model : 0.06992506980895996
			 train-loss:  2.1039706024469114 	 ± 0.2589255460514963
	data : 0.11656522750854492
	model : 0.07054367065429687
			 train-loss:  2.1059244558649155 	 ± 0.25842006567244913
	data : 0.11568756103515625
	model : 0.07048416137695312
			 train-loss:  2.1041995332791257 	 ± 0.2577697962881177
	data : 0.11561894416809082
	model : 0.07058086395263671
			 train-loss:  2.1058059283665247 	 ± 0.2570619153857189
	data : 0.11552510261535645
	model : 0.07059922218322753
			 train-loss:  2.1082134764149503 	 ± 0.2570331389128443
	data : 0.11546049118041993
	model : 0.06955265998840332
			 train-loss:  2.104771338890646 	 ± 0.2582721739999657
	data : 0.11608548164367676
	model : 0.06874551773071289
			 train-loss:  2.1077984692873777 	 ± 0.2589737012313918
	data : 0.11708130836486816
	model : 0.06885061264038086
			 train-loss:  2.104772077787907 	 ± 0.25969454711089557
	data : 0.11686773300170898
	model : 0.06903157234191895
			 train-loss:  2.1036173246123573 	 ± 0.2587923917722791
	data : 0.11675939559936524
	model : 0.06910457611083984
			 train-loss:  2.105005241729118 	 ± 0.2580349427645608
	data : 0.11667876243591309
	model : 0.07003164291381836
			 train-loss:  2.1047443694302013 	 ± 0.2568951211978982
	data : 0.11585774421691894
	model : 0.06905512809753418
			 train-loss:  2.1053495713039836 	 ± 0.2558360764141602
	data : 0.11665568351745606
	model : 0.06872563362121582
			 train-loss:  2.1052649742678593 	 ± 0.2547131042701229
	data : 0.11715149879455566
	model : 0.06880574226379395
			 train-loss:  2.1061474209246427 	 ± 0.2537782013657027
	data : 0.11702046394348145
	model : 0.06880550384521485
			 train-loss:  2.106723819313378 	 ± 0.25275755381861836
	data : 0.11718587875366211
	model : 0.06872625350952148
			 train-loss:  2.107290733573783 	 ± 0.251749131109017
	data : 0.11708040237426758
	model : 0.0694460391998291
			 train-loss:  2.106658162706989 	 ± 0.25077349055981457
	data : 0.11628222465515137
	model : 0.06975107192993164
			 train-loss:  2.110414133352392 	 ± 0.2530287369515051
	data : 0.11596250534057617
	model : 0.0696761131286621
			 train-loss:  2.1098990947008134 	 ± 0.25203487600253877
	data : 0.1159027099609375
	model : 0.06944303512573242
			 train-loss:  2.108693685413392 	 ± 0.25133835438043917
	data : 0.11611762046813964
	model : 0.06955199241638184
			 train-loss:  2.1097037958317117 	 ± 0.25055265444568603
	data : 0.1163550853729248
	model : 0.06892757415771485
			 train-loss:  2.1093725547557924 	 ± 0.24955888972723708
	data : 0.11701326370239258
	model : 0.06813321113586426
			 train-loss:  2.1100650823885396 	 ± 0.2486692068015708
	data : 0.11766877174377441
	model : 0.06836566925048829
			 train-loss:  2.1098891820907593 	 ± 0.24768027795822165
	data : 0.11753897666931153
	model : 0.06833095550537109
			 train-loss:  2.1079754801023576 	 ± 0.24762155155590632
	data : 0.1176445484161377
	model : 0.06846499443054199
			 train-loss:  2.107462743135888 	 ± 0.24671187960110194
	data : 0.11741533279418945
	model : 0.06878771781921386
			 train-loss:  2.1040233550593257 	 ± 0.24878416873519765
	data : 0.1170659065246582
	model : 0.06946654319763183
			 train-loss:  2.102191219034121 	 ± 0.24868338911826604
	data : 0.11651673316955566
	model : 0.06920647621154785
			 train-loss:  2.1050619382124682 	 ± 0.24986156574344157
	data : 0.11674022674560547
	model : 0.06920375823974609
			 train-loss:  2.1082333189840536 	 ± 0.2515188388722547
	data : 0.11658077239990235
	model : 0.06827139854431152
			 train-loss:  2.1076830968712317 	 ± 0.25064343262538713
	data : 0.11740779876708984
	model : 0.06875596046447754
			 train-loss:  2.1066495601395916 	 ± 0.2499815705865627
	data : 0.11696457862854004
	model : 0.06796202659606934
			 train-loss:  2.1049351211804064 	 ± 0.24983066967861448
	data : 0.11775140762329102
	model : 0.06734881401062012
			 train-loss:  2.1069884459177652 	 ± 0.25003597856703436
	data : 0.11815395355224609
	model : 0.06788992881774902
			 train-loss:  2.105792489998481 	 ± 0.24950228735725277
	data : 0.11761417388916015
	model : 0.06874079704284668
			 train-loss:  2.1070851157181454 	 ± 0.24904666536300643
	data : 0.11702399253845215
	model : 0.0687636375427246
			 train-loss:  2.10946561201759 	 ± 0.2497020964737855
	data : 0.11703548431396485
	model : 0.06872143745422363
			 train-loss:  2.112138790192364 	 ± 0.2507761936609053
	data : 0.11701912879943847
	model : 0.0694310188293457
			 train-loss:  2.112580579519272 	 ± 0.2499332391409832
	data : 0.11653623580932618
	model : 0.06899437904357911
			 train-loss:  2.1132154405539763 	 ± 0.2491586344477253
	data : 0.1168745994567871
	model : 0.06885728836059571
			 train-loss:  2.114028941577589 	 ± 0.24846761031659106
	data : 0.11696891784667969
	model : 0.06871509552001953
			 train-loss:  2.114591110836376 	 ± 0.2476879264140243
	data : 0.11697349548339844
	model : 0.06959853172302247
			 train-loss:  2.1141191894809404 	 ± 0.24689090611133482
	data : 0.1162691593170166
	model : 0.06952090263366699
			 train-loss:  2.1154041873997653 	 ± 0.24652082065345185
	data : 0.11629734039306641
	model : 0.06880221366882325
			 train-loss:  2.1150304136210925 	 ± 0.245716345415318
	data : 0.11684489250183105
	model : 0.06894149780273437
			 train-loss:  2.114536248907751 	 ± 0.24495193565592646
	data : 0.11676363945007324
	model : 0.06898226737976074
			 train-loss:  2.1158616373667845 	 ± 0.24465131063413573
	data : 0.1170119285583496
	model : 0.06897196769714356
			 train-loss:  2.117569993006303 	 ± 0.2447130814183348
	data : 0.1168548583984375
	model : 0.06898031234741211
			 train-loss:  2.1211380410194396 	 ± 0.24775426594939745
	data : 0.11676435470581055
	model : 0.0698740005493164
			 train-loss:  2.1222089623773335 	 ± 0.24728061529477283
	data : 0.11610016822814942
	model : 0.0699930191040039
			 train-loss:  2.122077185072397 	 ± 0.24647116938247998
	data : 0.11599807739257813
	model : 0.07006473541259765
			 train-loss:  2.1208132715786205 	 ± 0.24615809532433428
	data : 0.11586799621582031
	model : 0.07041559219360352
			 train-loss:  2.1208358817286306 	 ± 0.24535773846603892
	data : 0.11572213172912597
	model : 0.07077813148498535
			 train-loss:  2.1239840461361794 	 ± 0.24766573167666292
	data : 0.11551203727722167
	model : 0.07072868347167968
			 train-loss:  2.122515712028895 	 ± 0.24754656350901405
	data : 0.11560578346252441
	model : 0.07057490348815917
			 train-loss:  2.122455968978299 	 ± 0.2467580675186755
	data : 0.11583647727966309
	model : 0.07073283195495605
			 train-loss:  2.1189999587928194 	 ± 0.24975863554770572
	data : 0.11570076942443848
	model : 0.0695925235748291
			 train-loss:  2.1194567387958743 	 ± 0.24903818841778108
	data : 0.11670198440551757
	model : 0.0692408561706543
			 train-loss:  2.1187318690121173 	 ± 0.24842692794096688
	data : 0.11699786186218261
	model : 0.06830306053161621
			 train-loss:  2.1169837609581323 	 ± 0.24863939912735022
	data : 0.11793594360351563
	model : 0.06765213012695312
			 train-loss:  2.1152286330858865 	 ± 0.24886922786239746
	data : 0.11837644577026367
	model : 0.0675053596496582
			 train-loss:  2.116107761494221 	 ± 0.24835684446093345
	data : 0.11841435432434082
	model : 0.06839184761047364
			 train-loss:  2.1178740299329526 	 ± 0.2486232683624151
	data : 0.11757836341857911
	model : 0.06842117309570313
			 train-loss:  2.11522328636863 	 ± 0.2501824104849207
	data : 0.1174748420715332
	model : 0.06911606788635254
			 train-loss:  2.114203453782093 	 ± 0.24977148058893175
	data : 0.1168905258178711
	model : 0.06980347633361816
			 train-loss:  2.1119522755731364 	 ± 0.25070596372695464
	data : 0.11639494895935058
	model : 0.0705528736114502
			 train-loss:  2.1125050534804664 	 ± 0.2500607555773544
	data : 0.1157334327697754
	model : 0.07062335014343261
			 train-loss:  2.111526204284126 	 ± 0.24964243971829236
	data : 0.11560983657836914
	model : 0.06985082626342773
			 train-loss:  2.1093536643420947 	 ± 0.25050433066448774
	data : 0.1164522647857666
	model : 0.07004880905151367
			 train-loss:  2.108532330446076 	 ± 0.2500002533195473
	data : 0.11625933647155762
	model : 0.07011547088623046
			 train-loss:  2.1096226219521013 	 ± 0.24967985023338804
	data : 0.1161719799041748
	model : 0.0694234848022461
			 train-loss:  2.1092507735842223 	 ± 0.24900494629633968
	data : 0.1167790412902832
	model : 0.06949439048767089
			 train-loss:  2.1104719823804396 	 ± 0.2488074059325786
	data : 0.11682953834533691
	model : 0.07031469345092774
			 train-loss:  2.11146746976035 	 ± 0.24844277978150037
	data : 0.11602654457092285
	model : 0.0703059196472168
			 train-loss:  2.109361409463666 	 ± 0.24929765758537353
	data : 0.1161102294921875
	model : 0.07027745246887207
			 train-loss:  2.107266267814205 	 ± 0.2501414976081161
	data : 0.11597800254821777
	model : 0.0703317642211914
			 train-loss:  2.108427375220181 	 ± 0.24991573342770346
	data : 0.11612868309020996
	model : 0.0691835880279541
			 train-loss:  2.1084113141011924 	 ± 0.2492167591919727
	data : 0.11704216003417969
	model : 0.06816158294677735
			 train-loss:  2.1072706984149083 	 ± 0.24899161199211714
	data : 0.11771998405456544
	model : 0.06819572448730468
			 train-loss:  2.10680208351072 	 ± 0.24838242076773986
	data : 0.11742510795593261
	model : 0.0683126449584961
			 train-loss:  2.107747415264884 	 ± 0.2480254037593956
	data : 0.11750726699829102
	model : 0.06808676719665527
			 train-loss:  2.108867011435045 	 ± 0.24780754794238385
	data : 0.11749377250671386
	model : 0.06901144981384277
			 train-loss:  2.1078471722810166 	 ± 0.24751802295566916
	data : 0.1167764663696289
	model : 0.06999597549438477
			 train-loss:  2.1060416298943596 	 ± 0.24806016513483384
	data : 0.1161388874053955
	model : 0.0700263500213623
			 train-loss:  2.1061621404463247 	 ± 0.2473978680525829
	data : 0.1161849021911621
	model : 0.0698129653930664
			 train-loss:  2.103949015790766 	 ± 0.24857476770355694
	data : 0.11627440452575684
	model : 0.06998968124389648
			 train-loss:  2.1048734549512256 	 ± 0.24823488060394513
	data : 0.11617202758789062
	model : 0.06948952674865723
			 train-loss:  2.1060771607848072 	 ± 0.24812681417946567
	data : 0.11658759117126465
	model : 0.0689774990081787
			 train-loss:  2.1044383149398 	 ± 0.24849647619895532
	data : 0.1170724868774414
	model : 0.06920533180236817
			 train-loss:  2.1033397469845116 	 ± 0.24830726819411605
	data : 0.11695342063903809
	model : 0.0690216064453125
			 train-loss:  2.100936809554696 	 ± 0.24987642249816674
	data : 0.11717581748962402
	model : 0.06845860481262207
			 train-loss:  2.10018953078769 	 ± 0.24944324023735695
	data : 0.11761360168457032
	model : 0.06883859634399414
			 train-loss:  2.0989556472326063 	 ± 0.24938932404328462
	data : 0.11736874580383301
	model : 0.06920008659362793
			 train-loss:  2.0973736915832912 	 ± 0.24972302113106262
	data : 0.11695394515991211
	model : 0.06808328628540039
			 train-loss:  2.0983008857892482 	 ± 0.24942144112479914
	data : 0.11792430877685547
	model : 0.06736221313476562
			 train-loss:  2.0959249583597717 	 ± 0.251001372983003
	data : 0.11854724884033203
	model : 0.06791415214538574
			 train-loss:  2.0962267198947946 	 ± 0.250402551333629
	data : 0.11807737350463868
	model : 0.06791453361511231
			 train-loss:  2.0969172650246164 	 ± 0.24996154088763817
	data : 0.1179049015045166
	model : 0.06794352531433105
			 train-loss:  2.0957035100460053 	 ± 0.2499230583435238
	data : 0.11795029640197754
	model : 0.06871166229248046
			 train-loss:  2.0986683404267725 	 ± 0.2528019482187491
	data : 0.11715974807739257
	model : 0.06946916580200195
			 train-loss:  2.096631410688457 	 ± 0.25382358112638725
	data : 0.11647624969482422
	model : 0.06941437721252441
			 train-loss:  2.09598015799311 	 ± 0.253366756168055
	data : 0.11657538414001464
	model : 0.06957755088806153
			 train-loss:  2.0959679157126185 	 ± 0.25274505650773904
	data : 0.11640424728393554
	model : 0.06962919235229492
			 train-loss:  2.095702752834413 	 ± 0.25215629486216573
	data : 0.11643991470336915
	model : 0.06941089630126954
			 train-loss:  2.0987094953222183 	 ± 0.2552007940105074
	data : 0.11655564308166504
	model : 0.06965928077697754
			 train-loss:  2.1006058851877847 	 ± 0.256034483216097
	data : 0.116265869140625
	model : 0.06957645416259765
			 train-loss:  2.1014524560708265 	 ± 0.25570852158854696
	data : 0.11645898818969727
	model : 0.06939191818237304
			 train-loss:  2.101356205187346 	 ± 0.25509982211137566
	data : 0.11654219627380372
	model : 0.06873407363891601
			 train-loss:  2.1028733582723707 	 ± 0.2554351197262546
	data : 0.11711020469665527
	model : 0.06807546615600586
			 train-loss:  2.1033992021569707 	 ± 0.2549430130475013
	data : 0.11764230728149414
	model : 0.06792325973510742
			 train-loss:  2.1025331560170875 	 ± 0.2546519445398409
	data : 0.11780352592468261
	model : 0.06806278228759766
			 train-loss:  2.1013339058334277 	 ± 0.2546528275592143
	data : 0.11775031089782714
	model : 0.06818046569824218
			 train-loss:  2.0980471138642214 	 ± 0.2585460762855912
	data : 0.11795268058776856
	model : 0.06891088485717774
			 train-loss:  2.0975849434386853 	 ± 0.2580326962113429
	data : 0.1174581527709961
	model : 0.06955738067626953
			 train-loss:  2.1004192183415094 	 ± 0.26076761128098497
	data : 0.11715331077575683
	model : 0.06967358589172364
			 train-loss:  2.0995860297559044 	 ± 0.26045408827412164
	data : 0.11693372726440429
	model : 0.06888556480407715
			 train-loss:  2.0997130728642874 	 ± 0.25986276884702036
	data : 0.11762280464172363
	model : 0.06816802024841309
			 train-loss:  2.0993014819001496 	 ± 0.25934000722211026
	data : 0.1180917739868164
	model : 0.06797823905944825
			 train-loss:  2.099303622679277 	 ± 0.2587499287445838
	data : 0.11790790557861328
	model : 0.06812562942504882
			 train-loss:  2.1002786828381983 	 ± 0.25856863857503687
	data : 0.11765131950378419
	model : 0.06803994178771973
			 train-loss:  2.101053324905602 	 ± 0.25824251277280114
	data : 0.1178168773651123
	model : 0.06861677169799804
			 train-loss:  2.101411253882096 	 ± 0.25771802776502684
	data : 0.11714062690734864
	model : 0.06863675117492676
			 train-loss:  2.102024681866169 	 ± 0.2573052347832865
	data : 0.11727643013000488
	model : 0.0678713321685791
			 train-loss:  2.10131400903066 	 ± 0.2569530454124063
	data : 0.11791791915893554
	model : 0.06706643104553223
			 train-loss:  2.1007124734135854 	 ± 0.2565426616808101
	data : 0.11842536926269531
	model : 0.06695089340209961
			 train-loss:  2.0992036648258763 	 ± 0.25697995427709613
	data : 0.118477201461792
	model : 0.06706151962280274
			 train-loss:  2.1008257797935554 	 ± 0.2575778500271727
	data : 0.11840901374816895
	model : 0.06703338623046876
			 train-loss:  2.100066317220963 	 ± 0.2572705451317
	data : 0.11824560165405273
	model : 0.06735210418701172
			 train-loss:  2.1005584416182144 	 ± 0.25681865117252417
	data : 0.11829371452331543
	model : 0.06797270774841309
			 train-loss:  2.1007318228354186 	 ± 0.2562756535495254
	data : 0.11788001060485839
	model : 0.06776800155639648
			 train-loss:  2.101432153890873 	 ± 0.2559441663056178
	data : 0.11796045303344727
	model : 0.06767597198486328
			 train-loss:  2.1003064050183275 	 ± 0.2559693040154594
	data : 0.11804752349853516
	model : 0.06824221611022949
			 train-loss:  2.0997002333657355 	 ± 0.25558931482125363
	data : 0.11761274337768554
	model : 0.06854114532470704
			 train-loss:  2.0992237096137187 	 ± 0.25514907569915113
	data : 0.11709055900573731
	model : 0.06873078346252441
			 train-loss:  2.0978250341900324 	 ± 0.2555091539669915
	data : 0.117118501663208
	model : 0.06852874755859376
			 train-loss:  2.096403152127809 	 ± 0.25590349079300717
	data : 0.11742200851440429
	model : 0.06837363243103027
			 train-loss:  2.0968221646397054 	 ± 0.2554467719012992
	data : 0.11740965843200683
	model : 0.06833438873291016
			 train-loss:  2.0969797557367937 	 ± 0.2549233976438974
	data : 0.11738858222961426
	model : 0.06812982559204102
			 train-loss:  2.095782387256622 	 ± 0.255064337250707
	data : 0.1175260066986084
	model : 0.06800365447998047
			 train-loss:  2.095040412364659 	 ± 0.25479402094684817
	data : 0.11734237670898437
	model : 0.0683666706085205
			 train-loss:  2.0962991566697426 	 ± 0.2550168191458908
	data : 0.11687192916870118
	model : 0.06781764030456543
			 train-loss:  2.0959678304538807 	 ± 0.25454374123699863
	data : 0.11748409271240234
	model : 0.06737399101257324
			 train-loss:  2.095839467204985 	 ± 0.2540294806607825
	data : 0.11806330680847169
	model : 0.06796431541442871
			 train-loss:  2.0954445264777357 	 ± 0.25358557525456515
	data : 0.11757678985595703
	model : 0.06815805435180664
			 train-loss:  2.094718919052341 	 ± 0.25332436306766654
	data : 0.11764273643493653
	model : 0.06824235916137696
			 train-loss:  2.09535017235559 	 ± 0.25300483893458103
	data : 0.11779727935791015
	model : 0.06909761428833008
			 train-loss:  2.0963923618678124 	 ± 0.2530249377288495
	data : 0.11694154739379883
	model : 0.0697556972503662
			 train-loss:  2.0947908723210715 	 ± 0.25377266762012834
	data : 0.1162912368774414
	model : 0.0695920467376709
			 train-loss:  2.0930582785606386 	 ± 0.2547360064208555
	data : 0.11646156311035157
	model : 0.06932339668273926
			 train-loss:  2.090797646112176 	 ± 0.25672849493701105
	data : 0.11656179428100585
	model : 0.06889314651489258
			 train-loss:  2.0896924738846128 	 ± 0.2568161739779781
	data : 0.11714692115783691
	model : 0.0687171459197998
			 train-loss:  2.089683641558108 	 ± 0.2563081679643815
	data : 0.11732611656188965
	model : 0.06859192848205567
			 train-loss:  2.088728667713526 	 ± 0.2562537206373039
	data : 0.11719279289245606
	model : 0.06845231056213379
			 train-loss:  2.089847336095922 	 ± 0.2563714417567743
	data : 0.11735787391662597
	model : 0.06852731704711915
			 train-loss:  2.0925011900253594 	 ± 0.2593559766018196
	data : 0.11630139350891114
	model : 0.06002984046936035
#epoch  87    val-loss:  2.4591843705428276  train-loss:  2.0925011900253594  lr:  2.44140625e-06
			 train-loss:  1.761523962020874 	 ± 0.0
	data : 5.995982885360718
	model : 0.0752420425415039
			 train-loss:  1.9269176721572876 	 ± 0.16539371013641357
	data : 3.065036177635193
	model : 0.07307088375091553
			 train-loss:  1.978363275527954 	 ± 0.15339497964667073
	data : 2.0833381017049155
	model : 0.07067426045735677
			 train-loss:  1.9805437326431274 	 ± 0.13289762230865992
	data : 1.5922542214393616
	model : 0.07045328617095947
			 train-loss:  1.9520362138748169 	 ± 0.13183374726350794
	data : 1.2968488216400147
	model : 0.07031912803649902
			 train-loss:  1.9400068918863933 	 ± 0.12331654539755783
	data : 0.12074422836303711
	model : 0.06929588317871094
			 train-loss:  1.9494010039738245 	 ± 0.11646476755593091
	data : 0.11712923049926757
	model : 0.06906018257141114
			 train-loss:  1.9303569793701172 	 ± 0.12003025007626698
	data : 0.11636428833007813
	model : 0.07013974189758301
			 train-loss:  1.9341224034627278 	 ± 0.11366565669442252
	data : 0.1154637336730957
	model : 0.07000551223754883
			 train-loss:  1.9337711215019227 	 ± 0.10783785954728872
	data : 0.11585416793823242
	model : 0.06988787651062012
			 train-loss:  1.9274072538722644 	 ± 0.10477027195911964
	data : 0.11621675491333008
	model : 0.06995277404785157
			 train-loss:  1.9336244364579518 	 ± 0.10240733704930724
	data : 0.11621713638305664
	model : 0.06923146247863769
			 train-loss:  1.9329871030954213 	 ± 0.09841455427601382
	data : 0.11687922477722168
	model : 0.06822028160095214
			 train-loss:  1.9488971403666906 	 ± 0.11083451256607149
	data : 0.1178633689880371
	model : 0.06830601692199707
			 train-loss:  1.9489132722218832 	 ± 0.10707632888438014
	data : 0.11767635345458985
	model : 0.06843152046203613
			 train-loss:  1.9307304918766022 	 ± 0.1253313965479809
	data : 0.11740484237670898
	model : 0.06756649017333985
			 train-loss:  1.9344357392367195 	 ± 0.12248927750109448
	data : 0.11804547309875488
	model : 0.06829042434692383
			 train-loss:  1.957736525270674 	 ± 0.15297006632842722
	data : 0.11751065254211426
	model : 0.06905093193054199
			 train-loss:  1.9616248168443378 	 ± 0.14980123233123321
	data : 0.11669955253601075
	model : 0.0681772232055664
			 train-loss:  1.9768939673900605 	 ± 0.16046239733011955
	data : 0.1173670768737793
	model : 0.06797771453857422
			 train-loss:  1.9758574678784324 	 ± 0.15666385583978645
	data : 0.11747889518737793
	model : 0.06863288879394532
			 train-loss:  1.9943954240192066 	 ± 0.17505633198769244
	data : 0.11701083183288574
	model : 0.06773386001586915
			 train-loss:  2.0111781047738115 	 ± 0.18843785309440245
	data : 0.11772565841674805
	model : 0.06774506568908692
			 train-loss:  2.0120013703902564 	 ± 0.1845125436111951
	data : 0.11791462898254394
	model : 0.06814703941345215
			 train-loss:  2.0159171438217163 	 ± 0.18179956643918305
	data : 0.11766223907470703
	model : 0.06888437271118164
			 train-loss:  2.0120922372891354 	 ± 0.17929203727930018
	data : 0.1171682357788086
	model : 0.06909475326538086
			 train-loss:  2.009459888493573 	 ± 0.17645173939648556
	data : 0.11703095436096192
	model : 0.07001523971557617
			 train-loss:  1.9995241974081313 	 ± 0.18079997005632387
	data : 0.11628422737121583
	model : 0.06983809471130371
			 train-loss:  2.0077909066759307 	 ± 0.18296150568436148
	data : 0.11641092300415039
	model : 0.07013468742370606
			 train-loss:  2.022010886669159 	 ± 0.19550731250460232
	data : 0.11611528396606445
	model : 0.06961851119995117
			 train-loss:  2.0183249711990356 	 ± 0.19338481042832362
	data : 0.1162043571472168
	model : 0.06949491500854492
			 train-loss:  2.024314235895872 	 ± 0.19323823972239068
	data : 0.11622138023376465
	model : 0.06948313713073731
			 train-loss:  2.0352636936939126 	 ± 0.200114895364555
	data : 0.11600627899169921
	model : 0.06964526176452637
			 train-loss:  2.0383504734319797 	 ± 0.19794590059430772
	data : 0.11591687202453613
	model : 0.06997900009155274
			 train-loss:  2.0548092808042253 	 ± 0.21742450839971125
	data : 0.11555089950561523
	model : 0.06995830535888672
			 train-loss:  2.054617335398992 	 ± 0.2143864638709074
	data : 0.11574020385742187
	model : 0.06994524002075195
			 train-loss:  2.0637751946578153 	 ± 0.2184915137508979
	data : 0.11560378074645997
	model : 0.0698458194732666
			 train-loss:  2.0622854013192025 	 ± 0.21578782423071047
	data : 0.11574306488037109
	model : 0.06985664367675781
			 train-loss:  2.060072483160557 	 ± 0.21343971690336916
	data : 0.11571750640869141
	model : 0.07017312049865723
			 train-loss:  2.0592550158500673 	 ± 0.2108166544472724
	data : 0.11555790901184082
	model : 0.07020878791809082
			 train-loss:  2.066050878385218 	 ± 0.21261942513602736
	data : 0.11565680503845215
	model : 0.06952862739562989
			 train-loss:  2.0629800331024897 	 ± 0.21099122268047357
	data : 0.11654706001281738
	model : 0.06963582038879394
			 train-loss:  2.0717274532761683 	 ± 0.21609196931116378
	data : 0.11657018661499023
	model : 0.0687652587890625
			 train-loss:  2.0796391584656457 	 ± 0.21983189213119833
	data : 0.11728954315185547
	model : 0.06783394813537598
			 train-loss:  2.078769556681315 	 ± 0.21745211301661588
	data : 0.11807894706726074
	model : 0.0678257942199707
			 train-loss:  2.079482218493586 	 ± 0.2151286415597625
	data : 0.11794834136962891
	model : 0.06853828430175782
			 train-loss:  2.0832613021769424 	 ± 0.21436555757835746
	data : 0.11706652641296386
	model : 0.06854982376098633
			 train-loss:  2.0924061983823776 	 ± 0.22119180715851727
	data : 0.11704411506652831
	model : 0.06940817832946777
			 train-loss:  2.0888840033083547 	 ± 0.22027893842360427
	data : 0.11630263328552246
	model : 0.06946454048156739
			 train-loss:  2.091290192604065 	 ± 0.21871454477967897
	data : 0.11622042655944824
	model : 0.06867332458496093
			 train-loss:  2.0911370492448995 	 ± 0.21656237654947896
	data : 0.11697869300842285
	model : 0.0678478717803955
			 train-loss:  2.0938519056026754 	 ± 0.21534448386559493
	data : 0.11769270896911621
	model : 0.06772093772888184
			 train-loss:  2.0836365470346414 	 ± 0.22566493845279173
	data : 0.11772370338439941
	model : 0.06760797500610352
			 train-loss:  2.077504261776253 	 ± 0.22797955170864806
	data : 0.11779046058654785
	model : 0.06811838150024414
			 train-loss:  2.0733329664577136 	 ± 0.227967686171738
	data : 0.11750073432922363
	model : 0.06830201148986817
			 train-loss:  2.067372869168009 	 ± 0.23020641849458154
	data : 0.11754078865051269
	model : 0.06915144920349121
			 train-loss:  2.0646176066314963 	 ± 0.22910779290937638
	data : 0.11675910949707032
	model : 0.06916122436523438
			 train-loss:  2.0635458991445343 	 ± 0.22726821588646834
	data : 0.11686983108520507
	model : 0.0687739372253418
			 train-loss:  2.0625657166464855 	 ± 0.22545759668136747
	data : 0.11734724044799805
	model : 0.06866822242736817
			 train-loss:  2.066418892145157 	 ± 0.22552142505825476
	data : 0.11734652519226074
	model : 0.06853065490722657
			 train-loss:  2.068505019438071 	 ± 0.22424821006429202
	data : 0.11743745803833008
	model : 0.06829071044921875
			 train-loss:  2.073936633525356 	 ± 0.22644165520079937
	data : 0.11774158477783203
	model : 0.06750092506408692
			 train-loss:  2.069275000738719 	 ± 0.22761641441048144
	data : 0.11826629638671875
	model : 0.06719279289245605
			 train-loss:  2.071779441088438 	 ± 0.2267043511044583
	data : 0.11850051879882813
	model : 0.06810970306396484
			 train-loss:  2.070221383755024 	 ± 0.2252987690656354
	data : 0.11744170188903809
	model : 0.06891112327575684
			 train-loss:  2.072598291165901 	 ± 0.22440517206218155
	data : 0.11678380966186523
	model : 0.06905612945556641
			 train-loss:  2.0681982520800917 	 ± 0.22557450380436678
	data : 0.11674304008483886
	model : 0.07015037536621094
			 train-loss:  2.0656202733516693 	 ± 0.2249018564508401
	data : 0.11575779914855958
	model : 0.07073183059692383
			 train-loss:  2.064684862675874 	 ± 0.22339939311218257
	data : 0.11513385772705079
	model : 0.06977987289428711
			 train-loss:  2.0605599641799928 	 ± 0.22442893734587513
	data : 0.11616635322570801
	model : 0.06962776184082031
			 train-loss:  2.0601434774801763 	 ± 0.22287008962446458
	data : 0.1163135051727295
	model : 0.06951651573181153
			 train-loss:  2.061463064617581 	 ± 0.22159610512597927
	data : 0.11646957397460937
	model : 0.06870369911193848
			 train-loss:  2.068334030778441 	 ± 0.2276648798597816
	data : 0.11735424995422364
	model : 0.06936526298522949
			 train-loss:  2.068003818795488 	 ± 0.22613897164953056
	data : 0.11680474281311035
	model : 0.06944537162780762
			 train-loss:  2.0696350161234536 	 ± 0.22506417511799223
	data : 0.11676630973815919
	model : 0.06934571266174316
			 train-loss:  2.0744557317934538 	 ± 0.22744303007538413
	data : 0.11676034927368165
	model : 0.06941490173339844
			 train-loss:  2.0773177084984717 	 ± 0.22733459623964256
	data : 0.11660819053649903
	model : 0.06928224563598633
			 train-loss:  2.078620825058375 	 ± 0.22616188044956648
	data : 0.1166719913482666
	model : 0.06859536170959472
			 train-loss:  2.075597621217559 	 ± 0.2263065164401145
	data : 0.11721844673156738
	model : 0.06866955757141113
			 train-loss:  2.0727089166641237 	 ± 0.22634858271421754
	data : 0.11731433868408203
	model : 0.06798095703125
			 train-loss:  2.0733673690277854 	 ± 0.2250241122168464
	data : 0.11791677474975586
	model : 0.06808261871337891
			 train-loss:  2.0712887383088834 	 ± 0.2244288703774061
	data : 0.11786904335021972
	model : 0.06794757843017578
			 train-loss:  2.0706763885107384 	 ± 0.223141700231788
	data : 0.11805362701416015
	model : 0.06746268272399902
			 train-loss:  2.0680504639943442 	 ± 0.22309589570797367
	data : 0.11858091354370118
	model : 0.06723618507385254
			 train-loss:  2.067956066131592 	 ± 0.2217813717710668
	data : 0.11867551803588867
	model : 0.06763634681701661
			 train-loss:  2.063673587732537 	 ± 0.22399531932267755
	data : 0.11825242042541503
	model : 0.06753125190734863
			 train-loss:  2.062304066515517 	 ± 0.22306611609144797
	data : 0.11843867301940918
	model : 0.06843209266662598
			 train-loss:  2.0651349371129815 	 ± 0.22336127194053482
	data : 0.11767973899841308
	model : 0.06899986267089844
			 train-loss:  2.069334614142943 	 ± 0.22556988801301844
	data : 0.11700315475463867
	model : 0.0691500186920166
			 train-loss:  2.0699049605263604 	 ± 0.22437774529640597
	data : 0.11689033508300781
	model : 0.06976103782653809
			 train-loss:  2.0679593295841427 	 ± 0.22390359512616692
	data : 0.11625938415527344
	model : 0.06917295455932618
			 train-loss:  2.0656736480153124 	 ± 0.22374832494607996
	data : 0.11669387817382812
	model : 0.06897196769714356
			 train-loss:  2.060780948208224 	 ± 0.22743645436151896
	data : 0.11681079864501953
	model : 0.06952080726623536
			 train-loss:  2.05665267021098 	 ± 0.22969984562674872
	data : 0.11624350547790527
	model : 0.06951379776000977
			 train-loss:  2.0511987886930765 	 ± 0.23452642244719674
	data : 0.11622610092163085
	model : 0.0686152458190918
			 train-loss:  2.056513520578543 	 ± 0.23898348113655404
	data : 0.11725826263427734
	model : 0.06995644569396972
			 train-loss:  2.0558216104802396 	 ± 0.23784505117194257
	data : 0.11596403121948243
	model : 0.06926507949829101
			 train-loss:  2.0544469015938893 	 ± 0.23701547145543175
	data : 0.11668586730957031
	model : 0.06887612342834473
			 train-loss:  2.0604892499519116 	 ± 0.24328352475170975
	data : 0.11718111038208008
	model : 0.06892085075378418
			 train-loss:  2.0601463866233827 	 ± 0.24208808862148212
	data : 0.11718659400939942
	model : 0.06883234977722168
			 train-loss:  2.060724926466989 	 ± 0.2409561157116145
	data : 0.11728854179382324
	model : 0.06729750633239746
			 train-loss:  2.055883453172796 	 ± 0.2446590724162753
	data : 0.1187826156616211
	model : 0.06915688514709473
			 train-loss:  2.0550767634678815 	 ± 0.24360478609810876
	data : 0.11711359024047852
	model : 0.06988291740417481
			 train-loss:  2.0567190876373878 	 ± 0.24300308185004127
	data : 0.11633601188659667
	model : 0.0698702335357666
			 train-loss:  2.05632419132051 	 ± 0.2418766838244348
	data : 0.1163144588470459
	model : 0.06981086730957031
			 train-loss:  2.0564846610123255 	 ± 0.24073866814354297
	data : 0.1162109375
	model : 0.07037653923034667
			 train-loss:  2.051427678527119 	 ± 0.24520239690586967
	data : 0.11558389663696289
	model : 0.06923842430114746
			 train-loss:  2.053488901367894 	 ± 0.24499411051475517
	data : 0.11660194396972656
	model : 0.06837019920349122
			 train-loss:  2.0516439763777847 	 ± 0.2446202309632974
	data : 0.11755475997924805
	model : 0.06841535568237304
			 train-loss:  2.0497120066122574 	 ± 0.24433974144155235
	data : 0.11753149032592773
	model : 0.06884975433349609
			 train-loss:  2.0531347667848743 	 ± 0.24587138060655706
	data : 0.11731948852539062
	model : 0.06895136833190918
			 train-loss:  2.0546985534685 	 ± 0.24532513492836105
	data : 0.11705474853515625
	model : 0.06909990310668945
			 train-loss:  2.0552126970966307 	 ± 0.2442978158228153
	data : 0.11691827774047851
	model : 0.06831622123718262
			 train-loss:  2.0531194962953268 	 ± 0.24423965666372752
	data : 0.11767826080322266
	model : 0.06809306144714355
			 train-loss:  2.049286916981573 	 ± 0.24659439527385396
	data : 0.11794209480285645
	model : 0.06797246932983399
			 train-loss:  2.0486364097430787 	 ± 0.24562826666630364
	data : 0.11806020736694336
	model : 0.06873259544372559
			 train-loss:  2.0477976931465998 	 ± 0.24474308223707264
	data : 0.11763901710510254
	model : 0.06921658515930176
			 train-loss:  2.048720956858942 	 ± 0.24390836152743536
	data : 0.11734504699707031
	model : 0.07022905349731445
			 train-loss:  2.0490500997094547 	 ± 0.2429076893761483
	data : 0.11630525588989257
	model : 0.0702547550201416
			 train-loss:  2.0504907816648483 	 ± 0.2424034581440012
	data : 0.11615266799926757
	model : 0.07079758644104003
			 train-loss:  2.0490481479108826 	 ± 0.2419164409462235
	data : 0.11542396545410157
	model : 0.07019352912902832
			 train-loss:  2.0476167602617235 	 ± 0.24143689940729443
	data : 0.1160170555114746
	model : 0.06893620491027833
			 train-loss:  2.0475946490357564 	 ± 0.24045356965683584
	data : 0.11706624031066895
	model : 0.067966890335083
			 train-loss:  2.045756895695963 	 ± 0.2403477857432024
	data : 0.11800694465637207
	model : 0.06826567649841309
			 train-loss:  2.046326557159424 	 ± 0.2394684977308145
	data : 0.11781134605407714
	model : 0.06835684776306153
			 train-loss:  2.050822825658889 	 ± 0.24375622018576018
	data : 0.1179037094116211
	model : 0.06919260025024414
			 train-loss:  2.048156325272688 	 ± 0.24463264704752383
	data : 0.11683769226074218
	model : 0.06995677947998047
			 train-loss:  2.049585113301873 	 ± 0.24420658095601036
	data : 0.11621975898742676
	model : 0.07075152397155762
			 train-loss:  2.049872122993765 	 ± 0.24327987366633536
	data : 0.1152759075164795
	model : 0.07056283950805664
			 train-loss:  2.0508836691196146 	 ± 0.2426145565553808
	data : 0.1153796672821045
	model : 0.0712214469909668
			 train-loss:  2.049809681550237 	 ± 0.24199678627602927
	data : 0.1147428035736084
	model : 0.06966719627380372
			 train-loss:  2.0498569011688232 	 ± 0.24107899488453477
	data : 0.11639633178710937
	model : 0.06913361549377442
			 train-loss:  2.048324259600245 	 ± 0.2408156201344636
	data : 0.11671605110168456
	model : 0.06915321350097656
			 train-loss:  2.0478721750316335 	 ± 0.2399720153819411
	data : 0.11686992645263672
	model : 0.06901822090148926
			 train-loss:  2.0490329724771006 	 ± 0.23945888872607557
	data : 0.11702709197998047
	model : 0.06818218231201172
			 train-loss:  2.0480050865341637 	 ± 0.23887564089664898
	data : 0.11774091720581055
	model : 0.06902132034301758
			 train-loss:  2.046214422170263 	 ± 0.23891660703234913
	data : 0.11707863807678223
	model : 0.0696190357208252
			 train-loss:  2.044023895609206 	 ± 0.2394261793031761
	data : 0.11667699813842773
	model : 0.06980218887329101
			 train-loss:  2.049402494224713 	 ± 0.2467888421301981
	data : 0.11636767387390137
	model : 0.07001457214355469
			 train-loss:  2.051429523740496 	 ± 0.24706442228520129
	data : 0.11605968475341796
	model : 0.06922621726989746
			 train-loss:  2.0535366163186146 	 ± 0.24744593865845615
	data : 0.11668305397033692
	model : 0.06830029487609864
			 train-loss:  2.0527751680830835 	 ± 0.24673883202805272
	data : 0.11742615699768066
	model : 0.06841812133789063
			 train-loss:  2.0519025167385183 	 ± 0.2460943975367989
	data : 0.11718940734863281
	model : 0.06820096969604492
			 train-loss:  2.0522909685969353 	 ± 0.24528240433268628
	data : 0.11765165328979492
	model : 0.06745004653930664
			 train-loss:  2.0533106285950233 	 ± 0.2447412004828138
	data : 0.11846842765808105
	model : 0.069209623336792
			 train-loss:  2.0537082699880207 	 ± 0.24394860196357626
	data : 0.11686644554138184
	model : 0.07010674476623535
			 train-loss:  2.054711011802258 	 ± 0.24341915683467708
	data : 0.11589741706848145
	model : 0.07009100914001465
			 train-loss:  2.0572379626132347 	 ± 0.24452237974428556
	data : 0.11602907180786133
	model : 0.07012028694152832
			 train-loss:  2.0564417559028474 	 ± 0.2438928758530781
	data : 0.11586451530456543
	model : 0.07062363624572754
			 train-loss:  2.0543986193339028 	 ± 0.24435458519973918
	data : 0.11534352302551269
	model : 0.06969733238220215
			 train-loss:  2.052249876868646 	 ± 0.24496183889101183
	data : 0.11633567810058594
	model : 0.07050766944885253
			 train-loss:  2.0529814817403493 	 ± 0.24432017171284415
	data : 0.11562252044677734
	model : 0.07042474746704101
			 train-loss:  2.0517477989196777 	 ± 0.24399495936066887
	data : 0.11554880142211914
	model : 0.07041592597961426
			 train-loss:  2.0525351205429474 	 ± 0.24339638388926954
	data : 0.11553611755371093
	model : 0.07060098648071289
			 train-loss:  2.051890996194655 	 ± 0.24274160859274552
	data : 0.11538686752319335
	model : 0.07055816650390626
			 train-loss:  2.054008329525972 	 ± 0.24339403123252
	data : 0.11549758911132812
	model : 0.06949386596679688
			 train-loss:  2.054349433084962 	 ± 0.2426550561722719
	data : 0.11652932167053223
	model : 0.06908478736877441
			 train-loss:  2.0554151746291147 	 ± 0.2422542674663356
	data : 0.11703338623046874
	model : 0.06852254867553711
			 train-loss:  2.0547369441146373 	 ± 0.24164169368324454
	data : 0.11751103401184082
	model : 0.06835808753967285
			 train-loss:  2.058207231760025 	 ± 0.24482766738801037
	data : 0.11769165992736816
	model : 0.06845955848693848
			 train-loss:  2.058968581027866 	 ± 0.2442560733554353
	data : 0.11749186515808105
	model : 0.06873340606689453
			 train-loss:  2.0575691755907037 	 ± 0.2441475853657428
	data : 0.11728944778442382
	model : 0.06927614212036133
			 train-loss:  2.0575200209588362 	 ± 0.2433983182823896
	data : 0.11681694984436035
	model : 0.0701268196105957
			 train-loss:  2.0541584259126244 	 ± 0.24642130078285554
	data : 0.11607565879821777
	model : 0.07016806602478028
			 train-loss:  2.0561138759959827 	 ± 0.2469464259100914
	data : 0.11604313850402832
	model : 0.070151948928833
			 train-loss:  2.059359546167305 	 ± 0.24970651661397775
	data : 0.11598820686340332
	model : 0.07008504867553711
			 train-loss:  2.0625145920736347 	 ± 0.2522546081935552
	data : 0.11594223976135254
	model : 0.06897859573364258
			 train-loss:  2.064510850679307 	 ± 0.25282232146449907
	data : 0.11677727699279786
	model : 0.06852703094482422
			 train-loss:  2.0653740724868324 	 ± 0.252321406278203
	data : 0.11714329719543456
	model : 0.06913833618164063
			 train-loss:  2.065993649819318 	 ± 0.25170709306310435
	data : 0.11668720245361328
	model : 0.06842045783996582
			 train-loss:  2.066342086122747 	 ± 0.25101114400747776
	data : 0.11737432479858398
	model : 0.06860690116882324
			 train-loss:  2.0682325016620546 	 ± 0.2514982598992933
	data : 0.11732048988342285
	model : 0.0689690113067627
			 train-loss:  2.067412514907087 	 ± 0.2510008145671998
	data : 0.11693367958068848
	model : 0.0691610336303711
			 train-loss:  2.0688896830054535 	 ± 0.25103151650318917
	data : 0.1170832633972168
	model : 0.06867589950561523
			 train-loss:  2.0701539822987147 	 ± 0.2508682062775202
	data : 0.1172135353088379
	model : 0.06854848861694336
			 train-loss:  2.071143176745285 	 ± 0.2504965289098313
	data : 0.1172548770904541
	model : 0.06924481391906738
			 train-loss:  2.0710627514090243 	 ± 0.24979018804513672
	data : 0.11652750968933105
	model : 0.0698582649230957
			 train-loss:  2.0726720958613276 	 ± 0.2500060633533595
	data : 0.1161221981048584
	model : 0.06996831893920899
			 train-loss:  2.0723607346998247 	 ± 0.24934135058770152
	data : 0.11588115692138672
	model : 0.07006926536560058
			 train-loss:  2.0753804279698267 	 ± 0.25190858273371325
	data : 0.11596307754516602
	model : 0.07000341415405273
			 train-loss:  2.075903658708815 	 ± 0.25130980165382066
	data : 0.11606426239013672
	model : 0.06924853324890137
			 train-loss:  2.074909493163392 	 ± 0.25097509063701384
	data : 0.11684284210205079
	model : 0.06901898384094238
			 train-loss:  2.07327389521677 	 ± 0.25125918802091807
	data : 0.11693229675292968
	model : 0.06877660751342773
			 train-loss:  2.0734788250664007 	 ± 0.25059082313740755
	data : 0.11728057861328126
	model : 0.0687016487121582
			 train-loss:  2.074196237486762 	 ± 0.25010202982695545
	data : 0.11739554405212402
	model : 0.07002854347229004
			 train-loss:  2.0746069165968124 	 ± 0.2494913447292417
	data : 0.11625685691833496
	model : 0.06937408447265625
			 train-loss:  2.075907355961315 	 ± 0.2494546404974278
	data : 0.11683244705200195
	model : 0.06881804466247558
			 train-loss:  2.074504183327898 	 ± 0.24952916216851073
	data : 0.11739125251770019
	model : 0.06810226440429687
			 train-loss:  2.0743407717457525 	 ± 0.24887824252131774
	data : 0.11799817085266114
	model : 0.0677417278289795
			 train-loss:  2.0740735737900984 	 ± 0.24824961472252288
	data : 0.11826786994934083
	model : 0.0663874626159668
			 train-loss:  2.075408262108009 	 ± 0.24828144685103765
	data : 0.11940226554870606
	model : 0.06693239212036133
			 train-loss:  2.0803204495459795 	 ± 0.25677103974131243
	data : 0.11886229515075683
	model : 0.0683985710144043
			 train-loss:  2.0791206217800395 	 ± 0.2566440223156098
	data : 0.1173668384552002
	model : 0.06935877799987793
			 train-loss:  2.0792177282657818 	 ± 0.25598526882977035
	data : 0.11649432182312011
	model : 0.06888127326965332
			 train-loss:  2.079424194800548 	 ± 0.2553442468504736
	data : 0.11710953712463379
	model : 0.07042069435119629
			 train-loss:  2.0778141052139047 	 ± 0.2556825054625257
	data : 0.11569719314575196
	model : 0.07061419486999512
			 train-loss:  2.0780116598013088 	 ± 0.2550477360267145
	data : 0.11557564735412598
	model : 0.06989779472351074
			 train-loss:  2.0774627520580484 	 ± 0.25451949166319815
	data : 0.11642608642578126
	model : 0.06972475051879883
			 train-loss:  2.0777350867812956 	 ± 0.25390810944847975
	data : 0.11668105125427246
	model : 0.0705798625946045
			 train-loss:  2.0796336477994917 	 ± 0.2546846749561223
	data : 0.11578240394592285
	model : 0.0693901538848877
			 train-loss:  2.0810991276555986 	 ± 0.25489429577835504
	data : 0.11676044464111328
	model : 0.06916894912719726
			 train-loss:  2.0798798286088624 	 ± 0.25484953918865694
	data : 0.11691312789916992
	model : 0.06921577453613281
			 train-loss:  2.0783148217083784 	 ± 0.25519226699492764
	data : 0.11668014526367188
	model : 0.06938233375549316
			 train-loss:  2.0779660174659655 	 ± 0.25461453249651317
	data : 0.11626219749450684
	model : 0.06912426948547364
			 train-loss:  2.0784499685938767 	 ± 0.2540867998405555
	data : 0.11663756370544434
	model : 0.06981859207153321
			 train-loss:  2.079001143719386 	 ± 0.25359215499384047
	data : 0.11621856689453125
	model : 0.06972379684448242
			 train-loss:  2.078636470624214 	 ± 0.25303301144382306
	data : 0.11629862785339355
	model : 0.0696709156036377
			 train-loss:  2.078223304106639 	 ± 0.25249401032842334
	data : 0.11645889282226562
	model : 0.06866812705993652
			 train-loss:  2.081048039157995 	 ± 0.2551623885752237
	data : 0.11734199523925781
	model : 0.067985200881958
			 train-loss:  2.081112698146275 	 ± 0.25455585039920176
	data : 0.11795101165771485
	model : 0.06795554161071778
			 train-loss:  2.0791945451808767 	 ± 0.25546865460709034
	data : 0.11798553466796875
	model : 0.06820998191833497
			 train-loss:  2.077988571715805 	 ± 0.2554667411239181
	data : 0.11798839569091797
	model : 0.06853570938110351
			 train-loss:  2.0791476433265936 	 ± 0.2554244826209532
	data : 0.11764860153198242
	model : 0.06949238777160645
			 train-loss:  2.0802773560318992 	 ± 0.2553598248397853
	data : 0.11683592796325684
	model : 0.06966595649719239
			 train-loss:  2.0823508905809978 	 ± 0.2565647031143878
	data : 0.1166562557220459
	model : 0.06914467811584472
			 train-loss:  2.082822248891548 	 ± 0.2560634059293404
	data : 0.11707849502563476
	model : 0.0680772304534912
			 train-loss:  2.0828161832923713 	 ± 0.25547273233175005
	data : 0.11764383316040039
	model : 0.06763958930969238
			 train-loss:  2.081480976638444 	 ± 0.2556438787795771
	data : 0.11819157600402833
	model : 0.06693434715270996
			 train-loss:  2.0827807988206 	 ± 0.255780556008847
	data : 0.11875958442687988
	model : 0.06766333580017089
			 train-loss:  2.08138403784145 	 ± 0.2560343112557969
	data : 0.11789922714233399
	model : 0.06725935935974121
			 train-loss:  2.081942761106189 	 ± 0.2555887787622749
	data : 0.11851725578308106
	model : 0.06813106536865235
			 train-loss:  2.0833767824344807 	 ± 0.2559019977588333
	data : 0.11780905723571777
	model : 0.0674018383026123
			 train-loss:  2.0846212936623747 	 ± 0.25600001842781506
	data : 0.1183011531829834
	model : 0.06744589805603027
			 train-loss:  2.085147121122905 	 ± 0.2555486181569346
	data : 0.11829485893249511
	model : 0.06692242622375488
			 train-loss:  2.084961967468262 	 ± 0.2549951578166101
	data : 0.11864490509033203
	model : 0.06728978157043457
			 train-loss:  2.0847949601907647 	 ± 0.2544427160872225
	data : 0.11792678833007812
	model : 0.06703920364379883
			 train-loss:  2.0851385351844822 	 ± 0.25393418556433706
	data : 0.11821985244750977
	model : 0.06701221466064453
			 train-loss:  2.0862733878587423 	 ± 0.25395295523102723
	data : 0.11807503700256347
	model : 0.0673853874206543
			 train-loss:  2.0871359489890686 	 ± 0.2537323653635785
	data : 0.11758184432983398
	model : 0.06755905151367188
			 train-loss:  2.0902773991875025 	 ± 0.2576046081462387
	data : 0.11777200698852539
	model : 0.06732549667358398
			 train-loss:  2.090422741778485 	 ± 0.2570558683594541
	data : 0.11800956726074219
	model : 0.06736102104187011
			 train-loss:  2.0911020081618736 	 ± 0.25670895144711325
	data : 0.11791501045227051
	model : 0.06785874366760254
			 train-loss:  2.088531998605687 	 ± 0.2591312477060637
	data : 0.11776423454284668
	model : 0.06758575439453125
			 train-loss:  2.086961875613938 	 ± 0.2596852973713944
	data : 0.11826844215393066
	model : 0.0675973892211914
			 train-loss:  2.0859295621831366 	 ± 0.25961289847919766
	data : 0.11801600456237793
	model : 0.06836810111999511
			 train-loss:  2.087310800107859 	 ± 0.2599261548161581
	data : 0.11746563911437988
	model : 0.06777544021606445
			 train-loss:  2.0860834091524536 	 ± 0.26006166036102124
	data : 0.1179018497467041
	model : 0.06749629974365234
			 train-loss:  2.085425718491819 	 ± 0.2597121769390105
	data : 0.11796622276306153
	model : 0.06735396385192871
			 train-loss:  2.087329809635753 	 ± 0.2608276817277868
	data : 0.11810932159423829
	model : 0.06702709197998047
			 train-loss:  2.0869773983955384 	 ± 0.260340736370748
	data : 0.11821470260620118
	model : 0.0667417049407959
			 train-loss:  2.088249727898119 	 ± 0.2605466997974618
	data : 0.11823859214782714
	model : 0.06722760200500488
			 train-loss:  2.0881600705060093 	 ± 0.26001154827516837
	data : 0.11784720420837402
	model : 0.067634916305542
			 train-loss:  2.090847813052896 	 ± 0.262823114551842
	data : 0.11751594543457031
	model : 0.06748647689819336
			 train-loss:  2.0913796405323217 	 ± 0.2624149794859768
	data : 0.11728844642639161
	model : 0.06793785095214844
			 train-loss:  2.0898861053038615 	 ± 0.26291601624991995
	data : 0.11706814765930176
	model : 0.06789484024047851
			 train-loss:  2.0904233412044806 	 ± 0.262515806841193
	data : 0.11745638847351074
	model : 0.06740427017211914
			 train-loss:  2.089742690928069 	 ± 0.2622012788515129
	data : 0.11799311637878418
	model : 0.06719074249267579
			 train-loss:  2.087969344469809 	 ± 0.26315214231498685
	data : 0.11800203323364258
	model : 0.06762704849243165
			 train-loss:  2.0876350900734284 	 ± 0.26267593993127897
	data : 0.1179579734802246
	model : 0.06709280014038085
			 train-loss:  2.0869373769760133 	 ± 0.26238115197002176
	data : 0.11853137016296386
	model : 0.06687774658203124
			 train-loss:  2.0878141030847313 	 ± 0.2622246226317334
	data : 0.11860413551330566
	model : 0.06719346046447754
			 train-loss:  2.0880450729339843 	 ± 0.2617293998219714
	data : 0.11854910850524902
	model : 0.06688237190246582
			 train-loss:  2.0868827326024473 	 ± 0.26186252035726004
	data : 0.11912856101989747
	model : 0.06703453063964844
			 train-loss:  2.0885695941804903 	 ± 0.2627202374126906
	data : 0.11902313232421875
	model : 0.0676539421081543
			 train-loss:  2.0883245655134615 	 ± 0.26223367223884747
	data : 0.11837224960327149
	model : 0.06791510581970214
			 train-loss:  2.090377951040864 	 ± 0.2637670557734097
	data : 0.11694459915161133
	model : 0.05952286720275879
#epoch  88    val-loss:  2.4519299833398116  train-loss:  2.090377951040864  lr:  2.44140625e-06
			 train-loss:  2.3548290729522705 	 ± 0.0
	data : 5.941798686981201
	model : 0.07694053649902344
			 train-loss:  2.1532406210899353 	 ± 0.2015884518623352
	data : 3.037079691886902
	model : 0.07338225841522217
			 train-loss:  2.117798844973246 	 ± 0.17205863808049002
	data : 2.0635239283243814
	model : 0.07088724772135417
			 train-loss:  2.047565758228302 	 ± 0.19235693547349256
	data : 1.5774555206298828
	model : 0.06960892677307129
			 train-loss:  2.00327730178833 	 ± 0.1935118135803183
	data : 1.2856832981109618
	model : 0.06954655647277833
			 train-loss:  2.0545615752538047 	 ± 0.21060880447018604
	data : 0.12061767578125
	model : 0.06738071441650391
			 train-loss:  2.0416469063077654 	 ± 0.19753536226985113
	data : 0.117926025390625
	model : 0.06736574172973633
			 train-loss:  2.015640050172806 	 ± 0.19717298975992523
	data : 0.1178159236907959
	model : 0.06815605163574219
			 train-loss:  2.0100824965371027 	 ± 0.18655988734652365
	data : 0.1172968864440918
	model : 0.06874847412109375
			 train-loss:  2.0078910112380983 	 ± 0.1771083168019351
	data : 0.11704444885253906
	model : 0.07021465301513671
			 train-loss:  1.979832941835577 	 ± 0.1907572558490796
	data : 0.11558313369750976
	model : 0.07080798149108887
			 train-loss:  2.0008629063765206 	 ± 0.19550146614627278
	data : 0.11511144638061524
	model : 0.07064719200134277
			 train-loss:  2.015171316953806 	 ± 0.19426149205701299
	data : 0.11526384353637695
	model : 0.07043681144714356
			 train-loss:  2.0279211572238376 	 ± 0.19275696921784355
	data : 0.11529111862182617
	model : 0.0697291374206543
			 train-loss:  2.0156941175460816 	 ± 0.1917582842013665
	data : 0.11587533950805665
	model : 0.06841316223144531
			 train-loss:  2.0239728167653084 	 ± 0.18841732934843786
	data : 0.1173490047454834
	model : 0.06828198432922364
			 train-loss:  2.039387541658738 	 ± 0.19291088741630333
	data : 0.11736826896667481
	model : 0.06756424903869629
			 train-loss:  2.046654376718733 	 ± 0.18985481106349
	data : 0.11797213554382324
	model : 0.06695866584777832
			 train-loss:  2.0513592832966854 	 ± 0.18586608776091934
	data : 0.11848125457763672
	model : 0.06710968017578126
			 train-loss:  2.0406054615974427 	 ± 0.1871260040513218
	data : 0.11830692291259766
	model : 0.06717448234558106
			 train-loss:  2.0476004623231434 	 ± 0.18527629851353133
	data : 0.11801762580871582
	model : 0.06748781204223633
			 train-loss:  2.0363236611539666 	 ± 0.18824841513492016
	data : 0.11794795989990234
	model : 0.06840229034423828
			 train-loss:  2.022558735764545 	 ± 0.19510283488367106
	data : 0.11724891662597656
	model : 0.06833362579345703
			 train-loss:  2.0342362175385156 	 ± 0.19903625806755026
	data : 0.11729469299316406
	model : 0.06903619766235351
			 train-loss:  2.0266350507736206 	 ± 0.1985383600542149
	data : 0.11669154167175293
	model : 0.06909103393554687
			 train-loss:  2.028811514377594 	 ± 0.19498679011548553
	data : 0.11678133010864258
	model : 0.06893839836120605
			 train-loss:  2.0347075241583363 	 ± 0.19368929243478314
	data : 0.11684913635253906
	model : 0.06911826133728027
			 train-loss:  2.0411848042692458 	 ± 0.19315405891200862
	data : 0.11660985946655274
	model : 0.0701167106628418
			 train-loss:  2.0454819736809564 	 ± 0.19115184911829275
	data : 0.1156911849975586
	model : 0.07003731727600097
			 train-loss:  2.0506728370984395 	 ± 0.19000649620872012
	data : 0.11558070182800292
	model : 0.06914052963256836
			 train-loss:  2.0472701749494 	 ± 0.18784359486336077
	data : 0.11637420654296875
	model : 0.06931953430175782
			 train-loss:  2.0466251596808434 	 ± 0.18492011927501972
	data : 0.11617093086242676
	model : 0.06923699378967285
			 train-loss:  2.0369903174313633 	 ± 0.1900783803200447
	data : 0.11634445190429688
	model : 0.06944098472595214
			 train-loss:  2.0327151522916904 	 ± 0.18886580416590945
	data : 0.11657099723815918
	model : 0.0695568561553955
			 train-loss:  2.029307988711766 	 ± 0.1872053384606515
	data : 0.11660833358764648
	model : 0.07006206512451171
			 train-loss:  2.0242546598116555 	 ± 0.18699226690274856
	data : 0.11607060432434083
	model : 0.0699648380279541
			 train-loss:  2.0297981790594153 	 ± 0.18742299683362643
	data : 0.11614322662353516
	model : 0.06966748237609863
			 train-loss:  2.028488140357168 	 ± 0.18511205920710194
	data : 0.11634831428527832
	model : 0.06923494338989258
			 train-loss:  2.025025392190004 	 ± 0.18396600408381683
	data : 0.1165236473083496
	model : 0.06934700012207032
			 train-loss:  2.0250621259212496 	 ± 0.18165201902928624
	data : 0.11651263236999512
	model : 0.06965270042419433
			 train-loss:  2.028498376288065 	 ± 0.18073448116042776
	data : 0.11634411811828613
	model : 0.06957101821899414
			 train-loss:  2.0309639487947737 	 ± 0.17926644044820292
	data : 0.11643290519714355
	model : 0.0695620059967041
			 train-loss:  2.032815789067468 	 ± 0.1775756974760869
	data : 0.116447114944458
	model : 0.06973958015441895
			 train-loss:  2.0303868678483097 	 ± 0.1762672732895896
	data : 0.11595449447631836
	model : 0.06970314979553223
			 train-loss:  2.0352721929550173 	 ± 0.17728458895347715
	data : 0.11594862937927246
	model : 0.06967248916625976
			 train-loss:  2.0294838573621665 	 ± 0.17959478195882542
	data : 0.11607723236083985
	model : 0.06930065155029297
			 train-loss:  2.0351943716089775 	 ± 0.1818463171612767
	data : 0.11635713577270508
	model : 0.06955447196960449
			 train-loss:  2.033222739895185 	 ± 0.1804490766222615
	data : 0.11608309745788574
	model : 0.06966261863708496
			 train-loss:  2.035798510726617 	 ± 0.17948760928800062
	data : 0.11637248992919921
	model : 0.06944112777709961
			 train-loss:  2.0315196204185484 	 ± 0.18019051601379932
	data : 0.11661882400512695
	model : 0.06936349868774414
			 train-loss:  2.022569275369831 	 ± 0.18930772894314993
	data : 0.11658749580383301
	model : 0.0691904067993164
			 train-loss:  2.025201543019368 	 ± 0.1884186976869315
	data : 0.11682887077331543
	model : 0.06821990013122559
			 train-loss:  2.019394334757103 	 ± 0.19127308945038715
	data : 0.11766023635864258
	model : 0.06797962188720703
			 train-loss:  2.023363616731432 	 ± 0.1916844099177377
	data : 0.11776723861694335
	model : 0.0681882381439209
			 train-loss:  2.02541973807595 	 ± 0.19053386137607514
	data : 0.11752095222473144
	model : 0.06866812705993652
			 train-loss:  2.0203339053051814 	 ± 0.19255517385158125
	data : 0.11720223426818847
	model : 0.06876621246337891
			 train-loss:  2.021933168695684 	 ± 0.1912334715444926
	data : 0.11703896522521973
	model : 0.06973605155944824
			 train-loss:  2.023766918429013 	 ± 0.19008258631302072
	data : 0.11619801521301269
	model : 0.06989407539367676
			 train-loss:  2.0225076230905823 	 ± 0.18870869354824077
	data : 0.11604704856872558
	model : 0.06959147453308105
			 train-loss:  2.026642632484436 	 ± 0.18980583419788236
	data : 0.11620092391967773
	model : 0.06907196044921875
			 train-loss:  2.019834876060486 	 ± 0.19549013583286687
	data : 0.11653695106506348
	model : 0.0689094066619873
			 train-loss:  2.024194561666058 	 ± 0.1968741175026829
	data : 0.11677780151367187
	model : 0.06795773506164551
			 train-loss:  2.0232126201902116 	 ± 0.1954583593630604
	data : 0.11763834953308105
	model : 0.06780719757080078
			 train-loss:  2.0236348006874323 	 ± 0.19395427837836784
	data : 0.11778969764709472
	model : 0.0681429386138916
			 train-loss:  2.0237023225197426 	 ± 0.19245729759110028
	data : 0.11765079498291016
	model : 0.06838641166687012
			 train-loss:  2.0312762567491243 	 ± 0.2005175636040582
	data : 0.11759009361267089
	model : 0.06895737648010254
			 train-loss:  2.036599990147263 	 ± 0.20366090817757213
	data : 0.11706700325012206
	model : 0.06895852088928223
			 train-loss:  2.0347206259475037 	 ± 0.20274230697120268
	data : 0.11702690124511719
	model : 0.06883807182312011
			 train-loss:  2.0354965406915415 	 ± 0.20136947494887905
	data : 0.11714835166931152
	model : 0.0684288501739502
			 train-loss:  2.03727537052972 	 ± 0.20047123674543302
	data : 0.11751003265380859
	model : 0.06827845573425292
			 train-loss:  2.0379665418409965 	 ± 0.19913844114759394
	data : 0.11756563186645508
	model : 0.06753568649291992
			 train-loss:  2.0406721184651055 	 ± 0.19906046719880274
	data : 0.11832294464111329
	model : 0.0684584140777588
			 train-loss:  2.0375235864560897 	 ± 0.19948938493873067
	data : 0.11749601364135742
	model : 0.06873965263366699
			 train-loss:  2.040489618842666 	 ± 0.19975093775346087
	data : 0.11718330383300782
	model : 0.06838617324829102
			 train-loss:  2.0365269788106284 	 ± 0.20132167665376116
	data : 0.11761245727539063
	model : 0.06759505271911621
			 train-loss:  2.030475897224326 	 ± 0.2067445081519833
	data : 0.11817607879638672
	model : 0.06854701042175293
			 train-loss:  2.034810358827764 	 ± 0.20884453147974466
	data : 0.11730175018310547
	model : 0.06757254600524902
			 train-loss:  2.02969718285096 	 ± 0.2122969385796447
	data : 0.11817574501037598
	model : 0.06690511703491211
			 train-loss:  2.0294283583194397 	 ± 0.21096236802603172
	data : 0.11875195503234863
	model : 0.06746606826782227
			 train-loss:  2.034119674563408 	 ± 0.21374628444509697
	data : 0.11809325218200684
	model : 0.06816306114196777
			 train-loss:  2.0361381106906467 	 ± 0.2131885502923794
	data : 0.11748723983764649
	model : 0.06752252578735352
			 train-loss:  2.037211357093439 	 ± 0.21210468641163235
	data : 0.11809186935424805
	model : 0.06850943565368653
			 train-loss:  2.0368911329522192 	 ± 0.2108430166576267
	data : 0.11743240356445313
	model : 0.06836457252502441
			 train-loss:  2.0389414571580433 	 ± 0.21041499938342018
	data : 0.11754612922668457
	model : 0.06848287582397461
			 train-loss:  2.0462920609642476 	 ± 0.21975493495091303
	data : 0.117498779296875
	model : 0.0684882640838623
			 train-loss:  2.046381160270336 	 ± 0.21847509846385363
	data : 0.11751360893249511
	model : 0.06897058486938476
			 train-loss:  2.0461873213450112 	 ± 0.2172233034253728
	data : 0.11707568168640137
	model : 0.06807403564453125
			 train-loss:  2.048718888651241 	 ± 0.21727247385029996
	data : 0.11789321899414062
	model : 0.06859235763549805
			 train-loss:  2.0487276784489663 	 ± 0.21604840956931093
	data : 0.11746354103088379
	model : 0.0686483383178711
			 train-loss:  2.045225547419654 	 ± 0.21737033128026778
	data : 0.11728034019470215
	model : 0.06854739189147949
			 train-loss:  2.0480381328981 	 ± 0.2178131975262871
	data : 0.11736130714416504
	model : 0.06884751319885254
			 train-loss:  2.0489009709461876 	 ± 0.21678251139212074
	data : 0.11697087287902833
	model : 0.06963748931884765
			 train-loss:  2.0481609541882753 	 ± 0.21573066502503885
	data : 0.11616401672363282
	model : 0.0696420669555664
			 train-loss:  2.045766291466165 	 ± 0.21581917551059626
	data : 0.1162940502166748
	model : 0.0696143627166748
			 train-loss:  2.046038428105806 	 ± 0.21469649308954317
	data : 0.11638846397399902
	model : 0.06980156898498535
			 train-loss:  2.042665533721447 	 ± 0.21609070270822764
	data : 0.11634292602539062
	model : 0.06947650909423828
			 train-loss:  2.0407787032963074 	 ± 0.21576739896513697
	data : 0.11661720275878906
	model : 0.06964759826660157
			 train-loss:  2.043871972025657 	 ± 0.21681475923860066
	data : 0.11652779579162598
	model : 0.06888079643249512
			 train-loss:  2.0444169887388597 	 ± 0.21578441860646513
	data : 0.11695709228515624
	model : 0.06855154037475586
			 train-loss:  2.0431033718585967 	 ± 0.21510025453657622
	data : 0.11715359687805176
	model : 0.06790180206298828
			 train-loss:  2.0448800868327073 	 ± 0.21476892447124804
	data : 0.11767349243164063
	model : 0.06787986755371093
			 train-loss:  2.0431344041637347 	 ± 0.214432428677494
	data : 0.11783108711242676
	model : 0.06820025444030761
			 train-loss:  2.0444723842213457 	 ± 0.21381638349399906
	data : 0.11750845909118653
	model : 0.06894235610961914
			 train-loss:  2.045935795857356 	 ± 0.21330362656893137
	data : 0.1171377182006836
	model : 0.06937608718872071
			 train-loss:  2.045465824717567 	 ± 0.21233956164014295
	data : 0.11684150695800781
	model : 0.06989374160766601
			 train-loss:  2.0501916464769616 	 ± 0.2168126753441233
	data : 0.11633939743041992
	model : 0.06997427940368653
			 train-loss:  2.049966510211196 	 ± 0.21580960197459453
	data : 0.11614093780517579
	model : 0.06956233978271484
			 train-loss:  2.0502245547594846 	 ± 0.21482474338093788
	data : 0.1164560317993164
	model : 0.06960201263427734
			 train-loss:  2.053116720750791 	 ± 0.2159390152392814
	data : 0.11643762588500976
	model : 0.06909298896789551
			 train-loss:  2.0541727380319075 	 ± 0.21523778971227636
	data : 0.11688399314880371
	model : 0.06858434677124023
			 train-loss:  2.0543328158490293 	 ± 0.21427263414130754
	data : 0.11719069480895997
	model : 0.06909089088439942
			 train-loss:  2.054129439805235 	 ± 0.21332467632846774
	data : 0.11699104309082031
	model : 0.06917324066162109
			 train-loss:  2.05253196290109 	 ± 0.2130504950873473
	data : 0.11687335968017579
	model : 0.069514799118042
			 train-loss:  2.0569193959236145 	 ± 0.21718092066704445
	data : 0.11655664443969727
	model : 0.06982474327087403
			 train-loss:  2.056548988300821 	 ± 0.21627075779927937
	data : 0.11620635986328125
	model : 0.07027606964111328
			 train-loss:  2.0569618909523406 	 ± 0.21538205821309525
	data : 0.11598572731018067
	model : 0.06983761787414551
			 train-loss:  2.0548392014625745 	 ± 0.21567478799010345
	data : 0.1161677360534668
	model : 0.06974844932556153
			 train-loss:  2.055905049130068 	 ± 0.21506819781347913
	data : 0.11628284454345703
	model : 0.06870489120483399
			 train-loss:  2.0569795760787835 	 ± 0.21448049241766476
	data : 0.11715826988220215
	model : 0.06885757446289062
			 train-loss:  2.057723649342855 	 ± 0.21373913158758265
	data : 0.11711406707763672
	model : 0.06914987564086914
			 train-loss:  2.054732820219245 	 ± 0.21536078165095976
	data : 0.11672811508178711
	model : 0.06920385360717773
			 train-loss:  2.054473160720262 	 ± 0.21449535739969486
	data : 0.11673364639282227
	model : 0.06915020942687988
			 train-loss:  2.0553599499105437 	 ± 0.21384608404985314
	data : 0.11678671836853027
	model : 0.0699317455291748
			 train-loss:  2.0556965960610296 	 ± 0.21301477840189925
	data : 0.11613259315490723
	model : 0.06977052688598633
			 train-loss:  2.055613059043884 	 ± 0.21216304762326146
	data : 0.11632084846496582
	model : 0.06956768035888672
			 train-loss:  2.0549488512296525 	 ± 0.21144989474452772
	data : 0.11660709381103515
	model : 0.06864829063415527
			 train-loss:  2.0539931772262094 	 ± 0.2108887854431225
	data : 0.11731576919555664
	model : 0.0687939167022705
			 train-loss:  2.0586131187155843 	 ± 0.21641926027140893
	data : 0.11708922386169433
	model : 0.06894006729125976
			 train-loss:  2.0582792028900263 	 ± 0.21561189292904986
	data : 0.11684918403625488
	model : 0.06901917457580567
			 train-loss:  2.058477716262524 	 ± 0.21479284948725763
	data : 0.11690621376037598
	model : 0.06890959739685058
			 train-loss:  2.0609198371872646 	 ± 0.21577557629862584
	data : 0.11684665679931641
	model : 0.06905760765075683
			 train-loss:  2.0597407510786345 	 ± 0.21537989889567705
	data : 0.11678943634033204
	model : 0.06895613670349121
			 train-loss:  2.0618590053759123 	 ± 0.21594443566578783
	data : 0.1170649528503418
	model : 0.06896386146545411
			 train-loss:  2.0643550548980487 	 ± 0.21705442706411646
	data : 0.11717400550842286
	model : 0.06894865036010742
			 train-loss:  2.0664906907964635 	 ± 0.21765754744182356
	data : 0.1171614646911621
	model : 0.06899094581604004
			 train-loss:  2.065385300446959 	 ± 0.2172358594674692
	data : 0.1172985553741455
	model : 0.06978783607482911
			 train-loss:  2.0680934573612073 	 ± 0.21873361702174954
	data : 0.11666288375854492
	model : 0.06981534957885742
			 train-loss:  2.067641732485398 	 ± 0.21800378960961403
	data : 0.11651301383972168
	model : 0.06978883743286132
			 train-loss:  2.0719809060474095 	 ± 0.22311894601403678
	data : 0.1164982795715332
	model : 0.0698997974395752
			 train-loss:  2.072432767493384 	 ± 0.2223844840518487
	data : 0.11633353233337403
	model : 0.07017703056335449
			 train-loss:  2.071943412435816 	 ± 0.22167011670516545
	data : 0.11598362922668456
	model : 0.07007222175598145
			 train-loss:  2.0734924036012568 	 ± 0.22165268461983695
	data : 0.1160700798034668
	model : 0.06999154090881347
			 train-loss:  2.0738623217269256 	 ± 0.22092029778724762
	data : 0.11610870361328125
	model : 0.0696530818939209
			 train-loss:  2.073024513820807 	 ± 0.2203797264251305
	data : 0.11643595695495605
	model : 0.0695958137512207
			 train-loss:  2.0700171125346216 	 ± 0.22256387857757592
	data : 0.1164963722229004
	model : 0.06914782524108887
			 train-loss:  2.067717243547309 	 ± 0.2235226253292004
	data : 0.1170687198638916
	model : 0.06931099891662598
			 train-loss:  2.0650110163656223 	 ± 0.22514826043790556
	data : 0.11703662872314453
	model : 0.06932530403137208
			 train-loss:  2.0665262937545776 	 ± 0.22513717832720634
	data : 0.11717586517333985
	model : 0.06997318267822265
			 train-loss:  2.0646919572113345 	 ± 0.22548738000868934
	data : 0.11654915809631347
	model : 0.07000522613525391
			 train-loss:  2.0643240427970886 	 ± 0.2247793665152056
	data : 0.11646318435668945
	model : 0.07038869857788085
			 train-loss:  2.064218837693827 	 ± 0.22403753284217137
	data : 0.11607871055603028
	model : 0.07071242332458497
			 train-loss:  2.0620866049277153 	 ± 0.22483129036761834
	data : 0.11563844680786133
	model : 0.07067952156066895
			 train-loss:  2.0648904711592433 	 ± 0.22674588420196232
	data : 0.11550464630126953
	model : 0.07023158073425292
			 train-loss:  2.0633744380690833 	 ± 0.22678511713126664
	data : 0.11592063903808594
	model : 0.07003521919250488
			 train-loss:  2.063078716493422 	 ± 0.22608215494473086
	data : 0.11624526977539062
	model : 0.0695920467376709
			 train-loss:  2.065336694320043 	 ± 0.22710295978970807
	data : 0.11665520668029786
	model : 0.06914372444152832
			 train-loss:  2.066011156246161 	 ± 0.22653523033425005
	data : 0.11699786186218261
	model : 0.06848134994506835
			 train-loss:  2.065879475466813 	 ± 0.22582323648016625
	data : 0.11773953437805176
	model : 0.0687601089477539
			 train-loss:  2.062531698424861 	 ± 0.22901136715977638
	data : 0.11739211082458496
	model : 0.06874122619628906
			 train-loss:  2.061399537324905 	 ± 0.22874051217773317
	data : 0.11749639511108398
	model : 0.06842989921569824
			 train-loss:  2.064835108585239 	 ± 0.23213302899008975
	data : 0.11763176918029786
	model : 0.06839723587036133
			 train-loss:  2.065357933809728 	 ± 0.23151052642301798
	data : 0.11773982048034667
	model : 0.06834278106689454
			 train-loss:  2.065183184629569 	 ± 0.23080999582484718
	data : 0.11789121627807617
	model : 0.06823468208312988
			 train-loss:  2.062834917045221 	 ± 0.23205011967795655
	data : 0.11807184219360352
	model : 0.06849908828735352
			 train-loss:  2.0617557727929317 	 ± 0.23175827407914282
	data : 0.11774625778198242
	model : 0.06934733390808105
			 train-loss:  2.0634770364646453 	 ± 0.23211459557613529
	data : 0.11707453727722168
	model : 0.06938562393188477
			 train-loss:  2.0648670010937904 	 ± 0.23211049067117961
	data : 0.11685876846313477
	model : 0.06961822509765625
			 train-loss:  2.0636283648865565 	 ± 0.2319715680251624
	data : 0.11645855903625488
	model : 0.06952600479125977
			 train-loss:  2.0614930236127953 	 ± 0.23293438530546265
	data : 0.11645112037658692
	model : 0.06911983489990234
			 train-loss:  2.06037498922909 	 ± 0.23270262307115
	data : 0.1168795108795166
	model : 0.06859822273254394
			 train-loss:  2.0600821497844675 	 ± 0.23205262220095801
	data : 0.11719045639038086
	model : 0.06944236755371094
			 train-loss:  2.0617461953052256 	 ± 0.23239805159637023
	data : 0.11642923355102539
	model : 0.07001218795776368
			 train-loss:  2.060007308259865 	 ± 0.2328448962699563
	data : 0.11591134071350098
	model : 0.06929969787597656
			 train-loss:  2.0600676858562164 	 ± 0.23217619578650908
	data : 0.11656036376953124
	model : 0.0690831184387207
			 train-loss:  2.059793722288949 	 ± 0.23154008849001598
	data : 0.11670565605163574
	model : 0.06952934265136719
			 train-loss:  2.059838715602051 	 ± 0.23088213434132596
	data : 0.11653752326965332
	model : 0.06795411109924317
			 train-loss:  2.0611904644023227 	 ± 0.23092636168429084
	data : 0.11820259094238281
	model : 0.0679255485534668
			 train-loss:  2.0611906152093007 	 ± 0.23027677851683404
	data : 0.11827778816223145
	model : 0.06887292861938477
			 train-loss:  2.0614545684952974 	 ± 0.22965964772568204
	data : 0.11763839721679688
	model : 0.0692026138305664
			 train-loss:  2.0617587506771087 	 ± 0.229056971837443
	data : 0.11716198921203613
	model : 0.06911277770996094
			 train-loss:  2.0639514442306854 	 ± 0.2303098907785728
	data : 0.1172140121459961
	model : 0.06887226104736328
			 train-loss:  2.063672962424519 	 ± 0.22970685582881858
	data : 0.11728367805480958
	model : 0.0689420223236084
			 train-loss:  2.0652000050727137 	 ± 0.23000283336396007
	data : 0.11722984313964843
	model : 0.06801457405090332
			 train-loss:  2.063881572821866 	 ± 0.23006933413823277
	data : 0.11783223152160645
	model : 0.06748690605163574
			 train-loss:  2.063622289734918 	 ± 0.22947363691894923
	data : 0.11854085922241211
	model : 0.06769895553588867
			 train-loss:  2.0632398538692023 	 ± 0.22891504818963587
	data : 0.11823711395263672
	model : 0.06947712898254395
			 train-loss:  2.0637389415088183 	 ± 0.22840360026404583
	data : 0.11648592948913575
	model : 0.06896810531616211
			 train-loss:  2.0622163554455373 	 ± 0.22874490232432332
	data : 0.11702985763549804
	model : 0.06960248947143555
			 train-loss:  2.060613135812144 	 ± 0.22919555283655846
	data : 0.11655268669128419
	model : 0.0704343318939209
			 train-loss:  2.0620733179544146 	 ± 0.22947134356454596
	data : 0.11577367782592773
	model : 0.07088618278503418
			 train-loss:  2.06310645011083 	 ± 0.2293124608417022
	data : 0.11529908180236817
	model : 0.06996350288391114
			 train-loss:  2.0643385630100965 	 ± 0.22934752266088365
	data : 0.11636290550231934
	model : 0.07033109664916992
			 train-loss:  2.064885257439292 	 ± 0.22887798008835683
	data : 0.1159287929534912
	model : 0.07033591270446778
			 train-loss:  2.064619090753732 	 ± 0.22831727125906442
	data : 0.11590771675109864
	model : 0.07014803886413574
			 train-loss:  2.065722447786576 	 ± 0.22824904000868648
	data : 0.11599864959716796
	model : 0.06952500343322754
			 train-loss:  2.0650874272901185 	 ± 0.22783865784910912
	data : 0.11667404174804688
	model : 0.06969294548034669
			 train-loss:  2.065256764440972 	 ± 0.22727201649485979
	data : 0.11640453338623047
	model : 0.06891884803771972
			 train-loss:  2.0664822688006392 	 ± 0.22734899279061496
	data : 0.1171152114868164
	model : 0.06910929679870606
			 train-loss:  2.067444648574944 	 ± 0.2271810085242715
	data : 0.11679096221923828
	model : 0.06932764053344727
			 train-loss:  2.068196832537651 	 ± 0.22686062889574196
	data : 0.116511869430542
	model : 0.06929941177368164
			 train-loss:  2.0670354016384676 	 ± 0.22689090067119014
	data : 0.11664009094238281
	model : 0.06930279731750488
			 train-loss:  2.066477358341217 	 ± 0.22646683149887242
	data : 0.1166457176208496
	model : 0.07006120681762695
			 train-loss:  2.0663741744797806 	 ± 0.22591310279054003
	data : 0.11614069938659669
	model : 0.06901664733886718
			 train-loss:  2.0671894392546486 	 ± 0.22565787215152944
	data : 0.1172722339630127
	model : 0.06800823211669922
			 train-loss:  2.068499919263328 	 ± 0.22588364001336245
	data : 0.11828479766845704
	model : 0.06709022521972656
			 train-loss:  2.0704918607924747 	 ± 0.22713242325330432
	data : 0.11917228698730468
	model : 0.06681489944458008
			 train-loss:  2.073309722729927 	 ± 0.23016434576322298
	data : 0.11934776306152343
	model : 0.06670160293579101
			 train-loss:  2.0732581804578123 	 ± 0.2296115969808784
	data : 0.11923823356628419
	model : 0.06743640899658203
			 train-loss:  2.074051646524639 	 ± 0.229347299839814
	data : 0.11868495941162109
	model : 0.06831212043762207
			 train-loss:  2.0746056948389326 	 ± 0.22894074235517767
	data : 0.11766586303710938
	model : 0.06923351287841797
			 train-loss:  2.0760199283536576 	 ± 0.22931521629680154
	data : 0.11682229042053223
	model : 0.06945042610168457
			 train-loss:  2.0765466538240327 	 ± 0.22890164600549837
	data : 0.11662969589233399
	model : 0.06963992118835449
			 train-loss:  2.078769646340133 	 ± 0.2306460764871148
	data : 0.11653656959533691
	model : 0.06976609230041504
			 train-loss:  2.077900342295103 	 ± 0.23045604242995024
	data : 0.11619915962219238
	model : 0.06906542778015137
			 train-loss:  2.0799890290859135 	 ± 0.2319408617988608
	data : 0.11703267097473144
	model : 0.0687178134918213
			 train-loss:  2.0802603634419263 	 ± 0.23143753803084946
	data : 0.11746740341186523
	model : 0.0687746524810791
			 train-loss:  2.0816578694752286 	 ± 0.2318153389265166
	data : 0.11738753318786621
	model : 0.06804118156433106
			 train-loss:  2.0823170203681385 	 ± 0.23148677565020367
	data : 0.11797528266906739
	model : 0.06734566688537598
			 train-loss:  2.0839046446700076 	 ± 0.23214418340665988
	data : 0.11847233772277832
	model : 0.06799154281616211
			 train-loss:  2.0838587246157907 	 ± 0.2316169789724739
	data : 0.11775732040405273
	model : 0.06741576194763184
			 train-loss:  2.083274301900044 	 ± 0.23125488519033308
	data : 0.11816606521606446
	model : 0.06673073768615723
			 train-loss:  2.08311614367339 	 ± 0.23074543208824216
	data : 0.1188176155090332
	model : 0.06665163040161133
			 train-loss:  2.0853214263916016 	 ± 0.23256040268881342
	data : 0.11871204376220704
	model : 0.06654963493347169
			 train-loss:  2.084291880152055 	 ± 0.23254948988501406
	data : 0.11892786026000976
	model : 0.06588358879089355
			 train-loss:  2.0845845874150593 	 ± 0.23207348992326907
	data : 0.1195033073425293
	model : 0.06588172912597656
			 train-loss:  2.086397515461508 	 ± 0.2331508178199828
	data : 0.11928324699401856
	model : 0.06567559242248536
			 train-loss:  2.0879757598633284 	 ± 0.23384347087270083
	data : 0.11948089599609375
	model : 0.06567559242248536
			 train-loss:  2.088167595758773 	 ± 0.23334799333542536
	data : 0.1196329116821289
	model : 0.06575355529785157
			 train-loss:  2.0873417797046976 	 ± 0.2331716044109808
	data : 0.11926474571228027
	model : 0.06573424339294434
			 train-loss:  2.087829047182332 	 ± 0.2327809733136759
	data : 0.1192744255065918
	model : 0.06608071327209472
			 train-loss:  2.0879270942696246 	 ± 0.2322813313687061
	data : 0.11901731491088867
	model : 0.06683273315429687
			 train-loss:  2.0862929497299523 	 ± 0.23310710596957304
	data : 0.11835932731628418
	model : 0.06715083122253418
			 train-loss:  2.0874292845378104 	 ± 0.2332493947878613
	data : 0.11811957359313965
	model : 0.06789298057556152
			 train-loss:  2.086285802034231 	 ± 0.23340402447812827
	data : 0.11789941787719727
	model : 0.06849889755249024
			 train-loss:  2.0865581867542673 	 ± 0.23294415858463124
	data : 0.11730823516845704
	model : 0.06825747489929199
			 train-loss:  2.0850408314648323 	 ± 0.2336110215625708
	data : 0.11751799583435059
	model : 0.06820588111877442
			 train-loss:  2.0843018907031934 	 ± 0.23339387906657558
	data : 0.11775760650634766
	model : 0.06867799758911133
			 train-loss:  2.08571970512887 	 ± 0.23392358313988562
	data : 0.11739640235900879
	model : 0.06817164421081542
			 train-loss:  2.0863891660418967 	 ± 0.23366205105230548
	data : 0.11777210235595703
	model : 0.06735191345214844
			 train-loss:  2.0864539509018263 	 ± 0.23317689792988824
	data : 0.11849679946899414
	model : 0.06736874580383301
			 train-loss:  2.0861057132111545 	 ± 0.2327551560417164
	data : 0.11826906204223633
	model : 0.06712770462036133
			 train-loss:  2.0863417155486492 	 ± 0.2323026521239418
	data : 0.11825180053710938
	model : 0.06627144813537597
			 train-loss:  2.0862190433980996 	 ± 0.23183202472332504
	data : 0.11886100769042969
	model : 0.06641430854797363
			 train-loss:  2.085023122244194 	 ± 0.23210635944731065
	data : 0.1185715675354004
	model : 0.06689376831054687
			 train-loss:  2.0852556846579726 	 ± 0.23166067346323863
	data : 0.11825895309448242
	model : 0.06695466041564942
			 train-loss:  2.0842307113050444 	 ± 0.23174533476563852
	data : 0.11846346855163574
	model : 0.06661076545715332
			 train-loss:  2.0838502090469544 	 ± 0.23135272584431174
	data : 0.11870040893554687
	model : 0.06725401878356933
			 train-loss:  2.084815779039937 	 ± 0.23138397786758438
	data : 0.11791324615478516
	model : 0.0667327880859375
			 train-loss:  2.0852129124254586 	 ± 0.23100355901509934
	data : 0.11825618743896485
	model : 0.06702208518981934
			 train-loss:  2.0868554668426516 	 ± 0.23199351708372865
	data : 0.11798219680786133
	model : 0.0677337646484375
			 train-loss:  2.0856632340951746 	 ± 0.23229705214895247
	data : 0.11730537414550782
	model : 0.06766819953918457
			 train-loss:  2.08410411316251 	 ± 0.23314787412913476
	data : 0.11770453453063964
	model : 0.0676084041595459
			 train-loss:  2.0836890723865493 	 ± 0.2327799109824699
	data : 0.11818461418151856
	model : 0.06830601692199707
			 train-loss:  2.0839389534447137 	 ± 0.2323552275904905
	data : 0.11784801483154297
	model : 0.06763420104980469
			 train-loss:  2.0856261028962977 	 ± 0.2334528530991822
	data : 0.11850085258483886
	model : 0.06752314567565917
			 train-loss:  2.0831383913755417 	 ± 0.2363587607311001
	data : 0.11745181083679199
	model : 0.059317588806152344
#epoch  89    val-loss:  2.434039260211744  train-loss:  2.0831383913755417  lr:  1.220703125e-06
			 train-loss:  2.346991777420044 	 ± 0.0
	data : 5.874178647994995
	model : 0.0761263370513916
			 train-loss:  2.2987093925476074 	 ± 0.04828238487243652
	data : 3.0036368370056152
	model : 0.07273805141448975
			 train-loss:  2.2559708754221597 	 ± 0.07216153743380943
	data : 2.0412420431772866
	model : 0.07181413968404134
			 train-loss:  2.2413827180862427 	 ± 0.06740852035893333
	data : 1.5599027276039124
	model : 0.07120567560195923
			 train-loss:  2.2731327056884765 	 ± 0.08756354118326078
	data : 1.2711346626281739
	model : 0.0707930564880371
			 train-loss:  2.2912031014760337 	 ± 0.0895665905349494
	data : 0.11950688362121582
	model : 0.0686685562133789
			 train-loss:  2.272691454206194 	 ± 0.09451050613959397
	data : 0.11679596900939941
	model : 0.06883206367492675
			 train-loss:  2.234008029103279 	 ± 0.1352425892360348
	data : 0.11654648780822754
	model : 0.06871848106384278
			 train-loss:  2.215289447042677 	 ± 0.13806287061416703
	data : 0.11662755012512208
	model : 0.06825418472290039
			 train-loss:  2.1981808066368105 	 ± 0.1406754093892644
	data : 0.11720304489135742
	model : 0.06839561462402344
			 train-loss:  2.1811668547717007 	 ± 0.14451735015557518
	data : 0.11719651222229004
	model : 0.06919498443603515
			 train-loss:  2.1446252167224884 	 ± 0.18393757150866738
	data : 0.116511869430542
	model : 0.06950106620788574
			 train-loss:  2.1654592018861036 	 ± 0.1908904101323627
	data : 0.11624588966369628
	model : 0.06993021965026855
			 train-loss:  2.176816846643175 	 ± 0.1884497322198734
	data : 0.11592345237731934
	model : 0.0695526123046875
			 train-loss:  2.152970616022746 	 ± 0.2027479867784895
	data : 0.11613740921020507
	model : 0.06966056823730468
			 train-loss:  2.14816877245903 	 ± 0.19718884358138988
	data : 0.11622776985168456
	model : 0.0689021110534668
			 train-loss:  2.128156241248636 	 ± 0.20737454362025318
	data : 0.1171048641204834
	model : 0.06772103309631347
			 train-loss:  2.1075285540686712 	 ± 0.2187432380158359
	data : 0.11818094253540039
	model : 0.06643586158752442
			 train-loss:  2.088940745905826 	 ± 0.2270448757640931
	data : 0.1192744255065918
	model : 0.0673703670501709
			 train-loss:  2.0726856112480165 	 ± 0.232362358144177
	data : 0.118560791015625
	model : 0.06640963554382324
			 train-loss:  2.0795909677233015 	 ± 0.228855596790367
	data : 0.11930532455444336
	model : 0.06637377738952636
			 train-loss:  2.094791369004683 	 ± 0.2341928773681019
	data : 0.11928558349609375
	model : 0.06715970039367676
			 train-loss:  2.091951981834743 	 ± 0.2294320128800433
	data : 0.11849374771118164
	model : 0.06731834411621093
			 train-loss:  2.0925662418206534 	 ± 0.2246206426352229
	data : 0.11823153495788574
	model : 0.06670808792114258
			 train-loss:  2.0961503887176516 	 ± 0.220781707551709
	data : 0.1187476634979248
	model : 0.06766891479492188
			 train-loss:  2.0984978675842285 	 ± 0.2168122181844035
	data : 0.1178919792175293
	model : 0.06861162185668945
			 train-loss:  2.0790513621436224 	 ± 0.23473144005428231
	data : 0.11697063446044922
	model : 0.06784205436706543
			 train-loss:  2.0826930574008395 	 ± 0.23127711822267225
	data : 0.11813111305236816
	model : 0.0686417579650879
			 train-loss:  2.0748086016753624 	 ± 0.23105251238844793
	data : 0.11751489639282227
	model : 0.06928811073303223
			 train-loss:  2.073621594905853 	 ± 0.22725891680056995
	data : 0.11690058708190917
	model : 0.06917014122009277
			 train-loss:  2.0719103082533805 	 ± 0.22375980532326706
	data : 0.11688485145568847
	model : 0.06986780166625976
			 train-loss:  2.0686257742345333 	 ± 0.22099376569534535
	data : 0.11615796089172363
	model : 0.0706183910369873
			 train-loss:  2.06564157298117 	 ± 0.21827338889969683
	data : 0.11518545150756836
	model : 0.07048416137695312
			 train-loss:  2.0714070200920105 	 ± 0.21757511838397667
	data : 0.11542258262634278
	model : 0.07055635452270508
			 train-loss:  2.0733885935374667 	 ± 0.2147554347493737
	data : 0.11528806686401367
	model : 0.06969265937805176
			 train-loss:  2.074415018161138 	 ± 0.2118387657488536
	data : 0.11609072685241699
	model : 0.06943306922912598
			 train-loss:  2.0713383152678206 	 ± 0.20977031856539352
	data : 0.1163710117340088
	model : 0.06963458061218261
			 train-loss:  2.0634151665787948 	 ± 0.2125283980595395
	data : 0.11615142822265626
	model : 0.0696190357208252
			 train-loss:  2.0898901590934167 	 ± 0.2657918671058478
	data : 0.11623558998107911
	model : 0.06922273635864258
			 train-loss:  2.0816407054662704 	 ± 0.2674570425784921
	data : 0.1164787769317627
	model : 0.07014198303222656
			 train-loss:  2.08267768708671 	 ± 0.2642566346693871
	data : 0.11566805839538574
	model : 0.06958045959472656
			 train-loss:  2.0955347305252436 	 ± 0.2737633261925578
	data : 0.11610417366027832
	model : 0.06850438117980957
			 train-loss:  2.0949412417966267 	 ± 0.27058864315524095
	data : 0.11702437400817871
	model : 0.06833648681640625
			 train-loss:  2.0963662293824283 	 ± 0.26765925884321684
	data : 0.11708393096923828
	model : 0.06788010597229004
			 train-loss:  2.1013938983281455 	 ± 0.26676141775472156
	data : 0.11760077476501465
	model : 0.06790218353271485
			 train-loss:  2.1004312686298205 	 ± 0.26392491553719977
	data : 0.11765789985656738
	model : 0.0678786277770996
			 train-loss:  2.1089557257104428 	 ± 0.2674265576906815
	data : 0.11772308349609376
	model : 0.06874761581420899
			 train-loss:  2.1078070824344954 	 ± 0.2647433435196548
	data : 0.11692514419555664
	model : 0.06883130073547364
			 train-loss:  2.1194540700133966 	 ± 0.27417140761291575
	data : 0.11682615280151368
	model : 0.06932554244995118
			 train-loss:  2.1201242804527283 	 ± 0.27145638960432505
	data : 0.1164179801940918
	model : 0.06925687789916993
			 train-loss:  2.122482040349175 	 ± 0.2692984387131537
	data : 0.1166104793548584
	model : 0.06923227310180664
			 train-loss:  2.1191624861497145 	 ± 0.26774800361465667
	data : 0.11666879653930665
	model : 0.07021565437316894
			 train-loss:  2.1208909052722857 	 ± 0.2655027634891873
	data : 0.1157259464263916
	model : 0.07026309967041015
			 train-loss:  2.1195275915993586 	 ± 0.2632201024370963
	data : 0.115657377243042
	model : 0.07056117057800293
			 train-loss:  2.1138531251387165 	 ± 0.26412851993638614
	data : 0.11533541679382324
	model : 0.07066750526428223
			 train-loss:  2.1111773380211423 	 ± 0.2625107285032039
	data : 0.11516356468200684
	model : 0.07079510688781739
			 train-loss:  2.1097729247913026 	 ± 0.2604099753624722
	data : 0.11511697769165039
	model : 0.06983718872070313
			 train-loss:  2.106780672895497 	 ± 0.2591418782405671
	data : 0.11616072654724122
	model : 0.07002172470092774
			 train-loss:  2.1077450857324114 	 ± 0.25704133192013995
	data : 0.11609392166137696
	model : 0.06979198455810547
			 train-loss:  2.112462325890859 	 ± 0.2574528393150752
	data : 0.11635093688964844
	model : 0.06899747848510743
			 train-loss:  2.1103547342488023 	 ± 0.2558552157796977
	data : 0.116912841796875
	model : 0.06899452209472656
			 train-loss:  2.10539496714069 	 ± 0.2567228294458996
	data : 0.117108154296875
	model : 0.06865439414978028
			 train-loss:  2.1072363626389277 	 ± 0.25508959247932333
	data : 0.1174741268157959
	model : 0.06906003952026367
			 train-loss:  2.104447338730097 	 ± 0.25405516276138257
	data : 0.11703081130981445
	model : 0.06913981437683106
			 train-loss:  2.10193405518165 	 ± 0.25289385667315245
	data : 0.11698861122131347
	model : 0.06995868682861328
			 train-loss:  2.0966568805954675 	 ± 0.2545514489437033
	data : 0.11634926795959473
	model : 0.06917276382446289
			 train-loss:  2.0917990723652626 	 ± 0.2557084608789466
	data : 0.11684207916259766
	model : 0.06871037483215332
			 train-loss:  2.0917813970762142 	 ± 0.25382132900766924
	data : 0.11726179122924804
	model : 0.06791415214538574
			 train-loss:  2.0928690485332324 	 ± 0.2521349054844945
	data : 0.11779985427856446
	model : 0.0677973747253418
			 train-loss:  2.0912103687013897 	 ± 0.2507063483048018
	data : 0.11792454719543458
	model : 0.06770405769348145
			 train-loss:  2.093137180301505 	 ± 0.24945599268239865
	data : 0.11791863441467285
	model : 0.06849465370178223
			 train-loss:  2.093795875708262 	 ± 0.24777977298849013
	data : 0.11727075576782227
	model : 0.06945590972900391
			 train-loss:  2.0923395891712135 	 ± 0.24638686349168437
	data : 0.11640243530273438
	model : 0.06987514495849609
			 train-loss:  2.092401789652335 	 ± 0.24471700187209355
	data : 0.11628937721252441
	model : 0.0691911220550537
			 train-loss:  2.0940167156855267 	 ± 0.24347672680577584
	data : 0.11679892539978028
	model : 0.06879243850708008
			 train-loss:  2.087602659275657 	 ± 0.24816610044296877
	data : 0.11714849472045899
	model : 0.06876912117004394
			 train-loss:  2.083550847970046 	 ± 0.24906684875225715
	data : 0.11706056594848632
	model : 0.06863884925842285
			 train-loss:  2.079048740558135 	 ± 0.25059867208258846
	data : 0.11708006858825684
	model : 0.06875152587890625
			 train-loss:  2.078439335279827 	 ± 0.24906571189606777
	data : 0.11702942848205566
	model : 0.06952576637268067
			 train-loss:  2.0748920872807504 	 ± 0.2495042318934804
	data : 0.11626839637756348
	model : 0.06981363296508789
			 train-loss:  2.0698871656700417 	 ± 0.25196776238913904
	data : 0.1159475326538086
	model : 0.06973509788513184
			 train-loss:  2.0732913235338724 	 ± 0.2522938048018659
	data : 0.1161879062652588
	model : 0.06949434280395508
			 train-loss:  2.07568895816803 	 ± 0.2517074886663686
	data : 0.11641550064086914
	model : 0.06963887214660644
			 train-loss:  2.073751661039534 	 ± 0.2508264796103964
	data : 0.11612372398376465
	model : 0.06906003952026367
			 train-loss:  2.071372452904196 	 ± 0.25029832622040354
	data : 0.11697196960449219
	model : 0.06917200088500977
			 train-loss:  2.0677713294361912 	 ± 0.2510439405816381
	data : 0.11680150032043457
	model : 0.06931858062744141
			 train-loss:  2.06865520860957 	 ± 0.24973154378027532
	data : 0.11657371520996093
	model : 0.06955099105834961
			 train-loss:  2.0652330233292147 	 ± 0.25035181009674695
	data : 0.11639962196350098
	model : 0.06923012733459473
			 train-loss:  2.066092872887515 	 ± 0.24907200928034193
	data : 0.11672210693359375
	model : 0.06954741477966309
			 train-loss:  2.066746411058638 	 ± 0.24776113552883924
	data : 0.11638908386230469
	model : 0.06938700675964356
			 train-loss:  2.070579338859726 	 ± 0.2490647178141535
	data : 0.11691055297851563
	model : 0.0690577507019043
			 train-loss:  2.072466802337895 	 ± 0.24836092396679904
	data : 0.11712379455566406
	model : 0.069087553024292
			 train-loss:  2.072360598912803 	 ± 0.2470241416849477
	data : 0.11711225509643555
	model : 0.06817121505737304
			 train-loss:  2.071842567717775 	 ± 0.24575745140178584
	data : 0.11786227226257324
	model : 0.06832370758056641
			 train-loss:  2.0726752243543927 	 ± 0.24459383010483263
	data : 0.11759018898010254
	model : 0.06853923797607422
			 train-loss:  2.0706125125288963 	 ± 0.2441457696872403
	data : 0.11722526550292969
	model : 0.06906309127807617
			 train-loss:  2.0724701709354045 	 ± 0.2435650549257177
	data : 0.11681280136108399
	model : 0.06953668594360352
			 train-loss:  2.071781194939905 	 ± 0.2424141795025986
	data : 0.11629085540771485
	model : 0.07050156593322754
			 train-loss:  2.0747371659134375 	 ± 0.24295545511237987
	data : 0.1156245231628418
	model : 0.0707921028137207
			 train-loss:  2.073498408794403 	 ± 0.2420516412627083
	data : 0.11525120735168456
	model : 0.07085108757019043
			 train-loss:  2.070869541404271 	 ± 0.242280833608185
	data : 0.11532092094421387
	model : 0.07040367126464844
			 train-loss:  2.074881797912074 	 ± 0.24443901191430706
	data : 0.1158510684967041
	model : 0.06993341445922852
			 train-loss:  2.0718534745058967 	 ± 0.24516473039316705
	data : 0.11625261306762695
	model : 0.069968843460083
			 train-loss:  2.0738295236459146 	 ± 0.24480603853601401
	data : 0.11620874404907226
	model : 0.06984138488769531
			 train-loss:  2.075559574081784 	 ± 0.24427548870351382
	data : 0.11664772033691406
	model : 0.06981825828552246
			 train-loss:  2.0762257767173478 	 ± 0.24321633729111583
	data : 0.11659646034240723
	model : 0.06978011131286621
			 train-loss:  2.078215814082422 	 ± 0.24294264801067686
	data : 0.11640791893005371
	model : 0.06973652839660645
			 train-loss:  2.0782364551667816 	 ± 0.24181539211504394
	data : 0.11653647422790528
	model : 0.06975255012512208
			 train-loss:  2.076244566418709 	 ± 0.24159205682084015
	data : 0.11636514663696289
	model : 0.06963410377502441
			 train-loss:  2.0761848146265205 	 ± 0.24049221300528992
	data : 0.11632833480834961
	model : 0.07008142471313476
			 train-loss:  2.0783419802382186 	 ± 0.24047312720621852
	data : 0.1158571720123291
	model : 0.07043070793151855
			 train-loss:  2.0756299953375543 	 ± 0.24109624468379767
	data : 0.1154240608215332
	model : 0.07055907249450684
			 train-loss:  2.072221551321249 	 ± 0.24272238542493027
	data : 0.11523709297180176
	model : 0.06963801383972168
			 train-loss:  2.0703486800193787 	 ± 0.24247418345482624
	data : 0.11619572639465332
	model : 0.06979398727416992
			 train-loss:  2.0712310469668846 	 ± 0.24160140099373567
	data : 0.11605639457702636
	model : 0.06904621124267578
			 train-loss:  2.071222072017604 	 ± 0.24055778080684467
	data : 0.11692247390747071
	model : 0.06895565986633301
			 train-loss:  2.069007402811295 	 ± 0.24071227502597753
	data : 0.11719541549682617
	model : 0.06887907981872558
			 train-loss:  2.0693259421041454 	 ± 0.23971490094997772
	data : 0.1173243522644043
	model : 0.06941614151000977
			 train-loss:  2.0681825826148024 	 ± 0.23902846564607516
	data : 0.11673321723937988
	model : 0.06961159706115723
			 train-loss:  2.0710390508174896 	 ± 0.24006135782636245
	data : 0.11648020744323731
	model : 0.06978979110717773
			 train-loss:  2.0712219506255853 	 ± 0.2390757062319388
	data : 0.1162712574005127
	model : 0.06886520385742187
			 train-loss:  2.0724002787324247 	 ± 0.238446419638042
	data : 0.11720223426818847
	model : 0.06800055503845215
			 train-loss:  2.0751925231964607 	 ± 0.23946948498915616
	data : 0.11811022758483887
	model : 0.06804571151733399
			 train-loss:  2.0755798720544383 	 ± 0.23854061344486457
	data : 0.11797676086425782
	model : 0.06803150177001953
			 train-loss:  2.0738180952072143 	 ± 0.23839314087934352
	data : 0.11812820434570312
	model : 0.0680534839630127
			 train-loss:  2.072309007720342 	 ± 0.23804393663271534
	data : 0.11802449226379394
	model : 0.06902008056640625
			 train-loss:  2.0707065326961005 	 ± 0.23778623599801377
	data : 0.1170562744140625
	model : 0.06980676651000976
			 train-loss:  2.071755576878786 	 ± 0.23715041652407157
	data : 0.11623921394348144
	model : 0.06978631019592285
			 train-loss:  2.0739173020503316 	 ± 0.2374921045341087
	data : 0.1164320945739746
	model : 0.06954636573791503
			 train-loss:  2.0810744872459996 	 ± 0.2501533490160498
	data : 0.11649513244628906
	model : 0.0695080280303955
			 train-loss:  2.083345844545437 	 ± 0.25053880263110445
	data : 0.11655726432800292
	model : 0.06941938400268555
			 train-loss:  2.084109927668716 	 ± 0.2497411549434853
	data : 0.1167677879333496
	model : 0.0694432258605957
			 train-loss:  2.0825858286448886 	 ± 0.2494159420176903
	data : 0.11662378311157226
	model : 0.06897511482238769
			 train-loss:  2.0805521714153574 	 ± 0.2495879149858323
	data : 0.11706781387329102
	model : 0.06920514106750489
			 train-loss:  2.0806191877082543 	 ± 0.2486630071914884
	data : 0.11687145233154297
	model : 0.06913671493530274
			 train-loss:  2.0798749774694443 	 ± 0.24789797133440375
	data : 0.11690607070922851
	model : 0.0691401481628418
			 train-loss:  2.0839455467070978 	 ± 0.25151201921684146
	data : 0.11680564880371094
	model : 0.06895942687988281
			 train-loss:  2.0835988340170486 	 ± 0.2506319434750563
	data : 0.11705937385559081
	model : 0.0696725845336914
			 train-loss:  2.0859840250701356 	 ± 0.2512957531852625
	data : 0.11627039909362794
	model : 0.06923823356628418
			 train-loss:  2.0882026868207113 	 ± 0.25175922872817696
	data : 0.1166724681854248
	model : 0.06943025588989257
			 train-loss:  2.0857432708672596 	 ± 0.25254704164900166
	data : 0.11649413108825683
	model : 0.06936964988708497
			 train-loss:  2.084271080057386 	 ± 0.25226265877670534
	data : 0.11660928726196289
	model : 0.06951384544372559
			 train-loss:  2.0839045097777893 	 ± 0.25141702399689897
	data : 0.116463041305542
	model : 0.06951837539672852
			 train-loss:  2.083307884633541 	 ± 0.25064409132232757
	data : 0.11642928123474121
	model : 0.06894097328186036
			 train-loss:  2.083643108400805 	 ± 0.249810696653873
	data : 0.11694488525390626
	model : 0.06888608932495117
			 train-loss:  2.0834981129594046 	 ± 0.2489598330211254
	data : 0.11708106994628906
	model : 0.06887130737304688
			 train-loss:  2.083643889751564 	 ± 0.24811783821420838
	data : 0.1170417308807373
	model : 0.06899862289428711
			 train-loss:  2.0852172012264663 	 ± 0.24801284136917293
	data : 0.11682872772216797
	model : 0.06897072792053223
			 train-loss:  2.0874939212863075 	 ± 0.24872615350717858
	data : 0.11684370040893555
	model : 0.06993155479431153
			 train-loss:  2.086108076572418 	 ± 0.24847219639019583
	data : 0.11594109535217285
	model : 0.06994528770446777
			 train-loss:  2.087271708526359 	 ± 0.2480578047290864
	data : 0.11581053733825683
	model : 0.06985888481140137
			 train-loss:  2.087070155300592 	 ± 0.24725288358126515
	data : 0.11595377922058106
	model : 0.06998319625854492
			 train-loss:  2.0870949544158637 	 ± 0.24644373265873512
	data : 0.11593012809753418
	model : 0.0703585147857666
			 train-loss:  2.0858322320046363 	 ± 0.24613834930111736
	data : 0.1157869815826416
	model : 0.07039036750793456
			 train-loss:  2.0881845751116352 	 ± 0.24707364024842862
	data : 0.11579079627990722
	model : 0.07003216743469239
			 train-loss:  2.093025710338201 	 ± 0.2535482964586604
	data : 0.11608476638793945
	model : 0.06965231895446777
			 train-loss:  2.0921114424991 	 ± 0.2529973658569847
	data : 0.11631035804748535
	model : 0.06932024955749512
			 train-loss:  2.091150425657441 	 ± 0.2524827780114383
	data : 0.1166046142578125
	model : 0.06891016960144043
			 train-loss:  2.090934580976858 	 ± 0.2517021776528831
	data : 0.11697916984558106
	model : 0.0688478946685791
			 train-loss:  2.0900043308734895 	 ± 0.2511884091608319
	data : 0.11709532737731934
	model : 0.06915726661682128
			 train-loss:  2.0914324914446527 	 ± 0.25105788470764856
	data : 0.11683411598205566
	model : 0.06879315376281739
			 train-loss:  2.0948936056207725 	 ± 0.2541056003831846
	data : 0.11722893714904785
	model : 0.06890082359313965
			 train-loss:  2.0922504765855754 	 ± 0.25554897089202416
	data : 0.11730022430419922
	model : 0.06921305656433105
			 train-loss:  2.094886845931774 	 ± 0.25698248166180443
	data : 0.11731138229370117
	model : 0.06916532516479493
			 train-loss:  2.0944966901432385 	 ± 0.2562512786587226
	data : 0.11733293533325195
	model : 0.06904106140136719
			 train-loss:  2.0936376761241133 	 ± 0.25571644758212525
	data : 0.11758961677551269
	model : 0.06938753128051758
			 train-loss:  2.0955613787302716 	 ± 0.25615160382909835
	data : 0.11716365814208984
	model : 0.068473482131958
			 train-loss:  2.096672534942627 	 ± 0.2557914703738532
	data : 0.1177340030670166
	model : 0.06825094223022461
			 train-loss:  2.0977694960035516 	 ± 0.2554295973345317
	data : 0.1175511360168457
	model : 0.06836495399475098
			 train-loss:  2.098855757713318 	 ± 0.2550684286965403
	data : 0.11744856834411621
	model : 0.06868386268615723
			 train-loss:  2.099618098889178 	 ± 0.2545156850539632
	data : 0.11702260971069336
	model : 0.06903209686279296
			 train-loss:  2.0976154714129693 	 ± 0.2551223523597231
	data : 0.11680140495300292
	model : 0.06994791030883789
			 train-loss:  2.0971544471090238 	 ± 0.2544557806562746
	data : 0.11605544090270996
	model : 0.06931648254394532
			 train-loss:  2.0969511249969743 	 ± 0.25373762558163815
	data : 0.11673917770385742
	model : 0.06918091773986816
			 train-loss:  2.097089059693473 	 ± 0.25301816444941755
	data : 0.11671662330627441
	model : 0.06905031204223633
			 train-loss:  2.09546793658625 	 ± 0.25320813343812787
	data : 0.11680717468261718
	model : 0.068389892578125
			 train-loss:  2.095126353414719 	 ± 0.252532505508263
	data : 0.11748442649841309
	model : 0.06819362640380859
			 train-loss:  2.0960736616273943 	 ± 0.25213732660259003
	data : 0.11774845123291015
	model : 0.06881494522094726
			 train-loss:  2.096227775738892 	 ± 0.2514404530996929
	data : 0.11689414978027343
	model : 0.06855621337890624
			 train-loss:  2.0950468553437127 	 ± 0.2512383231321825
	data : 0.11724629402160644
	model : 0.06853795051574707
			 train-loss:  2.0936510622172064 	 ± 0.2512422034810783
	data : 0.11723771095275878
	model : 0.07035541534423828
			 train-loss:  2.091510649565812 	 ± 0.25220040522349363
	data : 0.11564898490905762
	model : 0.07088837623596192
			 train-loss:  2.089871406555176 	 ± 0.25248075423328764
	data : 0.11522979736328125
	model : 0.07047567367553711
			 train-loss:  2.088094337478928 	 ± 0.2529387120481647
	data : 0.11587071418762207
	model : 0.07078509330749512
			 train-loss:  2.0881054987778533 	 ± 0.25225421302773443
	data : 0.11570839881896973
	model : 0.07026124000549316
			 train-loss:  2.0869059953638303 	 ± 0.25210366719686284
	data : 0.11627435684204102
	model : 0.06855158805847168
			 train-loss:  2.0867930395717926 	 ± 0.2514334089720138
	data : 0.11777362823486329
	model : 0.0673560619354248
			 train-loss:  2.0865659910313625 	 ± 0.25078303211391656
	data : 0.11887598037719727
	model : 0.0685579776763916
			 train-loss:  2.0870654955112116 	 ± 0.25021245671685266
	data : 0.1174652099609375
	model : 0.06884503364562988
			 train-loss:  2.086456096172333 	 ± 0.24969372291152925
	data : 0.11726932525634766
	model : 0.06891875267028809
			 train-loss:  2.0844838169856845 	 ± 0.25051868113575804
	data : 0.11708831787109375
	model : 0.06972918510437012
			 train-loss:  2.0840506913761296 	 ± 0.24993712771401316
	data : 0.11636080741882324
	model : 0.07062306404113769
			 train-loss:  2.0829091671217292 	 ± 0.24979008662838906
	data : 0.11539421081542969
	model : 0.06995372772216797
			 train-loss:  2.081002331886095 	 ± 0.2505498257731125
	data : 0.1162808895111084
	model : 0.06989421844482421
			 train-loss:  2.0815035642721713 	 ± 0.2500040607510035
	data : 0.11622800827026367
	model : 0.06937508583068848
			 train-loss:  2.0805232190355962 	 ± 0.24974097071663867
	data : 0.11678514480590821
	model : 0.06923747062683105
			 train-loss:  2.0815076979283753 	 ± 0.24948730149786
	data : 0.11694622039794922
	model : 0.06930680274963379
			 train-loss:  2.0830571982595654 	 ± 0.24980499956983018
	data : 0.11689896583557129
	model : 0.06933894157409667
			 train-loss:  2.0834654485760025 	 ± 0.2492427682914888
	data : 0.11692223548889161
	model : 0.06859908103942872
			 train-loss:  2.084321170449257 	 ± 0.24891176646118135
	data : 0.11747455596923828
	model : 0.06931376457214355
			 train-loss:  2.084102014404031 	 ± 0.24831115407977045
	data : 0.11665568351745606
	model : 0.06956849098205567
			 train-loss:  2.0854671078153175 	 ± 0.24845069727421085
	data : 0.11637315750122071
	model : 0.06874938011169433
			 train-loss:  2.0846961519401064 	 ± 0.248080097414336
	data : 0.11709375381469726
	model : 0.06937756538391113
			 train-loss:  2.08524129671209 	 ± 0.24759317008325588
	data : 0.11631641387939454
	model : 0.06964311599731446
			 train-loss:  2.087607717514038 	 ± 0.24929045540149197
	data : 0.11615080833435058
	model : 0.06899662017822265
			 train-loss:  2.0858902954360814 	 ± 0.2498973954635161
	data : 0.11667857170104981
	model : 0.06853733062744141
			 train-loss:  2.0852957134661465 	 ± 0.24943907149561304
	data : 0.11721539497375488
	model : 0.06934394836425781
			 train-loss:  2.0838730472784777 	 ± 0.24967915444099992
	data : 0.11650676727294922
	model : 0.06867408752441406
			 train-loss:  2.0839373962730883 	 ± 0.24908284859801347
	data : 0.1171783447265625
	model : 0.06884684562683105
			 train-loss:  2.0842804556801204 	 ± 0.24853857500497156
	data : 0.1170694351196289
	model : 0.06947779655456543
			 train-loss:  2.086557885481848 	 ± 0.2501357069080284
	data : 0.11666202545166016
	model : 0.06979022026062012
			 train-loss:  2.0881418383346415 	 ± 0.2506035109496781
	data : 0.11635613441467285
	model : 0.06971001625061035
			 train-loss:  2.0887497176586742 	 ± 0.25017116486457425
	data : 0.11641926765441894
	model : 0.06941113471984864
			 train-loss:  2.0873685217349327 	 ± 0.25039867444618946
	data : 0.11659307479858398
	model : 0.06874284744262696
			 train-loss:  2.08643070209858 	 ± 0.2501920958269938
	data : 0.11723847389221191
	model : 0.06889395713806153
			 train-loss:  2.087359827425745 	 ± 0.24998378349038555
	data : 0.11714091300964355
	model : 0.06811485290527344
			 train-loss:  2.0855829622338993 	 ± 0.25077056629430045
	data : 0.11768980026245117
	model : 0.06853837966918945
			 train-loss:  2.085380836911158 	 ± 0.25021245982653884
	data : 0.1176407814025879
	model : 0.0687443733215332
			 train-loss:  2.086107828301382 	 ± 0.2498712033095043
	data : 0.11747031211853028
	model : 0.06948733329772949
			 train-loss:  2.0858767579902304 	 ± 0.24932611807153604
	data : 0.11660933494567871
	model : 0.06895790100097657
			 train-loss:  2.084252936807693 	 ± 0.2499246391192875
	data : 0.11707444190979004
	model : 0.06948275566101074
			 train-loss:  2.0831122398376465 	 ± 0.24993704469351302
	data : 0.1165503978729248
	model : 0.0691864013671875
			 train-loss:  2.0823717577040464 	 ± 0.2496199592397894
	data : 0.11669964790344238
	model : 0.06856832504272461
			 train-loss:  2.0828730538487434 	 ± 0.24917462404732127
	data : 0.11736617088317872
	model : 0.06762514114379883
			 train-loss:  2.082219812605116 	 ± 0.24881244509827966
	data : 0.11802549362182617
	model : 0.06735672950744628
			 train-loss:  2.0806774729121047 	 ± 0.24933699664471903
	data : 0.11819338798522949
	model : 0.06681413650512695
			 train-loss:  2.082368388049928 	 ± 0.2500824739310008
	data : 0.11856889724731445
	model : 0.06580185890197754
			 train-loss:  2.082145832609712 	 ± 0.24955597285269768
	data : 0.11906042098999023
	model : 0.06592135429382324
			 train-loss:  2.0818694725827878 	 ± 0.24904545754137508
	data : 0.11871576309204102
	model : 0.06659760475158691
			 train-loss:  2.081392698184304 	 ± 0.24860817924743844
	data : 0.11839642524719238
	model : 0.06689386367797852
			 train-loss:  2.0814063192962053 	 ± 0.24806956867817967
	data : 0.11800823211669922
	model : 0.0666494369506836
			 train-loss:  2.0815540742257546 	 ± 0.2475445450478389
	data : 0.11812758445739746
	model : 0.06717534065246582
			 train-loss:  2.081927344011135 	 ± 0.24707818482569952
	data : 0.11768484115600586
	model : 0.06758279800415039
			 train-loss:  2.081806277107989 	 ± 0.24655660047544137
	data : 0.11755290031433105
	model : 0.06765403747558593
			 train-loss:  2.0823203020907464 	 ± 0.24615707097632405
	data : 0.11751341819763184
	model : 0.06815342903137207
			 train-loss:  2.08196493276095 	 ± 0.24569540051651223
	data : 0.11739225387573242
	model : 0.06840920448303223
			 train-loss:  2.084038361718383 	 ± 0.24723694992424036
	data : 0.11740689277648926
	model : 0.0682291030883789
			 train-loss:  2.0839875330444144 	 ± 0.24671823872120296
	data : 0.11774506568908691
	model : 0.06766438484191895
			 train-loss:  2.0862685222506023 	 ± 0.2487036320340164
	data : 0.11817498207092285
	model : 0.06771492958068848
			 train-loss:  2.08766769717137 	 ± 0.2491257949393125
	data : 0.11806159019470215
	model : 0.0675776481628418
			 train-loss:  2.086868595780179 	 ± 0.24891643430059868
	data : 0.11788463592529297
	model : 0.0679318904876709
			 train-loss:  2.086540726590748 	 ± 0.24845375382554266
	data : 0.11741576194763184
	model : 0.06821870803833008
			 train-loss:  2.0853213910703308 	 ± 0.2486665205176391
	data : 0.11722612380981445
	model : 0.06834778785705567
			 train-loss:  2.086329217816963 	 ± 0.24865324213101458
	data : 0.11708717346191407
	model : 0.06751060485839844
			 train-loss:  2.0879569384516503 	 ± 0.24944447404816603
	data : 0.11757550239562989
	model : 0.06713924407958985
			 train-loss:  2.0879864440700873 	 ± 0.24893738519609265
	data : 0.1179731845855713
	model : 0.06691970825195312
			 train-loss:  2.0882826440247446 	 ± 0.24847638609595854
	data : 0.11836819648742676
	model : 0.06692967414855958
			 train-loss:  2.0876032027506057 	 ± 0.24820472588155154
	data : 0.11811375617980957
	model : 0.06657371520996094
			 train-loss:  2.0891443773445832 	 ± 0.24889200128206515
	data : 0.11861076354980468
	model : 0.06679811477661132
			 train-loss:  2.0878970046043395 	 ± 0.24917236632200104
	data : 0.11878199577331543
	model : 0.06652069091796875
			 train-loss:  2.088903686914786 	 ± 0.24918439440834655
	data : 0.11882519721984863
	model : 0.06648049354553223
			 train-loss:  2.0862099064721003 	 ± 0.2523248538895678
	data : 0.1186863899230957
	model : 0.06596598625183106
			 train-loss:  2.0889033404263584 	 ± 0.2554297088640301
	data : 0.11945090293884278
	model : 0.06630167961120606
			 train-loss:  2.08972995206127 	 ± 0.25526523512662924
	data : 0.11894655227661133
	model : 0.06697511672973633
			 train-loss:  2.090216265472711 	 ± 0.2548820917691011
	data : 0.11849088668823242
	model : 0.0676717758178711
			 train-loss:  2.0932957157492638 	 ± 0.25909318827558686
	data : 0.11706333160400391
	model : 0.059402084350585936
#epoch  90    val-loss:  2.4806973118531075  train-loss:  2.0932957157492638  lr:  1.220703125e-06
			 train-loss:  2.0909688472747803 	 ± 0.0
	data : 5.388219833374023
	model : 0.07310605049133301
			 train-loss:  2.1228833198547363 	 ± 0.031914472579956055
	data : 2.7681851387023926
	model : 0.07055521011352539
			 train-loss:  2.1121660073598227 	 ± 0.03014538020324916
	data : 1.9514531294504802
	model : 0.06998721758524577
			 train-loss:  2.107721745967865 	 ± 0.02721786800355548
	data : 1.4925925135612488
	model : 0.0698540210723877
			 train-loss:  2.077726149559021 	 ± 0.06474251374959265
	data : 1.2172839641571045
	model : 0.06983318328857421
			 train-loss:  2.0862716237703958 	 ± 0.062113765675035335
	data : 0.16289200782775878
	model : 0.06836214065551757
			 train-loss:  2.111595034599304 	 ± 0.08458493198335817
	data : 0.1571506977081299
	model : 0.06864595413208008
			 train-loss:  2.1269553154706955 	 ± 0.08894859177039774
	data : 0.11664724349975586
	model : 0.06791296005249023
			 train-loss:  2.1392474042044745 	 ± 0.09078282258904735
	data : 0.11744732856750488
	model : 0.06726236343383789
			 train-loss:  2.1048465847969053 	 ± 0.13441769293036018
	data : 0.11813864707946778
	model : 0.06728172302246094
			 train-loss:  2.118201331658797 	 ± 0.134940923349704
	data : 0.11806468963623047
	model : 0.06810121536254883
			 train-loss:  2.1126025021076202 	 ± 0.1305237432195354
	data : 0.11745891571044922
	model : 0.068363618850708
			 train-loss:  2.1158060018832865 	 ± 0.1258932107338019
	data : 0.11736578941345215
	model : 0.06935281753540039
			 train-loss:  2.1211604986871992 	 ± 0.12284030352205165
	data : 0.11648406982421874
	model : 0.06997790336608886
			 train-loss:  2.0954391797383627 	 ± 0.15279386430737116
	data : 0.11586189270019531
	model : 0.06956343650817871
			 train-loss:  2.07630168646574 	 ± 0.16547053178965332
	data : 0.11624231338500976
	model : 0.06886577606201172
			 train-loss:  2.0730571816949284 	 ± 0.16105374384320909
	data : 0.11682324409484864
	model : 0.0684445858001709
			 train-loss:  2.0854975846078663 	 ± 0.16470662673695924
	data : 0.11740288734436036
	model : 0.06836299896240235
			 train-loss:  2.07329985969945 	 ± 0.16845946078401502
	data : 0.11753864288330078
	model : 0.06764593124389648
			 train-loss:  2.0595989346504213 	 ± 0.17471763513750432
	data : 0.118259859085083
	model : 0.06806440353393554
			 train-loss:  2.05033654826028 	 ± 0.17546639852659285
	data : 0.11785349845886231
	model : 0.0682438850402832
			 train-loss:  2.0797063383189114 	 ± 0.21795242054866115
	data : 0.11768550872802734
	model : 0.06755785942077637
			 train-loss:  2.0830187849376514 	 ± 0.21372713738123342
	data : 0.11811356544494629
	model : 0.06773276329040527
			 train-loss:  2.0808050284783044 	 ± 0.20949630555335375
	data : 0.11799192428588867
	model : 0.06782016754150391
			 train-loss:  2.070596890449524 	 ± 0.211267839311229
	data : 0.11782450675964355
	model : 0.06772761344909668
			 train-loss:  2.0727053651442895 	 ± 0.2074332312206736
	data : 0.11799964904785157
	model : 0.06816067695617675
			 train-loss:  2.0762736841484353 	 ± 0.20436719648191162
	data : 0.1175351619720459
	model : 0.0691525936126709
			 train-loss:  2.083490946463176 	 ± 0.20415854017023954
	data : 0.11676430702209473
	model : 0.0690112590789795
			 train-loss:  2.0813917825961936 	 ± 0.2009149704952454
	data : 0.11674008369445801
	model : 0.06965279579162598
			 train-loss:  2.0827727993329366 	 ± 0.19767795408961583
	data : 0.11626338958740234
	model : 0.07012929916381835
			 train-loss:  2.0722612988564277 	 ± 0.20280726853588646
	data : 0.11575360298156738
	model : 0.0701183795928955
			 train-loss:  2.0600642934441566 	 ± 0.21084882312329792
	data : 0.11586790084838867
	model : 0.06988725662231446
			 train-loss:  2.052333940159191 	 ± 0.21218460151600674
	data : 0.11605463027954102
	model : 0.06969327926635742
			 train-loss:  2.057353798080893 	 ± 0.21102058146794672
	data : 0.1164543628692627
	model : 0.06982507705688476
			 train-loss:  2.0472878762653894 	 ± 0.21610734289629768
	data : 0.11636748313903808
	model : 0.06946778297424316
			 train-loss:  2.0392715864711337 	 ± 0.2182984809639435
	data : 0.11665010452270508
	model : 0.0694129467010498
			 train-loss:  2.0411346087584623 	 ± 0.2156182399271437
	data : 0.1165952205657959
	model : 0.06884832382202148
			 train-loss:  2.0406099432393123 	 ± 0.21278617723931192
	data : 0.1169015884399414
	model : 0.06920433044433594
			 train-loss:  2.036045539073455 	 ± 0.21191665297176673
	data : 0.1163261890411377
	model : 0.0693014144897461
			 train-loss:  2.0303927093744276 	 ± 0.21220786029793823
	data : 0.11628146171569824
	model : 0.06941032409667969
			 train-loss:  2.0257955964018657 	 ± 0.21161088848192317
	data : 0.11632742881774902
	model : 0.06961269378662109
			 train-loss:  2.0311331209682284 	 ± 0.21185148893232897
	data : 0.11613831520080567
	model : 0.0695657730102539
			 train-loss:  2.0451558883800063 	 ± 0.22824571010856784
	data : 0.11624484062194824
	model : 0.06924800872802735
			 train-loss:  2.0442856306379493 	 ± 0.22570925502133707
	data : 0.11664743423461914
	model : 0.06905074119567871
			 train-loss:  2.0480302148395113 	 ± 0.22456519799474986
	data : 0.1167405128479004
	model : 0.06894450187683106
			 train-loss:  2.047041584616122 	 ± 0.22220984811104033
	data : 0.11687026023864747
	model : 0.06889801025390625
			 train-loss:  2.0441558056689324 	 ± 0.22070276991252827
	data : 0.11698088645935059
	model : 0.06874594688415528
			 train-loss:  2.048551251490911 	 ± 0.2204607999933114
	data : 0.11702208518981934
	model : 0.06881566047668457
			 train-loss:  2.0545340119575966 	 ± 0.22210166905467985
	data : 0.11690831184387207
	model : 0.06889572143554687
			 train-loss:  2.0586806964874267 	 ± 0.22177719225800407
	data : 0.1166761875152588
	model : 0.06887760162353515
			 train-loss:  2.0546122158274933 	 ± 0.22146858823153953
	data : 0.11657414436340333
	model : 0.06802048683166503
			 train-loss:  2.0517176871116343 	 ± 0.22030068422849203
	data : 0.11751365661621094
	model : 0.06909737586975098
			 train-loss:  2.0595418529690437 	 ± 0.2253885526885063
	data : 0.11665201187133789
	model : 0.06842174530029296
			 train-loss:  2.0564060586470143 	 ± 0.22445582832206112
	data : 0.11722898483276367
	model : 0.06855616569519044
			 train-loss:  2.0566047126596625 	 ± 0.22241075126097087
	data : 0.1172666072845459
	model : 0.06851005554199219
			 train-loss:  2.075284487434796 	 ± 0.2603355150092356
	data : 0.11739645004272461
	model : 0.0692901611328125
			 train-loss:  2.071170030978688 	 ± 0.2598722046646778
	data : 0.1165609359741211
	model : 0.06913814544677735
			 train-loss:  2.0695445064840645 	 ± 0.2579143342059769
	data : 0.1168828010559082
	model : 0.06987953186035156
			 train-loss:  2.0648294202351973 	 ± 0.2582282085884348
	data : 0.11627163887023925
	model : 0.06952567100524902
			 train-loss:  2.070403363307317 	 ± 0.25962185171870317
	data : 0.11653838157653809
	model : 0.06952404975891113
			 train-loss:  2.0740602895861766 	 ± 0.2590384468992936
	data : 0.11643800735473633
	model : 0.06962952613830567
			 train-loss:  2.0789542448136116 	 ± 0.25976844069568455
	data : 0.11634984016418456
	model : 0.06968436241149903
			 train-loss:  2.0766130723650496 	 ± 0.2583570491072267
	data : 0.11628184318542481
	model : 0.06954617500305176
			 train-loss:  2.075580546632409 	 ± 0.25646166675452287
	data : 0.11657142639160156
	model : 0.06878643035888672
			 train-loss:  2.0795774441498978 	 ± 0.2564821879061205
	data : 0.11714348793029786
	model : 0.0688438892364502
			 train-loss:  2.0771023287917627 	 ± 0.25531275257313096
	data : 0.11713099479675293
	model : 0.068778657913208
			 train-loss:  2.078335445318649 	 ± 0.25359821601290355
	data : 0.1171454906463623
	model : 0.06846966743469238
			 train-loss:  2.0811553281896256 	 ± 0.2527826263571129
	data : 0.11740994453430176
	model : 0.06860651969909667
			 train-loss:  2.0792161416316377 	 ± 0.2514531629078605
	data : 0.11725754737854004
	model : 0.06907706260681153
			 train-loss:  2.0878549098968504 	 ± 0.2597590806386277
	data : 0.11698827743530274
	model : 0.0691218376159668
			 train-loss:  2.088070678039336 	 ± 0.2579296219376429
	data : 0.1168673038482666
	model : 0.06837859153747558
			 train-loss:  2.0907310114966497 	 ± 0.25711123612477094
	data : 0.11753292083740234
	model : 0.06867704391479493
			 train-loss:  2.089962371408123 	 ± 0.2554274097902161
	data : 0.11715126037597656
	model : 0.06873898506164551
			 train-loss:  2.0929311868306755 	 ± 0.2549606037629749
	data : 0.1170999526977539
	model : 0.06993641853332519
			 train-loss:  2.0945831775665282 	 ± 0.253653560542441
	data : 0.11601724624633789
	model : 0.06900992393493652
			 train-loss:  2.0988669771897164 	 ± 0.2546956390968291
	data : 0.11687211990356446
	model : 0.06970696449279785
			 train-loss:  2.0964109247381035 	 ± 0.25394064161988567
	data : 0.11617650985717773
	model : 0.06950774192810058
			 train-loss:  2.099003779582488 	 ± 0.2533313459035149
	data : 0.11646289825439453
	model : 0.06945905685424805
			 train-loss:  2.099983257583425 	 ± 0.2518714712859768
	data : 0.1162381649017334
	model : 0.06870579719543457
			 train-loss:  2.097139169275761 	 ± 0.2515656292063907
	data : 0.11691341400146485
	model : 0.06922636032104493
			 train-loss:  2.099227609457793 	 ± 0.2507047921595602
	data : 0.1165003776550293
	model : 0.06930599212646485
			 train-loss:  2.100139032049877 	 ± 0.24930639803541357
	data : 0.11650323867797852
	model : 0.06959176063537598
			 train-loss:  2.0983185035636627 	 ± 0.2483477705709593
	data : 0.11614842414855957
	model : 0.06868147850036621
			 train-loss:  2.0948962696960995 	 ± 0.24882612313545188
	data : 0.11713137626647949
	model : 0.06897401809692383
			 train-loss:  2.0949195679496317 	 ± 0.24735820176277182
	data : 0.1169654369354248
	model : 0.06927580833435058
			 train-loss:  2.09582167309384 	 ± 0.24605646993483765
	data : 0.1167755126953125
	model : 0.06923794746398926
			 train-loss:  2.1015199368027435 	 ± 0.25028048027263244
	data : 0.11689324378967285
	model : 0.06836199760437012
			 train-loss:  2.10297883505171 	 ± 0.2492261351015669
	data : 0.11758942604064941
	model : 0.06927413940429687
			 train-loss:  2.1060129404067993 	 ± 0.24945113758074922
	data : 0.11681451797485351
	model : 0.06903653144836426
			 train-loss:  2.1031924141777885 	 ± 0.24948446696114374
	data : 0.11690201759338378
	model : 0.06916728019714355
			 train-loss:  2.1039362058534725 	 ± 0.2482102054543771
	data : 0.11687335968017579
	model : 0.0683530330657959
			 train-loss:  2.102633649888246 	 ± 0.2471700749010267
	data : 0.11761646270751953
	model : 0.06958341598510742
			 train-loss:  2.101011736418611 	 ± 0.24632934652234342
	data : 0.11656680107116699
	model : 0.07045865058898926
			 train-loss:  2.1037841515338167 	 ± 0.2464699967836803
	data : 0.11578207015991211
	model : 0.07028565406799317
			 train-loss:  2.1000317598644056 	 ± 0.24785393690441326
	data : 0.11586194038391114
	model : 0.07007398605346679
			 train-loss:  2.105186695853869 	 ± 0.25162696892366904
	data : 0.11606860160827637
	model : 0.07113833427429199
			 train-loss:  2.109724875577946 	 ± 0.25424499028708164
	data : 0.11502251625061036
	model : 0.07066636085510254
			 train-loss:  2.1114655927735932 	 ± 0.25352482588460556
	data : 0.1154141902923584
	model : 0.06976032257080078
			 train-loss:  2.1085689200295343 	 ± 0.25386588276342753
	data : 0.11607365608215332
	model : 0.06996669769287109
			 train-loss:  2.103639198541641 	 ± 0.2573117191420029
	data : 0.11598515510559082
	model : 0.07026920318603516
			 train-loss:  2.098311444320301 	 ± 0.2615191761273607
	data : 0.11556835174560547
	model : 0.07025871276855469
			 train-loss:  2.0997275710105896 	 ± 0.2606229342047372
	data : 0.11577358245849609
	model : 0.0702082633972168
			 train-loss:  2.0962546612452533 	 ± 0.2617156604106318
	data : 0.11600704193115234
	model : 0.07024402618408203
			 train-loss:  2.099840822128149 	 ± 0.2629850127607284
	data : 0.1160473346710205
	model : 0.07019438743591308
			 train-loss:  2.0992036637805755 	 ± 0.26181035233853656
	data : 0.115960693359375
	model : 0.07004923820495605
			 train-loss:  2.0985117475941495 	 ± 0.260668911439679
	data : 0.11621289253234864
	model : 0.06995048522949218
			 train-loss:  2.0973191239009394 	 ± 0.2597383682842074
	data : 0.11615300178527832
	model : 0.07011966705322266
			 train-loss:  2.0982400841183133 	 ± 0.25870853687832424
	data : 0.11597027778625488
	model : 0.06937546730041504
			 train-loss:  2.096890620135386 	 ± 0.2579006457975296
	data : 0.11666369438171387
	model : 0.06927576065063476
			 train-loss:  2.0981571435928346 	 ± 0.25706599678621356
	data : 0.11692147254943848
	model : 0.06924419403076172
			 train-loss:  2.0969533995465115 	 ± 0.2562166563513596
	data : 0.11682944297790528
	model : 0.06897897720336914
			 train-loss:  2.0997546815446446 	 ± 0.2567720357808775
	data : 0.11686325073242188
	model : 0.06885147094726562
			 train-loss:  2.0972880937356866 	 ± 0.2569626910972442
	data : 0.1170389175415039
	model : 0.0696601390838623
			 train-loss:  2.100777366705108 	 ± 0.2585080144786793
	data : 0.11646685600280762
	model : 0.0698232650756836
			 train-loss:  2.099585219051527 	 ± 0.25769616431477754
	data : 0.11617026329040528
	model : 0.06991615295410156
			 train-loss:  2.101150600046947 	 ± 0.25713155056595544
	data : 0.11613097190856933
	model : 0.07019071578979492
			 train-loss:  2.102721944833413 	 ± 0.25658907477292153
	data : 0.11606864929199219
	model : 0.06996374130249024
			 train-loss:  2.100668090884968 	 ± 0.25646354031901986
	data : 0.11617112159729004
	model : 0.0697892189025879
			 train-loss:  2.101700339998518 	 ± 0.2556297364113336
	data : 0.11628470420837403
	model : 0.06960725784301758
			 train-loss:  2.1009530703226726 	 ± 0.2546928708807746
	data : 0.11655802726745605
	model : 0.06904053688049316
			 train-loss:  2.102512755669838 	 ± 0.2542130400288833
	data : 0.11698613166809083
	model : 0.06884942054748536
			 train-loss:  2.103274290678931 	 ± 0.2533075892977689
	data : 0.11713314056396484
	model : 0.06910414695739746
			 train-loss:  2.1029298576882214 	 ± 0.25230446615883106
	data : 0.1168440341949463
	model : 0.0691296100616455
			 train-loss:  2.1036756115574993 	 ± 0.25142112612660317
	data : 0.11683902740478516
	model : 0.06930356025695801
			 train-loss:  2.1008656415939333 	 ± 0.2523608081097159
	data : 0.11668281555175782
	model : 0.06994051933288574
			 train-loss:  2.1023980653475203 	 ± 0.2519406150747142
	data : 0.11625161170959472
	model : 0.06984920501708984
			 train-loss:  2.1009622033186783 	 ± 0.25146381815323526
	data : 0.11631441116333008
	model : 0.06969699859619141
			 train-loss:  2.104620287194848 	 ± 0.2538493572399356
	data : 0.11659388542175293
	model : 0.06948857307434082
			 train-loss:  2.1046592468439145 	 ± 0.2528639149392463
	data : 0.11677751541137696
	model : 0.06946101188659667
			 train-loss:  2.104679225041316 	 ± 0.2518895860994638
	data : 0.11681475639343261
	model : 0.06973137855529785
			 train-loss:  2.1022443043366645 	 ± 0.2524574696477969
	data : 0.11642179489135743
	model : 0.07015280723571778
			 train-loss:  2.105270044370131 	 ± 0.2538725137631266
	data : 0.11618132591247558
	model : 0.06988115310668945
			 train-loss:  2.1052688602218055 	 ± 0.2529163054064419
	data : 0.11632471084594727
	model : 0.06928319931030273
			 train-loss:  2.109293636991017 	 ± 0.2562103404510736
	data : 0.11680922508239747
	model : 0.06927804946899414
			 train-loss:  2.1110723442501493 	 ± 0.256088730274634
	data : 0.11693792343139649
	model : 0.06884441375732422
			 train-loss:  2.10885944611886 	 ± 0.2564377220072815
	data : 0.11723594665527344
	model : 0.06847715377807617
			 train-loss:  2.1084144567921212 	 ± 0.25555279913977075
	data : 0.11736826896667481
	model : 0.06822638511657715
			 train-loss:  2.105526320312334 	 ± 0.2568594067907637
	data : 0.11763801574707031
	model : 0.06863894462585449
			 train-loss:  2.106810414533821 	 ± 0.256377943404022
	data : 0.11722474098205567
	model : 0.068660306930542
			 train-loss:  2.103147269146783 	 ± 0.2590855891566357
	data : 0.11710357666015625
	model : 0.06881799697875976
			 train-loss:  2.1017820446203785 	 ± 0.25867008634699906
	data : 0.11714177131652832
	model : 0.06874480247497558
			 train-loss:  2.1024539453882567 	 ± 0.25788111490403925
	data : 0.11715083122253418
	model : 0.06840519905090332
			 train-loss:  2.101568098668452 	 ± 0.25719456965140997
	data : 0.11728615760803222
	model : 0.06882772445678711
			 train-loss:  2.1023199690712824 	 ± 0.2564576329148396
	data : 0.11696376800537109
	model : 0.06859698295593261
			 train-loss:  2.1014347734122443 	 ± 0.2557924197071619
	data : 0.1170842170715332
	model : 0.06783380508422851
			 train-loss:  2.101674689005499 	 ± 0.25493128299860207
	data : 0.11782851219177246
	model : 0.06878924369812012
			 train-loss:  2.1019259456063613 	 ± 0.25408082857378683
	data : 0.11691050529479981
	model : 0.06912527084350586
			 train-loss:  2.1012208542308293 	 ± 0.25336525543707705
	data : 0.1165860652923584
	model : 0.06856474876403809
			 train-loss:  2.099551161663644 	 ± 0.25332928375817376
	data : 0.11701092720031739
	model : 0.06891078948974609
			 train-loss:  2.10021290063858 	 ± 0.25261261787211775
	data : 0.11680150032043457
	model : 0.0692789077758789
			 train-loss:  2.104397667164834 	 ± 0.2569384591667405
	data : 0.11636981964111329
	model : 0.06855587959289551
			 train-loss:  2.10729659466367 	 ± 0.2585575716579994
	data : 0.11725258827209473
	model : 0.06895952224731446
			 train-loss:  2.107787765708624 	 ± 0.25778236249939385
	data : 0.11708097457885742
	model : 0.06969575881958008
			 train-loss:  2.108270918394064 	 ± 0.2570135353557916
	data : 0.11648263931274414
	model : 0.06859893798828125
			 train-loss:  2.1113828482166412 	 ± 0.2590774833929974
	data : 0.11747307777404785
	model : 0.06873483657836914
			 train-loss:  2.1101754009723663 	 ± 0.25868292877440163
	data : 0.11731257438659667
	model : 0.06871299743652344
			 train-loss:  2.108570827040703 	 ± 0.25863542330172234
	data : 0.11737895011901855
	model : 0.06899476051330566
			 train-loss:  2.111737095102479 	 ± 0.2608502982589637
	data : 0.11699433326721191
	model : 0.06880974769592285
			 train-loss:  2.110600744403383 	 ± 0.26042073602578264
	data : 0.11703124046325683
	model : 0.06898436546325684
			 train-loss:  2.110839496552944 	 ± 0.25962310117822207
	data : 0.1168642520904541
	model : 0.06860346794128418
			 train-loss:  2.107942839586957 	 ± 0.2613962409297922
	data : 0.1171182632446289
	model : 0.06871919631958008
			 train-loss:  2.1073664434162187 	 ± 0.2606908254361221
	data : 0.11710090637207031
	model : 0.06790614128112793
			 train-loss:  2.107991130074109 	 ± 0.26001152583332987
	data : 0.11796574592590332
	model : 0.06722187995910645
			 train-loss:  2.1064922518846467 	 ± 0.2599229954003029
	data : 0.1186955451965332
	model : 0.06806840896606445
			 train-loss:  2.1049856879494406 	 ± 0.25985139163663423
	data : 0.11811037063598633
	model : 0.0685840129852295
			 train-loss:  2.1064818207039893 	 ± 0.25977936867299867
	data : 0.117816162109375
	model : 0.06839017868041992
			 train-loss:  2.1070440300924336 	 ± 0.25910169017791884
	data : 0.11782207489013671
	model : 0.06929702758789062
			 train-loss:  2.1108070200397853 	 ± 0.26286653679870703
	data : 0.11686015129089355
	model : 0.06985416412353515
			 train-loss:  2.1132240196656897 	 ± 0.2639533755836688
	data : 0.11636939048767089
	model : 0.06960592269897461
			 train-loss:  2.111878547247718 	 ± 0.26375650292882064
	data : 0.1165015697479248
	model : 0.06954970359802246
			 train-loss:  2.1086609705149777 	 ± 0.2663092964938151
	data : 0.11646528244018554
	model : 0.06888213157653808
			 train-loss:  2.109656067088593 	 ± 0.26585266451213446
	data : 0.11732287406921386
	model : 0.06878571510314942
			 train-loss:  2.109129440577733 	 ± 0.26517315034497785
	data : 0.1175295352935791
	model : 0.06891183853149414
			 train-loss:  2.107485240903394 	 ± 0.2652929833976688
	data : 0.11745352745056152
	model : 0.06817970275878907
			 train-loss:  2.1104597296033587 	 ± 0.26742788366334314
	data : 0.11798043251037597
	model : 0.06822438240051269
			 train-loss:  2.1085007454861295 	 ± 0.2679233208049046
	data : 0.11792979240417481
	model : 0.06918625831604004
			 train-loss:  2.10801344874215 	 ± 0.26724360669504693
	data : 0.11704473495483399
	model : 0.06836729049682617
			 train-loss:  2.1074075437663646 	 ± 0.2666137553738008
	data : 0.11770153045654297
	model : 0.0685546875
			 train-loss:  2.107219467615948 	 ± 0.2658798219899086
	data : 0.11753730773925782
	model : 0.06931281089782715
			 train-loss:  2.1062398831049602 	 ± 0.2654639557939654
	data : 0.11688485145568847
	model : 0.06921052932739258
			 train-loss:  2.108971896092536 	 ± 0.2672550649849185
	data : 0.11677675247192383
	model : 0.06894106864929199
			 train-loss:  2.1115049018964664 	 ± 0.2686896735154634
	data : 0.11688976287841797
	model : 0.06974072456359863
			 train-loss:  2.1108567421553563 	 ± 0.26809717895588686
	data : 0.11615667343139649
	model : 0.06970210075378418
			 train-loss:  2.110721382757892 	 ± 0.2673739317073294
	data : 0.11611080169677734
	model : 0.06977372169494629
			 train-loss:  2.109268325083965 	 ± 0.26737779750972385
	data : 0.1159451961517334
	model : 0.06982607841491699
			 train-loss:  2.107225449495418 	 ± 0.2681018370316958
	data : 0.1162184715270996
	model : 0.06970558166503907
			 train-loss:  2.106433808484817 	 ± 0.2676019111803447
	data : 0.11632804870605469
	model : 0.06954646110534668
			 train-loss:  2.1062174261884485 	 ± 0.2669056575470549
	data : 0.11647911071777343
	model : 0.07038373947143554
			 train-loss:  2.108342554203417 	 ± 0.267788621582864
	data : 0.11566100120544434
	model : 0.07031068801879883
			 train-loss:  2.1108842573667825 	 ± 0.2693590708711177
	data : 0.1158106803894043
	model : 0.07028365135192871
			 train-loss:  2.111890805329328 	 ± 0.26901104055458375
	data : 0.11575698852539062
	model : 0.06994552612304687
			 train-loss:  2.1120391773680844 	 ± 0.2683174118814463
	data : 0.11598753929138184
	model : 0.07007126808166504
			 train-loss:  2.1116049672655492 	 ± 0.267689009444549
	data : 0.11594634056091309
	model : 0.06914620399475098
			 train-loss:  2.111352685800533 	 ± 0.267021200230134
	data : 0.11693615913391113
	model : 0.0691296100616455
			 train-loss:  2.1099943888493073 	 ± 0.26700674709926786
	data : 0.116900634765625
	model : 0.06909208297729492
			 train-loss:  2.1086895295551846 	 ± 0.2669473421720965
	data : 0.11690149307250977
	model : 0.06867313385009766
			 train-loss:  2.1078497994369663 	 ± 0.26652835096343797
	data : 0.11739740371704102
	model : 0.06871428489685058
			 train-loss:  2.1072796703589085 	 ± 0.2659748514503518
	data : 0.11737446784973145
	model : 0.0687185287475586
			 train-loss:  2.1079736654482892 	 ± 0.2654853924826398
	data : 0.11731038093566895
	model : 0.06898703575134277
			 train-loss:  2.1080790543556214 	 ± 0.2648250203645587
	data : 0.11717786788940429
	model : 0.06903271675109864
			 train-loss:  2.109627906362809 	 ± 0.26507199596794573
	data : 0.11720929145812989
	model : 0.06979365348815918
			 train-loss:  2.11185620326807 	 ± 0.2662956108018007
	data : 0.11636543273925781
	model : 0.06940875053405762
			 train-loss:  2.1107783076798388 	 ± 0.26608029045975623
	data : 0.11660380363464355
	model : 0.0694244384765625
			 train-loss:  2.1098982674234055 	 ± 0.2657233260376582
	data : 0.11659731864929199
	model : 0.06902685165405273
			 train-loss:  2.109643292427063 	 ± 0.26509944343399927
	data : 0.11680412292480469
	model : 0.06824612617492676
			 train-loss:  2.1104962160286393 	 ± 0.2647370281628979
	data : 0.11749854087829589
	model : 0.06848363876342774
			 train-loss:  2.1082653901427264 	 ± 0.26603062263525484
	data : 0.11751284599304199
	model : 0.06902537345886231
			 train-loss:  2.1071756871847005 	 ± 0.2658530486003395
	data : 0.11707634925842285
	model : 0.06910939216613769
			 train-loss:  2.107157612531379 	 ± 0.2652164020238879
	data : 0.1169783592224121
	model : 0.0694490909576416
			 train-loss:  2.1066966545014156 	 ± 0.2646680894288727
	data : 0.11671915054321289
	model : 0.06972594261169433
			 train-loss:  2.1048495374019676 	 ± 0.26539347688899956
	data : 0.11642017364501953
	model : 0.0695307731628418
			 train-loss:  2.105499995204638 	 ± 0.26493534356933524
	data : 0.11658291816711426
	model : 0.06904516220092774
			 train-loss:  2.1042363749983166 	 ± 0.26495227935718396
	data : 0.1169459342956543
	model : 0.06806678771972656
			 train-loss:  2.1041864329409377 	 ± 0.2643335120136953
	data : 0.1178673267364502
	model : 0.06801342964172363
			 train-loss:  2.1031387467717018 	 ± 0.264163046017034
	data : 0.11799569129943847
	model : 0.06854686737060547
			 train-loss:  2.1047948913441763 	 ± 0.2646672528402182
	data : 0.11768150329589844
	model : 0.06847476959228516
			 train-loss:  2.1022604088629446 	 ± 0.26667104701742
	data : 0.11770997047424317
	model : 0.06885108947753907
			 train-loss:  2.102466620983334 	 ± 0.2660760538447782
	data : 0.11748809814453125
	model : 0.06977462768554688
			 train-loss:  2.101706105824475 	 ± 0.2657052548488556
	data : 0.11656513214111328
	model : 0.06968197822570801
			 train-loss:  2.10070015408776 	 ± 0.26551834445723865
	data : 0.11661195755004883
	model : 0.06920647621154785
			 train-loss:  2.1001208119802346 	 ± 0.2650562713068877
	data : 0.1168391227722168
	model : 0.06909260749816895
			 train-loss:  2.1004815907091707 	 ± 0.26451300425494556
	data : 0.11694750785827637
	model : 0.06907315254211426
			 train-loss:  2.0985621347555665 	 ± 0.2654642961933432
	data : 0.11685562133789062
	model : 0.06827683448791504
			 train-loss:  2.0997321797268733 	 ± 0.26544674934191936
	data : 0.1176295280456543
	model : 0.06772022247314453
			 train-loss:  2.0993805991278753 	 ± 0.26490847625676783
	data : 0.11796092987060547
	model : 0.06795854568481445
			 train-loss:  2.0984397989458743 	 ± 0.2646981940314317
	data : 0.11775288581848145
	model : 0.06825594902038574
			 train-loss:  2.0965263092570368 	 ± 0.26567642540549924
	data : 0.11747217178344727
	model : 0.06808042526245117
			 train-loss:  2.0962220874794744 	 ± 0.2651327842480103
	data : 0.11770014762878418
	model : 0.06875224113464355
			 train-loss:  2.094850127873983 	 ± 0.2653631187269269
	data : 0.11690545082092285
	model : 0.06937160491943359
			 train-loss:  2.094704925495645 	 ± 0.26479473095723904
	data : 0.1164018154144287
	model : 0.06908245086669922
			 train-loss:  2.0934654293638286 	 ± 0.26488880053126806
	data : 0.11665754318237305
	model : 0.06887955665588379
			 train-loss:  2.0942933888270936 	 ± 0.2646166870379986
	data : 0.11656122207641602
	model : 0.06847958564758301
			 train-loss:  2.094378463188466 	 ± 0.2640514090594573
	data : 0.11691694259643555
	model : 0.06777276992797851
			 train-loss:  2.096010245828547 	 ± 0.2646612872321841
	data : 0.11761879920959473
	model : 0.06722850799560547
			 train-loss:  2.094888740397514 	 ± 0.2646542079892294
	data : 0.11794452667236328
	model : 0.06715679168701172
			 train-loss:  2.094678494384733 	 ± 0.2641125708914872
	data : 0.1178410530090332
	model : 0.06701288223266602
			 train-loss:  2.0957185814652264 	 ± 0.26403867833537115
	data : 0.11791982650756835
	model : 0.0672734260559082
			 train-loss:  2.0943249534158146 	 ± 0.2643554400867802
	data : 0.11758813858032227
	model : 0.06774592399597168
			 train-loss:  2.0946297555787794 	 ± 0.2638437209967566
	data : 0.11719264984130859
	model : 0.06792793273925782
			 train-loss:  2.0930080309510233 	 ± 0.2644844430492561
	data : 0.11717801094055176
	model : 0.06853699684143066
			 train-loss:  2.092264434608681 	 ± 0.2641864262088561
	data : 0.11708793640136719
	model : 0.0688321590423584
			 train-loss:  2.091744820933697 	 ± 0.26376339872841975
	data : 0.11700439453125
	model : 0.06904301643371583
			 train-loss:  2.092518404187489 	 ± 0.2634950661291482
	data : 0.11697306632995605
	model : 0.06924772262573242
			 train-loss:  2.0916973689540486 	 ± 0.2632658507111641
	data : 0.11683444976806641
	model : 0.06936578750610352
			 train-loss:  2.091451193848435 	 ± 0.2627561637347018
	data : 0.11682896614074707
	model : 0.06924891471862793
			 train-loss:  2.0909610865561943 	 ± 0.26233375341214293
	data : 0.11677742004394531
	model : 0.06850671768188477
			 train-loss:  2.090407610422204 	 ± 0.26194605812624194
	data : 0.1174088478088379
	model : 0.0681877613067627
			 train-loss:  2.0896722169653064 	 ± 0.26167277176088616
	data : 0.11760964393615722
	model : 0.06753768920898437
			 train-loss:  2.0906198967891525 	 ± 0.26157289047042886
	data : 0.1181562900543213
	model : 0.06757178306579589
			 train-loss:  2.090564850330353 	 ± 0.2610506656168161
	data : 0.11800098419189453
	model : 0.06670470237731933
			 train-loss:  2.0905923933621895 	 ± 0.2605304893648673
	data : 0.11841607093811035
	model : 0.06722936630249024
			 train-loss:  2.0896994495202623 	 ± 0.2603976203194279
	data : 0.11795034408569335
	model : 0.06723432540893555
			 train-loss:  2.0889148952461514 	 ± 0.26018074780227896
	data : 0.11786322593688965
	model : 0.06741871833801269
			 train-loss:  2.0886473106587027 	 ± 0.25970295495733586
	data : 0.11764402389526367
	model : 0.06663556098937988
			 train-loss:  2.0885827807819144 	 ± 0.2591952735871429
	data : 0.11823358535766601
	model : 0.06686239242553711
			 train-loss:  2.086978286039084 	 ± 0.2599542888329673
	data : 0.11723313331604004
	model : 0.05762391090393067
#epoch  91    val-loss:  2.4368405530327246  train-loss:  2.086978286039084  lr:  1.220703125e-06
			 train-loss:  2.505384683609009 	 ± 0.0
	data : 5.756757736206055
	model : 0.07201123237609863
			 train-loss:  2.15134459733963 	 ± 0.35404008626937866
	data : 2.943528652191162
	model : 0.0709148645401001
			 train-loss:  2.0925639470418296 	 ± 0.30078771831523315
	data : 2.002577225367228
	model : 0.07114577293395996
			 train-loss:  2.1241052746772766 	 ± 0.2661569173701539
	data : 1.5307225584983826
	model : 0.07043725252151489
			 train-loss:  2.056996202468872 	 ± 0.2732876031251285
	data : 1.2480841159820557
	model : 0.07043232917785644
			 train-loss:  2.0827760299046836 	 ± 0.25604965507624855
	data : 0.11982541084289551
	model : 0.06968541145324707
			 train-loss:  2.140126807349069 	 ± 0.2755543102998968
	data : 0.11725339889526368
	model : 0.06964359283447266
			 train-loss:  2.0933317989110947 	 ± 0.28594984767825254
	data : 0.1162954330444336
	model : 0.069012451171875
			 train-loss:  2.064972996711731 	 ± 0.2812753658240101
	data : 0.11676020622253418
	model : 0.06847848892211914
			 train-loss:  2.074061930179596 	 ± 0.2682307340364581
	data : 0.11739616394042969
	model : 0.06753449440002442
			 train-loss:  2.0508752085945825 	 ± 0.26605124475321296
	data : 0.11825652122497558
	model : 0.06792407035827637
			 train-loss:  2.068453013896942 	 ± 0.2613109853480406
	data : 0.117946195602417
	model : 0.06760034561157227
			 train-loss:  2.067557921776405 	 ± 0.2510786205994862
	data : 0.11828265190124512
	model : 0.0683474063873291
			 train-loss:  2.0773961544036865 	 ± 0.2445319271955269
	data : 0.11774873733520508
	model : 0.06947469711303711
			 train-loss:  2.09214555422465 	 ± 0.24260070002893408
	data : 0.11671867370605468
	model : 0.07012639045715333
			 train-loss:  2.103504925966263 	 ± 0.23898155958479947
	data : 0.11609539985656739
	model : 0.07010207176208497
			 train-loss:  2.0767076085595524 	 ± 0.2554254979485658
	data : 0.11614627838134765
	model : 0.0705897331237793
			 train-loss:  2.0729718009630838 	 ± 0.24870640434370872
	data : 0.1157045841217041
	model : 0.06934952735900879
			 train-loss:  2.0577556396785535 	 ± 0.25053327912962126
	data : 0.11666321754455566
	model : 0.06905179023742676
			 train-loss:  2.0617773473262786 	 ± 0.24481806769423764
	data : 0.11680011749267578
	model : 0.06932778358459472
			 train-loss:  2.0641354663031444 	 ± 0.23915060307129132
	data : 0.11663331985473632
	model : 0.0690730094909668
			 train-loss:  2.0511668270284478 	 ± 0.24109173841271148
	data : 0.11678090095520019
	model : 0.06912083625793457
			 train-loss:  2.0554247109786323 	 ± 0.23663662701170912
	data : 0.11692972183227539
	model : 0.06999678611755371
			 train-loss:  2.050855408112208 	 ± 0.23268840864574974
	data : 0.11609120368957519
	model : 0.07021079063415528
			 train-loss:  2.0436132287979127 	 ± 0.23073127132371452
	data : 0.11599016189575195
	model : 0.07003798484802246
			 train-loss:  2.0515346114452067 	 ± 0.22969121296491105
	data : 0.11615419387817383
	model : 0.06987814903259278
			 train-loss:  2.050958893917225 	 ± 0.22541665656334803
	data : 0.11619882583618164
	model : 0.06883172988891602
			 train-loss:  2.0376288422516415 	 ± 0.23193871683186565
	data : 0.11710977554321289
	model : 0.06878457069396973
			 train-loss:  2.0338264498217353 	 ± 0.22879112315854028
	data : 0.11723155975341797
	model : 0.06872138977050782
			 train-loss:  2.04093177318573 	 ± 0.22817671898050662
	data : 0.11743249893188476
	model : 0.06886520385742187
			 train-loss:  2.043697626360001 	 ± 0.22497690922792027
	data : 0.11725072860717774
	model : 0.0693136215209961
			 train-loss:  2.0448253601789474 	 ± 0.22152274922370055
	data : 0.11682758331298829
	model : 0.07033276557922363
			 train-loss:  2.0469599492622144 	 ± 0.21847447271044274
	data : 0.11578054428100586
	model : 0.07044925689697265
			 train-loss:  2.0434659438974716 	 ± 0.21617147339390963
	data : 0.11553621292114258
	model : 0.07048749923706055
			 train-loss:  2.038259768486023 	 ± 0.21521269699595655
	data : 0.1153299331665039
	model : 0.07038850784301758
			 train-loss:  2.0416846374670663 	 ± 0.21316771760795045
	data : 0.11548829078674316
	model : 0.07036876678466797
			 train-loss:  2.0454155335555204 	 ± 0.2114555743985528
	data : 0.11551628112792969
	model : 0.07004842758178711
			 train-loss:  2.044617141547956 	 ± 0.20871122357986943
	data : 0.11587533950805665
	model : 0.06997060775756836
			 train-loss:  2.045471952511714 	 ± 0.20608544060067324
	data : 0.11602268218994141
	model : 0.06901235580444336
			 train-loss:  2.044407269358635 	 ± 0.20360166272731134
	data : 0.11687707901000977
	model : 0.0692819595336914
			 train-loss:  2.0417291740091836 	 ± 0.2018154120345854
	data : 0.11678433418273926
	model : 0.06921143531799316
			 train-loss:  2.0358692038626898 	 ± 0.20289805501573804
	data : 0.11700549125671386
	model : 0.06941633224487305
			 train-loss:  2.0396163713100344 	 ± 0.2019900177817671
	data : 0.116851806640625
	model : 0.06846351623535156
			 train-loss:  2.0382671518759294 	 ± 0.1998773929244174
	data : 0.11763234138488769
	model : 0.06910576820373535
			 train-loss:  2.039962742063734 	 ± 0.19796381984526507
	data : 0.11716904640197753
	model : 0.0689476490020752
			 train-loss:  2.03729712444803 	 ± 0.19661503709263803
	data : 0.11722102165222167
	model : 0.06875572204589844
			 train-loss:  2.0404528810622846 	 ± 0.19568617268631167
	data : 0.11734509468078613
	model : 0.06882672309875489
			 train-loss:  2.039624735713005 	 ± 0.19372026089270802
	data : 0.1171494483947754
	model : 0.06970601081848145
			 train-loss:  2.0441419883650176 	 ± 0.1942707872036874
	data : 0.11654462814331054
	model : 0.06994547843933105
			 train-loss:  2.04650821685791 	 ± 0.19303022771647405
	data : 0.11624765396118164
	model : 0.07038321495056152
			 train-loss:  2.0466560616212734 	 ± 0.1911312647546608
	data : 0.11578078269958496
	model : 0.07070865631103515
			 train-loss:  2.049548332507794 	 ± 0.19040815325742594
	data : 0.11559834480285644
	model : 0.07066073417663574
			 train-loss:  2.044807661254451 	 ± 0.19167641834840785
	data : 0.11571178436279297
	model : 0.07064900398254395
			 train-loss:  2.040792577796512 	 ± 0.19212987590308656
	data : 0.11574316024780273
	model : 0.07050771713256836
			 train-loss:  2.0354152224280617 	 ± 0.1944329934281153
	data : 0.11584453582763672
	model : 0.06989464759826661
			 train-loss:  2.03700159064361 	 ± 0.1930479866700938
	data : 0.11627087593078614
	model : 0.06979031562805176
			 train-loss:  2.0384941331127235 	 ± 0.19167279195489245
	data : 0.11621074676513672
	model : 0.06891474723815919
			 train-loss:  2.0451720768007737 	 ± 0.1965882717361346
	data : 0.1170271873474121
	model : 0.06889634132385254
			 train-loss:  2.0360642025026223 	 ± 0.2068893696223739
	data : 0.11692576408386231
	model : 0.06905388832092285
			 train-loss:  2.035196852684021 	 ± 0.20526619267460006
	data : 0.11694231033325195
	model : 0.06910886764526367
			 train-loss:  2.044695525872903 	 ± 0.21646471454801489
	data : 0.11692333221435547
	model : 0.06908979415893554
			 train-loss:  2.0502043347204886 	 ± 0.21898031987174352
	data : 0.1169586181640625
	model : 0.06991219520568848
			 train-loss:  2.0478613603682745 	 ± 0.21801739008846224
	data : 0.11593718528747558
	model : 0.06998577117919921
			 train-loss:  2.051497057080269 	 ± 0.2182238614655213
	data : 0.11591205596923829
	model : 0.069465970993042
			 train-loss:  2.0673609440143292 	 ± 0.2509889221471656
	data : 0.11625890731811524
	model : 0.06964807510375977
			 train-loss:  2.0743984923218237 	 ± 0.255460805786752
	data : 0.11594796180725098
	model : 0.06961007118225097
			 train-loss:  2.075193903339443 	 ± 0.2536295465892618
	data : 0.11598100662231445
	model : 0.06961760520935059
			 train-loss:  2.077208739869735 	 ± 0.2522973218689532
	data : 0.11605949401855468
	model : 0.06942663192749024
			 train-loss:  2.0788037742393604 	 ± 0.2508075336352317
	data : 0.11615190505981446
	model : 0.06986651420593262
			 train-loss:  2.082433441707066 	 ± 0.2508282757206079
	data : 0.11571121215820312
	model : 0.06884393692016602
			 train-loss:  2.0866008476472238 	 ± 0.25148441041062986
	data : 0.11676611900329589
	model : 0.06893253326416016
			 train-loss:  2.0883916748894586 	 ± 0.2501873608550332
	data : 0.11675438880920411
	model : 0.06806402206420899
			 train-loss:  2.087214306609271 	 ± 0.24866860173829172
	data : 0.11768198013305664
	model : 0.0680267333984375
			 train-loss:  2.082018755577706 	 ± 0.25094022129546656
	data : 0.11771888732910156
	model : 0.06800775527954102
			 train-loss:  2.0827821636199952 	 ± 0.24934816622089248
	data : 0.11758718490600586
	model : 0.06811642646789551
			 train-loss:  2.081347200431322 	 ± 0.24801382283581783
	data : 0.11766295433044434
	model : 0.06812405586242676
			 train-loss:  2.0773916662513434 	 ± 0.24879937515419762
	data : 0.11750431060791015
	model : 0.06990313529968262
			 train-loss:  2.075754167177738 	 ± 0.24761662485749666
	data : 0.11580934524536132
	model : 0.07026004791259766
			 train-loss:  2.07568833646895 	 ± 0.24604512683764462
	data : 0.11557683944702149
	model : 0.06943411827087402
			 train-loss:  2.0765928581357 	 ± 0.2446346489793499
	data : 0.11648716926574706
	model : 0.07001638412475586
			 train-loss:  2.0746045480539768 	 ± 0.24376944180362678
	data : 0.11611409187316894
	model : 0.06976132392883301
			 train-loss:  2.072254957222357 	 ± 0.24319956861413722
	data : 0.11647138595581055
	model : 0.06893033981323242
			 train-loss:  2.07367900480707 	 ± 0.2420737824421507
	data : 0.11732430458068847
	model : 0.06837124824523926
			 train-loss:  2.0757045518784296 	 ± 0.24133511198191832
	data : 0.11785368919372559
	model : 0.06838603019714355
			 train-loss:  2.0742453505011165 	 ± 0.2402837642395661
	data : 0.11779403686523438
	model : 0.06844916343688964
			 train-loss:  2.0751513727875643 	 ± 0.23902867959967677
	data : 0.11782073974609375
	model : 0.06838159561157227
			 train-loss:  2.07871488182024 	 ± 0.23993763393886292
	data : 0.11783404350280761
	model : 0.0683753490447998
			 train-loss:  2.084087286483158 	 ± 0.24377637402331764
	data : 0.11786928176879882
	model : 0.06884531974792481
			 train-loss:  2.0815082547369967 	 ± 0.2436073191460601
	data : 0.11735906600952148
	model : 0.06987195014953614
			 train-loss:  2.079497602250841 	 ± 0.24299165562698932
	data : 0.11661815643310547
	model : 0.07009005546569824
			 train-loss:  2.076496326006376 	 ± 0.24332445104581435
	data : 0.11611886024475097
	model : 0.0703669548034668
			 train-loss:  2.0784258946128515 	 ± 0.2426974470688399
	data : 0.11580100059509277
	model : 0.07024149894714356
			 train-loss:  2.080870061792353 	 ± 0.2425248422804621
	data : 0.11582083702087402
	model : 0.06929435729980468
			 train-loss:  2.0805382398848837 	 ± 0.2412525903987895
	data : 0.11675410270690918
	model : 0.06834273338317871
			 train-loss:  2.08294264140882 	 ± 0.24110906034394755
	data : 0.11741876602172852
	model : 0.06745266914367676
			 train-loss:  2.082578554749489 	 ± 0.23987624721851833
	data : 0.11834506988525391
	model : 0.06749048233032226
			 train-loss:  2.0825751466849414 	 ± 0.23863657072773828
	data : 0.11827430725097657
	model : 0.06737303733825684
			 train-loss:  2.081284766294518 	 ± 0.23775581959092637
	data : 0.11847362518310547
	model : 0.06803803443908692
			 train-loss:  2.082328846960357 	 ± 0.23677768434730273
	data : 0.11780228614807128
	model : 0.06878952980041504
			 train-loss:  2.080424669981003 	 ± 0.23635142943025256
	data : 0.11710162162780761
	model : 0.06973147392272949
			 train-loss:  2.078639018653643 	 ± 0.23585538825514857
	data : 0.11633520126342774
	model : 0.06916599273681641
			 train-loss:  2.0773149109354208 	 ± 0.2350733360426722
	data : 0.11711750030517579
	model : 0.06947741508483887
			 train-loss:  2.0785371736415383 	 ± 0.23425489118009787
	data : 0.11677651405334473
	model : 0.06974658966064454
			 train-loss:  2.0772928973803153 	 ± 0.2334677138673551
	data : 0.11652588844299316
	model : 0.0699197769165039
			 train-loss:  2.0769383668899537 	 ± 0.2323814309179299
	data : 0.11637163162231445
	model : 0.06979537010192871
			 train-loss:  2.0839698460866822 	 ± 0.2422458736202156
	data : 0.11636176109313964
	model : 0.07098956108093261
			 train-loss:  2.0845028206566782 	 ± 0.2411736594298945
	data : 0.11507744789123535
	model : 0.07092056274414063
			 train-loss:  2.0835023941817106 	 ± 0.24027747072090058
	data : 0.11514906883239746
	model : 0.0708895206451416
			 train-loss:  2.0793843816179747 	 ± 0.24297132946219543
	data : 0.11515703201293945
	model : 0.0698397159576416
			 train-loss:  2.076311654394323 	 ± 0.2439826318538052
	data : 0.1160590648651123
	model : 0.06999073028564454
			 train-loss:  2.075603012566094 	 ± 0.24299481413595148
	data : 0.11573081016540528
	model : 0.06945700645446777
			 train-loss:  2.075594188911574 	 ± 0.2419076014256252
	data : 0.11639223098754883
	model : 0.06856369972229004
			 train-loss:  2.0748343699801284 	 ± 0.2409690399284672
	data : 0.11722254753112793
	model : 0.06847968101501464
			 train-loss:  2.0734587740479853 	 ± 0.2403550550579237
	data : 0.11739110946655273
	model : 0.06923398971557618
			 train-loss:  2.0738681005394977 	 ± 0.23934765583492087
	data : 0.11675014495849609
	model : 0.06842389106750488
			 train-loss:  2.075336998906629 	 ± 0.23883378214488535
	data : 0.11759777069091797
	model : 0.06829509735107422
			 train-loss:  2.0765627327128353 	 ± 0.23817708069950438
	data : 0.1176370620727539
	model : 0.0691978931427002
			 train-loss:  2.0749956652269526 	 ± 0.237770667324796
	data : 0.11701345443725586
	model : 0.06911630630493164
			 train-loss:  2.0733823145137116 	 ± 0.23741724745822423
	data : 0.11707530021667481
	model : 0.06853251457214356
			 train-loss:  2.0715873817602795 	 ± 0.2372353621606951
	data : 0.11753687858581544
	model : 0.06927571296691895
			 train-loss:  2.075300825528862 	 ± 0.23972953040374417
	data : 0.11687278747558594
	model : 0.06920742988586426
			 train-loss:  2.074050003387889 	 ± 0.2391411537618345
	data : 0.11701555252075195
	model : 0.06916947364807129
			 train-loss:  2.0732835996441725 	 ± 0.2383174438663858
	data : 0.11682162284851075
	model : 0.06936130523681641
			 train-loss:  2.076216519840302 	 ± 0.2395730092889039
	data : 0.11686453819274903
	model : 0.07013964653015137
			 train-loss:  2.074779483795166 	 ± 0.23914876952910671
	data : 0.11632499694824219
	model : 0.0693354606628418
			 train-loss:  2.073523065400502 	 ± 0.2386117171053317
	data : 0.11704611778259277
	model : 0.0693709373474121
			 train-loss:  2.072917378793551 	 ± 0.2377676681132281
	data : 0.11696133613586426
	model : 0.06847667694091797
			 train-loss:  2.074695535004139 	 ± 0.2376832975414493
	data : 0.1177701473236084
	model : 0.06754202842712402
			 train-loss:  2.0753570379212847 	 ± 0.23687850895972845
	data : 0.1184312343597412
	model : 0.06753745079040527
			 train-loss:  2.0749735282017636 	 ± 0.236005878993809
	data : 0.11839871406555176
	model : 0.06827287673950196
			 train-loss:  2.075731344805419 	 ± 0.23526208919484906
	data : 0.1178471565246582
	model : 0.0682753086090088
			 train-loss:  2.0760675831274553 	 ± 0.2344008448083108
	data : 0.11784448623657226
	model : 0.06911473274230957
			 train-loss:  2.074992380644146 	 ± 0.2338444890719136
	data : 0.1172393798828125
	model : 0.07005896568298339
			 train-loss:  2.0763316510328607 	 ± 0.23348172510719212
	data : 0.11645455360412597
	model : 0.06909584999084473
			 train-loss:  2.0785182387740524 	 ± 0.2339884332460065
	data : 0.11722831726074219
	model : 0.06878385543823243
			 train-loss:  2.076698240988395 	 ± 0.23408370716475066
	data : 0.11739163398742676
	model : 0.06881847381591796
			 train-loss:  2.077217134245991 	 ± 0.23330631186726575
	data : 0.11729168891906738
	model : 0.06897683143615722
			 train-loss:  2.07643040885096 	 ± 0.23264177594979674
	data : 0.11698741912841797
	model : 0.06905336380004883
			 train-loss:  2.077611847747144 	 ± 0.23221853437344897
	data : 0.11685400009155274
	model : 0.06988091468811035
			 train-loss:  2.0756111715521133 	 ± 0.2325868482206458
	data : 0.1160132884979248
	model : 0.07012557983398438
			 train-loss:  2.0760425565936043 	 ± 0.23181680453617673
	data : 0.11570472717285156
	model : 0.06986079216003419
			 train-loss:  2.0758749809063657 	 ± 0.23100767626460997
	data : 0.11604247093200684
	model : 0.06955351829528808
			 train-loss:  2.0764549153668064 	 ± 0.23030224890630827
	data : 0.11643667221069336
	model : 0.06857213973999024
			 train-loss:  2.0808341710103884 	 ± 0.23540016021082188
	data : 0.1174051284790039
	model : 0.06911544799804688
			 train-loss:  2.080889554681449 	 ± 0.23458797258491632
	data : 0.11707539558410644
	model : 0.06887297630310059
			 train-loss:  2.0819968679179883 	 ± 0.23416314688959092
	data : 0.11735730171203614
	model : 0.06894655227661133
			 train-loss:  2.0823050137279795 	 ± 0.2333950154534106
	data : 0.11728858947753906
	model : 0.06886110305786133
			 train-loss:  2.0816139047210283 	 ± 0.23275605836518223
	data : 0.1172184944152832
	model : 0.06971950531005859
			 train-loss:  2.0812788633692185 	 ± 0.2320094888875737
	data : 0.11619148254394532
	model : 0.06932096481323242
			 train-loss:  2.0792126711209615 	 ± 0.23260621299724746
	data : 0.1166773796081543
	model : 0.0697014331817627
			 train-loss:  2.079151466982254 	 ± 0.23183592614572138
	data : 0.11625771522521973
	model : 0.06919660568237304
			 train-loss:  2.0764121350489164 	 ± 0.23351099957860677
	data : 0.11659436225891114
	model : 0.06864652633666993
			 train-loss:  2.0758803077772554 	 ± 0.23283897975187678
	data : 0.11730546951293945
	model : 0.06867518424987792
			 train-loss:  2.0751415127283566 	 ± 0.23226162353176477
	data : 0.11760940551757812
	model : 0.06868109703063965
			 train-loss:  2.074233265076914 	 ± 0.23178538187955636
	data : 0.1176037311553955
	model : 0.06848196983337403
			 train-loss:  2.0736864446065364 	 ± 0.23114156403653163
	data : 0.11784367561340332
	model : 0.06922307014465331
			 train-loss:  2.072121450096179 	 ± 0.23123192547584112
	data : 0.11725816726684571
	model : 0.07019968032836914
			 train-loss:  2.0732804612268376 	 ± 0.2309560481490912
	data : 0.11634821891784668
	model : 0.07020859718322754
			 train-loss:  2.071273559294407 	 ± 0.23160654059838856
	data : 0.11607513427734376
	model : 0.0701028823852539
			 train-loss:  2.0696448631584645 	 ± 0.23179322949143147
	data : 0.11614646911621093
	model : 0.07038559913635253
			 train-loss:  2.0688044743508285 	 ± 0.2313166376619399
	data : 0.11585521697998047
	model : 0.07038731575012207
			 train-loss:  2.069565047452479 	 ± 0.23080344072405892
	data : 0.11603484153747559
	model : 0.07062296867370606
			 train-loss:  2.070662464832236 	 ± 0.2305179335073077
	data : 0.11572895050048829
	model : 0.07091550827026367
			 train-loss:  2.069939564640929 	 ± 0.22999931221800823
	data : 0.11551089286804199
	model : 0.07113838195800781
			 train-loss:  2.069376398577835 	 ± 0.22941467490986409
	data : 0.11520967483520508
	model : 0.07136883735656738
			 train-loss:  2.0689173800399505 	 ± 0.22879860895445733
	data : 0.11519408226013184
	model : 0.07126088142395019
			 train-loss:  2.069497733059044 	 ± 0.22823507173210603
	data : 0.11501445770263671
	model : 0.07079963684082032
			 train-loss:  2.067085882027944 	 ± 0.2296793944992323
	data : 0.1155327320098877
	model : 0.0706632137298584
			 train-loss:  2.0670212147503917 	 ± 0.2290003954228205
	data : 0.11575865745544434
	model : 0.07038226127624511
			 train-loss:  2.06851987137514 	 ± 0.22915556458798453
	data : 0.11612143516540527
	model : 0.07001395225524902
			 train-loss:  2.067625978536773 	 ± 0.2287816011313577
	data : 0.11641879081726074
	model : 0.06998405456542969
			 train-loss:  2.0674988987833953 	 ± 0.22812162168150849
	data : 0.11638736724853516
	model : 0.07018194198608399
			 train-loss:  2.0691904553099176 	 ± 0.22854063662262447
	data : 0.11611375808715821
	model : 0.06996345520019531
			 train-loss:  2.0693434915323365 	 ± 0.22789185400389872
	data : 0.11623740196228027
	model : 0.06978034973144531
			 train-loss:  2.0707345540182933 	 ± 0.2279794441734373
	data : 0.1163238525390625
	model : 0.06979618072509766
			 train-loss:  2.0717894529754464 	 ± 0.22775877294158936
	data : 0.11633896827697754
	model : 0.07002649307250977
			 train-loss:  2.0723522989089878 	 ± 0.22723719081626467
	data : 0.11628408432006836
	model : 0.06985864639282227
			 train-loss:  2.0733475028798822 	 ± 0.22698447770091076
	data : 0.1164881706237793
	model : 0.06978030204772949
			 train-loss:  2.0751590502328714 	 ± 0.22763625426410813
	data : 0.11649718284606933
	model : 0.06989111900329589
			 train-loss:  2.07659551832411 	 ± 0.22781514627618554
	data : 0.11631531715393066
	model : 0.06988048553466797
			 train-loss:  2.077752866797684 	 ± 0.22771496209309797
	data : 0.11619229316711426
	model : 0.06976308822631835
			 train-loss:  2.0775543569208503 	 ± 0.22710421356487787
	data : 0.11623678207397461
	model : 0.07056503295898438
			 train-loss:  2.079528381264275 	 ± 0.22804319438996745
	data : 0.11534528732299805
	model : 0.07038159370422363
			 train-loss:  2.0790397116671437 	 ± 0.22751872393453237
	data : 0.11555213928222656
	model : 0.07061724662780762
			 train-loss:  2.078507325455949 	 ± 0.22701786764252632
	data : 0.11535854339599609
	model : 0.0703813076019287
			 train-loss:  2.0787293193160847 	 ± 0.22642691539676837
	data : 0.1157076358795166
	model : 0.06951122283935547
			 train-loss:  2.0777719938818784 	 ± 0.22619780113595772
	data : 0.11654233932495117
	model : 0.06876444816589355
			 train-loss:  2.077912078258839 	 ± 0.22560354215845319
	data : 0.11720089912414551
	model : 0.0690892219543457
			 train-loss:  2.079122505490742 	 ± 0.22561716992921466
	data : 0.11688966751098633
	model : 0.0691112995147705
			 train-loss:  2.0793146622808356 	 ± 0.22503816333081728
	data : 0.11695270538330078
	model : 0.06931447982788086
			 train-loss:  2.0782800363620537 	 ± 0.2249009077983002
	data : 0.1168184757232666
	model : 0.07025527954101562
			 train-loss:  2.0782840320219598 	 ± 0.2243144705521184
	data : 0.11613225936889648
	model : 0.07018952369689942
			 train-loss:  2.0787836671493216 	 ± 0.22383967905479996
	data : 0.11626129150390625
	model : 0.07015833854675294
			 train-loss:  2.0793709466137837 	 ± 0.22341105163126443
	data : 0.11632204055786133
	model : 0.0702131748199463
			 train-loss:  2.0812700289946338 	 ± 0.22440187200053383
	data : 0.11611661911010743
	model : 0.07039465904235839
			 train-loss:  2.0821207463741302 	 ± 0.22414371769122313
	data : 0.11573319435119629
	model : 0.07034564018249512
			 train-loss:  2.083980380581115 	 ± 0.2250848587913565
	data : 0.11580538749694824
	model : 0.0709676742553711
			 train-loss:  2.0830794289858656 	 ± 0.22487157791893245
	data : 0.1151050090789795
	model : 0.07070651054382324
			 train-loss:  2.0824893431447857 	 ± 0.224459492441309
	data : 0.11547355651855469
	model : 0.07001581192016601
			 train-loss:  2.0822390496730803 	 ± 0.22392547900104778
	data : 0.11605114936828613
	model : 0.06991047859191894
			 train-loss:  2.082545836766561 	 ± 0.22340988795960673
	data : 0.11612501144409179
	model : 0.06937642097473144
			 train-loss:  2.0815325149215096 	 ± 0.22331878580054268
	data : 0.11646628379821777
	model : 0.06905436515808105
			 train-loss:  2.0810271413455457 	 ± 0.22288382636720874
	data : 0.11684365272521972
	model : 0.06915192604064942
			 train-loss:  2.0817827114871905 	 ± 0.22259733663286382
	data : 0.11650753021240234
	model : 0.06978435516357422
			 train-loss:  2.080959694559981 	 ± 0.22236467758081097
	data : 0.11603069305419922
	model : 0.069775390625
			 train-loss:  2.0818805544121752 	 ± 0.22221578916741583
	data : 0.11600842475891113
	model : 0.07026505470275879
			 train-loss:  2.0812872147214585 	 ± 0.22184190233439746
	data : 0.11572694778442383
	model : 0.06935324668884277
			 train-loss:  2.0792906679786167 	 ± 0.22316444128206994
	data : 0.11664323806762696
	model : 0.06930050849914551
			 train-loss:  2.0778364946967676 	 ± 0.22361556286970666
	data : 0.11674318313598633
	model : 0.06828122138977051
			 train-loss:  2.0798667584146773 	 ± 0.22500510648140037
	data : 0.1177591323852539
	model : 0.06830706596374511
			 train-loss:  2.082394642287521 	 ± 0.22744076038760638
	data : 0.11782298088073731
	model : 0.06843938827514648
			 train-loss:  2.0811578462708673 	 ± 0.2276138232657678
	data : 0.11765923500061035
	model : 0.06934924125671386
			 train-loss:  2.080112011219974 	 ± 0.22758888744887537
	data : 0.11684679985046387
	model : 0.0692441463470459
			 train-loss:  2.081038641595395 	 ± 0.227458902153176
	data : 0.11717429161071777
	model : 0.06926465034484863
			 train-loss:  2.081519701868989 	 ± 0.22703840215783055
	data : 0.11703457832336425
	model : 0.06900625228881836
			 train-loss:  2.0812191946638956 	 ± 0.22655509403120672
	data : 0.11723208427429199
	model : 0.06894874572753906
			 train-loss:  2.0802329070007746 	 ± 0.22649679156993707
	data : 0.11742200851440429
	model : 0.06887245178222656
			 train-loss:  2.079558096347599 	 ± 0.22619524088737264
	data : 0.11752614974975586
	model : 0.06905155181884766
			 train-loss:  2.0796764011252415 	 ± 0.225684982334376
	data : 0.11729273796081544
	model : 0.06995649337768554
			 train-loss:  2.0789194025776605 	 ± 0.22544997589569948
	data : 0.11652417182922363
	model : 0.06921000480651855
			 train-loss:  2.0791214521114645 	 ± 0.22495929267221365
	data : 0.11730403900146484
	model : 0.06908493041992188
			 train-loss:  2.079986117444597 	 ± 0.22481982813577722
	data : 0.11705145835876465
	model : 0.06880412101745606
			 train-loss:  2.0789855959169534 	 ± 0.22480999140246888
	data : 0.11715717315673828
	model : 0.06890220642089843
			 train-loss:  2.0796590575150082 	 ± 0.22453296214471868
	data : 0.11697559356689453
	model : 0.0682596206665039
			 train-loss:  2.080310592651367 	 ± 0.224245561283488
	data : 0.11756634712219238
	model : 0.06880602836608887
			 train-loss:  2.081342008261554 	 ± 0.22428313824110163
	data : 0.11696152687072754
	model : 0.06874761581420899
			 train-loss:  2.07990722666753 	 ± 0.2248256446764568
	data : 0.11717500686645507
	model : 0.0687868595123291
			 train-loss:  2.0793510884569404 	 ± 0.2244884936550715
	data : 0.11718611717224121
	model : 0.06869664192199706
			 train-loss:  2.080448377600924 	 ± 0.22460975006799633
	data : 0.11709275245666503
	model : 0.06927824020385742
			 train-loss:  2.080824695462766 	 ± 0.22419327346235973
	data : 0.11660194396972656
	model : 0.0694535255432129
			 train-loss:  2.0831980963289998 	 ± 0.22658471712361644
	data : 0.11636209487915039
	model : 0.06950325965881347
			 train-loss:  2.083310511605493 	 ± 0.2261023161967603
	data : 0.11626644134521484
	model : 0.0695080280303955
			 train-loss:  2.0827302303437 	 ± 0.22578965669637874
	data : 0.11624183654785156
	model : 0.06950879096984863
			 train-loss:  2.081344343148745 	 ± 0.22629763784619936
	data : 0.11624102592468262
	model : 0.06955552101135254
			 train-loss:  2.080821933644883 	 ± 0.225956997445184
	data : 0.1161573886871338
	model : 0.0695258617401123
			 train-loss:  2.081209870213169 	 ± 0.22555617835066533
	data : 0.11628165245056152
	model : 0.06951990127563476
			 train-loss:  2.080363313860028 	 ± 0.22545521920040665
	data : 0.11647210121154786
	model : 0.06950626373291016
			 train-loss:  2.081701141445577 	 ± 0.22592180658169647
	data : 0.11648263931274414
	model : 0.06953134536743164
			 train-loss:  2.0802241773286125 	 ± 0.22659718026862172
	data : 0.11653804779052734
	model : 0.06925315856933593
			 train-loss:  2.0796340639392534 	 ± 0.22630856622037773
	data : 0.11672320365905761
	model : 0.06938633918762208
			 train-loss:  2.0788066852142206 	 ± 0.22620200651429764
	data : 0.11662969589233399
	model : 0.06934175491333008
			 train-loss:  2.0788653754005746 	 ± 0.22573600192140517
	data : 0.1165886402130127
	model : 0.069252347946167
			 train-loss:  2.0791531335179205 	 ± 0.22531551828501453
	data : 0.11669325828552246
	model : 0.06915922164916992
			 train-loss:  2.078984672905969 	 ± 0.22486866624092552
	data : 0.11678109169006348
	model : 0.06894006729125976
			 train-loss:  2.0816476938675863 	 ± 0.22823210912453176
	data : 0.1170842170715332
	model : 0.06801981925964355
			 train-loss:  2.081689204626936 	 ± 0.22776867708767257
	data : 0.11780867576599122
	model : 0.068062162399292
			 train-loss:  2.0804322716678203 	 ± 0.22816043951334677
	data : 0.11779747009277344
	model : 0.06822571754455567
			 train-loss:  2.079704393782923 	 ± 0.22798714938654496
	data : 0.1176767349243164
	model : 0.06828322410583496
			 train-loss:  2.079179676661051 	 ± 0.2276788837209905
	data : 0.1176302433013916
	model : 0.06820049285888671
			 train-loss:  2.0803100996017454 	 ± 0.22792215676904667
	data : 0.11760182380676269
	model : 0.06876211166381836
			 train-loss:  2.0817617553163807 	 ± 0.22862276669487933
	data : 0.1170628547668457
	model : 0.06798825263977051
			 train-loss:  2.080996324145605 	 ± 0.2284907271442507
	data : 0.1174166202545166
	model : 0.0679551601409912
			 train-loss:  2.08191621727623 	 ± 0.22850579830803797
	data : 0.11752638816833497
	model : 0.06765317916870117
			 train-loss:  2.080803299044061 	 ± 0.2287415395098813
	data : 0.11777267456054688
	model : 0.06791653633117675
			 train-loss:  2.081256936110702 	 ± 0.22840703724647543
	data : 0.11750726699829102
	model : 0.06838021278381348
			 train-loss:  2.084607971366495 	 ± 0.23415698364399018
	data : 0.11619195938110352
	model : 0.0603844165802002
#epoch  92    val-loss:  2.4818818318216422  train-loss:  2.084607971366495  lr:  1.220703125e-06
			 train-loss:  1.752055048942566 	 ± 0.0
	data : 6.018718481063843
	model : 0.07380008697509766
			 train-loss:  1.8947356343269348 	 ± 0.1426805853843689
	data : 3.0758057832717896
	model : 0.07189226150512695
			 train-loss:  1.9998464981714885 	 ± 0.18886084919868218
	data : 2.0891166528066
	model : 0.07051833470662434
			 train-loss:  2.0244899690151215 	 ± 0.16903614180316143
	data : 1.5959852933883667
	model : 0.06963658332824707
			 train-loss:  1.9425205945968629 	 ± 0.22301230270691685
	data : 1.3003340244293213
	model : 0.0691688060760498
			 train-loss:  1.9749684929847717 	 ± 0.21612435428358118
	data : 0.12025213241577148
	model : 0.06790308952331543
			 train-loss:  1.9641637631825037 	 ± 0.20183500719405517
	data : 0.11742887496948243
	model : 0.06781401634216308
			 train-loss:  1.9778784364461899 	 ± 0.19225463505987034
	data : 0.11744747161865235
	model : 0.06814885139465332
			 train-loss:  2.0345548391342163 	 ± 0.241976631189234
	data : 0.11723165512084961
	model : 0.06865544319152832
			 train-loss:  2.0510255217552187 	 ± 0.23481688932344189
	data : 0.11681604385375977
	model : 0.06915087699890136
			 train-loss:  2.0618994561108677 	 ± 0.2265143753086569
	data : 0.1164306640625
	model : 0.06970477104187012
			 train-loss:  2.0413177410761514 	 ± 0.2273603064913076
	data : 0.11590566635131835
	model : 0.06988983154296875
			 train-loss:  2.0357996683854322 	 ± 0.21927548028488753
	data : 0.11585211753845215
	model : 0.06923322677612305
			 train-loss:  2.024362470422472 	 ± 0.21528550807218472
	data : 0.11646084785461426
	model : 0.06928949356079102
			 train-loss:  2.040392200152079 	 ± 0.21646090781716584
	data : 0.11649794578552246
	model : 0.06959238052368164
			 train-loss:  2.040341831743717 	 ± 0.2095874635548737
	data : 0.11622896194458007
	model : 0.06944952011108399
			 train-loss:  2.0549852357191196 	 ± 0.21159829341943132
	data : 0.11639442443847656
	model : 0.06863541603088379
			 train-loss:  2.059081243144141 	 ± 0.2063289026092634
	data : 0.11709733009338379
	model : 0.06981244087219238
			 train-loss:  2.0662856541181864 	 ± 0.20313854473112924
	data : 0.11627058982849121
	model : 0.06953883171081543
			 train-loss:  2.087168735265732 	 ± 0.2179173316278244
	data : 0.11641216278076172
	model : 0.06929354667663574
			 train-loss:  2.06837416830517 	 ± 0.22867301278656157
	data : 0.11653609275817871
	model : 0.06964430809020997
			 train-loss:  2.07349372993816 	 ± 0.22464388599631963
	data : 0.11620121002197266
	model : 0.07044873237609864
			 train-loss:  2.0678601472274116 	 ± 0.22128933567754186
	data : 0.11546378135681153
	model : 0.07011709213256836
			 train-loss:  2.0597556680440903 	 ± 0.22008929037151934
	data : 0.11569275856018066
	model : 0.07129545211791992
			 train-loss:  2.0558121824264526 	 ± 0.2165062350104166
	data : 0.11466903686523437
	model : 0.07137055397033691
			 train-loss:  2.0465643130815945 	 ± 0.21727895503090391
	data : 0.11477289199829102
	model : 0.07117218971252441
			 train-loss:  2.072255496625547 	 ± 0.25024504020159744
	data : 0.11494212150573731
	model : 0.0701756477355957
			 train-loss:  2.0569279321602414 	 ± 0.25832011865483073
	data : 0.11563177108764648
	model : 0.06936945915222167
			 train-loss:  2.0478964715168395 	 ± 0.2582869679192437
	data : 0.11632337570190429
	model : 0.0685051441192627
			 train-loss:  2.0386359810829164 	 ± 0.25879598724267844
	data : 0.11712970733642578
	model : 0.06834774017333985
			 train-loss:  2.0294179839472615 	 ± 0.2595457793292716
	data : 0.1172095775604248
	model : 0.06745624542236328
			 train-loss:  2.0342755764722824 	 ± 0.2568859057353351
	data : 0.11812524795532227
	model : 0.06774134635925293
			 train-loss:  2.045588977409132 	 ± 0.26093378035694975
	data : 0.11796417236328124
	model : 0.06757664680480957
			 train-loss:  2.043703689294703 	 ± 0.25729591469202845
	data : 0.11807007789611816
	model : 0.06757216453552246
			 train-loss:  2.0408118792942593 	 ± 0.25415360006042864
	data : 0.11814370155334472
	model : 0.06713633537292481
			 train-loss:  2.0343164205551147 	 ± 0.25352802140552977
	data : 0.11853213310241699
	model : 0.0681574821472168
			 train-loss:  2.034210108421944 	 ± 0.2500793133422516
	data : 0.11750154495239258
	model : 0.06866397857666015
			 train-loss:  2.0356228288851286 	 ± 0.24691643563043475
	data : 0.11717195510864258
	model : 0.06946935653686523
			 train-loss:  2.0313496620227127 	 ± 0.2451496062516973
	data : 0.11652135848999023
	model : 0.06937284469604492
			 train-loss:  2.0488413363695144 	 ± 0.265571571136588
	data : 0.11673150062561036
	model : 0.06895508766174316
			 train-loss:  2.0553844992707417 	 ± 0.2655571071866723
	data : 0.11705160140991211
	model : 0.0688441276550293
			 train-loss:  2.0710800914537337 	 ± 0.28096606910613864
	data : 0.11721863746643066
	model : 0.06896281242370605
			 train-loss:  2.0706616750983304 	 ± 0.2776930430166429
	data : 0.11698250770568848
	model : 0.06907567977905274
			 train-loss:  2.070461086251519 	 ± 0.2745224552128723
	data : 0.1169351577758789
	model : 0.06921844482421875
			 train-loss:  2.070763527022468 	 ± 0.2714624820297976
	data : 0.11674871444702148
	model : 0.07031812667846679
			 train-loss:  2.0657227894534236 	 ± 0.27061649684148176
	data : 0.11583294868469238
	model : 0.07022309303283691
			 train-loss:  2.070061518790874 	 ± 0.2693344827817927
	data : 0.11588931083679199
	model : 0.07021594047546387
			 train-loss:  2.0694030101100602 	 ± 0.266552381793467
	data : 0.11602139472961426
	model : 0.06927061080932617
			 train-loss:  2.068142309480784 	 ± 0.2639629866029562
	data : 0.11670398712158203
	model : 0.07002792358398438
			 train-loss:  2.0672148728370665 	 ± 0.2613906576909411
	data : 0.1159019947052002
	model : 0.06927385330200195
			 train-loss:  2.067704030111724 	 ± 0.2588384289454157
	data : 0.11647839546203613
	model : 0.06936559677124024
			 train-loss:  2.0653030482622294 	 ± 0.25691033935662244
	data : 0.11642313003540039
	model : 0.06946825981140137
			 train-loss:  2.0722066568878463 	 ± 0.2592988517819756
	data : 0.11636586189270019
	model : 0.07051811218261719
			 train-loss:  2.06840842962265 	 ± 0.2583706474937995
	data : 0.11547198295593261
	model : 0.06900858879089355
			 train-loss:  2.0779743606393986 	 ± 0.26548641594520883
	data : 0.116908597946167
	model : 0.06961030960083008
			 train-loss:  2.0817727042096004 	 ± 0.2646089929557397
	data : 0.11626806259155273
	model : 0.06930580139160156
			 train-loss:  2.0764721736573337 	 ± 0.26526004138975506
	data : 0.11636061668395996
	model : 0.07003731727600097
			 train-loss:  2.07729880563144 	 ± 0.2630374223448569
	data : 0.11573238372802734
	model : 0.06959500312805175
			 train-loss:  2.081471572488041 	 ± 0.26272778877778097
	data : 0.11613316535949707
	model : 0.07034783363342285
			 train-loss:  2.080490775903066 	 ± 0.2606380924461851
	data : 0.11551413536071778
	model : 0.06942410469055176
			 train-loss:  2.081985727685397 	 ± 0.2587521308341005
	data : 0.11646804809570313
	model : 0.06873960494995117
			 train-loss:  2.075637842378309 	 ± 0.26140164122365694
	data : 0.1171177864074707
	model : 0.06807632446289062
			 train-loss:  2.0797278635085577 	 ± 0.2613108394069975
	data : 0.1177781105041504
	model : 0.06821737289428711
			 train-loss:  2.0736598428338766 	 ± 0.26369706576334406
	data : 0.11778578758239747
	model : 0.06765432357788086
			 train-loss:  2.065891383244441 	 ± 0.2689399364277411
	data : 0.11825122833251953
	model : 0.06767086982727051
			 train-loss:  2.0628384297544304 	 ± 0.26802730140440495
	data : 0.1181854248046875
	model : 0.06838631629943848
			 train-loss:  2.0635038009330406 	 ± 0.26607449204483896
	data : 0.11756219863891601
	model : 0.06839418411254883
			 train-loss:  2.061934274785659 	 ± 0.2644230921435889
	data : 0.11761717796325684
	model : 0.06856002807617187
			 train-loss:  2.0619374358135723 	 ± 0.26249999088142845
	data : 0.11730308532714843
	model : 0.06881251335144042
			 train-loss:  2.064821835926601 	 ± 0.2617172787531806
	data : 0.11724457740783692
	model : 0.06970171928405762
			 train-loss:  2.069790927457138 	 ± 0.26317225058491583
	data : 0.11647210121154786
	model : 0.07003488540649414
			 train-loss:  2.0735784272352853 	 ± 0.2632796993664757
	data : 0.1161468505859375
	model : 0.06989049911499023
			 train-loss:  2.081524522337195 	 ± 0.27002365429974773
	data : 0.11622695922851563
	model : 0.06954517364501953
			 train-loss:  2.089648626946114 	 ± 0.27702986238326743
	data : 0.11647968292236328
	model : 0.06964893341064453
			 train-loss:  2.084131557146708 	 ± 0.2792394798413897
	data : 0.11616911888122558
	model : 0.06964449882507324
			 train-loss:  2.089797125050896 	 ± 0.2817021545051346
	data : 0.11617493629455566
	model : 0.06936225891113282
			 train-loss:  2.0922262405420278 	 ± 0.28066697500554405
	data : 0.11649928092956544
	model : 0.06848831176757812
			 train-loss:  2.091899173382001 	 ± 0.27887679237836804
	data : 0.11731009483337403
	model : 0.0688387393951416
			 train-loss:  2.0894514337370667 	 ± 0.27794808465482185
	data : 0.11711707115173339
	model : 0.06915106773376464
			 train-loss:  2.0833715841174127 	 ± 0.28144208939256765
	data : 0.11691455841064453
	model : 0.06906261444091796
			 train-loss:  2.079423302485619 	 ± 0.28191996559222376
	data : 0.11696853637695312
	model : 0.06949658393859863
			 train-loss:  2.0758134097587773 	 ± 0.2820729513607438
	data : 0.11668429374694825
	model : 0.06976408958435058
			 train-loss:  2.0773767706859543 	 ± 0.2807257551624013
	data : 0.11632304191589356
	model : 0.06978483200073242
			 train-loss:  2.0757970738978613 	 ± 0.27942063868144534
	data : 0.11633715629577637
	model : 0.07020711898803711
			 train-loss:  2.0745304289986106 	 ± 0.2780146077115168
	data : 0.11602869033813476
	model : 0.07048940658569336
			 train-loss:  2.0752119394235833 	 ± 0.27646492597472033
	data : 0.1159095287322998
	model : 0.0700685977935791
			 train-loss:  2.076471785019184 	 ± 0.2751196410057566
	data : 0.1162259578704834
	model : 0.06975083351135254
			 train-loss:  2.078865670345046 	 ± 0.2744617709502284
	data : 0.11646404266357421
	model : 0.06972451210021972
			 train-loss:  2.082347418485063 	 ± 0.2748629725178793
	data : 0.11659750938415528
	model : 0.06904525756835937
			 train-loss:  2.0832069012853833 	 ± 0.2734519305399164
	data : 0.11709251403808593
	model : 0.06861481666564942
			 train-loss:  2.0808589916962843 	 ± 0.27285597966295577
	data : 0.11748433113098145
	model : 0.06881422996520996
			 train-loss:  2.0785046660381816 	 ± 0.27229678955577413
	data : 0.11726202964782714
	model : 0.06984262466430664
			 train-loss:  2.082217490801247 	 ± 0.2731602166211937
	data : 0.11631078720092773
	model : 0.06977958679199218
			 train-loss:  2.083528533894965 	 ± 0.27199735827905797
	data : 0.11609253883361817
	model : 0.06958770751953125
			 train-loss:  2.0818612211628964 	 ± 0.2710444846788906
	data : 0.11631731986999512
	model : 0.06980667114257813
			 train-loss:  2.084526897718509 	 ± 0.2708780261979568
	data : 0.11613144874572753
	model : 0.06979608535766602
			 train-loss:  2.086218009290007 	 ± 0.2699870536856479
	data : 0.11614165306091309
	model : 0.06970510482788086
			 train-loss:  2.083262618707151 	 ± 0.270178521150724
	data : 0.11638431549072266
	model : 0.06928672790527343
			 train-loss:  2.082310099794407 	 ± 0.26897585435227334
	data : 0.11704716682434083
	model : 0.06966910362243653
			 train-loss:  2.080174129009247 	 ± 0.2684701191391711
	data : 0.11677069664001465
	model : 0.06962413787841797
			 train-loss:  2.0785964231679936 	 ± 0.2676032411088386
	data : 0.11674580574035645
	model : 0.06956114768981933
			 train-loss:  2.077185536132139 	 ± 0.2666654683306358
	data : 0.11677908897399902
	model : 0.06953139305114746
			 train-loss:  2.0783487164858476 	 ± 0.26562771673632685
	data : 0.11665668487548828
	model : 0.0700162410736084
			 train-loss:  2.077298110494247 	 ± 0.26456252455318124
	data : 0.11614522933959961
	model : 0.0699150562286377
			 train-loss:  2.0739979516892206 	 ± 0.2654418867986401
	data : 0.11602745056152344
	model : 0.06974878311157226
			 train-loss:  2.0743036000233777 	 ± 0.2642053996156293
	data : 0.11614441871643066
	model : 0.06982007026672363
			 train-loss:  2.080284562066337 	 ± 0.2700813523417542
	data : 0.1160311222076416
	model : 0.06958832740783691
			 train-loss:  2.081642554865943 	 ± 0.2691948244585083
	data : 0.11619777679443359
	model : 0.06948671340942383
			 train-loss:  2.083926964243618 	 ± 0.26900674712949063
	data : 0.1163412094116211
	model : 0.06928935050964355
			 train-loss:  2.0818762768398633 	 ± 0.26863571892191596
	data : 0.11655025482177735
	model : 0.06860408782958985
			 train-loss:  2.0814917205690264 	 ± 0.2674533233793982
	data : 0.11717090606689454
	model : 0.06825795173645019
			 train-loss:  2.0850858273250714 	 ± 0.2689357953427887
	data : 0.11763458251953125
	model : 0.06752653121948242
			 train-loss:  2.0881980404389644 	 ± 0.2697614174097071
	data : 0.11839895248413086
	model : 0.06759252548217773
			 train-loss:  2.086345792862407 	 ± 0.2692964185614426
	data : 0.11813645362854004
	model : 0.06785669326782226
			 train-loss:  2.085227185746898 	 ± 0.26838888503412844
	data : 0.11796336174011231
	model : 0.0688471794128418
			 train-loss:  2.0909249135132493 	 ± 0.2741258737710324
	data : 0.11711115837097168
	model : 0.06912198066711425
			 train-loss:  2.0930796392962465 	 ± 0.27393667072921457
	data : 0.11676106452941895
	model : 0.06997928619384766
			 train-loss:  2.0987239439608687 	 ± 0.27952238344311087
	data : 0.11603760719299316
	model : 0.06904802322387696
			 train-loss:  2.0968990295874974 	 ± 0.2790504647517257
	data : 0.11681189537048339
	model : 0.06886100769042969
			 train-loss:  2.0945308128992717 	 ± 0.2790836035661009
	data : 0.11705536842346191
	model : 0.06873769760131836
			 train-loss:  2.09420457556228 	 ± 0.2779509485941296
	data : 0.11723561286926269
	model : 0.06787052154541015
			 train-loss:  2.092694502384936 	 ± 0.27730740484466526
	data : 0.11813640594482422
	model : 0.06799430847167968
			 train-loss:  2.0966646448383486 	 ± 0.2796375622624968
	data : 0.11785798072814942
	model : 0.06891422271728516
			 train-loss:  2.0932982410154035 	 ± 0.2809990443183299
	data : 0.1171760082244873
	model : 0.06906337738037109
			 train-loss:  2.0906245460510253 	 ± 0.28145196953495866
	data : 0.11694092750549316
	model : 0.06816949844360351
			 train-loss:  2.0936025381088257 	 ± 0.2823031590154861
	data : 0.11767382621765136
	model : 0.06825861930847169
			 train-loss:  2.09296690760635 	 ± 0.28128003949574704
	data : 0.1172327995300293
	model : 0.06984939575195312
			 train-loss:  2.092065803706646 	 ± 0.2803631041192458
	data : 0.11584529876708985
	model : 0.07018604278564453
			 train-loss:  2.095526323762051 	 ± 0.2820052595330333
	data : 0.1156466007232666
	model : 0.07019534111022949
			 train-loss:  2.095102280836839 	 ± 0.28095981262235403
	data : 0.11560187339782715
	model : 0.07149128913879395
			 train-loss:  2.095888829413261 	 ± 0.2800290317170457
	data : 0.11441855430603028
	model : 0.0724398136138916
			 train-loss:  2.0991120085571753 	 ± 0.2813949889677929
	data : 0.11368803977966309
	model : 0.07133374214172364
			 train-loss:  2.0974297810317877 	 ± 0.2810005752124107
	data : 0.11476984024047851
	model : 0.07126479148864746
			 train-loss:  2.097113237452151 	 ± 0.279973902786749
	data : 0.11492791175842285
	model : 0.07068943977355957
			 train-loss:  2.0950726288336297 	 ± 0.279933458065518
	data : 0.1154792308807373
	model : 0.07055277824401855
			 train-loss:  2.098423676455722 	 ± 0.2816070481047554
	data : 0.1155618667602539
	model : 0.07020335197448731
			 train-loss:  2.0982893009255403 	 ± 0.2805817789596763
	data : 0.11596765518188476
	model : 0.07002615928649902
			 train-loss:  2.097143544667009 	 ± 0.27988480346685096
	data : 0.1161696434020996
	model : 0.0692166805267334
			 train-loss:  2.097861197354982 	 ± 0.27900360560911647
	data : 0.11680426597595214
	model : 0.06887788772583008
			 train-loss:  2.1017540557043892 	 ± 0.2817684236050552
	data : 0.11714534759521485
	model : 0.06858153343200683
			 train-loss:  2.1012341942347534 	 ± 0.28083483811447885
	data : 0.1173168659210205
	model : 0.06819915771484375
			 train-loss:  2.0997604081328487 	 ± 0.280390896441255
	data : 0.11761264801025391
	model : 0.06766514778137207
			 train-loss:  2.0985038513903853 	 ± 0.2798097209808106
	data : 0.11775379180908203
	model : 0.06768832206726075
			 train-loss:  2.1000410500499935 	 ± 0.27944173155289187
	data : 0.11764345169067383
	model : 0.06867966651916504
			 train-loss:  2.098634121335786 	 ± 0.27898778856884343
	data : 0.116782808303833
	model : 0.06865835189819336
			 train-loss:  2.09753914483606 	 ± 0.2783431811072799
	data : 0.11676321029663086
	model : 0.0690680980682373
			 train-loss:  2.096091778099942 	 ± 0.27794556524840003
	data : 0.11620430946350098
	model : 0.06903481483459473
			 train-loss:  2.0966733951826355 	 ± 0.27709471207423364
	data : 0.11647305488586426
	model : 0.06930880546569824
			 train-loss:  2.0964155501167245 	 ± 0.27618111296822484
	data : 0.1163088321685791
	model : 0.06920857429504394
			 train-loss:  2.0952258857091266 	 ± 0.275641760905288
	data : 0.11649088859558106
	model : 0.06938724517822266
			 train-loss:  2.094670671500907 	 ± 0.2748116658653674
	data : 0.11652655601501465
	model : 0.06965389251708984
			 train-loss:  2.094550724092283 	 ± 0.27391015412779385
	data : 0.11635289192199708
	model : 0.06896367073059081
			 train-loss:  2.095618092156703 	 ± 0.2733305164245435
	data : 0.11689138412475586
	model : 0.06927247047424316
			 train-loss:  2.0946848268632765 	 ± 0.27268609185590387
	data : 0.11649184226989746
	model : 0.06917157173156738
			 train-loss:  2.092058160228114 	 ± 0.27375259216470277
	data : 0.11642723083496094
	model : 0.06905803680419922
			 train-loss:  2.09258148761896 	 ± 0.2729515416792368
	data : 0.116615629196167
	model : 0.0687647819519043
			 train-loss:  2.0923650234368196 	 ± 0.2720943131698326
	data : 0.11692004203796387
	model : 0.0693784236907959
			 train-loss:  2.091158462476127 	 ± 0.2716528966216387
	data : 0.1163780689239502
	model : 0.06894350051879883
			 train-loss:  2.095194012863831 	 ± 0.27550736879095783
	data : 0.11672711372375488
	model : 0.06903338432312012
			 train-loss:  2.096337525546551 	 ± 0.27502330828247024
	data : 0.11660284996032715
	model : 0.06901736259460449
			 train-loss:  2.0970784907015214 	 ± 0.27432802385381505
	data : 0.11636915206909179
	model : 0.06927056312561035
			 train-loss:  2.095770024958952 	 ± 0.27398351701432616
	data : 0.11626901626586914
	model : 0.06955127716064453
			 train-loss:  2.0933490772188805 	 ± 0.27487436057507064
	data : 0.11610631942749024
	model : 0.06922059059143067
			 train-loss:  2.091351536715903 	 ± 0.27521919392041094
	data : 0.11645212173461914
	model : 0.06872196197509765
			 train-loss:  2.091807709318219 	 ± 0.27444611084135867
	data : 0.11692347526550292
	model : 0.0687230110168457
			 train-loss:  2.0913913494133087 	 ± 0.27367048153332296
	data : 0.11705141067504883
	model : 0.06859593391418457
			 train-loss:  2.09012631884592 	 ± 0.2733362515442039
	data : 0.11713480949401855
	model : 0.06771249771118164
			 train-loss:  2.0872773343608495 	 ± 0.27499723438840673
	data : 0.11798663139343261
	model : 0.0684694766998291
			 train-loss:  2.0879436075334716 	 ± 0.27431839395973984
	data : 0.11738395690917969
	model : 0.06895794868469238
			 train-loss:  2.0889786762349747 	 ± 0.2738411799680027
	data : 0.11678557395935059
	model : 0.06911630630493164
			 train-loss:  2.0910688380748903 	 ± 0.2743959761696679
	data : 0.11648883819580078
	model : 0.0682943344116211
			 train-loss:  2.0892571099968844 	 ± 0.27462098277699637
	data : 0.1171630859375
	model : 0.06898117065429688
			 train-loss:  2.086861241070521 	 ± 0.275623043143488
	data : 0.11653552055358887
	model : 0.06917705535888671
			 train-loss:  2.0865929345974976 	 ± 0.2748525383575583
	data : 0.11641817092895508
	model : 0.06931624412536622
			 train-loss:  2.0863376099722726 	 ± 0.2740868137774025
	data : 0.11659669876098633
	model : 0.06914982795715333
			 train-loss:  2.085394679822705 	 ± 0.2735915541042761
	data : 0.11674957275390625
	model : 0.07014808654785157
			 train-loss:  2.0858936976578275 	 ± 0.27289791354696147
	data : 0.1158524990081787
	model : 0.07034187316894532
			 train-loss:  2.089105092407612 	 ± 0.275463778033393
	data : 0.1156538963317871
	model : 0.07004213333129883
			 train-loss:  2.089874914238573 	 ± 0.2748851908032516
	data : 0.11590685844421386
	model : 0.06991500854492187
			 train-loss:  2.09352884888649 	 ± 0.2784455990715512
	data : 0.11596546173095704
	model : 0.06992077827453613
			 train-loss:  2.0943936030509063 	 ± 0.27791761778628205
	data : 0.11596050262451171
	model : 0.06979956626892089
			 train-loss:  2.0955719024270447 	 ± 0.27760604286325596
	data : 0.11604371070861816
	model : 0.06985950469970703
			 train-loss:  2.092066276268881 	 ± 0.2808570235430839
	data : 0.11598644256591797
	model : 0.06922369003295899
			 train-loss:  2.0920152936292733 	 ± 0.2800936344361512
	data : 0.1165071964263916
	model : 0.06902112960815429
			 train-loss:  2.0908940373240292 	 ± 0.2797493600987952
	data : 0.11664261817932128
	model : 0.0682586669921875
			 train-loss:  2.089435621615379 	 ± 0.2797006328318057
	data : 0.11772375106811524
	model : 0.06831903457641601
			 train-loss:  2.089479564345457 	 ± 0.27895241144357524
	data : 0.11774249076843261
	model : 0.06730613708496094
			 train-loss:  2.0895185312058064 	 ± 0.2782100378428412
	data : 0.11860404014587403
	model : 0.06801319122314453
			 train-loss:  2.0896577980152515 	 ± 0.2774796267962675
	data : 0.11789975166320801
	model : 0.0682447910308838
			 train-loss:  2.0902004436442727 	 ± 0.2768489849937212
	data : 0.11774067878723145
	model : 0.06941118240356445
			 train-loss:  2.0932096093112884 	 ± 0.2792213113362617
	data : 0.1165534496307373
	model : 0.06936821937561036
			 train-loss:  2.0918217040598392 	 ± 0.2791529957867016
	data : 0.1165236473083496
	model : 0.07037506103515626
			 train-loss:  2.0925653413169742 	 ± 0.27861946567821055
	data : 0.1156123161315918
	model : 0.07052764892578126
			 train-loss:  2.0910702568968547 	 ± 0.27867555814888756
	data : 0.11544928550720215
	model : 0.07064313888549804
			 train-loss:  2.092236740772541 	 ± 0.27843452152326736
	data : 0.11541743278503418
	model : 0.07030763626098632
			 train-loss:  2.0934088248379377 	 ± 0.278205194728352
	data : 0.11561427116394044
	model : 0.07016887664794921
			 train-loss:  2.0925014248959304 	 ± 0.27778881888161927
	data : 0.11570215225219727
	model : 0.0701796531677246
			 train-loss:  2.0901881408209753 	 ± 0.27898225793835274
	data : 0.11562342643737793
	model : 0.06982002258300782
			 train-loss:  2.0889080271649 	 ± 0.2788627798764586
	data : 0.11600127220153808
	model : 0.06936140060424804
			 train-loss:  2.0896842318773268 	 ± 0.27838017890609096
	data : 0.11622166633605957
	model : 0.0693429946899414
			 train-loss:  2.090142649204577 	 ± 0.27776249463577024
	data : 0.11624069213867187
	model : 0.06959018707275391
			 train-loss:  2.0881430819483087 	 ± 0.27852058259120693
	data : 0.11617102622985839
	model : 0.06955785751342773
			 train-loss:  2.089211028197716 	 ± 0.2782480211195032
	data : 0.11629886627197265
	model : 0.07004895210266113
			 train-loss:  2.087080218628341 	 ± 0.27922058085220763
	data : 0.11576328277587891
	model : 0.07035984992980956
			 train-loss:  2.087355841078409 	 ± 0.2785665402495298
	data : 0.11569323539733886
	model : 0.06937384605407715
			 train-loss:  2.088820867746779 	 ± 0.2786801280587123
	data : 0.11640233993530273
	model : 0.069091796875
			 train-loss:  2.0918969054153 	 ± 0.28148997435046264
	data : 0.11657676696777344
	model : 0.06910791397094726
			 train-loss:  2.091906587091776 	 ± 0.2808125350539423
	data : 0.11662259101867675
	model : 0.06884598731994629
			 train-loss:  2.0920951908284966 	 ± 0.2801531345259617
	data : 0.11703453063964844
	model : 0.06874308586120606
			 train-loss:  2.093666888986315 	 ± 0.28040741095187965
	data : 0.11697201728820801
	model : 0.06955399513244628
			 train-loss:  2.0953259100846204 	 ± 0.28077333167478785
	data : 0.11639084815979003
	model : 0.06880021095275879
			 train-loss:  2.0941909883382186 	 ± 0.2805950549640979
	data : 0.11716399192810059
	model : 0.06875591278076172
			 train-loss:  2.09256778300648 	 ± 0.28093152240922925
	data : 0.11720209121704102
	model : 0.06838464736938477
			 train-loss:  2.0920398279885264 	 ± 0.2803802673765102
	data : 0.11773781776428223
	model : 0.06766552925109863
			 train-loss:  2.0912027087322502 	 ± 0.2799953866152617
	data : 0.11833553314208985
	model : 0.06783690452575683
			 train-loss:  2.0909107752420284 	 ± 0.2793792922415309
	data : 0.11815667152404785
	model : 0.06880297660827636
			 train-loss:  2.091717527209339 	 ± 0.2789868849765817
	data : 0.11716947555541993
	model : 0.06787967681884766
			 train-loss:  2.0921601193760515 	 ± 0.2784226183497035
	data : 0.11794648170471192
	model : 0.06726207733154296
			 train-loss:  2.0909421716106538 	 ± 0.278367681602276
	data : 0.11822190284729003
	model : 0.06714973449707032
			 train-loss:  2.090059996734966 	 ± 0.2780409651927645
	data : 0.11845927238464356
	model : 0.06619324684143066
			 train-loss:  2.0886025606776792 	 ± 0.27825218825365955
	data : 0.11918702125549316
	model : 0.06554093360900878
			 train-loss:  2.088553572023237 	 ± 0.2776257420053683
	data : 0.11983141899108887
	model : 0.06567025184631348
			 train-loss:  2.0874474914619205 	 ± 0.2774923747695722
	data : 0.11967673301696777
	model : 0.06605062484741211
			 train-loss:  2.086838320429836 	 ± 0.2770216814083571
	data : 0.11936278343200683
	model : 0.06622600555419922
			 train-loss:  2.0883785602781506 	 ± 0.2773650026500031
	data : 0.11899285316467285
	model : 0.06690163612365722
			 train-loss:  2.088351397915224 	 ± 0.2767509828683733
	data : 0.11843161582946778
	model : 0.06658601760864258
			 train-loss:  2.0883172876509275 	 ± 0.27614120249667834
	data : 0.11861920356750488
	model : 0.0669243335723877
			 train-loss:  2.091010853386762 	 ± 0.27850757401506976
	data : 0.11836771965026856
	model : 0.06710987091064453
			 train-loss:  2.093170381008798 	 ± 0.27980536477512435
	data : 0.11807427406311036
	model : 0.0674006462097168
			 train-loss:  2.0931069213411084 	 ± 0.27919808117913947
	data : 0.11786847114562989
	model : 0.0675318717956543
			 train-loss:  2.0946305154205915 	 ± 0.27954968039304756
	data : 0.11782913208007813
	model : 0.0682447910308838
			 train-loss:  2.0936443646406304 	 ± 0.27934893027179775
	data : 0.11728129386901856
	model : 0.06815123558044434
			 train-loss:  2.0934162912450636 	 ± 0.2787704704496482
	data : 0.11740450859069824
	model : 0.06840667724609376
			 train-loss:  2.092841639987424 	 ± 0.2783124335543403
	data : 0.11707544326782227
	model : 0.06861300468444824
			 train-loss:  2.0912848782032096 	 ± 0.2787387731606591
	data : 0.11681065559387208
	model : 0.06846079826354981
			 train-loss:  2.0905710706266305 	 ± 0.2783627557162703
	data : 0.11687870025634765
	model : 0.06834821701049805
			 train-loss:  2.0911889805572446 	 ± 0.27793702013027743
	data : 0.11681809425354003
	model : 0.06868209838867187
			 train-loss:  2.091476359788109 	 ± 0.2773877875096399
	data : 0.11633915901184082
	model : 0.06861362457275391
			 train-loss:  2.0927628697710556 	 ± 0.2775174931241375
	data : 0.11663484573364258
	model : 0.06802263259887695
			 train-loss:  2.093733943005403 	 ± 0.2773453301425975
	data : 0.11720414161682129
	model : 0.06802396774291992
			 train-loss:  2.0934735922397913 	 ± 0.27679871394090816
	data : 0.11706423759460449
	model : 0.06799302101135254
			 train-loss:  2.093438302682451 	 ± 0.27622676701189697
	data : 0.11711039543151855
	model : 0.06795921325683593
			 train-loss:  2.092819966897062 	 ± 0.27582558994481665
	data : 0.11712961196899414
	model : 0.06747703552246094
			 train-loss:  2.0924161550451497 	 ± 0.2753317605077395
	data : 0.11745591163635254
	model : 0.06776657104492187
			 train-loss:  2.0914878202944385 	 ± 0.2751516675890951
	data : 0.11735930442810058
	model : 0.06793332099914551
			 train-loss:  2.09100007817028 	 ± 0.2746979539843399
	data : 0.1175912857055664
	model : 0.06818904876708984
			 train-loss:  2.090899793725265 	 ± 0.2741458335474975
	data : 0.11731481552124023
	model : 0.068363618850708
			 train-loss:  2.0905655209095246 	 ± 0.27364299603322134
	data : 0.11733884811401367
	model : 0.06826753616333008
			 train-loss:  2.089976282962355 	 ± 0.2732505633267365
	data : 0.1173922061920166
	model : 0.06821708679199219
			 train-loss:  2.091086974143982 	 ± 0.27326613761547625
	data : 0.1173326015472412
	model : 0.06804876327514649
			 train-loss:  2.0926828840339327 	 ± 0.27388611969870513
	data : 0.11742773056030273
	model : 0.06787104606628418
			 train-loss:  2.092548958838932 	 ± 0.27335038951337925
	data : 0.11753592491149903
	model : 0.06723608970642089
			 train-loss:  2.0921465271546436 	 ± 0.2728844251239941
	data : 0.1178469181060791
	model : 0.06772551536560059
			 train-loss:  2.090910686751989 	 ± 0.2730552029956833
	data : 0.1173633098602295
	model : 0.06813383102416992
			 train-loss:  2.090954717467813 	 ± 0.27252017817818974
	data : 0.11702117919921876
	model : 0.06790776252746582
			 train-loss:  2.0915185981430113 	 ± 0.2721364019011689
	data : 0.11600379943847657
	model : 0.058765506744384764
#epoch  93    val-loss:  2.4605163461283635  train-loss:  2.0915185981430113  lr:  1.220703125e-06
			 train-loss:  2.237549304962158 	 ± 0.0
	data : 5.766764402389526
	model : 0.07142901420593262
			 train-loss:  2.1375056505203247 	 ± 0.1000436544418335
	data : 2.9510200023651123
	model : 0.06939589977264404
			 train-loss:  1.9999112288157146 	 ± 0.2110377650986592
	data : 2.0279449621836343
	model : 0.06819748878479004
			 train-loss:  1.9246299266815186 	 ± 0.2245095255806264
	data : 1.5506340265274048
	model : 0.06841909885406494
			 train-loss:  2.040097188949585 	 ± 0.30603002513459376
	data : 1.2636301517486572
	model : 0.0686418056488037
			 train-loss:  2.059499661127726 	 ± 0.2827146851562858
	data : 0.13331422805786133
	model : 0.06796226501464844
			 train-loss:  2.0863535744803294 	 ± 0.2698817548051023
	data : 0.12973828315734864
	model : 0.06843729019165039
			 train-loss:  2.0631674975156784 	 ± 0.2597976145669378
	data : 0.11657962799072266
	model : 0.06955461502075196
			 train-loss:  2.0964286194907293 	 ± 0.2623848245104027
	data : 0.11564512252807617
	model : 0.07062730789184571
			 train-loss:  2.118143284320831 	 ± 0.2573032384914195
	data : 0.11465697288513184
	model : 0.07076196670532227
			 train-loss:  2.0846195112575185 	 ± 0.26725410703975383
	data : 0.11479887962341309
	model : 0.07127628326416016
			 train-loss:  2.098056823015213 	 ± 0.2597284532073638
	data : 0.11423258781433106
	model : 0.07135071754455566
			 train-loss:  2.087081340643076 	 ± 0.25241881895904794
	data : 0.11419916152954102
	model : 0.07177729606628418
			 train-loss:  2.1068449360983714 	 ± 0.25345998958826854
	data : 0.1138841152191162
	model : 0.07063126564025879
			 train-loss:  2.098561970392863 	 ± 0.24681911161735487
	data : 0.1149754524230957
	model : 0.07012877464294434
			 train-loss:  2.08701553940773 	 ± 0.2431295853465247
	data : 0.1153714656829834
	model : 0.06927127838134765
			 train-loss:  2.0705578677794514 	 ± 0.24488466339312207
	data : 0.1161954402923584
	model : 0.06928091049194336
			 train-loss:  2.081473317411211 	 ± 0.24220325103914858
	data : 0.11628603935241699
	model : 0.06861772537231445
			 train-loss:  2.0816214147366976 	 ± 0.23574417084655716
	data : 0.11692924499511718
	model : 0.06897535324096679
			 train-loss:  2.0845907866954803 	 ± 0.2301392504778068
	data : 0.11681256294250489
	model : 0.06948280334472656
			 train-loss:  2.0921990928195773 	 ± 0.2271556784697898
	data : 0.11615166664123536
	model : 0.07029247283935547
			 train-loss:  2.0838335644115102 	 ± 0.22521963641226625
	data : 0.1155369758605957
	model : 0.06935067176818847
			 train-loss:  2.0748067627782407 	 ± 0.2243014212785584
	data : 0.11631755828857422
	model : 0.06925935745239258
			 train-loss:  2.060006469488144 	 ± 0.23076600763249724
	data : 0.116522216796875
	model : 0.06869182586669922
			 train-loss:  2.0458347606658935 	 ± 0.236522574419694
	data : 0.11693649291992188
	model : 0.06866836547851562
			 train-loss:  2.051191568374634 	 ± 0.23347090149700786
	data : 0.11711535453796387
	model : 0.06834959983825684
			 train-loss:  2.0643680095672607 	 ± 0.23875490818677225
	data : 0.11733918190002442
	model : 0.06942524909973144
			 train-loss:  2.0711073790277754 	 ± 0.237053513179193
	data : 0.11622819900512696
	model : 0.06913270950317382
			 train-loss:  2.0748552125075768 	 ± 0.23377323722227672
	data : 0.1165614128112793
	model : 0.06869239807128906
			 train-loss:  2.072056293487549 	 ± 0.2303376797430452
	data : 0.11711578369140625
	model : 0.06867585182189942
			 train-loss:  2.0705309990913636 	 ± 0.22674606143520218
	data : 0.11716032028198242
	model : 0.06889920234680176
			 train-loss:  2.088918447494507 	 ± 0.24553643698388325
	data : 0.11702976226806641
	model : 0.06779451370239258
			 train-loss:  2.08942833813754 	 ± 0.24180477275522186
	data : 0.11810302734375
	model : 0.06814870834350586
			 train-loss:  2.101571153191959 	 ± 0.24822497483319494
	data : 0.11771416664123535
	model : 0.06811308860778809
			 train-loss:  2.1083317824772427 	 ± 0.24780879526466004
	data : 0.11760244369506836
	model : 0.06733908653259277
			 train-loss:  2.1039080089992948 	 ± 0.2457403710002559
	data : 0.1181565761566162
	model : 0.06725144386291504
			 train-loss:  2.1014916252445532 	 ± 0.24283000986320086
	data : 0.11818008422851563
	model : 0.06766009330749512
			 train-loss:  2.117138241466723 	 ± 0.25782335063375666
	data : 0.11774616241455078
	model : 0.06760683059692382
			 train-loss:  2.1133972375820846 	 ± 0.2555391584289668
	data : 0.11760549545288086
	model : 0.06865782737731933
			 train-loss:  2.109833136200905 	 ± 0.25330449044751996
	data : 0.1167609691619873
	model : 0.06938962936401367
			 train-loss:  2.1113613378710863 	 ± 0.2503829579936912
	data : 0.11621818542480469
	model : 0.06946511268615722
			 train-loss:  2.1164756757872447 	 ± 0.24954234599594213
	data : 0.11608014106750489
	model : 0.069801664352417
			 train-loss:  2.1148628218229426 	 ± 0.24684502278274573
	data : 0.1157193660736084
	model : 0.0697291374206543
			 train-loss:  2.109589872035113 	 ± 0.24646137322796086
	data : 0.11600613594055176
	model : 0.06968722343444825
			 train-loss:  2.113515247239007 	 ± 0.24509454607844686
	data : 0.1160963535308838
	model : 0.06972765922546387
			 train-loss:  2.1117143605066384 	 ± 0.24271666935644
	data : 0.11610870361328125
	model : 0.069915771484375
			 train-loss:  2.112391306998882 	 ± 0.24016458467939358
	data : 0.11610350608825684
	model : 0.070220947265625
			 train-loss:  2.111753098666668 	 ± 0.2376899764130481
	data : 0.1159299373626709
	model : 0.07052507400512695
			 train-loss:  2.117548643326273 	 ± 0.2386540902701753
	data : 0.11558237075805664
	model : 0.07064876556396485
			 train-loss:  2.1149459362030028 	 ± 0.23695693583589772
	data : 0.11542367935180664
	model : 0.07064695358276367
			 train-loss:  2.1154181115767536 	 ± 0.23464608282642277
	data : 0.11529016494750977
	model : 0.07046070098876953
			 train-loss:  2.1202416007335367 	 ± 0.23491813434238507
	data : 0.1153984546661377
	model : 0.07029490470886231
			 train-loss:  2.1185853661231273 	 ± 0.23299767475933614
	data : 0.11551012992858886
	model : 0.06930251121520996
			 train-loss:  2.1210899971149586 	 ± 0.2315492669104113
	data : 0.11629433631896972
	model : 0.06925287246704101
			 train-loss:  2.1228044509887694 	 ± 0.22978026175449975
	data : 0.11643819808959961
	model : 0.06875739097595215
			 train-loss:  2.11835538489478 	 ± 0.23009739321926725
	data : 0.11687307357788086
	model : 0.06863336563110352
			 train-loss:  2.113641490016067 	 ± 0.23078197312621535
	data : 0.11689600944519044
	model : 0.06848793029785157
			 train-loss:  2.112958383971247 	 ± 0.22884194520758977
	data : 0.11710853576660156
	model : 0.06890392303466797
			 train-loss:  2.1231570829779414 	 ± 0.23982037580491278
	data : 0.11682124137878418
	model : 0.06873974800109864
			 train-loss:  2.1254583577315014 	 ± 0.23846950574657066
	data : 0.11691203117370605
	model : 0.06925992965698242
			 train-loss:  2.122248256792788 	 ± 0.23781028697555875
	data : 0.11660232543945312
	model : 0.06852059364318848
			 train-loss:  2.11844172593086 	 ± 0.23775080635717785
	data : 0.1172553539276123
	model : 0.06870732307434083
			 train-loss:  2.1186586883332996 	 ± 0.2358625343913864
	data : 0.11713800430297852
	model : 0.06890683174133301
			 train-loss:  2.1175299119204283 	 ± 0.2341840498904976
	data : 0.11691093444824219
	model : 0.06898298263549804
			 train-loss:  2.1123079996842606 	 ± 0.2361008662764768
	data : 0.11686120033264161
	model : 0.06845946311950683
			 train-loss:  2.1133710687810723 	 ± 0.23446209992569517
	data : 0.11722359657287598
	model : 0.06901993751525878
			 train-loss:  2.110847914396827 	 ± 0.23360686614851978
	data : 0.11666545867919922
	model : 0.06910314559936523
			 train-loss:  2.1177492948139416 	 ± 0.2386645815616839
	data : 0.11643877029418945
	model : 0.06923146247863769
			 train-loss:  2.12302303314209 	 ± 0.24088690580857644
	data : 0.11634840965270996
	model : 0.06910367012023926
			 train-loss:  2.120398904596056 	 ± 0.24015138738590266
	data : 0.11654305458068848
	model : 0.06962275505065918
			 train-loss:  2.118606671481065 	 ± 0.2389251858608148
	data : 0.11627960205078125
	model : 0.06996402740478516
			 train-loss:  2.1140357388390436 	 ± 0.2403660271583585
	data : 0.11615033149719238
	model : 0.07008333206176758
			 train-loss:  2.111372709274292 	 ± 0.23978111191692233
	data : 0.11608076095581055
	model : 0.07028536796569824
			 train-loss:  2.1064162157677315 	 ± 0.24189129967022974
	data : 0.11596121788024902
	model : 0.07021784782409668
			 train-loss:  2.109468110402425 	 ± 0.24170330851784297
	data : 0.11589546203613281
	model : 0.06996827125549317
			 train-loss:  2.104003520388352 	 ± 0.24472725532885067
	data : 0.11595726013183594
	model : 0.07005324363708496
			 train-loss:  2.1016130648650133 	 ± 0.24402439186758285
	data : 0.1159067153930664
	model : 0.06978397369384766
			 train-loss:  2.09832418576265 	 ± 0.24416665833944323
	data : 0.11614723205566406
	model : 0.06912622451782227
			 train-loss:  2.094008540805382 	 ± 0.24559202080779935
	data : 0.11668953895568848
	model : 0.06838507652282715
			 train-loss:  2.0947998598217965 	 ± 0.24415357124763956
	data : 0.11733717918395996
	model : 0.0688939094543457
			 train-loss:  2.0943594229074174 	 ± 0.2426737465627678
	data : 0.11675987243652344
	model : 0.06877675056457519
			 train-loss:  2.0955878394406016 	 ± 0.24144274569050098
	data : 0.11670870780944824
	model : 0.06876487731933593
			 train-loss:  2.0960426057677672 	 ± 0.24001919400900645
	data : 0.11671218872070313
	model : 0.0683964729309082
			 train-loss:  2.094044831537065 	 ± 0.23927944119451763
	data : 0.1170154094696045
	model : 0.06911425590515137
			 train-loss:  2.09955079275019 	 ± 0.24316162936442165
	data : 0.11650457382202148
	model : 0.06884441375732422
			 train-loss:  2.0960637261701183 	 ± 0.24387213217555226
	data : 0.1169046401977539
	model : 0.06855192184448242
			 train-loss:  2.0961739455146353 	 ± 0.24246867182088766
	data : 0.11714234352111816
	model : 0.06865034103393555
			 train-loss:  2.0942044935443183 	 ± 0.24178591169881036
	data : 0.1170494556427002
	model : 0.06862363815307618
			 train-loss:  2.0952363523204673 	 ± 0.2406185047488939
	data : 0.11705646514892579
	model : 0.06786479949951171
			 train-loss:  2.099045907126533 	 ± 0.24196196577371734
	data : 0.1175765037536621
	model : 0.06866021156311035
			 train-loss:  2.1027282646724155 	 ± 0.2431514196054962
	data : 0.1168525218963623
	model : 0.06879572868347168
			 train-loss:  2.1060852019683174 	 ± 0.243937404488712
	data : 0.11668057441711426
	model : 0.06866531372070313
			 train-loss:  2.1109494522053707 	 ± 0.24706764109375465
	data : 0.11675467491149902
	model : 0.06933002471923828
			 train-loss:  2.109707552067777 	 ± 0.2460415965239276
	data : 0.11609840393066406
	model : 0.06934852600097656
			 train-loss:  2.1084646024202045 	 ± 0.2450397194109304
	data : 0.1161036491394043
	model : 0.06856307983398438
			 train-loss:  2.110927971700827 	 ± 0.2449397452468706
	data : 0.11687760353088379
	model : 0.06881561279296874
			 train-loss:  2.1094647019179824 	 ± 0.2440953090962069
	data : 0.11683545112609864
	model : 0.06911592483520508
			 train-loss:  2.1074471960262375 	 ± 0.24365827923108732
	data : 0.11669549942016602
	model : 0.06931009292602539
			 train-loss:  2.1091240439752137 	 ± 0.24299223203434955
	data : 0.11659016609191894
	model : 0.06942310333251953
			 train-loss:  2.1059745025634764 	 ± 0.24379666544082432
	data : 0.11644840240478516
	model : 0.0694155216217041
			 train-loss:  2.1085788094171205 	 ± 0.2439806799546541
	data : 0.11666593551635743
	model : 0.06859488487243652
			 train-loss:  2.108784130975312 	 ± 0.24279051916681654
	data : 0.11739001274108887
	model : 0.06848807334899902
			 train-loss:  2.1074214259397634 	 ± 0.242000709430075
	data : 0.1173213005065918
	model : 0.06833119392395019
			 train-loss:  2.1100767736251536 	 ± 0.24233750107130042
	data : 0.1174649715423584
	model : 0.0691157341003418
			 train-loss:  2.1098848978678384 	 ± 0.24118868990134562
	data : 0.1167691707611084
	model : 0.06915416717529296
			 train-loss:  2.107220027806624 	 ± 0.24159646673675955
	data : 0.11663923263549805
	model : 0.06990833282470703
			 train-loss:  2.1049419474378923 	 ± 0.24160598577165246
	data : 0.11598811149597169
	model : 0.06988840103149414
			 train-loss:  2.1061375450204918 	 ± 0.24080263487851353
	data : 0.11617388725280761
	model : 0.06994266510009765
			 train-loss:  2.104607030886029 	 ± 0.24022263691824025
	data : 0.11599650382995605
	model : 0.06978888511657715
			 train-loss:  2.1070228793404318 	 ± 0.24045470728685897
	data : 0.11623291969299317
	model : 0.06951737403869629
			 train-loss:  2.10824162036449 	 ± 0.23971016965958566
	data : 0.11631488800048828
	model : 0.06941866874694824
			 train-loss:  2.107986886586462 	 ± 0.23865272639548593
	data : 0.11645827293395997
	model : 0.06899752616882324
			 train-loss:  2.1081032752990723 	 ± 0.23759758702655356
	data : 0.11673393249511718
	model : 0.06818165779113769
			 train-loss:  2.103342266459214 	 ± 0.24190661252350834
	data : 0.1175868034362793
	model : 0.06850738525390625
			 train-loss:  2.1052089680796082 	 ± 0.24167579836513997
	data : 0.11735310554504394
	model : 0.06890387535095215
			 train-loss:  2.106240483193562 	 ± 0.24088595629099935
	data : 0.11713910102844238
	model : 0.06927547454833985
			 train-loss:  2.1052343325737195 	 ± 0.24009899245277558
	data : 0.11669812202453614
	model : 0.06959199905395508
			 train-loss:  2.1060257432824474 	 ± 0.2392326661136898
	data : 0.11650567054748535
	model : 0.06983137130737305
			 train-loss:  2.109613583869293 	 ± 0.2413924023490184
	data : 0.11628665924072265
	model : 0.06978025436401367
			 train-loss:  2.1088640222946804 	 ± 0.24052352361014637
	data : 0.11631565093994141
	model : 0.07013206481933594
			 train-loss:  2.1098897604902915 	 ± 0.2397909713907673
	data : 0.11606135368347167
	model : 0.0702207088470459
			 train-loss:  2.1111766207413596 	 ± 0.2392253707040496
	data : 0.11589627265930176
	model : 0.07054295539855956
			 train-loss:  2.111190986827137 	 ± 0.2382509780664009
	data : 0.11549963951110839
	model : 0.07114615440368652
			 train-loss:  2.1117063232006563 	 ± 0.23735716433709333
	data : 0.114985990524292
	model : 0.07097692489624023
			 train-loss:  2.1139399690628053 	 ± 0.23771069313410786
	data : 0.11494593620300293
	model : 0.07032952308654786
			 train-loss:  2.1195199366599793 	 ± 0.2448467176535295
	data : 0.11542553901672363
	model : 0.06942958831787109
			 train-loss:  2.1198726667193917 	 ± 0.2439129871493176
	data : 0.1163724422454834
	model : 0.06852598190307617
			 train-loss:  2.1166651071980596 	 ± 0.24563261919780294
	data : 0.11724891662597656
	model : 0.06863374710083008
			 train-loss:  2.11782070185787 	 ± 0.24502775054308173
	data : 0.11703329086303711
	model : 0.06872472763061524
			 train-loss:  2.117657711872688 	 ± 0.24409053677415904
	data : 0.11687054634094238
	model : 0.06886529922485352
			 train-loss:  2.1158932629432385 	 ± 0.24398792169720848
	data : 0.11661553382873535
	model : 0.06864666938781738
			 train-loss:  2.11233129826459 	 ± 0.24645728815320347
	data : 0.11694908142089844
	model : 0.06922826766967774
			 train-loss:  2.112807327643373 	 ± 0.24558991407300948
	data : 0.11649723052978515
	model : 0.06906776428222657
			 train-loss:  2.1102342650071897 	 ± 0.24646469324137815
	data : 0.11667704582214355
	model : 0.0688899040222168
			 train-loss:  2.112671125376666 	 ± 0.24716515501447056
	data : 0.11707744598388672
	model : 0.0679555892944336
			 train-loss:  2.112503245472908 	 ± 0.2462625082256957
	data : 0.11793889999389648
	model : 0.0687551498413086
			 train-loss:  2.114335642243824 	 ± 0.24629088755890047
	data : 0.11708192825317383
	model : 0.06891131401062012
			 train-loss:  2.1120257541753245 	 ± 0.24688178693591953
	data : 0.11693434715270996
	model : 0.06918087005615234
			 train-loss:  2.1131844083182245 	 ± 0.24636839333720337
	data : 0.11666707992553711
	model : 0.07014703750610352
			 train-loss:  2.11468717115266 	 ± 0.24612544697136507
	data : 0.11550731658935547
	model : 0.07011151313781738
			 train-loss:  2.1150141092056924 	 ± 0.24528161509721513
	data : 0.11540164947509765
	model : 0.07007818222045899
			 train-loss:  2.117046098474046 	 ± 0.24560450594952787
	data : 0.11543755531311035
	model : 0.07006378173828125
			 train-loss:  2.11566676459946 	 ± 0.24529555225000163
	data : 0.1154787540435791
	model : 0.06902108192443848
			 train-loss:  2.115463602874014 	 ± 0.24445442039533272
	data : 0.11641201972961426
	model : 0.06798095703125
			 train-loss:  2.113554994813327 	 ± 0.24468428793977154
	data : 0.1174835205078125
	model : 0.06888008117675781
			 train-loss:  2.112840238499315 	 ± 0.24399673487938384
	data : 0.11682853698730469
	model : 0.06813297271728516
			 train-loss:  2.1151569027479002 	 ± 0.24477128696286482
	data : 0.1173677921295166
	model : 0.0679821491241455
			 train-loss:  2.114898373146315 	 ± 0.2439630926797224
	data : 0.11746201515197754
	model : 0.06804132461547852
			 train-loss:  2.1156608154309677 	 ± 0.243319904832547
	data : 0.11759576797485352
	model : 0.0682973861694336
			 train-loss:  2.115064691702525 	 ± 0.24261662777980383
	data : 0.11739482879638671
	model : 0.06828231811523437
			 train-loss:  2.1133140593964534 	 ± 0.24276061262574014
	data : 0.11743102073669434
	model : 0.06806383132934571
			 train-loss:  2.110946104714745 	 ± 0.24370410014493119
	data : 0.11773557662963867
	model : 0.06810183525085449
			 train-loss:  2.1132770223555224 	 ± 0.24460038914246585
	data : 0.11784706115722657
	model : 0.06902084350585938
			 train-loss:  2.113039513687035 	 ± 0.24382263816095603
	data : 0.11699466705322266
	model : 0.06835079193115234
			 train-loss:  2.1110388086688134 	 ± 0.24429975076227128
	data : 0.11754264831542968
	model : 0.06839709281921387
			 train-loss:  2.1092357505590487 	 ± 0.2445479437939407
	data : 0.11738386154174804
	model : 0.0693929672241211
			 train-loss:  2.107564644449076 	 ± 0.24465981784084834
	data : 0.11654291152954102
	model : 0.06909584999084473
			 train-loss:  2.1071241135838665 	 ± 0.2439468061977501
	data : 0.11668930053710938
	model : 0.06917262077331543
			 train-loss:  2.1093004387129777 	 ± 0.2447123148410727
	data : 0.11661100387573242
	model : 0.06986970901489258
			 train-loss:  2.1099784560501575 	 ± 0.24409615891574657
	data : 0.1160283088684082
	model : 0.06983008384704589
			 train-loss:  2.108784292055213 	 ± 0.24380528886912256
	data : 0.11611757278442383
	model : 0.06934633255004882
			 train-loss:  2.1068781032974337 	 ± 0.24425212682666433
	data : 0.11663818359375
	model : 0.06944780349731446
			 train-loss:  2.106180208592327 	 ± 0.24366369844331548
	data : 0.11650614738464356
	model : 0.0693746566772461
			 train-loss:  2.106779586978075 	 ± 0.24304018605291158
	data : 0.11667056083679199
	model : 0.06933794021606446
			 train-loss:  2.1070176341316915 	 ± 0.2423217576503318
	data : 0.11654472351074219
	model : 0.06947722434997558
			 train-loss:  2.1073645568755737 	 ± 0.24163186654285437
	data : 0.11639418601989746
	model : 0.07073168754577637
			 train-loss:  2.105666536057067 	 ± 0.24189866888568162
	data : 0.1150670051574707
	model : 0.07079548835754394
			 train-loss:  2.105660014209293 	 ± 0.2411776725628363
	data : 0.11497678756713867
	model : 0.07073144912719727
			 train-loss:  2.106376094930976 	 ± 0.24064212826072817
	data : 0.11496438980102539
	model : 0.07062506675720215
			 train-loss:  2.102539501470678 	 ± 0.2450624039221449
	data : 0.11514215469360352
	model : 0.0705528736114502
			 train-loss:  2.101546031689783 	 ± 0.2446878962233348
	data : 0.11529769897460937
	model : 0.06975693702697754
			 train-loss:  2.101741153833478 	 ± 0.24398889931695292
	data : 0.11606278419494628
	model : 0.06997671127319335
			 train-loss:  2.102359105396822 	 ± 0.24341765795629858
	data : 0.11588773727416993
	model : 0.07015762329101563
			 train-loss:  2.10214100692464 	 ± 0.2427341254393481
	data : 0.115665864944458
	model : 0.06988534927368165
			 train-loss:  2.1020823076793125 	 ± 0.2420408442763422
	data : 0.11596746444702148
	model : 0.06910529136657714
			 train-loss:  2.103688408705321 	 ± 0.24228563939134204
	data : 0.11660370826721192
	model : 0.0692178726196289
			 train-loss:  2.1041437845445623 	 ± 0.2416757664447602
	data : 0.11651463508605957
	model : 0.06904182434082032
			 train-loss:  2.103552764051416 	 ± 0.24112418565638674
	data : 0.1167304515838623
	model : 0.06816115379333496
			 train-loss:  2.104569307918655 	 ± 0.24083189551605122
	data : 0.1175379753112793
	model : 0.06845221519470215
			 train-loss:  2.1016816271675958 	 ± 0.24324968034218528
	data : 0.11728339195251465
	model : 0.0691028118133545
			 train-loss:  2.1004485945675255 	 ± 0.2431402170587851
	data : 0.11667566299438477
	model : 0.0687105655670166
			 train-loss:  2.101035786854042 	 ± 0.24259998643460912
	data : 0.11708998680114746
	model : 0.06871771812438965
			 train-loss:  2.0980079134956737 	 ± 0.2453603917733717
	data : 0.11701340675354004
	model : 0.06947927474975586
			 train-loss:  2.095860849256101 	 ± 0.24641052637335112
	data : 0.1162412166595459
	model : 0.0695073127746582
			 train-loss:  2.0960562770431106 	 ± 0.24575794724996455
	data : 0.11635169982910157
	model : 0.06883573532104492
			 train-loss:  2.0964095925772064 	 ± 0.2451435245865184
	data : 0.11692671775817871
	model : 0.06883511543273926
			 train-loss:  2.094365276754859 	 ± 0.24607177490932014
	data : 0.11667428016662598
	model : 0.068336820602417
			 train-loss:  2.0913802106329737 	 ± 0.24878810827642792
	data : 0.11715807914733886
	model : 0.0684431552886963
			 train-loss:  2.090806977458732 	 ± 0.248253518364152
	data : 0.11716117858886718
	model : 0.06754598617553711
			 train-loss:  2.0920924851768894 	 ± 0.24822926932586062
	data : 0.11779699325561524
	model : 0.06825494766235352
			 train-loss:  2.092740289827916 	 ± 0.24773957634606947
	data : 0.1173438549041748
	model : 0.06772933006286622
			 train-loss:  2.091055423642198 	 ± 0.2481883208550556
	data : 0.11801600456237793
	model : 0.06746811866760254
			 train-loss:  2.0878479036024813 	 ± 0.25150271640227434
	data : 0.11829276084899902
	model : 0.06741156578063964
			 train-loss:  2.0885816972280287 	 ± 0.2510607261884125
	data : 0.11841449737548829
	model : 0.06752748489379883
			 train-loss:  2.0896307688492994 	 ± 0.25084209530851
	data : 0.1183126449584961
	model : 0.06767563819885254
			 train-loss:  2.088488828162758 	 ± 0.2507090205220015
	data : 0.11820878982543945
	model : 0.06854944229125977
			 train-loss:  2.0875651860600195 	 ± 0.2504059952944557
	data : 0.11747431755065918
	model : 0.06939158439636231
			 train-loss:  2.0871348645952015 	 ± 0.2498458717360555
	data : 0.11651053428649902
	model : 0.06939463615417481
			 train-loss:  2.085726594206077 	 ± 0.25000391008174255
	data : 0.11635499000549317
	model : 0.07018380165100098
			 train-loss:  2.084437898993492 	 ± 0.2500398600537842
	data : 0.11570591926574707
	model : 0.0700979232788086
			 train-loss:  2.084893097924949 	 ± 0.24950015710066556
	data : 0.11566801071166992
	model : 0.07003412246704102
			 train-loss:  2.0863447148020904 	 ± 0.24973126346910263
	data : 0.11588196754455567
	model : 0.07005777359008789
			 train-loss:  2.0883617606656304 	 ± 0.25075947736528087
	data : 0.11601796150207519
	model : 0.06934852600097656
			 train-loss:  2.087511673861859 	 ± 0.2504371699756049
	data : 0.11666254997253418
	model : 0.0693504810333252
			 train-loss:  2.0887538212101635 	 ± 0.2504547635466633
	data : 0.11666879653930665
	model : 0.06933002471923828
			 train-loss:  2.0889952657292192 	 ± 0.24987003880907824
	data : 0.1166501522064209
	model : 0.06921486854553223
			 train-loss:  2.091084612740411 	 ± 0.25106310795069237
	data : 0.11648335456848144
	model : 0.06836452484130859
			 train-loss:  2.092333116210424 	 ± 0.2511021822532704
	data : 0.11726255416870117
	model : 0.06873865127563476
			 train-loss:  2.093101647481964 	 ± 0.25074583434949976
	data : 0.1168741226196289
	model : 0.06878371238708496
			 train-loss:  2.0923427933738346 	 ± 0.250388559240513
	data : 0.11694841384887696
	model : 0.06881518363952636
			 train-loss:  2.091570763226369 	 ± 0.25004492998440964
	data : 0.11709489822387695
	model : 0.06895666122436524
			 train-loss:  2.093394249677658 	 ± 0.2508568245284796
	data : 0.11701579093933105
	model : 0.06979527473449706
			 train-loss:  2.091555164453569 	 ± 0.2516957252644663
	data : 0.11627860069274902
	model : 0.06994047164916992
			 train-loss:  2.090815178144758 	 ± 0.25133909587230957
	data : 0.11630878448486329
	model : 0.06903204917907715
			 train-loss:  2.090305226348167 	 ± 0.25086484767966116
	data : 0.11697850227355958
	model : 0.06885805130004882
			 train-loss:  2.0907936339025146 	 ± 0.25038590423863194
	data : 0.11700596809387206
	model : 0.0685791015625
			 train-loss:  2.0933013698472407 	 ± 0.2525124957075536
	data : 0.11724934577941895
	model : 0.06859803199768066
			 train-loss:  2.0953385140917717 	 ± 0.2537136422759405
	data : 0.11717162132263184
	model : 0.06828775405883789
			 train-loss:  2.0953232055385365 	 ± 0.2531338255334369
	data : 0.11719040870666504
	model : 0.06914482116699219
			 train-loss:  2.0964164397933267 	 ± 0.2530755144967807
	data : 0.11650333404541016
	model : 0.06922307014465331
			 train-loss:  2.095349117102127 	 ± 0.2529980804875254
	data : 0.11623826026916503
	model : 0.06919002532958984
			 train-loss:  2.0938667719428605 	 ± 0.25338768320790034
	data : 0.11614623069763183
	model : 0.06909117698669434
			 train-loss:  2.0939561588347226 	 ± 0.25282241902350033
	data : 0.11626744270324707
	model : 0.06947031021118164
			 train-loss:  2.092882653432233 	 ± 0.25276631576801295
	data : 0.11602644920349121
	model : 0.06930761337280274
			 train-loss:  2.0946706443362766 	 ± 0.25361971413831125
	data : 0.11599254608154297
	model : 0.06916642189025879
			 train-loss:  2.0934877886181384 	 ± 0.2536792320160116
	data : 0.1162649154663086
	model : 0.0691828727722168
			 train-loss:  2.0920168592016077 	 ± 0.2540839228049856
	data : 0.11649503707885742
	model : 0.06896681785583496
			 train-loss:  2.090430230947963 	 ± 0.2546506140789076
	data : 0.11661849021911622
	model : 0.06886587142944336
			 train-loss:  2.089497834313905 	 ± 0.254483743557803
	data : 0.11676020622253418
	model : 0.06900477409362793
			 train-loss:  2.0905773561933767 	 ± 0.2544548507740822
	data : 0.11665453910827636
	model : 0.06906657218933106
			 train-loss:  2.089217177201143 	 ± 0.2547400633009331
	data : 0.11654090881347656
	model : 0.06928133964538574
			 train-loss:  2.0890963288216757 	 ± 0.2541970975442142
	data : 0.11639246940612794
	model : 0.06952052116394043
			 train-loss:  2.0894822117596736 	 ± 0.25371911223795934
	data : 0.11626615524291992
	model : 0.06955494880676269
			 train-loss:  2.0904762821319776 	 ± 0.25363070191195464
	data : 0.1162987232208252
	model : 0.06948151588439941
			 train-loss:  2.090277873201573 	 ± 0.2531086845260611
	data : 0.11645598411560058
	model : 0.06952157020568847
			 train-loss:  2.089776278047238 	 ± 0.25268888774227416
	data : 0.11651024818420411
	model : 0.0693964958190918
			 train-loss:  2.0882551499056916 	 ± 0.25323570387430394
	data : 0.11631412506103515
	model : 0.06925334930419921
			 train-loss:  2.0873980842718556 	 ± 0.25304735932721867
	data : 0.11646747589111328
	model : 0.06905488967895508
			 train-loss:  2.086365067310413 	 ± 0.25301980401646423
	data : 0.11649675369262695
	model : 0.06889824867248535
			 train-loss:  2.0849417338768643 	 ± 0.25344912736056524
	data : 0.1163823127746582
	model : 0.06871709823608399
			 train-loss:  2.0860733995793765 	 ± 0.25352964121440774
	data : 0.11628856658935546
	model : 0.06857390403747558
			 train-loss:  2.0861881951655237 	 ± 0.25301155366487466
	data : 0.11639394760131835
	model : 0.0684584617614746
			 train-loss:  2.0875021390954163 	 ± 0.2533164252076839
	data : 0.11631479263305664
	model : 0.06846456527709961
			 train-loss:  2.0871247004290097 	 ± 0.25286526150556315
	data : 0.11626968383789063
	model : 0.0685655117034912
			 train-loss:  2.0872604379848556 	 ± 0.2523575896953311
	data : 0.11638636589050293
	model : 0.06861982345581055
			 train-loss:  2.0861491716004967 	 ± 0.2524441072240574
	data : 0.11668601036071777
	model : 0.06875619888305665
			 train-loss:  2.08627231352725 	 ± 0.2519399718035359
	data : 0.11679773330688477
	model : 0.0690035343170166
			 train-loss:  2.087555913675216 	 ± 0.2522395117768803
	data : 0.11662440299987793
	model : 0.06911640167236328
			 train-loss:  2.088027525618373 	 ± 0.2518420333306891
	data : 0.1166038990020752
	model : 0.06916642189025879
			 train-loss:  2.0890205664634705 	 ± 0.25182584957270615
	data : 0.11649799346923828
	model : 0.06932644844055176
			 train-loss:  2.087513888499651 	 ± 0.2524502400836515
	data : 0.11621079444885254
	model : 0.0694699764251709
			 train-loss:  2.0877448623142545 	 ± 0.2519754214370084
	data : 0.11628413200378418
	model : 0.06940269470214844
			 train-loss:  2.0872419818116743 	 ± 0.251603628737343
	data : 0.11637907028198242
	model : 0.06952023506164551
			 train-loss:  2.087384660882274 	 ± 0.2511181127058137
	data : 0.11630110740661621
	model : 0.06932759284973145
			 train-loss:  2.0873346913094615 	 ± 0.25062650585401586
	data : 0.11640777587890624
	model : 0.06915402412414551
			 train-loss:  2.0902416412718594 	 ± 0.2544073916880114
	data : 0.11550836563110352
	model : 0.060175514221191405
#epoch  94    val-loss:  2.4616068036932695  train-loss:  2.0902416412718594  lr:  1.220703125e-06
			 train-loss:  2.059396505355835 	 ± 0.0
	data : 5.515087366104126
	model : 0.10416364669799805
			 train-loss:  2.391762137413025 	 ± 0.33236563205718994
	data : 2.985517382621765
	model : 0.08458137512207031
			 train-loss:  2.2720194657643638 	 ± 0.31987688841537426
	data : 2.030041456222534
	model : 0.07957394917805989
			 train-loss:  2.1804131865501404 	 ± 0.31924293098070167
	data : 1.5513790249824524
	model : 0.07706516981124878
			 train-loss:  2.194603776931763 	 ± 0.28694656402618796
	data : 1.264378261566162
	model : 0.07564983367919922
			 train-loss:  2.1907726923624673 	 ± 0.26208521810710367
	data : 0.18436102867126464
	model : 0.06870403289794921
			 train-loss:  2.134614518710545 	 ± 0.278923714341422
	data : 0.11650452613830567
	model : 0.06911444664001465
			 train-loss:  2.1077568382024765 	 ± 0.2704126081290917
	data : 0.116542387008667
	model : 0.06907944679260254
			 train-loss:  2.0966361231274075 	 ± 0.25688045013491395
	data : 0.11654133796691894
	model : 0.06838383674621581
			 train-loss:  2.119205415248871 	 ± 0.25292917114706226
	data : 0.11708545684814453
	model : 0.06837902069091797
			 train-loss:  2.126931569793008 	 ± 0.24239298242463184
	data : 0.1172494888305664
	model : 0.06853690147399902
			 train-loss:  2.1061652998129525 	 ± 0.24207804339175149
	data : 0.1171802043914795
	model : 0.06928462982177734
			 train-loss:  2.1144235409223118 	 ± 0.23433380568341003
	data : 0.11647553443908691
	model : 0.0693281650543213
			 train-loss:  2.1160970500537326 	 ± 0.22589030800224208
	data : 0.1164736270904541
	model : 0.06998586654663086
			 train-loss:  2.1525750875473024 	 ± 0.2573979996763953
	data : 0.11584911346435547
	model : 0.06991972923278808
			 train-loss:  2.1555087342858315 	 ± 0.24948339893742183
	data : 0.11595163345336915
	model : 0.0697441577911377
			 train-loss:  2.15614713640774 	 ± 0.2420479190650356
	data : 0.11576337814331054
	model : 0.06936759948730468
			 train-loss:  2.1541055374675326 	 ± 0.2353788628915721
	data : 0.11584105491638183
	model : 0.06863160133361816
			 train-loss:  2.1299322404359518 	 ± 0.2510090036859949
	data : 0.11665549278259277
	model : 0.06804676055908203
			 train-loss:  2.1286586999893187 	 ± 0.2447162849811573
	data : 0.11742610931396484
	model : 0.06805357933044434
			 train-loss:  2.1161905186516896 	 ± 0.24524162351251538
	data : 0.11730399131774902
	model : 0.06816163063049316
			 train-loss:  2.1058534492145884 	 ± 0.24424089673787455
	data : 0.11729745864868164
	model : 0.06830129623413086
			 train-loss:  2.102018688036048 	 ± 0.23954853126091935
	data : 0.11725316047668458
	model : 0.06958460807800293
			 train-loss:  2.095019439856211 	 ± 0.2368950772774424
	data : 0.11607575416564941
	model : 0.07071309089660645
			 train-loss:  2.094996099472046 	 ± 0.23210885292749778
	data : 0.11500954627990723
	model : 0.07016377449035645
			 train-loss:  2.0952098552997294 	 ± 0.22760396522950058
	data : 0.11573500633239746
	model : 0.0703432559967041
			 train-loss:  2.0934303071763782 	 ± 0.22353355679333675
	data : 0.11560149192810058
	model : 0.07016158103942871
			 train-loss:  2.106191413743155 	 ± 0.2293022983650384
	data : 0.115899658203125
	model : 0.06961431503295898
			 train-loss:  2.098726613768216 	 ± 0.22875031583097627
	data : 0.11639881134033203
	model : 0.06920819282531739
			 train-loss:  2.100309145450592 	 ± 0.22506690386547662
	data : 0.11677818298339844
	model : 0.06983809471130371
			 train-loss:  2.0979013481447772 	 ± 0.22179945898469655
	data : 0.1160898208618164
	model : 0.06878738403320313
			 train-loss:  2.123736906796694 	 ± 0.26143721057315966
	data : 0.11709675788879395
	model : 0.06896910667419434
			 train-loss:  2.1143604986595386 	 ± 0.26285274512907314
	data : 0.11692194938659668
	model : 0.06842331886291504
			 train-loss:  2.106931837166057 	 ± 0.2624510737889136
	data : 0.11751680374145508
	model : 0.06771740913391114
			 train-loss:  2.1088332891464234 	 ± 0.2589121040532293
	data : 0.11802101135253906
	model : 0.06766557693481445
			 train-loss:  2.117902288834254 	 ± 0.260867816070503
	data : 0.11814050674438477
	model : 0.06842155456542968
			 train-loss:  2.1162493518880896 	 ± 0.25750948131272533
	data : 0.1172607421875
	model : 0.06759052276611328
			 train-loss:  2.118842159446917 	 ± 0.25458759050556373
	data : 0.11793932914733887
	model : 0.06796655654907227
			 train-loss:  2.1119516904537496 	 ± 0.25486684293024464
	data : 0.11752877235412598
	model : 0.069057035446167
			 train-loss:  2.120737561583519 	 ± 0.25757261046828533
	data : 0.11655750274658203
	model : 0.0691136360168457
			 train-loss:  2.11704374813452 	 ± 0.25548244984463775
	data : 0.1165684700012207
	model : 0.06925754547119141
			 train-loss:  2.1180126695405868 	 ± 0.25249890135967806
	data : 0.11672897338867187
	model : 0.07006330490112304
			 train-loss:  2.1195711618246036 	 ± 0.24974991156445445
	data : 0.11593303680419922
	model : 0.07056469917297363
			 train-loss:  2.1333049020983954 	 ± 0.2628076787607663
	data : 0.11551337242126465
	model : 0.06994032859802246
			 train-loss:  2.13156209786733 	 ± 0.26012819619612587
	data : 0.11598587036132812
	model : 0.06905603408813477
			 train-loss:  2.127232287241065 	 ± 0.2589194670353515
	data : 0.11670827865600586
	model : 0.06893682479858398
			 train-loss:  2.1291993628156947 	 ± 0.2564973962818325
	data : 0.11689591407775879
	model : 0.0690467357635498
			 train-loss:  2.1337268352508545 	 ± 0.25570232021641753
	data : 0.11684026718139648
	model : 0.06864986419677735
			 train-loss:  2.127700158527919 	 ± 0.2565009079308434
	data : 0.11721014976501465
	model : 0.06890177726745605
			 train-loss:  2.127138657569885 	 ± 0.25395336252337325
	data : 0.11709799766540527
	model : 0.06886401176452636
			 train-loss:  2.1300667874953327 	 ± 0.25230230374552115
	data : 0.11713638305664062
	model : 0.06899142265319824
			 train-loss:  2.1218624092065372 	 ± 0.25664214951189124
	data : 0.11695866584777832
	model : 0.06816420555114747
			 train-loss:  2.1271125573032306 	 ± 0.2570131986479004
	data : 0.11759448051452637
	model : 0.0682347297668457
			 train-loss:  2.1325997842682733 	 ± 0.2577369608387342
	data : 0.11752676963806152
	model : 0.06839108467102051
			 train-loss:  2.1349924369291826 	 ± 0.2559876783154542
	data : 0.11734557151794434
	model : 0.0691871166229248
			 train-loss:  2.129223142351423 	 ± 0.257274522997545
	data : 0.11658248901367188
	model : 0.06826238632202149
			 train-loss:  2.1312578017251536 	 ± 0.2554618952471795
	data : 0.11753778457641602
	model : 0.0687781810760498
			 train-loss:  2.1269065532191047 	 ± 0.25537187743667855
	data : 0.11705446243286133
	model : 0.06788859367370606
			 train-loss:  2.12327908055257 	 ± 0.25470111233490866
	data : 0.11774406433105469
	model : 0.06794309616088867
			 train-loss:  2.1222868939240773 	 ± 0.2526846400257356
	data : 0.11784334182739258
	model : 0.06737442016601562
			 train-loss:  2.1235717691358973 	 ± 0.2508024481562262
	data : 0.11836738586425781
	model : 0.06828560829162597
			 train-loss:  2.1190316888593856 	 ± 0.2512860398172183
	data : 0.11740398406982422
	model : 0.06864342689514161
			 train-loss:  2.116213902594551 	 ± 0.25026915991050797
	data : 0.11733431816101074
	model : 0.0697336196899414
			 train-loss:  2.108399810269475 	 ± 0.255935086314887
	data : 0.11637377738952637
	model : 0.06952333450317383
			 train-loss:  2.1053898866360004 	 ± 0.25509772732487757
	data : 0.11646132469177246
	model : 0.07003836631774903
			 train-loss:  2.0995478846810083 	 ± 0.2575019457167854
	data : 0.11586885452270508
	model : 0.07003197669982911
			 train-loss:  2.095989894511095 	 ± 0.2572024591189792
	data : 0.11580986976623535
	model : 0.069942045211792
			 train-loss:  2.1001230040017296 	 ± 0.2575360149569463
	data : 0.1159822940826416
	model : 0.06948666572570801
			 train-loss:  2.1055572050205176 	 ± 0.2595604877429303
	data : 0.11635994911193848
	model : 0.06974668502807617
			 train-loss:  2.1056639449937005 	 ± 0.257701340404648
	data : 0.11609020233154296
	model : 0.06974668502807617
			 train-loss:  2.1147574690026296 	 ± 0.26695145912040663
	data : 0.11618599891662598
	model : 0.06972365379333496
			 train-loss:  2.1113143960634866 	 ± 0.2666739654429635
	data : 0.11632022857666016
	model : 0.0698361873626709
			 train-loss:  2.1087290832441146 	 ± 0.2657481182645317
	data : 0.11612567901611329
	model : 0.07010149955749512
			 train-loss:  2.1058049669136873 	 ± 0.26512618406710187
	data : 0.11607894897460938
	model : 0.06953635215759277
			 train-loss:  2.106528464953105 	 ± 0.2634262771406351
	data : 0.11666345596313477
	model : 0.06965479850769044
			 train-loss:  2.1028488043107485 	 ± 0.26362060820972244
	data : 0.11655516624450683
	model : 0.06876893043518066
			 train-loss:  2.103835259165083 	 ± 0.2620443416136897
	data : 0.11732692718505859
	model : 0.06867036819458008
			 train-loss:  2.101266555297069 	 ± 0.26133302934387337
	data : 0.11749262809753418
	model : 0.06850719451904297
			 train-loss:  2.1044580423379244 	 ± 0.26119903358049645
	data : 0.11758761405944824
	model : 0.06871547698974609
			 train-loss:  2.1004461348056793 	 ± 0.2619993515561225
	data : 0.11730318069458008
	model : 0.06803336143493652
			 train-loss:  2.096883597197356 	 ± 0.2623195409370581
	data : 0.11791267395019531
	model : 0.06903514862060547
			 train-loss:  2.0953271548922467 	 ± 0.26109117170265345
	data : 0.11708531379699708
	model : 0.06812858581542969
			 train-loss:  2.090437900589173 	 ± 0.2632631492148681
	data : 0.11786379814147949
	model : 0.0679123878479004
			 train-loss:  2.0894592631430853 	 ± 0.2618432514166941
	data : 0.11816534996032715
	model : 0.06802868843078613
			 train-loss:  2.088371680764591 	 ± 0.26048922441792516
	data : 0.11810860633850098
	model : 0.06797895431518555
			 train-loss:  2.083268354105395 	 ± 0.2632097334755164
	data : 0.11823205947875977
	model : 0.06719775199890136
			 train-loss:  2.080592207524968 	 ± 0.26286681166433434
	data : 0.11887154579162598
	model : 0.0678497314453125
			 train-loss:  2.0865637063980103 	 ± 0.2672378354708705
	data : 0.11825666427612305
	model : 0.06824030876159667
			 train-loss:  2.0849288367153553 	 ± 0.26617445286904656
	data : 0.1176264762878418
	model : 0.06805558204650879
			 train-loss:  2.082650746239556 	 ± 0.2655626347909849
	data : 0.11782174110412598
	model : 0.06849522590637207
			 train-loss:  2.0847023869608785 	 ± 0.26481570951562866
	data : 0.11735701560974121
	model : 0.06949119567871094
			 train-loss:  2.08457605994266 	 ± 0.2633753183539939
	data : 0.11639480590820313
	model : 0.07026581764221192
			 train-loss:  2.092768666564777 	 ± 0.2734878630712219
	data : 0.1157726764678955
	model : 0.07051091194152832
			 train-loss:  2.0951641442927906 	 ± 0.273008381583779
	data : 0.11567187309265137
	model : 0.0708780288696289
			 train-loss:  2.094747641212062 	 ± 0.27159771553799805
	data : 0.11537213325500488
	model : 0.0710792064666748
			 train-loss:  2.0928246304392815 	 ± 0.27082879746446376
	data : 0.11513838768005372
	model : 0.0706812858581543
			 train-loss:  2.088654491090283 	 ± 0.2725096624140418
	data : 0.11559700965881348
	model : 0.07019557952880859
			 train-loss:  2.0855633020401 	 ± 0.27281976497614535
	data : 0.11608057022094727
	model : 0.0699812889099121
			 train-loss:  2.0833392564696496 	 ± 0.2723298455691245
	data : 0.1160822868347168
	model : 0.06910533905029297
			 train-loss:  2.0860801351070406 	 ± 0.2723336896063574
	data : 0.11682524681091308
	model : 0.06917672157287598
			 train-loss:  2.089852966884575 	 ± 0.27359596342846076
	data : 0.11660079956054688
	model : 0.06840662956237793
			 train-loss:  2.0888754655333126 	 ± 0.27242868331467013
	data : 0.11730790138244629
	model : 0.0685692310333252
			 train-loss:  2.087017506071665 	 ± 0.2717516066102973
	data : 0.11701207160949707
	model : 0.06855788230895996
			 train-loss:  2.092530832840846 	 ± 0.27616972552707403
	data : 0.11727490425109863
	model : 0.0690683364868164
			 train-loss:  2.098142278762091 	 ± 0.2807456586872446
	data : 0.1168337345123291
	model : 0.06819610595703125
			 train-loss:  2.093810445857498 	 ± 0.2829220019172452
	data : 0.11784019470214843
	model : 0.06885223388671875
			 train-loss:  2.0939901320733756 	 ± 0.28160290980266117
	data : 0.11728744506835938
	model : 0.0689666748046875
			 train-loss:  2.089370258428432 	 ± 0.2843407525948932
	data : 0.11714010238647461
	model : 0.06907691955566406
			 train-loss:  2.08437111618322 	 ± 0.2877620497175036
	data : 0.11696572303771972
	model : 0.06933913230895997
			 train-loss:  2.08330128734762 	 ± 0.28666872963552226
	data : 0.11679863929748535
	model : 0.07018923759460449
			 train-loss:  2.0836213354591853 	 ± 0.2853942482640451
	data : 0.11598200798034668
	model : 0.06944952011108399
			 train-loss:  2.084647899227483 	 ± 0.28432309329350125
	data : 0.11647005081176758
	model : 0.06894435882568359
			 train-loss:  2.0865440484696784 	 ± 0.2837726365926922
	data : 0.11698403358459472
	model : 0.06865873336791992
			 train-loss:  2.0880822183793053 	 ± 0.28299803376595306
	data : 0.11722979545593262
	model : 0.06837825775146485
			 train-loss:  2.0907088870587556 	 ± 0.2831572018822398
	data : 0.11720576286315917
	model : 0.06833329200744628
			 train-loss:  2.087392228430715 	 ± 0.28416867724381206
	data : 0.11732439994812012
	model : 0.06930298805236816
			 train-loss:  2.087056692848858 	 ± 0.2829747519533984
	data : 0.11651029586791992
	model : 0.06963644027709961
			 train-loss:  2.085761688523373 	 ± 0.2821211162594761
	data : 0.1163912296295166
	model : 0.06988639831542968
			 train-loss:  2.084233645631486 	 ± 0.2814231709686176
	data : 0.11617965698242187
	model : 0.07017326354980469
			 train-loss:  2.0857096483310062 	 ± 0.28071027942350973
	data : 0.11620845794677734
	model : 0.06941728591918946
			 train-loss:  2.086017737703875 	 ± 0.2795682849754597
	data : 0.1168860912322998
	model : 0.06923465728759766
			 train-loss:  2.08800436141061 	 ± 0.27927644127702556
	data : 0.11713051795959473
	model : 0.06905655860900879
			 train-loss:  2.0942804939378568 	 ± 0.2866474885499947
	data : 0.11722187995910645
	model : 0.0690086841583252
			 train-loss:  2.094461913070371 	 ± 0.28549640213955557
	data : 0.11721954345703126
	model : 0.06930041313171387
			 train-loss:  2.099081234931946 	 ± 0.28896722780048223
	data : 0.11692204475402831
	model : 0.06998710632324219
			 train-loss:  2.093699738146767 	 ± 0.2940398058883884
	data : 0.11634597778320313
	model : 0.07025933265686035
			 train-loss:  2.093957650379872 	 ± 0.2928941892045608
	data : 0.11624236106872558
	model : 0.07041583061218262
			 train-loss:  2.094906863756478 	 ± 0.29194386935031946
	data : 0.115974760055542
	model : 0.06962480545043945
			 train-loss:  2.096057560092719 	 ± 0.29110135822562805
	data : 0.11666808128356934
	model : 0.0682337760925293
			 train-loss:  2.097116166811723 	 ± 0.29022873430630375
	data : 0.11792111396789551
	model : 0.0686553955078125
			 train-loss:  2.09354184784052 	 ± 0.2919770002278963
	data : 0.11738972663879395
	model : 0.06817407608032226
			 train-loss:  2.0948159550175522 	 ± 0.29123425236845596
	data : 0.11774687767028809
	model : 0.06799712181091308
			 train-loss:  2.094013162125322 	 ± 0.29028388847144626
	data : 0.1177513599395752
	model : 0.06878890991210937
			 train-loss:  2.092616073231199 	 ± 0.28964718473455925
	data : 0.11714420318603516
	model : 0.0698019027709961
			 train-loss:  2.0957063030313563 	 ± 0.29078115352912737
	data : 0.11623663902282715
	model : 0.06964836120605469
			 train-loss:  2.0958756076938965 	 ± 0.2897168111574638
	data : 0.11642169952392578
	model : 0.0699087142944336
			 train-loss:  2.099163701934536 	 ± 0.29119329154088125
	data : 0.1159210205078125
	model : 0.06986379623413086
			 train-loss:  2.0983015920804893 	 ± 0.29031174644283825
	data : 0.11631178855895996
	model : 0.06988320350646973
			 train-loss:  2.096727594197225 	 ± 0.2898559356098017
	data : 0.11631889343261718
	model : 0.06988296508789063
			 train-loss:  2.0980755482401166 	 ± 0.2892557796252364
	data : 0.11631088256835938
	model : 0.0700190544128418
			 train-loss:  2.0990498607040298 	 ± 0.28845867887558174
	data : 0.11613693237304687
	model : 0.07027482986450195
			 train-loss:  2.0999602599882743 	 ± 0.28764439728809665
	data : 0.11614351272583008
	model : 0.0701591968536377
			 train-loss:  2.0972751253968354 	 ± 0.2884172591517723
	data : 0.11605844497680665
	model : 0.07055292129516602
			 train-loss:  2.0960163374741874 	 ± 0.28780798348350395
	data : 0.11568431854248047
	model : 0.07051353454589844
			 train-loss:  2.0986738254284036 	 ± 0.2885812418818562
	data : 0.11568589210510254
	model : 0.07000408172607422
			 train-loss:  2.0981417224831778 	 ± 0.2876626192405177
	data : 0.1161712646484375
	model : 0.06990633010864258
			 train-loss:  2.0977096346770825 	 ± 0.28673004202406144
	data : 0.11607875823974609
	model : 0.07022705078125
			 train-loss:  2.097424123738263 	 ± 0.2857806836256165
	data : 0.11575775146484375
	model : 0.06969923973083496
			 train-loss:  2.0957941304917305 	 ± 0.2855095309244844
	data : 0.11627273559570313
	model : 0.06950583457946777
			 train-loss:  2.098934996922811 	 ± 0.2871274029923645
	data : 0.11635541915893555
	model : 0.06971135139465331
			 train-loss:  2.1009368075440262 	 ± 0.28722335897309237
	data : 0.11614251136779785
	model : 0.06944165229797364
			 train-loss:  2.1004718573469865 	 ± 0.2863339934469985
	data : 0.11644649505615234
	model : 0.06879301071166992
			 train-loss:  2.0983416278377858 	 ± 0.28660259848732994
	data : 0.11718063354492188
	model : 0.06803412437438965
			 train-loss:  2.1002722831515523 	 ± 0.286666989384107
	data : 0.11793360710144044
	model : 0.06743602752685547
			 train-loss:  2.0992522231994135 	 ± 0.28602101816560155
	data : 0.11840171813964843
	model : 0.067694091796875
			 train-loss:  2.1008092157351665 	 ± 0.28576103202143477
	data : 0.11824150085449218
	model : 0.06709527969360352
			 train-loss:  2.100452610641528 	 ± 0.2848843311673321
	data : 0.11869683265686035
	model : 0.06763553619384766
			 train-loss:  2.099159463296963 	 ± 0.2844432407113764
	data : 0.11807103157043457
	model : 0.06829633712768554
			 train-loss:  2.1032121953724316 	 ± 0.2880871305686535
	data : 0.11743841171264649
	model : 0.06901407241821289
			 train-loss:  2.100952013581991 	 ± 0.28859611940598434
	data : 0.11676411628723145
	model : 0.06870121955871582
			 train-loss:  2.100824817367222 	 ± 0.2877029609243179
	data : 0.11695737838745117
	model : 0.06950716972351074
			 train-loss:  2.0997559597462785 	 ± 0.2871340885958019
	data : 0.11634578704833984
	model : 0.06954388618469239
			 train-loss:  2.09928035736084 	 ± 0.28631595355779316
	data : 0.11622037887573242
	model : 0.06977629661560059
			 train-loss:  2.0981558612207087 	 ± 0.2858025169533377
	data : 0.1159128189086914
	model : 0.0698009967803955
			 train-loss:  2.0971573280565665 	 ± 0.28522192932130225
	data : 0.11617851257324219
	model : 0.0691859245300293
			 train-loss:  2.097830877246627 	 ± 0.2844931188940588
	data : 0.11702437400817871
	model : 0.06907110214233399
			 train-loss:  2.0982532344178524 	 ± 0.2836922593120871
	data : 0.11682629585266113
	model : 0.06811928749084473
			 train-loss:  2.09581743819373 	 ± 0.28459281595006325
	data : 0.11772103309631347
	model : 0.06810479164123535
			 train-loss:  2.0957846246527496 	 ± 0.2837498948732669
	data : 0.11777706146240234
	model : 0.06827106475830078
			 train-loss:  2.0967898523106294 	 ± 0.28321575278582034
	data : 0.11762452125549316
	model : 0.06869001388549804
			 train-loss:  2.096537096458569 	 ± 0.2824056515689771
	data : 0.11691927909851074
	model : 0.0678795337677002
			 train-loss:  2.0953761165918308 	 ± 0.28199247965381063
	data : 0.11781415939331055
	model : 0.06854243278503418
			 train-loss:  2.095678853850833 	 ± 0.28120432175102117
	data : 0.11720213890075684
	model : 0.06759905815124512
			 train-loss:  2.094965008483536 	 ± 0.280552255482351
	data : 0.11805000305175781
	model : 0.06660146713256836
			 train-loss:  2.0952735151563373 	 ± 0.2797791267836101
	data : 0.11872820854187012
	model : 0.06696701049804688
			 train-loss:  2.093455608595501 	 ± 0.28001775834000425
	data : 0.11870579719543457
	model : 0.0680323600769043
			 train-loss:  2.097699868476997 	 ± 0.28484622084210287
	data : 0.11784176826477051
	model : 0.06753792762756347
			 train-loss:  2.0979228207234586 	 ± 0.2840604510891565
	data : 0.11822667121887206
	model : 0.06857457160949706
			 train-loss:  2.0960844821770097 	 ± 0.2843257020111976
	data : 0.11720452308654786
	model : 0.06855287551879882
			 train-loss:  2.0959749228424496 	 ± 0.2835385972958382
	data : 0.11727023124694824
	model : 0.06784958839416504
			 train-loss:  2.0954850872577224 	 ± 0.2828306183219933
	data : 0.11787934303283691
	model : 0.06766552925109863
			 train-loss:  2.0945359756658366 	 ± 0.28234142970865234
	data : 0.11807184219360352
	model : 0.06830620765686035
			 train-loss:  2.092605358915902 	 ± 0.2827709988472729
	data : 0.11754860877990722
	model : 0.06808524131774903
			 train-loss:  2.0920062155827233 	 ± 0.28211800303676654
	data : 0.11781926155090332
	model : 0.06889443397521973
			 train-loss:  2.098838298385208 	 ± 0.29622454616423516
	data : 0.11704444885253906
	model : 0.06951284408569336
			 train-loss:  2.1007455382295834 	 ± 0.29656392622595756
	data : 0.11640024185180664
	model : 0.06958379745483398
			 train-loss:  2.1016200952988893 	 ± 0.296010308481596
	data : 0.11622605323791504
	model : 0.06971588134765624
			 train-loss:  2.1018449144160494 	 ± 0.2952380046211695
	data : 0.11605710983276367
	model : 0.06975059509277344
			 train-loss:  2.101171127702824 	 ± 0.2946008082213974
	data : 0.11605987548828126
	model : 0.06969752311706542
			 train-loss:  2.0999943325394077 	 ± 0.2942695782809249
	data : 0.11631183624267578
	model : 0.06966633796691894
			 train-loss:  2.0977840205137643 	 ± 0.2950753310093094
	data : 0.1162322998046875
	model : 0.06933350563049316
			 train-loss:  2.0975810258338847 	 ± 0.29431927349886794
	data : 0.11687779426574707
	model : 0.06940841674804688
			 train-loss:  2.0970901954977004 	 ± 0.2936345724910341
	data : 0.11676735877990722
	model : 0.06962351799011231
			 train-loss:  2.0956587871325385 	 ± 0.2935511303649302
	data : 0.11663002967834472
	model : 0.06885328292846679
			 train-loss:  2.094924014653915 	 ± 0.2929762718325565
	data : 0.11722369194030761
	model : 0.06868491172790528
			 train-loss:  2.0945697122690627 	 ± 0.29226980699253147
	data : 0.11744732856750488
	model : 0.06947817802429199
			 train-loss:  2.0960140833394783 	 ± 0.29222752166747706
	data : 0.11676263809204102
	model : 0.06929240226745606
			 train-loss:  2.0958087516553476 	 ± 0.2915028860211087
	data : 0.11693215370178223
	model : 0.07004318237304688
			 train-loss:  2.09814002406058 	 ± 0.2926141220376012
	data : 0.1159372329711914
	model : 0.07009668350219726
			 train-loss:  2.0996465599536895 	 ± 0.2926543519628663
	data : 0.11583752632141113
	model : 0.07035984992980956
			 train-loss:  2.1006755247638 	 ± 0.29228790789556275
	data : 0.1155550479888916
	model : 0.06992063522338868
			 train-loss:  2.101115901871483 	 ± 0.2916303647525797
	data : 0.1157719612121582
	model : 0.0699230670928955
			 train-loss:  2.0998355455586477 	 ± 0.29147976556621064
	data : 0.11597394943237305
	model : 0.06903109550476075
			 train-loss:  2.0979044057574927 	 ± 0.29206339895855604
	data : 0.11686763763427735
	model : 0.06972079277038574
			 train-loss:  2.097342429509977 	 ± 0.2914607231865857
	data : 0.11639528274536133
	model : 0.0697129249572754
			 train-loss:  2.09679784705338 	 ± 0.2908569657414716
	data : 0.11642313003540039
	model : 0.0696894645690918
			 train-loss:  2.0973162098207334 	 ± 0.29024893076588304
	data : 0.11631588935852051
	model : 0.06980094909667969
			 train-loss:  2.097483992576599 	 ± 0.28956043881007704
	data : 0.11614274978637695
	model : 0.06988954544067383
			 train-loss:  2.0978032082461855 	 ± 0.28890356385425636
	data : 0.11600313186645508
	model : 0.07028741836547851
			 train-loss:  2.0964361962817963 	 ± 0.2888916377854337
	data : 0.11559624671936035
	model : 0.07007422447204589
			 train-loss:  2.097041157184619 	 ± 0.2883395504605177
	data : 0.1158216953277588
	model : 0.06981935501098632
			 train-loss:  2.09523727871337 	 ± 0.2888496446967767
	data : 0.11613421440124512
	model : 0.06977910995483398
			 train-loss:  2.0963419591876824 	 ± 0.2886193260824821
	data : 0.1163060188293457
	model : 0.06968936920166016
			 train-loss:  2.094852107707585 	 ± 0.28876399511181766
	data : 0.11650643348693848
	model : 0.06951413154602051
			 train-loss:  2.0953639296598214 	 ± 0.2881889469107005
	data : 0.11668581962585449
	model : 0.06888670921325683
			 train-loss:  2.0936867511934705 	 ± 0.2885708657046064
	data : 0.11719446182250977
	model : 0.06967620849609375
			 train-loss:  2.094667141887999 	 ± 0.2882655185086839
	data : 0.11648497581481934
	model : 0.06961078643798828
			 train-loss:  2.093700545096616 	 ± 0.28795585660039574
	data : 0.11625847816467286
	model : 0.06965036392211914
			 train-loss:  2.0938992865009394 	 ± 0.2873126560291299
	data : 0.11618170738220215
	model : 0.06989989280700684
			 train-loss:  2.0947197540239855 	 ± 0.2869159548372408
	data : 0.11582713127136231
	model : 0.07021336555480957
			 train-loss:  2.0943596519496106 	 ± 0.28631591147032553
	data : 0.11552534103393555
	model : 0.06930675506591796
			 train-loss:  2.0923098970103906 	 ± 0.28729090962765863
	data : 0.11621074676513672
	model : 0.06957049369812011
			 train-loss:  2.0927023256840727 	 ± 0.2867056642656417
	data : 0.11631274223327637
	model : 0.06980867385864258
			 train-loss:  2.0944127536245754 	 ± 0.28720301832416006
	data : 0.11619873046875
	model : 0.06965241432189942
			 train-loss:  2.0954614543914794 	 ± 0.2869935894814409
	data : 0.11625685691833496
	model : 0.07024178504943848
			 train-loss:  2.0962120969738582 	 ± 0.28657922385883206
	data : 0.11556453704833984
	model : 0.07067060470581055
			 train-loss:  2.0960421572697845 	 ± 0.28595870767196085
	data : 0.11516656875610351
	model : 0.070587158203125
			 train-loss:  2.094842189236691 	 ± 0.28590312044059524
	data : 0.11496753692626953
	model : 0.07045788764953613
			 train-loss:  2.0958722980782456 	 ± 0.28570191594480365
	data : 0.11514530181884766
	model : 0.07069330215454102
			 train-loss:  2.093990229005399 	 ± 0.2864993037732081
	data : 0.11503901481628417
	model : 0.07003669738769532
			 train-loss:  2.0933227157179926 	 ± 0.28605768717408947
	data : 0.11576647758483886
	model : 0.07028460502624512
			 train-loss:  2.094559222459793 	 ± 0.2860585191598168
	data : 0.1156644344329834
	model : 0.07057108879089355
			 train-loss:  2.094800522399051 	 ± 0.285467660633788
	data : 0.11546592712402344
	model : 0.07060356140136718
			 train-loss:  2.0945373121489825 	 ± 0.28488536629714434
	data : 0.11539769172668457
	model : 0.07052316665649414
			 train-loss:  2.0936482490377224 	 ± 0.28460371219762753
	data : 0.11567859649658203
	model : 0.07128148078918457
			 train-loss:  2.09458559549461 	 ± 0.2843633785450683
	data : 0.114947509765625
	model : 0.0711357593536377
			 train-loss:  2.0936335843323657 	 ± 0.2841394581945911
	data : 0.11496343612670898
	model : 0.07078003883361816
			 train-loss:  2.09481153468124 	 ± 0.284121209038121
	data : 0.11523151397705078
	model : 0.07000946998596191
			 train-loss:  2.0942663916982864 	 ± 0.2836508937872777
	data : 0.11565656661987304
	model : 0.06982340812683105
			 train-loss:  2.094662003715833 	 ± 0.28312540368082934
	data : 0.11554698944091797
	model : 0.06983885765075684
			 train-loss:  2.0936404491361245 	 ± 0.2829802781605347
	data : 0.11553401947021484
	model : 0.06964740753173829
			 train-loss:  2.093268189548461 	 ± 0.2824541286241834
	data : 0.11587920188903808
	model : 0.06970252990722656
			 train-loss:  2.094979685520439 	 ± 0.28312698831052846
	data : 0.11571455001831055
	model : 0.0704488754272461
			 train-loss:  2.0961844119869295 	 ± 0.28316964026824937
	data : 0.11507716178894042
	model : 0.0702634334564209
			 train-loss:  2.097281394686018 	 ± 0.28311019375019025
	data : 0.11510462760925293
	model : 0.07011618614196777
			 train-loss:  2.09786394650374 	 ± 0.2826812834306983
	data : 0.11540331840515136
	model : 0.07035655975341797
			 train-loss:  2.0981555515937957 	 ± 0.28214554615234283
	data : 0.11484274864196778
	model : 0.06983661651611328
			 train-loss:  2.096630178151592 	 ± 0.28259481166481654
	data : 0.11563630104064941
	model : 0.07002534866333007
			 train-loss:  2.0968631003276412 	 ± 0.28205063385906587
	data : 0.11567325592041015
	model : 0.06943998336791993
			 train-loss:  2.0963356108665465 	 ± 0.2816090070608884
	data : 0.11643552780151367
	model : 0.06917281150817871
			 train-loss:  2.0972026054602697 	 ± 0.28138159505498667
	data : 0.11671214103698731
	model : 0.06847271919250489
			 train-loss:  2.095781741161195 	 ± 0.28172352729790595
	data : 0.11766576766967773
	model : 0.06940622329711914
			 train-loss:  2.0969579865338774 	 ± 0.28178554460337296
	data : 0.1168492317199707
	model : 0.0692983627319336
			 train-loss:  2.0958535638381175 	 ± 0.28177842210800474
	data : 0.11711902618408203
	model : 0.07017946243286133
			 train-loss:  2.0966226105596504 	 ± 0.281492334130411
	data : 0.11623110771179199
	model : 0.07034249305725097
			 train-loss:  2.093317698687315 	 ± 0.2858559669260283
	data : 0.1148557186126709
	model : 0.061620235443115234
#epoch  95    val-loss:  2.457269310951233  train-loss:  2.093317698687315  lr:  6.103515625e-07
			 train-loss:  2.1878867149353027 	 ± 0.0
	data : 5.903056859970093
	model : 0.07646775245666504
			 train-loss:  2.2752691507339478 	 ± 0.08738243579864502
	data : 3.016779899597168
	model : 0.07154929637908936
			 train-loss:  2.244970719019572 	 ± 0.08322529655161505
	data : 2.0509184996287027
	model : 0.06949543952941895
			 train-loss:  2.2636367082595825 	 ± 0.0789942716435994
	data : 1.5679494738578796
	model : 0.06942570209503174
			 train-loss:  2.2542510986328126 	 ± 0.07310564023566643
	data : 1.277644443511963
	model : 0.06951389312744141
			 train-loss:  2.221752683321635 	 ± 0.09866321765097913
	data : 0.12023787498474121
	model : 0.06824960708618164
			 train-loss:  2.2633261340005055 	 ± 0.13679878798549558
	data : 0.11731667518615722
	model : 0.06893877983093262
			 train-loss:  2.257300615310669 	 ± 0.12895277419809162
	data : 0.11649246215820312
	model : 0.06991806030273437
			 train-loss:  2.283353805541992 	 ± 0.14216652665706167
	data : 0.11578803062438965
	model : 0.06998147964477539
			 train-loss:  2.255341815948486 	 ± 0.15890951268077014
	data : 0.11575913429260254
	model : 0.0699195384979248
			 train-loss:  2.2346998995000664 	 ± 0.16497715842376706
	data : 0.11575431823730468
	model : 0.06897106170654296
			 train-loss:  2.2587979435920715 	 ± 0.17702320164631347
	data : 0.11658902168273926
	model : 0.06802887916564941
			 train-loss:  2.2660306233626146 	 ± 0.1719139287087043
	data : 0.11755399703979492
	model : 0.06788392066955566
			 train-loss:  2.2672788756234303 	 ± 0.16572153188565755
	data : 0.11750645637512207
	model : 0.06770949363708496
			 train-loss:  2.276069148381551 	 ± 0.16344565535399608
	data : 0.11765789985656738
	model : 0.06775069236755371
			 train-loss:  2.279065430164337 	 ± 0.15868047353421916
	data : 0.11761841773986817
	model : 0.06858110427856445
			 train-loss:  2.261438285603243 	 ± 0.16932160422932105
	data : 0.11680269241333008
	model : 0.06957836151123047
			 train-loss:  2.2449545529153614 	 ± 0.17803417428763785
	data : 0.116023588180542
	model : 0.06970925331115722
			 train-loss:  2.214656666705483 	 ± 0.21575741527320627
	data : 0.11608901023864746
	model : 0.06998000144958497
			 train-loss:  2.2314283728599547 	 ± 0.22263919590317682
	data : 0.1158947467803955
	model : 0.06987180709838867
			 train-loss:  2.2318170297713507 	 ± 0.21728055966883755
	data : 0.11605286598205566
	model : 0.06984148025512696
			 train-loss:  2.232435844161294 	 ± 0.212303876131189
	data : 0.11609711647033691
	model : 0.0696643352508545
			 train-loss:  2.222741697145545 	 ± 0.21255758198323293
	data : 0.11617717742919922
	model : 0.0696530818939209
			 train-loss:  2.2118959923585257 	 ± 0.21448466053848828
	data : 0.11620407104492188
	model : 0.06962046623229981
			 train-loss:  2.2299513626098633 	 ± 0.22800753562101292
	data : 0.11603960990905762
	model : 0.06914877891540527
			 train-loss:  2.2229550159894504 	 ± 0.2262998921122668
	data : 0.11637401580810547
	model : 0.06921100616455078
			 train-loss:  2.2504662496072276 	 ± 0.26266610187413164
	data : 0.11641016006469726
	model : 0.06932272911071777
			 train-loss:  2.2314104565552304 	 ± 0.2762856418581235
	data : 0.11642537117004395
	model : 0.06857571601867676
			 train-loss:  2.225370197460569 	 ± 0.27335531795779594
	data : 0.11702499389648438
	model : 0.06771392822265625
			 train-loss:  2.221737865606944 	 ± 0.269471668393479
	data : 0.11790332794189454
	model : 0.06836080551147461
			 train-loss:  2.2238201672031033 	 ± 0.26533495953804326
	data : 0.11724472045898438
	model : 0.06827640533447266
			 train-loss:  2.2181521095335484 	 ± 0.26305606140066323
	data : 0.1170682430267334
	model : 0.0684295654296875
			 train-loss:  2.2058152971845684 	 ± 0.2682757340918045
	data : 0.11682376861572266
	model : 0.0691248893737793
			 train-loss:  2.2110537465880897 	 ± 0.2660086736035201
	data : 0.11637792587280274
	model : 0.06918368339538575
			 train-loss:  2.2207262345722745 	 ± 0.2681787116844357
	data : 0.11604146957397461
	model : 0.06897735595703125
			 train-loss:  2.2149378226863012 	 ± 0.2666359850239564
	data : 0.11623706817626953
	model : 0.06890439987182617
			 train-loss:  2.2139922702634656 	 ± 0.26306929755879943
	data : 0.11634149551391601
	model : 0.06869606971740723
			 train-loss:  2.204474477391494 	 ± 0.2659624583060325
	data : 0.11651997566223145
	model : 0.06875267028808593
			 train-loss:  2.2101810436982374 	 ± 0.2648768601662056
	data : 0.1164121150970459
	model : 0.06959319114685059
			 train-loss:  2.2181647211313247 	 ± 0.266254732134638
	data : 0.11601843833923339
	model : 0.06984848976135254
			 train-loss:  2.2154035422860123 	 ± 0.2635668487191478
	data : 0.11575498580932617
	model : 0.06997995376586914
			 train-loss:  2.2103050407909213 	 ± 0.2624486225013785
	data : 0.11577281951904297
	model : 0.0698549747467041
			 train-loss:  2.20449930845305 	 ± 0.2620937044642298
	data : 0.11597342491149902
	model : 0.06981282234191895
			 train-loss:  2.200306689197367 	 ± 0.2605527955976406
	data : 0.11606659889221191
	model : 0.06980156898498535
			 train-loss:  2.1979720830917358 	 ± 0.2581064884693202
	data : 0.11609535217285157
	model : 0.06969752311706542
			 train-loss:  2.202678195808245 	 ± 0.2572301663935965
	data : 0.11645340919494629
	model : 0.07025322914123536
			 train-loss:  2.2066536583798997 	 ± 0.25590338076472374
	data : 0.11592254638671876
	model : 0.07040972709655761
			 train-loss:  2.199936111768087 	 ± 0.2573774129383181
	data : 0.1157829761505127
	model : 0.07056679725646972
			 train-loss:  2.199377322683529 	 ± 0.25476699118062945
	data : 0.1155627727508545
	model : 0.07066349983215332
			 train-loss:  2.2051769208908083 	 ± 0.2554529839877667
	data : 0.11538686752319335
	model : 0.07044363021850586
			 train-loss:  2.206918954849243 	 ± 0.2532359123042625
	data : 0.11551604270935059
	model : 0.07017560005187988
			 train-loss:  2.201951345572105 	 ± 0.25328585142651205
	data : 0.11575436592102051
	model : 0.07009291648864746
			 train-loss:  2.189885989674982 	 ± 0.2655429554892276
	data : 0.11581859588623047
	model : 0.06984920501708984
			 train-loss:  2.1866512960857816 	 ± 0.264124621417218
	data : 0.11606364250183106
	model : 0.06971940994262696
			 train-loss:  2.1840344992550937 	 ± 0.2624179696135119
	data : 0.1160092830657959
	model : 0.06896376609802246
			 train-loss:  2.1766845328467235 	 ± 0.2657154517186015
	data : 0.11665158271789551
	model : 0.06870145797729492
			 train-loss:  2.1729960399761534 	 ± 0.2648167312933798
	data : 0.11683549880981445
	model : 0.06877107620239258
			 train-loss:  2.1614161421512734 	 ± 0.27669867969638956
	data : 0.11668629646301269
	model : 0.06836738586425781
			 train-loss:  2.16175956847304 	 ± 0.27435622144036204
	data : 0.1170931339263916
	model : 0.06838741302490234
			 train-loss:  2.162051961819331 	 ± 0.27206958321463304
	data : 0.11722836494445801
	model : 0.06945266723632812
			 train-loss:  2.156044864263691 	 ± 0.2738128891951264
	data : 0.11616582870483398
	model : 0.06931138038635254
			 train-loss:  2.151498004313438 	 ± 0.273907572689791
	data : 0.11647281646728516
	model : 0.06927804946899414
			 train-loss:  2.1506390628360568 	 ± 0.2718091647848475
	data : 0.11651296615600586
	model : 0.0697826862335205
			 train-loss:  2.1586441975086927 	 ± 0.2770614029458745
	data : 0.11617341041564941
	model : 0.06958646774291992
			 train-loss:  2.154089199579679 	 ± 0.27732638504294277
	data : 0.11641044616699218
	model : 0.069610595703125
			 train-loss:  2.1500906799778794 	 ± 0.27709899368275664
	data : 0.11656160354614258
	model : 0.06984930038452149
			 train-loss:  2.149700289341941 	 ± 0.2750416029871432
	data : 0.11630401611328126
	model : 0.06998910903930664
			 train-loss:  2.1446867725428413 	 ± 0.27607876230733375
	data : 0.11618547439575196
	model : 0.07016539573669434
			 train-loss:  2.14591257814048 	 ± 0.2742572324761601
	data : 0.11583752632141113
	model : 0.07026562690734864
			 train-loss:  2.147646611077445 	 ± 0.27267191754435766
	data : 0.11560592651367188
	model : 0.06995158195495606
			 train-loss:  2.1470264313926157 	 ± 0.2707946001587388
	data : 0.1158778190612793
	model : 0.069801664352417
			 train-loss:  2.149103297127618 	 ± 0.2694763376729212
	data : 0.11575770378112793
	model : 0.06950201988220214
			 train-loss:  2.1524977128799647 	 ± 0.2691696946839825
	data : 0.11592988967895508
	model : 0.06919169425964355
			 train-loss:  2.1542728623828373 	 ± 0.2677746691527469
	data : 0.11622934341430664
	model : 0.06956114768981933
			 train-loss:  2.157030798594157 	 ± 0.2670394905117605
	data : 0.1159980297088623
	model : 0.06973485946655274
			 train-loss:  2.161668222201498 	 ± 0.26829969468880155
	data : 0.11565947532653809
	model : 0.0695643424987793
			 train-loss:  2.156872724557852 	 ± 0.2698103337863151
	data : 0.11593050956726074
	model : 0.06985368728637695
			 train-loss:  2.1570620781336074 	 ± 0.2680803503870823
	data : 0.11578116416931153
	model : 0.0699242115020752
			 train-loss:  2.1519179993037936 	 ± 0.2702246567754828
	data : 0.11558871269226074
	model : 0.06954584121704102
			 train-loss:  2.1535129323601723 	 ± 0.26890436807571877
	data : 0.11613221168518066
	model : 0.06935391426086426
			 train-loss:  2.1560424301359387 	 ± 0.2681952969068823
	data : 0.11636123657226563
	model : 0.06900687217712402
			 train-loss:  2.1514690998123913 	 ± 0.2697140729995763
	data : 0.11661238670349121
	model : 0.06895236968994141
			 train-loss:  2.1512153407177292 	 ± 0.26809421378608395
	data : 0.11654763221740723
	model : 0.06904258728027343
			 train-loss:  2.149836588473547 	 ± 0.2667895014060678
	data : 0.11667370796203613
	model : 0.06913957595825196
			 train-loss:  2.149248628055348 	 ± 0.26527024792483755
	data : 0.11632437705993652
	model : 0.06933917999267578
			 train-loss:  2.1467068625050922 	 ± 0.26476256715327706
	data : 0.11639876365661621
	model : 0.07002782821655273
			 train-loss:  2.146573196882489 	 ± 0.263239463907746
	data : 0.11594724655151367
	model : 0.06998014450073242
			 train-loss:  2.1449520140886307 	 ± 0.26217594918248616
	data : 0.11591081619262696
	model : 0.06983442306518554
			 train-loss:  2.1469917123237354 	 ± 0.2614001216905584
	data : 0.11586461067199708
	model : 0.06957030296325684
			 train-loss:  2.1407629370689394 	 ± 0.26650289154102524
	data : 0.11623530387878418
	model : 0.06953945159912109
			 train-loss:  2.1426176094746854 	 ± 0.26561794527796806
	data : 0.11622905731201172
	model : 0.06945638656616211
			 train-loss:  2.1432662645111913 	 ± 0.264242884746159
	data : 0.11638193130493164
	model : 0.06944999694824219
			 train-loss:  2.145671425327178 	 ± 0.2638289306689292
	data : 0.11646103858947754
	model : 0.06904087066650391
			 train-loss:  2.145355208122984 	 ± 0.26243955078632275
	data : 0.11679720878601074
	model : 0.06935505867004395
			 train-loss:  2.141396945401242 	 ± 0.26386038013943175
	data : 0.11646795272827148
	model : 0.06968102455139161
			 train-loss:  2.139737620949745 	 ± 0.2629802975297882
	data : 0.11616935729980468
	model : 0.07002477645874024
			 train-loss:  2.1394237587132405 	 ± 0.2616392901747802
	data : 0.1158167839050293
	model : 0.0700721263885498
			 train-loss:  2.141027890906042 	 ± 0.2607799855015244
	data : 0.11577296257019043
	model : 0.07057957649230957
			 train-loss:  2.1378030295323844 	 ± 0.2614162258597385
	data : 0.11537866592407227
	model : 0.07056875228881836
			 train-loss:  2.141876530647278 	 ± 0.26324476525335333
	data : 0.11541476249694824
	model : 0.07033114433288574
			 train-loss:  2.140361153253234 	 ± 0.2623763068620179
	data : 0.11540675163269043
	model : 0.07021064758300781
			 train-loss:  2.1391819084391877 	 ± 0.2613558185487707
	data : 0.11567654609680175
	model : 0.07015366554260254
			 train-loss:  2.1388692207706783 	 ± 0.2601031782936137
	data : 0.1156991958618164
	model : 0.07000184059143066
			 train-loss:  2.1369975094611826 	 ± 0.2595457331510112
	data : 0.11593799591064453
	model : 0.06974687576293945
			 train-loss:  2.1350389889308383 	 ± 0.2590778826679498
	data : 0.11614160537719727
	model : 0.06956868171691895
			 train-loss:  2.1369300203503303 	 ± 0.2585799856785263
	data : 0.11666660308837891
	model : 0.06935162544250488
			 train-loss:  2.1342144614068146 	 ± 0.2588829601409418
	data : 0.11672234535217285
	model : 0.06936225891113282
			 train-loss:  2.1319927219991333 	 ± 0.25870445182390334
	data : 0.11685695648193359
	model : 0.06896162033081055
			 train-loss:  2.1323660688662747 	 ± 0.25754422726374315
	data : 0.11700706481933594
	model : 0.06927552223205566
			 train-loss:  2.1282160802321 	 ± 0.26000630769566274
	data : 0.11671481132507325
	model : 0.0698246955871582
			 train-loss:  2.1311943681390435 	 ± 0.26071049721257866
	data : 0.11601877212524414
	model : 0.07003178596496581
			 train-loss:  2.131021561367171 	 ± 0.25955038698878125
	data : 0.11601262092590332
	model : 0.07039403915405273
			 train-loss:  2.1300632130783215 	 ± 0.2585983465643001
	data : 0.11566128730773925
	model : 0.07097320556640625
			 train-loss:  2.129419544286895 	 ± 0.25755254927472065
	data : 0.11534581184387208
	model : 0.07076377868652343
			 train-loss:  2.132036528380021 	 ± 0.25794814450264303
	data : 0.11556892395019532
	model : 0.07042503356933594
			 train-loss:  2.1314792324756753 	 ± 0.2569034154383721
	data : 0.1156693458557129
	model : 0.06937932968139648
			 train-loss:  2.128528762067485 	 ± 0.2577694312779276
	data : 0.11653380393981934
	model : 0.06915183067321777
			 train-loss:  2.128568243172209 	 ± 0.25667521925619324
	data : 0.11690034866333007
	model : 0.06900234222412109
			 train-loss:  2.1284506180707146 	 ± 0.25559767041826914
	data : 0.11688523292541504
	model : 0.06913175582885742
			 train-loss:  2.126116387049357 	 ± 0.2558009744861933
	data : 0.11674680709838867
	model : 0.06908125877380371
			 train-loss:  2.124179646003345 	 ± 0.25562370087508224
	data : 0.11685795783996582
	model : 0.06971225738525391
			 train-loss:  2.124438898485215 	 ± 0.2545898796434563
	data : 0.11616616249084473
	model : 0.06965761184692383
			 train-loss:  2.1227024270267023 	 ± 0.2542772464324215
	data : 0.11618666648864746
	model : 0.06981215476989747
			 train-loss:  2.120098607194039 	 ± 0.2548909889628106
	data : 0.11613602638244629
	model : 0.06997838020324706
			 train-loss:  2.1174674625396728 	 ± 0.25555450157693743
	data : 0.11597175598144531
	model : 0.07041349411010742
			 train-loss:  2.1159090910639082 	 ± 0.25513398480666283
	data : 0.11570973396301269
	model : 0.0705488681793213
			 train-loss:  2.114289753080353 	 ± 0.2547767813839427
	data : 0.1155813217163086
	model : 0.06960229873657227
			 train-loss:  2.1125514367595315 	 ± 0.2545345771372361
	data : 0.11637425422668457
	model : 0.06933913230895997
			 train-loss:  2.1125685290772784 	 ± 0.2535461633519836
	data : 0.11651272773742676
	model : 0.06822967529296875
			 train-loss:  2.1124233548457805 	 ± 0.2525744853384079
	data : 0.11746649742126465
	model : 0.06769881248474122
			 train-loss:  2.1126741498481225 	 ± 0.2516248622244861
	data : 0.11796989440917968
	model : 0.06745939254760742
			 train-loss:  2.111226970499212 	 ± 0.2512165771508864
	data : 0.1180877685546875
	model : 0.06834402084350585
			 train-loss:  2.1097705409042815 	 ± 0.25082913666991263
	data : 0.11728506088256836
	model : 0.06814970970153808
			 train-loss:  2.114004438492789 	 ± 0.25461712860153785
	data : 0.11764264106750488
	model : 0.06831789016723633
			 train-loss:  2.114051079750061 	 ± 0.2536729239243963
	data : 0.11733579635620117
	model : 0.06741743087768555
			 train-loss:  2.1143916363225266 	 ± 0.25276955523424044
	data : 0.1180117130279541
	model : 0.06734666824340821
			 train-loss:  2.1127353193116014 	 ± 0.2525849962307388
	data : 0.11812787055969239
	model : 0.06740794181823731
			 train-loss:  2.1145897939585256 	 ± 0.2526024957549114
	data : 0.11794929504394532
	model : 0.06788268089294433
			 train-loss:  2.1128789512373563 	 ± 0.25249335432938075
	data : 0.11751732826232911
	model : 0.06878843307495117
			 train-loss:  2.1146254854542867 	 ± 0.2524312168344325
	data : 0.11682462692260742
	model : 0.06986193656921387
			 train-loss:  2.113560461828895 	 ± 0.2518499396652035
	data : 0.11584038734436035
	model : 0.0700674057006836
			 train-loss:  2.1118246308514768 	 ± 0.25180659579601966
	data : 0.11571125984191895
	model : 0.07011408805847168
			 train-loss:  2.1103167742282363 	 ± 0.2515671179700804
	data : 0.11572046279907226
	model : 0.0702176570892334
			 train-loss:  2.112025952173604 	 ± 0.2515239014331973
	data : 0.11559967994689942
	model : 0.07011847496032715
			 train-loss:  2.1110340365048113 	 ± 0.25093753960922566
	data : 0.11584701538085937
	model : 0.06999573707580567
			 train-loss:  2.110750782979678 	 ± 0.2500999471051074
	data : 0.11607375144958496
	model : 0.06971426010131836
			 train-loss:  2.1128995434767535 	 ± 0.25059644952438775
	data : 0.11624951362609863
	model : 0.06961488723754883
			 train-loss:  2.1122921399168066 	 ± 0.2498569587313235
	data : 0.11628026962280273
	model : 0.06963701248168945
			 train-loss:  2.1122963284486094 	 ± 0.2490171062450061
	data : 0.11620154380798339
	model : 0.06881923675537109
			 train-loss:  2.1143126090367637 	 ± 0.24940301748789526
	data : 0.11695275306701661
	model : 0.06893839836120605
			 train-loss:  2.1126327222546206 	 ± 0.2494258109950149
	data : 0.11671242713928223
	model : 0.06936931610107422
			 train-loss:  2.111114352941513 	 ± 0.24930314847912008
	data : 0.11642332077026367
	model : 0.06926460266113281
			 train-loss:  2.1109790209851234 	 ± 0.24849269821968265
	data : 0.11651630401611328
	model : 0.06837229728698731
			 train-loss:  2.1119751945718543 	 ± 0.2479909017571101
	data : 0.11733880043029785
	model : 0.06917190551757812
			 train-loss:  2.1135786948665496 	 ± 0.24798928061549638
	data : 0.1166407585144043
	model : 0.0691293716430664
			 train-loss:  2.1139147724860754 	 ± 0.24722857427313627
	data : 0.1168818473815918
	model : 0.069305419921875
			 train-loss:  2.1149131158354937 	 ± 0.24675522201687658
	data : 0.11656684875488281
	model : 0.0699070930480957
			 train-loss:  2.1144968361794194 	 ± 0.2460284088363915
	data : 0.11609230041503907
	model : 0.07106823921203613
			 train-loss:  2.1122546420907073 	 ± 0.24686761815533362
	data : 0.11505193710327148
	model : 0.0705897331237793
			 train-loss:  2.112362156808376 	 ± 0.24609868185345896
	data : 0.11553640365600586
	model : 0.07061505317687988
			 train-loss:  2.110880296422828 	 ± 0.246048225849225
	data : 0.1156153678894043
	model : 0.06994609832763672
			 train-loss:  2.1100997196303473 	 ± 0.2454875245928223
	data : 0.1163339614868164
	model : 0.06856422424316407
			 train-loss:  2.1094185406444996 	 ± 0.24488686153696498
	data : 0.1175875186920166
	model : 0.06806073188781739
			 train-loss:  2.1089721561931984 	 ± 0.24420562246442648
	data : 0.1181783676147461
	model : 0.06847219467163086
			 train-loss:  2.1102603775082214 	 ± 0.24402277295821054
	data : 0.11751508712768555
	model : 0.06847763061523438
			 train-loss:  2.1096053963684174 	 ± 0.24343208748662984
	data : 0.1174426555633545
	model : 0.06890625953674316
			 train-loss:  2.109626343864167 	 ± 0.2427023046078892
	data : 0.1169853687286377
	model : 0.06948542594909668
			 train-loss:  2.107250316512017 	 ± 0.24391922162921717
	data : 0.11651530265808105
	model : 0.06957764625549316
			 train-loss:  2.108129183216208 	 ± 0.24346313929281385
	data : 0.11625957489013672
	model : 0.06954154968261719
			 train-loss:  2.109109609968522 	 ± 0.24308039169880058
	data : 0.11641979217529297
	model : 0.06930432319641114
			 train-loss:  2.109328828359905 	 ± 0.24238544130998163
	data : 0.11653413772583007
	model : 0.06947016716003418
			 train-loss:  2.1084028593329496 	 ± 0.24198294753250965
	data : 0.11630816459655761
	model : 0.06885838508605957
			 train-loss:  2.1067394679681413 	 ± 0.24226674586971506
	data : 0.11678605079650879
	model : 0.06861844062805175
			 train-loss:  2.105102269813932 	 ± 0.24252746510178858
	data : 0.11716742515563965
	model : 0.06855831146240235
			 train-loss:  2.1050484418869018 	 ± 0.24183457909920994
	data : 0.11730241775512695
	model : 0.06875042915344239
			 train-loss:  2.107013526965271 	 ± 0.24254368944379243
	data : 0.11727824211120605
	model : 0.06863913536071778
			 train-loss:  2.1061612300280124 	 ± 0.24212172772354643
	data : 0.11742610931396484
	model : 0.06957917213439942
			 train-loss:  2.10587913735529 	 ± 0.24146981949613847
	data : 0.11660876274108886
	model : 0.0699185848236084
			 train-loss:  2.105180168951024 	 ± 0.24097488593058455
	data : 0.11634397506713867
	model : 0.07011985778808594
			 train-loss:  2.1046825832790796 	 ± 0.24039677525311026
	data : 0.1161238670349121
	model : 0.07013201713562012
			 train-loss:  2.102522915898107 	 ± 0.2414764456252542
	data : 0.11617493629455566
	model : 0.07012019157409669
			 train-loss:  2.1021846270823215 	 ± 0.24085513881337495
	data : 0.1163832664489746
	model : 0.07010316848754883
			 train-loss:  2.1003459906968915 	 ± 0.24147352454327853
	data : 0.11639790534973145
	model : 0.0699997901916504
			 train-loss:  2.100630725207536 	 ± 0.2408472551106235
	data : 0.11621084213256835
	model : 0.06920933723449707
			 train-loss:  2.1016828085925128 	 ± 0.24061901960947105
	data : 0.1169166088104248
	model : 0.06909747123718261
			 train-loss:  2.1022648888249553 	 ± 0.24010188851155956
	data : 0.11690773963928222
	model : 0.06893754005432129
			 train-loss:  2.1013947132436988 	 ± 0.23975294392305577
	data : 0.11683526039123535
	model : 0.06803898811340332
			 train-loss:  2.101518804722644 	 ± 0.2391204741419076
	data : 0.11759700775146484
	model : 0.0679811954498291
			 train-loss:  2.1014723071345576 	 ± 0.23848789347353588
	data : 0.11776981353759766
	model : 0.06867742538452148
			 train-loss:  2.10065501112687 	 ± 0.23812469908789202
	data : 0.11708192825317383
	model : 0.06849579811096192
			 train-loss:  2.102044844502554 	 ± 0.23827191822613938
	data : 0.1172020435333252
	model : 0.0679133415222168
			 train-loss:  2.1029648880163827 	 ± 0.23799052367476412
	data : 0.11788225173950195
	model : 0.06862354278564453
			 train-loss:  2.102522196547355 	 ± 0.23745241174530557
	data : 0.11725564002990722
	model : 0.0679253101348877
			 train-loss:  2.102141869436834 	 ± 0.2368985599045838
	data : 0.11794581413269042
	model : 0.06786251068115234
			 train-loss:  2.1008613898203925 	 ± 0.236962477825179
	data : 0.11802434921264648
	model : 0.06727437973022461
			 train-loss:  2.1002365247327455 	 ± 0.23651822148688365
	data : 0.11875863075256347
	model : 0.0681074619293213
			 train-loss:  2.0995914173610317 	 ± 0.23608996874693322
	data : 0.11803908348083496
	model : 0.06875777244567871
			 train-loss:  2.1000549251383003 	 ± 0.2355828713538844
	data : 0.1176058292388916
	model : 0.06866579055786133
			 train-loss:  2.099968790408954 	 ± 0.23499333474461878
	data : 0.11760926246643066
	model : 0.06889147758483886
			 train-loss:  2.09972421169281 	 ± 0.23443050562683646
	data : 0.11753625869750976
	model : 0.06980834007263184
			 train-loss:  2.0991055698537116 	 ± 0.23401022266056606
	data : 0.11648569107055665
	model : 0.0698430061340332
			 train-loss:  2.0987754523163975 	 ± 0.23347718478106735
	data : 0.11655049324035645
	model : 0.06942930221557617
			 train-loss:  2.098926910038652 	 ± 0.23291135556658704
	data : 0.11684412956237793
	model : 0.07014217376708984
			 train-loss:  2.097827130088619 	 ± 0.23286758295550072
	data : 0.1162287712097168
	model : 0.07000603675842285
			 train-loss:  2.1002646882359572 	 ± 0.2348933639452756
	data : 0.11624336242675781
	model : 0.06979584693908691
			 train-loss:  2.0979060906808353 	 ± 0.2367434572262756
	data : 0.1164466381072998
	model : 0.06968779563903808
			 train-loss:  2.0978768056141583 	 ± 0.2361712948474222
	data : 0.11633825302124023
	model : 0.06959218978881836
			 train-loss:  2.0994619062313666 	 ± 0.23670407480995156
	data : 0.1164370059967041
	model : 0.06971211433410644
			 train-loss:  2.099214731791373 	 ± 0.23616402427166397
	data : 0.11620001792907715
	model : 0.0697606086730957
			 train-loss:  2.0986241896947226 	 ± 0.23575568958918303
	data : 0.11617403030395508
	model : 0.06983962059020996
			 train-loss:  2.0997435052247972 	 ± 0.23575502421687564
	data : 0.11612672805786133
	model : 0.06983737945556641
			 train-loss:  2.0976536555110283 	 ± 0.23714931193740293
	data : 0.11608381271362304
	model : 0.0698862075805664
			 train-loss:  2.0981545728137236 	 ± 0.2367043603258859
	data : 0.11593356132507324
	model : 0.06955657005310059
			 train-loss:  2.098640220187535 	 ± 0.23625700713942596
	data : 0.11636915206909179
	model : 0.06961750984191895
			 train-loss:  2.0977881708810497 	 ± 0.23603626663834437
	data : 0.11632218360900878
	model : 0.06975002288818359
			 train-loss:  2.097310256074976 	 ± 0.23559349441804742
	data : 0.11633720397949218
	model : 0.06973533630371094
			 train-loss:  2.0979732137671263 	 ± 0.2352518846669359
	data : 0.11635317802429199
	model : 0.06975336074829101
			 train-loss:  2.097372671332928 	 ± 0.23487835472825402
	data : 0.11647248268127441
	model : 0.06974949836730956
			 train-loss:  2.098079474004981 	 ± 0.23457374071165812
	data : 0.11645407676696777
	model : 0.06960530281066894
			 train-loss:  2.0979012223807247 	 ± 0.23405487679310336
	data : 0.11643190383911133
	model : 0.06856274604797363
			 train-loss:  2.0987479692131146 	 ± 0.23386222453624717
	data : 0.1171633243560791
	model : 0.06827259063720703
			 train-loss:  2.099019999439652 	 ± 0.2333699549526844
	data : 0.11745634078979492
	model : 0.06816205978393555
			 train-loss:  2.0979225988345296 	 ± 0.23341950581777504
	data : 0.11735920906066895
	model : 0.0676279067993164
			 train-loss:  2.100548719721181 	 ± 0.23617652828916033
	data : 0.11766185760498046
	model : 0.06674337387084961
			 train-loss:  2.0993628533681234 	 ± 0.23631853668081182
	data : 0.11847376823425293
	model : 0.06747274398803711
			 train-loss:  2.0976254866186497 	 ± 0.23723088398153935
	data : 0.11764497756958008
	model : 0.06749186515808106
			 train-loss:  2.0967506747938986 	 ± 0.2370728290541143
	data : 0.11772503852844238
	model : 0.0670633316040039
			 train-loss:  2.0969751743893874 	 ± 0.23657654236430664
	data : 0.11827173233032226
	model : 0.06767497062683106
			 train-loss:  2.0975985428131305 	 ± 0.23624702061099162
	data : 0.1176844596862793
	model : 0.06819038391113282
			 train-loss:  2.098555744730908 	 ± 0.2361774946166261
	data : 0.11704559326171875
	model : 0.06742491722106933
			 train-loss:  2.097693356600675 	 ± 0.2360283711441516
	data : 0.11760444641113281
	model : 0.06718244552612304
			 train-loss:  2.0973422496483245 	 ± 0.23557958755507746
	data : 0.11786308288574218
	model : 0.06755461692810058
			 train-loss:  2.0975265922464525 	 ± 0.2350902767178847
	data : 0.11742706298828125
	model : 0.06778745651245117
			 train-loss:  2.0969773057179575 	 ± 0.23473719843643687
	data : 0.11760058403015136
	model : 0.0674403190612793
			 train-loss:  2.0966481416783433 	 ± 0.23429133850150377
	data : 0.11817741394042969
	model : 0.06836934089660644
			 train-loss:  2.096372043177233 	 ± 0.23383274019863526
	data : 0.11759204864501953
	model : 0.06882753372192382
			 train-loss:  2.094919137813874 	 ± 0.2344039739510527
	data : 0.11717061996459961
	model : 0.0688206672668457
			 train-loss:  2.0939731587882804 	 ± 0.23436391874521287
	data : 0.1171407699584961
	model : 0.06868820190429688
			 train-loss:  2.093984755512062 	 ± 0.23387317214550515
	data : 0.1171785831451416
	model : 0.06843352317810059
			 train-loss:  2.094039805730184 	 ± 0.23338697948371584
	data : 0.1173316478729248
	model : 0.06748037338256836
			 train-loss:  2.092178696418699 	 ± 0.23468012790228285
	data : 0.11801705360412598
	model : 0.06673631668090821
			 train-loss:  2.0911887337353603 	 ± 0.23469845986469487
	data : 0.11852812767028809
	model : 0.0660670280456543
			 train-loss:  2.091268486937378 	 ± 0.23421832933971995
	data : 0.11904983520507813
	model : 0.06586747169494629
			 train-loss:  2.0900594269643062 	 ± 0.23449652604446822
	data : 0.11904621124267578
	model : 0.06667866706848144
			 train-loss:  2.0902088428030208 	 ± 0.2340291107953788
	data : 0.11829462051391601
	model : 0.06750297546386719
			 train-loss:  2.0901598058095794 	 ± 0.23355421872107515
	data : 0.11780147552490235
	model : 0.06798624992370605
			 train-loss:  2.088412562362578 	 ± 0.2346864635693324
	data : 0.11725659370422363
	model : 0.06852703094482422
			 train-loss:  2.0888055721598286 	 ± 0.23429425802153836
	data : 0.11682238578796386
	model : 0.06863517761230468
			 train-loss:  2.0915221320577415 	 ± 0.23770466411236643
	data : 0.11674513816833496
	model : 0.06796450614929199
			 train-loss:  2.0904290413856508 	 ± 0.23785501943105053
	data : 0.11750812530517578
	model : 0.06778473854064941
			 train-loss:  2.089901527560565 	 ± 0.2375272185038477
	data : 0.11756134033203125
	model : 0.06824965476989746
			 train-loss:  2.089650596418078 	 ± 0.23708879870867802
	data : 0.1174849033355713
	model : 0.06827750205993652
			 train-loss:  2.090383670546792 	 ± 0.23690577121124196
	data : 0.11746635437011718
	model : 0.06844282150268555
			 train-loss:  2.089340297255929 	 ± 0.2370206853562143
	data : 0.11741504669189454
	model : 0.06893248558044433
			 train-loss:  2.0881637914507998 	 ± 0.23729743864484337
	data : 0.11687445640563965
	model : 0.06920452117919922
			 train-loss:  2.088131773751229 	 ± 0.23683406547256727
	data : 0.11558356285095214
	model : 0.05994977951049805
#epoch  96    val-loss:  2.4648835533543636  train-loss:  2.088131773751229  lr:  6.103515625e-07
			 train-loss:  2.032010316848755 	 ± 0.0
	data : 5.418058156967163
	model : 0.09069395065307617
			 train-loss:  1.9144166111946106 	 ± 0.11759370565414429
	data : 2.957898259162903
	model : 0.07889091968536377
			 train-loss:  1.9240092436472576 	 ± 0.09696850142043806
	data : 2.011311928431193
	model : 0.0755005677541097
			 train-loss:  1.9314737617969513 	 ± 0.08496660987950178
	data : 1.5376269817352295
	model : 0.07397717237472534
			 train-loss:  1.8874425649642945 	 ± 0.11632044108236972
	data : 1.2531996726989747
	model : 0.0731435775756836
			 train-loss:  1.9177212119102478 	 ± 0.12593392374198725
	data : 0.19274120330810546
	model : 0.0688939094543457
			 train-loss:  1.9604281868253435 	 ± 0.15664305126308115
	data : 0.11644134521484376
	model : 0.06920332908630371
			 train-loss:  1.9389661252498627 	 ± 0.15714405948005275
	data : 0.11630239486694335
	model : 0.06953558921813965
			 train-loss:  1.94313473171658 	 ± 0.1486252586540751
	data : 0.11631708145141602
	model : 0.07012715339660644
			 train-loss:  1.964967179298401 	 ± 0.15546839764308473
	data : 0.11584854125976562
	model : 0.07055253982543945
			 train-loss:  1.9295792796395042 	 ± 0.18573138113135781
	data : 0.11540355682373046
	model : 0.07076773643493653
			 train-loss:  1.9129520952701569 	 ± 0.1861788467293116
	data : 0.1151576042175293
	model : 0.07012224197387695
			 train-loss:  1.925013386286222 	 ± 0.183689708104446
	data : 0.11563363075256347
	model : 0.07002553939819336
			 train-loss:  1.9340853435652596 	 ± 0.18000466058257727
	data : 0.11553125381469727
	model : 0.06905541419982911
			 train-loss:  1.9401901006698608 	 ± 0.17539475136475843
	data : 0.11664552688598633
	model : 0.06844649314880372
			 train-loss:  1.9437044486403465 	 ± 0.1703698066142478
	data : 0.11728987693786622
	model : 0.06813640594482422
			 train-loss:  1.936197694610147 	 ± 0.1679883583052246
	data : 0.11747245788574219
	model : 0.06894154548645019
			 train-loss:  1.9504451155662537 	 ± 0.17350250400642522
	data : 0.11690964698791503
	model : 0.06888160705566407
			 train-loss:  1.9530828814757497 	 ± 0.16924533901888067
	data : 0.11680827140808106
	model : 0.06936688423156738
			 train-loss:  1.956176573038101 	 ± 0.1655102207200854
	data : 0.11629576683044433
	model : 0.06973462104797364
			 train-loss:  1.974152786391122 	 ± 0.18042189271548845
	data : 0.1161189079284668
	model : 0.06983838081359864
			 train-loss:  1.9774151877923445 	 ± 0.17690655394777188
	data : 0.11612272262573242
	model : 0.06992635726928711
			 train-loss:  1.969496576682381 	 ± 0.1769596934725599
	data : 0.11578044891357422
	model : 0.06993799209594727
			 train-loss:  1.9674950291713078 	 ± 0.17349955329506953
	data : 0.11590285301208496
	model : 0.06989755630493164
			 train-loss:  1.9880439615249634 	 ± 0.19756573107411052
	data : 0.11586308479309082
	model : 0.06969423294067383
			 train-loss:  2.0014075361765347 	 ± 0.20492829751905098
	data : 0.11593255996704102
	model : 0.06963038444519043
			 train-loss:  2.0048756820184215 	 ± 0.2018735822060568
	data : 0.11607060432434083
	model : 0.06952090263366699
			 train-loss:  2.0218015355723247 	 ± 0.21686992169520422
	data : 0.11632447242736817
	model : 0.06865205764770507
			 train-loss:  2.023896550310069 	 ± 0.21338614024834163
	data : 0.11688117980957032
	model : 0.06842823028564453
			 train-loss:  2.029499876499176 	 ± 0.21195843308413048
	data : 0.11722512245178222
	model : 0.0676910400390625
			 train-loss:  2.037005566781567 	 ± 0.21252576543733737
	data : 0.11790366172790527
	model : 0.06771993637084961
			 train-loss:  2.042846452444792 	 ± 0.21169156867840241
	data : 0.11789026260375976
	model : 0.06740446090698242
			 train-loss:  2.0476937546874536 	 ± 0.21025513952585553
	data : 0.11829080581665039
	model : 0.06817741394042968
			 train-loss:  2.05129370619269 	 ± 0.2081698338210441
	data : 0.11773409843444824
	model : 0.06856017112731934
			 train-loss:  2.068352975164141 	 ± 0.2280157458895413
	data : 0.11717023849487304
	model : 0.06941485404968262
			 train-loss:  2.074933717648188 	 ± 0.22817251571166436
	data : 0.11633620262145997
	model : 0.06947202682495117
			 train-loss:  2.090852766423612 	 ± 0.24449657893349186
	data : 0.11629014015197754
	model : 0.06960554122924804
			 train-loss:  2.09198805846666 	 ± 0.241356884129632
	data : 0.11603455543518067
	model : 0.06967568397521973
			 train-loss:  2.0918254943994374 	 ± 0.23824457884238423
	data : 0.11616549491882325
	model : 0.06928486824035644
			 train-loss:  2.096553859114647 	 ± 0.23709366766409398
	data : 0.11662893295288086
	model : 0.06946744918823242
			 train-loss:  2.096912008960073 	 ± 0.23419538719956862
	data : 0.11651711463928223
	model : 0.06986656188964843
			 train-loss:  2.0906389015061513 	 ± 0.23485104637088416
	data : 0.11616597175598145
	model : 0.07032337188720703
			 train-loss:  2.0881975362467213 	 ± 0.2326427955892927
	data : 0.11575374603271485
	model : 0.07051215171813965
			 train-loss:  2.0825816094875336 	 ± 0.2329136554278024
	data : 0.11554465293884278
	model : 0.0707676887512207
			 train-loss:  2.095621297094557 	 ± 0.24601771190295105
	data : 0.11537222862243653
	model : 0.0704237937927246
			 train-loss:  2.0954784491787786 	 ± 0.24333079984686673
	data : 0.11556377410888671
	model : 0.07001724243164062
			 train-loss:  2.08931110767608 	 ± 0.24433533139568825
	data : 0.11593375205993653
	model : 0.06973710060119628
			 train-loss:  2.0908877551555634 	 ± 0.24201826935510504
	data : 0.11620869636535644
	model : 0.06967015266418457
			 train-loss:  2.0863212833599167 	 ± 0.24161623800682952
	data : 0.11620874404907226
	model : 0.06975021362304687
			 train-loss:  2.095941894054413 	 ± 0.2484876047126273
	data : 0.11622204780578613
	model : 0.06931929588317871
			 train-loss:  2.095225308455673 	 ± 0.24609156182352232
	data : 0.11666598320007324
	model : 0.06888089179992676
			 train-loss:  2.0970332278655124 	 ± 0.24405556395060976
	data : 0.11710805892944336
	model : 0.0687563419342041
			 train-loss:  2.0991493868377975 	 ± 0.24222334439928803
	data : 0.11714315414428711
	model : 0.06847844123840333
			 train-loss:  2.094140962318138 	 ± 0.2427243211116027
	data : 0.11745100021362305
	model : 0.06882591247558593
			 train-loss:  2.092000529982827 	 ± 0.2410213908054804
	data : 0.11725454330444336
	model : 0.06931171417236329
			 train-loss:  2.0889951480286464 	 ± 0.23989736060993105
	data : 0.11671972274780273
	model : 0.06982016563415527
			 train-loss:  2.0853572749255 	 ± 0.23933698464450348
	data : 0.11612210273742676
	model : 0.07021441459655761
			 train-loss:  2.082941632846306 	 ± 0.23796466509604763
	data : 0.11579432487487792
	model : 0.07036466598510742
			 train-loss:  2.080605228068465 	 ± 0.2366094012373634
	data : 0.11551437377929688
	model : 0.06990866661071778
			 train-loss:  2.0862338463465373 	 ± 0.23857942329721843
	data : 0.11563229560852051
	model : 0.06994953155517578
			 train-loss:  2.0879641517263945 	 ± 0.2369950665184681
	data : 0.11581106185913086
	model : 0.06961708068847657
			 train-loss:  2.0836901933916154 	 ± 0.23743423840876957
	data : 0.1161895751953125
	model : 0.0691251277923584
			 train-loss:  2.0832333905356273 	 ± 0.23556976352812106
	data : 0.11656904220581055
	model : 0.06967949867248535
			 train-loss:  2.083089917898178 	 ± 0.2337249032676854
	data : 0.11621851921081543
	model : 0.06984071731567383
			 train-loss:  2.08470585162823 	 ± 0.23228006676457683
	data : 0.11610984802246094
	model : 0.06894745826721191
			 train-loss:  2.08097695943081 	 ± 0.2324657943188003
	data : 0.11694378852844238
	model : 0.06926083564758301
			 train-loss:  2.0815296244265427 	 ± 0.23076813504985655
	data : 0.11688942909240722
	model : 0.06965079307556152
			 train-loss:  2.0809517537846283 	 ± 0.22911385751056954
	data : 0.1166264533996582
	model : 0.06883392333984376
			 train-loss:  2.0899965348451035 	 ± 0.239364445259234
	data : 0.11730780601501464
	model : 0.06964073181152344
			 train-loss:  2.0874245830944607 	 ± 0.2386069223667983
	data : 0.11644673347473145
	model : 0.07048287391662597
			 train-loss:  2.0871866639231293 	 ± 0.2369289953536429
	data : 0.11570611000061035
	model : 0.07054243087768555
			 train-loss:  2.0808616793817944 	 ± 0.24123863021360303
	data : 0.11542730331420899
	model : 0.07088646888732911
			 train-loss:  2.0915769175307393 	 ± 0.25625308941645425
	data : 0.11519474983215332
	model : 0.07159590721130371
			 train-loss:  2.08978516991074 	 ± 0.25497574098151243
	data : 0.11453027725219726
	model : 0.0700963020324707
			 train-loss:  2.0937197907765706 	 ± 0.2555218294060796
	data : 0.11594548225402831
	model : 0.07017335891723633
			 train-loss:  2.0986144511323226 	 ± 0.2573502192823908
	data : 0.1158137321472168
	model : 0.06929001808166504
			 train-loss:  2.0978107018904253 	 ± 0.2557696496844913
	data : 0.11682686805725098
	model : 0.06797690391540527
			 train-loss:  2.092882732550303 	 ± 0.2577777256363889
	data : 0.11789283752441407
	model : 0.0675121784210205
			 train-loss:  2.0917167557945735 	 ± 0.2563479391135454
	data : 0.1182934284210205
	model : 0.06728572845458984
			 train-loss:  2.089796578884125 	 ± 0.2553118031627037
	data : 0.11859331130981446
	model : 0.06667160987854004
			 train-loss:  2.0960270593195784 	 ± 0.25977853176807897
	data : 0.11907873153686524
	model : 0.06728901863098144
			 train-loss:  2.0924729530404256 	 ± 0.26016353423403316
	data : 0.11842617988586426
	model : 0.0675684928894043
			 train-loss:  2.0906914702380996 	 ± 0.2590942370639075
	data : 0.11829838752746583
	model : 0.06777000427246094
			 train-loss:  2.0859287480513253 	 ± 0.2611769352187687
	data : 0.11813230514526367
	model : 0.06842470169067383
			 train-loss:  2.081661617054659 	 ± 0.2625650151587908
	data : 0.11750860214233398
	model : 0.06914963722229003
			 train-loss:  2.0840711787689563 	 ± 0.2619776015747911
	data : 0.11691536903381347
	model : 0.06931757926940918
			 train-loss:  2.083272972326169 	 ± 0.2605727933140653
	data : 0.11674156188964843
	model : 0.07023310661315918
			 train-loss:  2.0856313813816416 	 ± 0.2600202155525085
	data : 0.11594696044921875
	model : 0.07014307975769044
			 train-loss:  2.087041431598449 	 ± 0.25889343189480063
	data : 0.11593470573425294
	model : 0.07025685310363769
			 train-loss:  2.089407632086012 	 ± 0.2584170667521799
	data : 0.11589574813842773
	model : 0.07017297744750976
			 train-loss:  2.086847404857258 	 ± 0.2581384701192859
	data : 0.11596112251281739
	model : 0.07043728828430176
			 train-loss:  2.086762003276659 	 ± 0.25673300308560537
	data : 0.11568551063537598
	model : 0.07003283500671387
			 train-loss:  2.087105363927862 	 ± 0.25537022525638764
	data : 0.11596412658691406
	model : 0.07003922462463379
			 train-loss:  2.08780777708013 	 ± 0.25409854630682327
	data : 0.11603312492370606
	model : 0.06990866661071778
			 train-loss:  2.0865830145384137 	 ± 0.2530364251422917
	data : 0.11615476608276368
	model : 0.06895179748535156
			 train-loss:  2.0902812654773393 	 ± 0.25428291473895626
	data : 0.11712818145751953
	model : 0.06879925727844238
			 train-loss:  2.088262891032032 	 ± 0.25374060188090736
	data : 0.11717028617858886
	model : 0.06808333396911621
			 train-loss:  2.091968656802664 	 ± 0.25506740747428547
	data : 0.11789951324462891
	model : 0.06789002418518067
			 train-loss:  2.090622422671077 	 ± 0.25412561228326447
	data : 0.11814022064208984
	model : 0.06802511215209961
			 train-loss:  2.0853779554367065 	 ± 0.2581801072329683
	data : 0.11798157691955566
	model : 0.06890702247619629
			 train-loss:  2.087841992331023 	 ± 0.25807778977384593
	data : 0.11715612411499024
	model : 0.06793794631958008
			 train-loss:  2.0954393732781504 	 ± 0.2679195843495354
	data : 0.11805472373962403
	model : 0.06863670349121094
			 train-loss:  2.090660819729555 	 ± 0.27094857122761173
	data : 0.11748709678649902
	model : 0.06869826316833497
			 train-loss:  2.0908428751505337 	 ± 0.2696491176260342
	data : 0.11744422912597656
	model : 0.06897006034851075
			 train-loss:  2.089778959183466 	 ± 0.26858124222443003
	data : 0.11717944145202637
	model : 0.06877303123474121
			 train-loss:  2.087109440902494 	 ± 0.2687073145540423
	data : 0.11723208427429199
	model : 0.06938552856445312
			 train-loss:  2.0853394960688654 	 ± 0.2680688109408842
	data : 0.11672968864440918
	model : 0.07026982307434082
			 train-loss:  2.0877168940173254 	 ± 0.26795573339602385
	data : 0.115901517868042
	model : 0.07042584419250489
			 train-loss:  2.0898332869240996 	 ± 0.26762903763429235
	data : 0.11571059226989747
	model : 0.0692178726196289
			 train-loss:  2.0906327626921914 	 ± 0.26654048713437717
	data : 0.11673707962036133
	model : 0.06952872276306152
			 train-loss:  2.0888545609809257 	 ± 0.26599176098449523
	data : 0.11640605926513672
	model : 0.06977910995483398
			 train-loss:  2.091172877166952 	 ± 0.2659257143066689
	data : 0.11628365516662598
	model : 0.06898765563964844
			 train-loss:  2.094557626057515 	 ± 0.2671587668950782
	data : 0.11691932678222657
	model : 0.06892566680908203
			 train-loss:  2.092048245563842 	 ± 0.26731868738861164
	data : 0.11703715324401856
	model : 0.06977882385253906
			 train-loss:  2.0925511360168456 	 ± 0.2662080502958517
	data : 0.11639928817749023
	model : 0.06879258155822754
			 train-loss:  2.093729292524272 	 ± 0.2653590625118515
	data : 0.11740779876708984
	model : 0.0686643123626709
			 train-loss:  2.0916321481394973 	 ± 0.2651862749987705
	data : 0.11733574867248535
	model : 0.06876082420349121
			 train-loss:  2.089190083034968 	 ± 0.2653781211162282
	data : 0.11731815338134766
	model : 0.06876311302185059
			 train-loss:  2.08617040489902 	 ± 0.2662887786965947
	data : 0.11719002723693847
	model : 0.06879668235778809
			 train-loss:  2.085642510652542 	 ± 0.2652394416129445
	data : 0.1171731948852539
	model : 0.06933107376098632
			 train-loss:  2.083996428931055 	 ± 0.26475590724800735
	data : 0.11680707931518555
	model : 0.06912970542907715
			 train-loss:  2.0807354108231966 	 ± 0.2660974981360373
	data : 0.11717290878295898
	model : 0.06920347213745118
			 train-loss:  2.0800507388463836 	 ± 0.2651214728054501
	data : 0.1171149730682373
	model : 0.0690953254699707
			 train-loss:  2.0829879016645494 	 ± 0.26605198215161346
	data : 0.11734972000122071
	model : 0.069523286819458
			 train-loss:  2.0829540185928344 	 ± 0.2649859058684142
	data : 0.11702408790588378
	model : 0.06991987228393555
			 train-loss:  2.0796356522847734 	 ± 0.266527095604996
	data : 0.11662077903747559
	model : 0.07041935920715332
			 train-loss:  2.079032545014629 	 ± 0.26556200724215917
	data : 0.11625709533691406
	model : 0.07035279273986816
			 train-loss:  2.080544363707304 	 ± 0.2650707226583348
	data : 0.11613683700561524
	model : 0.07047452926635742
			 train-loss:  2.082221787105235 	 ± 0.2647224536484878
	data : 0.1159325122833252
	model : 0.07016057968139648
			 train-loss:  2.0816033381682177 	 ± 0.26379586028379637
	data : 0.11613192558288574
	model : 0.07088747024536132
			 train-loss:  2.079024364930073 	 ± 0.2644271000252048
	data : 0.11523880958557128
	model : 0.0707554817199707
			 train-loss:  2.078834767594482 	 ± 0.26343251606875945
	data : 0.11535148620605469
	model : 0.07075819969177247
			 train-loss:  2.07865650044348 	 ± 0.26244829156561594
	data : 0.1154282569885254
	model : 0.07076082229614258
			 train-loss:  2.074493141316656 	 ± 0.2658391354646995
	data : 0.11554093360900879
	model : 0.07086968421936035
			 train-loss:  2.0714904025748924 	 ± 0.2671238740983598
	data : 0.11541485786437988
	model : 0.07009615898132324
			 train-loss:  2.070482263670248 	 ± 0.26639763556729723
	data : 0.11629781723022461
	model : 0.06994633674621582
			 train-loss:  2.0718252789365112 	 ± 0.26588529418924706
	data : 0.11618461608886718
	model : 0.06892342567443847
			 train-loss:  2.0720460440801536 	 ± 0.26493279173283707
	data : 0.11701631546020508
	model : 0.0686650276184082
			 train-loss:  2.0728410739692853 	 ± 0.2641432386900245
	data : 0.11718792915344238
	model : 0.06851887702941895
			 train-loss:  2.0716931206839426 	 ± 0.263545926473522
	data : 0.11740627288818359
	model : 0.06858878135681153
			 train-loss:  2.070901441235914 	 ± 0.26277671562636085
	data : 0.11731681823730469
	model : 0.06888742446899414
			 train-loss:  2.0716214448633328 	 ± 0.261989348109092
	data : 0.11728935241699219
	model : 0.06983413696289062
			 train-loss:  2.072070917049488 	 ± 0.2611266305202504
	data : 0.11642389297485352
	model : 0.07003197669982911
			 train-loss:  2.069929256207413 	 ± 0.26147561113056317
	data : 0.11621975898742676
	model : 0.06998286247253419
			 train-loss:  2.0710999973889055 	 ± 0.2609508629677482
	data : 0.11626372337341309
	model : 0.06903510093688965
			 train-loss:  2.0740575651599937 	 ± 0.262482935425327
	data : 0.11707625389099122
	model : 0.06871957778930664
			 train-loss:  2.0766376581321766 	 ± 0.2634397592908309
	data : 0.11735844612121582
	model : 0.06866621971130371
			 train-loss:  2.076038192252855 	 ± 0.2626488344828212
	data : 0.117439603805542
	model : 0.0687169075012207
			 train-loss:  2.0758364400607627 	 ± 0.26177748531946016
	data : 0.117484712600708
	model : 0.06883120536804199
			 train-loss:  2.07421760002772 	 ± 0.261650679258078
	data : 0.1172935962677002
	model : 0.06988039016723632
			 train-loss:  2.078697000118281 	 ± 0.2664909887964026
	data : 0.11638178825378417
	model : 0.06949682235717773
			 train-loss:  2.0803853359661604 	 ± 0.26642193834429373
	data : 0.11657514572143554
	model : 0.06946682929992676
			 train-loss:  2.08234671595829 	 ± 0.26664858740135555
	data : 0.1165428638458252
	model : 0.06929249763488769
			 train-loss:  2.0830055421048943 	 ± 0.2659063392452717
	data : 0.11665868759155273
	model : 0.06830410957336426
			 train-loss:  2.083449258342866 	 ± 0.2651043803691316
	data : 0.11767683029174805
	model : 0.0677039623260498
			 train-loss:  2.0830932572866097 	 ± 0.2642904874103589
	data : 0.11808300018310547
	model : 0.06827993392944336
			 train-loss:  2.081412861301641 	 ± 0.2642821649097508
	data : 0.11757316589355468
	model : 0.06846213340759277
			 train-loss:  2.0803623931317388 	 ± 0.2637731074628714
	data : 0.1175142765045166
	model : 0.06878933906555176
			 train-loss:  2.080053448677063 	 ± 0.2629709989779972
	data : 0.11723246574401855
	model : 0.06958589553833008
			 train-loss:  2.0828931353986264 	 ± 0.26458209099761854
	data : 0.11648797988891602
	model : 0.06987671852111817
			 train-loss:  2.0831594326481317 	 ± 0.2637806355079559
	data : 0.11627554893493652
	model : 0.06987967491149902
			 train-loss:  2.084806220766939 	 ± 0.26379411252697854
	data : 0.11635198593139648
	model : 0.069793701171875
			 train-loss:  2.0848235301444866 	 ± 0.2629837755131434
	data : 0.1163442611694336
	model : 0.06947836875915528
			 train-loss:  2.082771624006876 	 ± 0.2634863146113987
	data : 0.11662263870239258
	model : 0.06964473724365235
			 train-loss:  2.0843801498413086 	 ± 0.26349308617221473
	data : 0.11637969017028808
	model : 0.07016849517822266
			 train-loss:  2.0852281242968087 	 ± 0.26292395641147026
	data : 0.11613698005676269
	model : 0.06980509757995605
			 train-loss:  2.084844435046533 	 ± 0.262182186277781
	data : 0.11628909111022949
	model : 0.06890568733215333
			 train-loss:  2.0897562134833563 	 ± 0.2689968491933194
	data : 0.11712355613708496
	model : 0.06911234855651856
			 train-loss:  2.0912559667282555 	 ± 0.2689033635979265
	data : 0.11682395935058594
	model : 0.0689962387084961
			 train-loss:  2.0930920600891114 	 ± 0.2691717131241541
	data : 0.11704702377319336
	model : 0.06850762367248535
			 train-loss:  2.0944133446230526 	 ± 0.26893585065360803
	data : 0.11734595298767089
	model : 0.06786990165710449
			 train-loss:  2.095772141634032 	 ± 0.2687409742128756
	data : 0.11801600456237793
	model : 0.0688936710357666
			 train-loss:  2.0941691405511316 	 ± 0.2687865659350973
	data : 0.1171755313873291
	model : 0.06897416114807128
			 train-loss:  2.096139361803559 	 ± 0.2692629874233541
	data : 0.11723651885986328
	model : 0.06891460418701172
			 train-loss:  2.0943460648400443 	 ± 0.26953260491037584
	data : 0.1170691967010498
	model : 0.06810760498046875
			 train-loss:  2.095957777039571 	 ± 0.26961015599131777
	data : 0.11787528991699218
	model : 0.06915445327758789
			 train-loss:  2.094046849988948 	 ± 0.2700400883735309
	data : 0.11712384223937988
	model : 0.0691798210144043
			 train-loss:  2.0939880649695235 	 ± 0.26928161615052754
	data : 0.11722798347473144
	model : 0.06923332214355468
			 train-loss:  2.093193137445929 	 ± 0.26873773548548957
	data : 0.11733732223510743
	model : 0.0693502426147461
			 train-loss:  2.0914850930372872 	 ± 0.2689627591811172
	data : 0.11737689971923829
	model : 0.07034363746643066
			 train-loss:  2.093524310470286 	 ± 0.2696104697058397
	data : 0.1166778564453125
	model : 0.06938104629516602
			 train-loss:  2.09265490089144 	 ± 0.2691230645185094
	data : 0.11720051765441894
	model : 0.06923842430114746
			 train-loss:  2.092251423929558 	 ± 0.2684419399464058
	data : 0.11723642349243164
	model : 0.06912884712219239
			 train-loss:  2.0908138719589813 	 ± 0.26841687452843915
	data : 0.11712799072265626
	model : 0.06915326118469238
			 train-loss:  2.0879063090762577 	 ± 0.2705802904457107
	data : 0.11707673072814942
	model : 0.06899118423461914
			 train-loss:  2.087161291671056 	 ± 0.27004213743785055
	data : 0.1170353889465332
	model : 0.06898889541625977
			 train-loss:  2.087406328655182 	 ± 0.2693398647638357
	data : 0.11715598106384277
	model : 0.06890201568603516
			 train-loss:  2.0866524558118047 	 ± 0.26882032521011723
	data : 0.11729488372802735
	model : 0.06874027252197265
			 train-loss:  2.089182874513051 	 ± 0.27034382484477565
	data : 0.11754846572875977
	model : 0.06881194114685059
			 train-loss:  2.0888876268738192 	 ± 0.26966200504229304
	data : 0.1175158977508545
	model : 0.0689422607421875
			 train-loss:  2.088347519879566 	 ± 0.2690581768277554
	data : 0.11740479469299317
	model : 0.06924047470092773
			 train-loss:  2.0898395590484142 	 ± 0.2691476543162475
	data : 0.11691217422485352
	model : 0.06921825408935547
			 train-loss:  2.09004371030343 	 ± 0.2684643789760041
	data : 0.11694769859313965
	model : 0.06906490325927735
			 train-loss:  2.090274099222164 	 ± 0.2677906945623948
	data : 0.11695609092712403
	model : 0.06847119331359863
			 train-loss:  2.08973321609008 	 ± 0.2672093908738491
	data : 0.11736335754394531
	model : 0.06870288848876953
			 train-loss:  2.089343723593926 	 ± 0.2665823528204968
	data : 0.11735992431640625
	model : 0.06951937675476075
			 train-loss:  2.0888458187810057 	 ± 0.26599623898964064
	data : 0.11684398651123047
	model : 0.06950559616088867
			 train-loss:  2.089122362209089 	 ± 0.26535207093834856
	data : 0.11671280860900879
	model : 0.06948480606079102
			 train-loss:  2.0887392615553124 	 ± 0.26473940676586954
	data : 0.11671462059020996
	model : 0.06971793174743653
			 train-loss:  2.088601831793785 	 ± 0.2640838450723598
	data : 0.11669754981994629
	model : 0.06856393814086914
			 train-loss:  2.0889904077966417 	 ± 0.26348341302030226
	data : 0.11760969161987304
	model : 0.06837277412414551
			 train-loss:  2.0896403689195613 	 ± 0.2629919020449849
	data : 0.11783723831176758
	model : 0.06754751205444336
			 train-loss:  2.0905645404543196 	 ± 0.2626719519187943
	data : 0.11860284805297852
	model : 0.06795096397399902
			 train-loss:  2.090086043465371 	 ± 0.2621160328424572
	data : 0.11820521354675292
	model : 0.06836633682250977
			 train-loss:  2.090889211398799 	 ± 0.26172746377943046
	data : 0.11768546104431152
	model : 0.0692054271697998
			 train-loss:  2.0935292087712334 	 ± 0.2638133810826831
	data : 0.11679410934448242
	model : 0.06907987594604492
			 train-loss:  2.0929522053631033 	 ± 0.26330564827881214
	data : 0.1167799949645996
	model : 0.06911001205444336
			 train-loss:  2.094203892808694 	 ± 0.2632885472294642
	data : 0.11692600250244141
	model : 0.06800661087036133
			 train-loss:  2.0933865388614707 	 ± 0.26292230484560497
	data : 0.11807670593261718
	model : 0.06775808334350586
			 train-loss:  2.0933169393312365 	 ± 0.26229748224056554
	data : 0.11836891174316407
	model : 0.06819772720336914
			 train-loss:  2.0923629383340265 	 ± 0.26204012621087497
	data : 0.11812005043029786
	model : 0.06836462020874023
			 train-loss:  2.0913152801540664 	 ± 0.26186394769230165
	data : 0.11789178848266602
	model : 0.06845693588256836
			 train-loss:  2.089702034220449 	 ± 0.26230236875188656
	data : 0.11780037879943847
	model : 0.06861143112182617
			 train-loss:  2.0901258693677245 	 ± 0.2617618918184071
	data : 0.11771073341369628
	model : 0.06878819465637206
			 train-loss:  2.0898347444312515 	 ± 0.2611871569266951
	data : 0.11774554252624511
	model : 0.06833119392395019
			 train-loss:  2.090893469474934 	 ± 0.2610438597434811
	data : 0.11791977882385254
	model : 0.0683786392211914
			 train-loss:  2.0893332040804324 	 ± 0.26144924173895157
	data : 0.11806912422180176
	model : 0.06854233741760254
			 train-loss:  2.087656561387788 	 ± 0.2620155779479933
	data : 0.11789212226867676
	model : 0.06847085952758789
			 train-loss:  2.0887027929906976 	 ± 0.26187269008864716
	data : 0.1178624153137207
	model : 0.06837801933288574
			 train-loss:  2.0895559701052577 	 ± 0.2615817335713361
	data : 0.1177337646484375
	model : 0.06839122772216796
			 train-loss:  2.0902383456942184 	 ± 0.26118542829996305
	data : 0.11778335571289063
	model : 0.0682438850402832
			 train-loss:  2.089108976157936 	 ± 0.26113678511338545
	data : 0.11769399642944336
	model : 0.06828045845031738
			 train-loss:  2.0891034763490137 	 ± 0.2605506316372847
	data : 0.11763992309570312
	model : 0.06907720565795898
			 train-loss:  2.090033871786935 	 ± 0.26033939978679543
	data : 0.11675291061401367
	model : 0.06872391700744629
			 train-loss:  2.091018878089057 	 ± 0.26017822144701125
	data : 0.11685171127319335
	model : 0.06829442977905273
			 train-loss:  2.089714987088094 	 ± 0.26033768706341914
	data : 0.11730213165283203
	model : 0.06814107894897461
			 train-loss:  2.0910306911636556 	 ± 0.2605155721651847
	data : 0.11745142936706543
	model : 0.06826763153076172
			 train-loss:  2.089498591527604 	 ± 0.2609665473393187
	data : 0.1169473648071289
	model : 0.06805353164672852
			 train-loss:  2.0885872242232075 	 ± 0.26075950235229656
	data : 0.11708831787109375
	model : 0.0682004451751709
			 train-loss:  2.0889249247053394 	 ± 0.2602421966356516
	data : 0.11715216636657715
	model : 0.06848354339599609
			 train-loss:  2.087365506531356 	 ± 0.2607529967577268
	data : 0.11666703224182129
	model : 0.06856098175048828
			 train-loss:  2.0858728746915687 	 ± 0.26117754922344805
	data : 0.11690020561218262
	model : 0.06902961730957032
			 train-loss:  2.085458820981529 	 ± 0.26069277645369476
	data : 0.11675214767456055
	model : 0.06921381950378418
			 train-loss:  2.0852531958849 	 ± 0.2601540791214299
	data : 0.11666631698608398
	model : 0.06909012794494629
			 train-loss:  2.0860022392678768 	 ± 0.25985271541501265
	data : 0.11674203872680664
	model : 0.06910915374755859
			 train-loss:  2.086539040177555 	 ± 0.2594321374462621
	data : 0.11671199798583984
	model : 0.06918115615844726
			 train-loss:  2.0882100032854685 	 ± 0.2601537742068084
	data : 0.11652116775512696
	model : 0.06905622482299804
			 train-loss:  2.0882675467419025 	 ± 0.25960816880462506
	data : 0.11690816879272461
	model : 0.06905207633972169
			 train-loss:  2.087570375977201 	 ± 0.25928765339494386
	data : 0.11692147254943848
	model : 0.06884989738464356
			 train-loss:  2.0872928222020466 	 ± 0.2587824829217715
	data : 0.11696128845214844
	model : 0.06801781654357911
			 train-loss:  2.08698449194184 	 ± 0.25828920343420225
	data : 0.11769261360168456
	model : 0.06755099296569825
			 train-loss:  2.085475860055813 	 ± 0.25881682050352206
	data : 0.11799793243408203
	model : 0.06742043495178222
			 train-loss:  2.0846858127617542 	 ± 0.25857597279101185
	data : 0.11769895553588867
	model : 0.0672917366027832
			 train-loss:  2.0877443459190306 	 ± 0.26241319501641064
	data : 0.11784920692443848
	model : 0.0675081729888916
			 train-loss:  2.0870559668054387 	 ± 0.2620977759440694
	data : 0.11759495735168457
	model : 0.06824078559875488
			 train-loss:  2.0858895836806877 	 ± 0.2622008873846653
	data : 0.1168889045715332
	model : 0.06812267303466797
			 train-loss:  2.08619373819606 	 ± 0.2617130595311015
	data : 0.116908597946167
	model : 0.06810030937194825
			 train-loss:  2.0861704032267294 	 ± 0.26118513672993027
	data : 0.11687850952148438
	model : 0.068115234375
			 train-loss:  2.0877998172039964 	 ± 0.26192011487717437
	data : 0.11676292419433594
	model : 0.06805419921875
			 train-loss:  2.088959277153015 	 ± 0.26203526573141683
	data : 0.11704096794128419
	model : 0.06825990676879883
			 train-loss:  2.0896518277931975 	 ± 0.2617419175162574
	data : 0.11725845336914062
	model : 0.06872706413269043
			 train-loss:  2.0895934048153104 	 ± 0.2612237119164599
	data : 0.11693115234375
	model : 0.06888389587402344
			 train-loss:  2.0894425367649365 	 ± 0.26071794864957704
	data : 0.11689267158508301
	model : 0.06910996437072754
			 train-loss:  2.087680910046645 	 ± 0.26170857328339214
	data : 0.11689929962158203
	model : 0.06909794807434082
			 train-loss:  2.0876143198387296 	 ± 0.261197071182682
	data : 0.11682696342468261
	model : 0.06835289001464843
			 train-loss:  2.087972490582615 	 ± 0.26074915790438924
	data : 0.11643972396850585
	model : 0.059105873107910156
#epoch  97    val-loss:  2.427256289281343  train-loss:  2.087972490582615  lr:  6.103515625e-07
			 train-loss:  2.380885124206543 	 ± 0.0
	data : 5.724702596664429
	model : 0.07244205474853516
			 train-loss:  2.3270343542099 	 ± 0.053850769996643066
	data : 2.928658962249756
	model : 0.06898093223571777
			 train-loss:  2.3081684907277427 	 ± 0.05143065132787096
	data : 1.9921828111012776
	model : 0.06809258460998535
			 train-loss:  2.258407473564148 	 ± 0.09701706246569948
	data : 1.5236729979515076
	model : 0.06752663850784302
			 train-loss:  2.2762166976928713 	 ± 0.09380043807918556
	data : 1.2426610469818116
	model : 0.06735172271728515
			 train-loss:  2.222586671511332 	 ± 0.1473533181202084
	data : 0.12135162353515624
	model : 0.06703200340270996
			 train-loss:  2.2247070925576344 	 ± 0.1365215011603789
	data : 0.11787528991699218
	model : 0.06784086227416992
			 train-loss:  2.20316544175148 	 ± 0.13984510817464443
	data : 0.11726374626159668
	model : 0.06896328926086426
			 train-loss:  2.226637946234809 	 ± 0.14761897041853567
	data : 0.11656608581542968
	model : 0.06943264007568359
			 train-loss:  2.2042711973190308 	 ± 0.15528898060789398
	data : 0.11635723114013671
	model : 0.06945762634277344
			 train-loss:  2.226389234716242 	 ± 0.1637513382584806
	data : 0.11637496948242188
	model : 0.0694185733795166
			 train-loss:  2.256386379400889 	 ± 0.18568272572106576
	data : 0.11653370857238769
	model : 0.06899762153625488
			 train-loss:  2.266242815898015 	 ± 0.18163619443616214
	data : 0.11690783500671387
	model : 0.06853795051574707
			 train-loss:  2.253158382007054 	 ± 0.18127544773970122
	data : 0.11727752685546874
	model : 0.06885910034179688
			 train-loss:  2.2570298353830975 	 ± 0.17572678473627035
	data : 0.11703972816467285
	model : 0.06943473815917969
			 train-loss:  2.227338045835495 	 ± 0.20536295781426817
	data : 0.1165541648864746
	model : 0.0692786693572998
			 train-loss:  2.21617634156171 	 ± 0.20417262906091105
	data : 0.11647067070007325
	model : 0.06978750228881836
			 train-loss:  2.1882563630739846 	 ± 0.22939589415571482
	data : 0.11614990234375
	model : 0.0689807415008545
			 train-loss:  2.1801509292502153 	 ± 0.225910241637115
	data : 0.11699352264404297
	model : 0.06900429725646973
			 train-loss:  2.170446991920471 	 ± 0.22421602754264713
	data : 0.11701955795288085
	model : 0.06819934844970703
			 train-loss:  2.152320243063427 	 ± 0.23334623062381216
	data : 0.11774101257324218
	model : 0.06722931861877442
			 train-loss:  2.142808659510179 	 ± 0.23211057050604744
	data : 0.11856060028076172
	model : 0.06642427444458007
			 train-loss:  2.1331006029377813 	 ± 0.231530412320607
	data : 0.11920490264892578
	model : 0.06709156036376954
			 train-loss:  2.144459515810013 	 ± 0.2331100757313802
	data : 0.11849923133850097
	model : 0.06707439422607422
			 train-loss:  2.1289008045196534 	 ± 0.2407830123567534
	data : 0.11848092079162598
	model : 0.06702780723571777
			 train-loss:  2.104007120315845 	 ± 0.26690631888614125
	data : 0.11874794960021973
	model : 0.06819190979003906
			 train-loss:  2.1011598154350564 	 ± 0.2623190575985944
	data : 0.11789555549621582
	model : 0.06813926696777343
			 train-loss:  2.0835190287658145 	 ± 0.2734155638337212
	data : 0.11799483299255371
	model : 0.06844983100891114
			 train-loss:  2.0906853634735634 	 ± 0.27132315216074465
	data : 0.11779246330261231
	model : 0.06815876960754394
			 train-loss:  2.089724290370941 	 ± 0.26681297566943174
	data : 0.1180816650390625
	model : 0.0691103458404541
			 train-loss:  2.0869489062216973 	 ± 0.26291409634357177
	data : 0.11715950965881347
	model : 0.06903014183044434
			 train-loss:  2.094781521707773 	 ± 0.26242246177268475
	data : 0.11731257438659667
	model : 0.06983256340026855
			 train-loss:  2.088251489581484 	 ± 0.26104259397230567
	data : 0.11635823249816894
	model : 0.0698277473449707
			 train-loss:  2.093835437999052 	 ± 0.2591678585122802
	data : 0.1163487434387207
	model : 0.0694127082824707
			 train-loss:  2.0846978562218803 	 ± 0.26093627076317005
	data : 0.11661725044250489
	model : 0.0686103343963623
			 train-loss:  2.0880024466249676 	 ± 0.2580283365863787
	data : 0.11727700233459473
	model : 0.06866135597229003
			 train-loss:  2.0848226708334847 	 ± 0.2552316503087551
	data : 0.11721315383911132
	model : 0.06778430938720703
			 train-loss:  2.080725632215801 	 ± 0.25308096045880507
	data : 0.11824054718017578
	model : 0.06674189567565918
			 train-loss:  2.086471899961814 	 ± 0.25231410997546294
	data : 0.11906628608703614
	model : 0.06666288375854493
			 train-loss:  2.0925461649894714 	 ± 0.25201154927862923
	data : 0.11914801597595215
	model : 0.06716480255126953
			 train-loss:  2.100444613433466 	 ± 0.2538822964664106
	data : 0.118701171875
	model : 0.06701316833496093
			 train-loss:  2.113838275273641 	 ± 0.2650972384247224
	data : 0.11885552406311035
	model : 0.06808300018310547
			 train-loss:  2.1070750009181887 	 ± 0.26563766017983914
	data : 0.1179274082183838
	model : 0.06814107894897461
			 train-loss:  2.1183315163308922 	 ± 0.27277856700601333
	data : 0.11788215637207031
	model : 0.06882152557373047
			 train-loss:  2.1168322960535684 	 ± 0.26991392952555404
	data : 0.11727185249328613
	model : 0.06880655288696289
			 train-loss:  2.1134965730750044 	 ± 0.26790011968128624
	data : 0.1172154426574707
	model : 0.06881928443908691
			 train-loss:  2.111089528875148 	 ± 0.265537116808109
	data : 0.11717457771301269
	model : 0.06836943626403809
			 train-loss:  2.1056297744313874 	 ± 0.26540915972875173
	data : 0.11757707595825195
	model : 0.06929187774658203
			 train-loss:  2.1069149654738757 	 ± 0.262837805784432
	data : 0.1168139934539795
	model : 0.0695875644683838
			 train-loss:  2.107278244495392 	 ± 0.2602085788695475
	data : 0.1166006088256836
	model : 0.06980671882629394
			 train-loss:  2.1081546030792535 	 ± 0.2577193958553593
	data : 0.11649522781372071
	model : 0.06932320594787597
			 train-loss:  2.1070446578355937 	 ± 0.2553523524855827
	data : 0.11684517860412598
	model : 0.0688960075378418
			 train-loss:  2.101794753434523 	 ± 0.25574937346312054
	data : 0.11720089912414551
	model : 0.06793437004089356
			 train-loss:  2.108553438274949 	 ± 0.2581036963038845
	data : 0.11804847717285157
	model : 0.06760354042053222
			 train-loss:  2.1085265658118506 	 ± 0.2557466117857227
	data : 0.11828999519348145
	model : 0.06757550239562989
			 train-loss:  2.1044772820813313 	 ± 0.2552257387127552
	data : 0.11830968856811523
	model : 0.06807618141174317
			 train-loss:  2.1039093795575594 	 ± 0.2530127038507008
	data : 0.11776156425476074
	model : 0.06880221366882325
			 train-loss:  2.0951748120373694 	 ± 0.25934609348364884
	data : 0.11706676483154296
	model : 0.06992316246032715
			 train-loss:  2.098346510175931 	 ± 0.2582708854994147
	data : 0.11621918678283691
	model : 0.07065739631652831
			 train-loss:  2.0946269392967225 	 ± 0.2576982667818631
	data : 0.11531963348388671
	model : 0.06980199813842773
			 train-loss:  2.0894105532130256 	 ± 0.2587515709861581
	data : 0.11603307723999023
	model : 0.06954607963562012
			 train-loss:  2.08280957898786 	 ± 0.26178319840625264
	data : 0.11651816368103027
	model : 0.0694777488708496
			 train-loss:  2.084075246538435 	 ± 0.2598883930621113
	data : 0.11648521423339844
	model : 0.06959009170532227
			 train-loss:  2.075201254338026 	 ± 0.26729710235474624
	data : 0.11628861427307129
	model : 0.06903314590454102
			 train-loss:  2.0725788629972017 	 ± 0.2660614005056388
	data : 0.11702971458435059
	model : 0.07005715370178223
			 train-loss:  2.067560264558503 	 ± 0.2671202439339463
	data : 0.11620397567749023
	model : 0.07022757530212402
			 train-loss:  2.0669453962525326 	 ± 0.2651663692406726
	data : 0.11588797569274903
	model : 0.06981816291809081
			 train-loss:  2.0707869740093456 	 ± 0.26508103081303397
	data : 0.1163752555847168
	model : 0.06963400840759278
			 train-loss:  2.071751331937486 	 ± 0.2632732721536362
	data : 0.11646566390991211
	model : 0.0694620132446289
			 train-loss:  2.072943002837045 	 ± 0.2615733515382579
	data : 0.11659221649169922
	model : 0.06941323280334473
			 train-loss:  2.074781065255823 	 ± 0.2601796315960652
	data : 0.116520357131958
	model : 0.0693979263305664
			 train-loss:  2.071700029902988 	 ± 0.25966755923514095
	data : 0.11657285690307617
	model : 0.06984467506408691
			 train-loss:  2.070392081182297 	 ± 0.2581215857474462
	data : 0.11610879898071289
	model : 0.07045154571533203
			 train-loss:  2.0752679737838537 	 ± 0.2597343209775718
	data : 0.11536226272583008
	model : 0.07051239013671876
			 train-loss:  2.0703013118108116 	 ± 0.2615106830817085
	data : 0.11533503532409668
	model : 0.07039365768432618
			 train-loss:  2.065997478209044 	 ± 0.2624447011770275
	data : 0.11564531326293945
	model : 0.07135114669799805
			 train-loss:  2.0666098532738624 	 ± 0.26078959345552755
	data : 0.11457700729370117
	model : 0.07042732238769531
			 train-loss:  2.0671636783159695 	 ± 0.2591580426038576
	data : 0.11533775329589843
	model : 0.06973414421081543
			 train-loss:  2.068021879920477 	 ± 0.2576240975685163
	data : 0.11611557006835938
	model : 0.06937355995178222
			 train-loss:  2.0794539213180543 	 ± 0.27543634931350436
	data : 0.1163973331451416
	model : 0.06931877136230469
			 train-loss:  2.0764856500390136 	 ± 0.275015319723075
	data : 0.1163020133972168
	model : 0.06821398735046387
			 train-loss:  2.0765577569240476 	 ± 0.2733340234572973
	data : 0.11756277084350586
	model : 0.06911797523498535
			 train-loss:  2.0737964704812293 	 ± 0.27283067202264366
	data : 0.11670546531677246
	model : 0.06821837425231933
			 train-loss:  2.0739943044526234 	 ± 0.27120780660612814
	data : 0.11755046844482422
	model : 0.06870665550231933
			 train-loss:  2.079103307163014 	 ± 0.27364374165876376
	data : 0.11723122596740723
	model : 0.06888298988342285
			 train-loss:  2.0839071052018987 	 ± 0.27562962407216324
	data : 0.117130708694458
	model : 0.06914682388305664
			 train-loss:  2.0849593891494576 	 ± 0.27421466004473255
	data : 0.11682624816894531
	model : 0.06829628944396973
			 train-loss:  2.0857215361161665 	 ± 0.2727448288518896
	data : 0.11773538589477539
	model : 0.06959338188171386
			 train-loss:  2.084580244642965 	 ± 0.2714194649235965
	data : 0.11643438339233399
	model : 0.06966447830200195
			 train-loss:  2.0847590234544544 	 ± 0.2699126365477407
	data : 0.11634421348571777
	model : 0.06979389190673828
			 train-loss:  2.0804300661925432 	 ± 0.2715489627884046
	data : 0.11613717079162597
	model : 0.06884722709655762
			 train-loss:  2.0783381889695707 	 ± 0.2708053573114226
	data : 0.1170166015625
	model : 0.06965227127075195
			 train-loss:  2.0779112372347104 	 ± 0.2693766096276483
	data : 0.11631584167480469
	model : 0.06846094131469727
			 train-loss:  2.0771496105701366 	 ± 0.2680405751663453
	data : 0.11750874519348145
	model : 0.06835322380065918
			 train-loss:  2.075369755845321 	 ± 0.26718394388632094
	data : 0.11748390197753907
	model : 0.06819252967834473
			 train-loss:  2.075021213541428 	 ± 0.2658104274579944
	data : 0.11780462265014649
	model : 0.06909327507019043
			 train-loss:  2.0734356690927878 	 ± 0.26489265506701803
	data : 0.11695833206176758
	model : 0.06924581527709961
			 train-loss:  2.079192722330288 	 ± 0.269568270961451
	data : 0.11697759628295898
	model : 0.06992545127868652
			 train-loss:  2.078606134713298 	 ± 0.2682662155562946
	data : 0.11610832214355468
	model : 0.07000398635864258
			 train-loss:  2.079841336011887 	 ± 0.2672043062665023
	data : 0.11607871055603028
	model : 0.06905579566955566
			 train-loss:  2.082397681651729 	 ± 0.267104322878577
	data : 0.11685881614685059
	model : 0.06912169456481934
			 train-loss:  2.08254740635554 	 ± 0.26579602223179727
	data : 0.116756010055542
	model : 0.0681985855102539
			 train-loss:  2.083875307759035 	 ± 0.2648423795148247
	data : 0.1174093246459961
	model : 0.06770968437194824
			 train-loss:  2.0844575453263063 	 ± 0.26363225463141365
	data : 0.11822309494018554
	model : 0.06758561134338378
			 train-loss:  2.0822719642094203 	 ± 0.26331886704237356
	data : 0.11830968856811523
	model : 0.06853723526000977
			 train-loss:  2.081700176562903 	 ± 0.262139339969723
	data : 0.11735286712646484
	model : 0.06833920478820801
			 train-loss:  2.083010069677763 	 ± 0.26125982318358526
	data : 0.11749815940856934
	model : 0.06881527900695801
			 train-loss:  2.0829315097243697 	 ± 0.26004874377908654
	data : 0.1170576572418213
	model : 0.06942405700683593
			 train-loss:  2.082124286835347 	 ± 0.25898900911720535
	data : 0.1164255142211914
	model : 0.0694352626800537
			 train-loss:  2.086911328272386 	 ± 0.2626087389829691
	data : 0.11651368141174316
	model : 0.06910939216613769
			 train-loss:  2.0867425933614507 	 ± 0.26142913038219767
	data : 0.11677212715148926
	model : 0.069248628616333
			 train-loss:  2.08623787441424 	 ± 0.26031373681525394
	data : 0.11674895286560058
	model : 0.06875858306884766
			 train-loss:  2.0922040506801776 	 ± 0.26674001556755417
	data : 0.1172457218170166
	model : 0.06865034103393555
			 train-loss:  2.090873974457122 	 ± 0.2659436399698292
	data : 0.11715230941772461
	model : 0.06863718032836914
			 train-loss:  2.0868229814197705 	 ± 0.26829426541965434
	data : 0.11720361709594726
	model : 0.06888117790222167
			 train-loss:  2.0863775888393663 	 ± 0.26717801754811815
	data : 0.11717309951782226
	model : 0.06882772445678711
			 train-loss:  2.0865663514178023 	 ± 0.2660415490849281
	data : 0.11716842651367188
	model : 0.07015652656555176
			 train-loss:  2.087845536611848 	 ± 0.2652729541627625
	data : 0.11600294113159179
	model : 0.07016716003417969
			 train-loss:  2.086452849772798 	 ± 0.264588865786275
	data : 0.1159660816192627
	model : 0.06927533149719238
			 train-loss:  2.085526510079702 	 ± 0.26367781196173967
	data : 0.11667923927307129
	model : 0.06929774284362793
			 train-loss:  2.0869269469552787 	 ± 0.2630337248102869
	data : 0.1166597843170166
	model : 0.06930575370788575
			 train-loss:  2.084333219489113 	 ± 0.26350266420016555
	data : 0.11663780212402344
	model : 0.06800503730773926
			 train-loss:  2.0855964383458705 	 ± 0.26279998260576587
	data : 0.11775074005126954
	model : 0.0680938720703125
			 train-loss:  2.0845443673672213 	 ± 0.26199810579665117
	data : 0.11778149604797364
	model : 0.06879348754882812
			 train-loss:  2.0845186948776244 	 ± 0.26094816555595574
	data : 0.11740002632141114
	model : 0.06892628669738769
			 train-loss:  2.088818339128343 	 ± 0.264318717756784
	data : 0.11723537445068359
	model : 0.0680384635925293
			 train-loss:  2.092174249371206 	 ± 0.2659573276311305
	data : 0.11786065101623536
	model : 0.0690047264099121
			 train-loss:  2.091914783231914 	 ± 0.26493253139909845
	data : 0.11692295074462891
	model : 0.06907691955566406
			 train-loss:  2.0923808332561524 	 ± 0.26395633246026756
	data : 0.11692705154418945
	model : 0.06949701309204101
			 train-loss:  2.0960880160331725 	 ± 0.2662890779698348
	data : 0.11644492149353028
	model : 0.06928620338439942
			 train-loss:  2.101115939271359 	 ± 0.27139450598123355
	data : 0.116473388671875
	model : 0.07002129554748535
			 train-loss:  2.103439017678752 	 ± 0.27166882861443403
	data : 0.11595888137817383
	model : 0.06978702545166016
			 train-loss:  2.1014111104764437 	 ± 0.27164659635469224
	data : 0.11591377258300781
	model : 0.06872615814208985
			 train-loss:  2.1039464785091915 	 ± 0.27220603189595416
	data : 0.11683235168457032
	model : 0.06827945709228515
			 train-loss:  2.1038653311906037 	 ± 0.2711976142947075
	data : 0.11709461212158204
	model : 0.06838407516479492
			 train-loss:  2.100869217339684 	 ± 0.272432019164427
	data : 0.11721868515014648
	model : 0.06876020431518555
			 train-loss:  2.0991077457901337 	 ± 0.2722121183899244
	data : 0.11686758995056153
	model : 0.06900062561035156
			 train-loss:  2.097786436910215 	 ± 0.271664622989096
	data : 0.11691713333129883
	model : 0.07004280090332031
			 train-loss:  2.096320250051485 	 ± 0.2712330718686282
	data : 0.11596126556396484
	model : 0.07018656730651855
			 train-loss:  2.0970311897141594 	 ± 0.2703925914075784
	data : 0.11585330963134766
	model : 0.07000946998596191
			 train-loss:  2.0980039099429515 	 ± 0.2696777584950837
	data : 0.11591053009033203
	model : 0.06902503967285156
			 train-loss:  2.097806201854222 	 ± 0.26873676584396916
	data : 0.1167527198791504
	model : 0.06868481636047363
			 train-loss:  2.097505574459796 	 ± 0.26781943863744495
	data : 0.11699891090393066
	model : 0.06860365867614746
			 train-loss:  2.0987138036224575 	 ± 0.2672786929964083
	data : 0.117100191116333
	model : 0.06807136535644531
			 train-loss:  2.0973016779998255 	 ± 0.2668939392020492
	data : 0.11754832267761231
	model : 0.0683858871459961
			 train-loss:  2.099065516909508 	 ± 0.266825027539065
	data : 0.11744112968444824
	model : 0.06899051666259766
			 train-loss:  2.097406800912351 	 ± 0.26667014644008075
	data : 0.11700177192687988
	model : 0.0692903995513916
			 train-loss:  2.0957454451032587 	 ± 0.2665299404220383
	data : 0.11675558090209961
	model : 0.06950244903564454
			 train-loss:  2.0965408118779227 	 ± 0.2658102117421266
	data : 0.1165278434753418
	model : 0.07007966041564942
			 train-loss:  2.0998605275154114 	 ± 0.2680039013716762
	data : 0.11597738265991211
	model : 0.06986374855041504
			 train-loss:  2.100076883044464 	 ± 0.2671281399064209
	data : 0.1160132884979248
	model : 0.06942553520202636
			 train-loss:  2.104776174614304 	 ± 0.2724381962889598
	data : 0.11624717712402344
	model : 0.0684053897857666
			 train-loss:  2.107447095166624 	 ± 0.2735357308983318
	data : 0.11708989143371581
	model : 0.06814084053039551
			 train-loss:  2.1076372916048225 	 ± 0.27265633127843647
	data : 0.11739630699157715
	model : 0.06828656196594238
			 train-loss:  2.106048123298153 	 ± 0.2724899498635073
	data : 0.11729187965393066
	model : 0.06868128776550293
			 train-loss:  2.1056335958150716 	 ± 0.27166420523279194
	data : 0.11698484420776367
	model : 0.06909728050231934
			 train-loss:  2.1056583497174985 	 ± 0.2707978270330677
	data : 0.1168436050415039
	model : 0.06987743377685547
			 train-loss:  2.1040936708450317 	 ± 0.2706505320101917
	data : 0.11632018089294434
	model : 0.06922106742858887
			 train-loss:  2.1054410694530175 	 ± 0.27032915901021787
	data : 0.1168630599975586
	model : 0.06910662651062012
			 train-loss:  2.1048469722270964 	 ± 0.26958716014130474
	data : 0.11701116561889649
	model : 0.0688551902770996
			 train-loss:  2.1053385171831023 	 ± 0.268820542620276
	data : 0.11732659339904786
	model : 0.06915230751037597
			 train-loss:  2.1033506018144115 	 ± 0.2691740107842984
	data : 0.11708970069885254
	model : 0.06942329406738282
			 train-loss:  2.103079631284702 	 ± 0.268369215620553
	data : 0.11670432090759278
	model : 0.07017402648925782
			 train-loss:  2.102875923965035 	 ± 0.2675624059820381
	data : 0.11614446640014649
	model : 0.07049274444580078
			 train-loss:  2.103106076067144 	 ± 0.2667666612614437
	data : 0.11596193313598632
	model : 0.0704871654510498
			 train-loss:  2.1029300108013382 	 ± 0.2659715489981843
	data : 0.11596708297729492
	model : 0.07028431892395019
			 train-loss:  2.1043033350013687 	 ± 0.2657637034975125
	data : 0.11603746414184571
	model : 0.06919975280761718
			 train-loss:  2.1039464395670664 	 ± 0.26501169575429856
	data : 0.11695389747619629
	model : 0.06912784576416016
			 train-loss:  2.103590497603783 	 ± 0.26426674851148785
	data : 0.11688551902770997
	model : 0.06900215148925781
			 train-loss:  2.1040545302278857 	 ± 0.2635573924800382
	data : 0.11688475608825684
	model : 0.06848230361938476
			 train-loss:  2.1051651783156813 	 ± 0.26318432199772884
	data : 0.11709737777709961
	model : 0.06848959922790528
			 train-loss:  2.1039292840070503 	 ± 0.26291532754396824
	data : 0.1172001838684082
	model : 0.06946854591369629
			 train-loss:  2.106677136669269 	 ± 0.26461977526603025
	data : 0.11639275550842285
	model : 0.06936917304992676
			 train-loss:  2.104549286694362 	 ± 0.265338445776279
	data : 0.1164313793182373
	model : 0.06831703186035157
			 train-loss:  2.1026255198887416 	 ± 0.26579340158971304
	data : 0.11735367774963379
	model : 0.06883511543273926
			 train-loss:  2.1029102436520835 	 ± 0.2650639935623803
	data : 0.11709527969360352
	model : 0.06902270317077637
			 train-loss:  2.103170241339732 	 ± 0.26433666983017046
	data : 0.11685962677001953
	model : 0.06849126815795899
			 train-loss:  2.1018081789606073 	 ± 0.2642152494121374
	data : 0.1175605297088623
	model : 0.06767134666442871
			 train-loss:  2.10188870523229 	 ± 0.26347837472260055
	data : 0.1183596134185791
	model : 0.0677065372467041
			 train-loss:  2.102591441075007 	 ± 0.262913635016558
	data : 0.11844854354858399
	model : 0.06708474159240722
			 train-loss:  2.1021834493342024 	 ± 0.26224348143540843
	data : 0.11877641677856446
	model : 0.06690611839294433
			 train-loss:  2.100263080099127 	 ± 0.2627951149466175
	data : 0.11890583038330078
	model : 0.06745638847351074
			 train-loss:  2.0987848186753486 	 ± 0.26283379763976733
	data : 0.11842012405395508
	model : 0.06847615242004394
			 train-loss:  2.0981621275777402 	 ± 0.26225392058933283
	data : 0.1175391674041748
	model : 0.06844115257263184
			 train-loss:  2.095995837288934 	 ± 0.26318971932738405
	data : 0.11755614280700684
	model : 0.06903553009033203
			 train-loss:  2.096396960237975 	 ± 0.26253796246195715
	data : 0.11721835136413575
	model : 0.06908092498779297
			 train-loss:  2.0962300134852607 	 ± 0.2618449475799314
	data : 0.11732926368713378
	model : 0.06904425621032714
			 train-loss:  2.0973862041818334 	 ± 0.26162579811323355
	data : 0.1173126220703125
	model : 0.0682957649230957
			 train-loss:  2.096190409054832 	 ± 0.26144736618564574
	data : 0.11788430213928222
	model : 0.06925129890441895
			 train-loss:  2.1004165480011388 	 ± 0.267152673386872
	data : 0.11704077720642089
	model : 0.06920857429504394
			 train-loss:  2.1005063437666567 	 ± 0.2664552778893885
	data : 0.11694841384887696
	model : 0.0693629264831543
			 train-loss:  2.100097651903828 	 ± 0.265820492531056
	data : 0.11672849655151367
	model : 0.0701831340789795
			 train-loss:  2.100115964449749 	 ± 0.26513106547282783
	data : 0.11586627960205079
	model : 0.07078518867492675
			 train-loss:  2.099852926337842 	 ± 0.26447210188408377
	data : 0.11539897918701172
	model : 0.07082033157348633
			 train-loss:  2.1005832409247374 	 ± 0.26398914667755935
	data : 0.11551375389099121
	model : 0.07000479698181153
			 train-loss:  2.1004124326365337 	 ± 0.26332564664430086
	data : 0.11630396842956543
	model : 0.07007083892822266
			 train-loss:  2.0997610576261723 	 ± 0.2628147164778764
	data : 0.1162522315979004
	model : 0.06900343894958497
			 train-loss:  2.098733192140406 	 ± 0.26254687358062667
	data : 0.11738567352294922
	model : 0.06903347969055176
			 train-loss:  2.0985286780937233 	 ± 0.26190218811365
	data : 0.11731481552124023
	model : 0.06893777847290039
			 train-loss:  2.0968695211410524 	 ± 0.26229296581294337
	data : 0.11727371215820312
	model : 0.07013654708862305
			 train-loss:  2.094666242006406 	 ± 0.26348854071380573
	data : 0.11619582176208496
	model : 0.0700456142425537
			 train-loss:  2.0929421062516695 	 ± 0.26396973015376124
	data : 0.11646471023559571
	model : 0.07029109001159668
			 train-loss:  2.0922473716031154 	 ± 0.2635038209854681
	data : 0.11613759994506836
	model : 0.07037596702575684
			 train-loss:  2.091169164461248 	 ± 0.26330570323561425
	data : 0.1161576271057129
	model : 0.07053418159484863
			 train-loss:  2.0918202307166123 	 ± 0.2628272660085196
	data : 0.1160813808441162
	model : 0.06924476623535156
			 train-loss:  2.091708978402962 	 ± 0.26219339832935606
	data : 0.1171987533569336
	model : 0.06833281517028808
			 train-loss:  2.0920123881187993 	 ± 0.26159556320279026
	data : 0.11789178848266602
	model : 0.06774091720581055
			 train-loss:  2.093133651293241 	 ± 0.26146411680929743
	data : 0.11839752197265625
	model : 0.06677384376525879
			 train-loss:  2.0904261341505643 	 ± 0.26374450018092227
	data : 0.11921391487121583
	model : 0.06586837768554688
			 train-loss:  2.092828478699639 	 ± 0.26539802363727516
	data : 0.11983084678649902
	model : 0.06686921119689941
			 train-loss:  2.092792177087323 	 ± 0.2647688940548119
	data : 0.11886076927185059
	model : 0.06772260665893555
			 train-loss:  2.0933582518460616 	 ± 0.2642716552061362
	data : 0.1180006980895996
	model : 0.06836109161376953
			 train-loss:  2.0926974899891957 	 ± 0.26382604733721315
	data : 0.11747188568115234
	model : 0.06923933029174804
			 train-loss:  2.0933765089400462 	 ± 0.2633954010143106
	data : 0.11666960716247558
	model : 0.07011165618896484
			 train-loss:  2.0935396055842554 	 ± 0.262792970656283
	data : 0.11598949432373047
	model : 0.0700911045074463
			 train-loss:  2.0938865150566452 	 ± 0.262233287284431
	data : 0.11609358787536621
	model : 0.06997308731079102
			 train-loss:  2.0940456406861405 	 ± 0.2616388176684624
	data : 0.11627564430236817
	model : 0.06949114799499512
			 train-loss:  2.0935644698799205 	 ± 0.2611342542475944
	data : 0.1167269229888916
	model : 0.06859703063964843
			 train-loss:  2.092739785642929 	 ± 0.26082175245391476
	data : 0.11741766929626465
	model : 0.06853480339050293
			 train-loss:  2.0931906136599454 	 ± 0.26031380924710507
	data : 0.11742496490478516
	model : 0.06768980026245117
			 train-loss:  2.093033649263339 	 ± 0.25973463079131465
	data : 0.11827588081359863
	model : 0.06695308685302734
			 train-loss:  2.0925984140988945 	 ± 0.25922974202125915
	data : 0.1188356876373291
	model : 0.06692233085632324
			 train-loss:  2.0929823354755284 	 ± 0.25871110416851484
	data : 0.11885390281677247
	model : 0.06697659492492676
			 train-loss:  2.0927898910428797 	 ± 0.25814897470804177
	data : 0.1190110683441162
	model : 0.06655235290527343
			 train-loss:  2.095621741082933 	 ± 0.2610384096583614
	data : 0.11923670768737793
	model : 0.06706066131591797
			 train-loss:  2.0957536164638215 	 ± 0.26046776238425695
	data : 0.1186258316040039
	model : 0.0676386833190918
			 train-loss:  2.0957690078256412 	 ± 0.25989351460286825
	data : 0.11786375045776368
	model : 0.06776514053344726
			 train-loss:  2.095940916684636 	 ± 0.2593358806049153
	data : 0.11760454177856446
	model : 0.06793227195739746
			 train-loss:  2.0960529667841814 	 ± 0.2587745566547679
	data : 0.11746811866760254
	model : 0.06806955337524415
			 train-loss:  2.0970514862433722 	 ± 0.2586531356489889
	data : 0.11732068061828613
	model : 0.06732678413391113
			 train-loss:  2.095751027524213 	 ± 0.25884513188449326
	data : 0.11777796745300292
	model : 0.06717109680175781
			 train-loss:  2.0949266260040216 	 ± 0.2585904137546897
	data : 0.11804370880126953
	model : 0.06710610389709473
			 train-loss:  2.094506743128208 	 ± 0.2581141465293637
	data : 0.11803703308105469
	model : 0.06698331832885743
			 train-loss:  2.0923342149481816 	 ± 0.25968814195526607
	data : 0.11815166473388672
	model : 0.06691336631774902
			 train-loss:  2.092659123907698 	 ± 0.2591826837529936
	data : 0.11852383613586426
	model : 0.06765570640563964
			 train-loss:  2.091753563638461 	 ± 0.2590052705535136
	data : 0.1181422233581543
	model : 0.0673912525177002
			 train-loss:  2.092818937221157 	 ± 0.2589759475051092
	data : 0.118414306640625
	model : 0.06763501167297363
			 train-loss:  2.09378243895138 	 ± 0.2588566327632952
	data : 0.11827874183654785
	model : 0.06809892654418945
			 train-loss:  2.0936404940473485 	 ± 0.25832380578848946
	data : 0.11788401603698731
	model : 0.06768794059753418
			 train-loss:  2.092606322467327 	 ± 0.25828038081227345
	data : 0.11803574562072754
	model : 0.06740407943725586
			 train-loss:  2.0922007540944207 	 ± 0.2578205419822333
	data : 0.1183328628540039
	model : 0.06738491058349609
			 train-loss:  2.0916994864290412 	 ± 0.2574049580415119
	data : 0.11822195053100586
	model : 0.06683511734008789
			 train-loss:  2.092315933341352 	 ± 0.25705371079168277
	data : 0.1184847354888916
	model : 0.0665811538696289
			 train-loss:  2.091913118225629 	 ± 0.25660326115694093
	data : 0.11852126121520996
	model : 0.0665900707244873
			 train-loss:  2.092906469714885 	 ± 0.25654871641256144
	data : 0.11840038299560547
	model : 0.06653418540954589
			 train-loss:  2.0931282096761996 	 ± 0.25605026934289515
	data : 0.1181530475616455
	model : 0.06658458709716797
			 train-loss:  2.094666210263364 	 ± 0.2566675046658264
	data : 0.11822528839111328
	model : 0.06665492057800293
			 train-loss:  2.0940057096942777 	 ± 0.25635975987570625
	data : 0.11843585968017578
	model : 0.06662960052490234
			 train-loss:  2.0935805495005533 	 ± 0.25593205778991485
	data : 0.11854538917541504
	model : 0.06677465438842774
			 train-loss:  2.092404769897461 	 ± 0.2560926498219913
	data : 0.11840195655822754
	model : 0.06683979034423829
			 train-loss:  2.0931567566328315 	 ± 0.25585841334364523
	data : 0.11865034103393554
	model : 0.06661458015441894
			 train-loss:  2.093651626791273 	 ± 0.25547058693740715
	data : 0.11896896362304688
	model : 0.0666748046875
			 train-loss:  2.093209073477583 	 ± 0.25506197395204344
	data : 0.11889171600341797
	model : 0.06721243858337403
			 train-loss:  2.0918418061076185 	 ± 0.2554866834958744
	data : 0.11861000061035157
	model : 0.06716346740722656
			 train-loss:  2.090332424406912 	 ± 0.256117438609604
	data : 0.11887049674987793
	model : 0.06774401664733887
			 train-loss:  2.0879810922779143 	 ± 0.25835971423186127
	data : 0.11721043586730957
	model : 0.059658241271972653
#epoch  98    val-loss:  2.4861906892374943  train-loss:  2.0879810922779143  lr:  6.103515625e-07
			 train-loss:  2.0175116062164307 	 ± 0.0
	data : 5.671262264251709
	model : 0.07690954208374023
			 train-loss:  1.9316896796226501 	 ± 0.08582192659378052
	data : 2.901668667793274
	model : 0.07216835021972656
			 train-loss:  1.955581545829773 	 ± 0.07779403103428703
	data : 1.9738086859385173
	model : 0.07005667686462402
			 train-loss:  2.076817065477371 	 ± 0.22052910719696905
	data : 1.5102936029434204
	model : 0.06917709112167358
			 train-loss:  2.1022124528884887 	 ± 0.20368154679282266
	data : 1.2320232391357422
	model : 0.06928868293762207
			 train-loss:  2.061273713906606 	 ± 0.20724794852317427
	data : 0.12100181579589844
	model : 0.06780614852905273
			 train-loss:  2.036290373120989 	 ± 0.20139701384806263
	data : 0.11785335540771484
	model : 0.06731467247009278
			 train-loss:  2.030367746949196 	 ± 0.1890402207043459
	data : 0.11821260452270507
	model : 0.06715559959411621
			 train-loss:  1.9941547579235501 	 ± 0.20556400508842157
	data : 0.11824998855590821
	model : 0.06808261871337891
			 train-loss:  1.9847097873687745 	 ± 0.19706286151433652
	data : 0.11760635375976562
	model : 0.06850905418395996
			 train-loss:  1.9924275875091553 	 ± 0.18947050674132052
	data : 0.11732926368713378
	model : 0.06859412193298339
			 train-loss:  2.0057964523633323 	 ± 0.18674441233084701
	data : 0.11736793518066406
	model : 0.06987910270690918
			 train-loss:  2.0223691646869364 	 ± 0.1883792929720064
	data : 0.11620311737060547
	model : 0.07099037170410157
			 train-loss:  2.044274483408247 	 ± 0.19796451307811747
	data : 0.1153707504272461
	model : 0.07077240943908691
			 train-loss:  2.036648925145467 	 ± 0.1933684905885799
	data : 0.11552977561950684
	model : 0.07022180557250976
			 train-loss:  2.030394062399864 	 ± 0.1887889349246112
	data : 0.11585369110107421
	model : 0.06926445960998535
			 train-loss:  2.0626278624815098 	 ± 0.22398438118011432
	data : 0.11647415161132812
	model : 0.06885929107666015
			 train-loss:  2.062813056839837 	 ± 0.2176750312432717
	data : 0.11687426567077637
	model : 0.06783790588378906
			 train-loss:  2.0638479182594702 	 ± 0.21191480499330392
	data : 0.11765766143798828
	model : 0.06775398254394531
			 train-loss:  2.0742231369018556 	 ± 0.21144206871446197
	data : 0.11763858795166016
	model : 0.06775431632995606
			 train-loss:  2.070801712217785 	 ± 0.2069128583693314
	data : 0.11786828041076661
	model : 0.06905674934387207
			 train-loss:  2.0607059435410933 	 ± 0.20738202451250973
	data : 0.11670265197753907
	model : 0.06923527717590332
			 train-loss:  2.079007843266363 	 ± 0.22024198292453231
	data : 0.11669092178344727
	model : 0.06990714073181152
			 train-loss:  2.0615371813376746 	 ± 0.23131272820931076
	data : 0.11625142097473144
	model : 0.06985726356506347
			 train-loss:  2.061976046562195 	 ± 0.22664945966583755
	data : 0.11640419960021972
	model : 0.07029976844787597
			 train-loss:  2.0523861967600308 	 ± 0.22736169351841845
	data : 0.11579880714416504
	model : 0.06972393989562989
			 train-loss:  2.0489524470435247 	 ± 0.22379751586710891
	data : 0.11631503105163574
	model : 0.06895160675048828
			 train-loss:  2.041912313018526 	 ± 0.22278864545026553
	data : 0.11697115898132324
	model : 0.06917018890380859
			 train-loss:  2.0586150062495263 	 ± 0.23608192497031102
	data : 0.11670908927917481
	model : 0.06861276626586914
			 train-loss:  2.0596680521965025 	 ± 0.23218314119459985
	data : 0.11707506179809571
	model : 0.06843891143798828
			 train-loss:  2.0581178242160427 	 ± 0.2285653230525595
	data : 0.11738553047180175
	model : 0.06858878135681153
			 train-loss:  2.051464483141899 	 ± 0.22799521142415496
	data : 0.11734366416931152
	model : 0.06846318244934083
			 train-loss:  2.052369731845278 	 ± 0.22457255638360668
	data : 0.11761207580566406
	model : 0.06757960319519044
			 train-loss:  2.0468118260888493 	 ± 0.22353723381522106
	data : 0.11844062805175781
	model : 0.06817798614501953
			 train-loss:  2.0483798231397357 	 ± 0.22051032876451823
	data : 0.11804342269897461
	model : 0.06801018714904786
			 train-loss:  2.0509737862481012 	 ± 0.21796701269405835
	data : 0.1181704044342041
	model : 0.06796741485595703
			 train-loss:  2.0462159594974003 	 ± 0.21688822888815118
	data : 0.1181797981262207
	model : 0.06858587265014648
			 train-loss:  2.048181050702145 	 ± 0.2143489547959033
	data : 0.11755051612854003
	model : 0.06946330070495606
			 train-loss:  2.041011862265758 	 ± 0.21614921078304777
	data : 0.11664996147155762
	model : 0.06871390342712402
			 train-loss:  2.0461083739995956 	 ± 0.21579034299836886
	data : 0.1173708438873291
	model : 0.06859493255615234
			 train-loss:  2.0419616495690693 	 ± 0.21474995191791463
	data : 0.11744303703308105
	model : 0.06870970726013184
			 train-loss:  2.04842495066779 	 ± 0.21617643220958155
	data : 0.11742124557495118
	model : 0.06866683959960937
			 train-loss:  2.040332328441531 	 ± 0.21999103568955178
	data : 0.11741752624511718
	model : 0.06865086555480956
			 train-loss:  2.049838727170771 	 ± 0.2262346678725638
	data : 0.1175187110900879
	model : 0.06941103935241699
			 train-loss:  2.0486291143629285 	 ± 0.22385067248368454
	data : 0.11667332649230958
	model : 0.0693842887878418
			 train-loss:  2.0454098193541816 	 ± 0.22245486848798554
	data : 0.11665868759155273
	model : 0.06947336196899415
			 train-loss:  2.0401680570967655 	 ± 0.22292862221682502
	data : 0.11642346382141114
	model : 0.06968574523925782
			 train-loss:  2.04457026720047 	 ± 0.22264915722061976
	data : 0.11613249778747559
	model : 0.07012577056884765
			 train-loss:  2.0511779347244574 	 ± 0.22507043674911745
	data : 0.11583447456359863
	model : 0.0701535701751709
			 train-loss:  2.062993288040161 	 ± 0.23766382474672149
	data : 0.11591825485229493
	model : 0.07012090682983399
			 train-loss:  2.0712837658676446 	 ± 0.24251425417223182
	data : 0.11594047546386718
	model : 0.0699070930480957
			 train-loss:  2.0681094435545115 	 ± 0.24123854072617026
	data : 0.11633987426757812
	model : 0.0699641227722168
			 train-loss:  2.072254531788376 	 ± 0.2408141326820933
	data : 0.11632161140441895
	model : 0.06950883865356446
			 train-loss:  2.068442940711975 	 ± 0.24018227834251135
	data : 0.11664175987243652
	model : 0.06955599784851074
			 train-loss:  2.0659936558116567 	 ± 0.23866840703758752
	data : 0.11646838188171386
	model : 0.06982631683349609
			 train-loss:  2.0625172470297133 	 ± 0.23792880672817898
	data : 0.1162912368774414
	model : 0.06994948387145997
			 train-loss:  2.061990838301809 	 ± 0.2358653748790038
	data : 0.11598010063171386
	model : 0.06991186141967773
			 train-loss:  2.0610357399644523 	 ± 0.23393437240763065
	data : 0.1160323143005371
	model : 0.07006325721740722
			 train-loss:  2.059156302678383 	 ± 0.23238462900386578
	data : 0.11581568717956543
	model : 0.06993870735168457
			 train-loss:  2.0550057172775267 	 ± 0.23263487784866113
	data : 0.11591849327087403
	model : 0.06993751525878907
			 train-loss:  2.052275737778085 	 ± 0.23168719683547434
	data : 0.11599411964416503
	model : 0.06984939575195312
			 train-loss:  2.0523057541539593 	 ± 0.2298112758450661
	data : 0.11611838340759277
	model : 0.06980071067810059
			 train-loss:  2.0537976745575195 	 ± 0.22828254122647273
	data : 0.11604833602905273
	model : 0.06978154182434082
			 train-loss:  2.0601409021764994 	 ± 0.23202059409175513
	data : 0.11610574722290039
	model : 0.06977434158325195
			 train-loss:  2.0610723953980665 	 ± 0.2303494715065991
	data : 0.11609559059143067
	model : 0.06954379081726074
			 train-loss:  2.05719278978579 	 ± 0.2307276823266703
	data : 0.11617131233215332
	model : 0.06963081359863281
			 train-loss:  2.0585301118110544 	 ± 0.22925693798653177
	data : 0.11624751091003419
	model : 0.069728422164917
			 train-loss:  2.0592224124599907 	 ± 0.22763552597819867
	data : 0.11627116203308105
	model : 0.06969757080078125
			 train-loss:  2.057148169780123 	 ± 0.2266263819128943
	data : 0.11625313758850098
	model : 0.06994853019714356
			 train-loss:  2.058898060662406 	 ± 0.22547083124521583
	data : 0.11615133285522461
	model : 0.06959872245788574
			 train-loss:  2.0559169708842964 	 ± 0.22526243150596012
	data : 0.11666417121887207
	model : 0.06991562843322754
			 train-loss:  2.0594242347611322 	 ± 0.2256363463353324
	data : 0.11627058982849121
	model : 0.06967167854309082
			 train-loss:  2.061414574923581 	 ± 0.22472108065819946
	data : 0.11647953987121581
	model : 0.06960120201110839
			 train-loss:  2.0645130614976623 	 ± 0.22476205805481392
	data : 0.11665425300598145
	model : 0.06906676292419434
			 train-loss:  2.0730776755015055 	 ± 0.2351010484927677
	data : 0.11712131500244141
	model : 0.06955208778381347
			 train-loss:  2.069968587473819 	 ± 0.23509618319603587
	data : 0.1165421485900879
	model : 0.06835565567016602
			 train-loss:  2.0714985674077813 	 ± 0.23394513049479798
	data : 0.11760425567626953
	model : 0.06885995864868164
			 train-loss:  2.0719929383351254 	 ± 0.2324811223775467
	data : 0.11723904609680176
	model : 0.06878099441528321
			 train-loss:  2.0748404490796823 	 ± 0.23236991157154424
	data : 0.117305326461792
	model : 0.06891436576843261
			 train-loss:  2.075528633594513 	 ± 0.23099403204933655
	data : 0.11709847450256347
	model : 0.06898951530456543
			 train-loss:  2.0749248428109253 	 ± 0.22962722871742014
	data : 0.11711726188659669
	model : 0.06994428634643554
			 train-loss:  2.0738371232660806 	 ± 0.22843262796074146
	data : 0.11650428771972657
	model : 0.06910700798034668
			 train-loss:  2.0709526941000695 	 ± 0.22854979228550962
	data : 0.11716437339782715
	model : 0.0693251609802246
			 train-loss:  2.0785245327722457 	 ± 0.23742741989618243
	data : 0.11695151329040528
	model : 0.06963906288146973
			 train-loss:  2.0785334615146414 	 ± 0.23602667011757278
	data : 0.11661691665649414
	model : 0.06943120956420898
			 train-loss:  2.0778198990710948 	 ± 0.23474261283730383
	data : 0.11678357124328613
	model : 0.06900792121887207
			 train-loss:  2.0768310256387994 	 ± 0.23356971285858208
	data : 0.11716117858886718
	model : 0.0699120044708252
			 train-loss:  2.0729046287861737 	 ± 0.23510872646219835
	data : 0.11632733345031739
	model : 0.0688976764678955
			 train-loss:  2.072384506129147 	 ± 0.23383506957109118
	data : 0.11735062599182129
	model : 0.06794819831848145
			 train-loss:  2.0683873256047565 	 ± 0.23557013889321518
	data : 0.11816010475158692
	model : 0.0683168888092041
			 train-loss:  2.0690449541741676 	 ± 0.23435527888444005
	data : 0.11780099868774414
	model : 0.06885232925415039
			 train-loss:  2.0778827641321267 	 ± 0.24785709731487382
	data : 0.11721539497375488
	model : 0.0684168815612793
			 train-loss:  2.077954092333394 	 ± 0.24652188007076412
	data : 0.11767792701721191
	model : 0.0692718505859375
			 train-loss:  2.0800573622926755 	 ± 0.24604455677815706
	data : 0.11678996086120605
	model : 0.06994309425354003
			 train-loss:  2.077385866014581 	 ± 0.24611288161723874
	data : 0.11632118225097657
	model : 0.06990118026733398
			 train-loss:  2.07865646854043 	 ± 0.24514071014602343
	data : 0.1162729263305664
	model : 0.06880855560302734
			 train-loss:  2.079934585954725 	 ± 0.24419514001292728
	data : 0.11728901863098144
	model : 0.06914196014404297
			 train-loss:  2.080041597084123 	 ± 0.24294833788216083
	data : 0.11710338592529297
	model : 0.06926312446594238
			 train-loss:  2.0776333050294356 	 ± 0.2428910892604946
	data : 0.11689953804016114
	model : 0.06952462196350098
			 train-loss:  2.0765414142608645 	 ± 0.2419176528270037
	data : 0.11683006286621093
	model : 0.06920080184936524
			 train-loss:  2.0758095306925255 	 ± 0.24082829789699775
	data : 0.1171048641204834
	model : 0.07014832496643067
			 train-loss:  2.073948764333538 	 ± 0.24037338909002237
	data : 0.11619782447814941
	model : 0.06991443634033204
			 train-loss:  2.0742783245531102 	 ± 0.23922683728234326
	data : 0.11647000312805175
	model : 0.06985316276550294
			 train-loss:  2.0765781425512753 	 ± 0.2392153422935727
	data : 0.11653499603271485
	model : 0.0706871509552002
			 train-loss:  2.076702304113479 	 ± 0.23807686358979138
	data : 0.11549220085144044
	model : 0.0701136589050293
			 train-loss:  2.076219878106747 	 ± 0.23700275855136524
	data : 0.11597700119018554
	model : 0.07007675170898438
			 train-loss:  2.077077977011137 	 ± 0.23605804962101387
	data : 0.1159144401550293
	model : 0.07009234428405761
			 train-loss:  2.07892487667225 	 ± 0.2357380462758563
	data : 0.11585531234741211
	model : 0.07010550498962402
			 train-loss:  2.0814039269718556 	 ± 0.2360642333115682
	data : 0.1159250259399414
	model : 0.06923727989196778
			 train-loss:  2.084941699288108 	 ± 0.23787380081097934
	data : 0.11685395240783691
	model : 0.06991915702819824
			 train-loss:  2.0847426577731296 	 ± 0.2368090745066481
	data : 0.11625003814697266
	model : 0.07008094787597656
			 train-loss:  2.084053573863847 	 ± 0.23586128000372406
	data : 0.11615972518920899
	model : 0.07022485733032227
			 train-loss:  2.084890943712893 	 ± 0.2349824904045487
	data : 0.11596245765686035
	model : 0.07007317543029785
			 train-loss:  2.082049064468919 	 ± 0.23589199335961814
	data : 0.11604628562927247
	model : 0.07007207870483398
			 train-loss:  2.082136162467625 	 ± 0.23486597772788229
	data : 0.11595954895019531
	model : 0.06994447708129883
			 train-loss:  2.08545209621561 	 ± 0.2365395647364635
	data : 0.11611723899841309
	model : 0.06976041793823243
			 train-loss:  2.083853957999466 	 ± 0.2361546561296279
	data : 0.11620502471923828
	model : 0.06954755783081054
			 train-loss:  2.0847964751518377 	 ± 0.23537276477672459
	data : 0.11646819114685059
	model : 0.06906757354736329
			 train-loss:  2.084523194978217 	 ± 0.2344005158244209
	data : 0.11696448326110839
	model : 0.06815056800842285
			 train-loss:  2.0847885966300965 	 ± 0.23343975796047806
	data : 0.11770458221435547
	model : 0.06787776947021484
			 train-loss:  2.0841554708717283 	 ± 0.23257656320172404
	data : 0.1180046558380127
	model : 0.06794910430908203
			 train-loss:  2.082524790138495 	 ± 0.2323149486056059
	data : 0.11817045211791992
	model : 0.06802616119384766
			 train-loss:  2.0830017900079247 	 ± 0.23142863147684334
	data : 0.11796588897705078
	model : 0.06879611015319824
			 train-loss:  2.081527945495421 	 ± 0.23107242395641744
	data : 0.11737465858459473
	model : 0.0695277214050293
			 train-loss:  2.081841290473938 	 ± 0.23017272718886406
	data : 0.11675376892089843
	model : 0.06997818946838379
			 train-loss:  2.0796291988993447 	 ± 0.2305876858928261
	data : 0.11623849868774414
	model : 0.06982555389404296
			 train-loss:  2.0794505556737346 	 ± 0.229686819812201
	data : 0.1162757396697998
	model : 0.06943845748901367
			 train-loss:  2.07800997979939 	 ± 0.22936311068782517
	data : 0.11655421257019043
	model : 0.06917481422424317
			 train-loss:  2.0837043218834457 	 ± 0.23738174956253863
	data : 0.11683874130249024
	model : 0.06841454505920411
			 train-loss:  2.0872522079027616 	 ± 0.23987584212989727
	data : 0.11758098602294922
	model : 0.06859135627746582
			 train-loss:  2.087660039654215 	 ± 0.23900377040716378
	data : 0.11741104125976562
	model : 0.06860470771789551
			 train-loss:  2.0890096841436443 	 ± 0.23859730832360956
	data : 0.11735644340515136
	model : 0.06834716796875
			 train-loss:  2.087089736658828 	 ± 0.23871995845825067
	data : 0.11755270957946777
	model : 0.06794471740722656
			 train-loss:  2.0859818164982014 	 ± 0.23817052000145392
	data : 0.1177607536315918
	model : 0.06797471046447753
			 train-loss:  2.085602213718273 	 ± 0.237327451266281
	data : 0.11782445907592773
	model : 0.06782073974609375
			 train-loss:  2.0845440634909798 	 ± 0.2367727323324979
	data : 0.11790547370910645
	model : 0.06787729263305664
			 train-loss:  2.08061819180955 	 ± 0.24030858436984368
	data : 0.11777420043945312
	model : 0.06857004165649414
			 train-loss:  2.0804861645767656 	 ± 0.23944130484019993
	data : 0.11735682487487793
	model : 0.06914496421813965
			 train-loss:  2.0789271653127326 	 ± 0.23928034415118005
	data : 0.11680269241333008
	model : 0.06998677253723144
			 train-loss:  2.079384240082332 	 ± 0.23848513104342103
	data : 0.11624670028686523
	model : 0.06968483924865723
			 train-loss:  2.0762582594621266 	 ± 0.2404991340797998
	data : 0.11667013168334961
	model : 0.0697329044342041
			 train-loss:  2.0747574668535043 	 ± 0.24031249672891025
	data : 0.11673040390014648
	model : 0.06968536376953124
			 train-loss:  2.070989582922075 	 ± 0.24364362512050736
	data : 0.11660399436950683
	model : 0.0690610408782959
			 train-loss:  2.0696798687179885 	 ± 0.24330078850202055
	data : 0.11725177764892578
	model : 0.06859712600708008
			 train-loss:  2.067906447114616 	 ± 0.24339250995316902
	data : 0.11745653152465821
	model : 0.06889162063598633
			 train-loss:  2.068983064938898 	 ± 0.2429037493074152
	data : 0.11717314720153808
	model : 0.06884593963623047
			 train-loss:  2.0700247352626047 	 ± 0.2424031290120713
	data : 0.11713252067565919
	model : 0.06871547698974609
			 train-loss:  2.0716342942134753 	 ± 0.24236972743240087
	data : 0.1173816204071045
	model : 0.06910295486450195
			 train-loss:  2.0745993204564854 	 ± 0.24423341339088295
	data : 0.11691775321960449
	model : 0.06872825622558594
			 train-loss:  2.0738691663742066 	 ± 0.24358105311726244
	data : 0.11718869209289551
	model : 0.06878905296325684
			 train-loss:  2.0708863924670693 	 ± 0.24550630359394998
	data : 0.11726727485656738
	model : 0.0678800106048584
			 train-loss:  2.07194332386318 	 ± 0.24504181814456333
	data : 0.11803669929504394
	model : 0.0680762767791748
			 train-loss:  2.072019061231925 	 ± 0.24424150002402573
	data : 0.11779556274414063
	model : 0.06835813522338867
			 train-loss:  2.0718008047574528 	 ± 0.2434621850941798
	data : 0.11768512725830078
	model : 0.06922144889831543
			 train-loss:  2.0717558399323495 	 ± 0.24267619391095344
	data : 0.11667098999023437
	model : 0.0691983699798584
			 train-loss:  2.072284120779771 	 ± 0.24198653158814323
	data : 0.11656761169433594
	model : 0.07012271881103516
			 train-loss:  2.071841619576618 	 ± 0.24127795138011612
	data : 0.11588497161865234
	model : 0.06982331275939942
			 train-loss:  2.0742264141010334 	 ± 0.24236232709062805
	data : 0.11617307662963867
	model : 0.06967597007751465
			 train-loss:  2.0744890881784306 	 ± 0.24162153964893573
	data : 0.1162109375
	model : 0.06982231140136719
			 train-loss:  2.074215291440487 	 ± 0.24089003040728435
	data : 0.11624727249145508
	model : 0.06974496841430664
			 train-loss:  2.0744233101791476 	 ± 0.2401551744908951
	data : 0.1161046028137207
	model : 0.06971077919006348
			 train-loss:  2.0745012804313943 	 ± 0.23941485152615058
	data : 0.11623282432556152
	model : 0.06993942260742188
			 train-loss:  2.07363493822835 	 ± 0.23893389638524282
	data : 0.11603245735168458
	model : 0.0697861671447754
			 train-loss:  2.0727841636029685 	 ± 0.23845184618504187
	data : 0.11609120368957519
	model : 0.06957225799560547
			 train-loss:  2.072961834705237 	 ± 0.23773905490177136
	data : 0.11632218360900878
	model : 0.06869254112243653
			 train-loss:  2.074296299233494 	 ± 0.23764092329010453
	data : 0.11736030578613281
	model : 0.06786184310913086
			 train-loss:  2.0762591204957332 	 ± 0.23827418716792315
	data : 0.11794462203979492
	model : 0.06771306991577149
			 train-loss:  2.0789429190612974 	 ± 0.2400822935461946
	data : 0.11805291175842285
	model : 0.0678466796875
			 train-loss:  2.080441659724219 	 ± 0.24015788777252653
	data : 0.11800837516784668
	model : 0.06793861389160157
			 train-loss:  2.080768616059247 	 ± 0.23948822049894075
	data : 0.11795406341552735
	model : 0.06826114654541016
			 train-loss:  2.0796317831117506 	 ± 0.2392465398922746
	data : 0.1175081729888916
	model : 0.06920375823974609
			 train-loss:  2.0795224613921586 	 ± 0.2385543254062566
	data : 0.11671080589294433
	model : 0.06934137344360351
			 train-loss:  2.079619075521568 	 ± 0.23786723761484133
	data : 0.11647439002990723
	model : 0.06859478950500489
			 train-loss:  2.079529745825406 	 ± 0.23718563635442735
	data : 0.11735353469848633
	model : 0.06853685379028321
			 train-loss:  2.080672803606306 	 ± 0.23698713568897975
	data : 0.11763629913330079
	model : 0.06824188232421875
			 train-loss:  2.0778409032659098 	 ± 0.23926394525866052
	data : 0.1180849552154541
	model : 0.0681839942932129
			 train-loss:  2.0774024415150874 	 ± 0.23865799916222377
	data : 0.11800460815429688
	model : 0.06826157569885254
			 train-loss:  2.0784763867935436 	 ± 0.2384151804457757
	data : 0.11813201904296874
	model : 0.0690164566040039
			 train-loss:  2.080267212910359 	 ± 0.2389458153588906
	data : 0.11723628044128417
	model : 0.06834421157836915
			 train-loss:  2.080990418460634 	 ± 0.23847752381432646
	data : 0.11753706932067871
	model : 0.06923198699951172
			 train-loss:  2.0809260554076556 	 ± 0.23781940141317895
	data : 0.11635384559631348
	model : 0.06909050941467285
			 train-loss:  2.0832528471946716 	 ± 0.23922214746651702
	data : 0.11659669876098633
	model : 0.06834025382995605
			 train-loss:  2.0835586278164975 	 ± 0.23860330263494478
	data : 0.11713113784790039
	model : 0.06774821281433105
			 train-loss:  2.0832794159650803 	 ± 0.23798401637635022
	data : 0.1176985740661621
	model : 0.06847267150878907
			 train-loss:  2.0845186278626726 	 ± 0.23793446214930397
	data : 0.1172633171081543
	model : 0.06886181831359864
			 train-loss:  2.083197449484179 	 ± 0.23797343994286696
	data : 0.11713685989379882
	model : 0.06907529830932617
			 train-loss:  2.0853884940478893 	 ± 0.23921003829080487
	data : 0.1170456886291504
	model : 0.0697822093963623
			 train-loss:  2.0864561574256166 	 ± 0.2390193199220536
	data : 0.11630825996398926
	model : 0.07010979652404785
			 train-loss:  2.0860285525599487 	 ± 0.2384582436233824
	data : 0.11592040061950684
	model : 0.07027587890625
			 train-loss:  2.0836298679050644 	 ± 0.24010519685338289
	data : 0.11576132774353028
	model : 0.06977849006652832
			 train-loss:  2.0856523064418613 	 ± 0.2410929683733052
	data : 0.11614279747009278
	model : 0.06956124305725098
			 train-loss:  2.085198494916161 	 ± 0.24054607922893304
	data : 0.11616759300231934
	model : 0.06922106742858887
			 train-loss:  2.086725989153966 	 ± 0.24085388264898785
	data : 0.11666769981384277
	model : 0.06910233497619629
			 train-loss:  2.085511384551058 	 ± 0.2408241996370448
	data : 0.11672234535217285
	model : 0.06899280548095703
			 train-loss:  2.0842251484210674 	 ± 0.24087306302042685
	data : 0.11692271232604981
	model : 0.06923766136169433
			 train-loss:  2.0848959915492 	 ± 0.24044036450506337
	data : 0.11681828498840333
	model : 0.06929340362548828
			 train-loss:  2.084420627748906 	 ± 0.23992165264593926
	data : 0.11687593460083008
	model : 0.06950750350952148
			 train-loss:  2.084039320849409 	 ± 0.23937485879869166
	data : 0.11674275398254394
	model : 0.06992688179016113
			 train-loss:  2.08326939961419 	 ± 0.2390183086866604
	data : 0.11649894714355469
	model : 0.06992096900939941
			 train-loss:  2.084771066904068 	 ± 0.23935924685134424
	data : 0.11645016670227051
	model : 0.06931214332580567
			 train-loss:  2.0853654650313342 	 ± 0.23891101236645867
	data : 0.11690077781677247
	model : 0.06943182945251465
			 train-loss:  2.0843332014461553 	 ± 0.23876784609261187
	data : 0.11677799224853516
	model : 0.0695098876953125
			 train-loss:  2.08427892177563 	 ± 0.23818027127150113
	data : 0.11661000251770019
	model : 0.06957569122314453
			 train-loss:  2.0850937541793373 	 ± 0.23787924749501477
	data : 0.11653976440429688
	model : 0.06972861289978027
			 train-loss:  2.08348466477743 	 ± 0.23840867207266295
	data : 0.11634116172790528
	model : 0.07015137672424317
			 train-loss:  2.083415060367399 	 ± 0.23783139427616484
	data : 0.11600008010864257
	model : 0.07006301879882812
			 train-loss:  2.082066109215004 	 ± 0.23804488867275894
	data : 0.11606621742248535
	model : 0.06985130310058593
			 train-loss:  2.084834823814722 	 ± 0.24078985845575443
	data : 0.11637921333312988
	model : 0.06973886489868164
			 train-loss:  2.08320380283885 	 ± 0.24136211085236814
	data : 0.11644697189331055
	model : 0.06957578659057617
			 train-loss:  2.0822182405562626 	 ± 0.24120793769960774
	data : 0.1166043758392334
	model : 0.06963868141174316
			 train-loss:  2.081811794172531 	 ± 0.24070774876627823
	data : 0.11659598350524902
	model : 0.06963324546813965
			 train-loss:  2.0802120875637486 	 ± 0.24126101904477373
	data : 0.11649198532104492
	model : 0.06982302665710449
			 train-loss:  2.0792693448178645 	 ± 0.2410851001623138
	data : 0.1161740779876709
	model : 0.0698582649230957
			 train-loss:  2.079882678027465 	 ± 0.24068766712176157
	data : 0.11620802879333496
	model : 0.06983318328857421
			 train-loss:  2.0797505761301793 	 ± 0.2401350519322047
	data : 0.11616110801696777
	model : 0.06985273361206054
			 train-loss:  2.0789881497621536 	 ± 0.23983922636383587
	data : 0.11626334190368652
	model : 0.06996564865112305
			 train-loss:  2.080496111773126 	 ± 0.2403101023680036
	data : 0.11609573364257812
	model : 0.06991724967956543
			 train-loss:  2.08122832239221 	 ± 0.2400007971030392
	data : 0.11619529724121094
	model : 0.06933484077453614
			 train-loss:  2.0799711539865084 	 ± 0.24017058551714882
	data : 0.11679816246032715
	model : 0.06889057159423828
			 train-loss:  2.0790781183676286 	 ± 0.23998828077639905
	data : 0.1172271728515625
	model : 0.06808719635009766
			 train-loss:  2.0804755396433006 	 ± 0.24034013365960935
	data : 0.11792802810668945
	model : 0.06721911430358887
			 train-loss:  2.0817508751207643 	 ± 0.24054653739141105
	data : 0.11890277862548829
	model : 0.06643795967102051
			 train-loss:  2.081349574396963 	 ± 0.24008105782878286
	data : 0.11935877799987793
	model : 0.06632604598999023
			 train-loss:  2.0818326340190003 	 ± 0.2396531536154236
	data : 0.1192892074584961
	model : 0.06599316596984864
			 train-loss:  2.080577596028646 	 ± 0.23985662467839888
	data : 0.11945652961730957
	model : 0.06593623161315917
			 train-loss:  2.079374674674684 	 ± 0.2400046187468807
	data : 0.11936478614807129
	model : 0.06583952903747559
			 train-loss:  2.0777049679062967 	 ± 0.24078731867836903
	data : 0.11944098472595215
	model : 0.06586246490478516
			 train-loss:  2.0774697666628317 	 ± 0.24028482802809528
	data : 0.11947627067565918
	model : 0.06616530418395997
			 train-loss:  2.0771828712854843 	 ± 0.2397987474212862
	data : 0.11898903846740723
	model : 0.06663780212402344
			 train-loss:  2.079821751946988 	 ± 0.24258629306671173
	data : 0.11852049827575684
	model : 0.06716022491455079
			 train-loss:  2.079501496248947 	 ± 0.242109366849975
	data : 0.1179114818572998
	model : 0.06781935691833496
			 train-loss:  2.0796359371522377 	 ± 0.24159565692480103
	data : 0.11717791557312011
	model : 0.06801924705505372
			 train-loss:  2.0793583214027174 	 ± 0.2411137353692737
	data : 0.1169816493988037
	model : 0.06820135116577149
			 train-loss:  2.080006294270866 	 ± 0.24080120242751202
	data : 0.1172450065612793
	model : 0.06821050643920898
			 train-loss:  2.079646962246996 	 ± 0.24035117524400607
	data : 0.11710290908813477
	model : 0.06837544441223145
			 train-loss:  2.0809011939218487 	 ± 0.24061085321408396
	data : 0.11712555885314942
	model : 0.06831874847412109
			 train-loss:  2.0798810795892644 	 ± 0.2406135811037826
	data : 0.11720118522644044
	model : 0.06877741813659669
			 train-loss:  2.0808932661008432 	 ± 0.24061265792165518
	data : 0.11698422431945801
	model : 0.06888771057128906
			 train-loss:  2.081572530658674 	 ± 0.24033732177013445
	data : 0.11680078506469727
	model : 0.0691075325012207
			 train-loss:  2.081551414728165 	 ± 0.23983631852560452
	data : 0.11691460609436036
	model : 0.0686863899230957
			 train-loss:  2.0832620883878334 	 ± 0.24080099351277143
	data : 0.11728630065917969
	model : 0.06842274665832519
			 train-loss:  2.0825517295805875 	 ± 0.24055585935742796
	data : 0.11760892868041992
	model : 0.06852598190307617
			 train-loss:  2.0832775365177985 	 ± 0.24032575752649815
	data : 0.11744985580444336
	model : 0.06825947761535645
			 train-loss:  2.0828820633106546 	 ± 0.23991200023866227
	data : 0.11747245788574219
	model : 0.06813516616821289
			 train-loss:  2.08191902540168 	 ± 0.23989400585515483
	data : 0.11756372451782227
	model : 0.06856794357299804
			 train-loss:  2.0807402536151853 	 ± 0.2401158526471734
	data : 0.11709117889404297
	model : 0.06861343383789062
			 train-loss:  2.0810081220348837 	 ± 0.2396661229240214
	data : 0.11676692962646484
	model : 0.06836323738098145
			 train-loss:  2.08152286900628 	 ± 0.23931921025050754
	data : 0.11679048538208008
	model : 0.06792716979980469
			 train-loss:  2.0818521052479264 	 ± 0.2388944367934457
	data : 0.11723766326904297
	model : 0.06733016967773438
			 train-loss:  2.082542054653168 	 ± 0.2386646208351807
	data : 0.1176145076751709
	model : 0.06721997261047363
			 train-loss:  2.0838607160218685 	 ± 0.23909952473107346
	data : 0.11764640808105468
	model : 0.06748919486999512
			 train-loss:  2.084571994959362 	 ± 0.23889057876111036
	data : 0.1177135944366455
	model : 0.06757221221923829
			 train-loss:  2.083942568349273 	 ± 0.23862727719739255
	data : 0.11785621643066406
	model : 0.06795663833618164
			 train-loss:  2.083897358319891 	 ± 0.2381581608735838
	data : 0.11737661361694336
	model : 0.06866245269775391
			 train-loss:  2.0828286484176037 	 ± 0.2383001987050344
	data : 0.11681122779846191
	model : 0.06895508766174316
			 train-loss:  2.0810770629905164 	 ± 0.23947340912615211
	data : 0.11565518379211426
	model : 0.060167884826660155
#epoch  99    val-loss:  2.5298000260403284  train-loss:  2.0810770629905164  lr:  6.103515625e-07
