model 9d7b33644c75a8c3887d49f23c54dadda00431dfc760ece6260e1b06 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=181.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  2.1715297322524223  train-loss:  2.031997865997255  lr:  0.0025
#epoch  8    val-loss:  2.1409168557116858  train-loss:  2.0295801516622305  lr:  0.0025
#epoch  9    val-loss:  2.152196633188348  train-loss:  2.0294179641641676  lr:  0.00125
#epoch  10    val-loss:  2.1924401207974085  train-loss:  2.0272208633832633  lr:  0.00125
#epoch  11    val-loss:  2.1680216287311755  train-loss:  2.029353921767324  lr:  0.00125
#epoch  12    val-loss:  2.164704103218882  train-loss:  2.0262113828212023  lr:  0.000625
#epoch  13    val-loss:  2.1615058748345626  train-loss:  2.029096297454089  lr:  0.000625
#epoch  14    val-loss:  2.1362947413795874  train-loss:  2.024056612048298  lr:  0.000625
#epoch  15    val-loss:  2.1282823713202226  train-loss:  2.024930181913078  lr:  0.000625
#epoch  16    val-loss:  2.1614219326721993  train-loss:  2.0293776248581707  lr:  0.000625
#epoch  17    val-loss:  2.169009992950841  train-loss:  2.0300499401055276  lr:  0.000625
#epoch  18    val-loss:  2.14737722120787  train-loss:  2.0337051865644753  lr:  0.000625
#epoch  19    val-loss:  2.139546933927034  train-loss:  2.0260360930114985  lr:  0.0003125
#epoch  20    val-loss:  2.1304333272733187  train-loss:  2.0256405589170754  lr:  0.0003125
#epoch  21    val-loss:  2.144081529818083  train-loss:  2.022664454765618  lr:  0.0003125
#epoch  22    val-loss:  2.1202635890559147  train-loss:  2.025555979926139  lr:  0.00015625
#epoch  23    val-loss:  2.1415526490462455  train-loss:  2.0263389288447797  lr:  0.00015625
#epoch  24    val-loss:  2.177265349187349  train-loss:  2.023197599221021  lr:  0.00015625
#epoch  25    val-loss:  2.1554379086745414  train-loss:  2.0245138527825475  lr:  0.00015625
#epoch  26    val-loss:  2.1655911646391215  train-loss:  2.0248082550242543  lr:  7.8125e-05
#epoch  27    val-loss:  2.1531901924233687  train-loss:  2.0245675528422  lr:  7.8125e-05
#epoch  28    val-loss:  2.164594612623516  train-loss:  2.0262662582099438  lr:  7.8125e-05
#epoch  29    val-loss:  2.1953143759777674  train-loss:  2.0223841699771583  lr:  3.90625e-05
#epoch  30    val-loss:  2.1434742212295532  train-loss:  2.020744090899825  lr:  3.90625e-05
#epoch  31    val-loss:  2.15067069781454  train-loss:  2.02855429565534  lr:  3.90625e-05
#epoch  32    val-loss:  2.1487232258445337  train-loss:  2.026461689732969  lr:  1.953125e-05
#epoch  33    val-loss:  2.1909132254751107  train-loss:  2.0286607779562473  lr:  1.953125e-05
#epoch  34    val-loss:  2.1382462099978796  train-loss:  2.028226829133928  lr:  1.953125e-05
#epoch  35    val-loss:  2.1745137854626306  train-loss:  2.026026525069028  lr:  9.765625e-06
#epoch  36    val-loss:  2.192903888852973  train-loss:  2.0300724948756397  lr:  9.765625e-06
#epoch  37    val-loss:  2.1359512994163916  train-loss:  2.023800684604794  lr:  9.765625e-06
#epoch  38    val-loss:  2.1950941713232743  train-loss:  2.0271610328927636  lr:  4.8828125e-06
#epoch  39    val-loss:  2.1522125005722046  train-loss:  2.023715518414974  lr:  4.8828125e-06
#epoch  40    val-loss:  2.132752487533971  train-loss:  2.0242259837687016  lr:  4.8828125e-06
#epoch  41    val-loss:  2.1254396501340365  train-loss:  2.024755463935435  lr:  2.44140625e-06
#epoch  42    val-loss:  2.1460428614365425  train-loss:  2.0268531907349825  lr:  2.44140625e-06
#epoch  43    val-loss:  2.1147949946554085  train-loss:  2.026560479775071  lr:  2.44140625e-06
#epoch  44    val-loss:  2.1337683263577913  train-loss:  2.0229565636254847  lr:  2.44140625e-06
#epoch  45    val-loss:  2.1690216942837366  train-loss:  2.02602779539302  lr:  2.44140625e-06
#epoch  46    val-loss:  2.138117978447362  train-loss:  2.0235944469459355  lr:  2.44140625e-06
#epoch  47    val-loss:  2.152151892059728  train-loss:  2.027967523317784  lr:  1.220703125e-06
#epoch  48    val-loss:  2.155076873929877  train-loss:  2.02826606342569  lr:  1.220703125e-06
#epoch  49    val-loss:  2.1729195557142558  train-loss:  2.0268717538565397  lr:  1.220703125e-06
#epoch  50    val-loss:  2.1301472187042236  train-loss:  2.030598131008446  lr:  6.103515625e-07
#epoch  51    val-loss:  2.1356922074368128  train-loss:  2.021920043975115  lr:  6.103515625e-07
#epoch  52    val-loss:  2.135671069747523  train-loss:  2.0286854985170066  lr:  6.103515625e-07
#epoch  53    val-loss:  2.131170153617859  train-loss:  2.0251434398815036  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.180705133237337  train-loss:  2.027686250861734  lr:  3.0517578125e-07
#epoch  55    val-loss:  2.1408022955844275  train-loss:  2.0229067211039364  lr:  3.0517578125e-07
#epoch  56    val-loss:  2.1404798909237512  train-loss:  2.0295651839114726  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.152938020856757  train-loss:  2.0207734000869095  lr:  1.52587890625e-07
#epoch  58    val-loss:  2.156055368875202  train-loss:  2.033739762380719  lr:  1.52587890625e-07
#epoch  59    val-loss:  2.147606661445216  train-loss:  2.0304188723675907  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.1459077282955774  train-loss:  2.0238553676754236  lr:  7.62939453125e-08
#epoch  61    val-loss:  2.142186346806978  train-loss:  2.028105622623116  lr:  7.62939453125e-08
#epoch  62    val-loss:  2.1384479434866654  train-loss:  2.0252363099716604  lr:  3.814697265625e-08
#epoch  63    val-loss:  2.171177362140856  train-loss:  2.025121751241386  lr:  3.814697265625e-08
#epoch  64    val-loss:  2.1553531006762854  train-loss:  2.0259080603718758  lr:  3.814697265625e-08
#epoch  65    val-loss:  2.1387829404128227  train-loss:  2.027345395181328  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.1504751381121183  train-loss:  2.030381674412638  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.151242450663918  train-loss:  2.025251168292016  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.1670896442312944  train-loss:  2.023849335964769  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.1710467150336816  train-loss:  2.0268041756935418  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.1578125075290076  train-loss:  2.0310075362212956  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.1723159300653556  train-loss:  2.0253175105899572  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.1450666126451994  train-loss:  2.0244019916281104  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.1521017927872506  train-loss:  2.0282096904702485  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.136537087591071  train-loss:  2.0286278179846704  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.178428097775108  train-loss:  2.034219907131046  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.1295418990285775  train-loss:  2.0237680450081825  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.148957873645582  train-loss:  2.0276857833378017  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.1195943355560303  train-loss:  2.02621420705691  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.2169391230532995  train-loss:  2.023966869805008  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.149076399050261  train-loss:  2.0260199611075222  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.169390559196472  train-loss:  2.0267369975335896  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.1156902940649736  train-loss:  2.0289382468909025  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.1511823691819845  train-loss:  2.028925863560289  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.179199212475827  train-loss:  2.030447970610112  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.155902780984577  train-loss:  2.02724123140797  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.156924580272875  train-loss:  2.023263123817742  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.1555083362679732  train-loss:  2.029365377034992  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.161084614301983  train-loss:  2.027105035725981  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.163876125687047  train-loss:  2.0272441362030804  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.173368548092089  train-loss:  2.0232173101976514  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.149037474080136  train-loss:  2.026438148226589  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.1642525321558903  train-loss:  2.02571641234681  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.225105593079015  train-loss:  2.0207625022158027  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.1602493712776587  train-loss:  2.023658139165491  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.1499687684209725  train-loss:  2.031414341647178  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.15445003384038  train-loss:  2.026996224652976  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.1413510975084806  train-loss:  2.024710075929761  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.126781607929029  train-loss:  2.021622759755701  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.159839492095144  train-loss:  2.0264870966784656  lr:  1.9073486328125e-08
