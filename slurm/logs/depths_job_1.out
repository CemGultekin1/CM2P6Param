model 7ef8fe75a8d1104d4916e40d16a42aafdcd8e091f5f40763229c39f6 state_dict has not been found
Model was not found
Namespace(co2=False, depth=5.0, disp=1, domain='global', epoch=0, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=24, mode='train', normalization='standard', num_workers=40, parts=[2, 3], persistent_workers=False, prefetch_factor=1, relog=True, rerun=True, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
epochs started
			 train-loss:  2.722835063934326 	 ± 0.0
	data : 6.875035524368286
	model : 3.997807264328003
			 train-loss:  3.1257532835006714 	 ± 0.4029182195663452
	data : 3.445780873298645
	model : 3.8997902870178223
			 train-loss:  3.0112318197886148 	 ± 0.3666865951651984
	data : 2.305821975072225
	model : 3.8791512648264566
			 train-loss:  3.031363308429718 	 ± 0.31946850362767404
	data : 1.732222855091095
	model : 3.8844597339630127
			 train-loss:  2.956558609008789 	 ± 0.322538481533454
	data : 1.3896625518798829
	model : 3.8745826721191405
			 train-loss:  2.9406511783599854 	 ± 0.29657678905721485
	data : 0.02436089515686035
	model : 3.887067127227783
			 train-loss:  2.958699566977365 	 ± 0.27811303271862375
	data : 0.02404165267944336
	model : 3.9119515895843504
			 train-loss:  2.9030688107013702 	 ± 0.2989012694918343
	data : 0.024352312088012695
	model : 3.9678555965423583
			 train-loss:  2.915565093358358 	 ± 0.28401468291624976
	data : 0.025238752365112305
	model : 3.886203145980835
			 train-loss:  2.8717641592025758 	 ± 0.2997742527254275
	data : 0.024368476867675782
	model : 4.000108814239502
			 train-loss:  2.8744103691794654 	 ± 0.28594600459264746
	data : 0.01778068542480469
	model : 3.9299043655395507
			 train-loss:  2.8318536480267844 	 ± 0.308014898041033
	data : 0.015477943420410156
	model : 4.000757551193237
			 train-loss:  2.799925290621244 	 ± 0.31592448586706706
	data : 0.010726261138916015
	model : 4.009680891036988
			 train-loss:  2.774585638727461 	 ± 0.31784649017014005
	data : 0.00975189208984375
	model : 4.070137023925781
			 train-loss:  2.752854919433594 	 ± 0.31765143918082417
	data : 0.009924077987670898
	model : 4.019944763183593
			 train-loss:  2.7248438596725464 	 ± 0.32613697512985285
	data : 0.007605791091918945
	model : 4.1120692729949955
			 train-loss:  2.718808482674991 	 ± 0.3173190114701159
	data : 0.009411954879760742
	model : 4.010107707977295
			 train-loss:  2.7249190542432995 	 ± 0.30940613328538935
	data : 0.011820316314697266
	model : 3.9556270599365235
			 train-loss:  2.6985470621209395 	 ± 0.32126670580283606
	data : 0.012756681442260743
	model : 3.9556077003479
			 train-loss:  2.685926342010498 	 ± 0.3179277439773881
	data : 0.010203886032104491
	model : 3.9107705116271974
			 train-loss:  2.6716944036029635 	 ± 0.31672665077710255
	data : 0.013350248336791992
	model : 3.7966263771057127
			 train-loss:  2.640945851802826 	 ± 0.3400160374445156
	data : 0.014005374908447266
	model : 3.84065957069397
			 train-loss:  2.6172167892041416 	 ± 0.3506734230892081
	data : 0.013994455337524414
	model : 3.8856608390808107
			 train-loss:  2.60124970972538 	 ± 0.35172690828930775
	data : 0.011434125900268554
	model : 3.8051785945892336
			 train-loss:  2.595954270362854 	 ± 0.3455956388138853
	data : 0.011453723907470703
	model : 3.8752795696258544
			 train-loss:  2.5902386491115275 	 ± 0.34008726545646367
	data : 0.010601615905761719
	model : 3.8687453269958496
			 train-loss:  2.5684752066930137 	 ± 0.3516966045314473
	data : 0.00894923210144043
	model : 3.8597265243530274
			 train-loss:  2.5536787807941437 	 ± 0.353813803640492
	data : 0.00891861915588379
	model : 3.7770339488983153
			 train-loss:  2.543926950158744 	 ± 0.35146871902335314
	data : 0.009030628204345702
	model : 3.8680670261383057
			 train-loss:  2.5277821024258933 	 ± 0.3563307624170426
	data : 0.011474800109863282
	model : 3.761745500564575
			 train-loss:  2.5254929950160365 	 ± 0.3507605367871646
	data : 0.012031364440917968
	model : 3.7831188678741454
			 train-loss:  2.507082983851433 	 ± 0.36013186222965615
	data : 0.011214923858642579
	model : 3.740255880355835
			 train-loss:  2.501726844094016 	 ± 0.3559253171719576
	data : 0.008719301223754883
	model : 3.7769307613372805
			 train-loss:  2.4828781766050003 	 ± 0.3669888970474579
	data : 0.011507081985473632
	model : 3.7224231719970704
			 train-loss:  2.4765698875699726 	 ± 0.3635737058880742
	data : 0.01014242172241211
	model : 3.7900619506835938
			 train-loss:  2.478327807452944 	 ± 0.35863933222119804
	data : 0.00960235595703125
	model : 3.790737247467041
			 train-loss:  2.4676056327046574 	 ± 0.3595617288624191
	data : 0.009597253799438477
	model : 3.8590341567993165
			 train-loss:  2.4659657760670313 	 ± 0.35493930097832005
	data : 0.009863138198852539
	model : 3.7765222072601317
			 train-loss:  2.4585044047771354 	 ± 0.35336544879139564
	data : 0.00963730812072754
	model : 3.772685146331787
			 train-loss:  2.452864834666252 	 ± 0.35069338057617094
	data : 0.011954498291015626
	model : 3.697749900817871
			 train-loss:  2.4570701616566355 	 ± 0.3474098204923374
	data : 0.012281274795532227
	model : 3.694693851470947
			 train-loss:  2.46163862375986 	 ± 0.3444932964703933
	data : 0.012634181976318359
	model : 3.6493818759918213
			 train-loss:  2.4535252909327663 	 ± 0.3445002641973789
	data : 0.012744474411010741
	model : 3.684241771697998
			 train-loss:  2.4491755826906725 	 ± 0.3417553333057264
	data : 0.010482692718505859
	model : 3.7358136653900145
			 train-loss:  2.442461551560296 	 ± 0.3408587197993683
	data : 0.009814119338989258
	model : 3.6873284339904786
			 train-loss:  2.4359540861585867 	 ± 0.3399478386513908
	data : 0.011743307113647461
	model : 3.6926671504974364
			 train-loss:  2.430360679930829 	 ± 0.33844479319547727
	data : 0.01460580825805664
	model : 3.6660250186920167
			 train-loss:  2.4192782243092856 	 ± 0.3434109856682458
	data : 0.014678287506103515
	model : 3.7242781639099123
			 train-loss:  2.4091249996302078 	 ± 0.3470915879959893
	data : 0.014647912979125977
	model : 3.7214893341064452
			 train-loss:  2.405724833011627 	 ± 0.3444265009651514
	data : 0.015708780288696288
	model : 3.8099503993988035
			 train-loss:  2.407682640879762 	 ± 0.3413139233545261
	data : 0.011366605758666992
	model : 3.7578713417053224
			 train-loss:  2.407322959258006 	 ± 0.33802588657138516
	data : 0.011005592346191407
	model : 3.794059705734253
			 train-loss:  2.4044273416951016 	 ± 0.3354722371369881
	data : 0.011794424057006836
	model : 3.7527685165405273
			 train-loss:  2.4020550140628107 	 ± 0.33279993767360966
	data : 0.01436619758605957
	model : 3.745792865753174
			 train-loss:  2.3927237575704403 	 ± 0.33681443481773854
	data : 0.01087646484375
	model : 3.700725269317627
			 train-loss:  2.3919196277856827 	 ± 0.3338468852605257
	data : 0.013959121704101563
	model : 3.7632484912872313
			 train-loss:  2.3796993284894707 	 ± 0.3433092104953172
	data : 0.015244150161743164
	model : 3.7651788234710692
			 train-loss:  2.3795989361302605 	 ± 0.3403376244199985
	data : 0.017251873016357423
	model : 3.8005229949951174
			 train-loss:  2.3745077323105375 	 ± 0.3396613961473686
	data : 0.017218828201293945
	model : 3.8589194297790526
			 train-loss:  2.367068576812744 	 ± 0.3416316056840575
	data : 0.018129539489746094
	model : 3.9013091564178466
			 train-loss:  2.3620221067647464 	 ± 0.3410672244572181
	data : 0.01492447853088379
	model : 3.8724292278289796
			 train-loss:  2.3692725935289936 	 ± 0.3430121792509149
	data : 0.01505107879638672
	model : 3.797664260864258
			 train-loss:  2.375241044967894 	 ± 0.3435089047303145
	data : 0.016220903396606444
	model : 3.7708515167236327
			 train-loss:  2.3671112209558487 	 ± 0.34686966941234076
	data : 0.017848443984985352
	model : 3.732508134841919
			 train-loss:  2.3635043657743013 	 ± 0.3453984866034553
	data : 0.019355058670043945
	model : 3.7111289501190186
			 train-loss:  2.36294381907492 	 ± 0.34280163487642323
	data : 0.021893644332885744
	model : 3.8119842529296877
			 train-loss:  2.361134589608036 	 ± 0.3405511336469146
	data : 0.017821788787841797
	model : 3.8644625186920165
			 train-loss:  2.3575160994249234 	 ± 0.3393329064438001
	data : 0.01626100540161133
	model : 3.8744131088256837
			 train-loss:  2.362359855485999 	 ± 0.33922476476755553
	data : 0.012095069885253907
	model : 3.8845787525177
			 train-loss:  2.3541630608694892 	 ± 0.34360656022278385
	data : 0.012605810165405273
	model : 3.8638936996459963
			 train-loss:  2.3505357789321684 	 ± 0.3425252932370462
	data : 0.010010290145874023
	model : 3.818685531616211
			 train-loss:  2.3445640338791742 	 ± 0.34384017487257845
	data : 0.012614822387695313
	model : 3.8702038288116456
			 train-loss:  2.3458071800127422 	 ± 0.3416398694443677
	data : 0.011343717575073242
	model : 3.8612164497375487
			 train-loss:  2.3412414144825293 	 ± 0.3415586428522715
	data : 0.013935708999633789
	model : 3.7951236724853517
			 train-loss:  2.339206600189209 	 ± 0.33972518865754603
	data : 0.011803102493286134
	model : 3.7922333240509034
			 train-loss:  2.336476583229868 	 ± 0.3383098926521722
	data : 0.014386558532714843
	model : 3.7245243072509764
			 train-loss:  2.3435903493460124 	 ± 0.3417794670345664
	data : 0.014372968673706054
	model : 3.673034143447876
			 train-loss:  2.3394379187852907 	 ± 0.3415307969013646
	data : 0.01708683967590332
	model : 3.61260404586792
			 train-loss:  2.3370000561581383 	 ± 0.34004463756600267
	data : 0.017791128158569335
	model : 3.662983703613281
			 train-loss:  2.328666257858276 	 ± 0.34593597590782077
	data : 0.019488191604614256
	model : 3.679359769821167
			 train-loss:  2.329058023146641 	 ± 0.34381179363512016
	data : 0.01693410873413086
	model : 3.7202831745147704
			 train-loss:  2.3296328259677423 	 ± 0.34174810667748823
	data : 0.015510082244873047
	model : 3.687143325805664
			 train-loss:  2.3312997961618813 	 ± 0.3400183810137099
	data : 0.011487436294555665
	model : 3.748860502243042
			 train-loss:  2.329939231986091 	 ± 0.3382156181240514
	data : 0.010795402526855468
	model : 3.7101595878601072
			 train-loss:  2.330506372451782 	 ± 0.33626040563357396
	data : 0.010734748840332032
	model : 3.671428346633911
			 train-loss:  2.3290853112242944 	 ± 0.3345563194752748
	data : 0.01323714256286621
	model : 3.6766461372375487
			 train-loss:  2.331154036795956 	 ± 0.33318080789815313
	data : 0.01618847846984863
	model : 3.6591346740722654
			 train-loss:  2.327498723160137 	 ± 0.333032152151901
	data : 0.020023250579833986
	model : 3.691944789886475
			 train-loss:  2.325655272837435 	 ± 0.33160711864487696
	data : 0.01746983528137207
	model : 3.7171477317810058
			 train-loss:  2.3204122914208307 	 ± 0.33344860911258306
	data : 0.016770553588867188
	model : 3.7880826950073243
			 train-loss:  2.3150765542145613 	 ± 0.33545258573899395
	data : 0.01895275115966797
	model : 3.82252984046936
			 train-loss:  2.3122518956661224 	 ± 0.33471086540393574
	data : 0.016863489151000978
	model : 3.92893648147583
			 train-loss:  2.312814721497156 	 ± 0.3329502488531773
	data : 0.015539979934692383
	model : 3.8561709403991697
			 train-loss:  2.311189224111273 	 ± 0.33154528943623035
	data : 0.01797909736633301
	model : 3.9123579978942873
			 train-loss:  2.3106720861635712 	 ± 0.3298338080042871
	data : 0.020452260971069336
	model : 3.8902924060821533
			 train-loss:  2.308371674269438 	 ± 0.3288766306667699
	data : 0.01836991310119629
	model : 3.863310384750366
			 train-loss:  2.3044494660859254 	 ± 0.32942620593348587
	data : 0.02057809829711914
	model : 3.8137913227081297
			 train-loss:  2.304246356292647 	 ± 0.32774725510945524
	data : 0.02055354118347168
	model : 3.880820608139038
			 train-loss:  2.303472450285247 	 ± 0.3261777513253434
	data : 0.02260246276855469
	model : 3.824134588241577
			 train-loss:  2.302974897623062 	 ± 0.32458052084432615
	data : 0.020843887329101564
	model : 3.8156839847564696
			 train-loss:  2.3037055329521103 	 ± 0.32305232255301075
	data : 0.020852327346801758
	model : 3.8251302242279053
			 train-loss:  2.299921315090329 	 ± 0.3237066405836005
	data : 0.01914067268371582
	model : 3.8266599655151365
			 train-loss:  2.2954786703424546 	 ± 0.3252411918525253
	data : 0.019161033630371093
	model : 3.827086353302002
			 train-loss:  2.2932940285939436 	 ± 0.3244322484045844
	data : 0.01710829734802246
	model : 3.8086748123168945
			 train-loss:  2.2925261225019185 	 ± 0.32297859016101077
	data : 0.018417501449584962
	model : 3.82987003326416
			 train-loss:  2.2911650054859667 	 ± 0.3217539300126887
	data : 0.018506813049316406
	model : 3.8127957820892333
			 train-loss:  2.292926561052554 	 ± 0.32076001754549976
	data : 0.020563411712646484
	model : 3.8124116897583007
			 train-loss:  2.2958110946196095 	 ± 0.32066279590687224
	data : 0.021309041976928712
	model : 3.7397429943084717
			 train-loss:  2.2966487451430853 	 ± 0.3193071602215011
	data : 0.02130999565124512
	model : 3.7468847751617433
			 train-loss:  2.294222838228399 	 ± 0.31885991883087433
	data : 0.020146703720092772
	model : 3.691661071777344
			 train-loss:  2.2955467743916556 	 ± 0.3177239301840366
	data : 0.01991925239562988
	model : 3.6623114585876464
			 train-loss:  2.295454738395555 	 ± 0.316303825767976
	data : 0.01788792610168457
	model : 3.6674896717071532
			 train-loss:  2.295206532014155 	 ± 0.3149120966348762
	data : 0.017212963104248045
	model : 3.69888596534729
			 train-loss:  2.2939970639714025 	 ± 0.3137913597393227
	data : 0.01717853546142578
	model : 3.7189913272857664
			 train-loss:  2.29062805072121 	 ± 0.3144880418011978
	data : 0.016996431350708007
	model : 3.754608154296875
			 train-loss:  2.2862024964957404 	 ± 0.3167056278018308
	data : 0.018099355697631835
	model : 3.810028886795044
			 train-loss:  2.2861604609041133 	 ± 0.315349605540987
	data : 0.01798868179321289
	model : 3.826561975479126
			 train-loss:  2.286300546031887 	 ± 0.3140141912400245
	data : 0.017888355255126952
	model : 3.8596898555755614
			 train-loss:  2.285806846217949 	 ± 0.3127380067300161
	data : 0.015341424942016601
	model : 3.833166742324829
			 train-loss:  2.284021810690562 	 ± 0.31204037326592005
	data : 0.015372657775878906
	model : 3.8076702117919923
			 train-loss:  2.2823246530264862 	 ± 0.3113039218467532
	data : 0.012609386444091797
	model : 3.8031681537628175
			 train-loss:  2.2810393513226117 	 ± 0.3103476735401208
	data : 0.012737369537353516
	model : 3.7834157943725586
			 train-loss:  2.2800336291150347 	 ± 0.30928308131648946
	data : 0.01270885467529297
	model : 3.7570353984832763
			 train-loss:  2.279226703028525 	 ± 0.30816342059257973
	data : 0.01623997688293457
	model : 3.812886381149292
			 train-loss:  2.281464391708374 	 ± 0.30793810306431907
	data : 0.018083763122558594
	model : 3.861259365081787
			 train-loss:  2.2783626658575877 	 ± 0.30866790716476583
	data : 0.019834280014038086
	model : 3.8942981243133543
			 train-loss:  2.2763189199402576 	 ± 0.30830498248885335
	data : 0.019171667098999024
	model : 3.899272632598877
			 train-loss:  2.2732306364923716 	 ± 0.3090641205214059
	data : 0.020073318481445314
	model : 3.9351706504821777
			 train-loss:  2.275989988977595 	 ± 0.3094426537093661
	data : 0.018767881393432616
	model : 3.8997332096099853
			 train-loss:  2.27516770362854 	 ± 0.30839164166888994
	data : 0.01699800491333008
	model : 3.8967040538787843
			 train-loss:  2.2720076082316973 	 ± 0.30931798456460474
	data : 0.017096614837646483
	model : 3.8598551750183105
			 train-loss:  2.2718663260792242 	 ± 0.3081483408989753
	data : 0.018097114562988282
	model : 3.8860348224639893
			 train-loss:  2.2689544321002817 	 ± 0.30880526926648727
	data : 0.01728348731994629
	model : 3.85945200920105
			 train-loss:  2.2660118928596154 	 ± 0.3095167730503154
	data : 0.017709827423095702
	model : 3.8528707027435303
			 train-loss:  2.265266584467005 	 ± 0.30848895185866887
	data : 0.019370412826538085
	model : 3.9054490089416505
			 train-loss:  2.265681349179324 	 ± 0.30739048717126377
	data : 0.019277334213256836
	model : 3.83188738822937
			 train-loss:  2.2675600591367178 	 ± 0.30704923134382883
	data : 0.020466184616088866
	model : 3.7965316772460938
			 train-loss:  2.2654899220535722 	 ± 0.30689274460753374
	data : 0.020487880706787108
	model : 3.842249870300293
			 train-loss:  2.2660075709116545 	 ± 0.3058472797865766
	data : 0.02040696144104004
	model : 3.8370065689086914
			 train-loss:  2.2630413489682333 	 ± 0.3067529708318716
	data : 0.020065927505493165
	model : 3.71700496673584
			 train-loss:  2.261949426738928 	 ± 0.3059361844480361
	data : 0.01755809783935547
	model : 3.7866982936859133
			 train-loss:  2.260790872741753 	 ± 0.3051672859099689
	data : 0.01640739440917969
	model : 3.817137622833252
			 train-loss:  2.258413161431159 	 ± 0.30541550827367003
	data : 0.017432498931884765
	model : 3.7665087223052978
			 train-loss:  2.2601195226113 	 ± 0.30503644536955044
	data : 0.01742067337036133
	model : 3.735272455215454
			 train-loss:  2.2598456349866143 	 ± 0.3040005428063419
	data : 0.016132259368896486
	model : 3.795367431640625
			 train-loss:  2.2567532201335854 	 ± 0.30523758084497676
	data : 0.018807649612426758
	model : 3.774478054046631
			 train-loss:  2.256919568087779 	 ± 0.3042042265105468
	data : 0.01835641860961914
	model : 3.716678762435913
			 train-loss:  2.2559105955265664 	 ± 0.30342147171146927
	data : 0.015743207931518555
	model : 3.7323316097259522
			 train-loss:  2.256100386581165 	 ± 0.30241037912073904
	data : 0.013148641586303711
	model : 3.7666597843170164
			 train-loss:  2.25266516049703 	 ± 0.3043035870799677
	data : 0.010594367980957031
	model : 3.750323247909546
			 train-loss:  2.250442238832941 	 ± 0.3045137611465919
	data : 0.009121227264404296
	model : 3.832509231567383
			 train-loss:  2.2502598174308477 	 ± 0.30351869606220294
	data : 0.0066264629364013675
	model : 3.8086827278137205
			 train-loss:  2.2508457275777083 	 ± 0.30261140761793043
	data : 0.009751462936401367
	model : 3.816700553894043
			 train-loss:  2.2504933987345015 	 ± 0.30165878503278915
	data : 0.012277841567993164
	model : 3.8354076862335207
			 train-loss:  2.252574324607849 	 ± 0.3017909830566213
	data : 0.014719772338867187
	model : 3.851572799682617
			 train-loss:  2.251628673229462 	 ± 0.30105244589835595
	data : 0.017248868942260742
	model : 3.7779181003570557
			 train-loss:  2.2534672568558127 	 ± 0.3009694975136614
	data : 0.01753058433532715
	model : 3.853791618347168
			 train-loss:  2.25421755751477 	 ± 0.3001628123449254
	data : 0.015968847274780273
	model : 3.7771430015563965
			 train-loss:  2.255447709335471 	 ± 0.29961668651688006
	data : 0.01798110008239746
	model : 3.7690265655517576
			 train-loss:  2.2552307836711405 	 ± 0.2986914417402678
	data : 0.01966543197631836
	model : 3.774696636199951
			 train-loss:  2.253591569314092 	 ± 0.29848343493418816
	data : 0.017020797729492186
	model : 3.7693578720092775
			 train-loss:  2.2543259969464056 	 ± 0.29770664906976496
	data : 0.01925792694091797
	model : 3.7027968883514406
			 train-loss:  2.254927586192734 	 ± 0.29689078890340026
	data : 0.01927776336669922
	model : 3.7671714305877684
			 train-loss:  2.2558192618009523 	 ± 0.2962030978614874
	data : 0.017333316802978515
	model : 3.718419075012207
			 train-loss:  2.255643606908394 	 ± 0.29531271618879573
	data : 0.0164947509765625
	model : 3.7179028034210204
			 train-loss:  2.2544729343379837 	 ± 0.29480564709426405
	data : 0.017852020263671876
	model : 3.7097999095916747
			 train-loss:  2.2558300759264096 	 ± 0.2944413226949927
	data : 0.017888736724853516
	model : 3.817087936401367
			 train-loss:  2.255733949087915 	 ± 0.2935663295803194
	data : 0.015404319763183594
	model : 3.8309471130371096
			 train-loss:  2.257285827010341 	 ± 0.29338684464153186
	data : 0.015396881103515624
	model : 3.852418851852417
			 train-loss:  2.2561908925280854 	 ± 0.2928687813876696
	data : 0.012021207809448242
	model : 3.8991715908050537
			 train-loss:  2.254765494525084 	 ± 0.29260200077521104
	data : 0.012141180038452149
	model : 3.965367889404297
			 train-loss:  2.2538494710312333 	 ± 0.29199597557816864
	data : 0.009681081771850586
	model : 3.865925407409668
			 train-loss:  2.2534102049866163 	 ± 0.2912078227811246
	data : 0.013402938842773438
	model : 3.8474093437194825
			 train-loss:  2.25383213402211 	 ± 0.29042284074559277
	data : 0.013476228713989258
	model : 3.8312692642211914
			 train-loss:  2.2531776230675833 	 ± 0.28972054031434125
	data : 0.016150712966918945
	model : 3.8063424587249757
			 train-loss:  2.250255351716822 	 ± 0.29147129069122146
	data : 0.01628594398498535
	model : 3.7710476398468016
			 train-loss:  2.2500866887259616 	 ± 0.2906553721399269
	data : 0.020669174194335938
	model : 3.730477523803711
			 train-loss:  2.250636765126432 	 ± 0.2899301518032329
	data : 0.01977572441101074
	model : 3.787037229537964
			 train-loss:  2.2491695747695157 	 ± 0.2897810519714007
	data : 0.017234325408935547
	model : 3.7837507247924806
			 train-loss:  2.24763818581899 	 ± 0.2897004026945662
	data : 0.01591830253601074
	model : 3.7487921714782715
			 train-loss:  2.246707581683417 	 ± 0.28916868140821017
	data : 0.0170565128326416
	model : 3.687245559692383
			 train-loss:  2.2444108102347826 	 ± 0.29002394323558756
	data : 0.016646337509155274
	model : 3.802333927154541
			 train-loss:  2.243164136110108 	 ± 0.28971902364101354
	data : 0.016270828247070313
	model : 3.6923017501831055
			 train-loss:  2.2408776911704438 	 ± 0.2905815301666011
	data : 0.020656251907348634
	model : 3.732504367828369
			 train-loss:  2.239014261477702 	 ± 0.29089538015610866
	data : 0.021835851669311523
	model : 3.7018742084503176
			 train-loss:  2.236646887435708 	 ± 0.29189381695363187
	data : 0.02055673599243164
	model : 3.7700055122375487
			 train-loss:  2.235728982298132 	 ± 0.29138134630727
	data : 0.016587257385253906
	model : 3.7216744422912598
			 train-loss:  2.2333895995261823 	 ± 0.29236086246003395
	data : 0.016535139083862303
	model : 3.764266347885132
			 train-loss:  2.2314643361581066 	 ± 0.29277888248161593
	data : 0.012119102478027343
	model : 3.7576629161834716
			 train-loss:  2.231589092078962 	 ± 0.2920124321056297
	data : 0.012070083618164062
	model : 3.8060065269470216
			 train-loss:  2.2298994906909804 	 ± 0.2921766882482611
	data : 0.012058782577514648
	model : 3.790831708908081
			 train-loss:  2.2327634499718747 	 ± 0.2940905105927358
	data : 0.014399576187133788
	model : 3.775175380706787
			 train-loss:  2.2318655746588436 	 ± 0.29359135593272917
	data : 0.012066030502319336
	model : 3.751706075668335
			 train-loss:  2.2311326517272243 	 ± 0.29301066630609846
	data : 0.01204233169555664
	model : 3.7670783519744875
			 train-loss:  2.2295899611253005 	 ± 0.2930472090394053
	data : 0.014070224761962891
	model : 3.709454870223999
			 train-loss:  2.2299004732346046 	 ± 0.29233084327886905
	data : 0.013632106781005859
	model : 3.7258585929870605
			 train-loss:  2.2296112430882333 	 ± 0.2916160568607164
	data : 0.013665533065795899
	model : 3.7051262855529785
			 train-loss:  2.2298253076245085 	 ± 0.29089423731739833
	data : 0.016286754608154298
	model : 3.761091709136963
			 train-loss:  2.2311757054161188 	 ± 0.29078394314620726
	data : 0.018777799606323243
	model : 3.7299295902252196
			 train-loss:  2.229615026116371 	 ± 0.29089041465671195
	data : 0.016845703125
	model : 3.739916467666626
			 train-loss:  2.2279193318305324 	 ± 0.2911551612129953
	data : 0.01812634468078613
	model : 3.7628780364990235
			 train-loss:  2.2258041973161227 	 ± 0.29197756841695083
	data : 0.018183231353759766
	model : 3.795002555847168
			 train-loss:  2.2269726063817594 	 ± 0.29173054621075195
	data : 0.01547865867614746
	model : 3.8208805084228517
			 train-loss:  2.2255178760079777 	 ± 0.2917518100083999
	data : 0.015506744384765625
	model : 3.8108366966247558
			 train-loss:  2.226775973017623 	 ± 0.2915935463616475
	data : 0.012903070449829102
	model : 3.8838940143585203
			 train-loss:  2.226056685725462 	 ± 0.29106718563638717
	data : 0.012777566909790039
	model : 3.7861172199249267
			 train-loss:  2.224553466419091 	 ± 0.2911637380298599
	data : 0.01429734230041504
	model : 3.833383893966675
			 train-loss:  2.223394784789819 	 ± 0.2909409739038901
	data : 0.016743230819702148
	model : 3.7336214065551756
			 train-loss:  2.2223508837120383 	 ± 0.29063431684457025
	data : 0.017671918869018553
	model : 3.789576768875122
			 train-loss:  2.2225460688273113 	 ± 0.2899552351992257
	data : 0.02027411460876465
	model : 3.72576699256897
			 train-loss:  2.2203244581041743 	 ± 0.2910533450145098
	data : 0.020832920074462892
	model : 3.7361105918884276
			 train-loss:  2.2208908993118213 	 ± 0.29048264155489145
	data : 0.016881990432739257
	model : 3.6378559112548827
			 train-loss:  2.2223742707794263 	 ± 0.2906036766942558
	data : 0.016921329498291015
	model : 3.7057453632354735
			 train-loss:  2.2221900976706888 	 ± 0.2899363608127349
	data : 0.01609458923339844
	model : 3.6528438091278077
			 train-loss:  2.2235460586326066 	 ± 0.2899406301943171
	data : 0.016081905364990233
	model : 3.7404922008514405
			 train-loss:  2.223719698963342 	 ± 0.28927989741008375
	data : 0.01482858657836914
	model : 3.6063393115997315
			 train-loss:  2.2229876776444746 	 ± 0.2888130337711289
	data : 0.01480555534362793
	model : 3.401126194000244
			 train-loss:  2.2219618094076803 	 ± 0.28854585806079697
	data : 0.012313032150268554
	model : 3.119093179702759
			 train-loss:  2.22262326821889 	 ± 0.28805193366376236
	data : 0.009698295593261718
	model : 2.891724109649658
			 train-loss:  2.2228559109297668 	 ± 0.28741714489179826
	data : 0.007145023345947266
	model : 2.5662255764007567
			 train-loss:  2.2243778452074907 	 ± 0.28765326872294716
	data : 0.0045149803161621095
	model : 2.480880308151245
			 train-loss:  2.222794984912013 	 ± 0.28796767908038434
	data : 0.004455661773681641
	model : 2.48769154548645
			 train-loss:  2.2236281521117207 	 ± 0.2875893368831696
	data : 0.004463768005371094
	model : 2.5012743949890135
			 train-loss:  2.22289013756173 	 ± 0.28715824315829597
	data : 0.00445098876953125
	model : 2.504245948791504
			 train-loss:  2.2226982837253146 	 ± 0.28653379090522796
	data : 0.004540443420410156
	model : 2.4996457576751707
			 train-loss:  2.224034626926996 	 ± 0.28660101153861983
	data : 0.0045506000518798825
	model : 2.506709623336792
			 train-loss:  2.2229942293419187 	 ± 0.28639643418047067
	data : 0.004559755325317383
	model : 2.507954454421997
			 train-loss:  2.223351387078302 	 ± 0.2858183415516344
	data : 0.004580783843994141
	model : 2.4973114013671873
			 train-loss:  2.2219217869912695 	 ± 0.2860093821205874
	data : 0.004628467559814453
	model : 2.5048890590667723
			 train-loss:  2.222164854277735 	 ± 0.28541064846235076
	data : 0.004560995101928711
	model : 2.5158671855926515
			 train-loss:  2.2226868981406804 	 ± 0.28490223360561096
	data : 0.0046332359313964845
	model : 2.5097281455993654
			 train-loss:  2.2232617535467805 	 ± 0.2844217837719597
	data : 0.004678058624267578
	model : 2.5128703117370605
			 train-loss:  2.2232739991896144 	 ± 0.2838108415988075
	data : 0.004628038406372071
	model : 2.5215089321136475
			 train-loss:  2.2246896899663486 	 ± 0.2840270111743032
	data : 0.004565191268920898
	model : 2.516738033294678
			 train-loss:  2.224703707086279 	 ± 0.28342213523626175
	data : 0.004582500457763672
	model : 2.511979675292969
			 train-loss:  2.223524389125533 	 ± 0.2833982520386748
	data : 0.004513978958129883
	model : 2.521060609817505
			 train-loss:  2.224357297651878 	 ± 0.2830890514665652
	data : 0.004480314254760742
	model : 2.519584321975708
			 train-loss:  2.2251818135005084 	 ± 0.28277872921926256
	data : 0.0044688224792480465
	model : 2.517994928359985
			 train-loss:  2.222725166935302 	 ± 0.2847201956793217
	data : 0.00458230972290039
	model : 2.5144327163696287
			 train-loss:  2.2228534042835237 	 ± 0.28413332583155404
	data : 0.004604196548461914
	model : 2.512525033950806
			 train-loss:  2.223474562910088 	 ± 0.28370647062783205
	data : 0.004608583450317383
	model : 2.504281520843506
			 train-loss:  2.223627382073521 	 ± 0.28312963294178867
	data : 0.004622077941894532
	model : 2.494855451583862
			 train-loss:  2.2212992273730996 	 ± 0.2848582389853978
	data : 0.005437231063842774
	model : 2.492935228347778
			 train-loss:  2.2195056877175317 	 ± 0.28564547377267396
	data : 0.00538945198059082
	model : 2.4964129447937013
			 train-loss:  2.220317984113888 	 ± 0.2853441782581036
	data : 0.0052926063537597655
	model : 2.4915318489074707
			 train-loss:  2.2209633254423373 	 ± 0.2849427191588096
	data : 0.005304670333862305
	model : 2.4907175064086915
			 train-loss:  2.221517142496611 	 ± 0.28449796255883014
	data : 0.0053078651428222655
	model : 2.4945123195648193
			 train-loss:  2.220524330773661 	 ± 0.28435222023552104
	data : 0.004517507553100586
	model : 2.4864375591278076
			 train-loss:  2.2212267999189446 	 ± 0.28399619771563284
	data : 0.004427003860473633
	model : 2.479199981689453
			 train-loss:  2.2228561825752258 	 ± 0.28459144756646504
	data : 0.004456710815429687
	model : 2.4851126194000246
			 train-loss:  2.222995056574088 	 ± 0.28403245425579676
	data : 0.004414892196655274
	model : 2.486549425125122
			 train-loss:  2.2227616154012226 	 ± 0.2834924630630315
	data : 0.004372262954711914
	model : 2.480764865875244
			 train-loss:  2.22314850567829 	 ± 0.2829982986110071
	data : 0.004236888885498047
	model : 2.4841784954071047
			 train-loss:  2.2235156950049513 	 ± 0.28250104649148433
	data : 0.004268360137939453
	model : 2.502315855026245
			 train-loss:  2.221591287500718 	 ± 0.28360980713184947
	data : 0.0042266368865966795
	model : 2.5083102226257323
			 train-loss:  2.221222458407283 	 ± 0.2831166089204315
	data : 0.0042895317077636715
	model : 2.1488521575927733
model ba4c19b045af244f92ef5415cc764437d466e1160cc3db553a461431 state_dict has not been found
Model was not found
Namespace(co2=False, depth=5.0, disp=1, domain='global', epoch=0, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=20, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=True, rerun=True, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
epochs started
			 train-loss:  3.3900115489959717 	 ± 0.0
	data : 4.307988405227661
	model : 5.068868160247803
			 train-loss:  3.3052430152893066 	 ± 0.08476853370666504
	data : 2.163490056991577
	model : 4.953428149223328
			 train-loss:  3.125699281692505 	 ± 0.2631774574039552
	data : 1.454830567042033
	model : 4.879490772883098
			 train-loss:  3.175441086292267 	 ± 0.24365861761639107
	data : 1.0930230021476746
	model : 4.841678619384766
			 train-loss:  3.0767460823059083 	 ± 0.29403814854275956
	data : 0.8782419681549072
	model : 4.827081966400146
			 train-loss:  3.0112520853678384 	 ± 0.3057711761650058
	data : 0.01977882385253906
	model : 4.788202476501465
			 train-loss:  3.035872152873448 	 ± 0.2894414022727172
	data : 0.01915087699890137
	model : 4.7583283424377445
			 train-loss:  3.019622355699539 	 ± 0.2741398840706136
	data : 0.01656789779663086
	model : 4.758838319778443
			 train-loss:  3.0019377602471247 	 ± 0.2632571709017453
	data : 0.018227434158325194
	model : 4.762448930740357
			 train-loss:  2.992303967475891 	 ± 0.2514143868128936
	data : 0.017591047286987304
	model : 4.7493932247161865
			 train-loss:  2.9164936542510986 	 ± 0.33902055904426676
	data : 0.015098810195922852
	model : 4.727381038665771
			 train-loss:  2.8841755588849387 	 ± 0.3418275588692662
	data : 0.015105247497558594
	model : 4.715700435638428
			 train-loss:  2.853860066487239 	 ± 0.34479888189219443
	data : 0.013357591629028321
	model : 4.738127756118774
			 train-loss:  2.8075195040021623 	 ± 0.3719021552893655
	data : 0.01356053352355957
	model : 4.742798376083374
			 train-loss:  2.761472813288371 	 ± 0.39846534821483015
	data : 0.013489961624145508
	model : 4.763027334213257
			 train-loss:  2.757914811372757 	 ± 0.38605842794121287
	data : 0.016025638580322264
	model : 4.7803691864013675
			 train-loss:  2.7378627692951873 	 ± 0.38302393651463007
	data : 0.016286516189575197
	model : 4.806450271606446
			 train-loss:  2.727963235643175 	 ± 0.3744635381739189
	data : 0.016259431838989258
	model : 4.788714170455933
			 train-loss:  2.691184031335931 	 ± 0.3964739137201737
	data : 0.01607089042663574
	model : 4.760216903686524
			 train-loss:  2.680819404125214 	 ± 0.3890669228997601
	data : 0.016138696670532228
	model : 4.761688184738159
			 train-loss:  2.680966649736677 	 ± 0.3796910094642245
	data : 0.01635422706604004
	model : 4.740740156173706
			 train-loss:  2.6890292059291494 	 ± 0.37279672375511397
	data : 0.016295576095581056
	model : 4.729071092605591
			 train-loss:  2.6628422322480576 	 ± 0.3847357127989216
	data : 0.016623544692993163
	model : 4.728867816925049
			 train-loss:  2.664440174897512 	 ± 0.37671306324127585
	data : 0.016765117645263672
	model : 4.743634223937988
			 train-loss:  2.6779784297943117 	 ± 0.3750133958028403
	data : 0.017029619216918944
	model : 4.724284601211548
			 train-loss:  2.6735734205979567 	 ± 0.36838988777059517
	data : 0.017081546783447265
	model : 4.7116223812103275
			 train-loss:  2.6484980009220265 	 ± 0.3834487781908303
	data : 0.017138099670410155
	model : 4.719692802429199
			 train-loss:  2.6451161205768585 	 ± 0.37694905524879635
	data : 0.017074441909790038
	model : 4.7006488800048825
			 train-loss:  2.6308065817273896 	 ± 0.3780532713591576
	data : 0.017176389694213867
	model : 4.707466077804566
			 train-loss:  2.6178329507509868 	 ± 0.3782079737038341
	data : 0.017109012603759764
	model : 4.65490984916687
			 train-loss:  2.6119978235613917 	 ± 0.3734280360287617
	data : 0.017130708694458006
	model : 4.667897319793701
			 train-loss:  2.6088189892470837 	 ± 0.3679728076127994
	data : 0.017134761810302733
	model : 4.634854507446289
			 train-loss:  2.603034832260825 	 ± 0.36382886515812807
	data : 0.017204904556274415
	model : 4.642361307144165
			 train-loss:  2.6000590078970967 	 ± 0.35884592500766366
	data : 0.01712064743041992
	model : 4.621438026428223
			 train-loss:  2.5837067433765957 	 ± 0.3663096047570856
	data : 0.017096805572509765
	model : 4.6473980903625485
			 train-loss:  2.5743417375617557 	 ± 0.36541078974880115
	data : 0.017075204849243165
	model : 4.636723613739013
			 train-loss:  2.558044707452929 	 ± 0.3734670201114593
	data : 0.017156219482421874
	model : 4.650718641281128
			 train-loss:  2.548908155215414 	 ± 0.37268724859668756
	data : 0.017832517623901367
	model : 4.647663879394531
			 train-loss:  2.5357089990224595 	 ± 0.37676866694955824
	data : 0.017983341217041017
	model : 4.652824401855469
			 train-loss:  2.5310564965009688 	 ± 0.37316209359185987
	data : 0.0180356502532959
	model : 4.678968191146851
			 train-loss:  2.5215149303761923 	 ± 0.37349065217350225
	data : 0.017950677871704103
	model : 4.695918083190918
			 train-loss:  2.5229524232092357 	 ± 0.36913232476344954
	data : 0.015288066864013673
	model : 4.717451333999634
			 train-loss:  2.517523624176203 	 ± 0.3665074145329771
	data : 0.014683246612548828
	model : 4.6988489627838135
			 train-loss:  2.503637720238079 	 ± 0.3735852929246775
	data : 0.014613580703735352
	model : 4.680227327346802
			 train-loss:  2.499012639787462 	 ± 0.37068278259057186
	data : 0.014608430862426757
	model : 4.66370701789856
			 train-loss:  2.4945520421733027 	 ± 0.36785052157432635
	data : 0.014657068252563476
	model : 4.623493289947509
			 train-loss:  2.4997677904494267 	 ± 0.3656314677338226
	data : 0.017061138153076173
	model : 4.616318893432617
			 train-loss:  2.485392158230146 	 ± 0.374985601276685
	data : 0.016879463195800783
	model : 4.6287917137146
			 train-loss:  2.477830069405692 	 ± 0.3748191731818331
	data : 0.01433115005493164
	model : 4.639186096191406
			 train-loss:  2.463141431808472 	 ± 0.38503463737913235
	data : 0.014321517944335938
	model : 4.640777111053467
			 train-loss:  2.4541524718789494 	 ± 0.38650335895481597
	data : 0.014403724670410156
	model : 4.640723323822021
			 train-loss:  2.4494964434550357 	 ± 0.38421044963529927
	data : 0.012022066116333007
	model : 4.651420640945434
			 train-loss:  2.4345536681841002 	 ± 0.3955291775143933
	data : 0.009498262405395507
	model : 4.645720624923706
			 train-loss:  2.4336443742116294 	 ± 0.3919056682157087
	data : 0.012125015258789062
	model : 4.655297565460205
			 train-loss:  2.43108605038036 	 ± 0.38878134920545143
	data : 0.012754106521606445
	model : 4.686366128921509
			 train-loss:  2.426923828465598 	 ± 0.3865289622085075
	data : 0.01264657974243164
	model : 4.692567682266235
			 train-loss:  2.418774866221244 	 ± 0.3879461527222026
	data : 0.015229368209838867
	model : 4.671843862533569
			 train-loss:  2.4103711576297364 	 ± 0.38978561251756905
	data : 0.01782393455505371
	model : 4.668227672576904
			 train-loss:  2.404523217071921 	 ± 0.3890259616384301
	data : 0.01777758598327637
	model : 4.666662788391113
			 train-loss:  2.4050324698289236 	 ± 0.38579028821207245
	data : 0.014672374725341797
	model : 4.644741487503052
			 train-loss:  2.40837070785585 	 ± 0.38348777350208135
	data : 0.014662694931030274
	model : 4.647254467010498
			 train-loss:  2.4072648767502076 	 ± 0.38048059769291037
	data : 0.014554548263549804
	model : 4.636079454421997
			 train-loss:  2.403175090986585 	 ± 0.3788200812083555
	data : 0.014548492431640626
	model : 4.654486417770386
			 train-loss:  2.4113777447491884 	 ± 0.38144626885093136
	data : 0.011986112594604493
	model : 4.651651096343994
			 train-loss:  2.4040349153371956 	 ± 0.383031946918215
	data : 0.014322948455810548
	model : 4.663171195983887
			 train-loss:  2.404167298114661 	 ± 0.3801206125673139
	data : 0.014338922500610352
	model : 4.663506126403808
			 train-loss:  2.3969801219541633 	 ± 0.3817647848282587
	data : 0.014375543594360352
	model : 4.664612483978272
			 train-loss:  2.4077848651829887 	 ± 0.38913082320043463
	data : 0.014270401000976563
	model : 4.650682401657105
			 train-loss:  2.4079129557678667 	 ± 0.38630218735605043
	data : 0.016855812072753905
	model : 4.653531551361084
			 train-loss:  2.410432495389666 	 ± 0.38410356570134013
	data : 0.016993904113769533
	model : 4.645257425308228
			 train-loss:  2.4058490538261306 	 ± 0.38331206511490884
	data : 0.017878627777099608
	model : 4.655939817428589
			 train-loss:  2.4014409449365406 	 ± 0.3824488224122834
	data : 0.017864322662353514
	model : 4.657009315490723
			 train-loss:  2.3957203789933086 	 ± 0.3829094329152038
	data : 0.01790142059326172
	model : 4.6576159477233885
			 train-loss:  2.391587014133866 	 ± 0.3819495658879649
	data : 0.017962551116943358
	model : 4.645813035964966
			 train-loss:  2.390597961743673 	 ± 0.379490078780599
	data : 0.015450096130371094
	model : 4.628213882446289
			 train-loss:  2.3882912130732286 	 ± 0.3775141022411385
	data : 0.015042495727539063
	model : 4.630489349365234
			 train-loss:  2.387324037489953 	 ± 0.375149465104141
	data : 0.015087366104125977
	model : 4.6456516742706295
			 train-loss:  2.3898944839453087 	 ± 0.373418736963099
	data : 0.01255335807800293
	model : 4.6442114353179935
			 train-loss:  2.3868164367313627 	 ± 0.3720422978467526
	data : 0.012477540969848632
	model : 4.641358757019043
			 train-loss:  2.3810904026031494 	 ± 0.3731963127831151
	data : 0.014929389953613282
	model : 4.661919116973877
			 train-loss:  2.379161516825358 	 ± 0.37128652740817625
	data : 0.0144683837890625
	model : 4.656112718582153
			 train-loss:  2.378672963235436 	 ± 0.3690418355457115
	data : 0.014461040496826172
	model : 4.6691512107849125
			 train-loss:  2.3774220254047806 	 ± 0.3669868222836292
	data : 0.014481401443481446
	model : 4.687483930587769
			 train-loss:  2.376313019366491 	 ± 0.36493572534671753
	data : 0.014384126663208008
	model : 4.724080419540405
			 train-loss:  2.372680936140173 	 ± 0.3643067567993842
	data : 0.01188502311706543
	model : 4.709616088867188
			 train-loss:  2.3662439848101418 	 ± 0.3670123746781255
	data : 0.01186981201171875
	model : 4.718308019638061
			 train-loss:  2.359729124211717 	 ± 0.3698647894547768
	data : 0.011829376220703125
	model : 4.7065479278564455
			 train-loss:  2.358241158452901 	 ± 0.36801907438733533
	data : 0.014258480072021485
	model : 4.711024951934815
			 train-loss:  2.358977278966582 	 ± 0.36601085810762435
	data : 0.014310693740844727
	model : 4.687206745147705
			 train-loss:  2.3555182602670457 	 ± 0.36543170095263217
	data : 0.016881799697875975
	model : 4.700777864456176
			 train-loss:  2.354233816429809 	 ± 0.36362251520771577
	data : 0.016980361938476563
	model : 4.693953943252564
			 train-loss:  2.3506664517133133 	 ± 0.3632385159100352
	data : 0.016987991333007813
	model : 4.706364107131958
			 train-loss:  2.347828738151058 	 ± 0.3623041909476111
	data : 0.017076396942138673
	model : 4.69671859741211
			 train-loss:  2.348330298636822 	 ± 0.3604043467018206
	data : 0.017002105712890625
	model : 4.71488356590271
			 train-loss:  2.34349775565298 	 ± 0.3615511619270357
	data : 0.017127847671508788
	model : 4.6996255874633786
			 train-loss:  2.3456590796510377 	 ± 0.36027955735525125
	data : 0.016667985916137697
	model : 4.710518932342529
			 train-loss:  2.3415755979793587 	 ± 0.3606438465707022
	data : 0.016665029525756835
	model : 4.704995250701904
			 train-loss:  2.3389683013059654 	 ± 0.3597168416475386
	data : 0.01667933464050293
	model : 4.697084093093872
			 train-loss:  2.340045745926674 	 ± 0.35805438231730174
	data : 0.014173603057861328
	model : 4.698059940338135
			 train-loss:  2.3435628151893617 	 ± 0.3579741878031346
	data : 0.014134359359741212
	model : 4.7078622341156
			 train-loss:  2.338119122061399 	 ± 0.360333361075942
	data : 0.01454787254333496
	model : 4.706413650512696
			 train-loss:  2.3363518551284193 	 ± 0.35900227731329404
	data : 0.014545154571533204
	model : 4.693419075012207
			 train-loss:  2.3326303542238995 	 ± 0.3592269525421999
	data : 0.014558792114257812
	model : 4.708702850341797
			 train-loss:  2.3310344150433173 	 ± 0.35786245815935636
	data : 0.0171048641204834
	model : 4.682626867294312
			 train-loss:  2.3280402546837218 	 ± 0.35746080452431805
	data : 0.017015600204467775
	model : 4.6723775386810305
			 train-loss:  2.3313404636562995 	 ± 0.35737426687468016
	data : 0.016972160339355467
	model : 4.6534035205841064
			 train-loss:  2.331425058507474 	 ± 0.3557014398168816
	data : 0.017004919052124024
	model : 4.636257362365723
			 train-loss:  2.3283598202246205 	 ± 0.3554677744040952
	data : 0.01698722839355469
	model : 4.599398851394653
			 train-loss:  2.327556977578259 	 ± 0.35393178591771557
	data : 0.014430952072143555
	model : 4.618833589553833
			 train-loss:  2.3294570250944657 	 ± 0.3528773461261906
	data : 0.014371538162231445
	model : 4.610600709915161
			 train-loss:  2.32662845517064 	 ± 0.3525346602141444
	data : 0.014283275604248047
	model : 4.607671403884888
			 train-loss:  2.3261063950402394 	 ± 0.3510004136135569
	data : 0.014383172988891602
	model : 4.621078014373779
			 train-loss:  2.3226799194791674 	 ± 0.3513203320979085
	data : 0.01442408561706543
	model : 4.630496072769165
			 train-loss:  2.3206474101334287 	 ± 0.35044272769854484
	data : 0.016983699798583985
	model : 4.62624340057373
			 train-loss:  2.317374174491219 	 ± 0.3506616540138651
	data : 0.01715826988220215
	model : 4.635721731185913
			 train-loss:  2.313047333010312 	 ± 0.3522166147093688
	data : 0.017222213745117187
	model : 4.650674676895141
			 train-loss:  2.3128492791428523 	 ± 0.35071467245753735
	data : 0.01652045249938965
	model : 4.665208339691162
			 train-loss:  2.310727032564454 	 ± 0.3499790877267949
	data : 0.016300153732299805
	model : 4.646830224990845
			 train-loss:  2.3122852750185157 	 ± 0.34891630973265786
	data : 0.01680917739868164
	model : 4.6378601551055905
			 train-loss:  2.309818454583486 	 ± 0.34849994084379254
	data : 0.01657242774963379
	model : 4.63670072555542
			 train-loss:  2.3101879269623558 	 ± 0.3470804700443834
	data : 0.016585922241210936
	model : 4.625019502639771
			 train-loss:  2.3097861946606244 	 ± 0.34568332894550463
	data : 0.01713080406188965
	model : 4.6149803638458256
			 train-loss:  2.3105099395038637 	 ± 0.34436804190287534
	data : 0.01739668846130371
	model : 4.64866623878479
			 train-loss:  2.3088975952517603 	 ± 0.3434424842859272
	data : 0.01683797836303711
	model : 4.654152250289917
			 train-loss:  2.3080396919250488 	 ± 0.3421993305940549
	data : 0.016948604583740236
	model : 4.6436317443847654
			 train-loss:  2.306160883297996 	 ± 0.34148536422687736
	data : 0.01689887046813965
	model : 4.6643458843231205
			 train-loss:  2.3028257062116007 	 ± 0.342192338368061
	data : 0.01782703399658203
	model : 4.681310749053955
			 train-loss:  2.301009716466069 	 ± 0.3414668506204188
	data : 0.01858229637145996
	model : 4.679481887817383
			 train-loss:  2.298646609912547 	 ± 0.34118986505608623
	data : 0.01858863830566406
	model : 4.655549335479736
			 train-loss:  2.2995484948158262 	 ± 0.3400293907598126
	data : 0.018625497817993164
	model : 4.684128046035767
			 train-loss:  2.29711557162627 	 ± 0.33986302571050225
	data : 0.0186159610748291
	model : 4.681706666946411
			 train-loss:  2.2939140697320304 	 ± 0.3405503246745732
	data : 0.01769223213195801
	model : 4.6522557735443115
			 train-loss:  2.295526247275503 	 ± 0.33977289195199234
	data : 0.016803789138793945
	model : 4.656889200210571
			 train-loss:  2.2934094640746046 	 ± 0.33938182950896856
	data : 0.016855382919311525
	model : 4.684614467620849
			 train-loss:  2.2958294771335743 	 ± 0.33928101463408955
	data : 0.016834545135498046
	model : 4.662861490249634
			 train-loss:  2.2944755580495384 	 ± 0.3383972016677494
	data : 0.016854190826416017
	model : 4.6546101570129395
			 train-loss:  2.29134303200854 	 ± 0.3391332143244168
	data : 0.014320707321166993
	model : 4.640975284576416
			 train-loss:  2.290685172530188 	 ± 0.3379899588861472
	data : 0.014288616180419923
	model : 4.630293130874634
			 train-loss:  2.288925473638576 	 ± 0.33740581560741223
	data : 0.011697959899902344
	model : 4.64500150680542
			 train-loss:  2.287842255830765 	 ± 0.33644110855853626
	data : 0.011664247512817383
	model : 4.633038902282715
			 train-loss:  2.2869960779839373 	 ± 0.3353954045082031
	data : 0.01175518035888672
	model : 4.609014987945557
			 train-loss:  2.2851400064750456 	 ± 0.33493826136425053
	data : 0.01487107276916504
	model : 4.635500097274781
			 train-loss:  2.2840164466337725 	 ± 0.33403352656283086
	data : 0.014996767044067383
	model : 4.630795478820801
			 train-loss:  2.2824896665083036 	 ± 0.33337199660382344
	data : 0.017573118209838867
	model : 4.611730432510376
			 train-loss:  2.2811678121829857 	 ± 0.33259891439645917
	data : 0.01754136085510254
	model : 4.635894298553467
			 train-loss:  2.279060086158857 	 ± 0.3324282125073793
	data : 0.017455244064331056
	model : 4.648722743988037
			 train-loss:  2.280197151664163 	 ± 0.33158034302119527
	data : 0.01689910888671875
	model : 4.628593778610229
			 train-loss:  2.281628202747654 	 ± 0.3309134193042628
	data : 0.01685943603515625
	model : 4.646730566024781
			 train-loss:  2.2820890509842227 	 ± 0.32984875227418703
	data : 0.016818809509277343
	model : 4.6560783863067625
			 train-loss:  2.28227530002594 	 ± 0.32875527878872346
	data : 0.016904830932617188
	model : 4.633490896224975
			 train-loss:  2.2803378697262695 	 ± 0.3285229321466333
	data : 0.014433622360229492
	model : 4.644796752929688
			 train-loss:  2.2766807251854946 	 ± 0.3305099838785922
	data : 0.014429235458374023
	model : 4.657089281082153
			 train-loss:  2.279956174052619 	 ± 0.3318939966765437
	data : 0.014408349990844727
	model : 4.665401935577393
			 train-loss:  2.278097066012296 	 ± 0.33161295662936674
	data : 0.014461469650268555
	model : 4.674809169769287
			 train-loss:  2.277113243841356 	 ± 0.33076690438294565
	data : 0.014409780502319336
	model : 4.698070383071899
			 train-loss:  2.2796603792752976 	 ± 0.33122657513975334
	data : 0.0169858455657959
	model : 4.694103097915649
			 train-loss:  2.281204530387927 	 ± 0.3307328445051338
	data : 0.016995048522949217
	model : 4.668363380432129
			 train-loss:  2.2797298506845403 	 ± 0.33020195952710335
	data : 0.0170196533203125
	model : 4.670952463150025
			 train-loss:  2.278188413044192 	 ± 0.32973171302731114
	data : 0.01704726219177246
	model : 4.643418836593628
			 train-loss:  2.2760761097073554 	 ± 0.3297770671928663
	data : 0.016992568969726562
	model : 4.641069793701172
			 train-loss:  2.2781325156644265 	 ± 0.32977877278835976
	data : 0.016892099380493165
	model : 4.6648108005523685
			 train-loss:  2.279474100948852 	 ± 0.32919977791099914
	data : 0.01936936378479004
	model : 4.701337814331055
			 train-loss:  2.283312265126983 	 ± 0.3318043633551013
	data : 0.01937394142150879
	model : 4.677960968017578
			 train-loss:  2.2812631399166294 	 ± 0.3318241293814829
	data : 0.019427967071533204
	model : 4.687063932418823
			 train-loss:  2.282164558497342 	 ± 0.33101842131663456
	data : 0.01957592964172363
	model : 4.670949411392212
			 train-loss:  2.2874902939221946 	 ± 0.3370357344571275
	data : 0.019618511199951172
	model : 4.652792835235596
			 train-loss:  2.285276762025799 	 ± 0.337233216444122
	data : 0.017121315002441406
	model : 4.638272762298584
			 train-loss:  2.2878906293993904 	 ± 0.33792054524245524
	data : 0.017207813262939454
	model : 4.628927278518677
			 train-loss:  2.2867400554510264 	 ± 0.3372491875566536
	data : 0.017064285278320313
	model : 4.624886560440063
			 train-loss:  2.285050256813274 	 ± 0.33697260887277297
	data : 0.01441493034362793
	model : 4.624500179290772
			 train-loss:  2.2839747249034414 	 ± 0.3362783840824964
	data : 0.014422607421875
	model : 4.622339487075806
			 train-loss:  2.2809091251949933 	 ± 0.33768732962060377
	data : 0.014429378509521484
	model : 4.608603620529175
			 train-loss:  2.2815949103735775 	 ± 0.3368300397359794
	data : 0.014410257339477539
	model : 4.596516132354736
			 train-loss:  2.2810141505866217 	 ± 0.33594759744039054
	data : 0.014424943923950195
	model : 4.577202987670899
			 train-loss:  2.279341380255563 	 ± 0.33571230149803827
	data : 0.01691603660583496
	model : 4.605418062210083
			 train-loss:  2.2799352664839136 	 ± 0.3348493922582171
	data : 0.016879749298095704
	model : 4.602454757690429
			 train-loss:  2.278249003119388 	 ± 0.3346507121228118
	data : 0.01687908172607422
	model : 4.61808180809021
			 train-loss:  2.2752362717403454 	 ± 0.33610784736480015
	data : 0.016765642166137695
	model : 4.649273681640625
			 train-loss:  2.2771720726396785 	 ± 0.33616127093525466
	data : 0.01675729751586914
	model : 4.673604154586792
			 train-loss:  2.2775375021828546 	 ± 0.33526183989620767
	data : 0.0143157958984375
	model : 4.658214092254639
			 train-loss:  2.2754119641214445 	 ± 0.3355483992138564
	data : 0.014315509796142578
	model : 4.655881881713867
			 train-loss:  2.2738486849344692 	 ± 0.33528558307797357
	data : 0.014380741119384765
	model : 4.628537034988403
			 train-loss:  2.272700365123853 	 ± 0.33472692902913165
	data : 0.014481544494628906
	model : 4.628752374649048
			 train-loss:  2.271275786601979 	 ± 0.3343719142719058
	data : 0.014496517181396485
	model : 4.62016978263855
			 train-loss:  2.2692226403468365 	 ± 0.33462794764481896
	data : 0.014470481872558593
	model : 4.618849420547486
			 train-loss:  2.2682926866316024 	 ± 0.3339668144413753
	data : 0.014525175094604492
	model : 4.617220878601074
			 train-loss:  2.267620018459259 	 ± 0.33319897550433425
	data : 0.01450490951538086
	model : 4.638077735900879
			 train-loss:  2.2666602927319546 	 ± 0.33257068093572795
	data : 0.014592170715332031
	model : 4.630304622650146
			 train-loss:  2.266176041471895 	 ± 0.33175614728428987
	data : 0.01457819938659668
	model : 4.617641353607178
			 train-loss:  2.2668230075585214 	 ± 0.3310014737737366
	data : 0.01710352897644043
	model : 4.5944504737854
			 train-loss:  2.265548282268784 	 ± 0.33060110162927475
	data : 0.017086219787597657
	model : 4.586308002471924
			 train-loss:  2.2649232602367797 	 ± 0.32985215986102256
	data : 0.017008113861083984
	model : 4.603591585159302
			 train-loss:  2.263818860054016 	 ± 0.3293522225050471
	data : 0.01682271957397461
	model : 4.594639539718628
			 train-loss:  2.2613798883772387 	 ± 0.330245097415709
	data : 0.01696043014526367
	model : 4.5997895240783695
			 train-loss:  2.261791033622546 	 ± 0.329447001504296
	data : 0.014407014846801758
	model : 4.614684057235718
			 train-loss:  2.2639072245481064 	 ± 0.32993156353907904
	data : 0.014437389373779298
	model : 4.612005710601807
			 train-loss:  2.262792191529637 	 ± 0.3294631398636794
	data : 0.014475727081298828
	model : 4.600756740570068
			 train-loss:  2.26071974785641 	 ± 0.32991494069589417
	data : 0.014502954483032227
	model : 4.610634517669678
			 train-loss:  2.2615369426545184 	 ± 0.3292858026643169
	data : 0.012736797332763672
	model : 4.597849035263062
			 train-loss:  2.2615438801050187 	 ± 0.32846157113826524
	data : 0.012730789184570313
	model : 4.593896102905274
			 train-loss:  2.2600137309648503 	 ± 0.3283573110661424
	data : 0.012668180465698241
	model : 4.604712677001953
			 train-loss:  2.2603086976721736 	 ± 0.3275702317666525
	data : 0.012638425827026368
	model : 4.60568380355835
			 train-loss:  2.258325183920085 	 ± 0.3279762311642371
	data : 0.012712907791137696
	model : 4.615955209732055
			 train-loss:  2.257336993427838 	 ± 0.3274741909542912
	data : 0.014331817626953125
	model : 4.653269815444946
			 train-loss:  2.2578107293059184 	 ± 0.32674456356704623
	data : 0.016934680938720702
	model : 4.659364891052246
			 train-loss:  2.2607451058128505 	 ± 0.3286470957749866
	data : 0.016915512084960938
	model : 4.67156286239624
			 train-loss:  2.2601333441941636 	 ± 0.3279698573642157
	data : 0.01451873779296875
	model : 4.664254856109619
			 train-loss:  2.2614803961836376 	 ± 0.32775402863587594
	data : 0.014425420761108398
	model : 4.653076219558716
			 train-loss:  2.261865505761507 	 ± 0.3270161577250914
	data : 0.014419794082641602
	model : 4.638104581832886
			 train-loss:  2.2599782313619343 	 ± 0.3273755466612782
	data : 0.014397621154785156
	model : 4.652626705169678
			 train-loss:  2.2594149050554395 	 ± 0.32670086015441807
	data : 0.011857223510742188
	model : 4.628525686264038
			 train-loss:  2.2600784262396254 	 ± 0.32607190508018885
	data : 0.014233922958374024
	model : 4.6323902130126955
			 train-loss:  2.25969900715519 	 ± 0.3253524828123417
	data : 0.014277505874633788
	model : 4.625617980957031
			 train-loss:  2.2635032423188752 	 ± 0.3293055903007528
	data : 0.014390277862548827
	model : 4.611613607406616
			 train-loss:  2.2620422873386117 	 ± 0.3292332743695959
	data : 0.014399528503417969
	model : 4.5965265274047855
			 train-loss:  2.260738510224554 	 ± 0.3290261187363118
	data : 0.016980981826782225
	model : 4.618352270126342
			 train-loss:  2.258644037532367 	 ± 0.3297072237342897
	data : 0.016923284530639647
	model : 4.611704301834107
			 train-loss:  2.257196588800588 	 ± 0.3296404654941232
	data : 0.018051671981811523
	model : 4.622170639038086
			 train-loss:  2.257012992689054 	 ± 0.32889817177603636
	data : 0.017994213104248046
	model : 4.627990674972534
			 train-loss:  2.2568479803475467 	 ± 0.328158910485374
	data : 0.01790785789489746
	model : 4.631378269195556
			 train-loss:  2.25644890161661 	 ± 0.32746913034985925
	data : 0.017935705184936524
	model : 4.632303857803345
			 train-loss:  2.256775787821761 	 ± 0.3267668909338118
	data : 0.01803288459777832
	model : 4.6538698196411135
			 train-loss:  2.2568021210854363 	 ± 0.32603364261344187
	data : 0.016934585571289063
	model : 4.65017876625061
			 train-loss:  2.2558196937399253 	 ± 0.3256357216526209
	data : 0.014407587051391602
	model : 4.655335903167725
			 train-loss:  2.2543944342931113 	 ± 0.32561075856349925
	data : 0.014499187469482422
	model : 4.656089973449707
			 train-loss:  2.2544801767948455 	 ± 0.3248921278337634
	data : 0.014436101913452149
	model : 4.634566926956177
			 train-loss:  2.252057441530774 	 ± 0.3262153203395139
	data : 0.01447887420654297
	model : 4.612636661529541
			 train-loss:  2.2508242820438586 	 ± 0.32602897319089086
	data : 0.014453315734863281
	model : 4.604516553878784
			 train-loss:  2.249612668716231 	 ± 0.325830365515151
	data : 0.01695704460144043
	model : 4.598285961151123
			 train-loss:  2.247659902987273 	 ± 0.3264614565343154
	data : 0.016979122161865236
	model : 4.618091011047364
			 train-loss:  2.246342436059729 	 ± 0.3263662441870136
	data : 0.01708540916442871
	model : 4.624622011184693
			 train-loss:  2.2460339141303094 	 ± 0.32569586611225576
	data : 0.017074966430664064
	model : 4.660553455352783
			 train-loss:  2.2462423492398895 	 ± 0.3250117028230514
	data : 0.017047739028930663
	model : 4.681827545166016
			 train-loss:  2.246150526226076 	 ± 0.3243195185402356
	data : 0.01714634895324707
	model : 4.705218648910522
			 train-loss:  2.2459756688868744 	 ± 0.3236397948402221
	data : 0.014552116394042969
	model : 4.681578588485718
			 train-loss:  2.24566719491603 	 ± 0.32298800819508533
	data : 0.014487648010253906
	model : 4.5241717338562015
			 train-loss:  2.245056235840552 	 ± 0.3224425085604883
	data : 0.011944293975830078
	model : 4.286132049560547
			 train-loss:  2.243869700351683 	 ± 0.32228246992604
	data : 0.009422063827514648
	model : 4.058715677261352
			 train-loss:  2.2431620873167923 	 ± 0.321792751852723
	data : 0.006816387176513672
	model : 3.831745958328247
			 train-loss:  2.242032872637113 	 ± 0.32159581694987516
	data : 0.006782388687133789
	model : 3.6231345176696776
			 train-loss:  2.2401897120772554 	 ± 0.32219568991046316
	data : 0.004198598861694336
	model : 3.5734198093414307
			 train-loss:  2.2387179909658825 	 ± 0.3223400261931708
	data : 0.004192066192626953
	model : 3.5718753814697264
			 train-loss:  2.237933574390019 	 ± 0.3219074596987136
	data : 0.004197072982788086
	model : 3.5745014190673827
			 train-loss:  2.237155218104847 	 ± 0.32147619064432353
	data : 0.0041254997253417965
	model : 3.5753555297851562
			 train-loss:  2.2365377168266143 	 ± 0.3209644154914558
	data : 0.0041541576385498045
	model : 3.574348211288452
			 train-loss:  2.2352849535825774 	 ± 0.3209110310935545
	data : 0.0041506290435791016
	model : 3.5717018127441404
			 train-loss:  2.2331075190532546 	 ± 0.3220765304240085
	data : 0.004104328155517578
	model : 3.5739384651184083
			 train-loss:  2.233265150458582 	 ± 0.32143607361490395
	data : 0.004114341735839844
	model : 3.568526792526245
			 train-loss:  2.233446678962095 	 ± 0.32080270773480546
	data : 0.004190397262573242
	model : 3.5679260730743407
			 train-loss:  2.233531946659088 	 ± 0.3201632867073934
	data : 0.00419764518737793
	model : 3.5728540420532227
			 train-loss:  2.2317533635523215 	 ± 0.32076001159731177
	data : 0.004208135604858399
	model : 3.573895740509033
			 train-loss:  2.233530715344444 	 ± 0.32135900046657273
	data : 0.004222583770751953
	model : 3.570747184753418
			 train-loss:  2.234549553497978 	 ± 0.3211308189326267
	data : 0.004250526428222656
	model : 3.5719104766845704
			 train-loss:  2.234465662888655 	 ± 0.3205008259607533
	data : 0.004239320755004883
	model : 3.570015287399292
			 train-loss:  2.2340936866461063 	 ± 0.3199267070784105
	data : 0.004198265075683594
	model : 3.5730552673339844
			 train-loss:  2.2388572469353676 	 ± 0.328237132731596
	data : 0.0041561126708984375
	model : 3.0479509353637697
#epoch  0    val-loss:  2.4601799437874243  train-loss:  2.2388572469353676  lr:  0.01
			 train-loss:  2.190659761428833 	 ± 0.0
	data : 4.0944554805755615
	model : 4.763955593109131
			 train-loss:  2.065042018890381 	 ± 0.12561774253845215
	data : 2.0579525232315063
	model : 4.7359459400177
			 train-loss:  2.0627883275349936 	 ± 0.1026159656669237
	data : 1.3803318341573079
	model : 4.705390214920044
			 train-loss:  2.1928465366363525 	 ± 0.24216304542864286
	data : 1.0392131209373474
	model : 4.725507855415344
			 train-loss:  2.1548924446105957 	 ± 0.22951340889590402
	data : 0.834553861618042
	model : 4.726796340942383
			 train-loss:  2.1355038483937583 	 ± 0.21395465026218335
	data : 0.01885075569152832
	model : 4.709996891021729
			 train-loss:  2.145296267100743 	 ± 0.19953051886382164
	data : 0.017770051956176758
	model : 4.699957180023193
			 train-loss:  2.1051139384508133 	 ± 0.2147980707860858
	data : 0.015956354141235352
	model : 4.712332105636596
			 train-loss:  2.1197401550081043 	 ± 0.20669579818435538
	data : 0.016277408599853514
	model : 4.713506650924683
			 train-loss:  2.11597193479538 	 ± 0.1964144420364889
	data : 0.01625370979309082
	model : 4.706125926971436
			 train-loss:  2.1368809938430786 	 ± 0.19860355506606403
	data : 0.01623687744140625
	model : 4.710135507583618
			 train-loss:  2.1442775825659433 	 ± 0.1917243573029429
	data : 0.016230058670043946
	model : 4.713378667831421
			 train-loss:  2.174465243632977 	 ± 0.21181645748285502
	data : 0.016197919845581055
	model : 4.691984605789185
			 train-loss:  2.179275861808232 	 ± 0.20484708655537126
	data : 0.015891027450561524
	model : 4.663567209243775
			 train-loss:  2.1633997201919555 	 ± 0.20662421565623998
	data : 0.015916728973388673
	model : 4.667478466033936
			 train-loss:  2.155518375337124 	 ± 0.20237824127211856
	data : 0.015848493576049803
	model : 4.663204383850098
			 train-loss:  2.152476626283982 	 ± 0.1967123682813468
	data : 0.015857839584350587
	model : 4.682045793533325
			 train-loss:  2.1430460479524402 	 ± 0.19508434163754312
	data : 0.015858793258666994
	model : 4.6858806133270265
			 train-loss:  2.1649135037472376 	 ± 0.21133431850523388
	data : 0.015843725204467772
	model : 4.69497275352478
			 train-loss:  2.14751198887825 	 ± 0.21950518477736275
	data : 0.01584181785583496
	model : 4.699807929992676
			 train-loss:  2.142673520814805 	 ± 0.21530521498862898
	data : 0.01614837646484375
	model : 4.6872845649719235
			 train-loss:  2.145847640254281 	 ± 0.21085730994239102
	data : 0.016393089294433595
	model : 4.673619890213013
			 train-loss:  2.13349535672561 	 ± 0.2142065887298773
	data : 0.014180755615234375
	model : 4.68420181274414
			 train-loss:  2.1491234749555588 	 ± 0.22268831552433713
	data : 0.014362955093383789
	model : 4.691810417175293
			 train-loss:  2.1578473615646363 	 ± 0.22233540243932598
	data : 0.014541435241699218
	model : 4.66959547996521
			 train-loss:  2.170830474450038 	 ± 0.2274770125810883
	data : 0.014488506317138671
	model : 4.650800895690918
			 train-loss:  2.173316129931697 	 ± 0.22358425934798343
	data : 0.01446990966796875
	model : 4.647723579406739
			 train-loss:  2.1918021653379713 	 ± 0.23964843279663994
	data : 0.016895198822021486
	model : 4.627918004989624
			 train-loss:  2.179931036357222 	 ± 0.24371467522170398
	data : 0.014379644393920898
	model : 4.625001764297485
			 train-loss:  2.172127823034922 	 ± 0.2432750750917969
	data : 0.014434480667114257
	model : 4.636852407455445
			 train-loss:  2.162260397788017 	 ± 0.24534592564578625
	data : 0.012010908126831055
	model : 4.665566873550415
			 train-loss:  2.1647588424384594 	 ± 0.24188230660970295
	data : 0.012787485122680664
	model : 4.678505945205688
			 train-loss:  2.1722254500244604 	 ± 0.24190518112914605
	data : 0.010243558883666992
	model : 4.713768196105957
			 train-loss:  2.171804095015806 	 ± 0.23833349408004684
	data : 0.012698745727539063
	model : 4.7124395847320555
			 train-loss:  2.1683725527354647 	 ± 0.235754707494017
	data : 0.012640857696533203
	model : 4.716051959991455
			 train-loss:  2.174092945125368 	 ± 0.23490782553746556
	data : 0.015047645568847657
	model : 4.708450603485107
			 train-loss:  2.1732615425780013 	 ± 0.2317653422179201
	data : 0.014294910430908202
	model : 4.692008638381958
			 train-loss:  2.1732937568112423 	 ± 0.2286955509924142
	data : 0.01683673858642578
	model : 4.67496395111084
			 train-loss:  2.1763515319579687 	 ± 0.22653010074138347
	data : 0.016902446746826172
	model : 4.666945171356201
			 train-loss:  2.1722313076257707 	 ± 0.2251556418829243
	data : 0.016889190673828124
	model : 4.664079046249389
			 train-loss:  2.1754584981174006 	 ± 0.2233275362724304
	data : 0.016910791397094727
	model : 4.673478126525879
			 train-loss:  2.1805190727824257 	 ± 0.22301943866843132
	data : 0.01686239242553711
	model : 4.68045825958252
			 train-loss:  2.1811200934787127 	 ± 0.22044534812572894
	data : 0.01690783500671387
	model : 4.665268421173096
			 train-loss:  2.190670395439321 	 ± 0.22674578061250095
	data : 0.0170501708984375
	model : 4.648233938217163
			 train-loss:  2.185219430923462 	 ± 0.22710899611069957
	data : 0.014600563049316406
	model : 4.639951896667481
			 train-loss:  2.1838056108225947 	 ± 0.22482698767550444
	data : 0.014639759063720703
	model : 4.640379428863525
			 train-loss:  2.1856688742942 	 ± 0.2227810654587607
	data : 0.01213212013244629
	model : 4.6378685474395756
			 train-loss:  2.1874589174985886 	 ± 0.22078952767614277
	data : 0.012128448486328125
	model : 4.641336059570312
			 train-loss:  2.183556152849781 	 ± 0.22019144743386299
	data : 0.01201162338256836
	model : 4.657140398025513
			 train-loss:  2.1832456684112547 	 ± 0.21798924671024725
	data : 0.014586639404296876
	model : 4.664586019515991
			 train-loss:  2.1803304915334665 	 ± 0.2168235994611827
	data : 0.0145751953125
	model : 4.655471611022949
			 train-loss:  2.178691267967224 	 ± 0.21504749942171103
	data : 0.014509439468383789
	model : 4.655779457092285
			 train-loss:  2.1710812685624608 	 ± 0.21996431644673528
	data : 0.014718008041381837
	model : 4.673199939727783
			 train-loss:  2.1757552182232893 	 ± 0.2205586602987411
	data : 0.014783573150634766
	model : 4.671031045913696
			 train-loss:  2.1694432865489612 	 ± 0.22341225050281074
	data : 0.014678668975830079
	model : 4.666807031631469
			 train-loss:  2.164226014699255 	 ± 0.2247639326213691
	data : 0.014686679840087891
	model : 4.67760910987854
			 train-loss:  2.1686004358425475 	 ± 0.2251757571405382
	data : 0.014691925048828125
	model : 4.667578649520874
			 train-loss:  2.1687908604227264 	 ± 0.223230776366807
	data : 0.01442127227783203
	model : 4.657819604873657
			 train-loss:  2.177875831975775 	 ± 0.2318932917657233
	data : 0.014364910125732423
	model : 4.680457401275635
			 train-loss:  2.17676095366478 	 ± 0.2301121278531299
	data : 0.014489459991455077
	model : 4.681056070327759
			 train-loss:  2.1757189895285935 	 ± 0.22836084135769774
	data : 0.015042781829833984
	model : 4.67606110572815
			 train-loss:  2.173739177565421 	 ± 0.22703890680362016
	data : 0.01766819953918457
	model : 4.6938543796539305
			 train-loss:  2.169401278571477 	 ± 0.2278050503476254
	data : 0.017766666412353516
	model : 4.711208248138428
			 train-loss:  2.167699374258518 	 ± 0.22642163659558448
	data : 0.015230607986450196
	model : 4.672279930114746
			 train-loss:  2.168815381710346 	 ± 0.2248505023802811
	data : 0.015110635757446289
	model : 4.661814737319946
			 train-loss:  2.1691416104634604 	 ± 0.22315608790127353
	data : 0.014563894271850586
	model : 4.663768243789673
			 train-loss:  2.1659266984284815 	 ± 0.22301912431491291
	data : 0.014600610733032227
	model : 4.6582142353057865
			 train-loss:  2.1584834316197563 	 ± 0.22960410349322838
	data : 0.014504480361938476
	model : 4.644690418243409
			 train-loss:  2.1548332321471064 	 ± 0.22991312214340642
	data : 0.01448664665222168
	model : 4.651895046234131
			 train-loss:  2.153478068964822 	 ± 0.22854237351552822
	data : 0.01459646224975586
	model : 4.652987337112426
			 train-loss:  2.155974332715424 	 ± 0.22788627253891286
	data : 0.014622640609741212
	model : 4.643297290802002
			 train-loss:  2.150878358218405 	 ± 0.23033599194922827
	data : 0.01455702781677246
	model : 4.6515415668487545
			 train-loss:  2.1539826540097797 	 ± 0.23026448242606565
	data : 0.014510011672973633
	model : 4.653520965576172
			 train-loss:  2.1574866046776644 	 ± 0.23065448599042399
	data : 0.017010974884033202
	model : 4.663016605377197
			 train-loss:  2.154741954803467 	 ± 0.23032496238800573
	data : 0.016942834854125975
	model : 4.683374261856079
			 train-loss:  2.151621876578582 	 ± 0.23039462669530678
	data : 0.016946744918823243
	model : 4.6833751678466795
			 train-loss:  2.1498079036737416 	 ± 0.22943929331223992
	data : 0.016917991638183593
	model : 4.673970460891724
			 train-loss:  2.147636743692251 	 ± 0.22875851882436876
	data : 0.01716156005859375
	model : 4.667289590835571
			 train-loss:  2.143615500836433 	 ± 0.23006377331013309
	data : 0.017198991775512696
	model : 4.657881212234497
			 train-loss:  2.145378164947033 	 ± 0.22915753279982337
	data : 0.014721059799194336
	model : 4.645287895202637
			 train-loss:  2.150451655741091 	 ± 0.23221560831566285
	data : 0.01461620330810547
	model : 4.666081094741822
			 train-loss:  2.1468836665153503 	 ± 0.23301856626322678
	data : 0.014576005935668945
	model : 4.662869501113891
			 train-loss:  2.1470733565020272 	 ± 0.23161695574856153
	data : 0.015075397491455079
	model : 4.682328367233277
			 train-loss:  2.1457496157714298 	 ± 0.2305497916558701
	data : 0.01507868766784668
	model : 4.7048797607421875
			 train-loss:  2.148334995438071 	 ± 0.2304112549028783
	data : 0.017639541625976564
	model : 4.698737287521363
			 train-loss:  2.1482828536698984 	 ± 0.22906824201452836
	data : 0.01770024299621582
	model : 4.691398763656617
			 train-loss:  2.1467846029106226 	 ± 0.22817138085029473
	data : 0.017814159393310547
	model : 4.7024054527282715
			 train-loss:  2.144335914741863 	 ± 0.2280180295158516
	data : 0.017159795761108397
	model : 4.701909351348877
			 train-loss:  2.1398290074273443 	 ± 0.23064152771935678
	data : 0.017238140106201172
	model : 4.68713002204895
			 train-loss:  2.1431582066747876 	 ± 0.23149706382072946
	data : 0.014577245712280274
	model : 4.68980131149292
			 train-loss:  2.1392485925129483 	 ± 0.2331901308650763
	data : 0.014572334289550782
	model : 4.7024742603302006
			 train-loss:  2.1418572936369027 	 ± 0.23325063798650794
	data : 0.014491558074951172
	model : 4.686015796661377
			 train-loss:  2.1425720812172018 	 ± 0.2320944972004168
	data : 0.014584732055664063
	model : 4.66851806640625
			 train-loss:  2.1391988764417933 	 ± 0.2331372874708303
	data : 0.01452932357788086
	model : 4.6782999515533445
			 train-loss:  2.1384959547143234 	 ± 0.2320071191225733
	data : 0.017083215713500976
	model : 4.69590573310852
			 train-loss:  2.1373400837183 	 ± 0.23107039151396372
	data : 0.017270565032958984
	model : 4.679829692840576
			 train-loss:  2.1377137690475307 	 ± 0.22990537754498294
	data : 0.017074775695800782
	model : 4.695541667938232
			 train-loss:  2.1390197374382796 	 ± 0.22909074499865895
	data : 0.017024135589599608
	model : 4.70625901222229
			 train-loss:  2.133584570403051 	 ± 0.23419535535240987
	data : 0.016980075836181642
	model : 4.708252859115601
			 train-loss:  2.1347357642650606 	 ± 0.23330278461141443
	data : 0.0170804500579834
	model : 4.715422630310059
			 train-loss:  2.139399782265767 	 ± 0.23678383165476338
	data : 0.016922140121459962
	model : 4.71332311630249
			 train-loss:  2.136221014985851 	 ± 0.23777609479260184
	data : 0.014567804336547852
	model : 4.705030059814453
			 train-loss:  2.1359517933095544 	 ± 0.23663464826316177
	data : 0.014461421966552734
	model : 4.700819683074951
			 train-loss:  2.135784551501274 	 ± 0.23550035029528213
	data : 0.015319442749023438
	model : 4.679846382141113
			 train-loss:  2.1335101717994327 	 ± 0.23552110749497474
	data : 0.015287542343139648
	model : 4.684246778488159
			 train-loss:  2.1342197566662193 	 ± 0.23452026990980038
	data : 0.015251731872558594
	model : 4.669338083267212
			 train-loss:  2.1343123400322743 	 ± 0.2334237545239629
	data : 0.017775630950927733
	model : 4.657344007492066
			 train-loss:  2.1305942193225578 	 ± 0.23550234877312878
	data : 0.01784806251525879
	model : 4.652660655975342
			 train-loss:  2.1310376628823238 	 ± 0.2344648669884081
	data : 0.01701664924621582
	model : 4.673521566390991
			 train-loss:  2.131997055357153 	 ± 0.23361151420420795
	data : 0.01702432632446289
	model : 4.651521682739258
			 train-loss:  2.131898556743656 	 ± 0.2325591238145725
	data : 0.017452144622802736
	model : 4.652586174011231
			 train-loss:  2.132964607860361 	 ± 0.23179086067543023
	data : 0.017460441589355467
	model : 4.651441192626953
			 train-loss:  2.132371264221394 	 ± 0.23084837715646606
	data : 0.016849613189697264
	model : 4.647605562210083
			 train-loss:  2.131416742216077 	 ± 0.23005752377067115
	data : 0.016857242584228514
	model : 4.641894721984864
			 train-loss:  2.1293865732524706 	 ± 0.23007845384999984
	data : 0.016782665252685548
	model : 4.643411111831665
			 train-loss:  2.1323392504248124 	 ± 0.23126252196532113
	data : 0.016309356689453124
	model : 4.655284976959228
			 train-loss:  2.1341513415687103 	 ± 0.23109769827682497
	data : 0.01386885643005371
	model : 4.653202676773072
			 train-loss:  2.1348822025929466 	 ± 0.23025214039582212
	data : 0.014493846893310547
	model : 4.669507312774658
			 train-loss:  2.138583294483794 	 ± 0.23278081692206998
	data : 0.014484024047851563
	model : 4.687656021118164
			 train-loss:  2.1404086182514828 	 ± 0.23266249412002768
	data : 0.014479970932006836
	model : 4.696655750274658
			 train-loss:  2.137442211474269 	 ± 0.23396669404733464
	data : 0.014523553848266601
	model : 4.714116144180298
			 train-loss:  2.138958797103069 	 ± 0.23360228139798048
	data : 0.016957807540893554
	model : 4.717894458770752
			 train-loss:  2.13854664806428 	 ± 0.2326952747278653
	data : 0.014454841613769531
	model : 4.703262710571289
			 train-loss:  2.137698609021402 	 ± 0.23194585301266643
	data : 0.011958551406860352
	model : 4.690138721466065
			 train-loss:  2.1369463090896605 	 ± 0.23116804740958527
	data : 0.011992788314819336
	model : 4.687778949737549
			 train-loss:  2.138779378126538 	 ± 0.2311591821817673
	data : 0.012091398239135742
	model : 4.667811441421509
			 train-loss:  2.1445983160199145 	 ± 0.23933279694806317
	data : 0.012131261825561523
	model : 4.681534671783448
			 train-loss:  2.1480754958465695 	 ± 0.2415951473665642
	data : 0.014091205596923829
	model : 4.683124113082886
			 train-loss:  2.1472742474356363 	 ± 0.24082758214204994
	data : 0.016544342041015625
	model : 4.661377096176148
			 train-loss:  2.145472043294173 	 ± 0.24077119983712067
	data : 0.016493797302246094
	model : 4.66162371635437
			 train-loss:  2.1445929066825458 	 ± 0.2400598264135419
	data : 0.01638646125793457
	model : 4.674469327926635
			 train-loss:  2.146027145060626 	 ± 0.23971151671878854
	data : 0.016393566131591798
	model : 4.6686805248260494
			 train-loss:  2.1460482563291277 	 ± 0.23880876844935386
	data : 0.014351463317871094
	model : 4.661042785644531
			 train-loss:  2.1486816842164567 	 ± 0.23984657803384862
	data : 0.014336538314819337
	model : 4.685052394866943
			 train-loss:  2.146809626508642 	 ± 0.23993723358843794
	data : 0.014331674575805664
	model : 4.674427795410156
			 train-loss:  2.146077149931122 	 ± 0.23920493015674774
	data : 0.014347696304321289
	model : 4.674515962600708
			 train-loss:  2.1462858375841685 	 ± 0.23834274597749588
	data : 0.01430516242980957
	model : 4.684305047988891
			 train-loss:  2.1498695689698923 	 ± 0.24115374061752187
	data : 0.016932916641235352
	model : 4.701407527923584
			 train-loss:  2.152425573026534 	 ± 0.24215350511143197
	data : 0.01440730094909668
	model : 4.697308349609375
			 train-loss:  2.150746885367802 	 ± 0.24209745107735176
	data : 0.01439976692199707
	model : 4.69712266921997
			 train-loss:  2.1518297144707215 	 ± 0.24157741277157443
	data : 0.011875438690185546
	model : 4.693464660644532
			 train-loss:  2.1520324592858975 	 ± 0.24073732325129543
	data : 0.010219430923461914
	model : 4.674288034439087
			 train-loss:  2.1515285852072124 	 ± 0.23996923786366717
	data : 0.010149192810058594
	model : 4.669066429138184
			 train-loss:  2.1484552605284586 	 ± 0.2419421812537871
	data : 0.01112194061279297
	model : 4.658694744110107
			 train-loss:  2.146602624860303 	 ± 0.2421292375383955
	data : 0.011202192306518555
	model : 4.665335273742675
			 train-loss:  2.146266064415239 	 ± 0.2413326343718843
	data : 0.013721036911010741
	model : 4.646185350418091
			 train-loss:  2.1467078713332715 	 ± 0.2405696123110065
	data : 0.012840604782104493
	model : 4.650295543670654
			 train-loss:  2.1459353445349514 	 ± 0.23993838552818275
	data : 0.012815713882446289
	model : 4.629980802536011
			 train-loss:  2.144963002844945 	 ± 0.23942426044649703
	data : 0.014426040649414062
	model : 4.642639923095703
			 train-loss:  2.144748604297638 	 ± 0.23863919562994898
	data : 0.011812210083007812
	model : 4.624189996719361
			 train-loss:  2.144768028859271 	 ± 0.23784780597380228
	data : 0.011767768859863281
	model : 4.639398193359375
			 train-loss:  2.145123195491339 	 ± 0.23710429142225878
	data : 0.01432504653930664
	model : 4.643988513946534
			 train-loss:  2.1444261611676683 	 ± 0.23648436403859774
	data : 0.012772655487060547
	model : 4.66339225769043
			 train-loss:  2.1442199597110996 	 ± 0.23572910605723973
	data : 0.012780523300170899
	model : 4.6608131408691404
			 train-loss:  2.145814911780819 	 ± 0.23579962501394594
	data : 0.015379238128662109
	model : 4.650215673446655
			 train-loss:  2.144511525447552 	 ± 0.23560212221206384
	data : 0.01542816162109375
	model : 4.659635877609253
			 train-loss:  2.1461447333074677 	 ± 0.23573483610256138
	data : 0.015442276000976562
	model : 4.670950031280517
			 train-loss:  2.1465418640571303 	 ± 0.23504033532148957
	data : 0.017010879516601563
	model : 4.664955902099609
			 train-loss:  2.150119109723553 	 ± 0.23857575195258113
	data : 0.01697964668273926
	model : 4.651980400085449
			 train-loss:  2.150660516321659 	 ± 0.23792699666384504
	data : 0.017141294479370118
	model : 4.688012361526489
			 train-loss:  2.154284065554601 	 ± 0.24157496332264541
	data : 0.017037153244018555
	model : 4.692323064804077
			 train-loss:  2.1544160931198686 	 ± 0.24083403396272407
	data : 0.01785144805908203
	model : 4.680696249008179
			 train-loss:  2.1540526027328397 	 ± 0.24013871336269613
	data : 0.017859792709350585
	model : 4.673411798477173
			 train-loss:  2.157102060027239 	 ± 0.24255049777617405
	data : 0.018015527725219728
	model : 4.664006853103638
			 train-loss:  2.1548143408515235 	 ± 0.24358266376822404
	data : 0.017792320251464842
	model : 4.658498525619507
			 train-loss:  2.1575566278882774 	 ± 0.24538930723735655
	data : 0.017945480346679688
	model : 4.6427349090576175
			 train-loss:  2.1566930609548876 	 ± 0.24490637379472363
	data : 0.017103147506713868
	model : 4.638821697235107
			 train-loss:  2.156431638768741 	 ± 0.24419976724659245
	data : 0.01711430549621582
	model : 4.6487589359283445
			 train-loss:  2.1566546054986806 	 ± 0.2434933617167255
	data : 0.01699485778808594
	model : 4.6729785919189455
			 train-loss:  2.156355540191426 	 ± 0.2428072767550597
	data : 0.01783719062805176
	model : 4.675384092330932
			 train-loss:  2.1587685932192886 	 ± 0.24413210688861509
	data : 0.017868185043334962
	model : 4.663884878158569
			 train-loss:  2.157032852256021 	 ± 0.24447731877801765
	data : 0.017859125137329103
	model : 4.660675764083862
			 train-loss:  2.1578404786269787 	 ± 0.24399971661347833
	data : 0.015246200561523437
	model : 4.6633576393127445
			 train-loss:  2.158176069287048 	 ± 0.2433375947762017
	data : 0.01267085075378418
	model : 4.656504440307617
			 train-loss:  2.1557976709093367 	 ± 0.24466120144568015
	data : 0.011899805068969727
	model : 4.644822359085083
			 train-loss:  2.154671236872673 	 ± 0.2444198116705921
	data : 0.009290122985839843
	model : 4.663442373275757
			 train-loss:  2.1536742192877214 	 ± 0.24408702560351717
	data : 0.00929274559020996
	model : 4.668136072158814
			 train-loss:  2.1525890425349887 	 ± 0.24382822282859531
	data : 0.012738847732543945
	model : 4.652705430984497
			 train-loss:  2.1526358167552417 	 ± 0.2431469853257289
	data : 0.015309476852416992
	model : 4.639525127410889
			 train-loss:  2.151081677940157 	 ± 0.24336054874878082
	data : 0.015227842330932616
	model : 4.641163110733032
			 train-loss:  2.1490785680423126 	 ± 0.24417082373633742
	data : 0.017812299728393554
	model : 4.6297860622406
			 train-loss:  2.1497715515094797 	 ± 0.24367751848144578
	data : 0.01784543991088867
	model : 4.63432765007019
			 train-loss:  2.1477719330396807 	 ± 0.24450354053651188
	data : 0.01713595390319824
	model : 4.6550342559814455
			 train-loss:  2.1478027170119076 	 ± 0.24383857915208293
	data : 0.01723346710205078
	model : 4.679277038574218
			 train-loss:  2.1473415104118554 	 ± 0.24325912325954677
	data : 0.017308950424194336
	model : 4.681690502166748
			 train-loss:  2.149423696661508 	 ± 0.24425176213089697
	data : 0.01722860336303711
	model : 4.698456478118897
			 train-loss:  2.1503293450503427 	 ± 0.2439107390899516
	data : 0.014649248123168946
	model : 4.705064582824707
			 train-loss:  2.1509922058024307 	 ± 0.2434299988229262
	data : 0.014415693283081055
	model : 4.688853216171265
			 train-loss:  2.1500744296129417 	 ± 0.24311105267990465
	data : 0.011805486679077149
	model : 4.674170637130738
			 train-loss:  2.1486460579069036 	 ± 0.2432643049066509
	data : 0.009322643280029297
	model : 4.676660013198853
			 train-loss:  2.1478497632511 	 ± 0.24287479957246724
	data : 0.00935816764831543
	model : 4.667282199859619
			 train-loss:  2.147907597323259 	 ± 0.24224280603821513
	data : 0.011859703063964843
	model : 4.645497179031372
			 train-loss:  2.1467465384636517 	 ± 0.242149446682822
	data : 0.011954498291015626
	model : 4.659438037872315
			 train-loss:  2.147063276816889 	 ± 0.2415646241758918
	data : 0.014467716217041016
	model : 4.665481615066528
			 train-loss:  2.149720896818699 	 ± 0.2437712662549295
	data : 0.0168609619140625
	model : 4.658399820327759
			 train-loss:  2.15003877817368 	 ± 0.24318912157267888
	data : 0.015177202224731446
	model : 4.678099679946899
			 train-loss:  2.1501896823118183 	 ± 0.24258030487376278
	data : 0.015215349197387696
	model : 4.673429918289185
			 train-loss:  2.150449660691348 	 ± 0.2419944654250584
	data : 0.015227079391479492
	model : 4.669770669937134
			 train-loss:  2.151462720866179 	 ± 0.24180622180089967
	data : 0.015102720260620118
	model : 4.6567299365997314
			 train-loss:  2.1509379798173907 	 ± 0.24131451041660607
	data : 0.01524648666381836
	model : 4.653163146972656
			 train-loss:  2.1504281937186396 	 ± 0.2408214160340922
	data : 0.01689181327819824
	model : 4.631665802001953
			 train-loss:  2.1505682084820057 	 ± 0.24023278526636535
	data : 0.014370250701904296
	model : 4.661116743087769
			 train-loss:  2.1517471685785377 	 ± 0.24022544797538756
	data : 0.014353513717651367
	model : 4.671153450012207
			 train-loss:  2.1525617797000733 	 ± 0.23991684231943308
	data : 0.014268064498901367
	model : 4.678933572769165
			 train-loss:  2.152663682728279 	 ± 0.23933538952283204
	data : 0.014178991317749023
	model : 4.678173017501831
			 train-loss:  2.1538664247225787 	 ± 0.23937400463037553
	data : 0.014171648025512695
	model : 4.681556987762451
			 train-loss:  2.155966522037119 	 ± 0.24068993978162417
	data : 0.016672277450561525
	model : 4.646911144256592
			 train-loss:  2.1562026171730113 	 ± 0.24013468709647076
	data : 0.016756343841552734
	model : 4.645553255081177
			 train-loss:  2.155685617022537 	 ± 0.23967552355037086
	data : 0.016928815841674806
	model : 4.649941110610962
			 train-loss:  2.1549194960367113 	 ± 0.23936057064442937
	data : 0.016889286041259766
	model : 4.665204334259033
			 train-loss:  2.1558623957972958 	 ± 0.2391833026713478
	data : 0.014420318603515624
	model : 4.674893760681153
			 train-loss:  2.1544421838139587 	 ± 0.23950863780199871
	data : 0.014432430267333984
	model : 4.699991178512573
			 train-loss:  2.154120558864074 	 ± 0.23899163383081845
	data : 0.01424717903137207
	model : 4.683076572418213
			 train-loss:  2.1528732787782903 	 ± 0.2391264621252526
	data : 0.01428227424621582
	model : 4.6797301292419435
			 train-loss:  2.15126189298408 	 ± 0.23973145250629552
	data : 0.014404726028442384
	model : 4.657137441635132
			 train-loss:  2.1516552379837743 	 ± 0.23924540529450092
	data : 0.017207908630371093
	model : 4.648722887039185
			 train-loss:  2.1516727676040017 	 ± 0.23869365108059568
	data : 0.017226696014404297
	model : 4.657351398468018
			 train-loss:  2.1522645447232307 	 ± 0.23830505838853172
	data : 0.017328310012817382
	model : 4.657277154922485
			 train-loss:  2.149791502517108 	 ± 0.24054784016434147
	data : 0.017357063293457032
	model : 4.645214176177978
			 train-loss:  2.1512134589932184 	 ± 0.24092126901174066
	data : 0.017380666732788087
	model : 4.658667755126953
			 train-loss:  2.1513028517028325 	 ± 0.24037923708963466
	data : 0.017081832885742186
	model : 4.665589952468872
			 train-loss:  2.1520893750963985 	 ± 0.24012207802314794
	data : 0.01705341339111328
	model : 4.6720798969268795
			 train-loss:  2.153140307007349 	 ± 0.24009423817335399
	data : 0.016891098022460936
	model : 4.678735828399658
			 train-loss:  2.1556861767811433 	 ± 0.24255568920590703
	data : 0.01692957878112793
	model : 4.709395694732666
			 train-loss:  2.155540691481696 	 ± 0.2420258712986865
	data : 0.01672391891479492
	model : 4.692698240280151
			 train-loss:  2.15439287434637 	 ± 0.24210280495154765
	data : 0.01682000160217285
	model : 4.665764379501343
			 train-loss:  2.153743091658874 	 ± 0.2417663725525526
	data : 0.016866779327392577
	model : 4.665624141693115
			 train-loss:  2.154944462734356 	 ± 0.2419137084650979
	data : 0.016956567764282227
	model : 4.673481225967407
			 train-loss:  2.1542506504267065 	 ± 0.24161216922596404
	data : 0.016959428787231445
	model : 4.66054253578186
			 train-loss:  2.151833742597829 	 ± 0.2438448692484644
	data : 0.017202425003051757
	model : 4.678052806854248
			 train-loss:  2.1512540026660605 	 ± 0.24347529423813885
	data : 0.017202472686767577
	model : 4.679198741912842
			 train-loss:  2.152102035695109 	 ± 0.2432916494326433
	data : 0.017223358154296875
	model : 4.6717578887939455
			 train-loss:  2.150571139073679 	 ± 0.24388627332609572
	data : 0.017406225204467773
	model : 4.6764922618865965
			 train-loss:  2.1486852607156477 	 ± 0.24506121122692656
	data : 0.016939210891723632
	model : 4.674831533432007
			 train-loss:  2.148597833958078 	 ± 0.24454290550475455
	data : 0.014251470565795898
	model : 4.677966976165772
			 train-loss:  2.149313747882843 	 ± 0.2442709208540426
	data : 0.011654329299926759
	model : 4.527113533020019
			 train-loss:  2.1496715183499493 	 ± 0.2438169924339612
	data : 0.009072589874267577
	model : 4.29616527557373
			 train-loss:  2.1483106512983308 	 ± 0.24420455351005985
	data : 0.006476593017578125
	model : 4.068733072280883
			 train-loss:  2.150343335323254 	 ± 0.2457024846147747
	data : 0.0043296337127685545
	model : 3.8466963291168215
			 train-loss:  2.1500531027714413 	 ± 0.24523112084899953
	data : 0.004317140579223633
	model : 3.6280874252319335
			 train-loss:  2.1501863626028985 	 ± 0.24473052133163184
	data : 0.004278850555419922
	model : 3.5748340129852294
			 train-loss:  2.149175774952597 	 ± 0.2447277395042568
	data : 0.004287433624267578
	model : 3.574136734008789
			 train-loss:  2.1496603744020186 	 ± 0.24433998686900052
	data : 0.004208421707153321
	model : 3.570555877685547
			 train-loss:  2.1503203911859483 	 ± 0.24405574141853967
	data : 0.004262304306030274
	model : 3.569609260559082
			 train-loss:  2.149910762358685 	 ± 0.24364119513458266
	data : 0.004272365570068359
	model : 3.5650563716888426
			 train-loss:  2.1489365066939254 	 ± 0.24362322264757705
	data : 0.004253387451171875
	model : 3.5661452770233155
			 train-loss:  2.150000411489232 	 ± 0.24370151370710863
	data : 0.004251909255981445
	model : 3.5670420646667482
			 train-loss:  2.1491652385842417 	 ± 0.24356361829246226
	data : 0.004276371002197266
	model : 3.5699235439300536
			 train-loss:  2.149958104493628 	 ± 0.24339451933485712
	data : 0.004430961608886719
	model : 3.5678701400756836
			 train-loss:  2.1503193259239195 	 ± 0.24297411019197296
	data : 0.004447031021118164
	model : 3.5670605182647703
			 train-loss:  2.1503543972494117 	 ± 0.2424902490195607
	data : 0.0044575691223144535
	model : 3.565303373336792
			 train-loss:  2.1506593875468725 	 ± 0.24205687201669435
	data : 0.004399919509887695
	model : 3.5658012866973876
			 train-loss:  2.1520751790095694 	 ± 0.24262124291909207
	data : 0.004421758651733399
	model : 3.5637967109680178
			 train-loss:  2.1521414016175457 	 ± 0.24214546205957463
	data : 0.004235363006591797
	model : 3.5655081272125244
			 train-loss:  2.1519621432996265 	 ± 0.24168708653047663
	data : 0.004208517074584961
	model : 3.564613389968872
			 train-loss:  2.152870539110154 	 ± 0.2416503570078627
	data : 0.004203128814697266
	model : 3.02467679977417
#epoch  1    val-loss:  2.4464856135217765  train-loss:  2.152870539110154  lr:  0.01
			 train-loss:  2.130728244781494 	 ± 0.0
	data : 4.109224319458008
	model : 4.684199810028076
			 train-loss:  2.108570694923401 	 ± 0.02215754985809326
	data : 2.062634229660034
	model : 4.688938140869141
			 train-loss:  2.2178002993265786 	 ± 0.15552979660917735
	data : 1.3803856372833252
	model : 4.696826934814453
			 train-loss:  2.243655502796173 	 ± 0.14194228708996984
	data : 1.0402119159698486
	model : 4.684446036815643
			 train-loss:  2.1672152042388917 	 ± 0.19872233710776066
	data : 0.8360121726989747
	model : 4.692402076721192
			 train-loss:  2.164095918337504 	 ± 0.18154188436092378
	data : 0.017264461517333983
	model : 4.699880838394165
			 train-loss:  2.132224304335458 	 ± 0.1853214679515023
	data : 0.017917633056640625
	model : 4.71453742980957
			 train-loss:  2.121005281805992 	 ± 0.17587525675561605
	data : 0.017913341522216797
	model : 4.719597339630127
			 train-loss:  2.113024274508158 	 ± 0.1673462790111744
	data : 0.017145109176635743
	model : 4.73318395614624
			 train-loss:  2.1068726420402526 	 ± 0.1598276653998893
	data : 0.013950395584106445
	model : 4.721424102783203
			 train-loss:  2.1610349850221113 	 ± 0.22925578409934974
	data : 0.014081096649169922
	model : 4.724975204467773
			 train-loss:  2.1865435341993966 	 ± 0.23523586083656595
	data : 0.013411521911621094
	model : 4.716178703308105
			 train-loss:  2.19874474635491 	 ± 0.22992550246015345
	data : 0.013689327239990234
	model : 4.733068895339966
			 train-loss:  2.20857173204422 	 ± 0.22437695526382456
	data : 0.01414022445678711
	model : 4.735187005996704
			 train-loss:  2.1903156916300457 	 ± 0.22727658409748744
	data : 0.016681861877441407
	model : 4.739543342590332
			 train-loss:  2.1852238327264786 	 ± 0.22094147580409365
	data : 0.016647720336914064
	model : 4.7302937507629395
			 train-loss:  2.183435383964987 	 ± 0.21446406212153296
	data : 0.016625070571899415
	model : 4.728329610824585
			 train-loss:  2.1488140953911676 	 ± 0.2526189560821849
	data : 0.016346454620361328
	model : 4.700592184066773
			 train-loss:  2.1593486886275444 	 ± 0.24991034428046954
	data : 0.01589822769165039
	model : 4.685437726974487
			 train-loss:  2.14414821267128 	 ± 0.2524330724887412
	data : 0.015884923934936523
	model : 4.68159761428833
			 train-loss:  2.1567295222055343 	 ± 0.25269317443744616
	data : 0.016043901443481445
	model : 4.675688934326172
			 train-loss:  2.140191126953472 	 ± 0.2582542971255586
	data : 0.016263341903686522
	model : 4.6671709537506105
			 train-loss:  2.1323730479116025 	 ± 0.2552257413210174
	data : 0.016369962692260744
	model : 4.6733633518219
			 train-loss:  2.139912153283755 	 ± 0.2524545135904272
	data : 0.016649818420410155
	model : 4.686814308166504
			 train-loss:  2.1520687437057493 	 ± 0.2544223550543606
	data : 0.016851282119750975
	model : 4.687079048156738
			 train-loss:  2.158281330878918 	 ± 0.2514080290916097
	data : 0.016994333267211913
	model : 4.658717298507691
			 train-loss:  2.150422281689114 	 ± 0.24994182372072152
	data : 0.01689000129699707
	model : 4.638112545013428
			 train-loss:  2.1370819551604137 	 ± 0.2550389137956139
	data : 0.014482688903808594
	model : 4.618361282348633
			 train-loss:  2.1368448734283447 	 ± 0.25060625608590426
	data : 0.014413404464721679
	model : 4.602215909957886
			 train-loss:  2.1394041379292807 	 ± 0.24677923523911163
	data : 0.014435100555419921
	model : 4.5936346530914305
			 train-loss:  2.14025221332427 	 ± 0.24281073300932648
	data : 0.014360857009887696
	model : 4.631030082702637
			 train-loss:  2.1395919546484947 	 ± 0.23901497520991652
	data : 0.014438533782958984
	model : 4.6494788646698
			 train-loss:  2.133996656446746 	 ± 0.23748439452549944
	data : 0.017040061950683593
	model : 4.660089445114136
			 train-loss:  2.135898803963381 	 ± 0.23422093790153348
	data : 0.017019176483154298
	model : 4.658995485305786
			 train-loss:  2.1491525411605834 	 ± 0.24344306859740394
	data : 0.017057466506958007
	model : 4.676531028747559
			 train-loss:  2.1419105297989316 	 ± 0.24383175812163238
	data : 0.016896867752075197
	model : 4.657027721405029
			 train-loss:  2.133898087449976 	 ± 0.24527175899966902
	data : 0.0168792724609375
	model : 4.670481538772583
			 train-loss:  2.1440200648809733 	 ± 0.24973173226194917
	data : 0.016767549514770507
	model : 4.679719257354736
			 train-loss:  2.1348474637056007 	 ± 0.25291105699216715
	data : 0.016785383224487305
	model : 4.682287359237671
			 train-loss:  2.137266570329666 	 ± 0.2501861980159139
	data : 0.016695594787597655
	model : 4.669657135009766
			 train-loss:  2.1461876950612884 	 ± 0.2534757000971265
	data : 0.016820526123046874
	model : 4.670977354049683
			 train-loss:  2.1326025554112027 	 ± 0.26511689323879006
	data : 0.016872596740722657
	model : 4.659301376342773
			 train-loss:  2.1309559234353 	 ± 0.26223322657340287
	data : 0.017043352127075195
	model : 4.642987442016602
			 train-loss:  2.1345278837464075 	 ± 0.26029219768801637
	data : 0.017058897018432616
	model : 4.64481782913208
			 train-loss:  2.1482921759287517 	 ± 0.27309793129838567
	data : 0.01776275634765625
	model : 4.644291019439697
			 train-loss:  2.153863170872564 	 ± 0.27268615866365503
	data : 0.017867231369018556
	model : 4.664202499389648
			 train-loss:  2.156095347505935 	 ± 0.2701941185598547
	data : 0.017943859100341797
	model : 4.670327138900757
			 train-loss:  2.1542089184125266 	 ± 0.2676773841353542
	data : 0.017855167388916016
	model : 4.687289237976074
			 train-loss:  2.1556855075213375 	 ± 0.26512934220228523
	data : 0.017793941497802734
	model : 4.681591129302978
			 train-loss:  2.1501306223869325 	 ± 0.26532937535947543
	data : 0.014580869674682617
	model : 4.6982721328735355
			 train-loss:  2.152106609998965 	 ± 0.26308652179247116
	data : 0.014487981796264648
	model : 4.679733037948608
			 train-loss:  2.155405826293505 	 ± 0.2616077135002581
	data : 0.014881515502929687
	model : 4.673774528503418
			 train-loss:  2.157833074623684 	 ± 0.25971842694519665
	data : 0.01483449935913086
	model : 4.6632524013519285
			 train-loss:  2.157468131294957 	 ± 0.25731610550374584
	data : 0.01487903594970703
	model : 4.661245107650757
			 train-loss:  2.1615289493040604 	 ± 0.2567064556899499
	data : 0.01750025749206543
	model : 4.641982650756836
			 train-loss:  2.16564310661384 	 ± 0.2562272364783882
	data : 0.017490148544311523
	model : 4.636633491516113
			 train-loss:  2.1729958998529533 	 ± 0.2598618096434036
	data : 0.01705760955810547
	model : 4.635858249664307
			 train-loss:  2.1730832375329117 	 ± 0.2576127253789067
	data : 0.01452016830444336
	model : 4.630431985855102
			 train-loss:  2.1756777945211376 	 ± 0.2561834059090776
	data : 0.014560174942016602
	model : 4.63944411277771
			 train-loss:  2.170005359252294 	 ± 0.257748947549736
	data : 0.014510679244995116
	model : 4.669583034515381
			 train-loss:  2.16204656147566 	 ± 0.2629562267655229
	data : 0.011978912353515624
	model : 4.683299922943116
			 train-loss:  2.161993449734103 	 ± 0.2608273213648331
	data : 0.01197361946105957
	model : 4.692807817459107
			 train-loss:  2.1622882797604515 	 ± 0.2587593968974989
	data : 0.014605188369750976
	model : 4.695106554031372
			 train-loss:  2.159470185637474 	 ± 0.2577024554931613
	data : 0.014568519592285157
	model : 4.689320659637451
			 train-loss:  2.159931736726027 	 ± 0.25573910259167393
	data : 0.014474582672119141
	model : 4.676034069061279
			 train-loss:  2.1529358300295742 	 ± 0.25998618805691553
	data : 0.017015838623046876
	model : 4.682272815704346
			 train-loss:  2.1534087230910117 	 ± 0.25806729624023816
	data : 0.01695704460144043
	model : 4.672505903244018
			 train-loss:  2.151438558802885 	 ± 0.25666982678158445
	data : 0.01685986518859863
	model : 4.691882562637329
			 train-loss:  2.146628887757011 	 ± 0.25787141308423656
	data : 0.016931438446044923
	model : 4.692561388015747
			 train-loss:  2.1446506806782315 	 ± 0.2565496383887801
	data : 0.017089271545410158
	model : 4.691642761230469
			 train-loss:  2.140343849088105 	 ± 0.25727246790828984
	data : 0.017141437530517577
	model : 4.687022733688354
			 train-loss:  2.1350029243363275 	 ± 0.25941306492479754
	data : 0.01714916229248047
	model : 4.683791780471802
			 train-loss:  2.134845929603054 	 ± 0.2576335802135556
	data : 0.014593029022216797
	model : 4.674063205718994
			 train-loss:  2.141022289121473 	 ± 0.26127162646332935
	data : 0.01198573112487793
	model : 4.667980051040649
			 train-loss:  2.135114541053772 	 ± 0.2644530229473442
	data : 0.011883783340454101
	model : 4.659310722351075
			 train-loss:  2.1308159279195884 	 ± 0.2653319700158695
	data : 0.01181807518005371
	model : 4.663264322280884
			 train-loss:  2.1331718324066755 	 ± 0.26440230074305227
	data : 0.01186513900756836
	model : 4.678966665267945
			 train-loss:  2.1353429540609703 	 ± 0.26339186218725863
	data : 0.014519739151000976
	model : 4.677949714660644
			 train-loss:  2.1412559204463717 	 ± 0.2668786848514012
	data : 0.017181205749511718
	model : 4.69322829246521
			 train-loss:  2.14406663030386 	 ± 0.26637949738108363
	data : 0.018007993698120117
	model : 4.6816582679748535
			 train-loss:  2.1421164318367287 	 ± 0.26530411494429607
	data : 0.017897701263427733
	model : 4.665872192382812
			 train-loss:  2.138683547333973 	 ± 0.26548533781593964
	data : 0.017879581451416014
	model : 4.646809673309326
			 train-loss:  2.1445745402071847 	 ± 0.2692192222547676
	data : 0.01778721809387207
	model : 4.64934892654419
			 train-loss:  2.1405426320575534 	 ± 0.27012111363784247
	data : 0.017727231979370116
	model : 4.648850679397583
			 train-loss:  2.1409506741692037 	 ± 0.2685535053511441
	data : 0.016927528381347656
	model : 4.660256862640381
			 train-loss:  2.1375114571216494 	 ± 0.26886384530186125
	data : 0.01702742576599121
	model : 4.6656440734863285
			 train-loss:  2.136828270451776 	 ± 0.267389254495385
	data : 0.017072153091430665
	model : 4.671584939956665
			 train-loss:  2.137373229319399 	 ± 0.2659142430043477
	data : 0.01699357032775879
	model : 4.687786197662353
			 train-loss:  2.1385479227880415 	 ± 0.2646456452436396
	data : 0.01710848808288574
	model : 4.6883903503417965
			 train-loss:  2.1399666163656446 	 ± 0.26351139381403355
	data : 0.01697998046875
	model : 4.694899368286133
			 train-loss:  2.14032512313717 	 ± 0.2620815986780854
	data : 0.016986942291259764
	model : 4.715352964401245
			 train-loss:  2.143065262099971 	 ± 0.26196074133454356
	data : 0.014372873306274413
	model : 4.722399425506592
			 train-loss:  2.1414029521326863 	 ± 0.2610359456162435
	data : 0.014436006546020508
	model : 4.710722351074219
			 train-loss:  2.137628193865431 	 ± 0.2621831653359092
	data : 0.01438908576965332
	model : 4.697141456604004
			 train-loss:  2.1348595945458664 	 ± 0.2621773378383322
	data : 0.014533090591430663
	model : 4.706127595901489
			 train-loss:  2.1346034705638885 	 ± 0.2608202033291268
	data : 0.014511489868164062
	model : 4.7100057125091555
			 train-loss:  2.1354678979853996 	 ± 0.2596104810751919
	data : 0.0170412540435791
	model : 4.693726110458374
			 train-loss:  2.135371993999092 	 ± 0.2582842686215276
	data : 0.017060518264770508
	model : 4.692079544067383
			 train-loss:  2.135500765810109 	 ± 0.2569796536222376
	data : 0.017028617858886718
	model : 4.695064401626587
			 train-loss:  2.138750238418579 	 ± 0.257727579244313
	data : 0.014608192443847656
	model : 4.7019330024719235
			 train-loss:  2.1363622870775734 	 ± 0.2575579115412049
	data : 0.014690971374511719
	model : 4.70116286277771
			 train-loss:  2.133031999363619 	 ± 0.25846836660527683
	data : 0.014674615859985352
	model : 4.707729482650757
			 train-loss:  2.1309267814876964 	 ± 0.25808787982677805
	data : 0.012149763107299805
	model : 4.690466117858887
			 train-loss:  2.133214165384953 	 ± 0.2578910390904795
	data : 0.009580898284912109
	model : 4.689778852462768
			 train-loss:  2.130922077950977 	 ± 0.2577222568536002
	data : 0.012013864517211915
	model : 4.675929927825928
			 train-loss:  2.130655843131947 	 ± 0.256518212260397
	data : 0.010312795639038086
	model : 4.670675563812256
			 train-loss:  2.1291739472719 	 ± 0.25577216949145654
	data : 0.01041240692138672
	model : 4.663514566421509
			 train-loss:  2.1282060786529824 	 ± 0.2547820673248027
	data : 0.012964725494384766
	model : 4.664337968826294
			 train-loss:  2.1329198189831655 	 ± 0.2582983759526543
	data : 0.015364646911621094
	model : 4.667314863204956
			 train-loss:  2.1350961966948074 	 ± 0.2581236425767739
	data : 0.015306758880615234
	model : 4.624837350845337
			 train-loss:  2.1355137953887113 	 ± 0.25699561674552046
	data : 0.01694188117980957
	model : 4.604047250747681
			 train-loss:  2.135711740170206 	 ± 0.25585424199163515
	data : 0.01693105697631836
	model : 4.622317361831665
			 train-loss:  2.134321811979851 	 ± 0.25514400173808666
	data : 0.016879940032958986
	model : 4.6174602031707765
			 train-loss:  2.138444683008027 	 ± 0.2577754912497954
	data : 0.01705279350280762
	model : 4.620658254623413
			 train-loss:  2.136595520765885 	 ± 0.25741057696648284
	data : 0.0171539306640625
	model : 4.642844486236572
			 train-loss:  2.1367340128997276 	 ± 0.25630294999755293
	data : 0.017192935943603514
	model : 4.6528332233428955
			 train-loss:  2.1353242152776475 	 ± 0.25565659059343665
	data : 0.017149829864501955
	model : 4.642674207687378
			 train-loss:  2.132263855408814 	 ± 0.2567142193708226
	data : 0.017164325714111327
	model : 4.658744812011719
			 train-loss:  2.132301616067646 	 ± 0.2556336416827072
	data : 0.017094135284423828
	model : 4.6450213432312015
			 train-loss:  2.1331571131944655 	 ± 0.2547372776051998
	data : 0.01705775260925293
	model : 4.63852481842041
			 train-loss:  2.129472151275509 	 ± 0.25687402360044126
	data : 0.01713905334472656
	model : 4.6340779781341555
			 train-loss:  2.128855558692432 	 ± 0.25590899150627316
	data : 0.017131280899047852
	model : 4.640530443191528
			 train-loss:  2.128943497572488 	 ± 0.25486843889835054
	data : 0.017618894577026367
	model : 4.628006887435913
			 train-loss:  2.131377404735934 	 ± 0.2552698712060668
	data : 0.017446613311767577
	model : 4.636500072479248
			 train-loss:  2.1339404907226562 	 ± 0.25584372632632285
	data : 0.014867115020751952
	model : 4.647924757003784
			 train-loss:  2.1334098066602434 	 ± 0.25489551454701076
	data : 0.014752388000488281
	model : 4.645501708984375
			 train-loss:  2.1330472690852607 	 ± 0.2539226173140393
	data : 0.014678764343261718
	model : 4.638871049880981
			 train-loss:  2.130926233716309 	 ± 0.2540557377052934
	data : 0.014212322235107423
	model : 4.624433803558349
			 train-loss:  2.1352571526239084 	 ± 0.2577689900093662
	data : 0.014215660095214844
	model : 4.62992639541626
			 train-loss:  2.1338250123537503 	 ± 0.25729034238553483
	data : 0.017598867416381836
	model : 4.631829357147216
			 train-loss:  2.133660574905745 	 ± 0.2563132941408388
	data : 0.017680835723876954
	model : 4.629655456542968
			 train-loss:  2.134074836066275 	 ± 0.25538458287891525
	data : 0.01756601333618164
	model : 4.62197437286377
			 train-loss:  2.136069389214193 	 ± 0.25545259354279726
	data : 0.014989089965820313
	model : 4.633109855651855
			 train-loss:  2.135288286564955 	 ± 0.254657001488089
	data : 0.015129566192626953
	model : 4.641562747955322
			 train-loss:  2.1390812909161605 	 ± 0.25748331757617726
	data : 0.014277172088623048
	model : 4.635273265838623
			 train-loss:  2.1352359170422837 	 ± 0.26039663510797406
	data : 0.014178895950317382
	model : 4.611633062362671
			 train-loss:  2.1342613305488642 	 ± 0.25969336852390185
	data : 0.011836767196655273
	model : 4.6193829536437985
			 train-loss:  2.132959701012874 	 ± 0.2591988737031152
	data : 0.014376497268676758
	model : 4.632765531539917
			 train-loss:  2.135206812577282 	 ± 0.2596103812032585
	data : 0.014442825317382812
	model : 4.6248390674591064
			 train-loss:  2.1366212912968225 	 ± 0.2592185240555654
	data : 0.014534330368041993
	model : 4.633641576766967
			 train-loss:  2.141036975468304 	 ± 0.263528832645416
	data : 0.014607763290405274
	model : 4.660226631164551
			 train-loss:  2.142679644302583 	 ± 0.2633227069655612
	data : 0.017092084884643553
	model : 4.6546087741851805
			 train-loss:  2.1443855412356503 	 ± 0.2631866123720227
	data : 0.017010259628295898
	model : 4.637483930587768
			 train-loss:  2.1437714480691485 	 ± 0.26237396507347105
	data : 0.01698589324951172
	model : 4.659503602981568
			 train-loss:  2.1439611944659003 	 ± 0.26147757606921
	data : 0.016880083084106445
	model : 4.666113758087159
			 train-loss:  2.1468196029532445 	 ± 0.26284397254215136
	data : 0.016848278045654298
	model : 4.66427845954895
			 train-loss:  2.1475195819828787 	 ± 0.2620849296262902
	data : 0.017230510711669922
	model : 4.641171789169311
			 train-loss:  2.149419745883426 	 ± 0.2622120522395561
	data : 0.017304515838623045
	model : 4.65869836807251
			 train-loss:  2.1504832018141777 	 ± 0.2616507122310556
	data : 0.017316341400146484
	model : 4.642491626739502
			 train-loss:  2.1510675144195557 	 ± 0.26087460526373896
	data : 0.017304372787475587
	model : 4.645143127441406
			 train-loss:  2.1502895844693217 	 ± 0.26018385201998995
	data : 0.014755630493164062
	model : 4.63762903213501
			 train-loss:  2.147787323907802 	 ± 0.26114311845349747
	data : 0.014408588409423828
	model : 4.673599100112915
			 train-loss:  2.1476697041318307 	 ± 0.26029234992929506
	data : 0.0145355224609375
	model : 4.67730746269226
			 train-loss:  2.1477481552532742 	 ± 0.2594476832285494
	data : 0.014462804794311524
	model : 4.685831832885742
			 train-loss:  2.1465749979019164 	 ± 0.2590188645337251
	data : 0.015058088302612304
	model : 4.710742235183716
			 train-loss:  2.1464721140189047 	 ± 0.25819051845749674
	data : 0.015071678161621093
	model : 4.723817682266235
			 train-loss:  2.1473670636013056 	 ± 0.25760956645101035
	data : 0.015115165710449218
	model : 4.7381504535675045
			 train-loss:  2.146570680262167 	 ± 0.2569868581429916
	data : 0.015089130401611328
	model : 4.7194249629974365
			 train-loss:  2.1462034406901904 	 ± 0.25621903496782833
	data : 0.01498255729675293
	model : 4.703105878829956
			 train-loss:  2.1452785529196263 	 ± 0.25568321030683727
	data : 0.014423036575317382
	model : 4.676843452453613
			 train-loss:  2.145194637849464 	 ± 0.2548901363679633
	data : 0.016936779022216797
	model : 4.689578104019165
			 train-loss:  2.1460088940314304 	 ± 0.25431217752042984
	data : 0.01695704460144043
	model : 4.667583084106445
			 train-loss:  2.1449086797749337 	 ± 0.2539173148948668
	data : 0.014295721054077148
	model : 4.6750468730926515
			 train-loss:  2.1461481321148757 	 ± 0.25363610961921484
	data : 0.014401721954345702
	model : 4.680967187881469
			 train-loss:  2.145695547624068 	 ± 0.2529327621073076
	data : 0.011823940277099609
	model : 4.680704832077026
			 train-loss:  2.143536750810692 	 ± 0.25368988410252263
	data : 0.011814737319946289
	model : 4.655942916870117
			 train-loss:  2.1496645312109393 	 ± 0.26496495181538154
	data : 0.011721229553222657
	model : 4.648986911773681
			 train-loss:  2.14959308717932 	 ± 0.2641768019891312
	data : 0.0118438720703125
	model : 4.657124853134155
			 train-loss:  2.1504439835012312 	 ± 0.2636248544929689
	data : 0.009398651123046876
	model : 4.670720911026001
			 train-loss:  2.1555842041969298 	 ± 0.27120941649045976
	data : 0.011992311477661133
	model : 4.674062252044678
			 train-loss:  2.155028412216588 	 ± 0.27051232475293946
	data : 0.012031078338623047
	model : 4.683947229385376
			 train-loss:  2.1520330808883488 	 ± 0.27255400256912554
	data : 0.012088727951049805
	model : 4.713765478134155
			 train-loss:  2.152383856690688 	 ± 0.2718040669441938
	data : 0.012854862213134765
	model : 4.709653615951538
			 train-loss:  2.1529925335412736 	 ± 0.27114011524624404
	data : 0.012778711318969727
	model : 4.671430397033691
			 train-loss:  2.155700558253697 	 ± 0.27271390942558016
	data : 0.012751340866088867
	model : 4.6559923648834225
			 train-loss:  2.1574248143217782 	 ± 0.27289299836347575
	data : 0.012782430648803711
	model : 4.65903491973877
			 train-loss:  2.158807072935805 	 ± 0.2727381953624063
	data : 0.010211849212646484
	model : 4.641458559036255
			 train-loss:  2.1562438734461753 	 ± 0.2741005511821036
	data : 0.011863517761230468
	model : 4.646437072753907
			 train-loss:  2.1578793485737378 	 ± 0.27420338421999624
	data : 0.014385652542114259
	model : 4.6633647918701175
			 train-loss:  2.157629026307 	 ± 0.2734611562334004
	data : 0.011806392669677734
	model : 4.686383008956909
			 train-loss:  2.1572792753988868 	 ± 0.2727450602229871
	data : 0.00920553207397461
	model : 4.693732213973999
			 train-loss:  2.1593190051697113 	 ± 0.2733755336394175
	data : 0.011781072616577149
	model : 4.675514888763428
			 train-loss:  2.158664053255092 	 ± 0.2727707278433638
	data : 0.011827325820922852
	model : 4.675633716583252
			 train-loss:  2.1605546215306157 	 ± 0.27322808890684025
	data : 0.011842632293701172
	model : 4.673274278640747
			 train-loss:  2.162135513408764 	 ± 0.2733311397856092
	data : 0.014512300491333008
	model : 4.646571111679077
			 train-loss:  2.1616096996491954 	 ± 0.27268919047769796
	data : 0.01706099510192871
	model : 4.634304428100586
			 train-loss:  2.1639317624709187 	 ± 0.2737967462756032
	data : 0.017015552520751952
	model : 4.679785203933716
			 train-loss:  2.1640900500277254 	 ± 0.2730761713599038
	data : 0.016993331909179687
	model : 4.669147109985351
			 train-loss:  2.164437656049375 	 ± 0.2723944896282928
	data : 0.016982126235961913
	model : 4.671255826950073
			 train-loss:  2.162629205302188 	 ± 0.27281195279324166
	data : 0.01698765754699707
	model : 4.678541803359986
			 train-loss:  2.164195802199279 	 ± 0.2729523725514875
	data : 0.017028045654296876
	model : 4.675258827209473
			 train-loss:  2.16446353495121 	 ± 0.27226577506745414
	data : 0.016922950744628906
	model : 4.637918472290039
			 train-loss:  2.1637789721316008 	 ± 0.27172512256440373
	data : 0.014392948150634766
	model : 4.650433969497681
			 train-loss:  2.1627982393982483 	 ± 0.27136614801915593
	data : 0.014411830902099609
	model : 4.664325761795044
			 train-loss:  2.162277398353968 	 ± 0.2707666427258356
	data : 0.014378595352172851
	model : 4.6560780048370365
			 train-loss:  2.1632066557602005 	 ± 0.2703865882928832
	data : 0.011913442611694336
	model : 4.66290397644043
			 train-loss:  2.1620142199666366 	 ± 0.2702156341458313
	data : 0.012149333953857422
	model : 4.65983829498291
			 train-loss:  2.1606948694797476 	 ± 0.27016778837421973
	data : 0.014721918106079101
	model : 4.661649131774903
			 train-loss:  2.158208699681651 	 ± 0.271749319672711
	data : 0.014799118041992188
	model : 4.650997924804687
			 train-loss:  2.1573302119970323 	 ± 0.27135222619440913
	data : 0.014815759658813477
	model : 4.683043050765991
			 train-loss:  2.155937249980756 	 ± 0.27139228210409744
	data : 0.01735820770263672
	model : 4.676142024993896
			 train-loss:  2.155971382514085 	 ± 0.27072011807217855
	data : 0.0172365665435791
	model : 4.6990392208099365
			 train-loss:  2.1568267973773 	 ± 0.27032602774388576
	data : 0.01715846061706543
	model : 4.677459621429444
			 train-loss:  2.15629278736956 	 ± 0.2697699642967771
	data : 0.01710648536682129
	model : 4.671917343139649
			 train-loss:  2.155341313524944 	 ± 0.2694540986429219
	data : 0.017029428482055665
	model : 4.641868686676025
			 train-loss:  2.155942367118539 	 ± 0.2689370126048582
	data : 0.01687798500061035
	model : 4.647060680389404
			 train-loss:  2.153283022452092 	 ± 0.2709881286754871
	data : 0.017935752868652344
	model : 4.624076890945434
			 train-loss:  2.1531100467993665 	 ± 0.2703473851267105
	data : 0.018024682998657227
	model : 4.652287435531616
			 train-loss:  2.152856484554601 	 ± 0.2697246369957726
	data : 0.017969703674316405
	model : 4.663190603256226
			 train-loss:  2.1530037493932817 	 ± 0.26909009128825484
	data : 0.01798076629638672
	model : 4.6776533126831055
			 train-loss:  2.1516005207577025 	 ± 0.26922073653126216
	data : 0.01792149543762207
	model : 4.678062772750854
			 train-loss:  2.151494138083368 	 ± 0.2685894768053704
	data : 0.016944217681884765
	model : 4.692848539352417
			 train-loss:  2.1518188502307227 	 ± 0.2679999496538997
	data : 0.016910505294799805
	model : 4.68710880279541
			 train-loss:  2.1523391459589805 	 ± 0.2674808548302488
	data : 0.016941165924072264
	model : 4.683409643173218
			 train-loss:  2.15347661694815 	 ± 0.2673763590627212
	data : 0.016925287246704102
	model : 4.680812406539917
			 train-loss:  2.1548211966399795 	 ± 0.26748428355272424
	data : 0.017055273056030273
	model : 4.679442977905273
			 train-loss:  2.1532398896283267 	 ± 0.2678772914474337
	data : 0.01696290969848633
	model : 4.659585475921631
			 train-loss:  2.152647156234181 	 ± 0.267404779788759
	data : 0.01783332824707031
	model : 4.625086450576783
			 train-loss:  2.1525460214919696 	 ± 0.26679774683445867
	data : 0.018659639358520507
	model : 4.627908515930176
			 train-loss:  2.1516731029207055 	 ± 0.2665039636867121
	data : 0.016126108169555665
	model : 4.642514657974243
			 train-loss:  2.1524273308154145 	 ± 0.26613555679745854
	data : 0.016165542602539062
	model : 4.654132318496704
			 train-loss:  2.152282731490092 	 ± 0.2655441767800939
	data : 0.01604948043823242
	model : 4.656214570999145
			 train-loss:  2.151620145870431 	 ± 0.26513198075522504
	data : 0.0152374267578125
	model : 4.681662750244141
			 train-loss:  2.1528922942067896 	 ± 0.26522074766343073
	data : 0.011924839019775391
	model : 4.680801248550415
			 train-loss:  2.152993376519945 	 ± 0.26463503628601565
	data : 0.014568424224853516
	model : 4.672830438613891
			 train-loss:  2.152051824384031 	 ± 0.26442635030596323
	data : 0.014489316940307617
	model : 4.672915029525757
			 train-loss:  2.151066701317674 	 ± 0.26425857991052937
	data : 0.014713191986083984
	model : 4.697197151184082
			 train-loss:  2.1495339337148165 	 ± 0.26468778250410213
	data : 0.01423487663269043
	model : 4.70935320854187
			 train-loss:  2.148855132827592 	 ± 0.2643080413059856
	data : 0.016768169403076173
	model : 4.715049171447754
			 train-loss:  2.1483673463697017 	 ± 0.26383611258264156
	data : 0.016738367080688477
	model : 4.721200323104858
			 train-loss:  2.149042686342677 	 ± 0.2634635721285517
	data : 0.016748666763305664
	model : 4.721016263961792
			 train-loss:  2.1484299451112747 	 ± 0.2630600482984909
	data : 0.017488384246826173
	model : 4.72232084274292
			 train-loss:  2.1474687485224186 	 ± 0.26290290092623525
	data : 0.015366554260253906
	model : 4.7093321800231935
			 train-loss:  2.1484331580308766 	 ± 0.26275324829882607
	data : 0.012795591354370117
	model : 4.7181501388549805
			 train-loss:  2.147539803829599 	 ± 0.2625494934115482
	data : 0.012680244445800782
	model : 4.708327531814575
			 train-loss:  2.1478696723105544 	 ± 0.2620414506300097
	data : 0.012778472900390626
	model : 4.536407232284546
			 train-loss:  2.149018693573867 	 ± 0.2620831395414564
	data : 0.009499025344848634
	model : 4.307237386703491
			 train-loss:  2.1475160818140044 	 ± 0.26255299915632824
	data : 0.009510278701782227
	model : 4.085778760910034
			 train-loss:  2.1471926213308357 	 ± 0.2620506657401696
	data : 0.009464645385742187
	model : 3.84892053604126
			 train-loss:  2.1456300169229507 	 ± 0.26261758926869055
	data : 0.007013893127441407
	model : 3.621207427978516
			 train-loss:  2.144188846295305 	 ± 0.26302147705777473
	data : 0.00432744026184082
	model : 3.5619882583618163
			 train-loss:  2.1427096724510193 	 ± 0.2634800288716575
	data : 0.004225873947143554
	model : 3.561582088470459
			 train-loss:  2.141543931921814 	 ± 0.2635619578734333
	data : 0.004215049743652344
	model : 3.5675004005432127
			 train-loss:  2.139948990989904 	 ± 0.2641938032768599
	data : 0.0041997432708740234
	model : 3.5685864448547364
			 train-loss:  2.140702496256147 	 ± 0.2639166730746818
	data : 0.004122161865234375
	model : 3.5713680744171143
			 train-loss:  2.1395516361647506 	 ± 0.2639950177866809
	data : 0.004142141342163086
	model : 3.574358034133911
			 train-loss:  2.1396137138127314 	 ± 0.26346187204204374
	data : 0.004200458526611328
	model : 3.574110269546509
			 train-loss:  2.139281061868514 	 ± 0.26298213369542234
	data : 0.004174995422363281
	model : 3.5732810497283936
			 train-loss:  2.1387824021190047 	 ± 0.26257098335144685
	data : 0.0042572498321533205
	model : 3.574701738357544
			 train-loss:  2.1403723759651183 	 ± 0.2632436578608369
	data : 0.004356479644775391
	model : 3.572926378250122
			 train-loss:  2.1391024181092404 	 ± 0.263484984557917
	data : 0.0043433189392089845
	model : 3.5724266529083253
			 train-loss:  2.138474058064203 	 ± 0.26315004770898454
	data : 0.0042531490325927734
	model : 3.570471429824829
			 train-loss:  2.137688147220687 	 ± 0.2629256350788842
	data : 0.004284095764160156
	model : 3.5682650089263914
			 train-loss:  2.1382185319277247 	 ± 0.26254313087605075
	data : 0.004252195358276367
	model : 3.565217971801758
			 train-loss:  2.138303771205977 	 ± 0.2620313562770373
	data : 0.004196548461914062
	model : 3.5671343326568605
			 train-loss:  2.1387607692740858 	 ± 0.2616208762807327
	data : 0.0041335105895996095
	model : 3.026385021209717
#epoch  2    val-loss:  2.470809083235891  train-loss:  2.1387607692740858  lr:  0.01
			 train-loss:  2.1337080001831055 	 ± 0.0
	data : 4.165693283081055
	model : 4.705317974090576
			 train-loss:  2.074331521987915 	 ± 0.05937647819519043
	data : 2.09085750579834
	model : 4.663740277290344
			 train-loss:  2.0062597195307412 	 ± 0.10778644637173188
	data : 1.4023802280426025
	model : 4.654269854227702
			 train-loss:  1.952657401561737 	 ± 0.13165509481609666
	data : 1.0557432770729065
	model : 4.691189289093018
			 train-loss:  1.9155930280685425 	 ± 0.139145687402091
	data : 0.8477838039398193
	model : 4.687529706954956
			 train-loss:  1.8913257718086243 	 ± 0.1381271174224057
	data : 0.017836523056030274
	model : 4.703449821472168
			 train-loss:  1.9061468499047416 	 ± 0.13293419766058748
	data : 0.0190248966217041
	model : 4.72317214012146
			 train-loss:  1.939777746796608 	 ± 0.15290462345679928
	data : 0.014612436294555664
	model : 4.731232690811157
			 train-loss:  1.9541996717453003 	 ± 0.14981989527428372
	data : 0.014607763290405274
	model : 4.722969055175781
			 train-loss:  2.00473438501358 	 ± 0.20781052938665812
	data : 0.014601898193359376
	model : 4.730950117111206
			 train-loss:  2.0156188986518164 	 ± 0.20110697093047403
	data : 0.014058208465576172
	model : 4.7075306415557865
			 train-loss:  2.0196708738803864 	 ± 0.1930136855585039
	data : 0.01288909912109375
	model : 4.715784358978271
			 train-loss:  2.0058320669027476 	 ± 0.1915377761516515
	data : 0.015397071838378906
	model : 4.728700017929077
			 train-loss:  2.0296372515814647 	 ± 0.2035513899330629
	data : 0.015417957305908203
	model : 4.725298881530762
			 train-loss:  2.0302861531575522 	 ± 0.19666431335233647
	data : 0.01542649269104004
	model : 4.733043622970581
			 train-loss:  2.0508004128932953 	 ± 0.20633000672413693
	data : 0.01597728729248047
	model : 4.752528142929077
			 train-loss:  2.0324375419055714 	 ± 0.2132204320863498
	data : 0.015942955017089845
	model : 4.755335092544556
			 train-loss:  2.038540873262617 	 ± 0.2087354645934283
	data : 0.015926551818847657
	model : 4.752632665634155
			 train-loss:  2.053626116953398 	 ± 0.21301049895406324
	data : 0.015932750701904298
	model : 4.7578541278839115
			 train-loss:  2.04452987909317 	 ± 0.21136908088084463
	data : 0.015903329849243163
	model : 4.7508995056152346
			 train-loss:  2.0387260743549893 	 ± 0.20790166056880655
	data : 0.01610598564147949
	model : 4.702819108963013
			 train-loss:  2.058875609527935 	 ± 0.2231243895314732
	data : 0.0162961483001709
	model : 4.671599721908569
			 train-loss:  2.0630550850992617 	 ± 0.21909871376653042
	data : 0.016516494750976562
	model : 4.672900533676147
			 train-loss:  2.0444484750429788 	 ± 0.23230756770063016
	data : 0.016729116439819336
	model : 4.641516971588135
			 train-loss:  2.0724100494384765 	 ± 0.26565489868387954
	data : 0.014507865905761719
	model : 4.652965879440307
			 train-loss:  2.0999633348905125 	 ± 0.2946825169274061
	data : 0.01451568603515625
	model : 4.687371110916137
			 train-loss:  2.1023251568829573 	 ± 0.2894246098306911
	data : 0.014359855651855468
	model : 4.706977510452271
			 train-loss:  2.1047283411026 	 ± 0.2844835202045986
	data : 0.014331579208374023
	model : 4.6892914295196535
			 train-loss:  2.103934575771463 	 ± 0.27956715700411555
	data : 0.014318323135375977
	model : 4.702014970779419
			 train-loss:  2.107042590777079 	 ± 0.27537732004257465
	data : 0.01685619354248047
	model : 4.687267923355103
			 train-loss:  2.107474603960591 	 ± 0.27090967562284296
	data : 0.016820955276489257
	model : 4.679943799972534
			 train-loss:  2.1035189665853977 	 ± 0.26755113490438415
	data : 0.01703505516052246
	model : 4.666615009307861
			 train-loss:  2.101626508163683 	 ± 0.26368354946050837
	data : 0.017021560668945314
	model : 4.674331283569336
			 train-loss:  2.0976267351823696 	 ± 0.26079107215801667
	data : 0.017062997817993163
	model : 4.671205329895019
			 train-loss:  2.100499779837472 	 ± 0.25758383665137624
	data : 0.01695904731750488
	model : 4.648070621490478
			 train-loss:  2.120785574118296 	 ± 0.28090810568100383
	data : 0.016858816146850586
	model : 4.656925249099731
			 train-loss:  2.1086620962297595 	 ± 0.286474976408538
	data : 0.015125417709350586
	model : 4.65337724685669
			 train-loss:  2.1050501873618677 	 ± 0.28353293985086564
	data : 0.015754890441894532
	model : 4.649442434310913
			 train-loss:  2.1006555679516916 	 ± 0.2811823322431777
	data : 0.015743732452392578
	model : 4.645217847824097
			 train-loss:  2.09896519780159 	 ± 0.27784591604898523
	data : 0.015391302108764649
	model : 4.658007860183716
			 train-loss:  2.096808869664262 	 ± 0.274775283543496
	data : 0.015567827224731445
	model : 4.646919107437133
			 train-loss:  2.099674792516799 	 ± 0.27210394482101624
	data : 0.017302751541137695
	model : 4.651064872741699
			 train-loss:  2.095511045566825 	 ± 0.2702717678511158
	data : 0.01670351028442383
	model : 4.656038379669189
			 train-loss:  2.0897572176022963 	 ± 0.2698337531940682
	data : 0.016663408279418944
	model : 4.6523108959198
			 train-loss:  2.094136373202006 	 ± 0.26839529965335707
	data : 0.017151308059692384
	model : 4.665843772888183
			 train-loss:  2.0863728549169456 	 ± 0.27052224640807965
	data : 0.01709904670715332
	model : 4.666710472106933
			 train-loss:  2.080467995176924 	 ± 0.2706087856006693
	data : 0.017026090621948244
	model : 4.6654548168182375
			 train-loss:  2.0879025359948478 	 ± 0.2725826681066899
	data : 0.017064905166625975
	model : 4.66121735572815
			 train-loss:  2.098228425395732 	 ± 0.27911092647825547
	data : 0.01707911491394043
	model : 4.668548154830932
			 train-loss:  2.1016272640228273 	 ± 0.277328152538825
	data : 0.016964530944824217
	model : 4.660284471511841
			 train-loss:  2.109494279412662 	 ± 0.2801737742953099
	data : 0.014439249038696289
	model : 4.651074504852295
			 train-loss:  2.1160571529315066 	 ± 0.28139725700336815
	data : 0.014442253112792968
	model : 4.660741472244263
			 train-loss:  2.1146935921794965 	 ± 0.27890330609637004
	data : 0.014455890655517578
	model : 4.6444319725036625
			 train-loss:  2.1257915982493647 	 ± 0.2878790324097998
	data : 0.014563608169555663
	model : 4.626611232757568
			 train-loss:  2.137115990031849 	 ± 0.297140692311885
	data : 0.014550495147705077
	model : 4.629519510269165
			 train-loss:  2.13544163107872 	 ± 0.2947373897236237
	data : 0.01717214584350586
	model : 4.643781614303589
			 train-loss:  2.1319186750211214 	 ± 0.29332766633484203
	data : 0.017177677154541014
	model : 4.6382834911346436
			 train-loss:  2.141764753851397 	 ± 0.30013918862581845
	data : 0.017125654220581054
	model : 4.629982900619507
			 train-loss:  2.138293767379502 	 ± 0.2987565268665188
	data : 0.017816686630249025
	model : 4.636645221710205
			 train-loss:  2.136765420436859 	 ± 0.2964889308633697
	data : 0.017894172668457033
	model : 4.618823051452637
			 train-loss:  2.136241889390789 	 ± 0.2940766131268564
	data : 0.01521892547607422
	model : 4.6220598220825195
			 train-loss:  2.1369525924805672 	 ± 0.29174819571681276
	data : 0.015101146697998048
	model : 4.625484561920166
			 train-loss:  2.133075373513358 	 ± 0.2910291760750815
	data : 0.015047121047973632
	model : 4.63976936340332
			 train-loss:  2.139281403273344 	 ± 0.2929180911240594
	data : 0.014266586303710938
	model : 4.638966035842896
			 train-loss:  2.147467873646663 	 ± 0.29794322213720353
	data : 0.014178752899169922
	model : 4.654762172698975
			 train-loss:  2.145950826731595 	 ± 0.29593031979793755
	data : 0.016756439208984376
	model : 4.6523354053497314
			 train-loss:  2.149648474223578 	 ± 0.29524576151544996
	data : 0.016881418228149415
	model : 4.65780234336853
			 train-loss:  2.1520527531118954 	 ± 0.2937268192853256
	data : 0.01684226989746094
	model : 4.652093887329102
			 train-loss:  2.1514392140982808 	 ± 0.2916344850266664
	data : 0.016835498809814452
	model : 4.672787141799927
			 train-loss:  2.152602107184274 	 ± 0.2897049763936778
	data : 0.016910839080810546
	model : 4.651813220977783
			 train-loss:  2.153003346752113 	 ± 0.2876771535149407
	data : 0.014359378814697265
	model : 4.628722476959228
			 train-loss:  2.1503052711486816 	 ± 0.2865756068265275
	data : 0.01437516212463379
	model : 4.634852933883667
			 train-loss:  2.1511964667333316 	 ± 0.2847064365857725
	data : 0.014480781555175782
	model : 4.6589864730834964
			 train-loss:  2.1508423444387077 	 ± 0.2827923873712253
	data : 0.01447744369506836
	model : 4.641625308990479
			 train-loss:  2.147076989809672 	 ± 0.2827621111395227
	data : 0.014394664764404297
	model : 4.6562575340271
			 train-loss:  2.14465059732136 	 ± 0.2816805518737823
	data : 0.01695880889892578
	model : 4.651646280288697
			 train-loss:  2.1463109827660896 	 ± 0.28021958439474787
	data : 0.017002677917480467
	model : 4.634744644165039
			 train-loss:  2.1449427543542323 	 ± 0.2786762600648278
	data : 0.01447906494140625
	model : 4.641409540176392
			 train-loss:  2.1450790966613384 	 ± 0.27690948721930847
	data : 0.014463663101196289
	model : 4.639002418518066
			 train-loss:  2.143463471531868 	 ± 0.27554779554701336
	data : 0.014480686187744141
	model : 4.636196708679199
			 train-loss:  2.136406296565209 	 ± 0.2810222763985607
	data : 0.014479255676269532
	model : 4.678755426406861
			 train-loss:  2.1320595799422843 	 ± 0.2820298525056497
	data : 0.014457225799560547
	model : 4.661778974533081
			 train-loss:  2.1327694467751375 	 ± 0.28039942056643846
	data : 0.014408683776855469
	model : 4.6438063621521
			 train-loss:  2.1425013343493142 	 ± 0.2924871566161834
	data : 0.014407682418823241
	model : 4.667469167709351
			 train-loss:  2.139898524564855 	 ± 0.29173849426422105
	data : 0.014455318450927734
	model : 4.681421422958374
			 train-loss:  2.143910211186076 	 ± 0.2923861157746541
	data : 0.01442875862121582
	model : 4.689570808410645
			 train-loss:  2.139902504011132 	 ± 0.29306707217691214
	data : 0.014359664916992188
	model : 4.714291906356811
			 train-loss:  2.1417497261004015 	 ± 0.29190609662563927
	data : 0.016896915435791016
	model : 4.7338543891906735
			 train-loss:  2.1466541959998313 	 ± 0.293885180726571
	data : 0.016934633255004883
	model : 4.713250064849854
			 train-loss:  2.1462029271655614 	 ± 0.29227893138866934
	data : 0.01688246726989746
	model : 4.691944789886475
			 train-loss:  2.1502641295338725 	 ± 0.2932108773587003
	data : 0.016938018798828124
	model : 4.669902801513672
			 train-loss:  2.151413787966189 	 ± 0.2918191385724402
	data : 0.01696176528930664
	model : 4.667054128646851
			 train-loss:  2.1522425349040697 	 ± 0.2903548094830079
	data : 0.017044687271118165
	model : 4.647192001342773
			 train-loss:  2.1479097833024694 	 ± 0.2918131395921445
	data : 0.014523696899414063
	model : 4.683826637268067
			 train-loss:  2.145761531277707 	 ± 0.29101950061292126
	data : 0.015932226181030275
	model : 4.714847898483276
			 train-loss:  2.146296857545773 	 ± 0.28954682231174433
	data : 0.015913915634155274
	model : 4.70366415977478
			 train-loss:  2.1427060451704203 	 ± 0.2901911046604157
	data : 0.015964365005493163
	model : 4.694290828704834
			 train-loss:  2.140803828531382 	 ± 0.2893139645579502
	data : 0.015880107879638672
	model : 4.706357526779175
			 train-loss:  2.140451440907488 	 ± 0.287870211956655
	data : 0.018348217010498047
	model : 4.6687627792358395
			 train-loss:  2.1434925317764284 	 ± 0.2880210781805593
	data : 0.014448881149291992
	model : 4.626425838470459
			 train-loss:  2.1422965030859014 	 ± 0.28684114419612
	data : 0.014432048797607422
	model : 4.635693454742432
			 train-loss:  2.1389608768855823 	 ± 0.28739339434105365
	data : 0.011843061447143555
	model : 4.634195137023926
			 train-loss:  2.1412114969735008 	 ± 0.2868967234471494
	data : 0.011866283416748048
	model : 4.6247772693634035
			 train-loss:  2.142142803623126 	 ± 0.2856704839805579
	data : 0.01193699836730957
	model : 4.622124481201172
			 train-loss:  2.138771771249317 	 ± 0.28637781216621117
	data : 0.014156293869018555
	model : 4.654427099227905
			 train-loss:  2.139131778816007 	 ± 0.28504764406881317
	data : 0.014207935333251953
	model : 4.656353235244751
			 train-loss:  2.142138361930847 	 ± 0.2853961893024918
	data : 0.01683034896850586
	model : 4.649416589736939
			 train-loss:  2.142380631632275 	 ± 0.2840828917862636
	data : 0.016800403594970703
	model : 4.665062189102173
			 train-loss:  2.1416955180124404 	 ± 0.2828663769389758
	data : 0.016785907745361327
	model : 4.677092790603638
			 train-loss:  2.1391983368180014 	 ± 0.28278208463499793
	data : 0.017060518264770508
	model : 4.6554343700408936
			 train-loss:  2.142372392319344 	 ± 0.28346693762772157
	data : 0.014445018768310548
	model : 4.6357622146606445
			 train-loss:  2.1428222922342166 	 ± 0.282238427972071
	data : 0.01441330909729004
	model : 4.63926854133606
			 train-loss:  2.1466229120187 	 ± 0.28385100859848067
	data : 0.014480113983154297
	model : 4.620187187194825
			 train-loss:  2.146765887737274 	 ± 0.2826073927044101
	data : 0.014446163177490234
	model : 4.616441679000855
			 train-loss:  2.145521214733953 	 ± 0.28168964009256364
	data : 0.014466953277587891
	model : 4.615420770645142
			 train-loss:  2.144517194608162 	 ± 0.2806794190170262
	data : 0.014478111267089843
	model : 4.629734516143799
			 train-loss:  2.1416109708639293 	 ± 0.2812247264245467
	data : 0.014428329467773438
	model : 4.638850688934326
			 train-loss:  2.141048164690955 	 ± 0.2800967240841212
	data : 0.014327716827392579
	model : 4.646354484558105
			 train-loss:  2.1413158308558105 	 ± 0.278932519067267
	data : 0.014324712753295898
	model : 4.621864557266235
			 train-loss:  2.141497164964676 	 ± 0.27777491230603735
	data : 0.01428065299987793
	model : 4.639367628097534
			 train-loss:  2.14156157320196 	 ± 0.2766256004817504
	data : 0.016885662078857423
	model : 4.640546464920044
			 train-loss:  2.1381950720411833 	 ± 0.2779673156027642
	data : 0.01695218086242676
	model : 4.651365518569946
			 train-loss:  2.141810875598008 	 ± 0.2797010607589519
	data : 0.017053794860839844
	model : 4.634751415252685
			 train-loss:  2.1400391507533287 	 ± 0.2792630887719957
	data : 0.014540672302246094
	model : 4.643158769607544
			 train-loss:  2.137428810119629 	 ± 0.27965852511275413
	data : 0.014584875106811524
	model : 4.646225452423096
			 train-loss:  2.135370282899766 	 ± 0.2794957548445265
	data : 0.014500808715820313
	model : 4.647954368591309
			 train-loss:  2.135896877979669 	 ± 0.27845594926711376
	data : 0.014544296264648437
	model : 4.625056457519531
			 train-loss:  2.1332300677895546 	 ± 0.2789895349389011
	data : 0.014556217193603515
	model : 4.613079309463501
			 train-loss:  2.1308139886042867 	 ± 0.2792471654502007
	data : 0.017069149017333984
	model : 4.632921266555786
			 train-loss:  2.129373483474438 	 ± 0.27865179562952636
	data : 0.017085695266723634
	model : 4.622049283981323
			 train-loss:  2.1293642056807305 	 ± 0.27758622184170006
	data : 0.01711244583129883
	model : 4.615360927581787
			 train-loss:  2.1288160835251664 	 ± 0.2766039129497649
	data : 0.01711244583129883
	model : 4.636005401611328
			 train-loss:  2.1269453288917255 	 ± 0.27639903580523917
	data : 0.01709432601928711
	model : 4.669377946853638
			 train-loss:  2.129510290587126 	 ± 0.2769500229191327
	data : 0.017060518264770508
	model : 4.664811849594116
			 train-loss:  2.1287364571182814 	 ± 0.27606774340440615
	data : 0.014489507675170899
	model : 4.655862855911255
			 train-loss:  2.1323460848892437 	 ± 0.27823007927996585
	data : 0.014451742172241211
	model : 4.640099096298218
			 train-loss:  2.1309056290744866 	 ± 0.2777212887965044
	data : 0.01441020965576172
	model : 4.626916551589966
			 train-loss:  2.133357657038647 	 ± 0.2781976133282927
	data : 0.014408016204833984
	model : 4.618875360488891
			 train-loss:  2.129727325851111 	 ± 0.2804565302370541
	data : 0.014998245239257812
	model : 4.616403961181641
			 train-loss:  2.127366874047688 	 ± 0.28083537631592903
	data : 0.017532539367675782
	model : 4.622841262817383
			 train-loss:  2.1294730614263115 	 ± 0.28094519217117114
	data : 0.01751265525817871
	model : 4.658235692977906
			 train-loss:  2.1251125679889196 	 ± 0.2847021511046674
	data : 0.014910125732421875
	model : 4.659555196762085
			 train-loss:  2.125404818908318 	 ± 0.2837263166597122
	data : 0.014870166778564453
	model : 4.645035934448242
			 train-loss:  2.125842800570859 	 ± 0.2827879453896538
	data : 0.01430826187133789
	model : 4.655438470840454
			 train-loss:  2.1289074642904873 	 ± 0.2842006036862462
	data : 0.014311504364013673
	model : 4.674534320831299
			 train-loss:  2.128622189776538 	 ± 0.2832464728775256
	data : 0.014459991455078125
	model : 4.675049495697022
			 train-loss:  2.129682788232557 	 ± 0.2825721549215941
	data : 0.016657400131225585
	model : 4.694549083709717
			 train-loss:  2.128966122865677 	 ± 0.28174991816523487
	data : 0.01666712760925293
	model : 4.710458898544312
			 train-loss:  2.129521716361078 	 ± 0.28088419271225923
	data : 0.016639041900634765
	model : 4.736578512191772
			 train-loss:  2.1282697931925454 	 ± 0.280363132886999
	data : 0.016638994216918945
	model : 4.723928022384643
			 train-loss:  2.128368508736819 	 ± 0.2794358515057241
	data : 0.01651029586791992
	model : 4.700967168807983
			 train-loss:  2.1288551860734035 	 ± 0.27857933710228516
	data : 0.016926383972167967
	model : 4.699425888061524
			 train-loss:  2.1268460914200427 	 ± 0.27877007999746395
	data : 0.0169039249420166
	model : 4.703932762145996
			 train-loss:  2.1259358424644965 	 ± 0.2780915278104862
	data : 0.014389419555664062
	model : 4.670275688171387
			 train-loss:  2.1292833620502103 	 ± 0.28028854716383594
	data : 0.01187591552734375
	model : 4.682672119140625
			 train-loss:  2.1322272205964112 	 ± 0.2817824448245665
	data : 0.011896371841430664
	model : 4.694804573059082
			 train-loss:  2.1328221840463626 	 ± 0.2809818966903321
	data : 0.009302473068237305
	model : 4.686090564727783
			 train-loss:  2.132316429403764 	 ± 0.28016298144461627
	data : 0.0093353271484375
	model : 4.70452847480774
			 train-loss:  2.1318325456583276 	 ± 0.2793468003948979
	data : 0.011863946914672852
	model : 4.717554235458374
			 train-loss:  2.133075323700905 	 ± 0.2789130572196896
	data : 0.01447763442993164
	model : 4.719641923904419
			 train-loss:  2.1329330776048745 	 ± 0.2780513400984851
	data : 0.014476966857910157
	model : 4.724474620819092
			 train-loss:  2.133081121209227 	 ± 0.2771981933633848
	data : 0.01707921028137207
	model : 4.732454061508179
			 train-loss:  2.1310502714906003 	 ± 0.27755284052093343
	data : 0.014591741561889648
	model : 4.71553111076355
			 train-loss:  2.1296745727701887 	 ± 0.27726221396952516
	data : 0.014581680297851562
	model : 4.709905815124512
			 train-loss:  2.1276785937222567 	 ± 0.27760006329230447
	data : 0.014516782760620118
	model : 4.692796134948731
			 train-loss:  2.1286471004945686 	 ± 0.27704212377190607
	data : 0.012044239044189452
	model : 4.69234824180603
			 train-loss:  2.12596672666287 	 ± 0.2783619128892373
	data : 0.010273075103759766
	model : 4.712146520614624
			 train-loss:  2.123620819477808 	 ± 0.27918305830736534
	data : 0.012741947174072265
	model : 4.701348114013672
			 train-loss:  2.1259941970102885 	 ± 0.28005054634873444
	data : 0.012750005722045899
	model : 4.704941892623902
			 train-loss:  2.1275906170115753 	 ± 0.2799958421506538
	data : 0.012715959548950195
	model : 4.701736116409302
			 train-loss:  2.1284834014044867 	 ± 0.2794185151005857
	data : 0.015220975875854493
	model : 4.701244640350342
			 train-loss:  2.129985581996829 	 ± 0.27929671198442274
	data : 0.016895008087158204
	model : 4.664644765853882
			 train-loss:  2.1303551844778776 	 ± 0.27853050828540526
	data : 0.016895580291748046
	model : 4.654332208633423
			 train-loss:  2.1298450971471854 	 ± 0.2778100052788216
	data : 0.017078542709350587
	model : 4.652792596817017
			 train-loss:  2.1316107041495185 	 ± 0.27799244892093194
	data : 0.01707324981689453
	model : 4.657227468490601
			 train-loss:  2.1304549317468298 	 ± 0.2776229071926661
	data : 0.017065668106079103
	model : 4.642842864990234
			 train-loss:  2.1324753505361955 	 ± 0.27813212287209543
	data : 0.01697707176208496
	model : 4.643234205245972
			 train-loss:  2.1339056063234136 	 ± 0.2780017302064219
	data : 0.017053699493408202
	model : 4.683106279373169
			 train-loss:  2.1325143915314912 	 ± 0.27784477253287254
	data : 0.01688075065612793
	model : 4.680550956726075
			 train-loss:  2.132018835014767 	 ± 0.2771512214759107
	data : 0.01705594062805176
	model : 4.682298040390014
			 train-loss:  2.1303626042044623 	 ± 0.2772763559104032
	data : 0.017028236389160158
	model : 4.693128967285157
			 train-loss:  2.1322070687681762 	 ± 0.277624780890218
	data : 0.014592170715332031
	model : 4.669235277175903
			 train-loss:  2.132141829839821 	 ± 0.27686660303929317
	data : 0.014548778533935547
	model : 4.635311460494995
			 train-loss:  2.132478045380634 	 ± 0.2761506807355763
	data : 0.014616918563842774
	model : 4.631780385971069
			 train-loss:  2.1332689156403415 	 ± 0.2756121806603467
	data : 0.014430046081542969
	model : 4.636760568618774
			 train-loss:  2.132801095644633 	 ± 0.2749439291142847
	data : 0.014434099197387695
	model : 4.632175493240356
			 train-loss:  2.135151648903913 	 ± 0.27607532668793244
	data : 0.01698193550109863
	model : 4.650041723251343
			 train-loss:  2.1349530841441866 	 ± 0.2753534934290098
	data : 0.016941213607788087
	model : 4.664927530288696
			 train-loss:  2.134440573434981 	 ± 0.2747139715438219
	data : 0.0168759822845459
	model : 4.68613224029541
			 train-loss:  2.1332594507618956 	 ± 0.27447082174145054
	data : 0.0169708251953125
	model : 4.685417747497558
			 train-loss:  2.132179464345203 	 ± 0.27415583588556225
	data : 0.017001914978027343
	model : 4.6816764831542965
			 train-loss:  2.1316000564644733 	 ± 0.273558180074595
	data : 0.017018747329711915
	model : 4.695971202850342
			 train-loss:  2.1353917016884205 	 ± 0.27786082606648804
	data : 0.01707592010498047
	model : 4.689248704910279
			 train-loss:  2.13512789158477 	 ± 0.2771679964964828
	data : 0.015409755706787109
	model : 4.668728256225586
			 train-loss:  2.134569198045975 	 ± 0.2765658938964496
	data : 0.015019083023071289
	model : 4.6763472080230715
			 train-loss:  2.1324354063491433 	 ± 0.2774640396442995
	data : 0.014920902252197266
	model : 4.703128385543823
			 train-loss:  2.132021986893591 	 ± 0.27681943464180925
	data : 0.015009021759033203
	model : 4.701737117767334
			 train-loss:  2.1348179765421933 	 ± 0.278894322630674
	data : 0.014992284774780273
	model : 4.711522388458252
			 train-loss:  2.1355410766361946 	 ± 0.2783787126535504
	data : 0.016646862030029297
	model : 4.719153833389282
			 train-loss:  2.13528846681118 	 ± 0.27770475806757405
	data : 0.01693577766418457
	model : 4.70518798828125
			 train-loss:  2.1367809659806056 	 ± 0.2778160581221595
	data : 0.017100048065185548
	model : 4.700966024398804
			 train-loss:  2.1354517694747095 	 ± 0.27776751727425847
	data : 0.016997051239013673
	model : 4.713881158828736
			 train-loss:  2.1332535737841 	 ± 0.2788383003911051
	data : 0.016982603073120116
	model : 4.710170936584473
			 train-loss:  2.131521574422425 	 ± 0.2792465402230476
	data : 0.017076921463012696
	model : 4.725075960159302
			 train-loss:  2.1315514843638352 	 ± 0.27856494604274756
	data : 0.01711421012878418
	model : 4.728180170059204
			 train-loss:  2.131067563029169 	 ± 0.2779743596647195
	data : 0.017057228088378906
	model : 4.699537229537964
			 train-loss:  2.1294548839762593 	 ± 0.2782664406435455
	data : 0.017062950134277343
	model : 4.700815963745117
			 train-loss:  2.1308039558621554 	 ± 0.2782744700719414
	data : 0.017008876800537108
	model : 4.709510564804077
			 train-loss:  2.130489333964991 	 ± 0.2776450243440687
	data : 0.017027902603149413
	model : 4.702560567855835
			 train-loss:  2.130143540813809 	 ± 0.2770282846073344
	data : 0.016921472549438477
	model : 4.729360771179199
			 train-loss:  2.1295671186175955 	 ± 0.27649724553821076
	data : 0.014357709884643554
	model : 4.7439642429351805
			 train-loss:  2.1281449952215516 	 ± 0.2766167817208838
	data : 0.014444637298583984
	model : 4.7518545627594
			 train-loss:  2.129282513694584 	 ± 0.27646324702422126
	data : 0.014493942260742188
	model : 4.751388645172119
			 train-loss:  2.131133364739819 	 ± 0.2771361250018171
	data : 0.014443159103393555
	model : 4.743193101882935
			 train-loss:  2.130309992612794 	 ± 0.27675310545847237
	data : 0.014639139175415039
	model : 4.731620216369629
			 train-loss:  2.1308174712790384 	 ± 0.2762119791462986
	data : 0.017209959030151368
	model : 4.715497398376465
			 train-loss:  2.129881969245348 	 ± 0.27591758180662573
	data : 0.017170047760009764
	model : 4.702865028381348
			 train-loss:  2.1304954306794963 	 ± 0.27543230402079344
	data : 0.014653730392456054
	model : 4.692492914199829
			 train-loss:  2.1293320644936062 	 ± 0.2753390522019806
	data : 0.01461338996887207
	model : 4.684498643875122
			 train-loss:  2.128062851862474 	 ± 0.2753539227401955
	data : 0.01452932357788086
	model : 4.668715238571167
			 train-loss:  2.126982037837689 	 ± 0.27519756866411976
	data : 0.011950016021728516
	model : 4.695081758499145
			 train-loss:  2.127081809280155 	 ± 0.2745810607216926
	data : 0.011783456802368164
	model : 4.700045585632324
			 train-loss:  2.125899990043298 	 ± 0.274530021103505
	data : 0.014313554763793946
	model : 4.699817657470703
			 train-loss:  2.1258114677454745 	 ± 0.2739197351997381
	data : 0.01434931755065918
	model : 4.712167549133301
			 train-loss:  2.1244199032253688 	 ± 0.2741027378334276
	data : 0.014236116409301757
	model : 4.712562894821167
			 train-loss:  2.1246005708137443 	 ± 0.27350906970195726
	data : 0.016794824600219728
	model : 4.7171323776245115
			 train-loss:  2.1224404878028165 	 ± 0.2748311675739726
	data : 0.016907930374145508
	model : 4.710917663574219
			 train-loss:  2.122796982526779 	 ± 0.2742804008794469
	data : 0.01693382263183594
	model : 4.720691394805908
			 train-loss:  2.123634278513979 	 ± 0.2739727484395384
	data : 0.016814327239990233
	model : 4.731170272827148
			 train-loss:  2.124224160546842 	 ± 0.2735222065004837
	data : 0.01695570945739746
	model : 4.726428365707397
			 train-loss:  2.1254979310097633 	 ± 0.2736123143659175
	data : 0.01690526008605957
	model : 4.709989452362061
			 train-loss:  2.126017483657804 	 ± 0.2731361660255757
	data : 0.01745758056640625
	model : 4.705890893936157
			 train-loss:  2.1262653626085863 	 ± 0.27257555663792477
	data : 0.01739048957824707
	model : 4.7143782615661625
			 train-loss:  2.1292951224196672 	 ± 0.27589623818734843
	data : 0.017433691024780273
	model : 4.707627582550049
			 train-loss:  2.129155385747869 	 ± 0.27531689723201297
	data : 0.014868831634521485
	model : 4.736363267898559
			 train-loss:  2.1292578461816754 	 ± 0.2747374693994532
	data : 0.01481614112854004
	model : 4.564729309082031
			 train-loss:  2.1289435454058747 	 ± 0.2741997565227927
	data : 0.011694526672363282
	model : 4.336373281478882
			 train-loss:  2.127493285832285 	 ± 0.27453246057371067
	data : 0.009169673919677735
	model : 4.09282693862915
			 train-loss:  2.127995100480243 	 ± 0.27406688416768593
	data : 0.00671076774597168
	model : 3.854396390914917
			 train-loss:  2.1283383811513583 	 ± 0.2735467999085486
	data : 0.006684541702270508
	model : 3.60992636680603
			 train-loss:  2.1281726345481715 	 ± 0.27299076173671094
	data : 0.004227256774902344
	model : 3.5574379444122313
			 train-loss:  2.1281528665014533 	 ± 0.27242632020263374
	data : 0.004243993759155273
	model : 3.559024477005005
			 train-loss:  2.128101729071189 	 ± 0.27186635821468413
	data : 0.004241466522216797
	model : 3.556964635848999
			 train-loss:  2.1270887973855754 	 ± 0.27176778126214957
	data : 0.004179048538208008
	model : 3.5605538368225096
			 train-loss:  2.126879450253078 	 ± 0.27123229973652535
	data : 0.004141950607299804
	model : 3.5602701187133787
			 train-loss:  2.126459380960077 	 ± 0.2707602998916287
	data : 0.0042935371398925785
	model : 3.5578834056854247
			 train-loss:  2.127159221452257 	 ± 0.27043450008156006
	data : 0.004292440414428711
	model : 3.5562655448913576
			 train-loss:  2.1263391024643377 	 ± 0.2701963200608868
	data : 0.004358959197998047
	model : 3.5580861568450928
			 train-loss:  2.12714615739493 	 ± 0.2699525627233421
	data : 0.004363918304443359
	model : 3.5583895683288573
			 train-loss:  2.1270058693885803 	 ± 0.26942121126012525
	data : 0.004537010192871093
	model : 3.5573253631591797
			 train-loss:  2.126831959443263 	 ± 0.26889803990611527
	data : 0.004477834701538086
	model : 3.5602187633514406
			 train-loss:  2.126985477549689 	 ± 0.2683750029258836
	data : 0.0044571876525878905
	model : 3.560235548019409
			 train-loss:  2.1274314289507656 	 ± 0.2679376303308272
	data : 0.004439449310302735
	model : 3.5596508502960207
			 train-loss:  2.1282083429689482 	 ± 0.26769505659847886
	data : 0.0044537544250488285
	model : 3.565256357192993
			 train-loss:  2.129398845691307 	 ± 0.26784251822009386
	data : 0.004397344589233398
	model : 3.564102840423584
			 train-loss:  2.1271350514143705 	 ± 0.2697521022194172
	data : 0.004300260543823242
	model : 3.024686002731323
#epoch  3    val-loss:  2.5105769508763363  train-loss:  2.1271350514143705  lr:  0.01
			 train-loss:  2.1125779151916504 	 ± 0.0
	data : 4.46002459526062
	model : 4.730506658554077
			 train-loss:  2.082029104232788 	 ± 0.030548810958862305
	data : 2.2380460500717163
	model : 4.700908184051514
			 train-loss:  2.143374760945638 	 ± 0.09027032972722468
	data : 1.4973239103953044
	model : 4.701611280441284
			 train-loss:  2.178097128868103 	 ± 0.09863304630916293
	data : 1.1269831657409668
	model : 4.699517726898193
			 train-loss:  2.119439458847046 	 ± 0.14678443810083303
	data : 0.9047714233398437
	model : 4.706161403656006
			 train-loss:  2.076354503631592 	 ± 0.16503421732094276
	data : 0.01600370407104492
	model : 4.697204351425171
			 train-loss:  2.0707692418779646 	 ± 0.15340327504996723
	data : 0.015982866287231445
	model : 4.699434804916382
			 train-loss:  2.0634025633335114 	 ± 0.14481322407152622
	data : 0.015724563598632814
	model : 4.711804056167603
			 train-loss:  2.045638680458069 	 ± 0.14548270513264763
	data : 0.01570725440979004
	model : 4.7330708503723145
			 train-loss:  2.024389052391052 	 ± 0.15202833939868077
	data : 0.015701723098754884
	model : 4.727552461624145
			 train-loss:  1.989377964626659 	 ± 0.18239855190620083
	data : 0.01566171646118164
	model : 4.754082775115966
			 train-loss:  1.9921565353870392 	 ± 0.17487630133763302
	data : 0.01568117141723633
	model : 4.769006967544556
			 train-loss:  2.002296860401447 	 ± 0.1716484634568858
	data : 0.015964794158935546
	model : 4.743444919586182
			 train-loss:  2.0159966009003774 	 ± 0.1726225901920866
	data : 0.013442516326904297
	model : 4.712178087234497
			 train-loss:  2.0290054082870483 	 ± 0.17372735836086883
	data : 0.013436460494995117
	model : 4.723616409301758
			 train-loss:  2.0416488274931908 	 ± 0.17519335344467024
	data : 0.011763334274291992
	model : 4.713887166976929
			 train-loss:  2.0531063009710873 	 ± 0.17603305562745034
	data : 0.011774492263793946
	model : 4.696649551391602
			 train-loss:  2.0387661457061768 	 ± 0.18100271325080894
	data : 0.011782407760620117
	model : 4.687551259994507
			 train-loss:  2.0191674483449837 	 ± 0.19481177644050648
	data : 0.014302682876586915
	model : 4.6962571144104
			 train-loss:  2.0139594256877897 	 ± 0.1912312561478107
	data : 0.014319753646850586
	model : 4.680696439743042
			 train-loss:  2.031114038966951 	 ± 0.20177613531025274
	data : 0.01620931625366211
	model : 4.663444232940674
			 train-loss:  2.0298887437040154 	 ± 0.19721693263487147
	data : 0.013838100433349609
	model : 4.672093343734741
			 train-loss:  2.023156331933063 	 ± 0.19544976489361723
	data : 0.014008045196533203
	model : 4.684356594085694
			 train-loss:  2.0097235242525735 	 ± 0.20188869680823263
	data : 0.014276695251464844
	model : 4.6711328506469725
			 train-loss:  2.009108328819275 	 ± 0.19783267483678488
	data : 0.014478635787963868
	model : 4.674333000183106
			 train-loss:  2.0369809728402357 	 ± 0.23886099642118946
	data : 0.015743255615234375
	model : 4.6821863651275635
			 train-loss:  2.0275090049814293 	 ± 0.23932011142891693
	data : 0.018238306045532227
	model : 4.691425752639771
			 train-loss:  2.0295433700084686 	 ± 0.23524530794404183
	data : 0.015726900100708006
	model : 4.6959411144256595
			 train-loss:  2.0414564157354422 	 ± 0.23959516949879417
	data : 0.015695953369140626
	model : 4.69891128540039
			 train-loss:  2.0487635254859926 	 ± 0.23883203052886726
	data : 0.015760231018066406
	model : 4.68807806968689
			 train-loss:  2.05546522525049 	 ± 0.23779844063003608
	data : 0.014446783065795898
	model : 4.683224678039551
			 train-loss:  2.065694596618414 	 ± 0.24088339811661302
	data : 0.014464616775512695
	model : 4.671665096282959
			 train-loss:  2.0791718128955727 	 ± 0.2491562067096741
	data : 0.017038536071777344
	model : 4.670901823043823
			 train-loss:  2.0707437325926388 	 ± 0.2501940078668062
	data : 0.016995906829833984
	model : 4.682356548309326
			 train-loss:  2.0702133893966677 	 ± 0.24661329575506014
	data : 0.016852951049804686
	model : 4.6857343196868895
			 train-loss:  2.0860095719496408 	 ± 0.2605031744529386
	data : 0.016735076904296875
	model : 4.689280319213867
			 train-loss:  2.0803671269803434 	 ± 0.25917935306961004
	data : 0.01685500144958496
	model : 4.691501188278198
			 train-loss:  2.081677505844518 	 ± 0.2558705420798958
	data : 0.016822385787963866
	model : 4.674134922027588
			 train-loss:  2.0837433704963098 	 ± 0.25288969817909734
	data : 0.016887950897216796
	model : 4.687298583984375
			 train-loss:  2.0856579720973967 	 ± 0.24999466340570475
	data : 0.016977596282958984
	model : 4.682245349884033
			 train-loss:  2.0899590515508883 	 ± 0.248420969215086
	data : 0.017046356201171876
	model : 4.6807904720306395
			 train-loss:  2.0922299396424067 	 ± 0.24587609800405286
	data : 0.016974353790283205
	model : 4.673915719985962
			 train-loss:  2.09026138727055 	 ± 0.2433349181258888
	data : 0.01782221794128418
	model : 4.693378019332886
			 train-loss:  2.097242832183838 	 ± 0.24487139976347838
	data : 0.015229415893554688
	model : 4.68647313117981
			 train-loss:  2.10217400126987 	 ± 0.2443346809927588
	data : 0.015256118774414063
	model : 4.704374599456787
			 train-loss:  2.1006511035172837 	 ± 0.24188010933696688
	data : 0.015603399276733399
	model : 4.679576444625854
			 train-loss:  2.1015726251805082 	 ± 0.23937469029259506
	data : 0.015565872192382812
	model : 4.676075506210327
			 train-loss:  2.1036742826302848 	 ± 0.23730588828211396
	data : 0.015566015243530273
	model : 4.675580453872681
			 train-loss:  2.1106519358498708 	 ± 0.23979538051049262
	data : 0.018705129623413086
	model : 4.665355253219604
			 train-loss:  2.098296785354614 	 ± 0.25264921426386855
	data : 0.018683195114135742
	model : 4.65979118347168
			 train-loss:  2.099801255207436 	 ± 0.2503860947530375
	data : 0.01860074996948242
	model : 4.6581682682037355
			 train-loss:  2.095623504657012 	 ± 0.24975526347853205
	data : 0.018655920028686525
	model : 4.672270202636719
			 train-loss:  2.09353166481234 	 ± 0.24784732264413725
	data : 0.017681026458740236
	model : 4.68446364402771
			 train-loss:  2.092372786115717 	 ± 0.2456866160542582
	data : 0.01454305648803711
	model : 4.675708341598511
			 train-loss:  2.098291433941234 	 ± 0.24729752353748158
	data : 0.0145233154296875
	model : 4.661085557937622
			 train-loss:  2.0966332278081348 	 ± 0.24538790347679817
	data : 0.014440584182739257
	model : 4.670873022079467
			 train-loss:  2.102429178723118 	 ± 0.24706279845649196
	data : 0.014514875411987305
	model : 4.648031520843506
			 train-loss:  2.1045021168116866 	 ± 0.2454231962455617
	data : 0.014639902114868163
	model : 4.632909488677979
			 train-loss:  2.106532163539175 	 ± 0.24382509686526166
	data : 0.017183494567871094
	model : 4.64883074760437
			 train-loss:  2.107296190659205 	 ± 0.24185589457216042
	data : 0.014621973037719727
	model : 4.663513088226319
			 train-loss:  2.107915176719916 	 ± 0.23991319230835312
	data : 0.014501047134399415
	model : 4.661675596237183
			 train-loss:  2.109551566262399 	 ± 0.23831349843923375
	data : 0.014440393447875977
	model : 4.675950717926026
			 train-loss:  2.1153249381080506 	 ± 0.24074554036444223
	data : 0.014441108703613282
	model : 4.685075378417968
			 train-loss:  2.1144993621855974 	 ± 0.23894717874630514
	data : 0.014444303512573243
	model : 4.682442045211792
			 train-loss:  2.1148102815334613 	 ± 0.23711504580203063
	data : 0.014463233947753906
	model : 4.685350656509399
			 train-loss:  2.1158906094955676 	 ± 0.23547300292274165
	data : 0.014454126358032227
	model : 4.707751893997193
			 train-loss:  2.1162619999985197 	 ± 0.23372861056019845
	data : 0.014334392547607423
	model : 4.6867717742919925
			 train-loss:  2.1120229135541355 	 ± 0.2345840468259296
	data : 0.014377355575561523
	model : 4.685716152191162
			 train-loss:  2.121092725491178 	 ± 0.24459338651343596
	data : 0.011842060089111327
	model : 4.692754316329956
			 train-loss:  2.1234721541404724 	 ± 0.2436430270430366
	data : 0.011855745315551757
	model : 4.693313646316528
			 train-loss:  2.1239761315600973 	 ± 0.2419578903611368
	data : 0.011836099624633788
	model : 4.68146276473999
			 train-loss:  2.1262967702415256 	 ± 0.2410661230887008
	data : 0.012805604934692382
	model : 4.682999563217163
			 train-loss:  2.1262082253416925 	 ± 0.23941047054157924
	data : 0.012836217880249023
	model : 4.662896537780762
			 train-loss:  2.126036719695942 	 ± 0.23779184498314884
	data : 0.015397882461547852
	model : 4.650288772583008
			 train-loss:  2.12319220383962 	 ± 0.2374653295404577
	data : 0.017879724502563477
	model : 4.63060941696167
			 train-loss:  2.1198926520975014 	 ± 0.2376222633419118
	data : 0.017891407012939453
	model : 4.652772712707519
			 train-loss:  2.1202829147314097 	 ± 0.23609873378009885
	data : 0.01703181266784668
	model : 4.6705059051513675
			 train-loss:  2.1196520190972548 	 ± 0.23464571480377908
	data : 0.016909122467041016
	model : 4.670541191101075
			 train-loss:  2.1178812075264846 	 ± 0.23367981806097737
	data : 0.016903400421142578
	model : 4.676407146453857
			 train-loss:  2.1179184168577194 	 ± 0.23221496189976867
	data : 0.01717534065246582
	model : 4.680871677398682
			 train-loss:  2.1247984009024536 	 ± 0.23884052667470942
	data : 0.01722555160522461
	model : 4.6446184635162355
			 train-loss:  2.121611080518583 	 ± 0.2391066892278778
	data : 0.018166255950927735
	model : 4.642626571655273
			 train-loss:  2.1268249046371643 	 ± 0.24230615825649104
	data : 0.018213891983032228
	model : 4.638255548477173
			 train-loss:  2.1232628538495018 	 ± 0.24303588164054765
	data : 0.01570858955383301
	model : 4.639247894287109
			 train-loss:  2.119565547213835 	 ± 0.243966856125032
	data : 0.015527057647705077
	model : 4.665197515487671
			 train-loss:  2.119040396324424 	 ± 0.24259261627474793
	data : 0.015551280975341798
	model : 4.675496435165405
			 train-loss:  2.113529930169555 	 ± 0.24654844870379605
	data : 0.014617061614990235
	model : 4.698098134994507
			 train-loss:  2.1173129962249235 	 ± 0.24767013186673115
	data : 0.01466541290283203
	model : 4.721531248092651
			 train-loss:  2.1178047804350264 	 ± 0.24631800204844084
	data : 0.01463460922241211
	model : 4.727622413635254
			 train-loss:  2.1148011061880325 	 ± 0.24657936216911178
	data : 0.01461639404296875
	model : 4.716897821426391
			 train-loss:  2.1104559204080604 	 ± 0.2486614009975749
	data : 0.014618635177612305
	model : 4.725403642654419
			 train-loss:  2.1099663078784943 	 ± 0.24735038830981454
	data : 0.014615917205810547
	model : 4.702054834365844
			 train-loss:  2.113392651721995 	 ± 0.2482023486554829
	data : 0.014603710174560547
	model : 4.700055932998657
			 train-loss:  2.111911915718241 	 ± 0.24729122450182073
	data : 0.014591503143310546
	model : 4.696008157730103
			 train-loss:  2.1146397465153743 	 ± 0.24740390897493483
	data : 0.011997175216674805
	model : 4.6895593166351315
			 train-loss:  2.1242239822944007 	 ± 0.2632443224101904
	data : 0.012032270431518555
	model : 4.716187000274658
			 train-loss:  2.119197160927291 	 ± 0.2664751074353568
	data : 0.012029266357421875
	model : 4.698727083206177
			 train-loss:  2.121965251406845 	 ± 0.26651012539918467
	data : 0.011975669860839843
	model : 4.6911825180053714
			 train-loss:  2.120269274470782 	 ± 0.2656916955087694
	data : 0.011966705322265625
	model : 4.699518060684204
			 train-loss:  2.1199335074424743 	 ± 0.2643810082096178
	data : 0.014555215835571289
	model : 4.715324640274048
			 train-loss:  2.1226505949945733 	 ± 0.26446837491461445
	data : 0.014458179473876953
	model : 4.7056649684906
			 train-loss:  2.119685293412676 	 ± 0.2648507030257919
	data : 0.014498186111450196
	model : 4.729340839385986
			 train-loss:  2.11846822905309 	 ± 0.26384835387840616
	data : 0.014556455612182616
	model : 4.749743795394897
			 train-loss:  2.1177621415028205 	 ± 0.2626745539912744
	data : 0.017201757431030272
	model : 4.71962456703186
			 train-loss:  2.1195502689906527 	 ± 0.2620559631935549
	data : 0.017101860046386717
	model : 4.691322660446167
			 train-loss:  2.117609732555893 	 ± 0.2615738190943269
	data : 0.018128347396850587
	model : 4.676369285583496
			 train-loss:  2.117207057008119 	 ± 0.26038164905505407
	data : 0.01887235641479492
	model : 4.6674590587615965
			 train-loss:  2.1179462560900935 	 ± 0.25928614455032317
	data : 0.018888235092163086
	model : 4.639254570007324
			 train-loss:  2.1247293708521293 	 ± 0.2675475133463717
	data : 0.018802070617675783
	model : 4.639573478698731
			 train-loss:  2.1264957232908768 	 ± 0.26696630785223885
	data : 0.01899123191833496
	model : 4.641619873046875
			 train-loss:  2.1261094153464377 	 ± 0.26579191876806224
	data : 0.017949914932250975
	model : 4.63473482131958
			 train-loss:  2.12883306826864 	 ± 0.2661541127681125
	data : 0.017094230651855467
	model : 4.641831874847412
			 train-loss:  2.131432442538506 	 ± 0.2663979772864796
	data : 0.017040491104125977
	model : 4.6429352283477785
			 train-loss:  2.134363377303408 	 ± 0.2670506876986733
	data : 0.014495563507080079
	model : 4.645451450347901
			 train-loss:  2.1298526846844217 	 ± 0.2702136445923474
	data : 0.014412546157836914
	model : 4.6633227348327635
			 train-loss:  2.1294405090397803 	 ± 0.2690827153678883
	data : 0.012007808685302735
	model : 4.6571966171264645
			 train-loss:  2.1261318585811515 	 ± 0.2702897125085721
	data : 0.012023544311523438
	model : 4.665127563476562
			 train-loss:  2.129617794085357 	 ± 0.27177041760502824
	data : 0.012111568450927734
	model : 4.665734767913818
			 train-loss:  2.131564500952969 	 ± 0.27145105536034314
	data : 0.014715814590454101
	model : 4.683340692520142
			 train-loss:  2.1340033650398254 	 ± 0.27162372331377854
	data : 0.01471099853515625
	model : 4.654492330551148
			 train-loss:  2.130625096234408 	 ± 0.2730187238725918
	data : 0.017172479629516603
	model : 4.660870742797852
			 train-loss:  2.1333555100394075 	 ± 0.27355131264106314
	data : 0.017163801193237304
	model : 4.64665675163269
			 train-loss:  2.1324193710234107 	 ± 0.27263319592522944
	data : 0.016973543167114257
	model : 4.657475328445434
			 train-loss:  2.1319488229290133 	 ± 0.27158178757341966
	data : 0.01691122055053711
	model : 4.668785619735718
			 train-loss:  2.13256929397583 	 ± 0.2705815072367544
	data : 0.016876697540283203
	model : 4.682431888580322
			 train-loss:  2.132829741826133 	 ± 0.2695213626183944
	data : 0.014337158203125
	model : 4.677889347076416
			 train-loss:  2.131135817587845 	 ± 0.26913068257221384
	data : 0.014370918273925781
	model : 4.680218458175659
