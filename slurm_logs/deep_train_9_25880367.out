model 3c6d64cd39a55f2d10a349cd85e8bd0f65fc75eaa73e8847d745eb71 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=110.0, disp=1, domain='global', epoch=6, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  6    val-loss:  2.1060655430743567  train-loss:  1.9765453594736755  lr:  0.005
#epoch  7    val-loss:  2.0786419103020117  train-loss:  1.971662403549999  lr:  0.005
#epoch  8    val-loss:  2.0952999591827393  train-loss:  1.9716613097116351  lr:  0.005
#epoch  9    val-loss:  2.139788031578064  train-loss:  1.967895658686757  lr:  0.0025
#epoch  10    val-loss:  2.0931109754662764  train-loss:  1.9696500580757856  lr:  0.0025
#epoch  11    val-loss:  2.1069544114564596  train-loss:  1.9682394424453378  lr:  0.0025
#epoch  12    val-loss:  2.1158681166799447  train-loss:  1.9700365900062025  lr:  0.00125
#epoch  13    val-loss:  2.071213439891213  train-loss:  1.9664557483047247  lr:  0.00125
#epoch  14    val-loss:  2.0557281469043933  train-loss:  1.9673411436378956  lr:  0.00125
#epoch  15    val-loss:  2.1020412068617973  train-loss:  1.9712266330607235  lr:  0.00125
#epoch  16    val-loss:  2.1098038146370337  train-loss:  1.9729662681929767  lr:  0.00125
#epoch  17    val-loss:  2.0930213363547074  train-loss:  1.9754316713660955  lr:  0.00125
#epoch  18    val-loss:  2.0729973692643013  train-loss:  1.9665188072249293  lr:  0.000625
#epoch  19    val-loss:  2.0693748561959517  train-loss:  1.966051110997796  lr:  0.000625
#epoch  20    val-loss:  2.115689390584042  train-loss:  1.9656819654628634  lr:  0.000625
#epoch  21    val-loss:  2.0580703835738334  train-loss:  1.966571083292365  lr:  0.0003125
#epoch  22    val-loss:  2.086048176414088  train-loss:  1.9669087026268244  lr:  0.0003125
#epoch  23    val-loss:  2.117563931565536  train-loss:  1.9644343368709087  lr:  0.0003125
#epoch  24    val-loss:  2.0950641004662764  train-loss:  1.9664252735674381  lr:  0.00015625
#epoch  25    val-loss:  2.1423481200870715  train-loss:  1.967194541823119  lr:  0.00015625
#epoch  26    val-loss:  2.102847638883089  train-loss:  1.9644154007546604  lr:  0.00015625
#epoch  27    val-loss:  2.1004841829601086  train-loss:  1.9648964814841747  lr:  7.8125e-05
#epoch  28    val-loss:  2.121919738618951  train-loss:  1.9646701239980757  lr:  7.8125e-05
#epoch  29    val-loss:  2.075012150563692  train-loss:  1.9644707799889147  lr:  7.8125e-05
#epoch  30    val-loss:  2.080888139574151  train-loss:  1.9681850355118513  lr:  3.90625e-05
#epoch  31    val-loss:  2.109622491033454  train-loss:  1.968101226259023  lr:  3.90625e-05
#epoch  32    val-loss:  2.128900935775355  train-loss:  1.9664798281155527  lr:  3.90625e-05
#epoch  33    val-loss:  2.0813933485432674  train-loss:  1.9690523953177035  lr:  1.953125e-05
#epoch  34    val-loss:  2.108407547599391  train-loss:  1.9675030685029924  lr:  1.953125e-05
#epoch  35    val-loss:  2.131213200719733  train-loss:  1.969861976802349  lr:  1.953125e-05
#epoch  36    val-loss:  2.0656417859228036  train-loss:  1.9650260768830776  lr:  9.765625e-06
#epoch  37    val-loss:  2.1276221087104394  train-loss:  1.9697423428297043  lr:  9.765625e-06
#epoch  38    val-loss:  2.1290459193681417  train-loss:  1.9651292255148292  lr:  9.765625e-06
#epoch  39    val-loss:  2.076409854386982  train-loss:  1.966268032323569  lr:  4.8828125e-06
#epoch  40    val-loss:  2.07903874547858  train-loss:  1.9647966935299337  lr:  4.8828125e-06
#epoch  41    val-loss:  2.081344039816605  train-loss:  1.9666783404536545  lr:  4.8828125e-06
#epoch  42    val-loss:  2.092299084914358  train-loss:  1.969020688906312  lr:  2.44140625e-06
#epoch  43    val-loss:  2.0737161573610807  train-loss:  1.9628110355697572  lr:  2.44140625e-06
#epoch  44    val-loss:  2.1167915369334973  train-loss:  1.9669358334504068  lr:  2.44140625e-06
#epoch  45    val-loss:  2.072778080639086  train-loss:  1.965422245208174  lr:  1.220703125e-06
#epoch  46    val-loss:  2.091845568857695  train-loss:  1.9690353404730558  lr:  1.220703125e-06
#epoch  47    val-loss:  2.125760843879298  train-loss:  1.967995895538479  lr:  1.220703125e-06
#epoch  48    val-loss:  2.114965558052063  train-loss:  1.9700234974734485  lr:  6.103515625e-07
#epoch  49    val-loss:  2.0657445066853573  train-loss:  1.9734875615686178  lr:  6.103515625e-07
#epoch  50    val-loss:  2.071689015940616  train-loss:  1.9619986987672746  lr:  6.103515625e-07
#epoch  51    val-loss:  2.071072534510964  train-loss:  1.9689659178256989  lr:  3.0517578125e-07
#epoch  52    val-loss:  2.070193598144933  train-loss:  1.966664629522711  lr:  3.0517578125e-07
#epoch  53    val-loss:  2.1383002306285657  train-loss:  1.9669950930401683  lr:  3.0517578125e-07
#epoch  54    val-loss:  2.080157010178817  train-loss:  1.962928663007915  lr:  1.52587890625e-07
#epoch  55    val-loss:  2.0785093244753385  train-loss:  1.9705091957002878  lr:  1.52587890625e-07
#epoch  56    val-loss:  2.085679104453639  train-loss:  1.9609207375906408  lr:  1.52587890625e-07
#epoch  57    val-loss:  2.09443293747149  train-loss:  1.973229840863496  lr:  7.62939453125e-08
#epoch  58    val-loss:  2.0921298767391003  train-loss:  1.9704705467447639  lr:  7.62939453125e-08
#epoch  59    val-loss:  2.0909911017668876  train-loss:  1.963265215512365  lr:  7.62939453125e-08
#epoch  60    val-loss:  2.0778410685689828  train-loss:  1.97174880374223  lr:  3.814697265625e-08
#epoch  61    val-loss:  2.084690909636648  train-loss:  1.96553573012352  lr:  3.814697265625e-08
#epoch  62    val-loss:  2.098986688413118  train-loss:  1.966098925564438  lr:  3.814697265625e-08
#epoch  63    val-loss:  2.0849499200519763  train-loss:  1.9673876143060625  lr:  1.9073486328125e-08
#epoch  64    val-loss:  2.064428072226675  train-loss:  1.967941369395703  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.0789261491675126  train-loss:  1.972551538143307  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.0923266912761487  train-loss:  1.9674962707795203  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.0955951903995715  train-loss:  1.9655241980217397  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.1087901843221566  train-loss:  1.9664490497671068  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.0922636672070154  train-loss:  1.9724156158044934  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.1099727655711926  train-loss:  1.9707361478358507  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.0800228934538993  train-loss:  1.9634712738916278  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.097050164875231  train-loss:  1.968253676313907  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.0856419801712036  train-loss:  1.970658911857754  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.124074176738137  train-loss:  1.9763900022953749  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.0639562230361137  train-loss:  1.9668281604535878  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.0774906434510885  train-loss:  1.971819568425417  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.0588181520763196  train-loss:  1.9675570530816913  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.154670106737237  train-loss:  1.964578552171588  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.08798212754099  train-loss:  1.9678189754486084  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.097123955425463  train-loss:  1.969585376791656  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.097321083671168  train-loss:  1.9694876624271274  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.0890692911650004  train-loss:  1.966079957317561  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.1234619931170813  train-loss:  1.9731504512019455  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.093654237295452  train-loss:  1.9680030774325132  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.1098703145980835  train-loss:  1.964362851344049  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.0943316283978914  train-loss:  1.9700398007407784  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.1088282748272547  train-loss:  1.969031669665128  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.0939540988520573  train-loss:  1.9715252248570323  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.1020132867913497  train-loss:  1.9653249294497073  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.0861995471151253  train-loss:  1.966880941297859  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.1171950101852417  train-loss:  1.9686288726516068  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.1550012324985706  train-loss:  1.961323190946132  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.09678946670733  train-loss:  1.967529637273401  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.08321970387509  train-loss:  1.9715720666572452  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.0846913927479793  train-loss:  1.9697269541211426  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.082564027685868  train-loss:  1.9641961362212896  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.076413788293537  train-loss:  1.9629037296399474  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.1046011447906494  train-loss:  1.967686536256224  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.1044197208002995  train-loss:  1.9709398741833866  lr:  1.9073486328125e-08
