model 864978db40811e9c0321c69837e4365df7afdaf4b6cc62630415bbfd state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=55.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=True, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  2.1742072607341565  train-loss:  2.073130298871547  lr:  0.0025
#epoch  8    val-loss:  2.1622918467772636  train-loss:  2.0664641833864152  lr:  0.0025
#epoch  9    val-loss:  2.158334355605276  train-loss:  2.0665820315480232  lr:  0.0025
#epoch  10    val-loss:  2.1980580342443368  train-loss:  2.0601041112095118  lr:  0.00125
#epoch  11    val-loss:  2.1591215321892188  train-loss:  2.0626806770451367  lr:  0.00125
#epoch  12    val-loss:  2.170360383234526  train-loss:  2.0619999142363667  lr:  0.00125
#epoch  13    val-loss:  2.1996961769304777  train-loss:  2.0617290809750557  lr:  0.000625
#epoch  14    val-loss:  2.144295102671573  train-loss:  2.0600980827584863  lr:  0.000625
#epoch  15    val-loss:  2.1284462778191817  train-loss:  2.060869373381138  lr:  0.000625
#epoch  16    val-loss:  2.19183896089855  train-loss:  2.063952420838177  lr:  0.000625
#epoch  17    val-loss:  2.185107318978561  train-loss:  2.0651765731163323  lr:  0.000625
#epoch  18    val-loss:  2.1686208561847082  train-loss:  2.066649745684117  lr:  0.000625
#epoch  19    val-loss:  2.1459325551986694  train-loss:  2.0594053054228425  lr:  0.0003125
#epoch  20    val-loss:  2.151011404238249  train-loss:  2.059241207782179  lr:  0.0003125
#epoch  21    val-loss:  2.167062433142411  train-loss:  2.0600843369029462  lr:  0.0003125
#epoch  22    val-loss:  2.1370695954874943  train-loss:  2.0586509313434362  lr:  0.00015625
#epoch  23    val-loss:  2.1722624490135596  train-loss:  2.0580140710808337  lr:  0.00015625
#epoch  24    val-loss:  2.1982711741798804  train-loss:  2.056774069555104  lr:  0.00015625
#epoch  25    val-loss:  2.1813254858318127  train-loss:  2.059634738601744  lr:  7.8125e-05
#epoch  26    val-loss:  2.2140676410574662  train-loss:  2.060208009555936  lr:  7.8125e-05
#epoch  27    val-loss:  2.1771943757408545  train-loss:  2.0562397381290793  lr:  7.8125e-05
#epoch  28    val-loss:  2.1895685509631506  train-loss:  2.0589046766981483  lr:  3.90625e-05
#epoch  29    val-loss:  2.17833117434853  train-loss:  2.058217736892402  lr:  3.90625e-05
#epoch  30    val-loss:  2.156452072294135  train-loss:  2.059748948086053  lr:  3.90625e-05
#epoch  31    val-loss:  2.157579723157381  train-loss:  2.057583292014897  lr:  1.953125e-05
#epoch  32    val-loss:  2.196176767349243  train-loss:  2.060604715254158  lr:  1.953125e-05
#epoch  33    val-loss:  2.196603618170086  train-loss:  2.05829244479537  lr:  1.953125e-05
#epoch  34    val-loss:  2.1606119243722213  train-loss:  2.0630204249173403  lr:  9.765625e-06
#epoch  35    val-loss:  2.174726134852359  train-loss:  2.058447527233511  lr:  9.765625e-06
#epoch  36    val-loss:  2.1960848569869995  train-loss:  2.063610435463488  lr:  9.765625e-06
#epoch  37    val-loss:  2.148029139167384  train-loss:  2.0608252715319395  lr:  4.8828125e-06
#epoch  38    val-loss:  2.1810146444722225  train-loss:  2.06199800549075  lr:  4.8828125e-06
#epoch  39    val-loss:  2.199823944192184  train-loss:  2.0555046261288226  lr:  4.8828125e-06
#epoch  40    val-loss:  2.1524713541332043  train-loss:  2.0595532236620784  lr:  2.44140625e-06
#epoch  41    val-loss:  2.1571252534264014  train-loss:  2.0585471596568823  lr:  2.44140625e-06
#epoch  42    val-loss:  2.1432197407672278  train-loss:  2.0570033001713455  lr:  2.44140625e-06
#epoch  43    val-loss:  2.17441861880453  train-loss:  2.063096242491156  lr:  1.220703125e-06
#epoch  44    val-loss:  2.156489522833573  train-loss:  2.0564561276696622  lr:  1.220703125e-06
#epoch  45    val-loss:  2.178971660764594  train-loss:  2.0586950723081827  lr:  1.220703125e-06
#epoch  46    val-loss:  2.147438595169469  train-loss:  2.0587604511529207  lr:  6.103515625e-07
#epoch  47    val-loss:  2.1717561295157983  train-loss:  2.0611912268213928  lr:  6.103515625e-07
#epoch  48    val-loss:  2.1971944131349264  train-loss:  2.059260305482894  lr:  6.103515625e-07
#epoch  49    val-loss:  2.16850535493148  train-loss:  2.0625956314615905  lr:  3.0517578125e-07
#epoch  50    val-loss:  2.145595726213957  train-loss:  2.0645669754594564  lr:  3.0517578125e-07
#epoch  51    val-loss:  2.1527958982869198  train-loss:  2.0550044737756252  lr:  3.0517578125e-07
#epoch  52    val-loss:  2.155009495584588  train-loss:  2.0612628255039454  lr:  1.52587890625e-07
#epoch  53    val-loss:  2.160384410306027  train-loss:  2.0582005213946104  lr:  1.52587890625e-07
#epoch  54    val-loss:  2.189408390145553  train-loss:  2.0586936944164336  lr:  1.52587890625e-07
#epoch  55    val-loss:  2.1543489192661487  train-loss:  2.0538441939279437  lr:  7.62939453125e-08
#epoch  56    val-loss:  2.1644472385707654  train-loss:  2.062522668391466  lr:  7.62939453125e-08
#epoch  57    val-loss:  2.1702725887298584  train-loss:  2.053685792721808  lr:  7.62939453125e-08
#epoch  58    val-loss:  2.2033693978660986  train-loss:  2.066879445221275  lr:  3.814697265625e-08
#epoch  59    val-loss:  2.1706194501174125  train-loss:  2.061745062470436  lr:  3.814697265625e-08
#epoch  60    val-loss:  2.1712082373468498  train-loss:  2.0566208017989993  lr:  3.814697265625e-08
#epoch  61    val-loss:  2.1553250488482023  train-loss:  2.0648211264051497  lr:  1.9073486328125e-08
#epoch  62    val-loss:  2.1479859289370085  train-loss:  2.057907536160201  lr:  1.9073486328125e-08
#epoch  63    val-loss:  2.182250612660458  train-loss:  2.06068554520607  lr:  1.9073486328125e-08
#epoch  64    val-loss:  2.162978071915476  train-loss:  2.0623841201886535  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.138017328161942  train-loss:  2.0607681409455836  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.160704399410047  train-loss:  2.064654828514904  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.160700948614823  train-loss:  2.0600848947651684  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.1609550087075484  train-loss:  2.0586939677596092  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.181651993801719  train-loss:  2.05920576909557  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.168300245937548  train-loss:  2.0635723057202995  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.2010346964785925  train-loss:  2.063086660578847  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.153784902472245  train-loss:  2.056483343243599  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.176704632608514  train-loss:  2.062345757614821  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.1605557579743233  train-loss:  2.062702896539122  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.197106436679238  train-loss:  2.0696380673907697  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.1378770752956995  train-loss:  2.0601864075288177  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.1495234401602494  train-loss:  2.0658599543385208  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.1211407059117366  train-loss:  2.061190854758024  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.224722906162864  train-loss:  2.0561926905065775  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.16459680230994  train-loss:  2.0605440991930664  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.1671833239103617  train-loss:  2.060735766310245  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.1593473020352816  train-loss:  2.0617627846077085  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.1613135965246904  train-loss:  2.0574106122367084  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.2004217097633765  train-loss:  2.066432491876185  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.1659490246521798  train-loss:  2.060811101924628  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.1865425423571936  train-loss:  2.0583335598930717  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.1658743619918823  train-loss:  2.0607038279995322  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.167252785281131  train-loss:  2.0623563402332366  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.1670174849660775  train-loss:  2.0634177504107356  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.1762088035282336  train-loss:  2.0582862086594105  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.164315800917776  train-loss:  2.0595501982606947  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.19112807825992  train-loss:  2.0625023702159524  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.2233505750957288  train-loss:  2.052779402118176  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.165411541336461  train-loss:  2.0610697893425822  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.1646564571481  train-loss:  2.064631974324584  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.156770197968734  train-loss:  2.063678394537419  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.1604816411670886  train-loss:  2.057304146233946  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.154963361589532  train-loss:  2.0564066572114825  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.1732386413373446  train-loss:  2.059045840986073  lr:  1.9073486328125e-08
