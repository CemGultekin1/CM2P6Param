model 0c38f58a1dbe330abbbb287b200b1338e4cf36c376206001686ee678 state_dict has been found
Loaded the existing model
Loaded the existing model
torch.cuda.is_available():	 True
runargs:	 Namespace(co2=False, depth=110.0, disp=1, domain='global', epoch=7, latitude=False, linsupres=False, lossfun='MSE', lr=0.01, lsrp_span=12, maxepoch=100, minibatch=16, mode='train', normalization='standard', num_workers=40, parts=[2, 2], persistent_workers=False, prefetch_factor=1, relog=False, rerun=False, sigma=8, temperature=True)
data_address /scratch/zanna/data/cm2.6/coarse_8_beneath_surface.zarr
using device: cuda:0
epochs started
#epoch  7    val-loss:  2.4639407270833065  train-loss:  2.224754439201206  lr:  0.0025
#epoch  8    val-loss:  2.4210351140875566  train-loss:  2.2175059518776834  lr:  0.00125
#epoch  9    val-loss:  2.4324208686226294  train-loss:  2.219874680507928  lr:  0.00125
#epoch  10    val-loss:  2.5048189037724544  train-loss:  2.2133915494196117  lr:  0.00125
#epoch  11    val-loss:  2.439563594366375  train-loss:  2.2164559438824654  lr:  0.000625
#epoch  12    val-loss:  2.433793155770553  train-loss:  2.21537381131202  lr:  0.000625
#epoch  13    val-loss:  2.4770215059581555  train-loss:  2.211576537694782  lr:  0.000625
#epoch  14    val-loss:  2.408958158994976  train-loss:  2.216642954852432  lr:  0.0003125
#epoch  15    val-loss:  2.3969101905822754  train-loss:  2.2143325693905354  lr:  0.0003125
#epoch  16    val-loss:  2.4285390565269873  train-loss:  2.2179488986730576  lr:  0.0003125
#epoch  17    val-loss:  2.449693303359182  train-loss:  2.2215986158698797  lr:  0.0003125
#epoch  18    val-loss:  2.442487095531664  train-loss:  2.223037023562938  lr:  0.0003125
#epoch  19    val-loss:  2.404871940612793  train-loss:  2.211957275867462  lr:  0.00015625
#epoch  20    val-loss:  2.402390567879928  train-loss:  2.212465768214315  lr:  0.00015625
#epoch  21    val-loss:  2.442892451035349  train-loss:  2.215918696951121  lr:  0.00015625
#epoch  22    val-loss:  2.389534115791321  train-loss:  2.2138653052970767  lr:  7.8125e-05
#epoch  23    val-loss:  2.430074327870419  train-loss:  2.210533333942294  lr:  7.8125e-05
#epoch  24    val-loss:  2.461600140521401  train-loss:  2.2108576027676463  lr:  7.8125e-05
#epoch  25    val-loss:  2.4428742496590865  train-loss:  2.2138222348876297  lr:  7.8125e-05
#epoch  26    val-loss:  2.4627980182045386  train-loss:  2.21486679231748  lr:  3.90625e-05
#epoch  27    val-loss:  2.4343187683507015  train-loss:  2.2119720741175115  lr:  3.90625e-05
#epoch  28    val-loss:  2.461010161199068  train-loss:  2.2108940044417977  lr:  3.90625e-05
#epoch  29    val-loss:  2.4712525543413664  train-loss:  2.2128302515484393  lr:  1.953125e-05
#epoch  30    val-loss:  2.4176840656682064  train-loss:  2.21513012656942  lr:  1.953125e-05
#epoch  31    val-loss:  2.435263006310714  train-loss:  2.2168649546802044  lr:  1.953125e-05
#epoch  32    val-loss:  2.4726551959389136  train-loss:  2.214226750191301  lr:  9.765625e-06
#epoch  33    val-loss:  2.475716967331736  train-loss:  2.2098195673897862  lr:  9.765625e-06
#epoch  34    val-loss:  2.4077610091159216  train-loss:  2.2173013403080404  lr:  9.765625e-06
#epoch  35    val-loss:  2.4463787957241663  train-loss:  2.2129479283466935  lr:  4.8828125e-06
#epoch  36    val-loss:  2.473038591836628  train-loss:  2.2146248249337077  lr:  4.8828125e-06
#epoch  37    val-loss:  2.4328268452694544  train-loss:  2.213047978002578  lr:  4.8828125e-06
#epoch  38    val-loss:  2.4596309410898307  train-loss:  2.2140488759614527  lr:  2.44140625e-06
#epoch  39    val-loss:  2.4623564858185616  train-loss:  2.2127854940481484  lr:  2.44140625e-06
#epoch  40    val-loss:  2.421555619490774  train-loss:  2.213308985810727  lr:  2.44140625e-06
#epoch  41    val-loss:  2.4088052322990015  train-loss:  2.2116976380348206  lr:  1.220703125e-06
#epoch  42    val-loss:  2.4264246476323983  train-loss:  2.2142853918485343  lr:  1.220703125e-06
#epoch  43    val-loss:  2.4254080998270133  train-loss:  2.2161147138103843  lr:  1.220703125e-06
#epoch  44    val-loss:  2.4146253999910856  train-loss:  2.210093178320676  lr:  6.103515625e-07
#epoch  45    val-loss:  2.4612776480223  train-loss:  2.2136803260073066  lr:  6.103515625e-07
#epoch  46    val-loss:  2.4033162154649435  train-loss:  2.2126694014295936  lr:  6.103515625e-07
#epoch  47    val-loss:  2.4358786030819544  train-loss:  2.2155138701200485  lr:  3.0517578125e-07
#epoch  48    val-loss:  2.4802280915410897  train-loss:  2.2155662314035  lr:  3.0517578125e-07
#epoch  49    val-loss:  2.4609010721507825  train-loss:  2.2203331808559597  lr:  3.0517578125e-07
#epoch  50    val-loss:  2.4080267768157158  train-loss:  2.223750793375075  lr:  1.52587890625e-07
#epoch  51    val-loss:  2.412741780281067  train-loss:  2.207183249294758  lr:  1.52587890625e-07
#epoch  52    val-loss:  2.4246804337752494  train-loss:  2.216984767932445  lr:  1.52587890625e-07
#epoch  53    val-loss:  2.408718422839516  train-loss:  2.2140488824807107  lr:  7.62939453125e-08
#epoch  54    val-loss:  2.4911323597556665  train-loss:  2.219939728733152  lr:  7.62939453125e-08
#epoch  55    val-loss:  2.412703689775969  train-loss:  2.207528182771057  lr:  7.62939453125e-08
#epoch  56    val-loss:  2.416194489127711  train-loss:  2.21948217228055  lr:  3.814697265625e-08
#epoch  57    val-loss:  2.417389248546801  train-loss:  2.2070975266397  lr:  3.814697265625e-08
#epoch  58    val-loss:  2.453243280711927  train-loss:  2.2208813880570233  lr:  3.814697265625e-08
#epoch  59    val-loss:  2.444948384636327  train-loss:  2.2172239208593965  lr:  1.9073486328125e-08
#epoch  60    val-loss:  2.452064313386616  train-loss:  2.2089200392365456  lr:  1.9073486328125e-08
#epoch  61    val-loss:  2.4208038229691353  train-loss:  2.2183029549196362  lr:  1.9073486328125e-08
#epoch  62    val-loss:  2.4253170113814506  train-loss:  2.215790937654674  lr:  1.9073486328125e-08
#epoch  63    val-loss:  2.4279850470392326  train-loss:  2.2119300356134772  lr:  1.9073486328125e-08
#epoch  64    val-loss:  2.419827674564562  train-loss:  2.215055264532566  lr:  1.9073486328125e-08
#epoch  65    val-loss:  2.4106475930464897  train-loss:  2.216595428530127  lr:  1.9073486328125e-08
#epoch  66    val-loss:  2.4145036998548006  train-loss:  2.2175034256651998  lr:  1.9073486328125e-08
#epoch  67    val-loss:  2.4206178251065706  train-loss:  2.216215332504362  lr:  1.9073486328125e-08
#epoch  68    val-loss:  2.434561277690687  train-loss:  2.2114520808681846  lr:  1.9073486328125e-08
#epoch  69    val-loss:  2.4506748350043046  train-loss:  2.210912168957293  lr:  1.9073486328125e-08
#epoch  70    val-loss:  2.437325163891441  train-loss:  2.223521466832608  lr:  1.9073486328125e-08
#epoch  71    val-loss:  2.4633467824835527  train-loss:  2.218838200904429  lr:  1.9073486328125e-08
#epoch  72    val-loss:  2.416886480231034  train-loss:  2.2116013509221375  lr:  1.9073486328125e-08
#epoch  73    val-loss:  2.4298149723755684  train-loss:  2.216188552323729  lr:  1.9073486328125e-08
#epoch  74    val-loss:  2.4170183194311043  train-loss:  2.2206737231463194  lr:  1.9073486328125e-08
#epoch  75    val-loss:  2.485608646744176  train-loss:  2.2254504039883614  lr:  1.9073486328125e-08
#epoch  76    val-loss:  2.3980050777134143  train-loss:  2.211433299817145  lr:  1.9073486328125e-08
#epoch  77    val-loss:  2.4313578605651855  train-loss:  2.2196894828230143  lr:  1.9073486328125e-08
#epoch  78    val-loss:  2.3876605159357975  train-loss:  2.2145796911790967  lr:  1.9073486328125e-08
#epoch  79    val-loss:  2.483978033065796  train-loss:  2.212640816345811  lr:  1.9073486328125e-08
#epoch  80    val-loss:  2.4307326580348767  train-loss:  2.212729523424059  lr:  1.9073486328125e-08
#epoch  81    val-loss:  2.449306801745766  train-loss:  2.2194414902478456  lr:  1.9073486328125e-08
#epoch  82    val-loss:  2.46461410271494  train-loss:  2.2176105892285705  lr:  1.9073486328125e-08
#epoch  83    val-loss:  2.419669565401579  train-loss:  2.2106562950648367  lr:  1.9073486328125e-08
#epoch  84    val-loss:  2.453951277230915  train-loss:  2.2187724211253226  lr:  1.9073486328125e-08
#epoch  85    val-loss:  2.425640194039596  train-loss:  2.2152424943633378  lr:  1.9073486328125e-08
#epoch  86    val-loss:  2.4542820265418603  train-loss:  2.2118049152195454  lr:  1.9073486328125e-08
#epoch  87    val-loss:  2.450603635687577  train-loss:  2.217498505488038  lr:  1.9073486328125e-08
#epoch  88    val-loss:  2.4512172811909725  train-loss:  2.2158502456732094  lr:  1.9073486328125e-08
#epoch  89    val-loss:  2.435743206425717  train-loss:  2.2195909661240876  lr:  1.9073486328125e-08
#epoch  90    val-loss:  2.45902953649822  train-loss:  2.2170692957006395  lr:  1.9073486328125e-08
#epoch  91    val-loss:  2.422263741493225  train-loss:  2.2178366729058325  lr:  1.9073486328125e-08
#epoch  92    val-loss:  2.465863409795259  train-loss:  2.2179856733419  lr:  1.9073486328125e-08
#epoch  93    val-loss:  2.502672421304803  train-loss:  2.207571600563824  lr:  1.9073486328125e-08
#epoch  94    val-loss:  2.4349191188812256  train-loss:  2.213613540865481  lr:  1.9073486328125e-08
#epoch  95    val-loss:  2.4186783966265226  train-loss:  2.222038039471954  lr:  1.9073486328125e-08
#epoch  96    val-loss:  2.419816813970867  train-loss:  2.2174222813919187  lr:  1.9073486328125e-08
#epoch  97    val-loss:  2.4160617652692293  train-loss:  2.213291776832193  lr:  1.9073486328125e-08
#epoch  98    val-loss:  2.429429204840409  train-loss:  2.211442125029862  lr:  1.9073486328125e-08
#epoch  99    val-loss:  2.435903135098909  train-loss:  2.2136256732046604  lr:  1.9073486328125e-08
